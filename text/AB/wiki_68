<doc id="21927" url="http://en.wikipedia.org/wiki?curid=21927" title="National Party of Australia">
National Party of Australia

The National Party of Australia (also known as The Nationals or simply, The Nats) is an Australian political party. Traditionally representing graziers, farmers and rural voters generally, it began as the Australian Country Party, and then adopted the name the National Country Party in 1975. The party's name was changed to the National Party of Australia in 1982. The party is commonly referred to as "The Nationals". Federally, in New South Wales, and to an extent Victoria and historically in Western Australia, it has generally been the minor party in a centre-right Coalition with the Liberal Party of Australia in government. In Opposition it has worked in formal Coalition or separately, but generally in co-operation with the Liberal Party and its predecessor, the United Australia Party. It was the major Coalition party in Queensland between 1936 and 2008, when it merged with the junior Queensland Division of the Liberal Party of Australia to form the Liberal National Party of Queensland (LNP).
The National Party of Australia party's federal parliamentary leader since 3 December 2007, following the coalition's defeat at the 2007 federal election, is Warren Truss.
History.
According to historian B. D. Graham (1959), the graziers who operated the sheep stations were politically conservative. They disliked the Labor party, which represented their workers, and feared that Labor governments would pass unfavorable legislation and listen to foreigners and Communists. The graziers were satisfied with the marketing organisation of their industry, opposed any change in land tenure and labour relations, and advocated lower tariffs, low freight rates, and low taxes. On the other hand, Graham reports, the small farmers, not the graziers, founded the Country party. The farmers advocated government intervention in the market through price support schemes and marketing pools. The graziers often politically and financially supported the Country party, which in turn made the Country party more conservative.
The Country Party was formally founded in 1913 in Western Australia, and nationally in 1920 from a number of state-based parties such as the Victorian Farmers' Union (VFU) and the Farmers and Settlers Party of New South Wales. Australia's first Country Party was founded in 1912 by Harry J. Stephens, editor of The Farmer & Settler, but under fierce opposition from rival newspapers, failed to gain momentum.
The VFU won a seat in the House of Representatives in 1918, and at the 1919 federal election the state-based country parties won seats in New South Wales, Victoria and Western Australia. They also began to win seats in the state parliaments. In 1920 the Country Party was established as a national party led by William McWilliams from Tasmania. In his first speech as leader, McWilliams laid out the principles of the new party, stating "we crave no alliance, we spurn no support but we intend drastic action to secure closer attention to the needs of primary producers" McWilliams was deposed as party leader in favour of Dr Earle Page in April 1921 following instances where McWilliams voted against the party line. McWilliams would later leave the Country Party to sit as an Independent.
At the 1922 election, it won enough seats to deny the Nationalists an overall majority, and was the Nationalists' only realistic coalition partner. However, Page let it be known that his party would not serve under Hughes, and forced his resignation. Page then entered negotiations with the Nationalists' new leader, Stanley Bruce, for a coalition government. Page's terms were stiff—five seats in a Cabinet of 11, including the Treasurer portfolio and the second rank in the ministry for himself. Nonetheless, Bruce readily agreed, and the "Bruce-Page Ministry" was formed—thus beginning the tradition of the party's leader ranking second in Coalition cabinets.
Page remained dominant in the party until 1939 and briefly served as an interim Prime Minister between the death of Joseph Lyons and the election of Robert Menzies as his successor, but Page's refusal to serve under Menzies led to his resignation as leader. The coalition was re-formed under Archie Cameron in 1940, and continued until October 1941 despite the election of Arthur Fadden as leader after the 1940 Election. Fadden was well regarded within conservative circles and proved to be a loyal deputy to Menzies in the difficult circumstances of 1941. When Menzies was forced to resign as Prime Minister, the UAP was so bereft of leadership that Fadden briefly succeeded him (despite the Country Party being the junior partner in the governing coalition). However, the two independents who had been propping up the government rejected Fadden's budget and brought the government down. Fadden stood down in favour of Labor leader John Curtin. 
The Fadden-led Coalition made almost no headway against Curtin, and was severely defeated in the 1943 election. After that loss, Fadden became deputy Leader of the Opposition under Menzies, a role that continued after Menzies folded the UAP into the Liberal Party of Australia in 1944. Fadden remained a loyal partner of Menzies, though he was still keen to assert the independence of his party. Indeed, in the lead up to the 1949 federal election, Fadden played a key role in the defeat of the Chifley Labor government, frequently making inflammatory claims about the "socialist" nature of the Labor Party, which Menzies could then "clarify" or repudiate as he saw fit, thus appearing more "moderate". In 1949, Fadden became Treasurer in the second Menzies government and remained so until his retirement in 1958. His successful partnership with Menzies was one of the elements that sustained the coalition, which remained in office until 1972 (Menzies himself retired in 1966).
Fadden's successor, Trade Minister John McEwen, took the then unusual step of declining to serve as Treasurer, believing he could better ensure that the interests of Australian primary producers were safeguarded. Accordingly McEwen personally supervised the signing of the first post-war trade treaty with Japan, new trade agreements with New Zealand and Britain, and Australia's first trade agreement with the USSR (1965). In addition to this he insisted on developing an all encompassing system of tariff protection that would encourage the development of those secondary industries that would "value add" Australia's primary produce. His success in this endeavour is sometimes dubbed "McEwenism". This was the period of the Country Party's greatest power, as was demonstrated in 1962 when McEwen was able to insist that Menzies sack a Liberal Minister who claimed that Britain's entry into the European Economic Community was unlikely to severely impact on the Australian economy as a whole.
Menzies retired in 1966 and was succeeded by Harold Holt. McEwen thus became the longest-tenured member of the government, with the informal right to veto government policy. The most significant instance that McEwen exercised this came when Holt disappeared in December 1967. John Gorton became the new Liberal Prime Minister in January 1968. McEwen was sworn in as an interim Prime Minister pending the election of the new Liberal leader. Logically, the Liberals' deputy leader, William McMahon, should have succeeded Holt. However, McMahon was a staunch free-trader, and there were also rumors that he was homosexual. As a result, McEwen told the Liberals that he and his party would not serve under McMahon. McMahon stood down in favour of John Gorton. It would be only after McEwen announced his retirement that MacMahon would be able to successfully challenge Gorton for the Liberal leadership. McEwen's reputation for political toughness led to him being nicknamed "Black Jack" by his allies and enemies alike.
At the state level, from 1957 to 1989, the Country Party under Frank Nicklin and Joh Bjelke-Petersen dominated governments in Queensland—the last six of those years ruling in its own right, without the Liberals. It also took part in governments in New South Wales, Victoria, and Western Australia.
However, successive electoral redistributions after 1964 indicated that the Country Party was losing ground electorally to the Liberals as the rural population declined, and the nature of some parliamentary seats on the urban/rural fringe changed. A proposed merger with the Democratic Labor Party (DLP) under the banner of "National Alliance" was rejected when it failed to find favour with voters at the 1974 state election.
Also in 1974, the Northern Territory members of the party joined with its Liberal party members to form the independent Country Liberal Party. This party continues to represent both parent parties in that territory. A separate party, the Joh-inspired NT Nationals, competed in the 1987 election with former Chief Minister Ian Tuxworth winning his seat of Barkly by a small margin. However, this splinter group were not endorsed by the national executive and soon disappeared from the political scene.
Countrymindedness.
"Countrymindedness" was a slogan that summed up the ideology of the Country Party from 1920 through the early 1970s. It was an ideology that was physiocratic, populist, and decentralist; it fostered rural solidarity and justified demands for government subsidies. "Countrymindedness" grew out of the failure of the country areas to participate in the rapid economic and population expansions that occurred after 1890. The growth of the ideology into urban areas came as most country people migrated to jobs in the cities. Its decline was due mainly to the reduction of real and psychological differences between country and city brought about by the postwar expansion of the Australian urban population and to the increased affluence and technological changes that accompanied it.
National Country Party, and National Party.
In 1975 the Country Party changed its name to the National Country Party as part of a strategy to expand into urban areas. This had some success in Queensland under Joh Bjelke-Petersen, but nowhere else. In Western Australia, the party briefly walked out of the coalition agreement in Western Australia in May 1975, returning within the month. However, the party split in two over the decision and other factors in late 1978, with a new National Party forming and becoming independent, holding three seats in the Western Australian lower house, while the National Country Party remained in coalition and also held three seats. They reconciled after the Burke Labor government came to power in 1983.
The 1980s were dominated by the feud between Bjelke-Petersen and the federal party leadership. Bjelke-Petersen briefly triumphed in 1987, forcing the Nationals to tear up the Coalition agreement and support his bid to become Prime Minister. The "Joh for Canberra" campaign backfired spectacularly when a large number of three-cornered contests allowed Labor to win a third term under Bob Hawke. It also proved to be the Queensland Nationals' last hurrah; Bjelke-Petersen was forced into retirement a few months after the federal election, and his party was heavily defeated in 1989. The Nationals experienced difficulties in the late 1990s from two fronts – firstly from the Liberal Party, who were winning seats on the basis that the Nationals were not seen to be a sufficiently separate party, and from the One Nation Party riding a swell of rural discontent with many of the policies such as multiculturalism and gun control embraced by all of the major parties. The rise of Labor in formerly safe National-held areas in rural Queensland, particularly on the coast, has been the biggest threat to the Queensland Nationals.
State parties.
Queensland.
Queensland is the only state in which the Nationals have consistently been the stronger coalition partner. The Nationals were the senior partner in the non-Labor Coalition from 1925 until the Coalition was broken in 1983. At the 1983 state election, the Nationals under Joh Bjelke-Petersen came up one seat short of a majority, but later gained a majority when two Liberal members crossed the floor to join the Nationals. The Nationals then governed in their own right until 1989. 
The continued success of the Australian Labor Party at a state level has put pressure on the Nationals' links with the Liberal Party, their traditional coalition partner. In most states, the Coalition agreement is not in force when the parties are in opposition, allowing the two parties greater freedom of action.
In Queensland the National Party merged with the Liberal Party forming the Liberal National Party (LNP) in 2008. The LNP led by Lawrence Springborg went on to lose the March 2009 election to Anna Bligh's Australian Labor Party. However, in the Queensland state election, 2012, the LNP defeated the Labor Party in a landslide, but lost government in 2015.
South Australia.
In South Australia, for the first time in the Nationals' history, in 2002 the single Nationals member in the House of Assembly entered the Rann Labor Government as a Minister forming an informal coalition between the two parties. Since the 2010 South Australian State election, the Nationals in South Australia have no representative in either the House of Assembly or the Upper House or at a Federal level.
Western Australia.
Western Australia's National Party chose to assert its independence after an acrimonious co-habitation with the Liberals on the 2005 campaign trail. Unlike its New South Wales and Queensland counterparts, the WA party had decided to oppose Liberal candidates in the 2008 election. The party aimed to hold the balance of power in the state "as an independent conservative party" ready to negotiate with the Liberals or Labor to form a minority government. After the election, the Nationals negotiated an agreement to form a government with the Liberals and an independent MP, though not described as a "traditional coalition" due to the reduced cabinet collective responsibility of National cabinet members.
Western Australia's one-vote-one-value reforms will cut the number of rural seats in the state assembly to reflect the rural population level: this, coupled with the Liberals' strength in country areas has put the Nationals under significant pressure.
Victoria.
The Nationals were stung in early 2006, when their only Victorian senator, Julian McGauran, defected to the Liberals and created a serious rift between the Nationals and the Liberals. Several commentators believed that changing demographics and unfavourable preference deals would demolish the Nationals at the state election that year, but they went on to enjoy considerable success by winning two extra lower house seats. The Nationals were in a coalition government with the Liberals at a State level in Victoria until their defeat at the 2014 election. Following the election, the ABC reported that the coalition parties would "review" whether to continue their joint working arrangement into opposition. However, both outgoing Nationals leader Peter Ryan and incoming Liberal leader Matthew Guy indicated they felt the coalition should continue.
Political role.
The Nationals see their main role as giving a voice to Australians who live outside the country's metropolitan areas.
Traditionally, the leader of the National Party serves as Deputy Prime Minister when the Coalition is in government. This tradition dates back to the creation of the office in 1968.
The National Party's support base and membership are closely associated with the agricultural community. Historically anti-union, the party has vacillated between state support for primary industries ("agrarian socialism") and free agricultural trade and has opposed tariff protection for Australia's manufacturing and service industries. This vacillation prompted those opposed to the policies of the Nationals to joke that its real aim was to "capitalise its gains and socialise its losses!". It is usually pro-mining, pro-development, and anti-environmentalist.
The Nationals vote is in decline and its traditional supporters are turning instead to prominent independents such as Bob Katter, Tony Windsor and Peter Andren in Federal Parliament and similar independents in the Parliaments of New South Wales, Queensland and Victoria, many of whom are former members of the National Party. In fact at the 2004 Federal election, National Party candidates received fewer first preference votes than the Australian Greens.
Demographic changes are not helping, with fewer people living and employed on the land or in small towns, the continued growth of the larger provincial centres, and, in some cases, the arrival of left-leaning "city refugees" in rural areas. The Liberals have also gained support as the differences between the coalition partners on a federal level have become invisible. This was highlighted in January 2006, when Nationals Senator Julian McGauran defected to the Liberals, saying that there was "no longer any real distinguishing policy or philosophical difference".
In Queensland, Nationals leader Lawrence Springborg advocated merger of the National and Liberal parties at a state level in order to present a more effective opposition to the Labor Party. Previously this plan had been dismissed by the Queensland branch of the Liberal party, but the idea received in-principle support from the Liberals. Federal leader Mark Vaile stated the Nationals will not merge with the Liberal Party at a federal level. The plan was opposed by key Queensland Senators Ron Boswell and Barnaby Joyce, and was scuttled in 2006. After suffering defeat in the 2006 Queensland poll, Lawrence Springborg was replaced by Jeff Seeney, who indicated he was not interested in merging with the Liberal Party until the issue is seriously raised at a Federal level.
Support for the Nationals in the 2006 Victorian state election was considerable with the party picking up two extra seats in the Lower House to maintain its total representation of 11 sitting members (two Upper House seats were lost, mostly due to a change from preferential to proportional representation). This success can be attributed to a more assertive National Party image (a differentation to that of the Liberals) and the growing popularity of state and federal Nationals identities such as Barnaby Joyce.
In September 2008, Barnaby Joyce replaced CLP Senator and Nationals deputy leader Nigel Scullion as leader of the Nationals in the Senate, and stated that his party in the upper house would no longer necessarily vote with their Liberal counterparts in the upper house, which opened up another possible avenue for the Rudd Labor Government to get legislation through.
Liberal/National merger.
Merger plans came to a head in May 2008, when the Queensland state Liberal Party gave an announcement not to wait for a federal blueprint but instead to merge immediately. The new party, the Liberal National Party, was founded in July 2008.
Deputy Leaders.
"Shown in chronological order of leadership"
Current State Parliamentary Leaders.
1 In the Northern Territory, the Nationals do not endorse their own candidates. Instead, the Country Liberal Party is their preferred party in the Territory.
2 Queensland is represented by the Liberal National Party of Queensland. This party is the result of a merger of the Queensland Division of the Liberal Party and the Queensland National Party to contest elections as a single party.
The National Party does not stand candidates in Tasmania or the Australian Capital Territory.

</doc>
<doc id="21930" url="http://en.wikipedia.org/wiki?curid=21930" title="Northern blot">
Northern blot

The northern blot is a technique used in molecular biology research to study gene expression by detection of RNA (or isolated mRNA) in a sample. 
With northern blotting it is possible to observe cellular control over structure and function by determining the particular gene expression levels during differentiation, morphogenesis, as well as abnormal or diseased conditions. Northern blotting involves the use of electrophoresis to separate RNA samples by size and detection with a hybridization probe complementary to part of or the entire target sequence. The term 'northern blot' actually refers specifically to the capillary transfer of RNA from the electrophoresis gel to the blotting membrane. However, the entire process is commonly referred to as northern blotting. The northern blot technique was developed in 1977 by James Alwine, David Kemp, and George Stark at Stanford University. Northern blotting takes its name from its similarity to the first blotting technique, the Southern blot, named for biologist Edwin Southern. The major difference is that RNA, rather than DNA, is analyzed in the northern blot. 
Procedure.
A general blotting procedure starts with extraction of total RNA from a homogenized tissue sample or from cells. Eukaryotic mRNA can then be isolated through the use of oligo (dT) cellulose chromatography to isolate only those RNAs with a poly(A) tail. RNA samples are then separated by gel electrophoresis. Since the gels are fragile and the probes are unable to enter the matrix, the RNA samples, now separated by size, are transferred to a nylon membrane through a capillary or vacuum blotting system. A nylon membrane with a positive charge is the most effective for use in northern blotting since the negatively charged nucleic acids have a high affinity for them. The transfer buffer used for the blotting usually contains formamide because it lowers the annealing temperature of the probe-RNA interaction, thus eliminating the need for high temperatures, which could cause RNA degradation. Once the RNA has been transferred to the membrane, it is immobilized through covalent linkage to the membrane by UV light or heat. After a probe has been labeled, it is hybridized to the RNA on the membrane. Experimental conditions that can affect the efficiency and specificity of hybridization include ionic strength, viscosity, duplex length, mismatched base pairs, and base composition. The membrane is washed to ensure that the probe has bound specifically and to prevent background signals from arising. The hybrid signals are then detected by X-ray film and can be quantified by densitometry. To create controls for comparison in a northern blot samples not displaying the gene product of interest can be used after determination by microarrays or RT-PCR.
Gels.
The RNA samples are most commonly separated on agarose gels containing formaldehyde as a denaturing agent for the RNA to limit secondary structure. The gels can be stained with ethidium bromide (EtBr) and viewed under UV light to observe the quality and quantity of RNA before blotting. Polyacrylamide gel electrophoeresis with urea can also be used in RNA separation but it is most commonly used for fragmented RNA or microRNAs. An RNA ladder is often run alongside the samples on an electrophoresis gel to observe the size of fragments obtained but in total RNA samples the ribosomal subunits can act as size markers. Since the large ribosomal subunit is 28S (approximately 5kb) and the small ribosomal subunit is 18S (approximately 2kb) two prominent bands appear on the gel, the larger at close to twice the intensity of the smaller.
Probes.
Probes for northern blotting are composed of nucleic acids with a complementary sequence to all or part of the RNA of interest, they can be DNA, RNA, or oligonucleotides with a minimum of 25 complementary bases to the target sequence. RNA probes (riboprobes) that are transcribed in vitro are able to withstand more rigorous washing steps preventing some of the background noise. Commonly cDNA is created with labelled primers for the RNA sequence of interest to act as the probe in the northern blot. The probes must be labelled either with radioactive isotopes (32P) or with chemiluminescence in which alkaline phosphatase or horseradish peroxidase break down chemiluminescent substrates producing a detectable emission of light. The chemiluminescent labelling can occur in two ways: either the probe is attached to the enzyme, or the probe is labelled with a ligand (e.g. biotin) for which the antibody (e.g., avidin or streptavidin) is attached to the enzyme. X-ray film can detect both the radioactive and chemiluminescent signals and many researchers prefer the chemiluminescent signals because they are faster, more sensitive, and reduce the health hazards that go along with radioactive labels. The same membrane can be probed up to five times without a significant loss of the target RNA.
Applications.
Northern blotting allows one to observe a particular gene's expression pattern between tissues, organs, developmental stages, environmental stress levels, pathogen infection, and over the course of treatment. The technique has been used to show overexpression of oncogenes and downregulation of tumor-suppressor genes in cancerous cells when compared to 'normal' tissue, as well as the gene expression in the rejection of transplanted organs. If an upregulated gene is observed by an abundance of mRNA on the northern blot the sample can then be sequenced to determine if the gene is known to researchers or if it is a novel finding. The expression patterns obtained under given conditions can provide insight into the function of that gene. Since the RNA is first separated by size, if only one probe type is used variance in the level of each band on the membrane can provide insight into the size of the product, suggesting alternative splice products of the same gene or repetitive sequence motifs. The variance in size of a gene product can also indicate deletions or errors in transcript processing. By altering the probe target used along the known sequence it is possible to determine which region of the RNA is missing.
 is an online database publishing northern blots. BlotBase has over 700 published northern blots of human and mouse samples, in over 650 genes across more than 25 different tissue types. Northern blots can be searched by a blot ID, paper reference, gene identifier, or by tissue. The results of a search provide the blot ID, species, tissue, gene, expression level, blot image (if available), and links to the publication that the work originated from. This new database provides sharing of information between members of the science community that was not previously seen in northern blotting as it was in sequence analysis, genome determination, protein structure, etc.
Advantages and disadvantages.
Analysis of gene expression can be done by several different methods including RT-PCR, RNase protection assays, microarrays, serial analysis of gene expression (SAGE), as well as northern blotting. Microarrays are quite commonly used and are usually consistent with data obtained from northern blots; however, at times northern blotting is able to detect small changes in gene expression that microarrays cannot. The advantage that microarrays have over northern blots is that thousands of genes can be visualized at a time, while northern blotting is usually looking at one or a small number of genes.
A problem in northern blotting is often sample degradation by RNases (both endogenous to the sample and through environmental contamination), which can be avoided by proper sterilization of glassware and the use of RNase inhibitors such as DEPC (diethylpyrocarbonate). The chemicals used in most northern blots can be a risk to the researcher, since formaldehyde, radioactive material, ethidium bromide, DEPC, and UV light are all harmful under certain exposures. Compared to RT-PCR, northern blotting has a low sensitivity, but it also has a high specificity, which is important to reduce false positive results.
The advantages of using northern blotting include the detection of RNA size, the observation of alternate splice products, the use of probes with partial homology, the quality and quantity of RNA can be measured on the gel prior to blotting, and the membranes can be stored and reprobed for years after blotting.
Reverse northern blot.
Researchers occasionally use a variant of the procedure known as the reverse northern blot. In this procedure, the substrate nucleic acid (that is affixed to the membrane) is a collection of isolated DNA fragments, and the probe is RNA extracted from a tissue and radioactively labelled.
The use of DNA microarrays that have come into widespread use in the late 1990s and early 2000s is more akin to the reverse procedure, in that they involve the use of isolated DNA fragments affixed to a substrate, and hybridization with a probe made from cellular RNA. Thus the reverse procedure, though originally uncommon, enabled northern analysis to evolve into gene expression profiling, in which many (possibly all) of the genes in an organism may have their expression monitored.

</doc>
<doc id="21932" url="http://en.wikipedia.org/wiki?curid=21932" title="Narrow-gauge railway">
Narrow-gauge railway

A narrow gauge railway (or narrow gauge railroad) is a railway with a track gauge narrower than the of standard gauge railways. Most existing narrow gauge railways are between and .
Since narrow gauge railways are usually built with smaller radius curves, smaller structure gauges, lighter rails, etc., they can be substantially less costly to build, equip, and operate than standard gauge or broad gauge railways, particularly in mountainous or difficult terrain. The lower costs of narrow gauge railways mean they are often built to serve industries and communities where the traffic potential would not justify the cost of building a standard or broad gauge line.
Narrow gauge railways also have specialized use in mines and other environments where a very small structure gauge makes a very small loading gauge necessary. Narrow gauge railways also have more general applications. Nonindustrial narrow gauge mountain railways are or were common in the Rocky Mountains of the United States and the Pacific Cordillera of Canada, in Mexico, Switzerland, the former Yugoslavia, Greece, India, and Costa Rica. In some countries, narrow gauge is the standard, like the gauge in Japan, Taiwan, New Zealand, South Africa, and Tasmania, and the in Malaysia and Thailand.
Many narrow gauge street tramways are used, particularly in Europe, where tramways are common.
History.
The earliest recorded railway is shown in the "De re metallica" of 1556, which shows a mine in Bohemia with a railway of about gauge. During the 16th century, railways were mainly restricted to hand-pushed narrow gauge lines in mines throughout Europe. During the 17th century, mine railways were extended to provide transportation above ground. These lines were industrial, connecting mines with nearby transportation points, usually canals or other waterways. These railways were usually built to the same narrow gauge as the mine railways from which they developed.
The world's first steam locomotive on rails, built in 1802 by Richard Trevithick for the Coalbrookdale Company, ran on a plateway. During the 1820s and 1830s, a number of industrial narrow gauge railways in the United Kingdom used steam locomotives. In 1842, the first narrow gauge steam locomotive outside the UK was built for the gauge Antwerp-Ghent Railway in Belgium. The first use of steam locomotives on a public, passenger-carrying narrow gauge railway came in 1865 when the Ffestiniog Railway introduced its passenger service, after receiving its first locomotives two years prior.
Historically, many narrow gauge railways were built as part of specific industrial enterprises and were primarily industrial railways rather than general carriers. Some common uses for these industrial narrow gauge railways were mining, logging, construction, tunnelling, quarrying, and the conveying of agricultural products. Extensive narrow gauge networks were constructed in many parts of the world for these purposes. For example, mountain logging operations in the 19th century often used narrow gauge railways to transport logs from mill sites to market. Significant sugarcane railways still operate in Cuba, Fiji, Java, the Philippines, and Queensland. Narrow gauge railway equipment remains in common use for the construction of tunnels.
Extensive narrow gauge railway systems served the front-line trenches of both sides in World War I. They were a short-lived military application, and after the end of the war, the surplus equipment from these created a small boom in narrow gauge railway building in Europe.
Advantages.
Narrow gauge railways usually cost less to build because they are usually lighter in construction, using smaller cars and locomotives (smaller loading gauge), as well as smaller bridges, smaller tunnels (smaller structure gauge) and tighter curves. Narrow gauge is thus often used in mountainous terrain, where the savings in civil engineering work can be substantial. It is also used in sparsely populated areas where the potential demand is too low for broader gauge railways to be economically viable. This is the case in some of Australia and most of Southern Africa, where extremely poor soils have led to population densities too low for standard gauge to be viable.
For temporary railways that will be removed after short-term use, such as for construction, the logging industry, the mining industry, or large-scale construction projects, especially in confined spaces, such as the Channel Tunnel, a narrow gauge railway is substantially cheaper and easier to install and remove. The use of such railways has almost vanished due to the capabilities of modern trucks.
In many countries, narrow gauge railways were built as "feeder" or "branch" lines to feed traffic to more important standard gauge lines, due to their lower construction costs. The choice was often not between a narrow gauge railway and a standard gauge one, but between a narrow gauge railway and none at all.
Disadvantages.
Narrow gauge railways cannot interchange rolling stock such as freight and passenger cars freely with the standard gauge or broad gauge railways with which they link, and the transfers of passengers and freight require time-consuming manual labour or substantial capital expenditure. Some bulk commodities, such as coal, ore, and gravel, can be mechanically transshipped, but this still incurs time penalties and the equipment required for the transfer is often complex to maintain.
Also in times of peak demand, it is very difficult to move rolling stock to where it is needed when a break of gauge exists, so enough rolling stock must be available to meet a narrow gauge railways' own peak demand, which might be much more than needed by equivalent standard gauge railways, and the surplus equipment generated no cash flow during periods of low demand.
Solutions to these problems of transshipment are bogie exchange between cars, a rollbock system, variable gauge, dual gauge, or even gauge conversion. European standard gauge trains normally use buffers and chain couplers, which do not allow such tight curves, a main reason to have narrow gauge. Therefore, narrow gauge trains normally use other couplers, which makes bogie exchange meaningless.
Another problem for narrow gauge railways was that they lacked the physical space to grow: their cheap construction meant they were engineered only for their initial traffic demands. While a standard or broad gauge railway could more easily be upgraded to handle heavier, faster traffic, many narrow gauge railways were impractical to improve. Speeds and loads hauled could not increase, so traffic density was significantly limited. In the case of Queensland, Australia, the Queensland Rail passenger network has nearly reached its capacity due to the narrow gauge and an ever increasing population, as such, new lines are to be built, thus negating the original cost savings.
Successful railways.
The heavy duty narrow gauge railways in Australia (e.g. Queensland), South Africa, and New Zealand show that if the track is built to a heavy-duty standard, performance almost as good as a standard gauge line is possible. Some 200-car trains operate on the Sishen-Saldanha railway in South Africa, and high-speed tilt-trains in Queensland. Another example of a heavy-duty narrow gauge line is EFVM in Brazil. gauge, it has over-100-pound rail (100 lb/yd) and a loading gauge almost as large as US nonexcess-height lines. It has multiple 4000 hp locomotives and 200+ car trains. In South Africa and New Zealand, the loading gauge is similar to the restricted British loading gauge, and in New Zealand some British Rail Mark 2 carriages have been rebuilt with new bogies for use by Tranz Scenic (Wellington-Palmerston North service), Tranz Metro (Wellington-Masterton service), and Veolia (Auckland suburban services).
Fastest trains.
The reduced stability of narrow gauge means its trains cannot run at the same high speeds as on broader gauges, unless the tracks are aligned with greater precision . In Japan and Queensland, recent permanent way improvements have allowed trains on gauge tracks to run at 160 km/h and faster. Queensland Rail's tilt train is currently the fastest train in Australia and the fastest gauge train in the world, setting a record at 210 km/h. A special gauge railcar was built for the Otavi Mining and Railway Company with a design speed of 137 km/h.
Compare these speeds with standard gauge or broad gauge trains which can run at up to 320 km/h. The contrast is most evident in Japan, home of the Shinkansen, a network of standard gauge lines built solely for high-speed rail in a country where narrow gauge is the predominant standard.
Curve radius is also important for high speeds: narrow gauge railways tend to have sharper curves, which limits the speed at which a vehicle can safely proceed along the track.
Costs.
Many engineers considered the cost of a railway varies with some power of the gauge, so the narrower gauge the cheaper it might be. This applied also to different narrow gauges, such as a proposed line in Papua using either or .
Nomenclature.
In general, a narrow gauge railway has a track gauge less than standard gauge. However, due to historical and local circumstances, the definition of a narrow gauge railway can be different.
Gauges used.
Many narrow gauges are in use or formerly used between gauge and gauge. They fall into several broad categories:
Scotch gauge.
Scotch gauge was the name given to a track gauge, that was adopted by early 19th-century railways mainly in the Lanarkshire area of Scotland. Also lines were constructed. Both gauges were eventually converted to standard gauge.
Three foot, six inch gauge railways.
 between the inside of the rail heads. The name and classification varies throughout the world. It has installations of around 112000 km.
Similar gauges are:
Metre gauge and Italian metre gauge railways.
Metre gauge is the system of narrow gauge railways and tramways with a track gauge of . It has installations of around 95000 km.
As a result of Italian law, track gauges in Italy were defined from the centres of each rail, rather than the inside edges of the rails. This gauge was measured between the edges of the rails and is known as Italian metre gauge
Three foot, 900 mm, and Swedish three foot (891 mm) gauge railways.
Three foot gauge railways have a track gauge of and are generally found throughout North and South America, as well as Ireland and the Isle of Man.
 gauge railways are generally found in Europe. Swedish three foot gauge railways () can only be found in Sweden.
, Bosnian gauge, and two foot six inch gauge railways.
 and Bosnian gauge () railways are predominantly found in Russia and Eastern Europe.
The Imperial gauge railways were generally constructed in the former British colonies, such as the Kelani Valley Line (now ) in Sri Lanka.
These lightweight lines can be built at a substantial cost saving over medium or standard gauge railways, but are generally restricted in their carrying capacity. The majority of these lines were built in mountainous areas, the majority for carrying mineral traffic from mines to ports or standard gauge railways.
Two foot (610 mm), 600 mm, and similar gauges.
Gauges: , , , and 
Minimum gauge railways.
Gauges below were rare, but did exist. In Britain, Sir Arthur Heywood developed gauge estate railways, while in France Decauville produced a range of industrial railways running on and tracks, most commonly in restricted environments such as underground mine railways, parks and farms. Several gauge railways were built in Britain to serve ammunition depots and other military facilities, particularly during World War I.
Further reading.
</dl>

</doc>
<doc id="21933" url="http://en.wikipedia.org/wiki?curid=21933" title="Neutron activation analysis">
Neutron activation analysis

Neutron activation analysis (NAA) is a nuclear process used for determining the concentrations of elements in a vast amount of materials. NAA allows discrete sampling of elements as it disregards the chemical form of a sample, and focuses solely on its nucleus. The method is based on neutron activation and therefore requires a source of neutrons. The sample is bombarded with neutrons, causing the elements to form radioactive isotopes. The radioactive emissions and radioactive decay paths for each element are well known. Using this information, it is possible to study spectra of the emissions of the radioactive sample, and determine the concentrations of the elements within it. A particular advantage of this technique is that it does not destroy the sample, and thus has been used for analysis of works of art and historical artifacts. NAA can also be used to determine the activity of a radioactive sample.
If NAA is conducted directly on irradiated samples it is termed Instrumental Neutron Activation Analysis (INAA). In some cases irradiated samples are subjected to chemical separation to remove interfering species or to concentrate the radioisotope of interest, this technique is known as Radiochemical Neutron Activation Analysis (RNAA).
NAA can perform non-destructive analyses on solids, liquids, suspensions, slurries, and gases with no or minimal preparation. Due to the penetrating nature of incident neutrons and resultant gamma rays, the technique provides a true bulk analysis. As different radioisotopes have different half-lives, counting can be delayed to allow interfering species to decay eliminating interference. Until the introduction of ICP-AES and PIXE, NAA was the standard analytical method for performing multi-element analyses with minimum detection limits in the sub-ppm range. Accuracy of NAA is in the region of 5%, and relative precision is often better than 0.1%. There are two noteworthy drawbacks to the use of NAA; even though the technique is essentially non-destructive, the irradiated sample will remain radioactive for many years after the initial analysis, requiring handling and disposal protocols for low-level to medium-level radioactive material; also, the number of suitable activation nuclear reactors is declining; with a lack of irradiation facilities, the technique has declined in popularity and become more expensive.
Overview.
Neutron activation analysis is a sensitive multi-element analytical technique used for both qualitative and quantitative analysis of major, minor, trace and rare elements. NAA was discovered in 1936 by Hevesy and Levi, who found that samples containing certain rare earth elements became highly radioactive after exposure to a source of neutrons. This observation led to the use of induced radioactivity for the identification of elements. NAA is significantly different from other spectroscopic analytical techniques in that it is based not on electronic transitions but on nuclear transitions. To carry out an NAA analysis, the specimen is placed into a suitable irradiation facility and bombarded with neutrons. This creates artificial radioisotopes of the elements present. Following irradiation, the artificial radioisotopes decay with emission of particles or, more importantly gamma rays, which are characteristic of the element from which they were emitted.
For the NAA procedure to be successful, the specimen or sample must be selected carefully. In many cases small objects can be irradiated and analysed intact without the need of sampling. But, more commonly, a small sample is taken, usually by drilling in an inconspicuous place. About 50 mg (one-twentieth of a gram) is a sufficient sample, so damage to the object is minimised. It is often good practice to remove two samples using two different drill bits made of different materials. This will reveal any contamination of the sample from the drill bit material itself. The sample is then encapsulated in a vial made of either high purity linear polyethylene or quartz. These sample vials come in many shapes and sizes to accommodate many specimen types. The sample and a standard are then packaged and irradiated in a suitable reactor at a constant, known neutron flux. A typical reactor used for activation uses uranium fission, providing a high neutron flux and the highest available sensitivities for most elements. The neutron flux from such a reactor is in the order of 1012 neutrons cm−2 s−1. The type of neutrons generated are of relatively low kinetic energy (KE), typically less than 0.5 eV. These neutrons are termed thermal neutrons. Upon irradiation, a thermal neutron interacts with the target nucleus via a non-elastic collision, causing neutron capture. This collision forms a compound nucleus which is in an excited state. The excitation energy within the compound nucleus is formed from the binding energy of the thermal neutron with the target nucleus. This excited state is unfavourable and the compound nucleus will almost instantaneously de-excite (transmutate) into a more stable configuration through the emission of a prompt particle and one or more characteristic prompt gamma photons. In most cases, this more stable configuration yields a radioactive nucleus. The newly formed radioactive nucleus now decays by the emission of both particles and one or more characteristic delayed gamma photons. This decay process is at a much slower rate than the initial de-excitation and is dependent on the unique half-life of the radioactive nucleus. These unique half-lives are dependent upon the particular radioactive species and can range from fractions of a second to several years. Once irradiated, the sample is left for a specific decay period, then placed into a detector, which will measure the nuclear decay according to either the emitted particles, or more commonly, the emitted gamma rays.
Variations.
NAA can vary according to a number of experimental parameters. The kinetic energy of the neutrons used for irradiation will be a major experimental parameter. The above description is of activation by slow neutrons, slow neutrons are fully moderated within the reactor and have KE <0.5 eV. Medium KE neutrons may also be used for activation, these neutrons have been only partially moderated and have KE of 0.5 eV to 0.5 MeV, and are termed epithermal neutrons. Activation with epithermal neutrons is known as Epithermal NAA (ENAA). High KE neutrons are sometimes used for activation, these neutrons are unmoderated and consist of primary fission neutrons. High KE or fast neutrons have a KE >0.5 MeV. Activation with fast neutrons is termed Fast NAA (FNAA).
Another major experimental parameter is whether nuclear decay products (gamma rays or particles) are measured during neutron irradiation (prompt gamma), or at some time after irradiation (delayed gamma). PGNAA is generally performed by using a neutron stream tapped off the nuclear reactor via a beam port. Neutron fluxes from beam ports are the order of 106 times weaker than inside a reactor. This is somewhat compensated for by placing the detector very close to the sample reducing the loss in sensitivity due to low flux. PGNAA is generally applied to elements with extremely high neutron capture cross-sections; elements which decay too rapidly to be measured by DGNAA; elements that produce only stable isotopes; or elements with weak decay gamma ray intensities. PGNAA is characterised by short irradiation times and short decay times, often in the order of seconds and minutes.
DGNAA is applicable to the vast majority of elements that form artificial radioisotopes. DG analyses are often performed over days, weeks or even months. This improves sensitivity for long-lived radionuclides as it allows short-lived radionuclide to decay, effectively eliminating interference. DGNAA is characterised by long irradiation times and long decay times, often in the order of hours, weeks or longer.
Neutron sources.
a range of different sources can be used:
Reactors.
Some reactors are used for the neutron irradiation of samples for radioisotope production for a range of purposes. The sample can be placed in an irradiation container which is then placed in the reactor; if epithermal neutrons are required for the irradiation then cadmium can be used to filter out the thermal neutrons.
Fusors.
A relatively simple Farnsworth–Hirsch fusor can be used to generate neutrons for NAA experiments. The advantages of this kind of apparatus is that it is compact, often benchtop-sized, and that it can simply be turned off and on. A disadvantage is that this type of source will not produce the neutron flux that can be obtained using a reactor.
Isotope sources.
For many workers in the field a reactor is an item which is too expensive, instead it is common to use a neutron source which uses a combination of an alpha emitter and beryllium. These sources tend to be much weaker than reactors.
Gas discharge tubes.
These can be used to create pulses of neutrons, they have been used for some activation work where the decay of the target isotope is very rapid. For instance in oil wells.
Detectors.
There are a number of detector types and configurations used in NAA. Most are designed to detect the emitted gamma radiation. The most common types of gamma detectors encountered in NAA are the gas ionisation type, scintillation type and the semiconductor type. Of these the scintillation and semiconductor type are the most widely employed. There are two detector configurations utilised, they are the planar detector, used for PGNAA and the well detector, used for DGNAA. The planar detector has a flat, large collection surface area and can be placed close to the sample. The well detector ‘surrounds’ the sample with a large collection surface area.
Scintillation-type detectors use a radiation-sensitive crystal, most commonly thallium-doped sodium iodide (NaI(Tl)), which emits light when struck by gamma photons. These detectors have excellent sensitivity and stability, and a reasonable resolution.
Semiconductor detectors utilise the semiconducting element germanium. The germanium is processed to form a p-i-n (positive-intrinsic-negative) diode, and when cooled to ~77 K by liquid nitrogen to reduce dark current and detector noise, produces a signal which is proportional to the photon energy of the incoming radiation. There are two types of germanium detector, the lithium-drifted germanium or Ge(Li) (pronounced ‘jelly’), and the high-purity germanium or HPGe.
The semiconducting element silicon may also be used but germanium is preferred, as its higher atomic number makes it more efficient at stopping and detecting high energy gamma rays. Both Ge(Li) and HPGe detectors have excellent sensitivity and resolution, but Ge(Li) detectors are unstable at room temperature, with the lithium drifting into the intrinsic region ruining the detector. The development of undrifted high purity germanium has overcome this problem.
Particle detectors can also be used to detect the emission of alpha (α) and beta (β) particles which often accompany the emission of a gamma photon but are less favourable, as these particles are only emitted from the surface of the sample and are often absorbed or attenuated by atmospheric gases requiring expensive vacuum conditions to be effectively detected. Gamma rays, however, are not absorbed or attenuated by atmospheric gases, and can also escape from deep within the sample with minimal absorption.
Analytical capabilities.
NAA can detect up to 74 elements depending upon the experimental procedure, with minimum detection limits ranging from 0.1 to 1x106 ng g−1 depending on element under investigation. Heavier elements have larger nuclei, therefore they have a larger neutron capture cross-section and are more likely to be activated. Some nuclei can capture a number of neutrons and remain relatively stable, not undergoing transmutation or decay for many months or even years. Other nuclei decay instantaneously or form only stable isotopes and can only be identified by PGNAA.
Applications.
Neutron Activation Analysis has a wide variety of applications including within the fields of archaeology, soil science, geology, and the semiconductor industry.
Archaeologists use NAA in order to determine the elements that comprise certain artifacts. This technique is used because it is nondestructive and it can relate an artifact to its source by its chemical signature. This method has proven to be very successful at determining trade routes, particularly for obsidian, with the ability of NAA to distinguish between chemical compositions. In agricultural processes, the movement of fertilizers and pesticides is influenced by surface and subsurface movement as it infiltrates the water supplies. In order to track the distribution of the fertilizers and pesticides, bromide ions in various forms are used as tracers that move freely with the flow of water while having minimal interaction with the soil. Neutron activation analysis is used to measure bromide so that extraction is not necessary for analysis. NAA is used in geology to aid in researching the processes that formed the rocks through the analysis of the rare earth elements and trace elements. It also assists in locating ore deposits and tracking certain elements. Neutron activation analysis is also used to create standards in the semiconductor industry. Semiconductors require a high level of purity, with contamination significantly reducing the quality of the semiconductor. NAA is used to detect trace impurities and establish contamination standards, because it involves limited sample handling and high sensitivity.

</doc>
<doc id="21935" url="http://en.wikipedia.org/wiki?curid=21935" title="Non-deterministic Turing machine">
Non-deterministic Turing machine

In theoretical computer science, a Turing machine is a theoretical machine that is used in thought experiments to examine the abilities and limitations of computers.
In essence, a Turing machine is imagined to be a simple computer that reads and writes symbols one at a time on an endless tape by strictly following a set of rules. It determines what action it should perform next according to its internal "state" and what symbol it currently sees. An example of one of a Turing Machine's rules might thus be: "If you are in state 2 and you see an 'A', change it to 'B' and move left."
In a deterministic Turing machine, the set of rules prescribes at most one action to be performed for any given situation. A non-deterministic Turing machine (NTM), by contrast, may have a set of rules that prescribes more than one action for a given situation. For example, a non-deterministic Turing machine may have both "If you are in state 2 and you see an 'A', change it to a 'B' and move left" and "If you are in state 2 and you see an 'A', change it to a 'C' and move right" in its rule set.
An ordinary (deterministic) Turing machine (DTM) has a transition function that, for a given state and symbol under the tape head, specifies three things: the symbol to be written to the tape, the direction (left, right or neither) in which the head should move, and the subsequent state of the finite control. For example, an X on the tape in state 3 might make the DTM write a Y on the tape, move the head one position to the right, and switch to state 5.
A non-deterministic Turing machine (NTM) differs in that the state and tape symbol no longer "uniquely" specify these things; rather, many different actions may apply for the same combination of state and symbol. For example, an X on the tape in state 3 might now allow the NTM to write a Y, move right, and switch to state 5 "or" to write an X, move left, and stay in state 3.
Definition.
A non-deterministic Turing machine can be formally defined as a 6-tuple formula_1, where
The difference with a standard (deterministic) Turing machine is that for those, the transition relation is a function (the transition function).
Configurations and the "yields" relation on configurations, which describes the possible actions of the Turing machine given any possible contents of the tape, are as for standard Turing machines, except that the "yields" relation is no longer single-valued. The notion of string acceptance is unchanged: a non-deterministic Turing machine accepts a string if, when the machine is started on the configuration in which the tape head is on the first character of the string (if any), and the tape is all blank otherwise, at least one of the machine's possible computations from that configuration puts the machine into a state in formula_10. (If the machine is deterministic, the possible computations are the prefixes of a single, possibly infinite, path.)
Resolution of multiple rules.
How does the NTM "know" which of these actions it should take? There are two ways of looking at it. One is to say that the machine is the "luckiest possible guesser"; it always picks the transition that eventually leads to an accepting state, if there is such a transition. The other is to imagine that the machine "branches" into many copies, each of which follows one of the possible transitions. Whereas a DTM has a single "computation path" that it follows, an NTM has a "computation tree". If at least one branch of the tree halts with an "accept" condition, we say that the NTM accepts the input.
Equivalence with DTMs.
In particular, nondeterministic Turing machines are equivalent with deterministic Turing machines. This equivalency refers to what can be computed, as opposed to how quickly.
NTMs effectively include DTMs as special cases, so it is immediately clear that DTMs are not more powerful. It might seem that NTMs are more powerful than DTMs, since they can allow trees of possible computations arising from the same initial configuration, accepting a string if any one branch in the tree accepts it.
However, it is possible to simulate NTMs with DTMs: One approach is to use a DTM of which the configurations represent multiple configurations of the NTM, and the DTM's operation consists of visiting each of them in turn, executing a single step at each visit, and spawning new configurations whenever the transition relation defines multiple continuations.
Another construction simulates NTMs with 3-tape DTMs, of which the first tape always holds the original input string, the second is used to simulate a particular computation of the NTM, and the third encodes a path in the NTM's computation tree. The 3-tape DTMs are easily simulated with a normal single-tape DTM.
In this construction, the resulting DTM effectively performs a breadth-first search of the NTM's computation tree, visiting all possible computations of the NTM in order of increasing length until it finds an accepting one. Therefore, the length of an accepting computation of the DTM is, in general, exponential in the length of the shortest accepting computation of the NTM. This is considered to be a general property of simulations of NTMs by DTMs; the most famous unresolved question in computer science, the P = NP problem, is related to this issue.
Bounded non-determinism.
An NTM has the property of bounded non-determinism, "i.e.", if an NTM always halts on a given input tape "T" then it halts in a bounded number of steps, and therefore can only have a bounded number of possible configurations.
Comparison with quantum computers.
It is a common misconception that quantum computers are NTMs. It is believed but has not been proven that the power of quantum computers is incomparable to that of NTMs. That is, problems likely exist that an NTM could efficiently solve that a quantum computer cannot. A likely example of problems solvable by NTMs but not by quantum computers in polynomial time are NP-complete problems.

</doc>
<doc id="21937" url="http://en.wikipedia.org/wiki?curid=21937" title="Nitrogen narcosis">
Nitrogen narcosis

Narcosis while diving (also known as nitrogen narcosis, inert gas narcosis, raptures of the deep, Martini effect), is a reversible alteration in consciousness that occurs while diving at depth. It is caused by the anesthetic effect of certain gases at high pressure. The Greek word "ναρκωσις" (narcosis) is derived from "narke", "temporary decline or loss of senses and movement, numbness", a term used by Homer and Hippocrates. Narcosis produces a state similar to drunkenness (alcohol intoxication), or nitrous oxide inhalation. It can occur during shallow dives, but does not usually become noticeable at depths less than 30 m.
Except for helium and probably neon, all gases that can be breathed have a narcotic effect, although widely varying in degree. The effect is consistently greater for gases with a higher lipid solubility, and there is good evidence that the two properties are mechanistically related. As depth increases, the mental impairment may become hazardous. Divers can learn to cope with some of the effects of narcosis, but it is impossible to develop a tolerance. Narcosis affects all divers, although susceptibility varies widely from dive to dive, and between individuals.
Narcosis may be completely reversed in a few minutes by ascending to a shallower depth, with no long-term effects. Thus narcosis while diving in open water rarely develops into a serious problem as long as the divers are aware of its symptoms, and are able to ascend to manage it. Diving beyond 40 m is generally considered outside the scope of recreational diving. Below these depths, as narcosis and oxygen toxicity become critical risk factors, specialist training is required in the use of various helium-containing gas mixtures such as trimix or heliox. These mixtures prevent narcosis by replacing some of the breathing gas with non-narcotic helium.
Classification.
Narcosis results from breathing gases under elevated pressure, and may be classified by the principal gas involved. The noble gases, except helium and probably neon, as well as nitrogen, oxygen and hydrogen cause a decrement in mental function, but their effect on psychomotor function (processes affecting the coordination of sensory or cognitive processes and motor activity) varies widely. The effects of carbon dioxide consistently result in a diminution of mental and psychomotor function. The noble gases argon, krypton, and xenon are more narcotic than nitrogen at a given pressure, and xenon has so much anesthetic activity that it is a usable anesthetic at 80% concentration and normal atmospheric pressure. Xenon has historically been too expensive to be used very much in practice, but it has been successfully used for surgical operations, and xenon anesthesia systems are still being proposed and designed.
Signs and symptoms.
Due to its perception-altering effects, the onset of narcosis may be hard to recognize. At its most benign, narcosis results in relief of anxiety – a feeling of tranquility and mastery of the environment. These effects are essentially identical to various concentrations of nitrous oxide. They also resemble (though not as closely) the effects of alcohol or marijuana and the familiar benzodiazepine drugs such as diazepam and alprazolam. Such effects are not harmful unless they cause some immediate danger not to be recognized and addressed. Once stabilized, the effects generally remain the same at a given depth, only worsening if the diver ventures deeper.
The most dangerous aspects of narcosis are the impairment of judgement, multi-tasking and coordination, and the loss of decision-making ability and focus. Other effects include vertigo and visual or auditory disturbances. The syndrome may cause exhilaration, giddiness, extreme anxiety, depression, or paranoia, depending on the individual diver and the diver's medical or personal history. When more serious, the diver may feel overconfident, disregarding normal safe diving practices.
The relation of depth to narcosis is sometimes informally known as "Martini's law", the idea that narcosis results in the feeling of one martini for every 10 m below 20 m depth. Professional divers use such a calculation only as a rough guide to give new divers a metaphor, comparing a situation they may be more familiar with.
Reported signs and symptoms are summarized against typical depths in meters and feet of sea water in the following table:
Causes.
The cause of narcosis is related to the increased solubility of gases in body tissues, as a result of the elevated pressures at depth (Henry's law). Modern theories have suggested that inert gases dissolving in the lipid bilayer of cell membranes cause narcosis. More recently, researchers have been looking at neurotransmitter receptor protein mechanisms as a possible cause of narcosis. The breathing gas mix entering the diver's lungs will have the same pressure as the surrounding water, known as the ambient pressure. After any change of depth, the pressure of gases in the blood passing through the brain catches up with ambient pressure within a minute or two, which results in a delayed narcotic effect after descending to a new depth. Rapid compression potentiates narcosis owing to carbon dioxide retention.
A divers' cognition may be affected on dives as shallow as 10 m, but the changes are not usually noticeable. There is no reliable method to predict the depth at which narcosis becomes noticeable, or the severity of the effect on an individual diver, as it may vary from dive to dive even on the same day.
Significant impairment due to narcosis is an increasing risk below depths of about 30 m, corresponding to an ambient pressure of about 4 bar. Most sport scuba training organizations recommend depths of no more than 40 m because of the risk of narcosis. When breathing air at depths of 90 m – an ambient pressure of about 10 bar – narcosis in most divers leads to hallucinations, loss of memory, and unconsciousness. A number of divers have died in attempts to set air depth records below 120 m. Because of these incidents, "Guinness World Records" no longer reports on this figure.
Narcosis has been compared with altitude sickness insofar as its variability (though not its symptoms); its effects depend on many factors, with variations between individuals. Thermal cold, stress, heavy work, fatigue, and carbon dioxide retention all increase the risk and severity of narcosis. Carbon dioxide has a high narcotic potential and also causes increased blood flow to the brain, increasing the effects of other gases. Increased risk of narcosis results from increasing the amount of carbon dioxide retained through heavy exercise, shallow or skip breathing, or because of poor gas exchange in the lungs.
Narcosis is known to be additive to even minimal alcohol intoxication, and also to the effects of other drugs such as marijuana (which is more likely than alcohol to have effects that last into a day of abstinence from use). Other sedative and analgesic drugs, such as opiate narcotics and benzodiazepines, add to narcosis.
Mechanism.
The precise mechanism is not well understood, but it appears to be the direct effect of gas dissolving into nerve membranes and causing temporary disruption in nerve transmissions. While the effect was first observed with air, other gases including argon, krypton and hydrogen cause very similar effects at higher than atmospheric pressure. Some of these effects may be due to antagonism at NMDA receptors and potentiation of GABAA receptors, similar to the mechanism of nonpolar anesthetics such diethyl ether or ethylene. However, their reproduction by the very chemically inactive gas argon makes them unlikely to be a strictly chemical bonding to receptors in the usual sense of a chemical bond. An indirect physical effect – such as a change in membrane volume – would therefore be needed to affect the ligand-gated ion channels of nerve cells. Trudell "et al." have suggested non-chemical binding due to the attractive van der Waals force between proteins and inert gases.
Similar to the mechanism of ethanol's effect, the increase of gas dissolved in nerve cell membranes may cause altered ion permeability properties of the neural cells' lipid bilayers. The partial pressure of a gas required to cause a measured degree of impairment correlates well with the lipid solubility of the gas: the greater the solubility, the less partial pressure is needed.
An early theory, the Meyer-Overton hypothesis, suggested that narcosis happens when the gas penetrates the lipids of the brain's nerve cells, causing direct mechanical interference with the transmission of signals from one nerve cell to another. More recently, specific types of chemically gated receptors in nerve cells have been identified as being involved with anesthesia and narcosis. However, the basic and most general underlying idea, that nerve transmission is altered in many diffuse areas of the brain as a result of gas molecules dissolved in the nerve cells' fatty membranes, remains largely unchallenged.
Management and diagnosis.
The management of narcosis is simply to ascend to shallower depths; the effects then disappear within minutes. In the event of complications or other conditions being present, ascending is always the correct initial response. Should problems remain, then it is necessary to abort the dive. The decompression schedule can still be followed unless other conditions require emergency assistance.
The symptoms of narcosis may be caused by other factors during a dive: ear problems causing disorientation or nausea; early signs of oxygen toxicity causing visual disturbances; or hypothermia causing rapid breathing and shivering. Nevertheless the presence of any of these symptoms should imply narcosis. Alleviation of the effects upon ascending to a shallower depth will confirm the diagnosis. Given the setting, other likely conditions do not produce reversible effects. In the rare event of misdiagnosis when another condition is causing the symptoms, the initial management – ascending closer to the surface – is still essential.
Prevention.
The most straightforward way to avoid nitrogen narcosis is for a diver to limit the depth of dives. Since narcosis becomes more severe as depth increases, a diver keeping to shallower depths can avoid serious narcosis. Most recreational dive schools will only certify basic divers to depths of 18 m, and at these depths narcosis does not present a significant risk. Further training is normally required for certification up to 30 m on air, and this training should include a discussion of narcosis, its effects, and cure. Some diver training agencies offer specialized training to prepare recreational divers to go to depths of 40 m, often consisting of further theory and some practice in deep dives under close supervision. Scuba organizations that train for diving beyond recreational depths, may forbid diving with gases that cause too much narcosis at depth in the average diver, and strongly encourage the use of other breathing gas mixes containing helium in place of some or all of the nitrogen in air – such as trimix and heliox – because helium has no narcotic potential. The use of these gases forms part of technical diving and requires further training and certification.
While the individual diver cannot predict exactly at what depth the onset of narcosis will occur on a given day, the first symptoms of narcosis for any given diver are often more predictable and personal. For example, one diver may have trouble with eye focus (close accommodation for middle-aged divers), another may experience feelings of euphoria, and another feelings of claustrophobia. Some divers report that they have hearing changes, and that the sound their exhaled bubbles make becomes different. Specialist training may help divers to identify these personal onset signs, which may then be used as a signal to ascend to avoid the narcosis, although severe narcosis may interfere with the judgement necessary to take preventive action.
Deep dives should be made only after a gradual training to test the individual diver's sensitivity to increasing depths, with careful supervision and logging of reactions. Diving organizations such as Global Underwater Explorers (GUE) emphasize that such sessions are for the purpose of gaining experience in recognizing the onset symptoms of narcosis for an individual, which are somewhat more repeatable than for the average group of divers. Scientific evidence does not show that a diver can train to overcome any measure of narcosis at a given depth or become tolerant of it.
Equivalent narcotic depth (END) is a commonly used way of expressing the narcotic effect of different breathing gases. The National Oceanic and Atmospheric Administration (NOAA) Diving Manual now states that oxygen and nitrogen should be considered equally narcotic. Standard tables, based on relative lipid solubilities, list conversion factors for narcotic effect of other gases. For example, hydrogen at a given pressure has a narcotic effect equivalent to nitrogen at 0.55 times that pressure, so in principle it should be usable at more than twice the depth. Argon, however, has 2.33 times the narcotic effect of nitrogen, and is a poor choice as a breathing gas for diving (it is used as a drysuit inflation gas, owing to its low thermal conductivity). Some gases have other dangerous effects when breathed at pressure; for example, high-pressure oxygen can lead to oxygen toxicity. Although helium is the least intoxicating of the breathing gases, at greater depths it can cause high pressure nervous syndrome, a still mysterious but apparently unrelated phenomenon. Inert gas narcosis is only one factor influencing the choice of gas mixture; the risks of decompression sickness and oxygen toxicity, cost, and other factors are also important.
Because of similar and additive effects, divers should avoid sedating medications and drugs, such as marijuana and alcohol before any dive. A hangover, combined with the reduced physical capacity that goes with it, makes nitrogen narcosis more likely. Experts recommend total abstinence from alcohol for at least 12 hours before diving, and longer for other drugs. Abstinence time needed for marijuana is unknown, but owing to the much longer half-life of the active agent of this drug in the body, it is likely to be longer than for alcohol.
Prognosis and epidemiology.
Narcosis is potentially one of the most dangerous conditions to affect the scuba diver below about 30 m. Except for occasional amnesia of events at depth, the effects of narcosis are entirely removed on ascent and therefore pose no problem in themselves, even for repeated, chronic or acute exposure. Nevertheless, the severity of narcosis is unpredictable and it can be fatal while diving, as the result of illogical behavior in a dangerous environment.
Tests have shown that all divers are affected by nitrogen narcosis, though some experience lesser effects than others. Even though it is possible that some divers can manage better than others because of learning to cope with the subjective impairment, the underlying behavioral effects remain. These effects are particularly dangerous because a diver may feel they are not experiencing narcosis, yet still be affected by it.
History.
French researcher Victor T. Junod was the first to describe symptoms of narcosis in 1834, noting "the functions of the brain are activated, imagination is lively, thoughts have a peculiar charm and, in some persons, symptoms of intoxication are present." Junod suggested that narcosis resulted from pressure causing increased blood flow and hence stimulating nerve centers. Walter Moxon (1836–1886), a prominent Victorian physician, hypothesized in 1881 that pressure forced blood to inaccessible parts of the body and the stagnant blood then resulted in emotional changes. The first report of anesthetic potency being related to lipid solubility was published by Hans H. Meyer in 1899, entitled "Zur Theorie der Alkoholnarkose". Two years later a similar theory was published independently by Charles Ernest Overton. What became known as the Meyer-Overton Hypothesis may be illustrated by a graph comparing narcotic potency with solubility in oil.
In 1939, Albert R. Behnke and O. D. Yarborough demonstrated that gases other than nitrogen also could cause narcosis. For an inert gas the narcotic potency was found to be proportional to its lipid solubility. As hydrogen has only 0.55 the solubility of nitrogen, deep diving experiments using hydrox were conducted by Arne Zetterström between 1943 and 1945. Jacques-Yves Cousteau in 1953 famously described it as "l’ivresse des grandes profondeurs" or the "rapture of the deep".
Further research into the possible mechanisms of narcosis by anesthetic action led to the "minimum alveolar concentration" concept in 1965. This measures the relative concentration of different gases required to prevent motor response in 50% of subjects in response to stimulus, and shows similar results for anesthetic potency as the measurements of lipid solubility. The (NOAA) Diving Manual was revised to recommend treating oxygen as if it were as narcotic as nitrogen, following research by Christian J. Lambertsen "et al." in 1977 and 1978.

</doc>
<doc id="21938" url="http://en.wikipedia.org/wiki?curid=21938" title="Neoproterozoic">
Neoproterozoic

The Neoproterozoic Era is the unit of geologic time from Cambrian.
It is the last Era of the Proterozoic Eon and Precambrian Supereon; it is subdivided into the Tonian, Cryogenian, and Ediacaran Periods. It is preceded by the Mesoproterozoic era and succeeded by the Paleozoic era.
The most severe glaciation known in the geologic record occurred during the Cryogenian, when ice sheets reached the equator and formed a possible "Snowball Earth".
The earliest fossils of multicellular life are found in the Ediacaran, including the earliest animals.
Geology.
At the onset of the Neoproterozoic the supercontinent Rodinia, which had assembled during the late Mesoproterozoic, straddled the equator. During the Tonian, rifting commenced which broke Rodinia into a number of individual land masses.
Possibly as a consequence of the low-latitude position of most continents, several large-scale glacial events occurred during the Neoproterozoic Era including the Sturtian and Marinoan glaciations of the Cryogenian.
These glaciations are believed to have been so severe that there were ice sheets at the equator—a state known as the "Snowball Earth".
Subdivisions.
The Russians divide the Siberian Neoproterozoic into the Baikalian from 850 to 650 Ma (loosely equivalent to the Cryogenian), which overlies the Mayanian, from 1000 to 850 Ma, then the Aimchanian.
Paleobiology.
The idea of the Neoproterozoic Era came on the scene relatively recently — after about 1960. Nineteenth century paleontologists set the start of multicelled life at the first appearance of hard-shelled animals called trilobites and archeocyathids.
This set the beginning of the Cambrian period. In the early 20th century, paleontologists started finding fossils of multicellular animals that predated the Cambrian boundary. A complex fauna was found in South West Africa in the 1920s but was misdated.
Another was found in South Australia in the 1940s but was not thoroughly examined until the late 1950s. Other possible early fossils were found in Russia, England, Canada, and elsewhere (see Ediacaran biota). Some were determined to be pseudofossils, but others were revealed to be members of rather complex biotas that are still poorly understood. At least 25 regions worldwide yielded metazoan fossils prior to the classical Cambrian boundary.
A few of the early animals appear possibly to be ancestors of modern animals. Most fall into ambiguous groups of frond-like organisms; discoids that might be holdfasts for stalked organisms ("medusoids"); mattress-like forms; small calcareous tubes; and armored animals of unknown provenance.
These were most commonly known as Vendian biota until the formal naming of the Period, and are currently known as Ediacaran biota. Most were soft bodied. The relationships, if any, to modern forms are obscure. Some paleontologists relate many or most of these forms to modern animals. Others acknowledge a few possible or even likely relationships but feel that most of the Ediacaran forms are representatives of unknown animal types.
In addition to Ediacaran biota, later two other types of biota were discovered in China (the so-called Doushantuo formation and Hainan formation).
Terminal period.
The nomenclature for the terminal period of the Neoproterozoic has been unstable. Russian geologists referred to the last period of the Neoproterozoic as the Vendian, while Chinese geologists referred to it as the Sinian, and most Australians and North Americans used the name Ediacaran.
However, in 2004, the International Union of Geological Sciences ratified the Ediacaran age to be a geological age of the Neoproterozoic, ranging from ~635 to 541.0 ± 1.0 million years ago. The Ediacaran boundaries are the only Precambrian boundaries defined by biologic Global Boundary Stratotype Section and Points, rather than the absolute Global Standard Stratigraphic Ages.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="21939" url="http://en.wikipedia.org/wiki?curid=21939" title="National Security Agency">
National Security Agency

The National Security Agency (NSA) is an intelligence organization of the United States government, responsible for global monitoring, collection, and processing of information and data for foreign intelligence and counterintelligence purposes – a discipline known as Signals intelligence (SIGINT). NSA is concurrently charged with protection of U.S. government communications and information systems against penetration and network warfare. Although many of NSA's programs rely on "passive" electronic collection, the agency is authorized to accomplish its mission through active clandestine means, among which are physically bugging electronic systems and allegedly engaging in sabotage through subversive software. Moreover, NSA maintains physical presence in a large number of countries across the globe, where its Special Collection Service (SCS) inserts eavesdropping devices in difficult-to-reach places. SCS collection tactics allegedly encompass "close surveillance, burglary, wiretapping, breaking and entering".
Unlike the Defense Intelligence Agency (DIA) and the Central Intelligence Agency (CIA), both of which specialize primarily in foreign human espionage, NSA does not unilaterally conduct human-source intelligence gathering, despite often being portrayed so in popular culture. Instead, NSA is entrusted with assistance to and coordination of SIGINT elements at other government organizations, which are prevented by law from engaging in such activities without the approval of the NSA via the Defense Secretary. As part of these streamlining responsibilities, the agency has a co-located organization called the Central Security Service (CSS), which was created to facilitate cooperation between NSA and other U.S. military cryptanalysis components. Additionally, the NSA Director simultaneously serves as the Commander of the United States Cyber Command and as Chief of the Central Security Service.
Originating as a unit to decipher coded communications in World War II, it was officially formed as the NSA by Harry S. Truman in 1952. Since then, it has become one of the largest of U.S. intelligence organizations in terms of personnel and budget, operating as part of the Department of Defense and simultaneously reporting to the Director of National Intelligence.
NSA surveillance has been a matter of political controversy on several occasions, such as its spying on prominent anti-Vietnam war leaders or economic espionage. In 2013, the extent of the NSA's secret surveillance programs was revealed to the public by Edward Snowden. According to the leaked documents, the NSA intercepts the communications of over a billion people worldwide and tracks the movement of hundreds of millions of people using cellphones. It has also created or maintained security vulnerabilities in most software and encryption, leaving the majority of the Internet susceptible to cyber attacks from the NSA and other parties. Internationally, in addition to the various data sharing concerns that persist, research has pointed to the NSA's ability to surveil the domestic internet traffic of foreign countries through "boomerang routing".
History.
Army predecessor.
The origins of the National Security Agency can be traced back to April 28, 1917, three weeks after the U.S. Congress declared war on Germany in World War I. A code and cipher decryption unit was established as the Cable and Telegraph Section which was also known as the "Cipher Bureau and Military Intelligence Branch, Section 8" (MI-8). It was headquartered in Washington, D.C. and was part of the war effort under the executive branch without direct Congressional authorization. During the course of the war it was relocated in the army's organizational chart several times. On July 5, 1917, Herbert O. Yardley was assigned to head the unit. At that point, the unit consisted of Yardley and two civilian clerks. It absorbed the navy's cryptoanalysis functions in July 1918. World War I ended on November 11, 1918, and MI-8 moved to New York City on May 20, 1919, where it continued intelligence activities as the Code Compilation Company under the direction of Yardley.
Black Chamber.
MI-8 also operated the so-called "Black Chamber". The Black Chamber was located on East 37th Street in Manhattan. Its purpose was to crack the communications codes of foreign governments. Jointly supported by the State Department and the War Department, the chamber persuaded Western Union, the largest U.S. telegram company, to allow government officials to monitor private communications passing through the company's wires.
Other "Black Chambers" were also found in Europe. They were established by the French and British governments to read the letters of targeted individuals, employing a variety of techniques to surreptitiously open, copy, and reseal correspondence before forwarding it to unsuspecting recipients.
Despite the American Black Chamber's initial successes, it was shut down in 1929 by U.S. Secretary of State Henry L. Stimson, who defended his decision by stating: "Gentlemen do not read each other's mail".
World War II and its aftermath.
During World War II, the Signal Security Agency (SSA) was created to intercept and decipher the communications of the Axis powers. When the war ended, the SSA was reorganized as the Army Security Agency (ASA), and it was placed under the leadership of the Director of Military Intelligence.
On May 20, 1949, all cryptologic activities were centralized under a national organization called the Armed Forces Security Agency (AFSA). This organization was originally established within the U.S. Department of Defense under the command of the Joint Chiefs of Staff. The AFSA was tasked to direct Department of Defense communications and electronic intelligence activities, except those of U.S. military intelligence units. However, the AFSA was unable to centralize communications intelligence and failed to coordinate with civilian agencies that shared its interests such as the Department of State, Central Intelligence Agency (CIA) and the Federal Bureau of Investigation (FBI). In December 1951, President Harry S. Truman ordered a panel to investigate how AFSA had failed to achieve its goals. The results of the investigation led to improvements and its redesignation as the National Security Agency.
The agency was formally established by Truman in a memorandum of October 24, 1952, that revised National Security Council Intelligence Directive (NSCID) 9. Since President Truman's memo was a classified document, the existence of the NSA was not known to the public at that time. Due to its ultra-secrecy the U.S. intelligence community referred to the NSA as "No Such Agency".
Vietnam War.
In the 1960s, the NSA played a key role in expanding America's commitment to the Vietnam War by providing evidence of a North Vietnamese attack on the American destroyer USS "Maddox" during the Gulf of Tonkin incident.
A secret operation code-named "MINARET" was set up by the NSA to monitor the phone communications of Senators Frank Church and Howard Baker, as well as major civil rights leaders including Dr. Martin Luther King, and prominent U.S. journalists and athletes who criticized the Vietnam War. However the project turned out to be controversial, and an internal review by the NSA concluded that its Minaret program was "disreputable if not outright illegal."
Church Committee hearings.
In the aftermath of the Watergate Scandal, a congressional hearing in 1975 led by Sen. Frank Church revealed that the NSA, in collaboration with Britain's SIGINT intelligence agency Government Communications Headquarters (GCHQ), had routinely intercepted the international communications of prominent anti-Vietnam war leaders such as Jane Fonda and Dr. Benjamin Spock. Following the resignation of President Richard Nixon, there were several investigations of suspected misuse of FBI, CIA and NSA facilities. Senator Frank Church uncovered previously unknown activity, such as a CIA plot (ordered by the administration of President John F. Kennedy) to assassinate Fidel Castro. The investigation also uncovered NSA's wiretaps on targeted American citizens.
After the Church Committee hearings, the Foreign Intelligence Surveillance Act of 1978 was passed into law. This was designed to limit the practice of mass surveillance in the United States.
From 1980s to 1990s.
In 1986, the NSA intercepted the communications of the Libyan government during the immediate aftermath of the Berlin discotheque bombing. The White House asserted that the NSA interception had provided "irrefutable" evidence that Libya was behind the bombing, which U.S. President Ronald Reagan cited as a justification for the 1986 United States bombing of Libya.
In 1999, a multi-year investigation by the European Parliament highlighted the NSA's role in economic espionage in a report entitled 'Development of Surveillance Technology and Risk of Abuse of Economic Information'. That year, the NSA founded the NSA Hall of Honor, a memorial at the National Cryptologic Museum in Fort Meade, Maryland. The memorial is a, "tribute to the pioneers and heroes who have made significant and long-lasting contributions to American cryptology". NSA employees must be retired for more than fifteen years to qualify for the memorial.
War on Terror.
In the aftermath of the September 11 attacks, the NSA created new IT systems to deal with the flood of information from new technologies like the internet and cellphones. ThinThread contained advanced data mining capabilities. It also had a 'privacy mechanism'; surveillance was stored encrypted; decryption required a warrant. The research done under this program may have contributed to the technology used in later systems. ThinThread was cancelled when Michael Hayden chose Trailblazer, which did not include ThinThread's privacy system.
Trailblazer Project ramped up in 2002. SAIC, Boeing, CSC, IBM, and Litton worked on it. Some NSA whistleblowers complained internally about major problems surrounding Trailblazer. This led to investigations by Congress and the NSA and DoD Inspectors General. The project was cancelled in early 2004; it was late, over budget, and didn't do what it was supposed to do. The Baltimore Sun ran articles about this in 2006–07. The government then raided the whistleblowers' houses. One of them, Thomas Drake, was charged with violating   in 2010 in an unusual use of espionage law. He and his defenders claim that he was actually being persecuted for challenging the Trailblazer Project. In 2011, all 10 original charges against Drake were dropped.
Turbulence started in 2005. It was developed in small, inexpensive 'test' pieces rather than one grand plan like Trailblazer. It also included offensive cyber-warfare capabilities, like injecting malware into remote computers. Congress criticized Turbulence in 2007 for having similar bureaucratic problems as Trailblazer. It was to be a realization of information processing at higher speeds in cyberspace.
Global surveillance disclosures.
The massive extent of the NSA's spying, both foreign and domestic, was revealed to the public in a series of detailed disclosures of internal NSA documents beginning in June 2013. Most of the disclosures were leaked by former NSA contractor, Edward Snowden.
Scope of surveillance.
It was revealed that the NSA intercepts telephone and internet communications of over a billion people worldwide, seeking information on terrorism as well as foreign politics, economics and "commercial secrets". In a declassified document it was revealed that 17,835 phone lines were on an improperly permitted "alert list" from 2006 to 2009 in breach of compliance, which tagged these phone lines for daily monitoring. Eleven percent of these monitored phone lines met the agency's legal standard for "reasonably articulable suspicion"(RAS).
A dedicated unit of the NSA locates targets for the CIA for extrajudicial assassination in the Middle East. The NSA has also spied extensively on the European Union, the United Nations and numerous governments including allies and trading partners in Europe, South America and Asia.
The NSA tracks the locations of hundreds of millions of cellphones per day, allowing them to map people's movements and relationships in detail. It reportedly has access to all communications made via Google, Microsoft, Facebook, Yahoo, YouTube, AOL, Skype, Apple and Paltalk, and collects hundreds of millions of contact lists from personal email and instant messaging accounts each year. It has also managed to weaken much of the encryption used on the Internet (by collaborating with, coercing or otherwise infiltrating numerous technology companies), so that the majority of Internet privacy is now vulnerable to the NSA and other attackers.
Domestically, the NSA collects and stores metadata records of phone calls, including over 120 million US Verizon subscribers as well as internet communications, relying on a secret interpretation of the Patriot Act whereby the entirety of US communications may be considered "relevant" to a terrorism investigation if it is expected that even a tiny minority may relate to terrorism.
The NSA supplies foreign intercepts to the DEA, IRS and other law enforcement agencies, who use these to initiate criminal investigations. Federal agents are then instructed to "recreate" the investigative trail via parallel construction.
The NSA also spies on influential Muslims to obtain information that could be used to discredit them, such as their use of pornography. The targets, both domestic and abroad, are not suspected of any crime but hold religious or political views deemed "radical" by the NSA.
According to a report in "The Washington Post" in July 2014, relying on information furnished by Snowden, 90% of those placed under surveillance in the U.S. are ordinary Americans, and are not the intended targets. The newspaper said it had examined documents including emails, message texts, and online accounts, that support the claim.
Legal accountability.
Despite President Obama's claims that these programs have congressional oversight, members of Congress were unaware of the existence of these NSA programs or the secret interpretation of the Patriot Act, and have consistently been denied access to basic information about them. Obama has also claimed that there are legal checks in place to prevent inappropriate access of data and that there have been no examples of abuse; however, the secret FISC court charged with regulating the NSA's activities is, according to its chief judge, incapable of investigating or verifying how often the NSA breaks even its own secret rules.
It has since been reported that the NSA violated its own rules on data access thousands of times a year, many of these violations involving large-scale data interceptions; and that NSA officers have even used data intercepts to spy on love interests. The NSA has "generally disregarded the special rules for disseminating United States person information" by illegally sharing its intercepts with other law enforcement agencies. A March 2009 opinion of the FISC court, released by court order, states that protocols restricting data queries had been "so frequently and systemically violated that it can be fairly said that this critical element of the overall ... regime has never functioned effectively." In 2011 the same court noted that the "volume and nature" of the NSA's bulk foreign internet intercepts was "fundamentally different from what the court had been led to believe". Email contact lists (including those of US citizens) are collected at numerous foreign locations to work around the illegality of doing so on US soil.
Legal opinions on the NSA's bulk collection program have differed. In mid-December 2013, U.S. District Court Judge Richard Leon ruled that the "almost-Orwellian" program likely violates the Constitution, and wrote, "I cannot imagine a more 'indiscriminate' and 'arbitrary invasion' than this systematic and high-tech collection and retention of personal data on virtually every single citizen for purposes of querying and analyzing it without prior judicial approval. Surely, such a program infringes on 'that degree of privacy' that the Founders enshrined in the Fourth Amendment. Indeed, I have little doubt that the author of our Constitution, James Madison, who cautioned us to beware 'the abridgement of freedom of the people by gradual and silent encroachments by those in power,' would be aghast."
Later that month, U.S. District Judge William Pauley ruled that the NSA's collection of telephone records is legal and valuable in the fight against terrorism. In his opinion, he wrote, "a bulk telephony metadata collection program [is] a wide net that could find and isolate gossamer contacts among suspected terrorists in an ocean of seemingly disconnected data" and noted that a similar collection of data prior to 9/11 might have prevented the attack.
An October 2014 United Nations report condemned mass surveillance by the United States and other countries as violating multiple international treaties and conventions that guarantee core privacy rights.
Official responses.
On March 20, 2013 the Director of National Intelligence, Lieutenant General James Clapper, testified before Congress that the NSA does not wittingly collect any kind of data on millions or hundreds of millions of Americans, but he retracted this in June after details of the PRISM program were published, and stated instead that meta-data of phone and internet traffic are collected, but no actual message contents. This was corroborated by the NSA Director, General Keith Alexander, before it was revealed that the XKeyscore program collects the contents of millions of emails from US citizens without warrant, as well as "nearly everything a user does on the Internet". Alexander later admitted that "content" is collected, but stated that it is simply stored and never analyzed or searched unless there is "a nexus to al-Qaida or other terrorist groups".
Regarding the necessity of these NSA programs, Alexander stated on June 27 that the NSA's bulk phone and Internet intercepts had been instrumental in preventing 54 terrorist "events", including 13 in the US, and in all but one of these cases had provided the initial tip to "unravel the threat stream". On July 31 NSA Deputy Director John Inglis conceded to the Senate that these intercepts had not been vital in stopping any terrorist attacks, but were "close" to vital in identifying and convicting four San Diego men for sending US$8,930 to Al-Shabaab, a militia that conducts terrorism in Somalia.
The U.S. government has aggressively sought to dismiss and challenge Fourth Amendment cases raised against it, and has granted retroactive immunity to ISPs and telecoms participating in domestic surveillance.
The U.S. military has acknowledged blocking access to parts of "The Guardian" website for thousands of defense personnel across the country, and blocking the entire "Guardian" website for personnel stationed throughout Afghanistan, the Middle East, and South Asia.
Organizational structure.
The NSA is led by the Director of the National Security Agency (DIRNSA), who also serves as Chief of the Central Security Service (CHCSS) and Commander of the United States Cyber Command (USCYBERCOM) and is the highest-ranking military official of these organizations. He is assisted by a Deputy Director, who is the highest-ranking civilian within the NSA/CSS.
NSA also has an Inspector General, head of the Office of the Inspector General (OIG), a General Counsel, head of the Office of the General Counsel (OGC) and a Director of Compliance, who is head of the Office of the Director of Compliance (ODOC).
Unlike other intelligence organizations such as CIA or DIA, NSA has always been particularly reticent concerning its internal organizational structure.
As of the mid-1990s, the National Security Agency was organized into five Directorates:
Each of these directorates consisted of several groups or elements, designated by a letter. There were for example the A Group, which was responsible for all SIGINT operations against the Soviet Union and Eastern Europe, and G Group, which was responsible for SIGINT related to all non-communist countries. These groups were divided in units designated by an additional number, like unit A5 for breaking Soviet codes, and G6, being the office for the Middle East, North Africa, Cuba, Central and South America.
Structure.
s of 2013[ [update]], NSA has about a dozen directorates, which are designated by a letter, although not all of them are publicly known. The directorates are divided in divisions and units starting with the letter of the parent directorate, followed by a number for the division, the sub-unit or a sub-sub-unit. New information about NSA units was revealed in top secret documents leaked by Edward Snowden since June 2013. These documents contained information about the NSA's invasion of the public's privacy.
The main elements of the organizational structure of the NSA are:
In the year 2000, a leadership team was formed, consisting of the Director, the Deputy Director and the Directors of the Signals Intelligence (SID), the Information Assurance (IAD) and the Technical Directorate (TD). The chiefs of other main NSA divisions became associate directors of the senior leadership team.
After president George W. Bush initiated the President's Surveillance Program (PSP) in 2001, the NSA created a 24-hour Metadata Analysis Center (MAC), followed in 2004 by the Advanced Analysis Division (AAD), with the mission of analyzing content, internet metadata and telephone metadata. Both units were part of the Signals Intelligence Directorate.
Watch centers.
The NSA maintains at least two watch centers:
Employees.
The number of NSA employees is officially classified but there are several sources providing estimates.
In 1961, NSA had 59,000 military and civilian employees, which grew to 93,067 in 1969, of which 19,300 worked at the headquarters at Fort Meade. In the early 1980s NSA had roughly 50,000 military and civilian personnel. By 1989 this number had grown again to 75,000, of which 25,000 worked at the NSA headquarters. Between 1990 and 1995 the NSA's budget and workforce were cut by one third, which led to a substantial loss of experience.
In 2012, the NSA said more than 30,000 employees worked at Ft. Meade and other facilities. In 2012, John C. Inglis, the deputy director, said that the total number of NSA employees is "somewhere between 37,000 and one billion" as a joke, and stated that the agency is "probably the biggest employer of introverts." In 2013 "Der Spiegel" stated that the NSA had 40,000 employees. More widely, it has been described as the world's largest single employer of mathematicians. Some NSA employees form part of the workforce of the National Reconnaissance Office (NRO), the agency that provides the NSA with satellite signals intelligence.
As of 2013 about 1,000 system administrators work for the NSA.
Security issues.
The NSA received criticism early on in 1960 after two agents had defected to the Soviet Union. Investigations by the House Un-American Activities Committee and a special subcommittee of the House Armed Services Committee revealed severe cases of ignorance in personnel security regulations, prompting the former personnel director and the director of security to step down and leading to the adoption of stricter security practices. Nonetheless, security breaches reoccurred only a year later when in an issue of "Izvestia" of July 23, 1963, a former NSA employee published several cryptologic secrets.
The very same day, an NSA clerk-messenger committed suicide as ongoing investigations disclosed that he had sold secret information to the Soviets on a regular basis. The reluctance of Congressional houses to look into these affairs had prompted a journalist to write "If a similar series of tragic blunders occurred in any ordinary agency of Government an aroused public would insist that those responsible be officially censured, demoted, or fired." David Kahn criticized the NSA's tactics of concealing its doings as smug and the Congress' blind faith in the agency's right-doing as shortsighted, and pointed out the necessity of surveillance by the Congress to prevent abuse of power.
Edward Snowden's leaking of PRISM in 2013 caused the NSA to institute a "two-man rule" where two system administrators are required to be present when one accesses certain sensitive information. Snowden claims he suggested such a rule in 2009.
Polygraphing.
The NSA conducts polygraph tests of employees. For new employees, the tests are meant to discover enemy spies who are applying to the NSA and to uncover any information that could make an applicant pliant to coercion. As part of the latter, historically "EPQs" or "embarrassing personal questions" about sexual behavior had been included in the NSA polygraph. The NSA also conducts five-year periodic reinvestigation polygraphs of employees, focusing on counterintelligence programs. In addition the NSA conducts aperiodic polygraph investigations in order to find spies and leakers; those who refuse to take them may receive "termination of employment", according to a 1982 memorandum from the director of the NSA.
There are also "special access examination" polygraphs for employees who wish to work in highly sensitive areas, and those polygraphs cover counterintelligence questions and some questions about behavior. NSA's brochure states that the average test length is between two and four hours. A 1983 report of the Office of Technology Assessment stated that "It appears that the NSA [National Security Agency] (and possibly CIA) use the polygraph not to determine deception or truthfulness per se, but as a technique of interrogation to encourage admissions." Sometimes applicants in the polygraph process confess to committing felonies such as murder, rape, and selling of illegal drugs. Between 1974 and 1979, of the 20,511 job applicants who took polygraph tests, 695 (3.4%) confessed to previous felony crimes; almost all of those crimes had been undetected.
In 2010 the NSA produced a video explaining its polygraph process. The video, ten minutes long, is titled "The Truth About the Polygraph" and was posted to the website of the Defense Security Service. Jeff Stein of "The Washington Post" said that the video portrays "various applicants, or actors playing them – it's not clear – describing everything bad they had heard about the test, the implication being that none of it is true." AntiPolygraph.org argues that the NSA-produced video omits some information about the polygraph process; it produced a video responding to the NSA video. George Maschke, the founder of the website, accused the NSA polygraph video of being "Orwellian".
After Edward Snowden revealed his identity in 2013, the NSA began requiring polygraphing of employees once per quarter.
Arbitrary firing.
The number of exemptions from legal requirements has been criticized. When in 1964 the Congress was hearing a bill giving the director of the NSA the power to fire at will any employee, the "Washington Post" wrote: "This is the very definition of arbitrariness. It means that an employee could be discharged and disgraced on the basis of anonymous allegations without the slightest opportunity to defend himself." Yet, the bill was accepted by an overwhelming majority.
Insignia and memorials.
The heraldic insignia of NSA consists of an eagle inside a circle, grasping a key in its talons. The eagle represents the agency's national mission. Its breast features a shield with bands of red and white, taken from the Great Seal of the United States and representing Congress. The key is taken from the emblem of Saint Peter and represents security.
When the NSA was created, the agency had no emblem and used that of the Department of Defense. The agency adopted its first of two emblems in 1963. The current NSA insignia has been in use since 1965, when then-Director, LTG Marshall S. Carter (USA) ordered the creation of a device to represent the agency.
The NSA's flag consists of the agency's seal on a light blue background.
Crews associated with NSA missions have been involved in a number of dangerous and deadly situations. The USS "Liberty" incident in 1967 and USS "Pueblo" incident in 1968 are examples of the losses endured during the Cold War.
The National Security Agency/Central Security Service Cryptologic Memorial honors and remembers the fallen personnel, both military and civilian, of these intelligence missions. It is made of black granite, and has 171 names carved into it, as of 2013[ [update]] . It is located at NSA headquarters. A tradition of declassifying the stories of the fallen was begun in 2001.
NSANet (NSA's Intranet).
NSANet stands for National Security Agency Network and is the official NSA intranet. It is a classified network, for information up to the level of TS/SCI to support the use and sharing of intelligence data between NSA and the signals intelligence agencies of the four other nations of the Five Eyes partnership. The management of NSANet has been delegated to the Central Security Service Texas (CSSTEXAS).
NSANet is a highly secured computer network consisting of fiber-optic and satellite communication channels which are almost completely separated from the public internet. The network allows NSA personnel and civilian and military intelligence analysts anywhere in the world to have access to the agency's systems and databases. This access is tightly controlled and monitored. For example, every keystroke is logged, activities are audited at random and downloading and printing of documents from NSANet are recorded.
In 1998, NSANet, along with NIPRNET and SIPRNET, had "significant problems with poor search capabilities, unorganized data and old information". In 2004, the network was reported to have used over twenty commercial off-the-shelf operating systems. Some universities that do highly sensitive research are allowed to connect to it.
The thousands of Top Secret internal NSA documents that were taken by Edward Snowden in 2013 were stored in "a file-sharing location on the NSA's intranet site" so they could easily be read online by NSA personnel. Everyone with a TS/SCI-clearance had access to these documents and as a system administrator, Snowden was responsible for moving accidentally misplaced highly sensitive documents to more secure storage locations.
National Computer Security Center.
The DoD Computer Security Center was founded in 1981 and renamed the National Computer Security Center (NCSC) in 1985. NCSC was responsible for computer security throughout the federal government. NCSC was part of NSA, and during the late 1980s and the 1990s, NSA and NCSC published Trusted Computer System Evaluation Criteria in a six-foot high Rainbow Series of books that detailed trusted computing and network platform specifications. The Rainbow books were replaced by the Common Criteria, however, in the early 2000s.
On July 18, 2013, Greenwald said that Snowden held "detailed blueprints of how the NSA does what they do", thereby sparking fresh controversy.
Facilities.
Headquarters.
Headquarters for the National Security Agency is located at in Fort George G. Meade, Maryland, although it is separate from other compounds and agencies that are based within this same military installation. Ft. Meade is about 20 mi southwest of Baltimore, and 25 mi northeast of Washington, DC. The NSA has its own exit off Maryland Route 295 South labeled "NSA Employees Only". The exit may only be used by people with the proper clearances, and security vehicles parked along the road guard the entrance.
NSA is the largest employer in the U.S. state of Maryland, and two-thirds of its personnel work at Ft. Meade. Built on 350 acre of Ft. Meade's 5000 acre, the site has 1,300 buildings and an estimated 18,000 parking spaces.
The main NSA headquarters and operations building is what James Bamford, author of "Body of Secrets", describes as "a modern boxy structure" that appears similar to "any stylish office building." The building is covered with one-way dark glass, which is lined with copper shielding in order to prevent espionage by trapping in signals and sounds. It contains 3000000 sqft, or more than 68 acre, of floor space; Bamford said that the U.S. Capitol "could easily fit inside it four times over."
The facility has over 100 watchposts, one of them being the visitor control center, a two-story area that serves as the entrance. At the entrance, a white pentagonal structure, visitor badges are issued to visitors and security clearances of employees are checked. The visitor center includes a painting of the NSA seal.
The OPS2A building, the tallest building in the NSA complex and the location of much of the agency's operations directorate, is accessible from the visitor center. Bamford described it as a "dark glass Rubik's Cube". The facility's "red corridor" houses non-security operations such as concessions and the drug store. The name refers to the "red badge" which is worn by someone without a security clearance. The NSA headquarters includes a cafeteria, a credit union, ticket counters for airlines and entertainment, a barbershop, and a bank. NSA headquarters has its own post office, fire department, and police force.
The employees at the NSA headquarters reside in various places in the Baltimore-Washington area, including Annapolis, Baltimore, and Columbia in Maryland and the District of Columbia, including the Georgetown community.
Power consumption.
Following a major power outage in 2000, in 2003 and in follow-ups through 2007, "The Baltimore Sun" reported that the NSA was at risk of electrical overload because of insufficient internal electrical infrastructure at Fort Meade to support the amount of equipment being installed. This problem was apparently recognized in the 1990s but not made a priority, and "now the agency's ability to keep its operations going is threatened."
Baltimore Gas & Electric (BGE, now Constellation Energy) provided NSA with 65 to 75 megawatts at Ft. Meade in 2007, and expected that an increase of 10 to 15 megawatts would be needed later that year. In 2011, NSA at Ft. Meade was Maryland's largest consumer of power. In 2007, as BGE's largest customer, NSA bought as much electricity as Annapolis, the capital city of Maryland.
One estimate put the potential for power consumption by the new Utah Data Center at 40 million per year.
History of headquarters.
When the agency was established, its headquarters and cryptographic center were in the Naval Security Station in Washington, D.C.. The COMINT functions were located in Arlington Hall in Northern Virginia, which served as the headquarters of the U.S. Army's cryptographic operations. Because the Soviet Union had detonated a nuclear bomb and because the facilities were crowded, the federal government wanted to move several agencies, including the AFSA/NSA. A planning committee considered Fort Knox, but Fort Meade, Maryland, was ultimately chosen as NSA headquarters because it was far enough away from Washington, D.C. in case of a nuclear strike and was close enough so its employees would not have to move their families.
Construction of additional buildings began after the agency occupied buildings at Ft. Meade in the late 1950s, which they soon outgrew. In 1963 the new headquarters building, nine stories tall, opened. NSA workers referred to the building as the "Headquarters Building" and since the NSA management occupied the top floor, workers used "Ninth Floor" to refer to their leaders. COMSEC remained in Washington, D.C., until its new building was completed in 1968. In September 1986, the Operations 2A and 2B buildings, both copper-shielded to prevent eavesdropping, opened with a dedication by President Ronald Reagan. The four NSA buildings became known as the "Big Four." The NSA director moved to 2B when it opened.
On March 30, 2015 a vehicle containing two individuals attempted an unauthorized entry at a NSA gate. NSA police fired on the vehicle when it accelerated toward a police car blocking its way and one of the two men in the unauthorized vehicle died on the scene, the other was hospitalized. The two men were dressed as women.
Fort Meade shooting.
On March 30, 2015, shortly before 9 am, a stolen sports utility vehicle approached an NSA police vehicle blocking the road near the gate of Fort Meade, after it was told to leave the area. NSA officers fired on the SUV, killing the 27-year-old driver, Ricky Hall (a trangender person also known as Mya), and seriously injuring his friend, 20-year-old Kevin Fleming. An NSA officer's arm was injured when Hall subsequently crashed into his vehicle.
The two, dressed in women's clothing after a night of partying at a motel with the man they'd stolen the SUV from that morning, "attempted to drive a vehicle into the National Security Agency portion of the installation without authorization", according to an NSA statement. The NSA is investigating the incident, with help from the FBI. FBI spokeswoman Amy Thoreson said the incident is not believed to be related to terrorism.
An anonymous police official told "The Washington Post", "This was not a deliberate attempt to breach the security of NSA. This was not a planned attack." The two are believed to have made a wrong turn off the highway, while fleeing from the motel. A small amount of cocaine was found in the SUV. A local CBS reporter initially said a gun was found, but her later revision does not. Dozens of journalists were corralled into a parking lot blocks away from the scene, and were barred from photographing the area.
Computing.
In 1995, "The Baltimore Sun" reported that the NSA is the owner of the single largest group of supercomputers.
NSA held a groundbreaking ceremony at Ft. Meade in May 2013 for its High Performance Computing Center 2, expected to open in 2016. Called Site M, the center has a 150 megawatt power substation, 14 administrative buildings and 10 parking garages. It cost 3.2 billion and covers 227 acre. The center is 1800000 sqft and initially uses 60 megawatts of electricity.
Increments II and III are expected to be completed by 2030, and would quadruple the space, covering 5800000 sqft with 60 buildings and 40 parking garages. Defense contractors are also establishing or expanding cybersecurity facilities near the NSA and around the Washington metropolitan area.
Other U.S. facilities.
As of 2012, NSA collected intelligence from four geostationary satellites. Satellite receivers were at Roaring Creek Station in Catawissa, Pennsylvania and Salt Creek Station in Arbuckle, California. It operated ten to twenty taps on U.S. telecom switches. NSA had installations in several U.S. states and from them observed intercepts from Europe, the Middle East, North Africa, Latin America, and Asia.
NSA had facilities at Friendship Annex (FANX) in Linthicum, Maryland, which is a 20 to 25-minute drive from Ft. Meade; the Aerospace Data Facility at Buckley Air Force Base in Aurora outside Denver, Colorado; NSA Texas in the Texas Cryptology Center at Lackland Air Force Base in San Antonio, Texas; NSA Georgia at Fort Gordon in Augusta, Georgia; NSA Hawaii in Honolulu; the Multiprogram Research Facility in Oak Ridge, Tennessee, and elsewhere.
On January 6, 2011 a groundbreaking ceremony was held to begin construction on NSA's first Comprehensive National Cyber-security Initiative (CNCI) Data Center, known as the "Utah Data Center" for short. The $1.5B data center is being built at Camp Williams, Utah, located 25 mi south of Salt Lake City, and will help support the agency's National Cyber-security Initiative. It is expected to be operational by September 2013.
In 2009, to protect its assets and to access more electricity, NSA sought to decentralize and expand its existing facilities in Ft. Meade and Menwith Hill, the latter expansion expected to be completed by 2015.
The "Yakima Herald-Republic" cited Bamford, saying that many of NSA's bases for its Echelon program were a legacy system, using outdated, 1990s technology. In 2004, NSA closed its operations at Bad Aibling Station (Field Station 81) in Bad Aibling, Germany. In 2012, NSA began to move some of its operations at Yakima Research Station, Yakima Training Center, in Washington state to Colorado, planning to leave Yakima closed. As of 2013, NSA also intended to close operations at Sugar Grove, West Virginia.
International stations.
Following the signing in 1946–1956 of the UKUSA Agreement between the United States, United Kingdom, Canada, Australia and New Zealand, who then cooperated on signals intelligence and ECHELON, NSA stations were built at GCHQ Bude in Morwenstow, United Kingdom; Geraldton, Pine Gap and Shoal Bay, Australia; Leitrim and Ottawa, Canada; Misawa, Japan; and Waihopai and Tangimoana, New Zealand.
NSA operates RAF Menwith Hill in North Yorkshire, United Kingdom, which was, according to BBC News in 2007, the largest electronic monitoring station in the world. Planned in 1954, and opened in 1960, the base covered 562 acre in 1999.
The agency's European Cryptologic Center (ECC), with 240 employees in 2011, is headquartered at a US military compound in Griesheim, near Frankfurt in Germany. A 2011 NSA report indicates that the ECC is responsible for the "largest analysis and productivity in Europe" and focusses on various priorities, including Africa, Europe, the Middle East and counterterrorism operations.
In 2013, a new Consolidated Intelligence Center, also to be used by NSA, is being built at the headquarters of the United States Army Europe in Wiesbaden, Germany. NSA's partnership with Bundesnachrichtendienst (BND), the German foreign intelligence service, was confirmed by BND president Gerhard Schindler.
Thailand.
Thailand is a "3rd party partner" of the NSA along with nine other nations. These are non-English-speaking countries that have made security agreements for the exchange of SIGINT raw material and end product reports.
Thailand is the site of at least two US SIGINT collection stations. One is at the US Embassy in Bangkok, a joint NSA-CIA Special Collection Service (SCS) unit. It presumably eavesdrops on foreign embassies, governmental communications, and other targets of opportunity.
The second installation is a FORNSAT (foreign satellite interception) station in the Thai city of Khon Kaen. It is codenamed INDRA, but has also been referred to as LEMONWOOD. The station is approximately 40 ha (100 acres) in size and consists of a large 3,700–4,600 m2 (40,000–50,000 ft2) operations building on the west side of the ops compound and four radome-enclosed parabolic antennas. Possibly two of the radome-enclosed antennas are used for SATCOM intercept and two antennas used for relaying the intercepted material back to NSA. There is also a PUSHER-type circularly-disposed antenna array (CDAA) array just north of the ops compound.
NSA activated Khon Kaen in October 1979. Its mission was to eavesdrop on the radio traffic of Chinese army and air force units in southern China, especially in and around the city of Kunming in Yunnan Province. Back in the late 1970s the base consisted only of a small CDAA antenna array that was remote-controlled via satellite from the NSA listening post at Kunia, Hawaii, and a small force of civilian contractors from Bendix Field Engineering Corp. who job it was to keep the antenna array and satellite relay facilities up and running 24/7.
According to the papers of the late General William Odom, the INDRA facility was upgraded in 1986 with a new British-made PUSHER CDAA antenna as part 
of an overall upgrade of NSA and Thai SIGINT facilities whose objective was to spy on the neighboring communist nations of Vietnam, Laos, and Cambodia.
The base apparently fell into disrepair in the 1990s as China and Vietnam became more friendly towards the US, and by 2002 archived satellite imagery
showed that the PUSHER CDAA antenna had been torn down, perhaps indicating that the base had been closed. At some point in the period since 9/11, the Khon Kaen base was reactivated and expanded to include a sizeable SATCOM intercept mission. It is likely that the NSA presence at Khon Kaen is relatively small, and that most of the work is done by civilian contractors.
Operations.
Operations by the National Security Agency can be divided in three types:
Edward Snowden, former CIA system administrator, revealed a plethora of NSA surveillance documents. Edward had been gradually collecting these documents over the years. Snowden now faces 30 years in prison, with charges of NSA reaction included an unauthorized communication of national defense information, theft of government property and potentially espionage.
Mission.
NSA's eavesdropping mission includes radio broadcasting, both from various organizations and individuals, the Internet, telephone calls, and other intercepted forms of communication. Its secure communications mission includes military, diplomatic, and all other sensitive, confidential or secret government communications.
According to the "Washington Post", "[e]very day, collection systems at the National Security Agency intercept and store 1.7 billion e-mails, phone calls and other types of communications. The NSA sorts a fraction of those into 70 separate databases."
Because of its listening task, NSA/CSS has been heavily involved in cryptanalytic research, continuing the work of predecessor agencies which had broken many World War II codes and ciphers (see, for instance, Purple, Venona project, and JN-25).
In 2004, NSA Central Security Service and the National Cyber Security Division of the Department of Homeland Security (DHS) agreed to expand NSA Centers of Academic Excellence in Information Assurance Education Program.
As part of the National Security Presidential Directive 54/Homeland Security Presidential Directive 23 (NSPD 54), signed on January 8, 2008 by President Bush, the NSA became the lead agency to monitor and protect all of the federal government's computer networks from cyber-terrorism.
Collection overseas.
Echelon.
Echelon was created in the incubator of the Cold War. Today it is a legacy system, and several NSA stations are closing.
NSA/CSS, in combination with the equivalent agencies in the United Kingdom (Government Communications Headquarters), Canada (Communications Security Establishment), Australia (Defence Signals Directorate), and New Zealand (Government Communications Security Bureau), otherwise known as the UKUSA group, was reported to be in command of the operation of the so-called ECHELON system. Its capabilities were suspected to include the ability to monitor a large proportion of the world's transmitted civilian telephone, fax and data traffic.
During the early 1970s, the first of what became more than eight large satellite communications dishes were installed at Menwith Hill. Investigative journalist Duncan Campbell reported in 1988 on the ECHELON surveillance program, an extension of the UKUSA Agreement on global signals intelligence SIGINT, and detailed how the eavesdropping operations worked. In November 3, 1999 the BBC reported that they had confirmation from the Australian Government of the existence of a powerful "global spying network" code-named Echelon, that could "eavesdrop on every single phone call, fax or e-mail, anywhere on the planet" with Britain and the United States as the chief protagonists. They confirmed that Menwith Hill was "linked directly to the headquarters of the US National Security Agency (NSA) at Fort Meade in Maryland".
NSA's United States Signals Intelligence Directive 18 (USSID 18) strictly prohibited the interception or collection of information about "... U.S. persons, entities, corporations or organizations..." without explicit written legal permission from the United States Attorney General when the subject is located abroad, or the Foreign Intelligence Surveillance Court when within U.S. borders. Alleged Echelon-related activities, including its use for motives other than national security, including political and industrial espionage, received criticism from countries outside the UKUSA alliance.
Other SIGINT operations overseas.
The NSA is also involved in planning to blackmail people with "SEXINT", intelligence gained about a potential target's sexual activity and preferences. Those targeted had not committed any apparent crime nor were charged with one.
In order to support its facial recognition program, the NSA is intercepting "millions of images per day".
The Real Time Regional Gateway is a data collection program introduced in 2005 in Iraq by NSA during the Iraq War that consisted of gathering all electronic communication, storing it, then searching and otherwise analyzing it. It was effective in providing information about Iraqi insurgents who had eluded less comprehensive techniques. This "collect it all" strategy introduced by NSA director, Keith B. Alexander, is believed by Glenn Greenwald of "The Guardian" to be the model for the comprehensive world-wide mass archiving of communications which NSA is engaged in as of 2013.
BoundlessInformant.
Edward Snowden revealed in June 2013 that between February 8 and March 8, 2013, the NSA collected about 124.8 billion telephone data items and 97.1 billion computer data items throughout the world, as was displayed in charts from an internal NSA tool codenamed Boundless Informant. It was erroneously reported that some of these data reflected eavesdropping on citizens in countries like Germany, Spain and France. Later it became clear that these "European" data were actually collected by European military intelligence agencies during military operations abroad, and subsequently shared with NSA.
BoundlessInformant employs big data databases, cloud computing technology, and Free and Open Source Software (FOSS) to analyze data collected worldwide by the NSA.
Bypassing encryption.
In 2013, reporters uncovered a secret memo that claims the NSA created and pushed for the adoption of the Dual_EC_DRBG encryption standard that contained built-in vulnerabilities in 2006 to the United States National Institute of Standards and Technology (NIST), and the International Organization for Standardization (aka ISO). This memo appears to give credence to previous speculation by cryptographers at Microsoft Research. Edward Snowden claims that the NSA often bypasses encryption altogether by lifting information before it is encrypted or after it is decrypted.
XKeyscore rules (as specified in a file xkeyscorerules100.txt, sourced by German TV stations NDR and WDR, who claim to have excerpts from its source code) reveal that the NSA tracks users of privacy-enhancing software tools, including Tor, the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in Cambridge, Massachusetts, and readers of the "Linux Journal".
Domestic activity.
NSA's mission, as set forth in Executive Order 12333 in 1981, is to collect information that constitutes "foreign intelligence or counterintelligence" while "not" "acquiring information concerning the domestic activities of United States persons". NSA has declared that it relies on the FBI to collect information on foreign intelligence activities within the borders of the United States, while confining its own activities within the United States to the embassies and missions of foreign nations.
The appearance of a 'Domestic Surveillance Directorate' of the NSA was soon exposed as a hoax in 2013.
NSA's domestic surveillance activities are limited by the requirements imposed by the Fourth Amendment to the U.S. Constitution. The Foreign Intelligence Surveillance Court for example held in October 2011, citing multiple Supreme Court precedents, that the Fourth Amendment prohibitions against unreasonable searches and seizures applies to the contents of all communications, whatever the means, because "a person's private communications are akin to personal papers." However, these protections do not apply to non-U.S. persons located outside of U.S. borders, so the NSA's foreign surveillance efforts are subject to far fewer limitations under U.S. law. The specific requirements for domestic surveillance operations are contained in the Foreign Intelligence Surveillance Act of 1978 (FISA), which does not extend protection to non-U.S. citizens located outside of U.S. territory.
These activities, especially the publicly acknowledged domestic telephone tapping and call database programs, have prompted questions about the extent of the NSA's activities and concerns about threats to privacy and the rule of law. It has been suggested that were the activities of the NSA put to a vote, the public would not have approved the surveillance program.
George W. Bush administration.
Warrantless wiretaps.
On December 16, 2005, "The New York Times" reported that, under White House pressure and with an executive order from President George W. Bush, the National Security Agency, in an attempt to thwart terrorism, had been tapping phone calls made to persons outside the country, without obtaining warrants from the United States Foreign Intelligence Surveillance Court, a secret court created for that purpose under the Foreign Intelligence Surveillance Act (FISA).
One such surveillance program, authorized by the U.S. Signals Intelligence Directive 18 of President George Bush, was the Highlander Project undertaken for the National Security Agency by the U.S. Army 513th Military Intelligence Brigade. NSA relayed telephone (including cell phone) conversations obtained from ground, airborne, and satellite monitoring stations to various U.S. Army Signal Intelligence Officers, including the 201st Military Intelligence Battalion. Conversations of citizens of the U.S. were intercepted, along with those of other nations.
Proponents of the surveillance program claim that the President has executive authority to order such action, arguing that laws such as FISA are overridden by the President's Constitutional powers. In addition, some argued that FISA was implicitly overridden by a subsequent statute, the Authorization for Use of Military Force, although the Supreme Court's ruling in Hamdan v. Rumsfeld deprecates this view. In the August 2006 case "ACLU v. NSA", U.S. District Court Judge Anna Diggs Taylor concluded that NSA's warrantless surveillance program was both illegal and unconstitutional. On July 6, 2007 the 6th Circuit Court of Appeals vacated the decision on the grounds that the ACLU lacked standing to bring the suit.
On January 17, 2006, the Center for Constitutional Rights filed a lawsuit, CCR v. Bush, against the George W. Bush Presidency. The lawsuit challenged the National Security Agency's (NSA's) surveillance of people within the U.S., including the interception of CCR emails without securing a warrant first.
In September 2008, the Electronic Frontier Foundation (EFF) filed a class action lawsuit against the NSA and several high-ranking officials of the Bush administration, charging an "illegal and unconstitutional program of dragnet communications surveillance," based on documentation provided by former AT&T technician Mark Klein.
AT&T Internet monitoring.
In May 2006, Mark Klein, a former AT&T employee, alleged that his company had cooperated with NSA in installing Narus hardware to replace the FBI Carnivore program, to monitor network communications including traffic between American citizens.
Data mining.
NSA was reported in 2008 to use its computing capability to analyze "transactional" data that it regularly acquires from other government agencies, which gather it under their own jurisdictional authorities. As part of this effort, NSA now monitors huge volumes of records of domestic email data, web addresses from Internet searches, bank transfers, credit-card transactions, travel records, and telephone data, according to current and former intelligence officials interviewed by "The Wall Street Journal". The sender, recipient, and subject line of emails can be included, but the content of the messages or of phone calls are not.
A 2013 advisory group for the Obama administration, seeking to reform NSA spying programs following the revelations of documents released by Edward J. Snowden. mentioned in 'Recommendation 30' on page 37, "...that the National Security Council staff should manage an interagency process to review on a regular basis the activities of the US Government regarding attacks that exploit a previously unknown vulnerability in a computer application." Retired cyber security expert Richard A. Clarke was a group member and stated on April 11 that NSA had no advance knowledge of Heartbleed.
Illegally obtained evidence.
In August 2013 it was revealed that a 2005 IRS training document showed that NSA intelligence intercepts and wiretaps, both foreign and domestic, were being supplied to the Drug Enforcement Administration (DEA) and Internal Revenue Service (IRS) and were illegally used to launch criminal investigations of US citizens. Law enforcement agents were directed to conceal how the investigations began and recreate an apparently legal investigative trail by re-obtaining the same evidence by other means.
Barack Obama Administration.
In the months leading to April 2009, the NSA intercepted the communications of American citizens, including a Congressman, although the Justice Department believed that the interception was unintentional. The Justice Department then took action to correct the issues and bring the program into compliance with existing laws. United States Attorney General Eric Holder resumed the program according to his understanding of the Foreign Intelligence Surveillance Act amendment of 2008, without explaining what had occurred.
Polls conducted in June 2013 found divided results among Americans regarding NSA's secret data collection. Rasmussen Reports found that 59% of Americans disapprove, Gallup found that 53% disapprove, and Pew found that 56% are in favor of NSA data collection.
Section 215 metadata collection.
On April 25, 2013, the NSA obtained a court order requiring Verizon's Business Network Services to provide metadata on all calls in its system to the NSA "on an ongoing daily basis" for a three-month period, as reported by "The Guardian" on June 6, 2013. This information includes "the numbers of both parties on a call ... location data, call duration, unique identifiers, and the time and duration of all calls" but not "[t]he contents of the conversation itself". The order relies on the so-called "business records" provision of the Patriot Act.
In August 2013, following the Snowden leaks, new details about the NSA's data mining activity were revealed. Reportedly, the majority of emails into or out of the United States are captured at "selected communications links" and automatically analyzed for keywords or other "selectors". Emails that do not match are deleted.
The utility of such a massive metadata collection in preventing terrorist attacks is disputed. Many studies reveal the dragnet like system to be ineffective. One such report, released by the New America Foundation concluded that after an analysis of 225 terrorism cases, the NSA "had no discernible impact on preventing acts of terrorism."
Defenders of the program say that while metadata alone can't provide all the information necessary to prevent an attack, it assures the ability to "connect the dots" between suspect foreign numbers and domestic numbers with a speed only the NSA's software is capable of. One benefit of this is quickly being able to determine the difference between suspicious activity and real threats. As an example, NSA director General Keith Alexander mentioned at the annual Cybersecurity Summit in 2013, that metadata analysis of domestic phone call records after the Boston Marathon Bombing helped determine that another attack in New York was baseless.
In addition to doubts about its effectiveness, many people argue that the collection of metadata is an unconstitutional invasion of privacy. s of 2015[ [update]], the collection process remains legal and grounded in the ruling from Smith v. Maryland (1979). A prominent opponent of the data collection and its legality is U.S. District Judge Richard J. Leon, who issued a report in 2013 in which he stated:
"I cannot imagine a more 'indiscriminate' and 'arbitrary invasion' than this systematic and high tech collection and retention of personal data on virtually every single citizen for purposes of querying and analyzing it without prior judicial approval...Surely, such a program infringes on 'that degree of privacy' that the founders enshrined in the Fourth Amendment".
The PRISM program.
Under the PRISM program, which started in 2007, NSA gathers internet communications from foreign targets from nine major U.S. internet-based communication service providers: Microsoft, Yahoo, Google, Facebook, PalTalk, AOL, Skype, YouTube and Apple. Data gathered include email, video and voice chat, videos, photos, VoIP chats such as Skype, and file transfers.
Prevented terrorist attacks.
According to former NSA director General Keith Alexander, in September 2009 the NSA prevented Najibullah Zazi and his friends from carrying out a terrorist attack. The NSA tagged Zazi as a possible threat because he was contacting people affiliated with terrorist activity through emails, which the NSA was able to obtain through one of PRISM's dragnets. The NSA tipped off the FBI, which began a program called Operation High-Rise. Operation High-Rise discovered that Zazi was planning to suicide bomb the New York City Subway. Zazi called off the attacks after receiving a tip about law-enforcement inquiries and was later arrested.
Hacking operations.
The espionage group named Equation Group is suspected of being a part of NSA.
Software backdoors.
Linux kernel.
Linus Torvalds, the founder of Linux kernel, joked during a LinuxCon keynote on September 18, 2013 that the NSA, who are the founder of SELinux, wanted a backdoor in the kernel. However later, Linus' father, a Member of the European Parliament (MEP), revealed that the NSA actually did this. When my oldest son [Linus Torvalds] was asked the same question: "Has he been approached by the NSA about backdoors?" he said "No", but at the same time he nodded. Then he was sort of in the legal free. He had given the right answer, [but] everybody understood that the NSA had approached him.—Nils Torvalds, LIBE Committee Inquiry on Electronic Mass Surveillance of EU Citizens – 11th Hearing, 11 November 2013
Microsoft Windows.
codice_1 was a variable name discovered in Microsoft's Windows NT 4 Service Pack 5 (which had been released unstripped of its symbolic debugging data) in August 1999 by Andrew D. Fernandes of Cryptonym Corporation. That variable contained a 1024-bit public key.
IBM Notes.
IBM Notes was the first widely adopted software product to use public key cryptography for client–server and server–server authentication and for encryption of data. Until US laws regulating encryption were changed in 2000, IBM and Lotus were prohibited from exporting versions of Notes that supported symmetric encryption keys that were longer than 40 bits. In 1997, Lotus negotiated an agreement with the NSA that allowed export of a version that supported stronger keys with 64 bits, but 24 of the bits were encrypted with a special key and included in the message to provide a "workload reduction factor" for the NSA. This strengthened the protection for users of Notes outside the US against private-sector industrial espionage, but not against spying by the US government.
Boomerang routing.
While it is assumed that foreign transmissions terminating in the U.S. (such as a non-U.S. citizen accessing a U.S. website) subject non-U.S. citizens to NSA surveillance, recent research into boomerang routing has raised new concerns about the NSA's ability to surveil the domestic internet traffic of foreign countries. Boomerang routing occurs when an internet transmission that originates and terminates in a single country transits another. Research at the University of Toronto has suggested that approximately 25% of Canadian domestic traffic may be subject to NSA surveillance activities as a result of the boomerang routing of Canadian internet service providers.
Hardware implanting.
A document included in NSA files released with Glenn Greenwald's book "No Place to Hide" details how the agency's Tailored Access Operations (TAO) and other NSA units gain access to hardware. They intercept routers, servers and other network hardware being shipped to organizations targeted for surveillance and install covert implant firmware onto them before they are delivered. This was described by an NSA manager as "some of the most productive operations in TAO because they preposition access points into hard target networks around the world."
Computers seized by the NSA due to interdiction are often modified with a physical device known as Cottonmouth Cottonmouth is a device that can be inserted in the USB port of a computer in order to establish remote access to the targeted machine. According to NSA's Tailored Access Operations (TAO) group implant catalog, after implanting Cottonmouth, the NSA can establish Bridging (networking) "that allows the NSA to load exploit software onto modified computers as well as allowing the NSA to relay commands and data between hardware and software implants."
Role in scientific research and development.
NSA has been involved in debates about public policy, both indirectly as a behind-the-scenes adviser to other departments, and directly during and after Vice Admiral Bobby Ray Inman's directorship. NSA was a major player in the debates of the 1990s regarding the export of cryptography in the United States. Restrictions on export were reduced but not eliminated in 1996.
Its secure government communications work has involved the NSA in numerous technology areas, including the design of specialized communications hardware and software, production of dedicated semiconductors (at the Ft. Meade chip fabrication plant), and advanced cryptography research. For 50 years, NSA designed and built most of its computer equipment in-house, but from the 1990s until about 2003 (when the U.S. Congress curtailed the practice), the agency contracted with the private sector in the fields of research and equipment.
Data Encryption Standard.
NSA was embroiled in some minor controversy concerning its involvement in the creation of the Data Encryption Standard (DES), a standard and public block cipher algorithm used by the U.S. government and banking community. During the development of DES by IBM in the 1970s, NSA recommended changes to some details of the design. There was suspicion that these changes had weakened the algorithm sufficiently to enable the agency to eavesdrop if required, including speculation that a critical component—the so-called S-boxes—had been altered to insert a "backdoor" and that the reduction in key length might have made it feasible for NSA to discover DES keys using massive computing power. It has since been observed that the S-boxes in DES are particularly resilient against differential cryptanalysis, a technique which was not publicly discovered until the late 1980s, but which was known to the IBM DES team.
The United States Senate Select Committee on Intelligence reviewed NSA's involvement, and concluded that while the agency had provided some assistance, it had not tampered with the design. In late 2009 NSA declassified information stating that "NSA worked closely with IBM to strengthen the algorithm against all except brute force attacks and to strengthen substitution tables, called S-boxes. Conversely, NSA tried to convince IBM to reduce the length of the key from 64 to 48 bits. Ultimately they compromised on a 56-bit key."
Advanced Encryption Standard.
The involvement of NSA in the selection of a successor to Data Encryption Standard (DES), the Advanced Encryption Standard (AES), was limited to hardware performance testing (see AES competition). NSA has subsequently certified AES for protection of classified information () when used in NSA-approved systems.
NSA encryption systems.
The NSA is responsible for the encryption-related components in these legacy systems:
The NSA oversees encyption in following systems which are in use today:
The NSA has specified Suite A and Suite B cryptographic algorithm suites to be used in U.S. government systems; the Suite B algorithms are a subset of those previously specified by NIST and are expected to serve for most information protection purposes, while the Suite A algorithms are secret and are intended for especially high levels of protection.
SHA.
The widely used SHA-1 and SHA-2 hash functions were designed by NSA. SHA-1 is a slight modification of the weaker SHA-0 algorithm, also designed by NSA in 1993. This small modification was suggested by NSA two years later, with no justification other than the fact that it provides additional security. An attack for SHA-0 that does not apply to the revised algorithm was indeed found between 1998 and 2005 by academic cryptographers. Because of weaknesses and key length restrictions in SHA-1, NIST deprecates its use for digital signatures, and approves only the newer SHA-2 algorithms for such applications from 2013 on.
A new hash standard, SHA-3, has recently been selected through the competition concluded October 2, 2012 with the selection of Keccak as the algorithm. The process to select SHA-3 was similar to the one held in choosing the AES, but some doubts have been cast over it, since fundamental modifications have been made to Keccak in order to turn it into a standard. These changes potentially undermine the cryptanalysis performed during the competition and reduce the security levels of the algorithm.
Dual_EC_DRBG random number generator.
NSA promoted the inclusion of a random number generator called Dual_EC_DRBG in the U.S. National Institute of Standards and Technology's 2007 guidelines. This led to speculation of a backdoor which would allow NSA access to data encrypted by systems using that pseudo random number generator.
This is now deemed to be plausible based on the fact that the output of the next iterations of the PRNG can provably be determined if the relation between two internal elliptic curve points is known. Both NIST and RSA are now officially recommending against the use of this PRNG.
Clipper chip.
Because of concerns that widespread use of strong cryptography would hamper government use of wiretaps, NSA proposed the concept of key escrow in 1993 and introduced the Clipper chip that would offer stronger protection than DES but would allow access to encrypted data by authorized law enforcement officials. The proposal was strongly opposed and key escrow requirements ultimately went nowhere. However, NSA's Fortezza hardware-based encryption cards, created for the Clipper project, are still used within government, and NSA ultimately declassified and published the design of the Skipjack cipher used on the cards.
Perfect Citizen.
Perfect Citizen is a program to perform vulnerability assessment by the NSA on U.S. critical infrastructure. It was originally reported to be a program to develop a system of sensors to detect cyber attacks on critical infrastructure computer networks in both the private and public sector through a network monitoring system named "Einstein". It is funded by the Comprehensive National Cybersecurity Initiative and thus far Raytheon has received a contract for up to $100 million for the initial stage.
Academic research.
NSA has invested many millions of dollars in academic research under grant code prefix "MDA904", resulting in over 3,000 papers (as of 2007-10-11). NSA/CSS has, at times, attempted to restrict the publication of academic research into cryptography; for example, the Khufu and Khafre block ciphers were voluntarily withheld in response to an NSA request to do so. In response to a FOIA lawsuit, in 2013 the NSA released the 643-page research paper titled, "Untangling the Web: A Guide to Internet Research, " written and compiled by NSA employees to assist other NSA workers in searching for information of interest to the agency on the public Internet.
Patents.
NSA has the ability to file for a patent from the U.S. Patent and Trademark Office under gag order. Unlike normal patents, these are not revealed to the public and do not expire. However, if the Patent Office receives an application for an identical patent from a third party, they will reveal NSA's patent and officially grant it to NSA for the full term on that date.
One of NSA's published patents describes a method of geographically locating an individual computer site in an Internet-like network, based on the latency of multiple network connections. Although no public patent exists, NSA is reported to have used a similar locating technology called trilateralization that allows real-time tracking of an individual's location, including altitude from ground level, using data obtained from cellphone towers.
Legality.
In the United States, at least since 2001, there has been legal controversy over what signal intelligence can be used for and how much freedom the National Security Agency has to use signal intelligence. The government has made, in 2015, slight changes in how it uses and collects certain types of data, specifically phone records. President Barack Obama has asked lawyers and his national security team to look at the tactics that are being used by the NSA. President Obama made a speech on January 17, 2014 where he defended the national security measures, including the NSA, and their intentions for keeping the country safe through surveillance. He said that it is difficult to determine where the line should be drawn between what is too much surveillance and how much is needed for national security because technology is ever changing and evolving. Therefore, the laws cannot keep up with the rapid advancements.
President Obama did make some changes to national security regulations and how much data can be collected and surveyed. The first thing he added, was more presidential directive and oversight so that privacy and basic rights are not violated. The president would look over requests on behalf of American citizens to make sure that their personal privacy is not violated by the data that is being requested. Secondly, surveillance tactics and procedures are becoming more public, including over 40 rulings of the FISC that have been declassified. Thirdly, further protections are being placed on activities that are justified under Section 702, such as the ability to retain, search and use data collected in investigations, which allows the NSA to monitor and intercept interaction of targets overseas. Finally, national security letters, which are secret requests for information that the FBI uses in their investigations, are becoming less secretive. The secrecy of the information requested will not be definite and will terminate after a set time if future secrecy is not required. Concerning the bulk surveillance of American's phone records, President Obama also ordered a transition from bulk surveillance under Section 215 to a new policy that will eliminate unnecessary bulk collection of metadata.
The details of this transition are still being worked out. One of the proposals is a third party source holding the bulk metadata, where the NSA would need to ask permission to access the data if it is relevant to national security. President Obama emphasized that the government is not spying on ordinary citizens, but rather working to keep America safe.

</doc>
<doc id="21944" url="http://en.wikipedia.org/wiki?curid=21944" title="Nervous system">
Nervous system

The nervous system is the part of an animal's body that coordinates its voluntary and involuntary actions and transmits signals between different parts of its body. Nervous tissue first arose in wormlike organisms about 550 to 600 million years ago. In most animal species it consists of two main parts, the central nervous system (CNS) and the peripheral nervous system (PNS). The CNS contains the brain and spinal cord. The PNS consists mainly of nerves, which are enclosed bundles of the long fibers or axons, that connect the CNS to every other part of the body. The PNS includes motor neurons, mediating voluntary movement; the autonomic nervous system, comprising the sympathetic nervous system and the parasympathetic nervous system, which regulate involuntary functions, and the enteric nervous system, which functions to control the gastrointestinal system.
At the cellular level, the nervous system is defined by the presence of a special type of cell, called the neuron, also known as a "nerve cell". Neurons have special structures that allow them to send signals rapidly and precisely to other cells. They send these signals in the form of electrochemical waves traveling along thin fibers called axons, which cause chemicals called neurotransmitters to be released at junctions called synapses. A cell that receives a synaptic signal from a neuron may be excited, inhibited, or otherwise modulated. The connections between neurons can form neural circuits and also neural networks that generate an organism's perception of the world and determine its behavior. Along with neurons, the nervous system contains other specialized cells called glial cells (or simply glia), which provide structural and metabolic support.
Nervous systems are found in most multicellular animals, but vary greatly in complexity. The only multicellular animals that have no nervous system at all are sponges, placozoans, and mesozoans, which have very simple body plans. The nervous systems of the radially symmetric organisms ctenophores (comb jellies) and cnidarians (which include anemones, hydras, corals and jellyfish) consist of a diffuse nerve net. All other animal species, with the exception of a few types of worm, have a nervous system containing a brain, a central cord (or two cords running in parallel), and nerves radiating from the brain and central cord. The size of the nervous system ranges from a few hundred cells in the simplest worms, to around 100 billion cells in humans.
The central nervous system functions to send signals from one cell to others, or from one part of the body to others and to receive feedback. Malfunction of the nervous system can occur as a result of genetic defects, physical damage due to trauma or toxicity, infection or simply of ageing. The medical specialty of neurology studies disorders of the nervous system and looks for interventions that can prevent or treat them. In the peripheral nervous system, the most common problem is the failure of nerve conduction, which can be due to different causes including diabetic neuropathy and demyelinating disorders such as multiple sclerosis and amyotrophic lateral sclerosis.
Neuroscience is the field of science that focuses on the study of the nervous system.
Structure.
The nervous system derives its name from nerves, which are cylindrical bundles of fibers (the axons of neurons), that emanate from the brain and spinal cord, and branch repeatedly to innervate every part of the body. Nerves are large enough to have been recognized by the ancient Egyptians, Greeks, and Romans, but their internal structure was not understood until it became possible to examine them using a microscope. A microscopic examination shows that nerves consist primarily of axons, along with different membranes that wrap around them and segregate them into fascicles. The neurons that give rise to nerves do not lie entirely within the nerves themselves—their cell bodies reside within the brain, spinal cord, or peripheral ganglia.
All animals more advanced than sponges have nervous systems. However, even sponges, unicellular animals, and non-animals such as slime molds have cell-to-cell signalling mechanisms that are precursors to those of neurons. In radially symmetric animals such as the jellyfish and hydra, the nervous system consists of a nerve net, a diffuse network of isolated cells. In bilaterian animals, which make up the great majority of existing species, the nervous system has a common structure that originated early in the Ediacaran period, over 550 million years ago.
Cells.
The nervous system contains two main categories or types of cells: neurons and glial cells. 
Neurons.
The nervous system is defined by the presence of a special type of cell—the neuron (sometimes called "neurone" or "nerve cell"). Neurons can be distinguished from other cells in a number of ways, but their most fundamental property is that they communicate with other cells via synapses, which are membrane-to-membrane junctions containing molecular machinery that allows rapid transmission of signals, either electrical or chemical. Many types of neuron possess an axon, a protoplasmic protrusion that can extend to distant parts of the body and make thousands of synaptic contacts. Axons frequently travel through the body in bundles called nerves.
Even in the nervous system of a single species such as humans, hundreds of different types of neurons exist, with a wide variety of morphologies and functions. These include sensory neurons that transmute physical stimuli such as light and sound into neural signals, and motor neurons that transmute neural signals into activation of muscles or glands; however in many species the great majority of neurons receive all of their input from other neurons and send their output to other neurons.
Glial cells.
Glial cells (named from the Greek for "glue") are non-neuronal cells that provide support and nutrition, maintain homeostasis, form myelin, and participate in signal transmission in the nervous system. In the human brain, it is estimated that the total number of glia roughly equals the number of neurons, although the proportions vary in different brain areas. Among the most important functions of glial cells are to support neurons and hold them in place; to supply nutrients to neurons; to insulate neurons electrically; to destroy pathogens and remove dead neurons; and to provide guidance cues directing the axons of neurons to their targets. A very important type of glial cell (oligodendrocytes in the central nervous system, and Schwann cells in the peripheral nervous system) generates layers of a fatty substance called myelin that wraps around axons and provides electrical insulation which allows them to transmit action potentials much more rapidly and efficiently.
Anatomy in vertebrates.
The nervous system of vertebrates (including humans) is divided into the central nervous system (CNS) and the peripheral nervous system (PNS).
The (CNS) is the major division, and consists of the brain and the spinal cord. The spinal canal contains the spinal cord, while the head contains the brain. The CNS is enclosed and protected by the meninges, a three-layered system of membranes, including a tough, leathery outer layer called the dura mater. The brain is also protected by the skull, and the spinal cord by the vertebrae.
The peripheral nervous system (PNS) is a collective term for the nervous system structures that do not lie within the CNS. The large majority of the axon bundles called nerves are considered to belong to the PNS, even when the cell bodies of the neurons to which they belong reside within the brain or spinal cord. The PNS is divided into somatic and visceral parts. The somatic part consists of the nerves that innervate the skin, joints, and muscles. The cell bodies of somatic sensory neurons lie in dorsal root ganglia of the spinal cord. The visceral part, also known as the autonomic nervous system, contains neurons that innervate the internal organs, blood vessels, and glands. The autonomic nervous system itself consists of two parts: the sympathetic nervous system and the parasympathetic nervous system. Some authors also include sensory neurons whose cell bodies lie in the periphery (for senses such as hearing) as part of the PNS; others, however, omit them.
The vertebrate nervous system can also be divided into areas called grey matter ("gray matter" in American spelling) and white matter. Grey matter (which is only grey in preserved tissue, and is better described as pink or light brown in living tissue) contains a high proportion of cell bodies of neurons. White matter is composed mainly of myelinated axons, and takes its color from the myelin. White matter includes all of the nerves, and much of the interior of the brain and spinal cord. Grey matter is found in clusters of neurons in the brain and spinal cord, and in cortical layers that line their surfaces. There is an anatomical convention that a cluster of neurons in the brain or spinal cord is called a nucleus, whereas a cluster of neurons in the periphery is called a ganglion. There are, however, a few exceptions to this rule, notably including the part of the forebrain called the basal ganglia.
Comparative anatomy and evolution.
Neural precursors in sponges.
Sponges have no cells connected to each other by synaptic junctions, that is, no neurons, and therefore no nervous system. They do, however, have homologs of many genes that play key roles in synaptic function. Recent studies have shown that sponge cells express a group of proteins that cluster together to form a structure resembling a postsynaptic density (the signal-receiving part of a synapse). However, the function of this structure is currently unclear. Although sponge cells do not show synaptic transmission, they do communicate with each other via calcium waves and other impulses, which mediate some simple actions such as whole-body contraction.
Radiata.
Jellyfish, comb jellies, and related animals have diffuse nerve nets rather than a central nervous system. In most jellyfish the nerve net is spread more or less evenly across the body; in comb jellies it is concentrated near the mouth. The nerve nets consist of sensory neurons, which pick up chemical, tactile, and visual signals; motor neurons, which can activate contractions of the body wall; and intermediate neurons, which detect patterns of activity in the sensory neurons and, in response, send signals to groups of motor neurons. In some cases groups of intermediate neurons are clustered into discrete ganglia.
The development of the nervous system in radiata is relatively unstructured. Unlike bilaterians, radiata only have two primordial cell layers, endoderm and ectoderm. Neurons are generated from a special set of ectodermal precursor cells, which also serve as precursors for every other ectodermal cell type.
Bilateria.
The vast majority of existing animals are bilaterians, meaning animals with left and right sides that are approximate mirror images of each other. All bilateria are thought to have descended from a common wormlike ancestor that appeared in the Ediacaran period, 550–600 million years ago. The fundamental bilaterian body form is a tube with a hollow gut cavity running from mouth to anus, and a nerve cord with an enlargement (a "ganglion") for each body segment, with an especially large ganglion at the front, called the "brain".
Even mammals, including humans, show the segmented bilaterian body plan at the level of the nervous system. The spinal cord contains a series of segmental ganglia, each giving rise to motor and sensory nerves that innervate a portion of the body surface and underlying musculature. On the limbs, the layout of the innervation pattern is complex, but on the trunk it gives rise to a series of narrow bands. The top three segments belong to the brain, giving rise to the forebrain, midbrain, and hindbrain.
Bilaterians can be divided, based on events that occur very early in embryonic development, into two groups (superphyla) called protostomes and deuterostomes. Deuterostomes include vertebrates as well as echinoderms, hemichordates (mainly acorn worms), and Xenoturbellidans. Protostomes, the more diverse group, include arthropods, molluscs, and numerous types of worms. There is a basic difference between the two groups in the placement of the nervous system within the body: protostomes possess a nerve cord on the ventral (usually bottom) side of the body, whereas in deuterostomes the nerve cord is on the dorsal (usually top) side. In fact, numerous aspects of the body are inverted between the two groups, including the expression patterns of several genes that show dorsal-to-ventral gradients. Most anatomists now consider that the bodies of protostomes and deuterostomes are "flipped over" with respect to each other, a hypothesis that was first proposed by Geoffroy Saint-Hilaire for insects in comparison to vertebrates. Thus insects, for example, have nerve cords that run along the ventral midline of the body, while all vertebrates have spinal cords that run along the dorsal midline.
Worms.
Worms are the simplest bilaterian animals, and reveal the basic structure of the bilaterian nervous system in the most straightforward way. As an example, earthworms have dual nerve cords running along the length of the body and merging at the tail and the mouth. These nerve cords are connected by transverse nerves like the rungs of a ladder. These transverse nerves help coordinate the two sides of the animal. Two ganglia at the head end function similar to a simple brain. Photoreceptors on the animal's eyespots provide sensory information on light and dark.
The nervous system of one very small roundworm, the nematode "Caenorhabditis elegans", has been completely mapped out in a connectome including its synapses. Every neuron and its cellular lineage has been recorded and most, if not all, of the neural connections are known. In this species, the nervous system is sexually dimorphic; the nervous systems of the two sexes, males and female hermaphrodites, have different numbers of neurons and groups of neurons that perform sex-specific functions. In "C. elegans", males have exactly 383 neurons, while hermaphrodites have exactly 302 neurons.
Arthropods.
Arthropods, such as insects and crustaceans, have a nervous system made up of a series of ganglia, connected by a ventral nerve cord made up of two parallel connectives running along the length of the belly. Typically, each body segment has one ganglion on each side, though some ganglia are fused to form the brain and other large ganglia. The head segment contains the brain, also known as the supraesophageal ganglion. In the insect nervous system, the brain is anatomically divided into the protocerebrum, deutocerebrum, and tritocerebrum. Immediately behind the brain is the subesophageal ganglion, which is composed of three pairs of fused ganglia. It controls the mouthparts, the salivary glands and certain muscles. Many arthropods have well-developed sensory organs, including compound eyes for vision and antennae for olfaction and pheromone sensation. The sensory information from these organs is processed by the brain.
In insects, many neurons have cell bodies that are positioned at the edge of the brain and are electrically passive—the cell bodies serve only to provide metabolic support and do not participate in signalling. A protoplasmic fiber runs from the cell body and branches profusely, with some parts transmitting signals and other parts receiving signals. Thus, most parts of the insect brain have passive cell bodies arranged around the periphery, while the neural signal processing takes place in a tangle of protoplasmic fibers called neuropil, in the interior.
"Identified" neurons.
A neuron is called "identified" if it has properties that distinguish it from every other neuron in the same animal—properties such as location, neurotransmitter, gene expression pattern, and connectivity—and if every individual organism belonging to the same species has one and only one neuron with the same set of properties. In vertebrate nervous systems very few neurons are "identified" in this sense—in humans, there are believed to be none—but in simpler nervous systems, some or all neurons may be thus unique. In the roundworm "C. elegans", whose nervous system is the most thoroughly described of any animal's, every neuron in the body is uniquely identifiable, with the same location and the same connections in every individual worm. One notable consequence of this fact is that the form of the "C. elegans" nervous system is completely specified by the genome, with no experience-dependent plasticity.
The brains of many molluscs and insects also contain substantial numbers of identified neurons. In vertebrates, the best known identified neurons are the gigantic Mauthner cells of fish. Every fish has two Mauthner cells, located in the bottom part of the brainstem, one on the left side and one on the right. Each Mauthner cell has an axon that crosses over, innervating neurons at the same brain level and then travelling down through the spinal cord, making numerous connections as it goes. The synapses generated by a Mauthner cell are so powerful that a single action potential gives rise to a major behavioral response: within milliseconds the fish curves its body into a C-shape, then straightens, thereby propelling itself rapidly forward. Functionally this is a fast escape response, triggered most easily by a strong sound wave or pressure wave impinging on the lateral line organ of the fish. Mauthner cells are not the only identified neurons in fish—there are about 20 more types, including pairs of "Mauthner cell analogs" in each spinal segmental nucleus. Although a Mauthner cell is capable of bringing about an escape response individually, in the context of ordinary behavior other types of cells usually contribute to shaping the amplitude and direction of the response.
Mauthner cells have been described as command neurons. A command neuron is a special type of identified neuron, defined as a neuron that is capable of driving a specific behavior individually. Such neurons appear most commonly in the fast escape systems of various species—the squid giant axon and squid giant synapse, used for pioneering experiments in neurophysiology because of their enormous size, both participate in the fast escape circuit of the squid. The concept of a command neuron has, however, become controversial, because of studies showing that some neurons that initially appeared to fit the description were really only capable of evoking a response in a limited set of circumstances.
Function.
At the most basic level, the function of the nervous system is to send signals from one cell to others, or from one part of the body to others. There are multiple ways that a cell can send signals to other cells. One is by releasing chemicals called hormones into the internal circulation, so that they can diffuse to distant sites. In contrast to this "broadcast" mode of signaling, the nervous system provides "point-to-point" signals—neurons project their axons to specific target areas and make synaptic connections with specific target cells. Thus, neural signaling is capable of a much higher level of specificity than hormonal signaling. It is also much faster: the fastest nerve signals travel at speeds that exceed 100 meters per second.
At a more integrative level, the primary function of the nervous system is to control the body. It does this by extracting information from the environment using sensory receptors, sending signals that encode this information into the central nervous system, processing the information to determine an appropriate response, and sending output signals to muscles or glands to activate the response. The evolution of a complex nervous system has made it possible for various animal species to have advanced perception abilities such as vision, complex social interactions, rapid coordination of organ systems, and integrated processing of concurrent signals. In humans, the sophistication of the nervous system makes it possible to have language, abstract representation of concepts, transmission of culture, and many other features of human society that would not exist without the human brain.
Neurons and synapses.
Most neurons send signals via their axons, although some types are capable of dendrite-to-dendrite communication. (In fact, the types of neurons called amacrine cells have no axons, and communicate only via their dendrites.) Neural signals propagate along an axon in the form of electrochemical waves called action potentials, which produce cell-to-cell signals at points where axon terminals make synaptic contact with other cells.
Synapses may be electrical or chemical. Electrical synapses make direct electrical connections between neurons, but chemical synapses are much more common, and much more diverse in function. At a chemical synapse, the cell that sends signals is called presynaptic, and the cell that receives signals is called postsynaptic. Both the presynaptic and postsynaptic areas are full of molecular machinery that carries out the signalling process. The presynaptic area contains large numbers of tiny spherical vessels called synaptic vesicles, packed with neurotransmitter chemicals. When the presynaptic terminal is electrically stimulated, an array of molecules embedded in the membrane are activated, and cause the contents of the vesicles to be released into the narrow space between the presynaptic and postsynaptic membranes, called the synaptic cleft. The neurotransmitter then binds to receptors embedded in the postsynaptic membrane, causing them to enter an activated state. Depending on the type of receptor, the resulting effect on the postsynaptic cell may be excitatory, inhibitory, or modulatory in more complex ways. For example, release of the neurotransmitter acetylcholine at a synaptic contact between a motor neuron and a muscle cell induces rapid contraction of the muscle cell. The entire synaptic transmission process takes only a fraction of a millisecond, although the effects on the postsynaptic cell may last much longer (even indefinitely, in cases where the synaptic signal leads to the formation of a memory trace).
There are literally hundreds of different types of synapses. In fact, there are over a hundred known neurotransmitters, and many of them have multiple types of receptors. Many synapses use more than one neurotransmitter—a common arrangement is for a synapse to use one fast-acting small-molecule neurotransmitter such as glutamate or GABA, along with one or more peptide neurotransmitters that play slower-acting modulatory roles. Molecular neuroscientists generally divide receptors into two broad groups: chemically gated ion channels and second messenger systems. When a chemically gated ion channel is activated, it forms a passage that allow specific types of ion to flow across the membrane. Depending on the type of ion, the effect on the target cell may be excitatory or inhibitory. When a second messenger system is activated, it starts a cascade of molecular interactions inside the target cell, which may ultimately produce a wide variety of complex effects, such as increasing or decreasing the sensitivity of the cell to stimuli, or even altering gene transcription.
According to a rule called Dale's principle, which has only a few known exceptions, a neuron releases the same neurotransmitters at all of its synapses. This does not mean, though, that a neuron exerts the same effect on all of its targets, because the effect of a synapse depends not on the neurotransmitter, but on the receptors that it activates. Because different targets can (and frequently do) use different types of receptors, it is possible for a neuron to have excitatory effects on one set of target cells, inhibitory effects on others, and complex modulatory effects on others still. Nevertheless, it happens that the two most widely used neurotransmitters, glutamate and GABA, each have largely consistent effects. Glutamate has several widely occurring types of receptors, but all of them are excitatory or modulatory. Similarly, GABA has several widely occurring receptor types, but all of them are inhibitory. Because of this consistency, glutamatergic cells are frequently referred to as "excitatory neurons", and GABAergic cells as "inhibitory neurons". Strictly speaking this is an abuse of terminology—it is the receptors that are excitatory and inhibitory, not the neurons—but it is commonly seen even in scholarly publications.
One very important subset of synapses are capable of forming memory traces by means of long-lasting activity-dependent changes in synaptic strength. The best-known form of neural memory is a process called long-term potentiation (abbreviated LTP), which operates at synapses that use the neurotransmitter glutamate acting on a special type of receptor known as the NMDA receptor. The NMDA receptor has an "associative" property: if the two cells involved in the synapse are both activated at approximately the same time, a channel opens that permits calcium to flow into the target cell. The calcium entry initiates a second messenger cascade that ultimately leads to an increase in the number of glutamate receptors in the target cell, thereby increasing the effective strength of the synapse. This change in strength can last for weeks or longer. Since the discovery of LTP in 1973, many other types of synaptic memory traces have been found, involving increases or decreases in synaptic strength that are induced by varying conditions, and last for variable periods of time. The reward system, that reinforces desired behaviour for example, depends on a variant form of LTP that is conditioned on an extra input coming from a reward-signalling pathway that uses dopamine as neurotransmitter. All these forms of synaptic modifiability, taken collectively, give rise to neural plasticity, that is, to a capability for the nervous system to adapt itself to variations in the environment.
Neural circuits and systems.
The basic neuronal function of sending signals to other cells includes a capability for neurons to exchange signals with each other. Networks formed by interconnected groups of neurons are capable of a wide variety of functions, including feature detection, pattern generation and timing,and there are seen to be countless types of information processing possible. Warren McCulloch and Walter Pitts showed in 1943 that even artificial neural networks formed from a greatly simplified mathematical abstraction of a neuron are capable of universal computation. 
Historically, for many years the predominant view of the function of the nervous system was as a stimulus-response associator. In this conception, neural processing begins with stimuli that activate sensory neurons, producing signals that propagate through chains of connections in the spinal cord and brain, giving rise eventually to activation of motor neurons and thereby to muscle contraction, i.e., to overt responses. Descartes believed that all of the behaviors of animals, and most of the behaviors of humans, could be explained in terms of stimulus-response circuits, although he also believed that higher cognitive functions such as language were not capable of being explained mechanistically. Charles Sherrington, in his influential 1906 book "The Integrative Action of the Nervous System", developed the concept of stimulus-response mechanisms in much more detail, and Behaviorism, the school of thought that dominated Psychology through the middle of the 20th century, attempted to explain every aspect of human behavior in stimulus-response terms.
However, experimental studies of electrophysiology, beginning in the early 20th century and reaching high productivity by the 1940s, showed that the nervous system contains many mechanisms for generating patterns of activity intrinsically, without requiring an external stimulus. Neurons were found to be capable of producing regular sequences of action potentials, or sequences of bursts, even in complete isolation. When intrinsically active neurons are connected to each other in complex circuits, the possibilities for generating intricate temporal patterns become far more extensive. A modern conception views the function of the nervous system partly in terms of stimulus-response chains, and partly in terms of intrinsically generated activity patterns—both types of activity interact with each other to generate the full repertoire of behavior.
Reflexes and other stimulus-response circuits.
The simplest type of neural circuit is a reflex arc, which begins with a sensory input and ends with a motor output, passing through a sequence of neurons connected in series. This can be shown in the "withdrawal reflex" causing a hand to jerk back after a hot stove is touched. The circuit begins with sensory receptors in the skin that are activated by harmful levels of heat: a special type of molecular structure embedded in the membrane causes heat to change the electrical field across the membrane. If the change in electrical potential is large enough to pass the given threshold, it evokes an action potential, which is transmitted along the axon of the receptor cell, into the spinal cord. There the axon makes excitatory synaptic contacts with other cells, some of which project (send axonal output) to the same region of the spinal cord, others projecting into the brain. One target is a set of spinal interneurons that project to motor neurons controlling the arm muscles. The interneurons excite the motor neurons, and if the excitation is strong enough, some of the motor neurons generate action potentials, which travel down their axons to the point where they make excitatory synaptic contacts with muscle cells. The excitatory signals induce contraction of the muscle cells, which causes the joint angles in the arm to change, pulling the arm away.
In reality, this straightforward schema is subject to numerous complications. Although for the simplest reflexes there are short neural paths from sensory neuron to motor neuron, there are also other nearby neurons that participate in the circuit and modulate the response. Furthermore, there are projections from the brain to the spinal cord that are capable of enhancing or inhibiting the reflex.
Although the simplest reflexes may be mediated by circuits lying entirely within the spinal cord, more complex responses rely on signal processing in the brain. For example, when an object in the periphery of the visual field moves, and a person looks toward it many stages of signal processing are initiated. The initial sensory response, in the retina of the eye, and the final motor response, in the oculomotor nuclei of the brain stem, are not all that different from those in a simple reflex, but the intermediate stages are completely different. Instead of a one or two step chain of processing, the visual signals pass through perhaps a dozen stages of integration, involving the thalamus, cerebral cortex, basal ganglia, superior colliculus, cerebellum, and several brainstem nuclei. These areas perform signal-processing functions that include feature detection, perceptual analysis, memory recall, decision-making, and motor planning.
Feature detection is the ability to extract biologically relevant information from combinations of sensory signals. In the visual system, for example, sensory receptors in the retina of the eye are only individually capable of detecting "points of light" in the outside world. Second-level visual neurons receive input from groups of primary receptors, higher-level neurons receive input from groups of second-level neurons, and so on, forming a hierarchy of processing stages. At each stage, important information is extracted from the signal ensemble and unimportant information is discarded. By the end of the process, input signals representing "points of light" have been transformed into a neural representation of objects in the surrounding world and their properties. The most sophisticated sensory processing occurs inside the brain, but complex feature extraction also takes place in the spinal cord and in peripheral sensory organs such as the retina.
Intrinsic pattern generation.
Although stimulus-response mechanisms are the easiest to understand, the nervous system is also capable of controlling the body in ways that do not require an external stimulus, by means of internally generated rhythms of activity. Because of the variety of voltage-sensitive ion channels that can be embedded in the membrane of a neuron, many types of neurons are capable, even in isolation, of generating rhythmic sequences of action potentials, or rhythmic alternations between high-rate bursting and quiescence. When neurons that are intrinsically rhythmic are connected to each other by excitatory or inhibitory synapses, the resulting networks are capable of a wide variety of dynamical behaviors, including attractor dynamics, periodicity, and even chaos. A network of neurons that uses its internal structure to generate temporally structured output, without requiring a corresponding temporally structured stimulus, is called a central pattern generator.
Internal pattern generation operates on a wide range of time scales, from milliseconds to hours or longer. One of the most important types of temporal pattern is circadian rhythmicity—that is, rhythmicity with a period of approximately 24 hours. All animals that have been studied show circadian fluctuations in neural activity, which control circadian alternations in behavior such as the sleep-wake cycle. Experimental studies dating from the 1990s have shown that circadian rhythms are generated by a "genetic clock" consisting of a special set of genes whose expression level rises and falls over the course of the day. Animals as diverse as insects and vertebrates share a similar genetic clock system. The circadian clock is influenced by light but continues to operate even when light levels are held constant and no other external time-of-day cues are available. The clock genes are expressed in many parts of the nervous system as well as many peripheral organs, but in mammals all of these "tissue clocks" are kept in synchrony by signals that emanate from a master timekeeper in a tiny part of the brain called the suprachiasmatic nucleus.
Mirror neurons.
A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another. Thus, the neuron "mirrors" the behavior of the other, as though the observer were itself acting. Such neurons have been directly observed in primate species. Birds have been shown to have imitative resonance behaviors and neurological evidence suggests the presence of some form of mirroring system. In humans, brain activity consistent with that of mirror neurons has been found in the premotor cortex, the supplementary motor area, the primary somatosensory cortex and the inferior parietal cortex. The function of the mirror system is a subject of much speculation. Many researchers in cognitive neuroscience and cognitive psychology consider that this system provides the physiological mechanism for the perception/action coupling (see the common coding theory). They argue that mirror neurons may be important for understanding the actions of other people, and for learning new skills by imitation. Some researchers also speculate that mirror systems may simulate observed actions, and thus contribute to theory of mind skills, while others relate mirror neurons to language abilities. However, to date, no widely accepted neural or computational models have been put forward to describe how mirror neuron activity supports cognitive functions such as imitation. There are neuroscientists who caution that the claims being made for the role of mirror neurons are not supported by adequate research.
Development.
In vertebrates, landmarks of embryonic neural development include the birth and differentiation of neurons from stem cell precursors, the migration of immature neurons from their birthplaces in the embryo to their final positions, outgrowth of axons from neurons and guidance of the motile growth cone through the embryo towards postsynaptic partners, the generation of synapses between these axons and their postsynaptic partners, and finally the lifelong changes in synapses which are thought to underlie learning and memory.
All bilaterian animals at an early stage of development form a gastrula, which is polarized, with one end called the animal pole and the other the vegetal pole. The gastrula has the shape of a disk with three layers of cells, an inner layer called the endoderm, which gives rise to the lining of most internal organs, a middle layer called the mesoderm, which gives rise to the bones and muscles, and an outer layer called the ectoderm, which gives rise to the skin and nervous system.
In vertebrates, the first sign of the nervous system is the appearance of a thin strip of cells along the center of the back, called the neural plate. The inner portion of the neural plate (along the midline) is destined to become the central nervous system (CNS), the outer portion the peripheral nervous system (PNS). As development proceeds, a fold called the neural groove appears along the midline. This fold deepens, and then closes up at the top. At this point the future CNS appears as a cylindrical structure called the neural tube, whereas the future PNS appears as two strips of tissue called the neural crest, running lengthwise above the neural tube. The sequence of stages from neural plate to neural tube and neural crest is known as neurulation.
In the early 20th century, a set of famous experiments by Hans Spemann and Hilde Mangold showed that the formation of nervous tissue is "induced" by signals from a group of mesodermal cells called the "organizer region". For decades, though, the nature of the induction process defeated every attempt to figure it out, until finally it was resolved by genetic approaches in the 1990s. Induction of neural tissue requires inhibition of the gene for a so-called bone morphogenetic protein, or BMP. Specifically the protein BMP4 appears to be involved. Two proteins called Noggin and Chordin, both secreted by the mesoderm, are capable of inhibiting BMP4 and thereby inducing ectoderm to turn into neural tissue. It appears that a similar molecular mechanism is involved for widely disparate types of animals, including arthropods as well as vertebrates. In some animals, however, another type of molecule called Fibroblast Growth Factor or FGF may also play an important role in induction.
Induction of neural tissues causes formation of neural precursor cells, called neuroblasts. In drosophila, neuroblasts divide asymmetrically, so that one product is a "ganglion mother cell" (GMC), and the other is a neuroblast. A GMC divides once, to give rise to either a pair of neurons or a pair of glial cells. In all, a neuroblast is capable of generating an indefinite number of neurons or glia.
As shown in a 2008 study, one factor common to all bilateral organisms (including humans) is a family of secreted signaling molecules called neurotrophins which regulate the growth and survival of neurons. Zhu et al. identified DNT1, the first neurotrophin found in flies. DNT1 shares structural similarity with all known neurotrophins and is a key factor in the fate of neurons in Drosophila. Because neurotrophins have now been identified in both vertebrate and invertebrates, this evidence suggests that neurotrophins were present in an ancestor common to bilateral organisms and may represent a common mechanism for nervous system formation.
Pathology.
The central nervous system is protected by major physical and chemical barriers. Physically, the brain and spinal cord are surrounded by tough meningeal membranes, and enclosed in the bones of the skull and spinal vertebrae, which combine to form a strong physical shield. Chemically, the brain and spinal cord are isolated by the so-called blood–brain barrier, which prevents most types of chemicals from moving from the bloodstream into the interior of the CNS. These protections make the CNS less susceptible in many ways than the PNS; the flip side, however, is that damage to the CNS tends to have more serious consequences.
Although nerves tend to lie deep under the skin except in a few places such as the ulnar nerve near the elbow joint, they are still relatively exposed to physical damage, which can cause pain, loss of sensation, or loss of muscle control. Damage to nerves can also be caused by swelling or bruises at places where a nerve passes through a tight bony channel, as happens in carpal tunnel syndrome. If a nerve is completely transected, it will often regenerate, but for long nerves this process may take months to complete. In addition to physical damage, peripheral neuropathy may be caused by many other medical problems, including genetic conditions, metabolic conditions such as diabetes, inflammatory conditions such as Guillain–Barré syndrome, vitamin deficiency, infectious diseases such as leprosy or shingles, or poisoning by toxins such as heavy metals. Many cases have no cause that can be identified, and are referred to as idiopathic. It is also possible for nerves to lose function temporarily, resulting in numbness as stiffness—common causes include mechanical pressure, a drop in temperature, or chemical interactions with local anesthetic drugs such as lidocaine.
Physical damage to the spinal cord may result in loss of sensation or movement. If an injury to the spine produces nothing worse than swelling, the symptoms may be transient, but if nerve fibers in the spine are actually destroyed, the loss of function is usually permanent. Experimental studies have shown that spinal nerve fibers attempt to regrow in the same way as nerve fibers, but in the spinal cord, tissue destruction usually produces scar tissue that cannot be penetrated by the regrowing nerves.

</doc>
<doc id="21946" url="http://en.wikipedia.org/wiki?curid=21946" title="Nutcracker">
Nutcracker

A nutcracker is a mechanical device for cracking nuts. The earliest use of the term "nutcracker" in English dates from 1481.
Functional.
Manufacturers produce modern nutcrackers—designed to crack nuts—usually somewhat resembling pliers, but with the pivot point at the end beyond the nut, rather than in the middle. The spring-jointed nutcracker was patented by Henry Quackenbush in 1913. These are also used for cracking the shells of crab and lobster to make the meat inside available for eating.
Nuts have long been a popular choice for desserts, particularly throughout Europe. The nutcrackers were placed on dining tables to serve as a fun and entertaining center of conversation while diners awaited their final course. At one time, nutcrackers were actually made of metals such as brass, and it wasn't until the 1800s in Germany that the popularity of wooden ones began to spread.
Parrots use their beaks as natural nutcrackers, in much the same way smaller birds crack seeds. In this case, the pivot point stands opposite the nut, at the jaw.
Some artists, among them the multi-instrumentalist Mike Oldfield, have used the sound nutcrackers make in music.
However, Nutcrackers were also used as a token of good luck and goodwill to protect people from evil spirits and danger. They were thought to bare their teeth at any evil spirits to scare them away, just like a guard dog might do. 
Decorative.
Nutcrackers in the form of wooden carvings of a soldier, knight, king, or other profession have existed since at least the 15th century. These nutcrackers portray a person with a large mouth which the operator opens by lifting a lever in the back of the figurine. Originally one could insert a nut in the big-toothed mouth, press down and thereby crack the nut. Modern nutcrackers in this style serve mostly for decoration, mainly at Christmas time, a season of which they have long been a traditional symbol. The ballet "The Nutcracker" derives its name from this festive holiday decoration. The original nutcrackers were first seen in Germany and were thought to have guardian-like properties because of their strong appearance.
The carving of nutcrackers—as well as of religious figures and of cribs—developed as a cottage industry in forested rural areas of Germany. The most famous nutcracker carvings come from Sonneberg in Thuringia (also a center of dollmaking) and from the Ore Mountains. Wood-carving usually provided the only income for the people living there. Today the travel industry supplements their income by bringing visitors to the remote areas.
Steinbach nutcrackers have become popular in the United States as well, and the recreated "Bavarian village" of Leavenworth, Washington, even features a nutcracker museum. Many other materials also serve to make decorated nutcrackers, such as porcelain, silver, and brass; the museum displays samples.
Carvings by famous names like Junghanel, Klaus Mertens, Karl, Olaf Kolbe, Petersen, Christian Ulbricht and especially the Steinbach nutcrackers have become collectors' items. 
The United States Postal Service (USPS) in October 2008 issued four stamps with nutcrackers for the first time. These featured custom-made nutcrackers made by Richmond, Virginia artist Glenn Crider.

</doc>
<doc id="21949" url="http://en.wikipedia.org/wiki?curid=21949" title="Nicolai Abildgaard">
Nicolai Abildgaard

Nicolai Abraham Abildgaard (September 11, 1743 – June 4, 1809) was a Danish neoclassical and royal history painter, sculptor, architect, and professor of painting, mythology, and anatomy at the New Royal Danish Academy of Art in Copenhagen, Denmark. Many of his works were in the royal Christiansborg Palace (some destroyed by fire 1794), Fredensborg Palace, and Levetzau Palace at Amalienborg.
Abildgaard had studied at the Academy from 1764 to 1767, then worked there as apprentice, and moved to Rome in 1772–1777, where he studied sculpture, architecture, decoration, frescoes (at Palazzo Farnese) and murals. He returned to the Academy in Copenhagen, promoted to professor in 1778, and elected as Academy Director during 1789–1791 and 1801–1809. He was also assigned as a royal artist/decorator during 1780 to 1805. Abildgaard was married twice, in 1781 and 1803.
Life.
Nicolai Abraham Abildgaard was born on September 11, 1743 in Copenhagen, Denmark, as the son of Søren Abildgaard, an antiquarian draughtsman of repute, and Anne Margrethe Bastholm.
Training as an artist.
He was trained by a painting master before he joined the New Royal Danish Academy of Art ("Det Kongelige Danske Kunstakademi") in Copenhagen, where he studied under the guidance of Johan Edvard Mandelberg and Johannes Wiedewelt. He won a series of medallions at the Academy for his brilliance from 1764 to 1767. The large gold medallion from the Academy won in 1767 included a travel stipend, which he waited five years to receive. He assisted Professor Mandelberg of the Academy as an apprentice around 1769 and for painting decorations for the royal palace at Fredensborg. These paintings are classical, influenced by French classical artists such as Claude Lorrain and Nicolas Poussin. Mandelberg had studied in Paris under François Boucher.
Student travels.
Although artists of that time used to travel to Paris for further studies, but he chose to travel to Rome where he stayed from 1772 to 1777. He took a side trip to Naples in 1776 with Jens Juel. His ambitions focused in the genre of history painting. While in Rome, he studied Annibale Carracci's frescoes at the Palazzo Farnese and the paintings of Rafael, Titian, and Michelangelo. In addition he studied various other artistic disciplines (sculpture, architecture, decoration, wall paintings) and developed his knowledge of mythology, antiquities, anatomy, and perspective.
In the company of Swedish sculptor Johan Tobias Sergel and painter Johann Heinrich Füssli, he began to move away from the classicism he had learned at the Academy. He developed an appreciation for the literature of Shakespeare, Homer, and Ossian (the legendary Gaelic poet). He worked with themes from Greek as well as Norse mythology, which placed him at the forefront of Nordic romanticism.
He left Rome in June 1777 with the hope of becoming professor at the Academy in Copenhagen. He stopped for a stay in Paris and arrived in Denmark in December of the same year.
An academic and artistic career.
Very soon after joining the academy he was honored with the designation of Professor in 1778. He worked as an academic painter of the neoclassical school. From 1777 to 1794, he was very productive as an artist in addition to his role at the school. He taught painting, mythology, and anatomy at the school. He produced not only monumental works, but also smaller pieces such as vignettes and illustrations. He designed Old Norse costumes. He illustrated the works of Socrates and Ossian. Additionally he did some sculpting, etching, and authoring. He was interested in all manners of mythological, biblical, and literary allusion.
He taught some famous painters, including Asmus Jacob Carstens, sculptor Bertel Thorvaldsen, and painters J. L. Lund and Christoffer Wilhelm Eckersberg; later both of them took over his position as professor at the Academy after his death. Eckersberg, referred to as the "Father of Danish painting," went on to lay the foundation for the period of art known as the Golden Age of Danish Painting, as professor at the same Academy.
Around 1780 as royal historical painter, Abildgaard was requested by the Danish government to paint large monumental pieces, a history of Denmark, to decorate the entirety of the Knights' Room ("Riddersal)" at Christiansborg Palace. It was a prestigious and lucrative assignment. The paintings combined not only historical depictions, but also allegorical and mythological elements that glorified and flattered the government. The door pieces depicted, in allegory, four historical periods in Europe's history. Abilgaard used pictorial allegory like ideograms, to communicate ideas and transmit messages through symbols to a refined public who was initiated into this form of symbology. Abildgaard's professor Johan Edvard Mandelberg supplied the decorations to the room.
Abildgaard married Anna Maria Oxholm on March 23, 1781.
He made a failed attempt to be elected to the post of Academy Director in 1787 and was unanimously elected to the post two years later, serving as director during the period 1789–1791. He had the reputation for being a tyrant and for taking as many of the academy's monumental assignments as possible for himself.
Abilgaard was also known as a religious freethinker and an advocate of political reform. In spite of his service to (and in his artwork the glorification of) the government, he was hardly a great supporter of the monarchy or of the state church. He supported the emancipation of the farmers and participated in the collection of monies for the Freedom Monument ("Frihedsstøtten") in 1792. He contributed a design for the monument, as well as for two of the reliefs at its base. He got caught into controversies at the end of the 18th century because of his controversial statements and satirical drawings. He was inspired by the French Revolution, and in 1789–1790 he tried to incorporate these revolutionary ideals into the Knights' Room at Christiansborg Palace. However, the King rejected his designs.
His showdowns with the establishment culminated in 1794, when his allegorical painting "Jupiter Weighs the Fate of Mankind" ("Jupiter vejer menneskenes skæbne") was exhibited at the Salon. He was politically isolated and cut out of the public debate by censors.
The fire at Christiansborg Palace, in February 1794, also had a dampening effect on his career, for seven of the ten monumental paintings of the grandiose project were destroyed in that accident. The project was stopped and so were his earnings.
However after that devastating fire accident, he started getting decorative assignments and also got the opportunity to practice as an architect. He decorated the Levetzau Palace (now known as Christian VIII's Palace) at Amalienborg (1794–1798), recently occupied home of King Christian VII of Denmark's half-brother Frederik. His protégé Bertel Thorvaldsen headed the sculptural efforts.
He also planned for rebuilding the Christiansborg Palace, but he could not get the assignment. 
At the start of the 19th century, his interest in painting was restored when he painted four scenes from Terence's comedy "Andria." This coincided with his second marriage in 1803 to Juliane Marie Ottesen, which was a very happy situation for the aging Abilgaard. He had two sons and a daughter from the marriage. He bought a lovely little place in the country for the family, "Spurveskjul" ("Sparrow Hideaway").
In 1804 he received a commission for a series of painting for the throne room in the new palace, but disagreements between the artist and the crown prince put a halt to this project. He continued, however, to provide the court with designs for furniture and room decorations.
He was once again selected to serve as the Academy's director from 1801 until his death in 1809, at Frederiksdal. Nicolai Abraham Abildgaard is buried in Copenhagen's Assistens Cemetery.
Works.
Though Nicolai Abildgaard won immense fame in his own generation and helped lead the way to the period of art known as the Golden Age of Danish Painting, his works are scarcely known outside of Denmark. He was a cold theorist, inspired not by nature but by art. His style was classical, though with a romantic trend. He had a keen sense of color. As a technical painter, he attained remarkable success, his tone being very harmonious and even, but the effect to a viewer's eye is rarely interesting. A portrait of him painted by Jens Juel was made into a medallion by his friend Johan Tobias Sergel. August Vilhelm Saabye sculpted a statue of him in 1868, based on contemporary portraits.

</doc>
<doc id="21950" url="http://en.wikipedia.org/wiki?curid=21950" title="Khyber Pakhtunkhwa">
Khyber Pakhtunkhwa

Khyber Pakhtunkhwa known as KPK (Pashto: خیبر پښتونخوا‎ ]; ]), formerly the North-West Frontier Province, is one of the four provinces of Pakistan. Located in the northwestern part of Pakistan, the province borders the Federally Administered Tribal Areas to the west and south, Gilgit–Baltistan to the north-east, Azad Kashmir to the east, while both Punjab and the Islamabad Capital Territory lie to the south-east of the province. The province of Balochistan is located to the south of Khyber Pakhtunkhwa. Khyber Pakhtunkhwa also shares an international border with Afghanistan, and is connected to the northern reaches of Afghanistan via the famous Khyber Pass - the same pass through which the armies of Alexander the Great marched on their military expedition through the Indian Subcontinent. Khyber Pakhtunkhwa is also the site of ancient kingdom of Gandhara and the ruins of both its capital, Purushapura (modern day Charsadda), and its most prominent center of learning in the Peshawar Valley, Takht-i-Bahi. It has been under the suzerainty of the Persians, Greeks, Mauryans, Kushans, Shahis, Ghaznavids, Mughals, Sikhs, and British Raj throughout its long history.
Khyber Pakhtunkhwa is the third largest province of Pakistan by the size of both population and economy. It comprises 10.5% of Pakistan's economy, and is home to 11.9% of Pakistan's total population, with the majority of the province's inhabitants being Pashtun. Khyber Pakhtunkhwa is Pakistan's smallest province by size, but is the most climactically diverse province - with barren deserts, lush forests, and high altitude tundras all located within its boundaries. Its largest city, and capital, is Peshawar, followed by Mardan.
Khyber Pakhtunkhwa has been the site of militancy and terrorism that started after the attacks of September 11, 2001, and intensified when the Pakistani Taliban began an attempt to seize power in Pakistan starting in 2004. It has also been the location of Pakistan's Zarb-e-Azb operation - a broad military campaign against militants located in the province, and neighboring F.A.T.A. By 2014, casualty rates in the country as a whole dropped by 40% as compared to 2011-2013, with even greater drops noted in Khyber Pakhtunkhwa, despite the province being the site of a large massacre of schoolchildren by terrorists in December 2014.
History.
Early history.
The ancient Aryan Migration is believed to have taken place at approximately 2000 B.C., when semi-nomadic peoples entered the Gangetic plains of India after having passed modern Khyber Pakhtunkhwa's Khyber Pass.
At around 516 BCE., Darius Hystaspes sent Scylax, a Greek seaman from Karyanda, to explore the course of the Indus river. Darius Hystaspes subsequently subdued the races dwelling west of the Indus and north of Kabul, and incorporated them into the Indo-Greek Gandharan civilization, which reached its zenith between the sixth and first centuries B.C.E., and which features prominently in the Hindu epic poem, the Mahabharatha. Gandhara was incorporated into the Persian Empire under the satrapy system of government. The "satrapy" of Gandhara is recorded to have sent troops for Xerxes' invasion of Greece in 480 BCE.
In the spring of 327 BCE Alexander the Great crossed the Indian Caucasus (Hindu Kush) and advanced to Nicaea, where Omphis, king of Taxila and other chiefs joined him. Alexander then dispatched part of his force through the valley of the Kabul River, while he himself advanced into modern Khyber Pakhtunkhwa's Bajaur and Swat regions with his troops. Having defeated the Aspasians, from whom he took 40,000 prisoners and 230,000 oxen, Alexander crossed the Gouraios (Panjkora River) and entered into the territory of the Assakenoi - also in modern day Khyber Pakhtunkhwa. Alexander then made Embolima (thought to be the region of Amb in Khyber Pakhtunkhwa) his base. The ancient region of Peukelaotis (modern Hashtnagar, 17 mi north-west of Peshawar) submitted to the Greek invasion, leading to Nicanor, a Macedonian, being appointed satrap of the country west of the Indus, which includes the modern Khyber Pakhtunkhwa province.
After Alexander's death in 323 BCE Porus obtained possession of the region, but was murdered by Eudemus in 317 BC. Eudemus then left the region, and with his departure Macedonian power collapsed. Sandrocottus (Chandragupta), the founder of the Mauryan dynasty, then declared himself master of the province. His grandson, Ashoka, made Buddhism the dominant religion in ancient Gandhara.
After Ashoka's death the Mauryan empire collapse, just as in the west the Seleucid power was rising. The Greek princes of neighboring Bactria (in modern Afghanistan) took advantage of the power vacuum to declare their independence. The Bactrian kingdoms were then attacked from the west by the Parthians and from the north (about 139 BCE) by the Sakas, a Central Asian tribe. Local Greek rulers still exercised a feeble and precarious power along the borderland, but the last vestige of Greek dominion was extinguished by the arrival of the Yueh-chi.
The Yueh-Chi were a race of nomads that were themselves forced southwards out of Central Asia by the nomadic Xiongnu people. The Kushan clan of the Yuek Chi seized vast swathes of territory under the rule of Kujula Kadphises. His successors, Vima Takto and Vima Kadphises, conquered the north-western portion of the Indian subcontinent. Vima Kadphises was then succeeded by his son, the legendary king Kanishka, who himself was succeeded by Huvishka, and Vasudeva I. Under the reign of Vasudeva, who abandoned Buddhism in favor of Hinduism, the dominions of the Kushan empire shrank to an area roughly approximating the boundaries of modern Khyber Pakhtunkhwa.
Common Era.
During the early 1st millennium C.E., prior to Muslim conquests, the Khyber Pakhtunkhwa region was ruled by the Shahi dynasty. The early Shahi kings were Buddhist, like their Kushan predecessors prior to the reign of Vasudeva. The later Shahi kings of Kabul and Gandhara were Hindu, and had strong ties to ruling dynasties in neighboring regions of modern Kashmir and Punjab. The Hindu Shahis are believed to have been a ruling elite of a predominantly Buddhist, Hindu and Shamanistic population and were thus patrons of numerous faiths. Various artefacts and coins from their rule have been found that show evidence of their multicultural domain. By the time the Chinese monk Xuanzang visited the region early in the 7th century, the region was ruled by affiliates of the Shahi kings, but was no longer under direct rule of the Shahis, whose efforts were focused on regions to the east of modern Khyber Pakhtunkhwa. Shahi suzerainty continued in the region until 870 C.E. when local noblemen began to carve out their own fiefdoms largely independent of Shahi control, but nominally subservient to the Shahi dynasty. The remnants of Shahi rule were wiped out by Mahmud of Ghazni after the defeat of Jayapala at the Battle of Peshawar on November 27, 1001.
Muslim Rule.
When Ghazni arrived in the region, Hinduism, Zoroastrianism, Buddhism and Shamanism were the prominent religions. Local Pashtun and Dardic tribes converted to Islam, while retaining some local traditions (albeit altered by Islam) such as Pashtunwali or the Pashtun code of honor. Vestiges of shamanism are still to be found in the Chitral Valley, where the Kalash people still practice their pre-Islamic faith. Between 963 and 1187 C.E., the area of modern Khyber Pakhtunkhwa became part of larger Islamic empires, including the Ghaznavid Empire (975-1187), headed by Sultan Mahmud of Ghazni, and the empire of Muhammad Shahabuddin Ghauri (reigned 1202–1206). The Ghaznavid domain included large swathes of modern Afghanistan, Pakistan, and India. It was ruled from its capital at Lahore from 1151 to 1186.
Following the collapse of Ghaznavid rule, local Pashtuns of the Delhi Sultanate controlled the region. 
Several Turkic and Pashtun dynasties ruled from Delhi, having shifted their capital from Lahore to Delhi. Several Muslim dynasties ruled modern Khyber Pakhtunkhwa during the Delhi Sultanate period: the Mamluk dynasty (1206–90), the Khilji dynasty (1290–1320), the Tughlaq dynasty (1320–1413), the Sayyid dynasty (1414–51), and the Lodi dynasty (1451–1526).
Mughal suzerainty over the Khyber Pakhtunkhwa region was partially established after Babar, the founder of the Mughal Empire, invaded the region in 1505 C.E. via the Khyber Pass. He was forced to retreat westwards to Kabul, but returned to defeat Ibrahim Lodi, the last Lodhi king, in 1525 C.E. The local Pashtun tribesmen quickly pledged loyalty to the Mughal Empire, and largely remained under nominal Mughal rule until the arrival of the Sikhs in the 19th century.
Under the reign of Babar's son, Humayun, direct Mughal rule was briefly challenged with the rise of the Pashtun king, Sher Shah Suri, who began construction of the famous Grand Trunk Road - which links Kabul, Afghanistan with Chittagong, Bangladesh over 2000 miles to the east. Later, local rulers once again pledged loyalty to the Mughal emperor. Mughal rule over the region was again interrupted by the invasion of Persia's Nadir Shah, who in 1739 sacked the Mughal capital at Delhi with the help of Pashtuns from the modern Khyber Pakhtunkhwa region. The area fell subsequently under the rule of Afghans under the Durrani Empire. Muslim rule was interrupted by a brief invasion of the Hindu Marathas, who established a tenuous rule over the region from 1758 following the 1758 battle of Peshawar. Durrani rule was re-established eleven months later in 1759, and lasted until 1818 when the Sikhs invaded the region under the command of Maharaja Ranjit Singh.
Sikh Rule.
Sikh's continue to rule the region for several decades. Sikh rule came to an end after the British East India Company defeated the Sikhs during the Second Anglo-Sikh War in 1849.
British Raj.
British East India Company defeated the Sikhs during the Second Anglo-Sikh War in 1849, and incorporated small parts of the region into the Province of Punjab. While Peshawar was the site of a small mutiny against British during the Indian Rebellion of 1857, local Pashtun tribes throughout the region generally remained neutral and supportive of the British as they detasted the Sikh's, in contrast to the rest of British India which rose up in revolt against the British. However, British control of parts of the region was routinely challenged by Wazir tribesmen in Waziristan and other Pashtun tribes, who resisted any foreign occupation until the British granted Pakistan its independence. By the late 19th century, the official boundaries of Khyber Pakhtunkhwa region still had not been defined as the region was still claimed by the Kingdom of Afghanistan. It was only in 1893 The British demarcated the boundary with Afghanistan under a treaty agreed to by the Afghan king, Abdur Rahman Khan, following the Second Anglo-Afghan War. Several princely states within the boundaries of the region were allowed to maintain their autonomy under the terms of maintaining friendly ties with the British. As the British war effort during World War One demanded the reallocation of resources from British India to the European war fronts, some tribesmen from Afghanistan crossed the Durand Line in 1917 to attack British posts in an attempt to gain territory and weaken the legitimacy of the border. The validity of the Durand Line, however, was re-affirmed in 1919 by the Afghan government with the signing of the Treaty of Rawalpindi, which ended the Third Anglo-Afghan War - a war in which Waziri tribesmen allied themselves with the forces of Afghanistan's King Amanullah in their resistance to British rule. The Wazirs and other tribes, taking advantage of instability on the frontier, continued to resist British occupation until 1920 - even after Afghanistan had signed a peace treaty with the British.
British campaigns to subdue tribesmen along the Durand Line, as well as three Anglo-Afghan wars, made travel between Afghanistan and the densely populated heartlands of Khyber Pakhtunkwa increasingly difficult. The two regions were largely isolated from one another from the start of the Second Anglo-Afghan War in 1878 until the start of World War Two in 1939 when conflict along the Afghan frontier largely dissipated. Concurrently, the British continued their large public works projects in the region, and extended the Great Indian Peninsula Railway into the region, which connected the modern Khyber Pakhtunkhwa region to the plains of India to the east. Other projects, such as the Attock Bridge, Islamia College University, Khyber Railway, and establishment of cantonments in Peshawar, Kohat, Mardan, and Nowshera further cemented British rule in the region. In 1901, the British carved out the northwest portions of Punjab to create the Northwest Frontier Province (NWFP), which was renamed Khyber Pakhtunkhwa in 2010. 
Disassociation from Afghanistan, and increased connectivity to Punjab and the Gangetic Plains beyond Punjab, had a profound effect on Pashtun tribes living on the British side of the Durand Line. With few exception of the tribesmen living close to the border with Afghanistan, the vast majority of Pashtuns under British held areas increasingly viewed themselves as Indians, and found it easier to travel to Lahore and Delhi than to Kabul or Kandahar. Large numbers of Pashtuns also enlisted in the British Indian Army, and were stationed throughout British held territories in India and educated in the British Indian system, both of which helped to further re-orient the local population eastwards towards the heartlands of India. The in migration of Hindu and Sikh traders to the NWFP from India also strengthened cultural re-orientation towards British India. This dramatic shift in self-identification is epitomized the Khudai Khidmatgar movement of the popular Pashtun nationalist Abdul Ghaffar Khan, who non-violently campaigned for the independence of a united India, and not for joining Afghanistan. Further, no prominent leaders amongst the Pashtuns campaigned for unification with Afghanistan during the period preceding independence.
The NWFP was granted limited home-rule by the British in 1937. Beginning in 1940, support for the Pakistan Movement, which sought the establishment of an Indian Muslim homeland, increased in the NWFP. Immediately prior to Pakistani independence from Britain in 1947,the British held a referendum in the NWFP to allow voters to choose between joining Pakistan or India. The referendum was held on 2July 1947 while polling began on 6 July 1947 and the referendum results were made public on 20 July 1947. According to the official results, there were 572,798 registered voters out of which 289,244 (99.02%) votes were cast in favor of Pakistan while only 2874 (0.98%) were cast in favor of India. According to an estimate total turnout for referendum was only 15% less as compared to that of 1946 elections. Although large number of Khudai Khidmatgar supporters boycotted the referendum, and intimidation against Hindu and Sikh voters by supporters of the Pakistan Movement was also reported. Abdul Ghaffar Khan pledged allegiance to the new state of Pakistan in 1947, and thereafter abandoned his goal of a United India, in favor of supporting increased autonomy for the NWFP under Pakistani rule. He was subsequently arrested several times for his opposition to strong centralized rule.
As the region came under British control, as had been agreed to by the Afghan government following the British victory over Afghanistan in the Second Anglo-Afghan War and after the treaty ending Third Anglo-Afghan War, no option was available to cede the territory to the rule of the Afghan king even though Afghanistan continued to claim the entire region as it was part of the Durrani Empire prior the conquest of the region by the Sikhs in 1818. By 1947 Pashtun nationalists were advocateing for a united India, and no prominent voices advocated for a union with Afghanistan. Also in line with similar votes held throughout the British controlled territories in India, no option was accommodated for independence. However, all the princely states within the boundaries of the NWFP were allowed to maintain certain autonomy, but in 70's most of the princely states were merged completely into Pakistan.
After Pakistani Independence.
After the independence of Pakistan in 1947, Afghanistan was the sole member of the United Nations to vote against Pakistan’s accession to the UN because of Kabul’s claim to the Pashtun territories on the Pakistani side of the Durand Line. 
Afghanistan's Loya Jirga of 1949 declared the Durand Line invalid, which led to border tensions with Pakistan, and decades of mistrust between the two states. Afghan governments have also periodically refused to recognize Pakistan's inheritance of British treaties regarding the region. During the 1950s, Afghanistan supported the secessionist Pushtunistan Movement, although it failed to gain substantial support amongst the population of the North-West Frontier Province.
Afghanistan's refusal to recognize the Durrand Line and its subsequent support for the Pashtunistan Movement has been cited as the main cause of tensions between the two countries that have existed since Pakistan's independence. Ayub Khan eliminated the NWFP as a provincial unit of Pakistan in 1960, in favor of ruling the entire country as a single unit under a strong and centralized federal government. His successor, Yahya Khan, in 1969 announced intentions to eliminater the "one unit" system - NWFP was re-established a as a province of Pakistan in 1970. The previously autonomous "Princely States" of Amb, Swat, Dir and, Chitral were amalgamated to the new North-West Frontier Province as the Provincially Administered Tribal Areas - and lost a large degree of their autonomy, relegating their local noblemen to ceremonial roles.
During the Soviet occupation of Afghanistan (1979–1989), the NWFP served as a major supply base for the Afghan Mujahideen who fought the Soviet Union during the 1980s. As a result of the Soviet invasion, over five million Afghan refugees poured into Pakistan, mostly choosing to reside in the NWFP (as of 2007[ [update]], nearly 3 million remained). The province remained heavily influenced by events in Afghanistan thereafter. The 1989-1992 Civil war in Afghanistan following the withdrawal of Soviet forces led to the rise of the Afghan Taliban, which had emerged in the border region between Afghanistan, Balochistan, and FATA as a formidable political force.
In 2010 the province was renamed "Khyber Pakhtunkhwa." Protests arose among the local Hindkowan, Chitrali, Kohistani and Kalash populations over the name change, as they began to demand their own provinces. Seven people were killed and 100 injured in protests on 11April 2011.
The Awami National Party sought to rename the province "Pakhtunkhwa", which translates to "Land of Pashtuns" in the Pashto language. The name change was largely opposed by non-Pashtuns, and by political parties such as the Pakistan Muslim League-N, who draw much of their support from non-Pashtun regions of the province, and by the Islamist Muttahida Majlis-e-Amal coalition.
War and Militancy in the Province.
Khyber Pakhtunkhwa has been a site of militancy and terrorism that started after the attacks of September 11, 2001, and intensified when the Pakistani Taliban began an attempt to seize power in Pakistan starting in 2004. Armed conflict began in 2004, when tensions, rooted in the Pakistan Army's search for al-Qaeda fighters in Pakistan's mountainous Waziristan area (in the Federally Administered Tribal Areas), escalated into armed resistance. 
Fighting is ongoing between the Pakistani Army and armed militant groups such as the Tehrik-i-Taliban Pakistan (TTP), Jundallah, Lashkar-e-Islam (LeI), Tehreek-e-Nafaz-e-Shariat-e-Mohammadi (TNSM), al-Qaeda, and elements of organized crime have lead to the deaths of over 50,000 Pakistanis since the country joined the U.S-led War on Terror, with Khyber Pakhtunkhwa being the site of most of the conflict.
Khyber Pakhtunkhwa is also the main theater for Pakistan's Zarb-e-Azb operation - a broad military campaign against militants located in the province, and neighboring F.A.T.A. By 2014, casualty rates in the country as a whole dropped by 40% as compared to 2011-2013, with even greater drops noted in Khyber Pakhtunkhwa, despite the province being the site of a large massacre of schoolchildren by terrorists in December 2014.
Geography.
Khyber Pakhtunkhwa sits primarily on the Iranian plateau and comprises the junction where the slopes of the Hindu Kush mountains on the Eurasian plate give way to the Indus-watered hills approaching South Asia. This situation has led to seismic activity in the past. The famous Khyber Pass links the province to Afghanistan, while the Kohalla Bridge in Circle Bakote Abbottabad is a major crossing point over the Jhelum River in the east.
Geographically the province could be divided into two zones: the northern one extending from the ranges of the Hindu Kush to the borders of Peshawar basin and the southern one extending from Peshawar to the Derajat basin.
The northern zone is cold and snowy in winters with heavy rainfall and pleasant summers with the exception of Peshawar basin, which is hot in summer and cold in winter. It has moderate rainfall. The southern zone is arid with hot summers and relatively cold winters and scanty rainfall.
The major rivers that criss-cross the province are the Kabul, Swat, Chitral, Kunar, Siran, Panjkora, Bara, Kurram, Dor, Haroo, Gomal and Zhob.
Its snow-capped peaks and lush green valleys of unusual beauty have enormous potential for tourism.
Climate.
The climate of Khyber Pakhtunkhwa varies immensely for a region of its size, encompassing most of the many climate types found in Pakistan. The province stretching southwards from the Baroghil Pass in the Hindu Kush covers almost six degrees of latitude; it is mainly a mountainous region. Dera Ismail Khan is one of the hottest places in South Asia while in the mountains to the north the weather is temperate in the summer and intensely cold in the winter. The air is generally very dry; consequently, the daily and annual range of temperature is quite large.
Rainfall also varies widely. Although large parts of Khyber Pakhtunkhwa are typically dry, the province also contains the wettest parts of Pakistan in its eastern fringe.
Chitral District.
Chitral District lies completely sheltered from the monsoon that controls the weather in eastern Pakistan, owing to its relatively westerly location and the shielding effect of the Nanga Parbat massif. In many ways Chitral District has more in common regarding climate with Central Asia than South Asia. The winters are generally cold even in the valleys, and heavy snow during the winter blocks passes and isolates the region. In the valleys, however, summers can be hotter than on the windward side of the mountains due to lower cloud cover: Chitral can reach 40 C frequently during this period. However, the humidity is extremely low during these hot spells and, as a result the summer climate is less torrid than in the rest of the Indian subcontinent.
Most precipitation falls as thunderstorms or snow during winter and spring, so that the climate at the lowest elevations is classed as Mediterranean ("Csa"), continental Mediterranean ("Dsa") or semi-arid ("BSk"). Summers are extremely dry in the north of Chitral district and receive only a little rain in the south around Drosh.
At elevations above 5000 m, as much as a third of the snow which feeds the large Karakoram and Hindukush glaciers comes from the monsoon since these elevations are too high to be shielded from its moisture.
Central Khyber Pakhtunkhwa.
On the southern flanks of Nanga Parbat and in Upper and Lower Dir Districts, rainfall is much heavier than further north because moist winds from the Arabian Sea are able to penetrate the region. When they collide with the mountain slopes, winter depressions provide heavy precipitation. The monsoon, although short, is generally powerful. As a result, the southern slopes of Khyber Pakhtunkhwa are the wettest part of Pakistan. Annual rainfall ranges from around 500 mm in the most sheltered areas to as much as 1750 mm in parts of Abbottabad and Mansehra Districts.
This region’s climate is classed at lower elevations as humid subtropical ("Cfa" in the west; "Cwa" in the east); whilst at higher elevations with a southerly aspect it becomes classed as humid continental ("Dfb"). However, accurate data for altitudes above 2000 m are practically nonexistent here, in Chitral, or in the south of the province.
The seasonality of rainfall in central Khyber Pakhtunkhwa shows very marked gradients from east to west. At Dir, March remains the wettest month due to frequent frontal cloud-bands, whereas in Hazara more than half the rainfall comes from the monsoon. This creates a unique situation characterized by a bimodal rainfall regime, which extends into the southern part of the province described below.
Since cold air from the Siberian High loses its chilling capacity upon crossing the vast Karakoram and Himalaya ranges, winters in central Khyber Pakhtunkhwa are somewhat milder than in Chitral. Snow remains very frequent at high altitudes but rarely lasts long on the ground in the major towns and agricultural valleys. Outside of winter, temperatures in central Khyber Pakhtunkhwa are not so hot as in Chitral. 
Significantly higher humidity when the monsoon is active means that heat discomfort can be greater. However, even during the most humid periods the high altitudes typically allow for some relief from the heat overnight.
Southern Khyber Pakhtunkhwa.
As one moves further away from the foothills of the Himalaya and Karakoram ranges, the climate changes from the humid subtropical climate of the foothills to the typically arid climate of Sindh, Balochistan and southern Punjab. As in central Pakhtunkhwa, the seasonality of precipitation shows a very sharp gradient from west to east, but the whole region very rarely receives significant monsoon rainfall. Even at high elevations annual rainfall is less than 400 mm and in some places as little as 200 mm.
Temperatures in southern Pakhtunkhwa are extremely hot: Dera Ismail Khan in the southernmost district of the province is known as one of the hottest places in the world with temperatures known to have reached 50 C. In the cooler months, nights can be cold and frosts remain frequent; snow is very rare, and daytime temperatures remain comfortably warm with abundant sunshine.
Demographics.
The province has an estimated population of about 21 million. The largest ethnic group is the Pashtun, who historically have been living in the areas for centuries. Around 1.5 million Afghan refugees also remain in the province, the majority of whom are Pashtuns followed by Tajiks, Hazaras, and other smaller groups. Despite having lived in the province for over two decades, they are registered as citizens of Afghanistan.
According to the 1998 census, the population of the province was approximately 17 million, of whom 52% are males and 48% are females. The density of population is 187 per km² and the intercensal change of population is of about 30%.
In most rural areas of the centre and south, Pashtun tribes can be found including the Gigyani (Their main land is Shabqadar Doaaba), Yusufzai, Bangash, Bhittani, Daavi, Khattak, Qazi khail also known as Qaziye Babar, Gandapur, Gharghasht, Marwat, Afridi, Shinwari, Orakzai, Mahsud, Mohmand, Wazir and Bannuchi as well as other tribes of Hazara division•, Swati, Kakar, Tareen, Jadoon, Tanoli, Gujar, Maliar, and Mashwani.
There are non-Pashtun tribes including Arain Maliar, Jat, Mughal, Turks, Karlal, Rajpoot, Dhund Abbasi, Syed, Awan, Kashmiri, Qureshi and Sarrara. The mountainous extreme north includes the Chitral and Kohistan districts that are home to diverse Dardic ethnic groups such as the Khowar, Kohistani, Shina, Torwali, Kalasha and Kalami.
However in the southernmost district such as Dera Ismail Khan live some of the Baloch tribe: Kori, Buzdar, Kanera, Leghari, Rind and some other sub tribes of Lashari tribe. These Baloch tribes speak Saraiki as their first language. In this southern district, most of its population speaks Saraiki.
Languages.
Other languages include, Kashmiri, Shina, Romani, Burushaski, Wakhi, Balti, Balochi, Brahui, Sindhi and English (official and used in tourism).
Only Urdu and English are found as written languages in the city. English, the official language of Pakistan, is mainly used for official and literary purposes.
Religion.
Most of the inhabitants of Khyber Pakhtunkhwa profess Islam, with a Sunni majority and significant minorities of Shias, Ismailis, and Ahmadis. Many of the Kalasha of Southern Chitral still retain their ancient Animist/Shamanist religion.
There are very small communities of Hindus and Sikhs.
Government and politics.
The Provincial Assembly of Khyber Pakhtunkhwa consists of a 124-member Assembly. Members serve five-year.
The Khyber Pakhtunkhwa executive branch] consists of the Governor of Khyber Pakhtunkhwa appointed by President of Pakistan (subject to Prime Minister advice), Chief Minister of Khyber Pakhtunkhwa elected by Provincial Assembly of Khyber Pakhtunkhwa and Cabinet of Khyber Pakhtunkhwa appointed by Governor of Khyber Pakhtunkhwa (subject to Chief Minister advice).
The High Court and lower courts, judges are appointed by Chief Justice of Pakistan with Supreme Judicial Council of Pakistan approval, interpret laws and overturn those they find unconstitutional.
Khyber Pakhtunkhwa consists of 26 districts, comprising 21 Settled Area Districts and 5 Provincially Administered Tribal Area (PATA) Districts. The administration of the PATA districts is vested in the President of Pakistan and the Governor of Khyber Pakhtunkhwa, by Articles 246 and 247 of the Constitution of Pakistan.
The 26 districts are:
Major cities.
Peshawar is the capital and largest city of Khyber Pakhtunkhwa. The city is the most populous and comprises more than one-eighth of the province's population.
Economy.
Khyber Pakhtunkhwa has the third largest provincial economy in Pakistan. Khyber Pakhtunkhwa's share of Pakistan's GDP has historically comprised 10.5%, although the province accounts for 11.9% of Pakistan's total population. The part of the economy that Khyber Pakhtunkhwa dominates is forestry, where its share has historically ranged from a low of 34.9% to a high of 81%, giving an average of 61.56%. Currently, Khyber Pakhtunkhwa accounts for 10% of Pakistan's GDP, 20% of Pakistan's mining output and, since 1972, it has seen its economy grow in size by 3.6 times. It has the second poorest economy after Balochistan.
After suffering for decades due to the fallout of the Soviet invasion of Afghanistan, today they are again being targeted for a different situation of terrorism.
Agriculture remains important and the main cash crops include wheat, maize, tobacco (in Swabi), rice, sugar beets, as well as fruits are grown in the province.
Some manufacturing and high tech investments in Peshawar has helped improve job prospects for many locals, while trade in the province involves nearly every product. The bazaars in the province are renowned throughout Pakistan. Unemployment has been reduced due to establishment of industrial zones.
Workshops throughout the province support the manufacture of small arms and weapons. The province accounts for at least 78% of the marble production in Pakistan.
Social issues.
The Awami National Party sought to rename the province "Pakhtunkhwa", which translates to "Land of Pakhtuns" in the Pashto language. This was opposed by some of the non-Pashtuns, and especially by parties such as the Pakistan Muslim League-N (PML-N) and Muttahida Majlis-e-Amal (MMA). The PML-N derives its support in the province from primarily non-Pashtun Hazara regions.
In 2010 the announcement that the province would have a new name led to a wave of protests in the Hazara region. On 15 April 2010 Pakistan's senate officially named the province "Khyber Pakhtunkhwa" with 80 senators in favor and 12 opposed. The MMA, who until the elections of 2008 had a majority in the Khyber Pakhtunkhwa government, had proposed "Afghania" as a compromise name.
After the 2008 general election, the Awami National Party formed a coalition provincial government with the Pakistan Peoples Party. The Awami National Party has its strongholds in the Pashtun areas of Pakistan, particularly in the Peshawar valley, while Karachi in Sindh has one of the largest Pashtun populations in the world — around 7 million by some estimates. In the 2008 election the ANP won two Sindh assembly seats in Karachi. The Awami National Party has been instrumental in fighting the Taliban. In the 2013 general election Pakistan Tehreek-e-Insaf won a majority in the provincial assembly and has now formed their government in coalition with Jamaat-e-Islami Pakistan.
Folk music and culture.
Hindko and Pashto folk music are popular in Pakhtunkhwa and has a rich tradition going back hundreds of years. The main instruments are the rubab, mangey and harmonium. Khowar folk music is popular in Chitral and northern Swat. The tunes of Khowar music are very different from those of Pashto and the main instrument is the Chitrali sitar. A form of band music composed of clarinets (surnai) and drums is popular in Chitral. It is played at polo matches and dances. The same form of band music is played in the neighbouring Northern Areas.
Education.
Abbottabad is the only city in Khyber Pakhtunkhwa with higher literacy rate in province and also in Pakistan.The trend towards higher education is rapidly increasing in the province and the Pakhtunkhwa is home to Pakistan's foremost engineering university (Ghulam Ishaq Khan Institute of Engineering Sciences and Technology), which is in Topi, a town in Swabi district. The University of Peshawar is also a notable institution of higher learning.
The "Frontier Post" is perhaps the province's best-known newspaper and addresses many of the issues facing the population.
Sources:
This is a chart of the education market of Pakhtunkhwa by the government in 1998. Also see
Sports.
Cricket is the main sport played in Pakhtunkhwa. It has created world-class sportsmen like Younus Khan and Umar Gul. Besides producing cricket players, Pakhtunkhwa has the honour of being the birthplace of many world-class squash players, including greats like Hashim Khan, Qamar Zaman, Jehangir Khan and Jan Sher Khan.

</doc>
<doc id="21952" url="http://en.wikipedia.org/wiki?curid=21952" title="Naiad (moon)">
Naiad (moon)

Naiad ( or ; Greek: Ναϊάδ-ες), also known as Neptune III, is the innermost satellite of Neptune, named after the Naiads of Greek legend.
History.
Naiad was discovered sometime before mid-September 1989 from the images taken by the "Voyager 2" probe. The last moon to be discovered during the flyby, it was designated S/1989 N 6. The discovery was announced on September 29, 1989, in the IAU Circular No. 4867, but the text only talks of "25 frames taken over 11 days", giving a discovery date of sometime before September 18. The name was given on 16 September 1991.
Physical characteristics.
Naiad is irregularly shaped and probably has not been modified by any internal geological processes since its formation. It is likely that it is a rubble pile re-accreted from fragments of Neptune's original satellites, which were smashed up by perturbations from Triton soon after that moon's capture into a very eccentric initial orbit.
Orbital characteristics.
Naiad orbits about 23,500 km above Neptune's cloud tops. Since this is below the synchronous orbit radius, its orbit is slowly decaying due to tidal deceleration and it may eventually impact Neptune's atmosphere, or break up into a planetary ring upon passing its Roche limit due to tidal stretching. Naiad orbits Neptune well within its fluid Roche limit, and its density is expected to be low enough that it may be very close to its actual Roche limit already.
Exploration.
Since the "Voyager 2" flyby, the Neptune system has been extensively studied from ground-based observatories and the Hubble Space Telescope as well. In 2002–03 the Keck telescope observed the system using adaptive optics and detected easily the largest four inner satellites. Thalassa was found with some image processing, but Naiad was not located. Hubble has the ability to detect all the known satellites and possible new satellites even dimmer than those found by "Voyager 2". On October 8, 2013 the SETI Institute announced that Naiad had been located in archived Hubble imagery from 2004. The suspicion that the loss of positioning was due to considerable errors in Naiad's ephemeris proved correct as Naiad was ultimately located 80 degrees from its expected position.

</doc>
<doc id="21953" url="http://en.wikipedia.org/wiki?curid=21953" title="Nilo-Saharan languages">
Nilo-Saharan languages

The Nilo-Saharan languages are a proposed family of African languages spoken by some 50-60 million Nilotic people, mainly in the upper parts of the Chari and Nile rivers, including historic Nubia, north of where the two tributaries of the Nile meet. The languages extend through 17 nations in the northern half of Africa: from Algeria to Benin in west; from Libya to the Democratic Republic of the Congo in the center; and from Egypt to Tanzania in the east.
Eight of its proposed constituent divisions (excluding Kunama, Kuliak, and Songhay) are found in the modern two nations of Sudan and Southern Sudan, through which the Nile River flows. As indicated by its hyphenated name, Nilo-Saharan is a family of the African interior, including the greater Nile basin and the central Sahara desert.
Joseph Greenberg named the group and argued it was a genetic family in his 1963 book "The Languages of Africa". It contains the languages not included in the Niger–Congo, Afroasiatic, or Khoisan families. It has not been demonstrated that the Nilo-Saharan languages constitute a valid genetic grouping, and linguists have generally seen the phylum as "Greenberg's wastebasket", into which he placed all the otherwise unaffiliated non-click languages of Africa. Its supporters accept that it is a challenging proposal to demonstrate, but contend that it looks more promising the more work is done.
Some of the constituent groups of Nilo-Saharan are estimated to predate the African neolithic. Thus, the unity of Eastern Sudanic is estimated to date to at least the 5th millennium BC.
Nilo-Saharan genetic unity would necessarily be much older still and date to the late Upper Paleolithic.
This larger classification system is not accepted by all linguists, however. "Glottolog" (2013), for example, a publication of the Max Planck Institute in Germany, does not recognize the unity of origin of all the languages on this page, offering a more conservative, variant classification into "Nilotic" and "Central Sudanic".
Characteristics.
The constituent families of Nilo-Saharan are quite diverse. One characteristic feature is a tripartite singulative–collective–plurative number system, which Blench (2010) believes is a result of a noun-classifier system in the protolanguage. The distribution of the families may reflect ancient water courses in a green Sahara, when the desert was more habitable than it is today.
Major languages.
Within the Nilo-Saharan languages are a number of languages with at least a million speakers (most data from SIL's "Ethnologue" 16 (2009)). In descending order: 
Some other important Nilo-Saharan languages under 1 million speakers:
The total for all speakers of Nilo-Saharan languages according to "Ethnologue" 16 is 38–39 million people. However, the data spans a range from ca. 1980 to 2005, with a weighted median at ca. 1990. Given population growth rates, the figure in 2010 might be half again higher, or about 60 million.
History of the proposal.
The Saharan family (which includes Kanuri, Kanembu, the Tebu languages, and Zaghawa) was recognized by Heinrich Barth in 1853, the Nilotic languages by Karl Richard Lepsius in 1880, the various constituent branches of Central Sudanic (but not the connection between them) by Friedrich Müller in 1889, and the Maban family by Maurice Gaudefroy-Demombynes in 1907. The first inklings of a wider family came in 1912, when Diedrich Westermann included three of the (still independent) Central Sudanic families within Nilotic in a proposal he called "Niloto-Sudanic"; this expanded Nilotic was in turn linked to Nubian, Kunama, and possibly Berta, essentially Greenberg's Macro-Sudanic (Chari–Nile) proposal of 1954. In 1920 G. W. Murray fleshed out the Eastern Sudanic languages when he grouped Nilotic, Nubian, Nera, Gaam, and Kunama. Carlo Conti Rossini made similar proposals in 1926, and in 1935 Westermann added Murle. In 1940 A. N. Tucker published evidence linking five of the six branches of Central Sudanic alongside his more explicit proposal for East Sudanic. In 1950 Greenberg retained Eastern Sudanic and Central Sudanic as separate families, but accepted Westermann's conclusions of four decades earlier in 1954 when he linked them together as "Macro-Sudanic" (later "Chari–Nile", from the Chari and Nile watersheds). Greenberg's later contribution came in 1963, when he tied Chari–Nile to Songhai, Saharan, Maban, Fur, and Koman-Gumuz and coined the current name "Nilo-Saharan" for the resulting family. Lionel Bender noted that Chari–Nile was a historical artifact of the discovery of the family, and did not reflect an exclusive relationship between these languages, and the group has been abandoned, with its constituents becoming primary branches of Nilo-Saharan—or, equivalently, Chari–Nile and Nilo-Saharan have merged, with the name "Nilo-Saharan" retained. When it was realized that the Kadu languages were not Niger–Congo, they were commonly assumed to therefore be Nilo-Saharan, but evidence for this has not been presented. 
Although progress has been made since Greenberg established the plausibility of the family, Nilo-Saharan has not actually been demonstrated. Koman and Gumuz remain poorly attested, and so are difficult to work with, while arguments continue over the inclusion of Songhai. Blench (2010) believes that the distribution of Nilo-Saharan reflects the waterways of the wet Sahara 12,000 years ago, and that the protolanguage had noun classifiers, which today are reflected in a diverse range of prefixes, suffixes, and number marking.
Internal relationships.
Dimmendaal (2008) notes that Greenberg (1963) based his conclusion on sound evidence, and that the proposal as a whole has become more convincing in the decades since. Mikkola (1999) reviewed Greenberg's evidence and found it convincing. Roger Blench notes morphological similarities in all putative branches, which leads him to believe that the family is likely to be valid. 
Koman and Gumuz, however, are poorly known, and have been difficult to evaluate until recently. Songhai is markedly divergent, probably due to massive influence from the Mande languages. Also problematic are the Kuliak languages, which are spoken by hunter-gatherers and appear to retain a non-Nilo-Saharan core; Blench believes they may have been similar to Hadza or Dahalo and shifted incompletely to Nilo-Saharan. 
Dimmendaal (who had originally supported their inclusion) believes the Kadu languages form a small family of their own. Anbessa Tefera and Peter Unseth consider the poorly attested Shabo language to be Nilo-Saharan, though unclassified within the family due to lack of data; Dimmendaal considers it to be a language isolate on current evidence. Proposals have sometimes been made to add Mande (usually included in Niger–Congo), largely due to its many noteworthy similarities with Songhay rather than with Nilo-Saharan as a whole.
The extinct Meroitic language of ancient Kush has been accepted by linguists such as Rille, Dimmendaal, and Blench as Nilo-Saharan, though others argue for an Afroasiatic affiliation. It is poorly attested. 
There is little doubt that the constituent families of Nilo-Saharan—of which only Eastern Sudanic and Central Sudanic show much internal diversity—are valid groups. However, there have been several conflicting classifications in grouping them together. Each of the proposed higher-order groups has been rejected by other researchers: Greenberg's Chari–Nile by Bender and Blench, Bender's Core Nilo-Saharan by Dimmendaal and Blench, and Ehret's Sahelian "etc." by everyone. What remains are eight (Dimmendaal) to twelve (Bender) constituent families of no consensus arrangement.
Greenberg 1963.
Joseph Greenberg, in "The Languages of Africa", set up the family with the following branches. The Chari–Nile core are the connections that had been suggested by previous researchers.
Gumuz was not recognized as distinct from neighboring Koman; it was separated out (forming "Komuz") by Bender (1989).
Bender 2000.
By 2000 Bender had abandoned the Chari–Nile and Komuz branches, added Kadu, and removed Kuliak from Eastern Sudanic. He stated that Shabo could not yet be adequately classified, but might prove to be Nilo-Saharan. 
Blench 2010.
With a better understanding of Nilo-Saharan classifiers, and the affixes or number marking they have developed into in various branches, Blench believes that all of the families postulated as Nilo-Saharan belong together. He proposes the following tentative internal classification, with Shabo closest to Koman and Gumuz, and Songhai closest to Saharan, a relationship which had not previously been suggested: 
? Mimi of Decorse
Ehret 1984 (1989, 2001).
In his non-peer reviewed 2001 reconstruction of Nilo-Saharan, circulated in manuscript form since 1984 and first published in 1989, historian Christopher Ehret classifies the families in a radically different fashion, moving Koman to the periphery, Songhay deep into the family next to Maban, and Berta into East Sudanic: 
Blench notes that Ehret failed to consider existing scholarship, such as reconstructions of Proto-Central and Proto-Eastern Sudanic, and provided no evidence for his classification. It has not been accepted by linguists.
External relations.
Proposals for the external relationships of Nilo-Saharan typically center on Niger–Congo: Gregersen (1972) grouped the two together as "Kongo–Saharan". However, Blench (2011) proposed that the similarities between Niger–Congo and Nilo-Saharan (specifically Atlantic–Congo and Central Sudanic) are due to contact, with the noun-class system of Niger–Congo developed from, or elaborated on the model of, the noun classifiers of Central Sudanic.

</doc>
<doc id="21957" url="http://en.wikipedia.org/wiki?curid=21957" title="Nuclear pore">
Nuclear pore

Nuclear pores are large protein complexes that cross the nuclear envelope, which is the double membrane surrounding the eukaryotic cell nucleus. There are about an average of 2000 nuclear pore complexes (NPCs), in the nuclear envelope of a vertebrate cell, but it varies depending on cell type and the stage in the life cycle. The proteins that make up the nuclear pore complex are known as nucleoporins. About half of the nucleoporins typically contain solenoid protein domains—either an alpha solenoid or a beta-propeller fold, or in some cases both as separate structural domains. Each NPC contains at least 456 individual protein molecules and is composed of 30 distinct proteins (nucleoporins). The other half show structural characteristics typical of "natively unfolded" or intrinsically disordered proteins, i.e. they are highly flexible proteins that lack ordered secondary structure. These disordered proteins are the "FG" nucleoporins, so called because their amino-acid sequence contains many phenylalanine—glycine repeats.
Nuclear pore complexes allow the transport of molecules across the nuclear envelope. This transport includes RNA and ribosomal proteins moving from nucleus to the cytoplasm and proteins (such as DNA polymerase and lamins), carbohydrates, signaling molecules and lipids moving into the nucleus. It is notable that the "nuclear pore complex" (NPC) can actively conduct 1000 translocations per complex per second. Although smaller molecules simply diffuse through the pores, larger molecules may be recognized by specific signal sequences and then be diffused with the help of nucleoporins into or out of the nucleus. This is not directly energy requiring, but depends on concentrations gradients associated with the RAN cycle. Each of the eight protein subunits surrounding the actual pore (the outer ring) projects a spoke-shaped protein over the pore channel. The center of the pore often appears to contain a plug-like structure. It is yet unknown whether this corresponds to an actual plug or is merely cargo caught in transit.
Size and complexity.
The entire nuclear pore complex has a diameter of about 120 nanometers in vertebrates. The diameter of the channel ranges from 5.2 nanometers in humans to 10.7 nm in the frog "Xenopus laevis", with a depth of roughly 45 nm. mRNA, which is single-stranded, has a thickness of about 0.5 to 1 nm. The molecular mass of the mammalian NPC is about 124 megadaltons (MDa) and it contains approximately 30 different protein components, each in multiple copies. In contrast, the yeast "Saccharomyces cerevisiae" is smaller, weighing only 66 MDa.
Transport through the nuclear pore complex.
Small particles (< ~40 kDa) are able to pass through the nuclear pore complex by passive diffusion. Larger particles are also able to pass through the large diameter of the pore but at almost negligible rates. Efficient passage through the complex requires several protein factors. Karyopherins, which may act as importins or exportins are part of the Importin-β super-family which all share a similar three-dimensional structure.
Three models have been suggested to explain the translocation mechanism:
Import of proteins.
Any cargo with a "nuclear localization signal" (NLS) exposed will be destined for quick and efficient transport through the pore. Several NLS sequences are known, generally containing a conserved phospholipids sequence with basic residues such as PKKKRKV. Any material with an NLS will be taken up by importins to the nucleus.
The classical scheme of NLS-protein importation begins with Importin-α first binding to the NLS sequence, and acts as a bridge for Importin-β to attach. The importinβ—importinα—cargo complex is then directed towards the nuclear pore and diffuses through it. Once the complex is in the nucleus, RanGTP binds to Importin-β and displaces it from the complex. Then the "cellular apoptosis susceptibility protein" (CAS), an exportin which in the nucleus is bound to RanGTP, displaces Importin-α from the cargo. The NLS-protein is thus free in the nucleoplasm. The Importinβ-RanGTP and Importinα-CAS-RanGTP complex diffuses back to the cytoplasm where GTPs are hydrolyzed to GDP leading to the release of Importinβ and Importinα which become available for a new NLS-protein import round.
Although cargo passes through the pore with the assistance of chaperone proteins, the translocation through the pore itself is not energy dependent. However, the whole import cycle needs the hydrolysis of 2 GTPs and is thus energy dependent and has to be considered as active transport. The import cycle is powered by the nucleo-cytoplasmic RanGTP gradient. This gradient arises from the exclusive nuclear localization of RanGEFs, proteins that exchange GDP to GTP on Ran molecules. Thus there is an elevated RanGTP concentration in the nucleus compared to the cytoplasm.
Export of proteins.
Some molecules or macromolecular complexes need to be exported from the nucleus to the cytoplasm, as do ribosome subunits and messenger RNAs. Thus there is an export mechanism similar to the import mechanism.
In the classical export scheme, proteins with a "nuclear export sequence" (NES) can bind in the nucleus to form a heterotrimeric complex with an exportin and RanGTP (for example the exportin CRM1). The complex can then diffuse to the cytoplasm where GTP is hydrolysed and the NES-protein is released. CRM1-RanGDP diffuses back to the nucleus where GDP is exchanged to GTP by RanGEFs. This process is also energy dependent as it consumes one GTP. Export with the exportin CRM1 can be inhibited by Leptomycin B.
Export of RNA.
There are different export pathways through the NPC for each RNA class that exists. RNA export is also signal mediated (NES); the NES is in RNA-binding proteins (except for tRNA which has no adapter). It is notable that all viral RNAs and cellular RNAs (tRNA, rRNA, U snRNA, microRNA) except mRNA are dependent on RanGTP. Conserved mRNA export factors are necessary for mRNA nuclear export. Export factors are Mex67/Tap (large subunit) and Mtr2/p15 (small subunit). In higher eukaryotes, mRNA export is thought to be dependent on splicing which in turn recruits a protein complex, TREX, to spliced messages. TREX functions as an adapter for TAP, which is a very poor RNA binding protein. However, there are alternative mRNA export pathways that do not rely on splicing for specialized messages such as histones. Recent work also suggest an interplay between splicing-dependent export and one of these alternative mRNA export pathways for secretory and mitochondrial transcripts.
Assembly of the NPC.
As the NPC controls access to the genome, it is essential that it exists in large amounts in areas of the cell cycle where plenty of transcription is necessary. For example, cycling mammalian and yeast cells double the amount of NPC in the nucleus between the G1 and G2 phase of the cell cycle, and oocytes accumulate large numbers of NPCs to prepare for the rapid mitosis that exists in the early stages of development. Interphase cells must also keep up a level of NPC generation to keep the levels of NPC in the cell constant as some may get damaged. Some cells can even increase the NPC numbers due to increased transcriptional demand.
Theories of assembly.
There are several theories as to how NPCs are assembled. As the immunodepletion of certain protein complexes, such as the Nup 107–160 complex, leads to the formation of poreless nuclei, it seems likely that the Nup complexes are involved in fusing the outer membrane of the nuclear envelope with the inner and not that the fusing of the membrane begins the formation of the pore. There are several ways that this could lead to the formation of the full NPC.
Disassembly.
During mitosis the NPC appears to disassemble in stages. Peripheral nucleoporins such as the Nup 153 Nup 98 and Nup 214 disassociate from the NPC. The rest, which can be considered a scaffold proteins remain stable, as cylindrical ring complexes within the nuclear envelope. This disassembly of the NPC peripheral groups is largely thought to be phosphate driven, as several of these nucleoporins are phosphorylated during the stages of mitosis. However, the enzyme involved in the phosphorlyation is unknown in vivo. In metazoans (which undergo open mitosis) the NE degrades quickly after the loss of the peripheral Nups. The reason for this may be due to the change in the NPC’s architecture. This change may make the NPC more permeable to enzymes involved in the degradation of the NE such as cytoplasmic tubulin, as well as allowing the entry of key mitotic regulator proteins.
Preservation of integrity.
It was shown, in fungi that undergo closed mitosis (where the nucleus does not degrade), that the change of the permeability barrier of the NE was due to changes within the NPC and is what allows the entry of mitotic regulators. In Aspergillus nidulans the NPC composition appears to be effected by the mitotic kinase NIMA, possibly by phosphorylating the nucleoporins Nup98 and Gle2/Rae1. This remodelling seems to allow the proteins complex cdc2/cyclinB enter the nucleus as well as many other proteins such as soluble tubulin. The NPC scaffold remains intact throughout the whole closed mitosis. This seems to preserve the integrity of the NE.

</doc>
<doc id="21958" url="http://en.wikipedia.org/wiki?curid=21958" title="Nucleolus">
Nucleolus

The nucleolus is the largest structure in the nucleus of eukaryotic cells where it primarily serves as the site of ribosome synthesis and assembly. Nucleoli also have other important functions like assembly of signal recognition particles and playing a role in the cell's response to stress. Nucleoli are made of proteins and RNA and form around specific chromosomal regions. Malfunction of nucleoli can be the cause of several human diseases.
History.
Until 1964 little was known about the function of the nucleolus, even though the nucleolus could be easily seen through microscopy and was recognized as part of the nucleus. During that year, a study of nucleoli by John Gurdon and Donald Brown in the African clawed frog "Xenopus laevis", generated increasing interest in the function and detailed structure of the nucleolus. They found that 25% of the frog eggs had no nucleolus and that such eggs were not capable of life. Half of the eggs had one nucleolus and 25% had two. They concluded that the nucleolus had a function necessary for life. In 1966 Max L. Birnstiel and Hugh Wallace showed via hybridization experiments that genes within nucleoli code for ribosomal RNA.
Structure.
Three major components of the nucleolus are recognized: the fibrillar center (FC), the dense fibrillar component (DFC), and granular component (GC). The DFC consists of newly transcribed rRNA bound to ribosomal proteins, while the GC contains RNA bound to ribosomal proteins that are being assembled into immature ribosomes.
However, it has been proposed that this particular organization is only observed in higher eukaryotes and that it evolved from a bipartite organization with the transition from anamniotes to amniotes. Reflecting the substantial increase in the DNA intergenic region, an original fibrillar component would have separated into the FC and the DFC.
Another structure identified within many nucleoli (particularly in plants) is a clear area in the center of the structure referred to as a nucleolar vacuole.
Nucleoli of various plant species have been shown to have very high concentrations of iron in contrast to human and animal cell nucleoli.
The nucleolus ultrastructure can be visualized through an electron microscope, while the organization and dynamics can be studied through fluorescent protein tagging and fluorescent recovery after photobleaching (FRAP). Antibodies against the PAF49 protein can also be used as a marker for the nucleolus in immunofluorescence experiments.
Function and ribosome assembly.
Nucleoli are formed around specific genetic loci called nucleolar organizing regions (NORs), first described by Barbara McClintock. Because of this non-random organization, the nucleolus is defined as a "genetically determined element." An NOR is composed of tandem repeats of rDNA, which can be found on several different chromosomes. The human genome, for example, contains more than 200 clustered copies of rDNA on five different chromosomes (13, 14, 15, 21, 22). In a typical eukaryote rDNA consists of a promoter, internal and external transcribed spacers (ITS/ETS), rRNA coding sequences (18S, 5.8S, 28S) and an intergenic spacer.
In ribosome biogenesis, two of the three eukaryotic RNA polymerases (pol I and III) are required, and these function in a coordinated manner. In an initial stage, the rRNA genes are transcribed as a single unit within the nucleolus by RNA polymerase I. In order for this transcription to occur, several pol I-associated factors and DNA-specific trans-acting factors are required. In yeast, the most important are: UAF (upstream activating factor), TBP (TATA-box binding protein), and CF (core factor), which bind promoter elements and form the preinitiation complex (PIC), which is in turn recognized by RNA pol. In humans, a similar PIC is assembled with SL1, the promoter selectivity factor (composed of TBP and TBP-associated factors, or TAFs), transcription initiation factors, and UBF (upstream binding factor). RNA polymerase I transcribes most rRNA transcripts (28S, 18S, and 5.8S) but the 5S rRNA subunit (component of the 60S ribosomal subunit) is transcribed by RNA polymerase III.
Transcription of rRNA yields a long precursor molecule (45S pre-rRNA) which still contains the ITS and ETS. Further processing is needed to generate the 18S RNA, 5.8S and 28S RNA molecules. In eukaryotes, the RNA-modifying enzymes are brought to their respective recognition sites by interaction with guide RNAs, which bind these specific sequences. These guide RNAs belong to the class of small nucleolar RNAs (snoRNAs) which are complexed with proteins and exist as small-nucleolar-ribonucleoproteins (snoRNPs). Once the rRNA subunits are processed, they are ready to be assembled into larger ribosomal subunits. However, an additional rRNA molecule, the 5S rRNA, is also necessary. In yeast, the 5S rDNA sequence is localized in the intergenic spacer and is transcribed in the nucleolus by RNA pol.
In higher eukaryotes and plants, the situation is more complex, for the 5S DNA sequence lies outside the NOR and is transcribed by RNA pol III in the nucleoplasm, after which it finds its way into the nucleolus to participate in the ribosome assembly. This assembly not only involves the rRNA, but ribosomal proteins as well. The genes encoding these r-proteins are transcribed by pol II in the nucleoplasm by a "conventional" pathway of protein synthesis (transcription, pre-mRNA processing, nuclear export of mature mRNA and translation on cytoplasmic ribosomes). The mature r-proteins are then "imported" back into the nucleus and finally the nucleolus. Association and maturation of rRNA and r-proteins result in the formation of the 40S (small) and 60S (large) subunits of the complete ribosome. These are exported through the nuclear pore complexes to the cytoplasm, where they remain free or become associated with the endoplasmic reticulum, forming rough endoplasmic reticulum (RER).
A continuous chain between the nucleoplasm and the inner parts of the nucleolus exists through a network of nucleolar channels. In this way, macromolecules with a molecular weight up to 2000 kDa are easily distributed throughout the nucleolus. 
Sequestration of proteins.
In addition to its role in ribosomal biogenesis, the nucleolus is known to capture and immobilize proteins, a process known as nucleolar detention. Proteins that are detained in the nucleolus are unable to diffuse and to interact with their binding partners. Targets of this post-translational regulatory mechanism include VHL, PML, MDM2, POLD1, RelA, HAND1 and hTERT, among many others. It is now known that long noncoding RNAs originating from intergenic regions of the nucleolus are responsible for this phenomenon.

</doc>
<doc id="21961" url="http://en.wikipedia.org/wiki?curid=21961" title="Nucleon">
Nucleon

In chemistry and physics, a nucleon is one of the particles that makes up the atomic nucleus. Each atomic nucleus consists of one or more nucleons, and each atom in turn consists of a cluster of nucleons surrounded by one or more electrons. There are two known kinds of nucleon: the neutron and the proton. The mass number of a given atomic isotope is identical to its number of nucleons. Thus the term nucleon number may be used in place of the more common terms mass number or atomic mass number.
Until the 1960's, nucleons were thought to be elementary particles, each of which would not then have been made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interactions or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term "strong interaction" referred to just internucleon interactions.)
Nucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, for example, whether or not a certain nuclide undergoes radioactive decay.
The proton and neutron are both baryons and both fermions. They are quite similar. One carries a non-zero net charge and the other carries a zero net charge; the proton's mass is only 0.1% less than the neutron's. Thus, they can be viewed as two states of the same nucleon. They together form the isospin doublet (). In isospin space, neutrons can be rotationally transformed into protons, and vice versa. These nucleons are acted upon equally by the strong interaction. This implies that strong interaction is invariant when doing rotation transformation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.:129–130
Overview.
Properties.
Protons and neutrons are most important and best known for constituting atomic nuclei, but they can also be found on their own, not part of a larger nucleus. A proton on its own is the nucleus of the hydrogen-1 atom (1H). A neutron on its own is unstable (see below), but they can be found in nuclear reactions (see neutron radiation) and are used in scientific analysis (see neutron scattering).
Both the proton and neutron are made of three quarks. The proton is made of two up quarks and one down quark, while the neutron is one up quark and two down quarks. The quarks are held together by the strong force. It is also said that the quarks are held together by gluons, but this is just a different way to say the same thing (gluons mediate the strong force).
An up quark has electric charge +2⁄3 e, and a down quark has charge −1⁄3 e, so the total electric charge of the proton and neutron are +e and 0, respectively. The word "neutron" comes from the fact that it is electrically "neutral".
The mass of the proton and neutron is quite similar: The proton is or , while the neutron is or . The neutron is roughly 0.1% heavier. The similarity in mass can be explained roughly by the slight difference in mass of up quark and down quark composing the nucleons. However, detailed explanation remains an unsolved problem in particle physics.:135–136
The spin of both protons and neutrons is 1⁄2. This means they are fermions not bosons, and therefore, like electrons, they are subject to the Pauli exclusion principle. This is a very important fact in nuclear physics: Protons and neutrons in an atomic nucleus cannot all be in the same quantum state, but instead they spread out into nuclear shells analogous to electron shells in chemistry. Another reason that the spin of the proton and neutron is important is because it is the source of nuclear spin in larger nuclei. Nuclear spin is best known for its crucial role in the NMR/MRI technique for chemistry and biochemistry analysis.
The magnetic moment of a proton, denoted μp, is , while the magnetic moment of a neutron is μn = . These parameters are also important in NMR/MRI.
Stability.
A neutron by itself is an unstable particle: It undergoes #redirect [[Template:Subatomic particle]] decay (a type of radioactive decay) by turning into a proton, electron, and electron antineutrino, with a half-life around ten minutes. (See the Neutron article for further discussion of neutron decay.) A proton by itself is thought to be stable, or at least its lifetime is too long to measure. (This is an important issue in particle physics, see Proton decay.)
Inside a nucleus, on the other hand, both protons and neutrons can be stable or unstable, depending on the nuclide. Inside some nuclides, a neutron can turn into a proton (plus other particles) as described above; inside other nuclides the reverse can happen, where a proton turns into a neutron (plus other particles) through #redirect [[Template:Subatomic particle]] decay or electron capture; and inside still other nuclides, both protons and neutrons are stable and do not change form.
Antinucleons.
Both of the nucleons have corresponding antiparticles: The antiproton and the antineutron. These antimatter particles have the same mass and opposite charge as the proton and neutron respectively, and they interact in the same way. (This is generally believed to be "exactly" true, due to CPT symmetry. If there is a difference, it is too small to measure in all experiments to date.) In particular, antinucleons can bind into an "antinucleus". So far, scientists have created antideuterium and antihelium-3 nuclei.
Tables of detailed properties.
Nucleons.
^a The masses of the proton and neutron are known with far greater precision in atomic mass units (u) than in MeV/c2, due to the relatively poorly known value of the elementary charge. The conversion factor used is 1 u = MeV/c2.
The masses of their antiparticles are assumed to be identical, and no experiments have refuted this to date. Current experiments show any percent difference between the masses of the proton and antiproton must be less than and the difference between the neutron and antineutron masses is on the order of MeV/c2.
† "The P11(939) nucleon represents the excited state of a normal proton or neutron, for example, within the nucleus of an atom. Such particles are usually stable within the nucleus, i.e. Lithium-6."
Quark model classification.
In the quark model with SU(2) flavour, the two nucleons are part of the ground state doublet. The proton has quark content of "uud", and the neutron, "udd". In SU(3) flavour, they are part of the ground state octet (8) of spin 1⁄2 baryons, known as the Eightfold way. The other members of this octet are the hyperons strange isotriplet #redirect [[Template:Subatomic particle]], #redirect [[Template:Subatomic particle]], #redirect [[Template:Subatomic particle]], the #redirect [[Template:Subatomic particle]] and the strange isodoublet #redirect [[Template:Subatomic particle]], #redirect [[Template:Subatomic particle]]. One can extend this multiplet in SU(4) flavour (with the inclusion of the charm quark) to the ground state 20-plet, or to SU(6) flavour (with the inclusion of the top and bottom quarks) to the ground state 56-plet.
The article on isospin provides an explicit expression for the nucleon wave functions in terms of the quark flavour eigenstates.
Models.
Although it is known that the nucleon is made from three quarks, as of 2006[ [update]], it is not known how to solve the equations of motion for quantum chromodynamics. Thus, the study of the low-energy properties of the nucleon are performed by means of models. The only first-principles approach available is to attempt to solve the equations of QCD numerically, using lattice QCD. This requires complicated algorithms and very powerful supercomputers. However, several analytic models also exist:
The Skyrmion models the nucleon as a topological soliton in a non-linear SU(2) pion field. The topological stability of the Skyrmion is interpreted as the conservation of baryon number, that is, the non-decay of the nucleon. The local topological winding number density is identified with the local baryon number density of the nucleon. With the pion isospin vector field oriented in the shape of a hedgehog space, the model is readily solvable, and is thus sometimes called the hedgehog model. The hedgehog model is able to predict low-energy parameters, such as the nucleon mass, radius and axial coupling constant, to approximately 30% of experimental values.
The MIT bag model confines three non-interacting quarks to a spherical cavity, with the boundary condition that the quark vector current vanish on the boundary. The non-interacting treatment of the quarks is justified by appealing to the idea of asymptotic freedom, whereas the hard boundary condition is justified by quark confinement. Mathematically, the model vaguely resembles that of a radar cavity, with solutions to the Dirac equation standing in for solutions to the Maxwell equations and the vanishing vector current boundary condition standing for the conducting metal walls of the radar cavity. If the radius of the bag is set to the radius of the nucleon, the bag model predicts a nucleon mass that is within 30% of the actual mass. 
Although the basic bag model does not provide a pion-mediated interaction, it describes excellently the nucleon-nucleon forces through the 6-quark bag s-channel mechanism using the P matrix. 
The chiral bag model merges the MIT bag model and the Skyrmion model. In this model, a hole is punched out of the middle of the Skyrmion, and replaced with a bag model. The boundary condition is provided by the requirement of continuity of the axial vector current across the bag boundary. Very curiously, the missing part of the topological winding number (the baryon number) of the hole punched into the Skyrmion is exactly made up by the non-zero vacuum expectation value (or spectral asymmetry) of the quark fields inside the bag. s of 2006[ [update]], this remarkable trade-off between topology and the spectrum of an operator does not have any grounding or explanation in the mathematical theory of Hilbert spaces and their relationship to geometry. Several other properties of the chiral bag are notable: it provides a better fit to the low energy nucleon properties, to within 5–10%, and these are almost completely independent of the chiral bag radius (as long as the radius is less than the nucleon radius). This independence of radius is referred to as the Cheshire Cat principle, after the fading to a smile of Lewis Carroll's Cheshire Cat. It is expected that a first-principles solution of the equations of QCD will demonstrate a similar duality of quark-pion descriptions.

</doc>
<doc id="21966" url="http://en.wikipedia.org/wiki?curid=21966" title="Nicolas Chauvin">
Nicolas Chauvin

Nicolas Chauvin (]) is a legendary, possibly apocryphal French soldier and patriot who is supposed to have served in the First Army of the French Republic and subsequently in "La Grande Armée" of Napoleon. His name is the eponym of "chauvinism", originally a term for excessive nationalistic fervor, but later used to refer to any form of bigotry or bias (e.g., male chauvinism).
According to the stories that developed about him, Chauvin was born in Rochefort, around 1780. He enlisted at age 18, and served honorably and well. He is said to have been wounded 17 times in his nation's service, resulting in his severe disfigurement and maiming. For his loyalty and dedication, Napoleon himself presented the soldier with a Sabre of Honor and a pension of 200 francs.
Chauvin's distinguished record of service and his love and devotion for Napoleon, which endured despite the price he willingly paid for them, is said to have earned him only ridicule and derision in Restoration France, when Bonapartism became increasingly unpopular. 
Historicity.
Historical research has not identified any biographical details of a real Nicolas Chauvin, leading to the claim that he may have been a wholly fictional figure. Researcher Gérard Puymège concludes that "Nicolas Chauvin did not exist, believing him to be a legend, which crystallized under the Restoration and July Monarchy, from the pen of songwriters, vaudeville and historians." He argues that the figure of Chauvin continues the long tradition of the mythological farmer-soldier or Miles Gloriosus from ancient times in Greek mythology.
Many writers and historians falsely attribute to Chauvin the exploits of other Bonapartists. It is claimed that he served in the Old Guard at Waterloo, which is certainly possible considering his age. When the Old Guard was surrounded and made its last stand at La Belle Alliance, he supposedly shouted in defiance to a call for their honorable surrender: "The Old Guard dies but does not surrender!", implying blind and unquestioned zealous devotion to one's country (or other group of reference). The apocryphal phrase was, in fact, attributed to the Old Guard's commander, Pierre Cambronne. However, Cambronne's actual reply was later asserted by other sources to be "Merde!" (in English, literally "Shit!").

</doc>
<doc id="21968" url="http://en.wikipedia.org/wiki?curid=21968" title="Nicotinamide">
Nicotinamide

Nicotinamide, (ni-kə-tē-nə-mīd) also known as niacinamide and nicotinic amide, is the amide of nicotinic acid (vitamin B3 / niacin). Nicotinamide is a water-soluble vitamin and is part of the vitamin B group. Nicotinic acid, also known as niacin, is converted to nicotinamide "in vivo", and, though the two are identical in their vitamin functions, nicotinamide does not have the same pharmacological and toxic effects of niacin, which occur incidental to niacin's conversion. Thus nicotinamide does not reduce cholesterol or cause flushing, although nicotinamide may be toxic to the liver at doses exceeding 3 g/day for adults. In cells, niacin is incorporated into nicotinamide adenine dinucleotide (NAD) and nicotinamide adenine dinucleotide phosphate (NADP), although the pathways for nicotinic acid amide and nicotinic acid are very similar. NAD+ and NADP+ are coenzymes in a wide variety of enzymatic oxidation-reduction reactions. Commercial production of niacin and niacinamide (several thousand tons annually) is by hydrolysis or aminolysis of 3-cyanopyridine (nicotinonitrile).
Use in medicine.
Skin conditions.
Nicotinamide has demonstrated anti-inflammatory actions that may be of benefit to patients with inflammatory skin conditions. These conditions include acne vulgaris, and the compound can suppress antigen-induced, lymphocytic transformation and inhibit 3',5'-cyclic-AMP phosphodiesterase. Nicotinamide has demonstrated the ability to block the inflammatory actions of iodides known to precipitate or exacerbate inflammatory acne.
NicAzel and Nicomide are the names of oral acne medications that include nicotinamide as their most predominant ingredient, based on this area of research. Nicotinamide is also found as part of a new adjunct supplement combination called, AzerizinTM. According to the makers of Azerizin, this adjunct is part of their prescription dietary supplement product, which they claim helps manage inflammatory skin conditions. Nicotinamide is also used topically as a 4% or 5% gel or cream - as effective as topical 1% clindamycin (8-week double-blind trial) performed at the New York University College of Medicine. Unlike topical Erythromycin or Clindamycin it does not precipitate bacterial resistance in treating inflammatory acne. Nicotinamide acne treatment is also available as Nicotinamide pads and cream.
Nicotinamide is also reported to be an effective skin whitener in topical application.
Anxiety.
Studies show that nicotinamide has anxiolytic (anti-anxiety) properties. It may work in a way similar to benzodiazepines.
Alzheimer's disease.
It is an activator of sirtuins (but inhibits at higher doses) and has been reported to restore cognition in Alzheimer's disease transgenic mice. A safety study of niacinamide for the treatment of Alzheimer's disease was begun in 2007[ [update]] at the University of California, Irvine.
Cancer.
Nicotinamide conflicts as a chemo- and radio-sensitizing agent/cancer-growth-promoter by enhancing tumor blood flow, thus reducing tumor hypoxia. Niacinamide also inhibits poly(ADP-ribose) polymerases (PARP-1), enzymes involved in the rejoining of DNA strand breaks induced by radiation or chemotherapy. PARP-1 appears to be an important target for triple negative breast cancers because the cells are sensitive to inhibition of PARP-1. Niacinamide is also used by some patients in combination with the popular but entirely unproven "intravenous vitamin C" treatment for cancer. It has also been seen to prevent immunosuppression caused by UVA and UVB radiation (so it could potentially be added to sunscreen).
Apoptosis.
Nicotinamide can prevent apoptosis (programmed cell death) in cells exposed to agents that induce oxidative stress. Thus nicotinamide prevents apoptosis in human cortical neuronal cells when oxidative stress is induced by tertiary butylhydroperoxide, and in Jurkat (human T-cell lymphoma cells) when oxidative stress is induced by sodium deoxycholate.
Other.
Nicotinamide reportedly increases the endurance of mice.
Food sources.
Nicotinamide occurs in trace amounts mainly in meat, fish, nuts, and mushrooms, as well as to a lesser extent in some vegetables.
Toxicity.
Nicotinamide lacks the vasodilator, gastrointestinal, hepatic, and hypolipidemic actions of nicotinic acid. As such, nicotinamide has not been shown to produce the flushing, itching, and burning sensations of the skin as is commonly seen when large doses of nicotinic acid are administered orally. High-dose nicotinamide should still, however, be considered as a drug with toxic potential at adult doses in excess of 3 g/day and unsupervised use should be discouraged. Overall, however, it rarely causes side effects, and is considered generally safe as a food additive, and as a component in cosmetics and medication.

</doc>
<doc id="21970" url="http://en.wikipedia.org/wiki?curid=21970" title="Virtual Boy">
Virtual Boy

The Virtual Boy (バーチャルボーイ, Bācharu Bōi) (Originally known as VR-32) is a 32-bit table-top 3D video game console developed and manufactured by Nintendo. It was marketed as the first "portable" video game console capable of displaying "true 3D graphics" out of the box.
It was released on July 21, 1995 in Japan and August 14, 1995 in North America at a price of around US$180. It proved to be a commercial failure and was not released in other regions. Its lukewarm reception was unaffected by continued price drops. Nintendo discontinued it the following year on March 2, 1996.
History.
Development.
"The New York Times" previewed the Virtual Boy on November 13, 1994. The console was officially announced via press release the next day, November 14. Nintendo promised that Virtual Boy would "totally immerse players into their own private universe." The system was formally unveiled the next day at the Shoshinkai (初心会) Show. Nintendo of America showed the Virtual Boy at the Consumer Electronics Show on 6 January 1995.
While Nintendo's Research & Development 3 division (R&D3) was focused on developing the Nintendo 64, the other two engineering units were free to experiment with new product ideas. The Virtual Boy was designed by Gunpei Yokoi, the general manager of Nintendo's R&D1, and the inventor of the Game & Watch and Game Boy handheld consoles. He saw the Virtual Boy as a unique technology that competitors would find difficult to emulate. Additionally, the console was intended to enhance Nintendo's reputation as an innovator.
Initial press releases and interviews about the system focused on its technological capabilities, avoiding discussion of the actual games that would be released. The company entered into an exclusive agreement with Massachusetts-based Reflection Technology, Inc. to license the technology for the Scanned Linear Array displays.
Problems emerged when Nintendo attempted to turn its vision into an affordable console, searching for low-cost hardware components. Yokoi opted for red LEDs because they were the cheapest; Nintendo said a color display would have been prohibitively expensive. Color was also said to have caused "jumpy images in tests." Even with cost-saving measures in place, Nintendo priced the Virtual Boy at a relatively high US$180. While slightly less expensive than a home console (and significantly less powerful), this was considerably more costly than the Game Boy handheld.
With seemingly more advanced graphics, the Virtual Boy was not intended to replace the Game Boy in Nintendo's product line, as use of the system requires a steady surface and completely blocks the player's peripheral vision. According to David Sheff's book "Game Over", Yokoi never actually intended for the console to be released in its ultimate form. However, Nintendo pushed the Virtual Boy to market so that it could focus development resources on the Nintendo 64. "Design News" described the Virtual Boy as the logical evolution of the View-Master 3-dimensional image viewer.
A number of technology demonstrations were used to show what the Virtual Boy was capable of. Driving Demo was one of the more advanced demos; its 30-second clip showed a first-person view of driving by road signs and palm trees. This demo was shown at E3 and CES in 1995. The start-up screen of the Virtual Boy Prototype was shown at Shoshinkai in 1994. The demo of what would have been a "Star Fox" game showed a "Star Fox"-like Arwing doing various spins and motions. Cinematic camera angles were a key element, as they were in "Star Fox 2". It was shown at E3 and CES in 1995.
Promotion.
Nintendo extensively advertised the Virtual Boy, and claimed to have spent US$25 million on early promotional activities. Advertising promoted the system as a paradigm shift from past consoles; some pieces used cavemen to indicate a historical evolution, while others utilized psychedelic imagery. Nintendo targeted an older audience with advertisements for the Virtual Boy, shifting away from the traditional focus on children it had employed in the past.
Nintendo portrayed the system as a type of virtual reality, as its name indicates; it was to be more than just another gaming console. Nintendo also focused on the technological aspects of the new console in its press releases, neglecting to detail specific games.
Confronted with the challenge of showing 3-dimensional gameplay on 2-dimensional advertisements, the company partnered with Blockbuster and NBC in a coordinated effort. A $5 million campaign promoted NBC's fall lineup alongside the Virtual Boy. American viewers were encouraged via television advertisements on NBC to rent the console for US$10 at a local Blockbuster. This made it affordable for a large number of gamers to try the system, and produced 750,000 rentals. Upon returning the unit, renters received a coupon for $10 off the purchase of a Virtual Boy from any store. 3,000 Blockbuster locations were included in the promotion, which included a sweepstakes with prizes including trips to see the taping of NBC shows. Despite its popularity, the rental system proved harmful to the Virtual Boy's long-term success, allowing gamers to see just how un-immersive the console was. Taken as a whole, the marketing campaign was commonly thought of as a failure.
Release.
The Virtual Boy was released on  21, 1995 (1995--) in Japan and on  14, 1995 (1995--) in North America with the launch titles "Mario's Tennis", "Red Alarm", "Teleroboxer", and "Galactic Pinball". It was not released in PAL markets. In North America, Nintendo shipped "Mario's Tennis" with every Virtual Boy sold, as a pack-in game. Nintendo had initially projected sales of 3 million consoles and 14 million games. The system arrived later than other 32-bit systems from Sony, Panasonic, and Sega, but at a lower price.
At the system's release, Nintendo of America projected hardware sales of 1.5 million units and software sales numbering 2.5 million by the end of the year. Nintendo had shipped 350,000 units of the Virtual Boy by December 1995, around three and a half months after its North American release. The system made number 5 on "GamePro"‍ '​s the "Top 10 Worst Selling Consoles of All Time" list in 2007.
The Virtual Boy did not live very long following its disappointing sales. The last official title to be released for the Virtual Boy was "3D Tetris", released on  22, 1996 (1996--). Nintendo announced additional titles for the system at the Electronic Entertainment Expo in 1996, but these games never saw the light of day. The Virtual Boy was discontinued in late 1995 in Japan and in early 1996 in North America. Nintendo killed the system without fanfare, avoiding an official press release.
Hardware.
The main processor is a 32-bit RISC chip, making the Virtual Boy Nintendo's first 32-bit system. The Virtual Boy system uses a pair of 1×224 linear arrays (one per eye) and rapidly scans the array across the eye's field of view using flat oscillating mirrors. These mirrors vibrate back and forth at a very high speed, thus the mechanical humming noise from inside the unit. Each Virtual Boy game cartridge has a yes/no option to automatically pause every 15–30 minutes so that the player may take a break before any injuries to the eyes. One speaker per ear provides the player with audio.
Display.
The Virtual Boy was the first video game console that was supposed to be capable of displaying "true 3D graphics" out of the box, in a form of virtual reality. Whereas most video games use monocular cues to achieve the illusion of three dimensions on a two-dimensional screen, the Virtual Boy creates an illusion of depth through the effect known as parallax. In a manner similar to using a head-mounted display, the user looks into an eyepiece made of neoprene on the front of the machine, and then an eyeglass-style projector allows viewing of the monochromatic (in this case, red) image. Nintendo claimed that a color display would have made the system too expensive and resulted in "jumpy" images, so the company opted for a monochrome display.
Control.
The Virtual Boy was meant to be used sitting down at a table, although Nintendo said it would release a harness for players to use while standing. One of the unique features of the controller is the extendable power supply that slides onto the back. It houses the six AA batteries required to power the system. This can be substituted with a wall adapter, though a "slide-on" attachment is required for the switchout. Once the slide-on adapter is installed, a power adapter can be attached to provide constant power.
The Virtual Boy, being a system with heavy emphasis on three-dimensional movement, needed a controller that could operate along a Z axis. The Virtual Boy's controller was an attempt to implement dual digital "D-pads" to control elements in the aforementioned 3D environment. The controller itself is shaped like an "M" (similar to a Nintendo 64 controller). One holds onto either side of the controller and the part that dips down in the middle contains the battery pack.
In more traditional 2-dimensional games, the two directional pads are interchangeable. For others with a more 3D environment, like "Red Alarm", "3D Tetris", or "Teleroboxer", each pad controls a different feature. The symmetry of the controller also allows left-handed gamers to reverse the controls (similar to the Atari Lynx).
Connectivity.
During development, Nintendo promised the ability to link systems for competitive play. The system's EXT (extension) port, located on the underside of the system below the controller port, was never officially supported since no "official" multiplayer games were ever published, nor was an official link cable released. (Although "Waterworld" and "Faceball" were going to use the EXT port for multiplayer play, the multiplayer features in the former were removed and the latter was canceled.)
Games.
Nintendo initially showcased three games for the Virtual Boy. They planned to release three titles at launch, and two or three per month thereafter. Given the system's short lifespan, only 22 games were released. Of them, 19 games were released in the Japanese market, while 14 were released in North America.
s of 2007[ [update]], the homebrew community at Planet Virtual Boy were still developing unofficial software. Two previously unreleased games, namely "Bound High" and the Japanese version of "Faceball" (known as "NikoChan Battle") have finally seen the light of day. Another cancelled game was called "Dragon Hopper", which was slated to be developed by Intelligent Systems and published by Nintendo.
When asked if Virtual Boy games were going to be available for download on the Virtual Console for the Nintendo 3DS, Nintendo of America President Reggie Fils-Aime said he could not answer, as he was unfamiliar with the platform. He noted that, given his lack of familiarity, he would be hard-pressed to make the case for inclusion of the games on the Virtual Console.
Reception.
The Virtual Boy was a commercial failure. The Virtual Boy failed for a number of reasons, among them "its high price, the discomfort caused by play [...] and what was widely judged to have been a poorly handled marketing campaign."
Gamers who previewed the system at the Shoshinkai 1994 show complained that the "Mario" demo was not realistic enough, was not in full color, and didn't allow for "tracking" (the movement of the image when the player turns his or her head). In the lead editorial of "Electronic Gaming Monthly" following the show, Ed Semrad predicted that the Virtual Boy would have poor launch sales due to the monochrome screen, lack of true portability, unimpressive lineup of games seen at the Shoshinkai show, and the price, which he argued was as low as it could get given the hardware but still too expensive for the experience the system offered. Following its release, reviews of the Virtual Boy tended to praise its novelty, but questioned its ultimate purpose and longtime viability. "The Los Angeles Times" described gameplay as being "at once familiar and strange." The column praised the quality of motion and immersive graphics, but considered the hardware itself tedious to use and non-portable. A later column by the same reviewer found the system to be somewhat asocial, although it held out hope for the console's future.
While Nintendo had promised a virtual reality experience, the monochrome display limited the Virtual Boy's potential for immersion. Reviewers often considered the 3-dimensional features a gimmick, added to games that were essentially 2- or even 1-dimensional. "The Washington Post" felt that, even when a game gives the impression of 3-dimensionality, it suffers from "hollow vector graphics." Yokoi, the system's inventor, noted the system's relative strengths with action and puzzle games, although those types of games provided only minimal immersion. Multiple critics lamented the absence of head-tracking in the Virtual Boy hardware. Critics found that, as a result, players were unable to immerse themselves in the game worlds of Virtual Boy games. Instead, they interacted with the fictional worlds in the manner of any traditional 2-dimensional game (that is, via a controller). Boyer said the console "struggles to merge the two distinct media forms of home consoles and virtual reality devices." While the device employed virtual reality techniques, it did so via the traditional home console. No feedback from the body was incorporated into gameplay.
Many reviewers complained of painful and frustrating physiological symptoms when playing the Virtual Boy. Bill Frischling, writing for "The Washington Post", experienced "dizziness, nausea and headaches." Reviewers attributed the problems to both the monochromatic display and uncomfortable ergonomics. Nintendo, in the years after Virtual Boy's demise, has been frank about its failure. Howard Lincoln, chairman of Nintendo of America, said flatly that the Virtual Boy "just failed."
Legacy.
According to "Game Over", Nintendo laid blame for the machine's faults directly on its creator, Gunpei Yokoi. The commercial demise of the Virtual Boy was said by members of the video game press to be a contributing factor to Yokoi's withdrawal from Nintendo, despite the fact that he had planned to retire years before and finished another more successful project for the company, the Game Boy Pocket, which was released shortly before his leave. Nevertheless, "The New York Times" maintained that Yokoi kept a close relationship with Nintendo After leaving Nintendo, Yokoi founded his own company, Koto, and collaborated with Bandai to create the WonderSwan, a rival handheld system competing against the Game Boy.
Although considered a failure in the traditional sense, the Virtual Boy did little to alter Nintendo's development approach and focus on innovation. If anything, it encouraged a more open-ended metric for success than finances or sales. While the console itself failed in many regards, its focus on peripherals and haptic technology reemerged in later years. The hope of developing a virtual reality gaming platform has considerably outlived the Virtual Boy itself. Because Nintendo only shipped 1.26m Virtual Boy units worldwide, it is considered a valuable collector's item.
With the launch of the Nintendo 3DS console in 2011, Nintendo released a true handheld gaming console with auto-stereoscopic 3D visuals. In other words, this console produced the desired effects without any special glasses and was portable. In the period leading up to the release of the Nintendo 3DS, Shigeru Miyamoto discussed what he felt were the issues with the Virtual Boy. One was the actual use of the three-dimensional effects - while it was designed to render wireframe graphics, it was generally used to separate two-dimensional games into different planes separated by depth. Further, Miyamoto stated that the graphics were not as appealing, and while developing the Nintendo 64, had ruled out the use of wireframe graphics as too sparse to draw players. Finally, he stated that he perceived the Virtual Boy as a novelty that should not have used the Nintendo license so prominently.
In popular culture.
In the eponymous anime series, Maria Holic, a dorm adviser is going over a list of items not approved in dorm rooms, with one item of the list reading "Game Consoles (except Virtual Boy)". When asked why the Virtual Boy was an exception, she replies "Because God (referring to herself) loves things that have a tragic history." referencing the Virtual Boy's short lived shelf life. It is referenced frequently throughout the show and at one point is lined up with other failed devices that include Sony's PocketStation and Betamax, SEGA's Dreamcast, Enterbrain's Jashin MOK-KOS figurine and Mattel's Power Glove.
In the game Tomodachi Life (Tomodachi Collection: New Life in Japan), the game made several self-references to the Virtual Boy. The first reference the western market received of the Virtual Boy, was in the Localisation Trailer for the game, as it has previously been a Japan-exclusive game. In said trailer, there is a clip of Nintendo Staff (personified as in-game "Mii's") ritualising a Virtual Boy, exclaiming: "All Hail The Virtual Boy!". This is a reference to the utter failure of the system, and the fact that Nintendo mocked themselves of this mistake. Another reference comes from The shopkeeper in the game; upon buying (or trading in) a Virtual Boy, he/ she would say "dig the red and black styling", both a reference to the colour scheme and the colours the screen would display. The final reference is a Virtual Boy can be an opponent/ Boss in the minigame: Tomodachi Quest.

</doc>
<doc id="21971" url="http://en.wikipedia.org/wiki?curid=21971" title="Nuclear">
Nuclear

Nuclear may refer to:

</doc>
<doc id="21974" url="http://en.wikipedia.org/wiki?curid=21974" title="NSAP address">
NSAP address

A Network Service Access Point address (NSAP address), defined in ISO/IEC 8348, is an identifying label for a Service Access Point (SAP) used in OSI networking.
These are roughly comparable to IP addresses used in the Internet Protocol; they can specify a piece of equipment connected to an Asynchronous Transfer Mode (ATM) network. A specific stream, analogous to a TCP/IP port or socket, is specified by using a Transport Service Access Point (TSAP). ATM can also use a Presentation (PSAP) and Session (SSAP) Access Point, but these may also be unspecified; this is up to the application.
NSAP addresses are allocated by the International Organization for Standardization (ISO), through a system of delegated authorities, which are generally national standards organizations. One of the schemes to generate NSAPs uses E.164 which is the addressing format describing telephone numbers.
NSAP addresses do not specify where a network terminal is located. Routing equipment must translate NSAP addresses to SNPAs (SubNetwork Point of Attachment) to route OSI packets; VCI (Virtual Circuit Identifier) numbers are an example of a datalink layer SNPAs in ATM; when OSI packets are sent encapsulated in IP packets the IP address is considered an SNPA.
Currently SDH/SONET networks are a major part of the network infrastructure and NSAPs are used extensively. They are usually assigned by the Network Management/NOC personnel and agreed upon within an organization to be unique (to that organization and based on geographical location using country code telephone prefixes) and are required before any operational connectivity is established at the commissioning stage.
NSAP addresses are used in the following OSI-based network technologies: 
NSAP-style addresses are used in the IS-IS routing protocol.

</doc>
<doc id="21976" url="http://en.wikipedia.org/wiki?curid=21976" title="American submarine NR-1">
American submarine NR-1

Deep Submergence Vessel "NR-1" was a unique United States Navy nuclear-powered ocean engineering and research submarine, built by the Electric Boat Division of General Dynamics at Groton, Connecticut. "NR-1" was launched on 25 January 1969, completed initial sea trials 19 August 1969, and was home-ported at Naval Submarine Base New London. "NR-1" was the smallest nuclear submarine ever put into operation. The vessel was casually known as "Nerwin" and was never officially named or commissioned. The U.S. Navy is allocated a specific number of warships by the U.S. Congress. Admiral Hyman Rickover avoided using one of those allocations, and he also wanted to avoid the oversight that a warship receives from various bureaus.
History.
"NR-1's" missions included search, object recovery, geological survey, oceanographic research, and installation and maintenance of underwater equipment. "NR-1's" unique capability to remain at one site and completely map or search an area with a high degree of accuracy was a valuable asset on several occasions.
Through the 1970s and 1980s, "NR-1" conducted numerous classified missions involving recovery of objects from the floor of the deep-sea. These missions remain classified and few details have been made public. One publicly acknowledged mission in 1976 was to recover parts of an F-14 that was lost from the deck of an aircraft carrier and sank with at least one AIM-54A Phoenix air-to-air missiles. The secrecy normal to USN submarine operations was heightened by Rickover's personal involvement. Rickover shared details of "NR-1" operations on a need-to-know basis. Rickover envisioned building a small fleet of "NR-1" type submarines, but only one was built due to budget restrictions. 
Following the loss of the Space Shuttle "Challenger" in 1986, "NR-1" was used to search for, identify, and recover critical parts of the "Challenger" craft. Because it could remain on the sea floor without resurfacing frequently, "NR-1" was a major tool for searching deep waters. "NR-1" remained submerged and on station even when heavy weather and rough seas hit the area and forced all other search and recovery ships into port.
In 1995, Dr. Robert Ballard used the "NR-1" and its support ship, , to explore the wreck of , the sister ship of , which sank off the coast of Greece while serving as a hospital ship during World War I.
On 25 February 2007, "NR-1", towed by "Carolyn Chouest", arrived in Galveston, Texas, in preparation for an expedition to survey the Flower Garden Banks National Marine Sanctuary and other sites in the Gulf of Mexico.
"NR-1" was deactivated on 21 November 2008 at the U.S. Navy submarine base at Groton, Connecticut, defuelled at Portsmouth Naval Shipyard in Kittery, Maine, then sent to Puget Sound Naval Shipyard to be scrapped. On 13 November, 2013, the U.S. Navy announced that salvaged pieces of the sub would be put on display at the Submarine Force Library and Museum in Groton.
Capabilities.
"NR-1" performed underwater search and recovery, oceanographic research missions, and installation and maintenance of underwater equipment to a depth of almost half a nautical mile. Its features included extending bottoming wheels, three viewing ports, exterior lighting, television and still cameras for color photographic studies, an object recovery claw, a manipulator that could be fitted with various gripping and cutting tools, and a work basket that could be used in conjunction with the manipulator to deposit or recover items in the sea. Surface vision was provided by a television periscope permanently installed on a fixed mast in her sail area.
"NR-1" had sophisticated electronics, computers, and sonar systems that aided in navigation, communications, and object location and identification. It could maneuver or hold a steady position on or close to the seabed or underwater ridges, detect and identify objects at a considerable distance, and lift objects off the ocean floor.
"NR-1" was equipped with two electric-motor driven propellers and its maneuverability was enhanced by four ducted thrusters, two forward and two aft. The vehicle had diving planes mounted on the sail, and a conventional rudder.
"NR-1" could travel submerged at approximately four knots for long periods, limited only by consumable supplies — primarily food. It could study and map the ocean bottom, including temperature, currents, and other information for military, commercial, and scientific uses. Its nuclear propulsion provided independence from surface support ships and essentially unlimited endurance.
"NR-1"‍ '​s size limited its crew comforts. The crew of about 10 men could stay at sea for as long as a month, but had no kitchen or bathing facilities. They ate frozen TV dinners, bathed once a week with a bucket of water, and burned chlorate candles to produce oxygen. The sub was so slow that it was towed to sea by a surface vessel, and so tiny that the crew felt the push and pull of the ocean's currents. "Everybody on NR-1 got sick," said Allison J. Holifield, who commanded the sub in the mid-1970s. "It was only a matter of whether you were throwing up or not throwing up." 
"NR-1" was generally towed to and from remote mission locations by an accompanying surface tender, which was also capable of conducting research in conjunction with the submarine. "NR-1's" last mother ship was MV "Carolyn Chouest", which provided towing, communications, berthing, and direct mission support for all "NR-1" operations -- an extremely versatile platform and an indispensable member of the "NR-1" deep submergence team. "NR-1" command was manned with thirty-five Navy personnel and ten civilian contractor personnel. "NR-1" carried as many as thirteen persons (crew and specialists) at one time, including three of the four assigned officers. (The operations officer rode on "Carolyn Chouest"). All personnel that crewed "NR-1" were nuclear-trained and specifically screened and interviewed by the Director, Navy Nuclear Propulsion Program.

</doc>
<doc id="21977" url="http://en.wikipedia.org/wiki?curid=21977" title="Neo Geo Pocket">
Neo Geo Pocket

The Neo Geo Pocket is a monochrome handheld video game console released by SNK. It was the company's first handheld system and is part of the Neo Geo family. It debuted in Japan in late 1998, however never saw a western release, being exclusive to Japan and smaller Asian markets such as Hong Kong.
The Neo Geo Pocket is considered to be an unsuccessful console. Lower than expected sales resulted in its discontinuation in 1999, and was immediately succeeded by the Neo Geo Pocket Color, a full color device allowing the system to compete more easily with the dominant Game Boy Color handheld, and which also saw a western release. Though the system enjoyed only a short life, there were some significant games released on the system such as "Samurai Shodown", and "King of Fighters R-1".
Games.
The Neo Geo Pocket is forwards compatible with the majority of Neo Geo Pocket Color titles, although games for the color system will play in monochrome on a Neo Geo Pocket. Likewise, the Neo Geo Pocket Color is backwards compatible and the entire Neo Geo Pocket library can be played on the color system.
Only ten monochrome games were released for the Neo Geo Pocket before it was discontinued:
After the release of the Neo Geo Pocket Color, several of these titles began receiving re-releases, updated to include color. All but King of Fighters R-1, Melon-chan's Growth Diary, Samurai Shodown and the original Neo Geo Cup 98 (the "Plus" version was updated instead) were eventually re-released for the color system.

</doc>
<doc id="21978" url="http://en.wikipedia.org/wiki?curid=21978" title="Neil Kinnock">
Neil Kinnock

Neil Gordon Kinnock, Baron Kinnock (born 28 March 1942) is a British Labour Party politician. He served as a Member of Parliament from 1970 until 1995, first for Bedwellty and then for Islwyn. He was the Leader of the Labour Party and Leader of the Opposition from 1983 until 1992, making him the longest-serving Leader of the Opposition in British political history to date. He is the only Leader of the Labour Party since the position was created in 1908 never to hold ministerial office.
Following Labour's fourth consecutive defeat in the 1992 general election, Kinnock resigned as leader and resigned from the House of Commons three years later to become a European Commissioner. He went on to become the Vice-President of the European Commission under Romano Prodi from 1999 to 2004. Until the summer of 2009, he was also the Chairman of the British Council and the President of Cardiff University.
Early life.
Kinnock, an only child, was born in Tredegar, Wales. His father Gordon Herbert Kinnock was a former coal miner who suffered from dermatitis and later worked as a labourer; and his mother Mary (Howells) Kinnock was a district nurse. Gordon died of a heart attack in November 1971 aged 64; Mary died the following month aged 61.
In 1953, 11-year-old Kinnock began his secondary education at Lewis School, Pengam, which he later criticised for its record on caning in schools. He went on to the University College of South Wales and Monmouthshire, obtaining a degree in industrial relations and history in 1965. A year later, Kinnock obtained a postgraduate diploma in education. Between August 1966 and May 1970, he worked as a tutor for a Workers' Educational Association (WEA).
He has been married to Glenys Kinnock since 1967. They have two children – son Stephen (born January 1970), and daughter Rachel (born 1971).
Member of Parliament.
In June 1969 he won the Labour Party nomination for the constituency of Bedwellty in Wales (later Islwyn) for the following general election. He was elected on 18 June 1970 and became a member of the National Executive Committee of the Labour Party in October 1978. On his becoming an MP for the first time, his father said "Remember Neil, MP stands not just for Member of Parliament, but also for Man of Principle". 
Labour government policy at the time was in favour of devolution for Wales, but the wider party was split. Calling himself a 'unionist', Kinnock was one of six south Wales Labour MPs to campaign against devolution. He dismissed the idea of a Welsh identity, saying that "between the mid-sixteenth century and the mid-eighteenth century Wales had practically no history at all, and even before that it was the history of rural brigands who have been ennobled by being called princes". In the Wales referendum, 1979, the proposal for devolution was rejected.
Following Labour's defeat in the 1979 general election, James Callaghan appointed Neil Kinnock to the Shadow Cabinet as Education spokesman. His ambition was noted by other MPs, and David Owen's opposition to the changes to the electoral college was thought to be motivated by the realisation that they would favour Kinnock's succession. He remained as Education spokesman following the resignation of Callaghan as party leader and the election of Michael Foot as his successor in late 1980.
He was known as a left-winger, and gained prominence for his attacks on Margaret Thatcher's handling of the Falklands War in 1982.
Leadership of the Labour Party.
First period (1983–1987).
After Labour's heavy election defeat in June 1983, the almost 70-year-old Michael Foot resigned as leader and from the outset it was expected that Kinnock would succeed him. He was finally elected as Labour Party leader on 2 October 1983, with 71% of the vote, and Roy Hattersley was elected as his deputy; their prospective partnership was considered to be a 'dream ticket'.
His first period as party leader – between the 1983 and 1987 elections – was dominated by his struggle with the hard left, then still strong in the party. Kinnock was determined to move the party's political standing to a centre-left position. Although Kinnock had come from the Tribune left of the party, he parted company with many of his former allies after his appointment to the shadow cabinet.
In 1981, when still Labour's Education spokesman, Kinnock was alleged to have effectively scuppered Tony Benn's attempt to replace Denis Healey as Labour's deputy leader by first supporting the candidacy of the more traditionalist Tribunite John Silkin and then urging Silkin supporters to abstain on the second, run-off, ballot.
All this meant that Kinnock had made plenty of enemies on the left by the time he was elected as leader, though a substantial number of former Bennites gave him strong backing. He was almost immediately in serious difficulty as a result of Arthur Scargill's decision to lead his union, the National Union of Mineworkers (NUM) into a national strike (in opposition to pit closures) without a members' ballot. The NUM was widely regarded as the labour movement's praetorian guard and the strike convulsed the Labour movement. Kinnock supported the aim of the strike – which he famously dubbed the "case for coal" – but, as an MP from a mining area, was bitterly critical of the tactics employed. When heckled at a Labour Party rally for referring to the killing of David Wilkie as "an outrage", Kinnock lost his temper and accused the hecklers of "living like parasites off the struggle of the miners" and implied that Scargill had lied to the striking miners. In 1985 he made his criticisms public in a speech to Labour's conference:The strike wore on. The violence built up because the single tactic chosen was that of mass picketing, and so we saw policing on a scale and with a system that has never been seen in Britain before. The court actions came, and by the attitude to the court actions, the NUM leadership ensured that they would face crippling damages as a consequence. To the question: "How did this position arise?", the man from the lodge in my constituency said: "It arose because nobody really thought it out."
In 2004, Kinnock said of Scargill, "Oh I detest him (Scargill). I did then, I do now, and it's mutual. He hates me as well. And I'd much prefer to have his savage hatred than even the merest hint of friendship from that man."
The strike's defeat early in the year, and the bad publicity associated with the entryism practised by the Militant tendency were the immediate background for the 1985 Labour Party conference. Earlier in the year left-wing councils had protested at Government restriction of their budgets by refusing to set budgets, resulting in a budget crisis in Militant-dominated Liverpool City Council. Kinnock attacked Militant and their conduct in Liverpool in one of the best remembered passages of any post-war British political speech:
I'll tell you what happens with impossible promises. You start with far-fetched resolutions. They are then pickled into a rigid dogma, a code, and you go through the years sticking to that, out-dated, misplaced, irrelevant to the real needs, and you end in the grotesque chaos of a Labour council – a "Labour" council! – hiring taxis to scuttle round a city handing out redundancy notices to its own workers.
I'm telling you, I'm telling you, and you'll listen, you can't play politics with people's jobs and people's services.
One Liverpool MP, Eric Heffer, a member of the NEC left the conference stage in disgust at Kinnock's comments. In June 1986 the Labour Party finally expelled the deputy leader of Liverpool council, the high profile Militant supporter Derek Hatton, who was found guilty of "manipulating the rules of the district Labour party". By 1986, the party's position appeared to strengthen further with excellent local election results and a thorough rebranding of the party under the direction of Kinnock's director of communications Peter Mandelson. Labour, now sporting a continental social democratic style emblem of a rose, appeared to be able to run the governing Conservatives close, but Margaret Thatcher did not let Labour's makeover go unchallenged.
The Conservatives' 1986 conference was well-managed, and effectively relaunched the Conservatives as a party of radical free-market liberalism. Labour suffered from a persistent image of extremism, especially as Kinnock's campaign to root out Militant dragged on as figures on the hard left of the party tried to stop its progress. Opinion polls showed that voters favoured retaining the United Kingdom's nuclear weapons, (Labour's policy, supported by Kinnock, was of unilateral nuclear disarmament), and believed that the Conservatives would be better than Labour at defending the country.
1987 general election.
In early 1987, Labour lost a by-election in Greenwich to the Social Democratic Party's Rosie Barnes. As a result, Labour faced the 1987 election in some danger of coming third in the popular vote. In secret, Labour's aim became to secure second place.
Labour fought a professional campaign that at one point scared the Tories into thinking they might lose. Mandelson and his team had revolutionised Labour's communications – a transformation symbolised by a party election broadcast popularly known as "Kinnock: The Movie". This was directed by Hugh Hudson and featured Kinnock's 1985 conference speech, and shots of him and Glenys walking on the Great Orme in Llandudno (so emphasising his appeal as a family man and associating him with images of Wales away from the coalmining communities where he grew up), and a speech to that year's Welsh Labour Party conference asking why he was the "first Kinnock in a thousand generations" to go to university.
On polling day, Labour easily took second place, but with only 31 per cent to the SDP-Liberal Alliance's 22 per cent. Labour was still more than ten percentage points behind the Conservatives, who retained a three-figure majority in the House of Commons. However, the Conservative government's majority had come down from 144 in 1983 to 102. Significantly, Labour had gained 20 seats at the election.
Labour won extra seats in Scotland, Wales and Northern England, but lost ground particularly in Southern England and London, where the Tories still dominated.
Second period (1987–1992).
A few months after the election, Kinnock gained brief attention in the United States in August 1987 when it was discovered that then-US senator Joe Biden of Delaware plagiarised one of Kinnock's speeches during his 1988 presidential campaign in a speech at a Democratic debate in Iowa. This led to Biden's withdrawing from the race.
The second period of Kinnock's leadership was dominated by his drive to reform the party's policies to gain office. This began with an exercise dubbed the policy review, the most high-profile aspect of which was a series of consultations with the public known as "Labour Listens" in the autumn of 1987.
After Labour Listens, the party went on, in 1988, to produce a new statement of aims and values—meant to supplement and supplant the formulation of Clause IV of the party's constitution (though, crucially, this was not actually replaced until 1995 under the leadership of Tony Blair) and was closely modelled on Anthony Crosland's social-democratic thinking—emphasising equality rather than public ownership. At the same time the commitment to unilateral nuclear disarmament was dropped, and reforms of Party Conference and the National Executive meant that local parties lost much of their ability to influence policy.
In 1988, Kinnock was challenged by Tony Benn for the party leadership. Later many identified this as a particularly low period in Kinnock's leadership—as he appeared mired in internal battles after five years of leadership and the Conservatives still dominating the scene and ahead in the opinion polls. In the end, though, Kinnock won a decisive victory over Benn and would soon enjoy a substantial rise in support.
The policy review—reporting in 1989—coincided with Labour's move ahead in the polls as the poll tax row was destroying Conservative support, and Labour won big victories in local by-elections.
In December 1989, he abandoned the Labour policy on closed shops—a decision seen by many as a move away from traditional socialist policies to a more Europe-wide agenda, and also a move to rid the party of its image of being run by the unions.
Kinnock was also perceived as scoring in debates over Margaret Thatcher in the Commons—previously an area in which he was seen as weak—and finally Michael Heseltine challenged Thatcher's leadership and she resigned on 28 November 1990 to be succeeded by John Major. Kinnock greeted Thatcher's resignation by describing it as "very good news" and demanded an immediate general election.
Public reaction to Major's elevation was highly positive. A new Prime Minister and the fact that Kinnock was now current leader of a major party reduced the impact of calls for "Time for a Change". Neil Kinnock's showing in the opinion polls dipped; before Thatcher's resignation, Labour had been up to 10 points ahead of the Tories in the opinion polls (an Ipsos MORI poll in April 1990 had actually shown Labour more than 20 points ahead of the Tories), but many opinion polls were actually showing the Tories with more support than Labour, in spite of the deepening recession.
By now Militant had finally been routed in the party, and their two MPs were expelled at the end of 1991. The majority in the group were now disenchanted with entryism, and choose to function outside Labour's ranks, and to take advantage of opportunities created by Margaret Thatcher's unpopular 'poll tax'.
1992 general election, backbenches and retirement.
In the three years leading up to the 1992 election, Labour had consistently topped the opinion polls, with 1991 seeing the Tories (rejuvenated by the arrival of a new leader in John Major the previous November) snatch the lead off Labour more than once before Labour regained it. Since Major's election as Conservative leader (and becoming Prime Minister), Kinnock had spent the end of 1990 and most of 1991 putting pressure on Major to hold the election that year, but Major had held out and insisted that there would be no general election in 1991. In the run-up to the election, held on 9 April 1992, most opinion polls had suggested that the election would end in a hung parliament or a narrow Labour majority.
In the 1992 election, Labour made considerable progress – reducing the Conservative majority to just 21 seats. It came as a shock to many when the Conservatives won a majority, but the "triumphalism" perceived by some observers of a Labour party rally in Sheffield (together with Kinnock's performance on the podium) may have helped put floating voters off. Although internal polls suggested no impact, while public polls suggested a decline in support had already occurred, most of those directly involved in the campaign believe that the rally really came to widespread attention only after the electoral defeat itself, with Kinnock himself changing his mind to a rejection of its negative impact over time.
On the day of the general election, "The Sun" newspaper ran an "infamous" front page featuring Kinnock (headline: 'If Kinnock wins today will the last person to leave Britain please turn out the lights') that he blamed in his resignation speech for losing Labour the election, along with other newspapers who had backed the Conservatives in the run-up to the election. The following day's headline in "The Sun" was the triumphalist 'It's The Sun Wot Won It', which Rupert Murdoch, many years later at his April 2012 appearance before the Leveson Inquiry, stated was both "tasteless and wrong" and led to the editor Kelvin MacKenzie receiving a reprimand.
The Labour supporting "Daily Mirror" had backed Kinnock in the 1987 election and again in 1992. Less expected was the "Financial Times" backing of Kinnock at the 1992 election.
Kinnock himself later claimed to have half-expected his defeat in the 1992 election and proceeded to turn himself into a media personality, even hosting a chat show on BBC Wales and twice appearing – with considerable success – on the topical panel show "Have I Got News for You" within a year of the defeat. Many years later, he returned to appear as a guest host of the programme.
Kinnock announced his resignation as Labour Party leader on 13 April 1992, ending eight and a half years in the role – making him the longest serving opposition leader in British political history. He had gained this distinction in November 1990, and no subsequent opposition party leader has yet matched this record.
John Smith, previously Shadow Chancellor, was elected on 18 July as his successor.
He remains on the Advisory Council of the Institute for Public Policy Research, which he helped set up in the 1980s.
He was an enthusiastic supporter of Ed Miliband's campaign to lead the Labour Party in 2010, and was reported as telling activists, when Ed Miliband won, "We've got our party back".
In 2011 he participated in the family history TV programme "Coming Home" where he discovered hitherto unknown information about his family.
European Union Commissioner.
Kinnock was appointed one of the UK's two members of the European Commission, which he served first as Transport Commissioner under President Jacques Santer, in early 1995; marking the end of his 25 years in parliament. This came less than a year after the death of his successor as Labour leader John Smith and the election of Tony Blair as the party's new leader.
He was obliged to resign as part of the forced, collective resignation of the Commission in 1999. He was re-appointed to the Commission under new President Romano Prodi. He now became one of the Vice-Presidents of the European Commission, with responsibility for Administrative Reform and the Audit, Linguistics and Logistics Directorates General. His term of office as a Commissioner was due to expire on 30 October 2004, but was delayed owing to the withdrawal of the new Commissioners. During this second term of office on the Commission, he was responsible for introducing new staff regulations for EU officials, a significant feature of which was substantial salary cuts for everyone employed after 1 May 2004, reduced pension prospects for many others, and gradually worsening employment conditions. This made him disliked by many EU staff members, although the pressure on budgets that largely drove these changes had actually been imposed on the Commission from above by the Member States in Council.
In February 2004, it was announced that with effect from 1 November 2004 Kinnock would become head of the British Council. Coincidentally, at the same time, his son Stephen became head of the British Council branch in St. Petersburg, Russia. At the end of October, it was announced that he would become a member of the House of Lords (intending to be a working peer), when he was able to leave his EU responsibilities. In 1977, he had remained in the House of Commons, with Dennis Skinner, while other MPs walked to the Lords to hear the Queen's speech opening the new parliament. He had dismissed going to the Lords in recent interviews. Kinnock explained his change of attitude, despite the continuing presence of 90 hereditary peers and appointment by patronage, by asserting that the Lords was a good base for campaigning.
Life peerage.
He was introduced to the House of Lords on 31 January 2005, after being created, on 28 January, Baron Kinnock, of Bedwellty in the County of Gwent. On assuming his seat he stated, "I accepted the kind invitation to enter the House of Lords as a working peer for practical political reasons." When his peerage was first announced, he said, "It will give me the opportunity... to contribute to the national debate on issues like higher education, research, Europe and foreign policy." His peerage meant that the Labour and Conservative parties were equal in numbers in the upper house of Parliament (since then, the number of Labour members has overtaken the number of Conservative members). Kinnock was a long-time critic of the House of Lords, and his acceptance of a peerage led him to be accused of hypocrisy, by Will Self, among others.
Personal life.
He is married to Glenys Kinnock, the UK's Minister for Africa and the United Nations from 2009 to 2010, and a Labour Member of the European Parliament (MEP) from 1994 to 2009. When she was made a life peer in 2009, they became one of the few couples both to hold titles in their own right. The two met while studying at University College, Cardiff, where they were known as "the power and the glory" (Glenys the power), and they married on 25 March 1967. Previously living together in Peterston-Super-Ely, a village near the western outskirts of Cardiff, in 2008 they moved to Tufnell Park, London, to be closer to their daughter and grandchildren.
They have a son, Stephen and a daughter, Rachel. Stephen is married to Helle Thorning-Schmidt, Prime Minister of Denmark and leader of the Danish Social Democrats political party. He is assistant director of the British Council in Sierra Leone and Member of Parliament (MP) for Aberavon. Rachel worked in the Political Office at 10 Downing Street under Gordon Brown as an events organiser, having previously worked with Glenys Kinnock during her time as an MEP after switching from a career in the film industry. She subsequently became organiser for Ed Miliband, Brown's successor as Labour leader. She is married to film producer Stuart Bentham, who she met when they both worked on the sitcom "Drop the Dead Donkey".
On 26 April 2006, Kinnock was given a six-month driving ban after being found guilty of two speeding offences along the M4 motorway, west of London.
Neil Kinnock is a lifelong Cardiff City F.C. fan and regularly attends matches.
He was portrayed by both Chris Barrie and Steve Coogan in the television show Spitting Image and by Euan Cuthbertson in the Scottish film In Search of La Che.
Kinnock has been described as an agnostic and an atheist.

</doc>
<doc id="21979" url="http://en.wikipedia.org/wiki?curid=21979" title="Netscape">
Netscape

Netscape Communications (formerly known as Netscape Communications Corporation and commonly known as Netscape) is an American computer services company, best known for Netscape Navigator, its web browser. When it was an independent company, its headquarters was in Mountain View, California.
Netscape's web browser was once dominant in terms of usage share, but lost most of that share to Internet Explorer during the so-called first browser war. The usage share of Netscape had fallen from over 90 percent in the mid-1990s to less than one percent by the end of 2006.
Netscape is credited with developing the Secure Sockets Layer Protocol (SSL) for securing online communication, which is still widely used, as well as JavaScript, the most widely used language for client-side scripting of web pages.
Netscape stock traded from 1995 until 1999 when it was acquired by AOL in a pooling-of-interests transaction ultimately worth US$10 billion. Shortly before its acquisition by AOL, Netscape released the source code for its browser and created the Mozilla Organization to coordinate future development of its product. The Mozilla Organization rewrote the entire browser's source code based on the Gecko rendering engine; all future Netscape releases were based on this rewritten code. The Gecko engine would later be used to power the Mozilla Foundation's Firefox browser.
Under AOL, Netscape's browser development continued until December 2007, when AOL announced that the company would stop supporting the Netscape browser as of early 2008. AOL has continued to use the Netscape brand in recent years to market a discount Internet service provider.
History.
Early years.
Netscape was the first company to attempt to capitalize on the nascent World Wide Web. It was originally founded under the name Mosaic Communications Corporation on April 4, 1994, the brainchild of Jim Clark who had recruited Marc Andreessen as co-founder and Kleiner Perkins Caufield & Byers as investors. Clark recruited other early team members from SGI and NCSA Mosaic. Jim Barksdale came on board as CEO in January 1995. Jim Clark and Marc Andreessen originally created a 20-page concept pitch for an online gaming network to Nintendo for the Nintendo 64 console, but a deal was never reached. Marc Andreessen explains, "If they had shipped a year earlier, we probably would have done that instead of Netscape."
The company's first product was the web browser, called "Mosaic Netscape 0.9," released on October 13, 1994. This browser was subsequently renamed Netscape Navigator, and the company took the 'Netscape' name (coined by employee Greg Sands, although it was also a trademark of Cisco Systems) on November 14, 1994 to avoid trademark ownership problems with NCSA, where the initial Netscape employees had previously created the NCSA Mosaic web browser. The Mosaic Netscape web browser did not use any NCSA Mosaic code. The internal codename for the company's browser was "Mozilla", which stood for "Mosaic killer", as the company's goal was to displace NCSA Mosaic as the world's number one web browser. A cartoon Godzilla-like lizard mascot was drawn by artist-employee Dave Titus, which went well with the theme of crushing the competition. The Mozilla mascot featured prominently on Netscape's web site in the company's early years. However, the need to project a more "professional" image (especially towards corporate clients) led to this being removed.
On August 9, 1995, Netscape made an extremely successful IPO. The stock was set to be offered at US$14 per share, but a last-minute decision doubled the initial offering to US$28 per share. The stock's value soared to US$75 during the first day of trading, nearly a record for first-day gain. The stock closed at US$58.25, which gave Netscape a market value of US$2.9 billion. While it was unusual for a company to go public prior to becoming profitable, Netscape's revenues had, in fact, doubled every quarter in 1995. The success of this IPO subsequently inspired the use of the term "Netscape moment" to describe a high-visibility IPO that signals the dawn of a new industry. During this period, Netscape also pursued a publicity strategy (crafted by Rosanne Siino, then head of public relations) packaging Andreessen as the company's "rock star." The events of this period ultimately landed Andreessen, barefoot, on the cover of Time Magazine.
Netscape advertised that "the web is for everyone" and stated one of its goals was to "level the playing field" among operating systems by providing a consistent web browsing experience across them. The Netscape web browser interface was identical on any computer. Netscape later experimented with prototypes of a web-based system which would enable users to access and edit their files anywhere across a network, no matter what computer or operating system they happened to be using. This did not escape the attention of Microsoft, which viewed the commoditization of operating systems as a direct threat to its bottom line, i.e. a move from Windows to another operating system would yield a similar browsing experience thus reducing barriers to change. It is alleged that several Microsoft executives visited the Netscape campus in June 1995 to propose dividing the market (an allegation denied by Microsoft and, if true, would have breached antitrust laws), which would have allowed Microsoft to produce web browser software for Windows while leaving all other operating systems to Netscape. Netscape refused the proposition.
Microsoft released version 1.0 of Internet Explorer as a part of the Windows 95 Plus Pack add-on. According to former Spyglass developer Eric Sink, Internet Explorer was based not on NCSA Mosaic as commonly believed, but on a version of Mosaic developed at Spyglass (which itself was based upon NCSA Mosaic). Microsoft quickly released several successive versions of Internet Explorer, bundling them with Windows, never charging for them, financing their development and marketing with revenues from other areas of the company. This period of time became known as the browser wars, in which Netscape Communicator and Internet Explorer added many new features and went through many version numbers (not always in a logical fashion) in attempts to outdo each other. But Internet Explorer had the upper hand, as the amount of manpower and capital dedicated to it eventually surpassed the resources available in Netscape's entire business. By version 3.0, IE was roughly a feature-for-feature equivalent of Netscape Communicator, and by version 4.0, it was generally considered to be more stable on Windows than on the Macintosh platform. Microsoft also targeted other Netscape products with free workalikes, such as the Internet Information Server (IIS), a web server which was bundled with Windows NT.
Netscape could not compete with this strategy. In fact, it didn't attempt to. Netscape Navigator was not free to the general public until January 1998, while Internet Explorer and IIS have always been free or came bundled with an operating system and/or other applications. Meanwhile, Netscape faced increasing criticism for the bugs in its products; critics claimed that the company suffered from 'featuritis' – putting a higher priority on adding new features than on making them work properly. This was particularly true with Netscape Navigator 2, which was only on the market for 5 months in early 1996 before being replaced by Netscape Navigator 3. The tide of public opinion, having once lauded Netscape as the David to Microsoft's Goliath, steadily turned negative, especially when Netscape experienced its first bad quarter at the end of 1997 and underwent a large round of lay-offs in January 1998. Later, former Netscape executives Mike Homer and Peter Currie described the period as "hectic and crazy" and that the company was undone by factors both internal and external.
Open sourcing.
January 1998 was also the month that Netscape started the open source Mozilla project. Netscape publicly released the source code of Netscape Communicator 4.0 in the hopes that it would become a popular open source project. It placed this code under the Netscape Public License, which was similar to the GNU General Public License but allowed Netscape to continue to publish proprietary work containing the publicly released code. However, after having released the Communicator 4.0 code this way, Netscape proceeded to work on Communicator 4.5 which was focused on improving email and enterprise functionality. It eventually became clear that the Communicator 4.0 browser was too difficult to develop, and open source development was halted on this codebase. Instead, the open source development shifted to a next generation browser built from scratch. Using the newly built Gecko layout engine, this browser had a much more modular architecture than Communicator 4.0 and was therefore easier to develop with a large number of programmers. It also included an XML user interface language named XUL that allowed single development of a user interface that ran on Windows, Macintosh, and Unix. The slogan for this open sourcing effort, "Free The Lizard", carried comedic sexual overtones.
The United States Department of Justice filed an antitrust case against Microsoft in May 1998. Netscape was not a plaintiff in the case, though its executives were subpoenaed and it contributed much material to the case, including the entire contents of the 'Bad Attitude' internal discussion forum. In October 1998, Netscape acquired web directory site NewHoo for the sum of US$1 million, renamed it the Open Directory Project, and released its database under an open content license.
Acquisition by America Online.
America Online (AOL) on November 24, 1998 announced it would acquire Netscape Communications in a tax-free stock-swap valued at US$4.2 billion at the time of the announcement. By the time the deal closed on March 17, 1999, it was valued at US$10 billion. This merger was ridiculed by many who believed that the two corporate cultures could not possibly mesh; one of its most prominent critics was longtime Netscape developer Jamie Zawinski. The acquisition was seen as a way for AOL to gain a bargaining chip against Microsoft, to let it become less dependent on the Internet Explorer web browser. Others believed that AOL was interested in Netcenter, or Netscape's web properties, which drew some of the highest traffic worldwide. Eventually, Netscape's server products and its Professional Services group became part of iPlanet, a joint marketing and development alliance between AOL and Sun Microsystems. On November 14, 2000, AOL released Netscape 6, based on the Mozilla 0.6 source code. (Version 5 was skipped.) Unfortunately, Mozilla 0.6 was far from being stable yet, and so the effect of Netscape 6 was to further drive people away from the Netscape brand. It was not until August 2001 that Netscape 6.1 appeared, based on Mozilla 0.9.2 which was significantly more robust. A year later came Netscape 7.0, based on the Mozilla 1.0 core.
Disbanding.
During the acquisition of Netscape by AOL, joint development and marketing of Netscape software products would occur through the Sun-Netscape Alliance. The software in the newly branded iPlanet included "messaging and calendar, collaboration, web, application, directory, and certificate servers", as well as "production-ready applications for e-commerce, including commerce exchange, procurement, selling, and billing." In March 2002, when the alliance was ended, "iPlanet became a division of Sun... Sun retained the intellectual property rights for all products and the engineering"
On July 15, 2003, Time Warner (formerly AOL Time Warner) disbanded Netscape. Most of the programmers were laid-off, and the Netscape logo was removed from the building. However, the Netscape 7.2 web browser (developed in-house rather than with Netscape staff, with some work outsourced to Sun's Beijing development center) was released by AOL on August 18, 2004.
On October 12, 2004, the popular developer website Netscape DevEdge was shut down by AOL. DevEdge was an important resource for Internet-related technologies, maintaining definitive documentation on the Netscape browser, documentation on associated technologies like HTML and JavaScript, and popular articles written by industry and technology leaders such as Danny Goodman. Some content from DevEdge has been republished at the Mozilla website.
After the Sun acquisition by Oracle in January 2010, Oracle continued to sell iPlanet branded applications, which originated from Netscape. Applications include: Oracle iPlanet Web Server and Oracle iPlanet Web Proxy Server.
Final release of the browser.
The Netscape brand name continued to be used extensively. The company once again had its own programming staff devoted to the development and support for the series of web browsers. Additionally, Netscape also maintained the Propeller web portal, which was a popular social-news site, similar to Digg, which was given a new look in June 2006. AOL marketed a discount ISP service under the Netscape brand name.
A new version of the Netscape browser, Netscape Navigator 9, based on Firefox 2, was released in October 2007. It featured a green and grey interface. In November 2007, IE had 77.4% of the browser market, Firefox 16.0% and Netscape 0.6%, according to Net Applications, an Internet metrics firm. On December 28, 2007, AOL announced that on February 1, 2008 it would drop support for the Netscape web browser and would no longer develop new releases. The date was later extended to March 1 to allow a major security update and to add a tool to assist users in migrating to other browsers. These additional features were included in the final version of Netscape Navigator 9 (version 9.0.0.6), released on February 20, 2008.
Software.
Classic releases.
Netscape Navigator (versions 0.9–4.08).
Netscape Navigator was Netscape's web browser from versions 1.0–4.8. The first beta versions were released in 1994 and were called Mosaic and later Mosaic Netscape. Then, a legal challenge from the National Center for Supercomputing Applications (makers of NCSA Mosaic), which many of Netscape's founders used to develop, led to the name Netscape Navigator. The company's name also changed from Mosaic Communications Corporation to Netscape Communications Corporation.
The browser was easily the most advanced available and so was an instant success, becoming market leader while still in beta. Netscape's feature-count and market share continued to grow rapidly after version 1.0 was released. Version 2.0 added a full email reader called Netscape Mail, thus transforming Netscape from a single-purpose web browser to an Internet suite. The main distinguishing feature of the email client was its ability to display HTML email. During this period, the entire suite was called Netscape Navigator.
Version 3.0 of Netscape (the first beta was codenamed "Atlas") was the first to face any serious competition in the form of Microsoft Internet Explorer 3.0. But Netscape easily remained the number one browser for the time being.
Netscape also released a Gold version of Navigator 3.0 that incorporated WYSIWYG editing with drag and drop between web editor and email components.
Netscape Communicator (versions 4.0–4.8).
Netscape 4 addressed the problem of Netscape Navigator being used as both the name of the suite and the browser contained within it by renaming the suite to Netscape Communicator. After five preview releases in 1996–1997, Netscape released the final version of Netscape Communicator in June 1997. This version, more or less based on Netscape Navigator 3 Code, updated and added new features. The new suite was successful, despite increasing competition from Internet Explorer (IE) 4.0 (which had a more advanced HTML engine) and problems with the outdated browser core. IE was slow and unstable on the Mac platform until version 4.5. Despite this, Apple entered into an agreement with Microsoft to make IE the default browser on new Mac OS installations, a further blow to Netscape's prestige. The Communicator suite was made up of Netscape Navigator, Netscape Mail & Newsgroups, Netscape Address Book and Netscape Composer (an HTML editor).
In January 1998, Netscape Communications Corporation announced that all future versions of its software would be available free of charge and developed by an open source community, Mozilla. Netscape Communicator 5.0 was announced (codenamed "Gromit"). However, its release was greatly delayed, and meanwhile there were newer versions of Internet Explorer, starting with version 4. These had more features than the old Netscape version, including better support of HTML 4, CSS, DOM, and ECMAScript. The more advanced Internet Explorer 5.0 became the market leader.
In October 1998, Netscape Communicator 4.5 was released. It featured various functionality improvements, especially in the Mail and Newsgroups component, but did not update the browser core, whose functionality was essentially identical to that of version 4.08. One month later, Netscape Communications Corporation was bought by AOL. In November, work on Netscape 5.0 was canceled in favor of developing a completely new program from scratch.
Mozilla-based releases.
Netscape 6 (versions 6.0–6.2.3).
In 1998, an informal group called the Mozilla Organization was formed and largely funded by Netscape (the vast majority of programmers working on the code were paid by Netscape) to co-ordinate the development of Netscape 5 (codenamed "Gromit"), which would be based on the Communicator source code. However, the aging Communicator code proved difficult to work with and the decision was taken to scrap Netscape 5 and re-write the source code. The re-written source code was in the form of the Mozilla web browser, which, with a few additions, Netscape 6 was based on.
This decision meant that Netscape's next major version was severely delayed. In the meantime, Netscape was taken over by AOL who, acting under pressure from the Web Standards Project, forced its new division to release Netscape 6.0 in 2000. The suite again consisted of Netscape Navigator and the other Communicator components, with the addition of a built-in AOL Instant Messenger client, Netscape Instant Messenger. However, it was clear that Netscape 6 was not yet ready for release and it flopped badly. It was based on Mozilla 0.6, which was not ready to be used by the general public yet due to many serious bugs that would cause it to crash often or render web pages slowly. Later versions of Netscape 6 were much improved (especially 6.2.x was regarded as a good release), but the browser still struggled to make an impact on a disappointed community.
Netscape 7 (versions 7.0–7.2).
Netscape 7.0 (based on Mozilla 1.0.1) was released in August 2002 was a direct continuation of Netscape 6 with very similar components. It picked up a few users, but was still very much a minority browser. It did, however, come with the popular Radio@Netscape Internet radio client. AOL had decided to deactivate Mozilla's popup-blocker functionality in Netscape 7.0, which created an outrage in the community. AOL learned the lesson for Netscape 7.01 and allowed Netscape to reinstate the popup-blocker. Netscape also introduced a new AOL-free-version (without the usual AOL addons) of the browser suite. Netscape 7.1 (codenamed "Buffy" and based on Mozilla 1.4) was released in June 2003.
In 2003, AOL closed down its Netscape division and laid-off or reassigned all of Netscape's employees. Mozilla.org continued, however, as the independent Mozilla Foundation, taking on many of Netscape's ex-employees. AOL continued to develop Netscape in-house (with help from Sun's Beijing development center), but, due to there being no staff committed to it, improvements were minimal. One year later, in August 2004, the last version based on Mozilla was released: Netscape 7.2, based on Mozilla 1.7.2.
After an official poll posted on Netscape's community support board in late 2006, speculation arose of the Netscape 7 series of suites being fully supported and updated by Netscape's in-house development team. This was not to be.
Mozilla Firefox-based releases.
Netscape Browser (version 8.0–8.1.3).
Between 2005 and 2007, Netscape's releases became known as "Netscape Browser". AOL chose to base Netscape Browser on the relatively successful Mozilla Firefox, a re-written version of Mozilla produced by the Mozilla Foundation. This release is not a full Internet suite as before, but is solely a web browser. Other controversial decisions include the browser's being made only for Microsoft Windows and its featuring both the Gecko rendering engine of previous releases and the Trident engine used in Internet Explorer. AOL's acquisition of Netscape Communications in November 1998 made it less of a surprise when the company laid off the Netscape team and outsourced development to Mercurial Communications. Netscape Browser 8.1.3 was released on April 2, 2007, and included general bug fixes identified in versions 8.0–8.1.2
Netscape Navigator (version 9.0).
Netscape Navigator 9's features were said to include newsfeed support and become more integrated with the Propeller Internet portal, alongside more enhanced methods of discussion, submission and voting on web pages. It also sees the browser return to multi-platform support across Windows, Linux and Mac OS X. Like Netscape version 8.x, the new release was based upon the popular Mozilla Firefox (version 2.0), and supposedly had full support of all Firefox add-ons and plugins, some of which Netscape was already providing. Also for the first time since 2004, the browser was produced in-house with its own programming staff. A beta of the program was first released on June 5, 2007. The final version was released on October 15, 2007.
End of development and support.
AOL officially announced that support for Netscape Navigator would end on March 1, 2008, and recommended that its users download either the Flock or Firefox browsers, both of which were based on the same technology.
The decision met mixed reactions from communities, with many arguing that the termination of product support is significantly belated. Internet security site "Security Watch" stated that a trend of infrequent security updates for AOL's Netscape caused the browser to become a "security liability", specifically the 2005–2007 versions, Netscape Browser 8. Asa Dotzler, one of Firefox's original bug testers, greeted the news with "good riddance" in his blog post, but praised the various members of the Netscape team over the years for enabling the creation of Mozilla in 1998. Others protested and petitioned AOL to continue providing vital security fixes to unknowing or loyal users of its software, as well as protection of a well-known brand.
Mozilla Thunderbird-based releases.
Netscape Messenger 9.
On June 11, 2007, Netscape announced Netscape Mercury, a stand-alone Email / News Client that was to accompany Navigator 9. Mercury was based on Mozilla Thunderbird. The product was later renamed Netscape Messenger 9, and an alpha version was released. In December 2007, AOL announced it was canceling Netscape's development of Messenger 9 as well as Navigator 9.
Product list.
Initial product line.
Netscape's initial product line consisted of:
Later Netscape products.
Netscape's later products included:
Propeller.
Between June 2006 and September 2007, AOL operated Netscape's website as social news website similar to Digg. The format did not do well; traffic dropped 55.1 percent between November 2006 and August 2007. In September 2007, AOL reverted Netscape's website to a traditional news portal, and rebranded the social news portal as "Propeller", moving the site to the domain "propeller.com." AOL shut down the Propeller website on October 1, 2010.
Netscape Search.
Netscape operated a search engine, Netscape Search, which now redirects to AOL Search (which itself now merely serves Google search results). Another version of Netscape Search was incorporated into Propeller.
Other sites.
Netscape also operates a number of country-specific Netscape portals, including among others. The portal of was shut down in June 2008.
The Netscape Blog was written by Netscape employees discussing the latest on Netscape products and services. Netscape NewsQuake (formally "Netscape Reports") is Netscape's news and opinion blog, including video clips and discussions. As of January 2012, no new posts have been made on either of these blogs since August 2008.
Netscape technologies.
Netscape created the JavaScript web page scripting language. It also pioneered the development of push technology, which effectively allowed web sites to send regular updates of information (weather, stock updates, package tracking, etc.) directly to a user's desktop (aka "webtop"); Netscape's implementation of this was named Netcaster. Unfortunately, businesses quickly recognized the use of push technology to deliver ads to users, and annoyed users turned off the feature, so Netcaster was short-lived.
Netscape was notable for its cross-platform efforts. Its client software continued to be made available for Windows (3.1, 95, 98, NT), Macintosh, Linux, OS/2, BeOS, and many versions of Unix including DEC, Sun Solaris, BSDI, IRIX, IBM AIX, and HP-UX. Its server software generally was only available for Unix and Windows NT, though some of its servers were made available on Linux, and a version of Netscape FastTrack Server was made available for Windows 95/98. Today, most of Netscape's server offerings live on as the Sun Java System, formerly under the Sun ONE branding. Although Netscape Browser 8 was Windows only, multi-platform support exists in the Netscape Navigator 9 series of browsers.
Current services.
Netscape Internet Service.
Netscape ISP is a 56 kbit/s dial-up service offered at US$9.95 per month (US$6.95 with 12-month commitment). The company serves webpages in a compressed format to increase effective speeds up to 1300 kbit/s (average 500 kbit/s). The Internet service provider is run by AOL under the Netscape brand. The low-cost ISP was officially launched on January 8, 2004. Its main competitor is NetZero. Netscape ISP's advertising is generally aimed at a younger demographic, e.g., college students, and people just out of school, as an affordable way to gain access to the Internet.
Web Accelerator.
The Web Accelerator precompresses text at the Server side to approximately 4% its original size, increasing effective throughput to 1300 kbit/s. The accelerator also precompresses Flash executables and images to approximately 30% and 10%, respectively. Netscape advertises this as "DSL speeds over regular phone lines", although such speeds are limited to only web browsing, not downloads of files.
Another drawback of this approach is a loss in quality, where the graphics become heavily compacted and smeared, but the speed is dramatically improved such that web pages load in less than 5 seconds.
Netscape.com.
Netscape always drove lots of traffic from various links included in the browser menus to its web properties. Some say it was very late to leverage this traffic for what would become the start of the major online portal wars. When it did, Netcenter, the new name for its site entered the race with Yahoo!, Infoseek, and MSN, which Google would only join years later.
The original Netscape.com was discontinued in June 2006, replaced by the site that would eventually become Propeller.com. Two continuations of the original Netscape.com portal are available; Compuserve.com, the Web site of Compuserve, and ISP.Netscape.com, the web site for Netscape's dial-up discount ISP service, continue to use the Netscape.com layout as it was before June 2006. Of the two, only the latter explicitly uses the Netscape branding.
Netscape.com is currently an AOL Netscape-branded mirror duplicate of the AOL.com portal with the URL, replacing the former social news website in September 2007. The social news site moved to the Propeller.com domain, where it stayed until ending operations in October 2010. It features facilities such as news, sports, horoscopes, dating, movies, music and more. The change has come to much criticism amongst many site users, because the site has effectively become an AOL clone, and simply re-directs to regional AOL portals in some areas across the globe. Netscape's exclusive features, such as the Netscape Blog, Netscape NewsQuake, Netscape Navigator, My Netscape and Netscape Community pages, are less accessible from the AOL Netscape designed portal and in some countries not accessible at all without providing a full URL or completing an Internet search. The new AOL Netscape site was originally previewed in August 2007 before moving the existing site in September 2007.
Netscape.co.uk now redirects to AOL Search, with no Netscape branding at all.
DMOZ.
DMOZ (from directory.mozilla.org, its original domain name, also known as the Open Directory Project or ODP), is a multilingual open content directory of World Wide Web links owned by Netscape that is constructed and maintained by a community of volunteer editors.
Netscape Forum Center.
Netscape also has a wide variety of community-based forums within Netscape Forum Center, including its browser's community support board. To post on the forums, users must possess an AOL Screenname account in which to sign in, referred to within the site as the "Netscape Network". The same service is also available through Compuserve Forum Center.

</doc>
<doc id="21980" url="http://en.wikipedia.org/wiki?curid=21980" title="Newfoundland and Labrador">
Newfoundland and Labrador

Newfoundland and Labrador (, French: "Terre-Neuve-et-Labrador") is the most easterly province of Canada. Situated in the country's Atlantic region, it incorporates the island of Newfoundland and mainland Labrador to the northwest, with a combined area of 405212 km2. In 2013, the province's population was estimated at 526,702. Approximately 92 percent of the province's population lives on the island of Newfoundland (including its associated smaller islands), of which more than half live on the Avalon Peninsula. The province is Canada's most linguistically homogenous, with 97.6% of residents reporting English (Newfoundland English) as their mother tongue in the 2006 census. Historically, Newfoundland was also home to unique varieties of French and Irish, as well as the extinct Beothuk language. In Labrador, local dialects of Innu-aimun and Inuktitut are also spoken.
Newfoundland and Labrador's capital and largest city, St. John's, is Canada's 20th-largest census metropolitan area, and is home to almost 40 percent of the province's population. St. John's is the seat of government, home to the House of Assembly of Newfoundland and Labrador and the highest court in the jurisdiction, the Newfoundland and Labrador Court of Appeal.
A former colony and dominion of the United Kingdom, Newfoundland and Labrador became the tenth province to enter the Canadian Confederation on March 31, 1949, as "Newfoundland". On December 6, 2001, an amendment was made to the Constitution of Canada to change the province's official name to "Newfoundland and Labrador". In day-to-day conversation, however, Canadians generally still refer to the province itself as Newfoundland and to the region on the Canadian mainland as Labrador.
Etymology.
The name "Newfoundland" is derived from English as "New Found Land" (a translation from the Portuguese "Terra Nova", still reflected in the province's French language name, "Terre-Neuve"). The origin of "Labrador" is credited to João Fernandes Lavrador, the Portuguese navigator who explored the region.
Geography.
Newfoundland and Labrador is the most easterly province in Canada, and is located on the north-eastern corner of North America. The Strait of Belle Isle separates the province into two geographical divisions, Labrador, which is a large land mass connected to mainland Canada, and Newfoundland, which is an island in the Atlantic Ocean. The province also includes over 7,000 tiny islands.
Newfoundland is roughly triangular, with each side being approximately 400 km, and has an area of 108860 km2. Newfoundland and its associated small islands have a total area of 111390 km2. Newfoundland extends between latitudes 46°36′N and 51°38′N.
Labrador is an irregular shape: the western part of its border with Quebec is the drainage divide of the Labrador Peninsula. Lands drained by rivers that flow into the Atlantic Ocean are part of Labrador, the rest belong to Quebec. Labrador's extreme northern tip, at 60°22′N, shares a short border with Nunavut. Labrador's area (including associated small islands) is 294330 km2. Together, Newfoundland and Labrador make up 4.06% of Canada's area.
Labrador is the easternmost part of the Canadian Shield, a vast area of ancient metamorphic rock comprising much of northeastern North America. Colliding tectonic plates have shaped much of the geology of Newfoundland. Gros Morne National Park has a reputation as an outstanding example of tectonics at work, and as such has been designated a World Heritage Site. The Long Range Mountains on Newfoundland's west coast are the northeasternmost extension of the Appalachian Mountains.
The north-south extent of the province (46°36′N to 60°22′N), prevalent westerly winds, cold ocean currents and local factors such as mountains and coastline combine to create the various climates of the province. Northern Labrador is classified as a polar tundra climate, southern Labrador has a subarctic climate while most of Newfoundland would be humid continental climate, Dfb: Cool summer subtype.
Climate.
Newfoundland and Labrador is home to a variety of climates and weather. One of the main reasons for this diversity is the geography of the province. The island portion of the province spans 5 degrees of latitude, which is comparable to that of the Great Lakes.
The province has been divided into six climate types, but in broader terms Newfoundland has a cool summer subtype of a humid continental climate, which is greatly influenced by the sea since no part of the island is more than 100 km (62 mi) from the ocean. Northern Labrador is classified as a polar tundra climate, southern Labrador has a subarctic climate.
Monthly average temperatures, rainfall and snowfall for four communities are shown in the attached graphs. St. John's represents the east coast, Gander the interior of the island, Corner Brook the west coast of the island and Wabush the interior of Labrador. The detailed information and information for 73 communities in the province is available from a government website. The data used in making the graphs is the average taken over thirty years. Error bars on the temperature graph indicate the range of daytime highs and night time lows. Snowfall is the total amount that fell during the month, not the amount accumulated on the ground. This distinction is particularly important for St. John's where a heavy snowfall can be followed by rain so that no snow remains on the ground.
Surface water temperatures on the Atlantic side reach a summer average of 12 °C inshore and 9 °C offshore to winter lows of -1 °C inshore and 2 °C offshore. Sea temperatures on the west coast are warmer than Atlantic side by 1 to 3 °C (1 to 5 °F). The sea keeps winter temperatures slightly higher and summer temperatures a little lower on the coast than at places inland. The maritime climate produces more variable weather, ample precipitation in a variety of forms, greater humidity, lower visibility, more clouds, less sunshine, and higher winds than a continental climate.
History.
Pre-colonisation.
Human habitation in Newfoundland and Labrador can be traced back about 9,000 years. The Maritime Archaic peoples were groups of Archaic cultures of sea-mammal hunters in the subarctic. They prospered from approximately 7,000 BC to 1,500 BC along the Atlantic Coast of North America. Their settlements included longhouses and boat-topped temporary or seasonal houses. They engaged in long-distance trade, using as currency white chert, a rock quarried from northern Labrador to Maine. The southern branch of these people was established on the north peninsula of Newfoundland by 5,000 years ago. Maritime Archaic period is best known from a mortuary site in Newfoundland at Port au Choix.
The Maritime Archaic peoples were gradually displaced by people of the Dorset Culture (Late Paleo-Eskimo) who also occupied Port au Choix. The number of their sites discovered on Newfoundland indicate they may have been the most numerous group of Aboriginal people to live there. They thrived from about 2000 BC to 1,200 years ago. Many of their sites were located on exposed headlands and outer islands. They were more oriented to the sea than earlier peoples, and had developed sleds and boats similar to kayaks. They could burn seal blubber in soapstone lamps.
"Many of these sites, such as Port au Choix, recently excavated by Memorial archaeologist, Priscilla Renouf, are quite large and show evidence of a long-term commitment to place. Renouf has excavated huge amounts of harp seal bones at Port au Choix, indicating that this place was a prime location for the hunting of these animals."
The Dorset Culture (800 BC – 1500) were highly adapted to living in a very cold climate, and much of their food came from hunting sea mammals through holes in the ice. The massive decline in sea-ice which the Medieval Warm Period produced would have had a devastating impact upon their way of life.
The appearance of the Beothuk culture is believed the most recent cultural manifestation of peoples who first migrated from Labrador to Newfoundland around 1 AD. The Inuit, found mostly in Labrador, are the descendants of what anthropologists call the Thule culture, who emerged from western Alaska around 1000 AD and spread eastwards across the High Arctic, reaching Labrador around 1300–1500. Researchers believe that the Dorset culture lacked the dogs, larger weapons and other technologies that gave the expanding Inuit society an advantage. With the passage of time, groups started to focus on resources available to them locally.
The inhabitants eventually organised themselves into small bands of a few families, grouped into larger tribes and chieftainships. The Innu are the inhabitants of an area they refer to as "Nitassinan", which comprises most of what is now referred to as northeastern Quebec and Labrador. Their subsistence activities were historically centred on hunting and trapping caribou, deer and small game. Coastal clans also practised agriculture, fished and managed maple sugarbush. The Innu engaged in tribal warfare along the coast of Labrador with the Inuit groups that had significant populations.
The Míkmaq of southern Newfoundland spent most of their time on the shores harvesting seafood; during the winter they would move inland to the woods to hunt. Over time, the Mi'kmaq and Innu divided their lands into traditional "districts". Each district was independently governed and had a district chief and a council. The council members were band chiefs, elders and other worthy community leaders. In addition to the district councils, the Mi'kmaq tribes also had (have) a Grand Council or "Santé Mawiómi," which according to oral tradition was formed before 1600.
European contact.
The oldest confirmed accounts of European contact date from a thousand years ago as described in the Viking (Norse) Icelandic Sagas. Around the year 1001, the sagas refer to Leif Ericson landing in three places to the west, the first two being Helluland (possibly Baffin Island) and Markland (possibly Labrador). Leif's third landing was at a place he called Vinland (possibly Newfoundland). Archaeological evidence of a Norse settlement was found in L'Anse aux Meadows, Newfoundland, which was declared a World Heritage site by UNESCO in 1978.
There are several other unconfirmed accounts of European discovery and exploration. One tale by men from the Channel Islands being blown off course in the late 15th century into a strange land full of fish, and another from Portuguese maps that depict the Terra do Bacalhau, or land of codfish, west of the Azores. The earliest, though, is the Voyage of Saint Brendan, the fantastical account of an Irish monk who made a sea voyage in the early 6th century. While the story itself became a part of myth and legend, some historians believe it is based on fact.
In 1496 John Cabot obtained a charter from English King Henry VII to "sail to all parts, countries and seas of the East, the West and of the North, under our banner and ensign and to set up our banner on any new-found-land" and on June 24, 1497, landed in Cape Bonavista. Historians disagree on whether Cabot landed in Nova Scotia in 1497 or in Newfoundland, or possibly Maine, if he landed at all, but Bonavista is recognised by the governments of Canada and the United Kingdom as being Cabot's "official" landing place. In 1499 and 1500, Portuguese mariners João Fernandes Lavrador and Pêro de Barcelos explored and mapped the coast, the former's name appearing as "Labrador" on topographical maps of the period. Based on the Treaty of Tordesillas, the Portuguese Crown claimed it had territorial rights in the area visited by John Cabot in 1497 and 1498. Subsequently, in 1501 and 1502 the Corte-Real brothers explored Newfoundland and Labrador, claiming them as part of the Portuguese Empire. In 1506, king Manuel I of Portugal created taxes for the cod fisheries in Newfoundland waters. João Álvares Fagundes and Pêro de Barcelos established seasonal fishing outposts in Newfoundland and Nova Scotia around 1521, and older Portuguese settlements may have existed. Sir Humphrey Gilbert, provided with letters patent from Queen Elizabeth I, landed in St John's in August 1583, and formally took possession of the island.
Colony of Newfoundland.
In 1583 Newfoundland became England's first possession in North America and one of the earliest permanent English colonies in the New World when it was claimed by Sir Humphrey Gilbert for Queen Elizabeth. Though English fishing boats had visited Newfoundland continuously since Cabot's second voyage in 1498 and seasonal fishing camps had existed for a century prior, similar was true of Basque, French, and Portuguese ships and camps, thus pressure to secure the island from foreign control led to the appointment of Proprietary Governors to establish colonial settlements on the island from 1610 to 1728. John Guy was governor of the first settlement at Cuper's Cove. Other settlements were Bristol's Hope, Renews, New Cambriol, South Falkland and Avalon which became a province in 1623. The first governor given jurisdiction over all of Newfoundland was Sir David Kirke in 1638.
Explorers soon realized that the waters around Newfoundland had the best fishing in the North Atlantic. By 1620, 300 fishing boats worked the Grand Bank, employing some 10,000 sailors; many continuing to come from Basque Country, Normandy, or Brittany. They dried and salted the cod on the coast and sold it to Spain and Portugal. Heavy investment by Sir George Calvert, 1st Baron Baltimore, in the 1620s in wharves, warehouses, and fishing stations failed to pay off. French raids hurt the business, and the weather was terrible, so he redirected his attention to his other colony in Maryland. After Calvert left, small-scale entrepreneurs such as Sir David Kirke made good use of the facilities. Kirke became the first governor in 1639. A triangular trade with New England, the West Indies, and Europe gave Newfoundland an important economic role. By the 1670s there were 1700 permanent residents and another 4500 in the summer months.
Basque fishermen, who had been fishing cod shoals off Newfoundland's coasts since the beginning of the sixteenth century, founded Plaisance (today Placentia), a haven which started to be also used by French fishermen. In 1655, France appointed a governor in Plaisance, thus starting a formal French colonization period of Newfoundland as well as a period of periodic war and unrest between England and France. The Mi'kmaq, as allies with the French, were amenable to limited French settlement in their midst and fought with them against the English. English attacks on Placentia provoked retaliation by New France explorer Pierre Le Moyne d'Iberville who during King William's War in the 1690s destroyed nearly every English settlement on the island. The entire population of the English colony was either killed, captured for ransom, or sentenced to expulsion to England, with the exception of the those who withstood the attack at Carbonear Island and those in the then remote Bonavista. After France lost political control of the area after the Siege of Port Royal in 1710, the Mí'kmaq engaged in warfare with the British throughout Dummer's War, King George's War, Father Le Loutre's War and the French and Indian War. The French colonization period lasted until the Treaty of Utrecht, in 1713, which ended the War of the Spanish Succession and France ceded its claims to Newfoundland to the British (as well as its claims to the shores of Hudson Bay) and to the French possessions in Acadia. Afterward, under the supervision of the last French governor, the French population of Plaisance moved to Île Royale (now Cape Breton Island), part of Acadia which remained then under French control.
In the Treaty of Utrecht (1713), France had acknowledged British ownership of the island. However, in the Seven Years' War (1756–63), control of Newfoundland once again became a major source of conflict between Britain, France and Spain who all pressed for a share in the valuable fishery there. Britain's victories around the globe led William Pitt to insist that nobody other than Britain should have access to Newfoundland. The Battle of Signal Hill was fought in Newfoundland in 1762, when a French force landed and tried to occupy the island, only to be repulsed by the British.
From 1763 to 1767 James Cook made a detailed survey of the coasts of Newfoundland and southern Labrador, while he was commander of the HMS "Grenville". The following year, 1768, Cook began is his first circumnavigation of the world. In 1796 a Franco-Spanish expedition again succeeded in raiding the coasts of Newfoundland and Labrador destroying many of the settlements.
By the Treaty of Utrecht (1713), French fishermen were given the right to land and cure fish on the "French Shore" on the western coast. They had a permanent base on nearby St. Pierre and Miquelon islands; the French gave up their rights in 1904. In 1783, the British signed the Treaty of Paris with the United States that gave American fishermen similar rights along the coast. These rights were reaffirmed by treaties in 1818, 1854 and 1871 and confirmed by arbitration in 1910.
In 1854 the British government established Newfoundland's responsible government. In 1855, Philip Francis Little, a native of Prince Edward Island, won a parliamentary majority over Sir Hugh Hoyles and the Conservatives. Little formed the first administration from 1855 to 1858. Newfoundland rejected confederation with Canada in the 1869 general election. Prime Minister of Canada Sir John Thompson came very close to negotiating Newfoundland's entry into Confederation in 1892.
Dominion of Newfoundland.
Newfoundland remained a colony until acquiring Dominion status in 1907. A dominion constituted a self-governing state of the British Empire or British Commonwealth and the Dominion of Newfoundland was relatively autonomous from British rule.
Newfoundland's own regiment, the 1st Newfoundland Regiment, fought in the First World War. On July 1, 1916, the German Army wiped out nearly the entire regiment at Beaumont Hamel on the first day on the Somme. The regiment went on to serve with distinction in several subsequent battles, earning the prefix "Royal". Despite people's pride in the accomplishments of the regiment, the Dominion's war debt due to the regiment and the cost of maintaining a trans-island railway led to increased and ultimately unsustainable government debt in the post-war era.
Since the early 1800s, Newfoundland and Quebec (or Lower Canada) had been in a border dispute over the Labrador region. In 1927, however, the British government ruled that the area known as modern day Labrador was to be considered part of the Dominion of Newfoundland.
Commission of Government and confederation with Canada.
Due to Newfoundland's high debt load, arising from World War I and construction of the Newfoundland railroad, and decreasing revenue, due to the collapse of fish prices, the Newfoundland legislature voted itself out of existence in 1933, in exchange for loan guarantees by the Crown and a promise it would be re-established.:8–10 On February 16, 1934, the Commission of Government was sworn in, ending 79 years of responsible government. The Commission consisted of seven persons appointed by the British government. For 15 years no elections took place, and no legislature was convened.
When prosperity returned with World War II, agitation began to end the Commission, and reinstate responsible government. But, the British government created the National Convention in 1946, reflecting efforts in self-determination among European nationalities that followed WWII. The Convention, made of up representatives from throughout the country, was formally tasked to advise on the future of Newfoundland. Chaired by Judge Cyril J. Fox, it consisted of 45 elected members from across the province.
Several motions were made by Joey Smallwood (a member of the convention who later was elected as the first premier of Newfoundland) to examine joining Canada by sending a delegation to Ottawa. The first motion was defeated, although the Convention later decided to send delegations to both London and Ottawa to explore alternatives. In January 1948, the National Convention voted against putting Confederation onto the referendum 29 to 16; but, this vote was overruled by the British, which controlled the National Convention and the subsequent referendum.:145 Those who supported Confederation were extremely disappointed with the recommendations of the National Convention and organized a petition which more than 50,000 Newfoundlanders signed demanding that Confederation with Canada be placed before the people in the upcoming referendum. As most historians agree, the British government keenly wanted Confederation on the ballot and they made sure that it was.
Three main factions actively campaigned during the leadup to the referendums. Smallwood led the Confederate Association (CA), advocating union with the Canadian Confederation. They campaigned through a newspaper known as "The Confederate". The Responsible Government League (RGL), led by Peter Cashin, advocated an independent Newfoundland with a return to responsible government. Their newspaper was "The Independent". A third, smaller Economic Union Party (EUP), led by Chesley Crosbie, advocated closer economic ties with the United States. The EUP failed to gain much attention, and merged with the RGL after the first referendum.
The first referendum took place on June 3, 1948; 44.5% of people voted for responsible government, 41.1% voted for confederation with Canada, while 14.3% voted for Commission of Government. Since none of the choices had gained over 50%, a second referendum with only the two more popular choices was held on July 22, 1948. The official outcome of that referendum was 52.3% for confederation with Canada and 47.7% for responsible (independent) government.
After the referendum, a seven-man delegation was picked by the British governor to negotiate Canada's offer on behalf of Newfoundland. After six of the seven-man delegation signed, the British Government passed the British North America Act, 1949 through Parliament. Newfoundland officially joined Canada at midnight, March 31, 1949.
As documents in British and Canadian archives came available in the 1980s, it became clear that both Canada and the United Kingdom wanted Newfoundland to join Canada. Some have charged that it was a conspiracy to manoeuvre Newfoundland into Confederation, in exchange for forgiveness of Britain's war debt and for other considerations,:68 but most historians who have examined the government documents have concluded that while Britain engineered the inclusion of Confederation in the referendum, Newfoundlanders made the final decision themselves although it was a close vote.
Subsequent to the referendum, there has been hearsay that the referendum was narrowly won by the "responsible government" side, but the result was fixed by the British governor.:225–26 The governor ordered the ballots from the referendum to be burned shortly afterward. Some have argued that independent oversight of the vote tallying was lacking but the process was supervised by respected Corner Brook Magistrate Nehemiah Short who has also overseen elections to the National Convention.:224–25
Provincial flag.
Newfoundland and Labrador's present provincial flag, designed by Newfoundland artist Christopher Pratt, was officially adopted by the legislature on May 28, 1980, and first flown on "Discovery Day" that year. Labrador has its own unofficial flag, created in 1973 by Mike Martin, former Member of the Legislative Assembly for Labrador South.
The blue is meant to represent the sea, the white represents snow and ice, the red represents the efforts and struggles of the people, and the gold represents the confidence of Newfoundlanders and Labradorians. The blue triangles are a tribute to the Union Flag, and represent the British heritage of the province. The two red triangles represent Labrador (the mainland portion of the province) and the island. The golden arrow is a union jack, placed on the flag as a reminder that Newfoundland was once owned by Britain. In Pratt's words, the union jack points towards a "brighter future".
The "Pink, White and Green" is a flag of 19th-century origins that enjoyed popularity in portions of the island in the late 19th century. It was flown on some vessels into the 20th century. It was never adopted by the Newfoundland government.
A 1976 article reported that the tricolour flag was created in 1843 by then Roman Catholic Bishop of Newfoundland, Michael Anthony Fleming. The colours were intended to represent the symbolic union of Newfoundland's historically dominant ethnic/religious groups: English, Scottish and Irish. Though popular, there is no historical evidence to support this legend.
Recent scholarship suggests that the flag was first used in the 1870s or later by the Roman Catholic "Star of the Sea" fishermen's association. It resembled the unofficial flag of Ireland. The tri-colour flag remained relatively unknown outside of St. John's and the Avalon peninsula until the growth of the tourist industry since the late 20th century. It has been used as an emblem on items in gift shops in St. John's and other towns. Some tourists assume it is the Irish flag.
The "Pink, White and Green" has been adopted by some residents as a symbol of ties with Irish heritage and as a political statement. Many of the province's Protestants, who make up approximately 60% of the province's total population (with 57% claiming British Isles descent), consider it a Catholic flag.
Similarly, many of the province's Catholics, approximately 37% of the total population (with roughly 22% of the population claiming Irish ancestry), think that the current provincial flag does not satisfactorily represent them. But, a government-sponsored poll in 2005 revealed that 75% of Newfoundlanders rejected adoption of the Tricolour flag as the province's official flag.
Demographics.
Newfoundland and Labrador has a population of 514,536, more than half of whom live on the Avalon Peninsula of Newfoundland, site of the capital and historical early settlement. Since 2006, the population of the province has started to increase for the first time since the early 1990s. In the 2006 census the population of the province decreased by 1.5% compared to 2001, and stood at 505,469. But, by the 2011 census, the population had risen by 1.8%.
The largest single religious denomination by number of adherents according to the 2011 National Household Survey was the Roman Catholic Church, at 35.8% of the province's population (181,590 members). The major Protestant denominations made up 57.3% of the population, with the largest groups being the Anglican Church of Canada at 25.1% of the total population (127,255 members), the United Church of Canada at 15.5% (78,380 members), and the Pentecostal churches at 6.5% (33,195 members), with other Protestant denominations in much smaller numbers. Non-Christians comprised only 6.8% of the population, with the majority of those respondents indicating "no religious affiliation" (6.2% of the total population).
According to the 2001 Canadian census, the largest ethnic group in Newfoundland and Labrador is English (39.4%), followed by Irish (19.7%), Scottish (6.0%), French (5.5%), and First Nations (3.2%). While half of all respondents also identified their ethnicity as "Canadian," 38% report their ethnicity as "Newfoundlander" in a 2003 Statistics Canada Ethnic Diversity Survey.
Economy.
"All currency is in Canadian dollars."
For many years, Newfoundland and Labrador had experienced a depressed economy. Following the collapse of the cod fishery during the early 1990s, the province suffered record unemployment rates and the population decreased by roughly 60,000. Due to a major energy and resources boom, the provincial economy has had a major turnaround since the turn of the 21st century. Unemployment rates decreased, the population stabilized and had moderate growth. The province has gained record surpluses, which has rid it of its status as a "have not" province.
Economic growth, gross domestic product (GDP), exports and employment resumed in 2010, after suffering the impacts of the late-2000s recession. Total capital investment in the province grew to $6.2 billion, an increase of 23.0% compared to 2009. GDP reached $28.1 billion, compared to $25.0 billion in 2009.
Service industries accounted for the largest share of GDP, especially financial services, health care and public administration. Other significant industries are mining, oil production and manufacturing. The total workforce in 2010 was 263,800 people. Per capita GDP in 2008 was $61,763, higher than the national average and third only to Alberta and Saskatchewan out of Canadian provinces.
Mines in Labrador, the iron ore mine at Wabush/Labrador City, and the nickel mine in Voisey's Bay produced a total of $3.3 billion worth of ore in 2010. A mine at Duck Pond (30 km (18 mi) south of the now-closed mine at Buchans), started producing copper, zinc, silver and gold in 2007, and prospecting for new ore bodies continues. Mining accounted for 3.5% of the provincial GDP in 2006. The province produces 55% of Canada's total iron ore. Quarries producing dimension stone such as slate and granite, account for less than $10 million worth of material per year. Oil production from offshore oil platforms on the Hibernia, White Rose and Terra Nova oil fields on the Grand Banks was of 110000000 oilbbl, which contributed to more than 15% of the province's GDP in 2006. Total production from the Hibernia field from 1997 to 2006 was 733000000 oilbbl with an estimated value of $36 billion. This will increase with the inclusion of the latest project, Hebron. Remaining reserves are estimated at almost 2 Goilbbl as of December 31, 2006. Exploration for new reserves is ongoing.
On June 16, 2009, provincial premier Danny Williams announced a tentative agreement to expand the Hibernia Oil Field. The government negotiated a 10-per-cent equity stake in the Hibernia South expansion, which will add an estimated $10 billion to Newfoundland and Labrador's treasury.
The fishing industry remains an important part of the provincial economy, employing roughly 20,000 and contributing over $440 million to the GDP. The combined harvest of fish such as cod, haddock, halibut, herring and mackerel was 150,000 tonnes (165,000 tons) valued at about $130 million in 2006. Shellfish, such as crab, shrimp and clams, accounted for 195,000 tonnes (215,000 tons) with a value of $316 million in the same year. The value of products from the seal hunt was $55 million. Aquaculture is a new industry for the province, which in 2006 produced over 10,000 tonnes of Atlantic salmon, mussels and steelhead trout worth over $50 million.
Newsprint is produced by one paper mill in Corner Brook with a capacity of 420,000 tonnes (462,000 tons) per year. The value of newsprint exports varies greatly from year to year, depending on the global market price. Lumber is produced by numerous mills in Newfoundland.
Apart from seafood processing, paper manufacture and oil refining, manufacturing in the province consists of smaller industries producing food, brewing and other beverage production.
Agriculture in Newfoundland is limited to areas south of St. John's, Cormack, Wooddale, areas near Musgravetown and in the Codroy Valley. Potatoes, rutabagas, turnips, carrots and cabbage are grown for local consumption. Poultry and eggs are also produced. Wild blueberries, partridgeberries (lingonberries) and bakeapples (cloudberries) are harvested commercially and used in jams and wine making. Dairy production is another huge part of the Newfoundland Agriculture Industry.
Tourism is also a significant contributor to the province's economy. In 2006 nearly 500,000 non-resident tourists visited Newfoundland and Labrador, spending an estimated $366 million. Tourism is most popular throughout the months of June–September, the warmest months of the year with the longest hours of daylight.
Government and politics.
Newfoundland and Labrador is governed by a parliamentary government within the construct of constitutional monarchy; the monarchy in Newfoundland and Labrador is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces, and the Canadian federal realm, and resides predominantly in the United Kingdom. As such, the Queen's representative, the Lieutenant Governor of Newfoundland and Labrador (presently Frank Fagan), carries out most of the royal duties in Newfoundland and Labrador.
The direct participation of the royal and viceroyal figures in any of these areas of governance is limited; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected House of Assembly and chosen and headed by the Premier of Newfoundland and Labrador (presently Paul Davis), the head of government. To ensure the stability of government, the lieutenant governor will usually appoint as premier the person who is usually the current leader of the political party that can obtain the confidence of a plurality in the House of Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition (presently Dwight Ball) and is part of an adversarial parliamentary system intended to keep the government in check.
Each of the 48 Members of the House of Assembly (MHA) is elected by simple plurality in an electoral district. General elections must be called by the lieutenant governor on the second Tuesday in October four years after the previous election, or may be called, on the advice of the premier, should the government lose a confidence vote in the legislature. Traditionally, politics in the province have been dominated by both the Liberal Party and the Progressive Conservative Party. However, in the 2011 provincial election the New Democratic Party, which had only ever attained minor success, had a major breakthrough and placed second in the popular vote behind the Progressive Conservatives.
Culture.
Music.
Newfoundland and Labrador has a folk musical heritage based on the Irish, English and Scottish traditions that were brought to its shores centuries ago. Though similar in its Celtic influence to neighbouring Nova Scotia and Prince Edward Island, Newfoundland and Labrador are more Irish than Scottish, and have more elements imported from English and French music than those provinces. Much of the region's music focuses on the strong seafaring tradition in the area, and includes sea shanties and other sailing songs. Some modern traditional musicians include Great Big Sea, The Ennis Sisters, Shanneyganock, Sharecroppers, Ron Hynes, and The Navigators.
Sports.
Newfoundland and Labrador has a somewhat different sports culture than the rest of Canada, owing in part to its long history separate from the rest of Canada and under British rule. Ice hockey, however, remains popular; the St. John's IceCaps play professionally at the Mile One Centre in St. John's, and the Newfoundland Senior Hockey League has teams around the island. Since the departure of the St. John's Fog Devils in 2008, Newfoundland and Labrador is the only province in Canada to not have a team in the Canadian Hockey League (should one ever join it would be placed in the QMJHL, which hosted the Fog Devils and has jurisdiction over Atlantic Canada).
Association football (soccer) and rugby union are both more popular in Newfoundland and Labrador than the rest of Canada in general. Soccer is hosted at King George V Park, a 10,000-seat stadium built as Newfoundland's national stadium during the time as an independent dominion. Swilers Rugby Park hosts the Swilers RFC in rugby union. Other sports facilities in Newfoundland and Labrador include Pepsi Centre, an indoor arena in Corner Brook; Shamrock Field, Canada's national Gaelic Games venue in St. John's; and St. Patrick's Park, a baseball park in St. John's.
Gridiron football, be it either American or Canadian, is practically nonexistent; it is the only Canadian province other than Prince Edward Island to have never hosted a Canadian Football League or Canadian Interuniversity Sport game; unlike Prince Edward Island, Newfoundland and Labrador also lacks an amateur team.
Transportation.
Within the province, the Newfoundland and Labrador Department of Transportation and Works operates or sponsors 15 automobile, passenger and freight ferry routes which connect various communities along the province's significant coastline.
A regular passenger and car ferry service, lasting about 90 minutes, crosses the Strait of Belle Isle, connecting the province's island of Newfoundland with the region of Labrador on the mainland. The ferry "MV Apollo" travels from St. Barbe, Newfoundland on the Great Northern Peninsula to the port town of Blanc-Sablon, Quebec, located on the provincial border and beside the town of L'Anse-au-Clair, Labrador. The "MV Sir Robert Bond" once provided seasonal ferry service between Lewisporte on the island and the towns of Cartwright and Happy Valley-Goose Bay in Labrador, but has not ran since the completion of the Trans Labrador highway in 2010, allowing access from Blanc-Sablon, Quebec to major parts of Labrador. Several smaller ferries connect numerous other coastal towns and offshore island communities around the island of Newfoundland and up the Labrador coast as far north as Nain.
Inter-provincial ferry services are provided by Marine Atlantic, a federal Crown corporation which operates auto-passenger ferries from North Sydney, Nova Scotia to the towns of Port aux Basques and Argentia on the southern coast of Newfoundland island.
The St. John's International Airport YYT and the Gander International Airport YQX are the only airports in the province that are part of the National Airports System. The St. John's International Airport handles nearly 1,200,000 passengers a year making it the busiest airport in the province and the eleventh busiest airport in Canada. The airport is currently undergoing a major expansion of the terminal building which is scheduled to be complete in 2021. The Deer Lake Airport YDF handles over 300,000 passengers a year.
Further reading.
</dl>

</doc>
<doc id="21981" url="http://en.wikipedia.org/wiki?curid=21981" title="New Oxford American Dictionary">
New Oxford American Dictionary

The New Oxford American Dictionary (NOAD) is a single-volume dictionary of American English compiled by American editors at the Oxford University Press.
"NOAD" is based upon the "New Oxford Dictionary of English" ("NODE"), published in the United Kingdom in 1998, although with substantial editing, additional entries, and the inclusion of illustrations. It is based on a corpus linguistics analysis of Oxford's 200 million word database of contemporary American English.
"NOAD" includes a diacritical respelling scheme to convey pronunciations, as opposed to the Gimson phonemic IPA system that is used in "NODE".
Editions.
First.
Published in September 2001, the first edition was edited by Elizabeth J. Jewell and Frank Abate.
Second edition.
Published in May 2005, the second edition was edited by Erin McKean. The edition added nearly 3,000 new words, senses, and phrases. It was in a large format, with 2096 pages, and was 8½" by 11" in size. It included a CD-ROM with the full text of the dictionary for Palm OS devices.
Since 2005 Apple Inc.'s Mac OS X operating system has come bundled with a dictionary application and widget which credits as its source "Oxford American Dictionaries", and contains the full text of "NOAD2". The Amazon Kindle reading device also uses "NOAD" as its built-in dictionary, along with a choice for the "Oxford Dictionary of English".
Oxford University Press published "NOAD2" in electronic form in 2006 at the OxfordAmericanDictionary.com, and in 2010, along with the "Oxford Dictionary of English", as part of Oxford Dictionaries Online.
Third edition.
Published in August 2010, the third edition was edited by Angus Stevenson and Christine A. Lindberg. The edition added more than 2,000 new words, senses, and phrases.
Fictitious entry.
The dictionary includes an entry for the word "esquivalience", which it defines as meaning "the willful avoidance of one's official responsibilities". This is a fictitious entry, intended to protect the copyright of the publication. The entry was invented by Christine Lindberg, one of the editors of the "NOAD". 
With the publication of the second edition, a rumor circulated that the dictionary contained a fictitious entry in the letter 'e'. "New Yorker" contributing editor Henry Alford combed the section, and discussed several unusual entries he found with a group of American lexicographers. Most found "esquivalience" to be the most likely candidate, and when Alford approached "NOAD" editor in chief Erin McKean she confirmed it was a fake entry, which had been present since the first edition, in order to protect the copyright of the CD-ROM edition. Of the word, she said "its inherent fakeitude is fairly obvious".
The fake entry apparently ensnared dictionary.com, which included an entry for it (that has since been removed) which it attributed to "Webster's New Millennium Dictionary", both of which are owned by the private company Lexico. Possibly due to its licensing of Oxford dictionaries, Google Dictionary included the word, listing three meanings and giving usage examples. Apple has also included the word in its dictionary, and incorrectly explains its etymology as "perhaps from French "esquiver ‘dodge, slink away."'"

</doc>
<doc id="21983" url="http://en.wikipedia.org/wiki?curid=21983" title="New Latin">
New Latin

New Latin or neo-Latin was used in original, scholarly, and scientific works between c. 1375 and c. 1900. Modern scholarly and technical usage, such as in zoological and botanical taxonomy and international scientific vocabulary, draws from New Latin vocabulary.
Extent.
Classicists use the term "neo-Latin" to describe the Latin that developed in Renaissance Italy as a result of renewed interest in classical civilization in the 14th and 15th centuries.
Neo-Latin also describes the use of the Latin language for any purpose, scientific or literary, during and after the Renaissance. The beginning of the period is imprecise; however, the spread of secular education, the acceptance of humanistic literary norms, and the wide availability of Latin texts following the invention of printing mark the transition to a new era of scholarship at the end of the 15th century. The end of the New Latin period is likewise indeterminate, but Latin as a regular vehicle of communicating ideas became rare after the first few decades of the 19th century, and by 1900 it survived primarily in international scientific vocabulary and taxonomy. The term "New Latin" came into widespread use towards the end of the 1890s among linguists and scientists.
New Latin was, at least in its early days, an international language used throughout Catholic and Protestant Europe, as well as in the colonies of the major European powers. This area consisted of most of Europe, including Central Europe and Scandinavia; its southern border was the Mediterranean Sea, with the division more or less corresponding to the modern eastern borders of Finland, the Baltic states, Poland, Slovakia, Hungary and Croatia.
Russia's acquisition of Kiev in the later 17th century introduced the study of Latin to Russia. Nevertheless the use of Latin in Orthodox eastern Europe did not reach high levels due to their strong cultural links to cultural heritage of Ancient Greece and Byzantium, as well as Greek and Old Church Slavonic languages.
In Turkey and parts of Russia, the Muslim population or Muslim minorities maintained close links with the Arabic language or script.
History of New Latin.
Beginnings.
New Latin was inaugurated by the triumph of the humanist reform of Latin education, led by such writers as Erasmus, More, and Colet. Medieval Latin had been the practical working language of the Roman Catholic Church, taught throughout Europe to aspiring clerics and refined in the medieval universities. It was a flexible and living language, full of neologisms and often composed without reference to the grammar or style of classical (usually pre-Christian) authors. While accepting many of the strengths of Medieval Latin, the humanist reformers sought both to purify Latin grammar and style, and to make Latin applicable to concerns beyond the ecclesiastical, creating a body of Latin literature outside the bounds of the Church. Attempts at reforming Latin use occurred sporadically throughout the period, becoming most successful in the mid-to-late 19th century.
Height.
The Protestant Reformation (1520–1580), though it removed Latin from the liturgies of the churches of Northern Europe, may have advanced the cause of the new secular Latin. The period during and after the Reformation, coinciding with the growth of printed literature, saw the growth of an immense body of New Latin literature, on all kinds of secular as well as religious subjects.
The heyday of New Latin was its first two centuries (1500–1700), when in the continuation of the Medieval Latin tradition, it served as the lingua franca of science, education, and to some degree diplomacy in Europe. Classic works such as Newton's Principia Mathematica (1687) were written in the language. Throughout this period, Latin was a universal school subject, and indeed, the pre-eminent subject for elementary education in most of Europe and other places of the world that shared its culture. All universities required Latin proficiency (obtained in local grammar schools) to obtain admittance as a student. Latin was an official language of Poland - recognised and widely used between 9th and 18th centuries commonly used in foreign relations and popular as a second language among some of the nobility.
Through most of the 17th century, Latin was also supreme as an international language of diplomatic correspondence, used in negotiations between nations and the writing of treaties, e.g. the peace treaties of Osnabrück and Münster (1648). As an auxiliary language to the local vernaculars, New Latin appeared in a wide variety of documents, ecclesiastical, legal, diplomatic, academic, and scientific. While a text written in English, French, or Spanish at this time might be understood by a significant cross section of the learned, only a Latin text could be certain of finding someone to interpret it anywhere between Lisbon and Helsinki. 
As late as the 1720s, Latin was still used conversationally, and was serviceable as an international auxiliary language between people of different countries who had no other language in common. For instance, the Hanoverian king George I of Great Britain (reigned 1714–1727), who had no command of spoken English, communicated in Latin with his Prime Minister Robert Walpole, who knew neither German nor French.
Decline.
By about 1700, the growing movement for the use of national languages (already found earlier in literature and the Protestant religious movement) had reached academia, and an example of the transition is Newton's writing career, which began in New Latin and ended in English (e.g. "Opticks", 1704). A much earlier example is Galileo c. 1600, some of whose scientific writings were in Latin, some in Italian, the latter to reach a wider audience. By contrast, while German philosopher Christian Wolff (1679–1754) popularized German as a language of scholarly instruction and research, and wrote some works in German, he continued to write primarily in Latin, so that his works could more easily reach an international audience (e.g., "Philosophia moralis," 1750–53).
Likewise, in the early 18th century, French replaced Latin as a diplomatic language, due to the commanding presence in Europe of the France of Louis XIV. At the same time, some (like King Frederick William I of Prussia) were dismissing Latin as a useless accomplishment, unfit for a man of practical affairs. The last international treaty to be written in Latin was the Treaty of Vienna in 1738; after the War of the Austrian Succession (1740–48) international diplomacy was conducted predominantly in French.
A diminishing audience combined with diminishing production of Latin texts pushed Latin into a declining spiral from which it has not recovered. As it was gradually abandoned by various fields, and as less written material appeared in it, there was less of a practical reason for anyone to bother to learn Latin; as fewer people knew Latin, there was less reason for material to be written in the language. Latin came to be viewed as esoteric, irrelevant, and too difficult. As languages like French, German, and English became more widely known, use of a 'difficult' auxiliary language seemed unnecessary—while the argument that Latin could expand readership beyond a single nation was fatally weakened if, in fact, Latin readers did not compose a majority of the intended audience. 
As the 18th century progressed, the extensive literature in Latin being produced at the beginning slowly contracted. By 1800 Latin publications were far outnumbered, and often outclassed, by writings in the vernacular. Latin literature lasted longest in very specific fields (e.g. botany and zoology) where it had acquired a technical character, and where a literature available only to a small number of learned individuals could remain viable. By the end of the 19th century, Latin in some instances functioned less as a language than as a code capable of concise and exact expression, as for instance in physicians' prescriptions, or in a botanist's description of a specimen. In other fields (e.g. anatomy or law) where Latin had been widely used, it survived in technical phrases and terminology. The perpetuation of Ecclesiastical Latin in the Roman Catholic Church through the 20th century can be considered a special case of the technicalizing of Latin, and the narrowing of its use to an elite class of readers. 
By 1900, creative Latin composition, for purely artistic purposes, had become rare. Authors such as Arthur Rimbaud and Max Beerbohm wrote Latin verse, but these texts were either school exercises or occasional pieces. The last survivals of New Latin to convey non-technical information appear in the use of Latin to cloak passages and expressions deemed too indecent (in the 19th century) to be read by children, the lower classes, or (most) women. Such passages appear in translations of foreign texts and in works on folklore, anthropology, and psychology, e.g. Krafft-Ebing's "Psychopathia Sexualis" (1886).
Crisis and transformation.
Latin as a language held a place of educational pre-eminence until the second half of the 19th century. At that point its value was increasingly questioned; in the 20th century, educational philosophies such as that of John Dewey dismissed its relevance. At the same time, the philological study of Latin appeared to show that the traditional methods and materials for teaching Latin were dangerously out of date and ineffective. 
In secular academic use, however, New Latin declined sharply and then continuously after about 1700. Although Latin texts continued to be written throughout the 18th and into the 19th century, their number and their scope diminished over time. By 1900, very few new texts were being created in Latin for practical purposes, and the production of Latin texts had become little more than a hobby for Latin enthusiasts. 
Around the beginning of the 19th century came a renewed emphasis on the study of Classical Latin as the spoken language of the Romans of the 1st centuries BC and AD. This new emphasis, similar to that of the Humanists but based on broader linguistic, historical, and critical studies of Latin literature, led to the exclusion of Neo-Latin literature from academic studies in schools and universities (except for advanced historical language studies); to the abandonment of New Latin neologisms; and to an increasing interest in the reconstructed Classical pronunciation, which displaced the several regional pronunciations in Europe in the early 20th century. 
Coincident with these changes in Latin instruction, and to some degree motivating them, came a concern about lack of Latin proficiency among students. Latin had already lost its privileged role as the core subject of elementary instruction; and as education spread to the middle and lower classes, it tended to be dropped altogether. By the mid-20th century, even the trivial acquaintance with Latin typical of the 19th-century student was a thing of the past.
Relics.
Ecclesiastical Latin, the form of New Latin used in the Roman Catholic Church, remained in use throughout the period and after. Until the Second Vatican Council of 1962-65 all priests were expected to have competency in it, and it was studied in Catholic schools. It is today still the official language of the Church, and all Catholic priests of the Latin liturgical rites are required by canon law to have competency in the language, although most do not. Use of Latin in the Mass, largely abandoned through the later 20th century, has recently seen a resurgence, due in large part to Pope Benedict XVI's motu proprio Summorum Pontificum and its use by traditional Catholic priests and their organizations.
New Latin is also the source of the biological system of binomial nomenclature and classification of living organisms devised by Carolus Linnæus, although the rules of the ICZN allow the construction of names that deviate considerably from historical norms. See also classical compounds. Another continuation is the use of Latin names for the surface features of planets and planetary satellites (planetary nomenclature), originated in the mid-17th century for selenographic toponyms. New Latin has also contributed a vocabulary for specialized fields such as anatomy and law; some of these words have become part of the normal, non-technical vocabulary of various European languages.
Pronunciation.
New Latin had no single pronunciation, but a host of local variants or dialects, all distinct both from each other and from the historical pronunciation of Latin at the time of the Roman Republic and Roman Empire. As a rule, the local pronunciation of Latin used sounds identical to those of the dominant local language; the result of a concurrently evolving pronunciation in the living languages and the corresponding spoken dialects of Latin. Despite this variation, there are some common characteristics to nearly all of the dialects of New Latin, for instance:
The regional dialects of New Latin can be grouped into families, according to the extent to which they share common traits of pronunciation. The major division is between Western and Eastern family of New Latin. The Western family includes most Romance-speaking regions (France, Spain, Portugal, Italy) and the British Isles; the Eastern family includes Central Europe (Germany and Poland), Eastern Europe (Russia and Ukraine) and Scandinavia (Denmark, Sweden).
The Western family is characterized, "inter alia", by having a front variant of the letter "g" before the vowels "æ, e, i, œ, y" and also pronouncing "j" in the same way (except in Italy). In the Eastern Latin family, "j" is always pronounced [ j ], and "g" had the same sound (usually [ɡ]) in front of both front and back vowels; exceptions developed later in some Scandinavian countries.
The following table illustrates some of the variation of New Latin consonants found in various countries of Europe, compared to the Classical Latin pronunciation of the 1st centuries BCE-CE. In Eastern Europe, the pronunciation of Latin was generally similar to that shown in the table below for German, but usually with [z] for "z" instead of [ts].
Orthography.
New Latin texts are primarily found in early printed editions, which present certain features of spelling and the use of diacritics distinct from the Latin of antiquity, medieval Latin manuscript conventions, and representations of Latin in modern printed editions.
Characters.
In spelling, New Latin, in all but the earliest texts, distinguishes the letter "u" from "v" and "i" from "j". In older texts printed down to c. 1630, "v" was used in initial position (even when it represented a vowel, e.g. in "vt", later printed "ut") and "u" was used elsewhere, e.g. in "nouus", later printed "novus". By the mid-17th century, the letter "v" was commonly used for the consonantal sound of Roman V, which in most pronunciations of Latin in the New Latin period was [v] (and not [w]), as in "vulnus" "wound", "corvus" "crow". Where the pronunciation remained [w], as after "g", "q" and "s", the spelling "u" continued to be used for the consonant, e.g. in "lingua", "qualis", and "suadeo". 
The letter "j" generally represented a consonantal sound (pronounced in various ways in different European countries, e.g. [j], [dʒ], [ʒ], [x]). It appeared, for instance, in "jam" "already" or "jubet" "orders" (now spelled "iam" and "iubet"). It was also found between vowels in the words "ejus", "hujus", "cujus" (now normally spelled "eius, huius, cuius"), and pronounced as a consonant; likewise in such forms as "major" and "pejor". "J" was also used when the last in a sequence of two or more "i"'s, e.g. "radij" (now spelled "radii") "rays", "alijs" "to others", "iij", the Roman numeral 3; however, "ij" was for the most part replaced by "ii" by 1700.
In common with texts in other languages using the Roman alphabet, Latin texts down to c. 1800 used the letter-form "ſ" (the "long s") for "s" in positions other than at the end of a word; e.g. "ipſiſſimus".
The digraphs "ae" and "oe" were rarely so written (except when part of a word in all capitals, e.g. in titles, chapter headings, or captions) ; instead the ligatures "æ" and "œ" were used, e.g. "Cæsar", "pœna". More rarely (and usually in 16th- to early 17th-century texts) the e caudata is found substituting for either.
Diacritics.
Three kinds of diacritic were in common use: the acute accent ´, the grave accent `, and the circumflex accent ˆ. These were normally only marked on vowels (e.g. í, è, â); but see below regarding "que". 
The acute accent marked a stressed syllable, but was usually confined to those where the stress was not in its normal position, as determined by vowel length and syllabic weight. In practice, it was typically found on the vowel in the syllable immediately preceding a final clitic, particularly "que" "and", "ve" "or" and "ne", a question marker; e.g. "idémque" "and the same (thing)". Some printers, however, put this acute accent over the "q" in the enclitic "que", e.g. "eorumq́ue" "and their". The acute accent fell out of favor by the 19th century.
The grave accent had various uses, none related to pronunciation or stress. It was always found on the preposition "à" (variant of "ab" "by" or "from") and likewise on the preposition "è" (variant of "ex" "from" or "out of"). It might also be found on the interjection "ò" "O". Most frequently, it was found on the last (or only) syllable of various adverbs and conjunctions, particularly those that might be confused with prepositions or with inflected forms of nouns, verbs, or adjectives. Examples include "certè" "certainly", "verò" "but", "primùm" "at first", "pòst" "afterwards", "cùm" "when", "adeò" "so far, so much", "unà" "together", "quàm" "than". In some texts the grave was found over the clitics such as "que", in which case the acute accent did not appear before them.
The circumflex accent represented metrical length (generally not distinctively pronounced in the New Latin period) and was chiefly found over an "a" representing an ablative singular case, e.g. "eâdem formâ" "with the same shape". It might also be used to distinguish two words otherwise spelled identically, but distinct in vowel length; e.g. "hîc" "here" differentiated from "hic" "this", "fugêre" "they have fled" (="fūgērunt") distinguished from "fugere" "to flee", or "senatûs" "of the senate" distinct from "senatus" "the senate". It might also be used for vowels arising from contraction, e.g. "nôsti" for "novisti" "you know", "imperâsse" for "imperavisse" "to have commanded", or "dî" for "dei" or "dii".

</doc>
<doc id="21986" url="http://en.wikipedia.org/wiki?curid=21986" title="Naive Set Theory (book)">
Naive Set Theory (book)

Naive Set Theory is a mathematics textbook by Paul Halmos providing an undergraduate introduction to set theory. Originally published by "Van Nostrand" in 1960, it was reprinted in the Springer-Verlag Undergraduate Texts in Mathematics series in 1974.
While the title states that it is naive, which is usually taken to mean without axioms, the book does introduce all the axioms of Zermelo–Fraenkel set theory and gives correct and rigorous definitions for basic objects. Where it differs from a "true" axiomatic set theory book is its character: there are no long-winded discussions of axiomatic minutiae, and there is next to nothing about advanced topics like large cardinals. Instead, it tries to be intelligible to someone who has never thought about set theory before.
Halmos later stated that it was the fastest book he wrote, taking about six months, and that the book "wrote itself".

</doc>
<doc id="21989" url="http://en.wikipedia.org/wiki?curid=21989" title="Nitrogen fixation">
Nitrogen fixation

Nitrogen fixation is a process in which nitrogen (N2) in the atmosphere is converted into ammonium (NH4+) or nitrogen dioxide (NO2), for example. Atmospheric nitrogen or molecular nitrogen (N2) is relatively inert: it does not easily react with other chemicals to form new compounds. The fixation process frees up the nitrogen atoms from their triply bonded diatomic form, N≡N, to be used in other ways.
Nitrogen fixation, natural and synthetic, is essential for all forms of life because nitrogen is required to biosynthesize basic building blocks of plants, animals and other life forms, e.g., nucleotides for DNA and RNA and amino acids for proteins. Therefore, nitrogen fixation is essential for agriculture and the manufacture of fertilizer. It is also an important process in the manufacture of explosives (e.g. gunpowder, dynamite, TNT, etc.). Nitrogen fixation occurs naturally in the air by means of lightning.
Biological nitrogen fixation can include conversion to nitrogen dioxide. All biological nitrogen fixation is done by way of nitrogenase metalo-enzymes which contain iron, molybdenum, or vanadium. Microorganisms that can fix nitrogen are prokaryotes (both bacteria and archaea, distributed throughout their respective kingdoms) called diazotrophs. Some higher plants, and some animals (termites), have formed associations (symbiosis) with diazotrophs.
Biological nitrogen fixation.
Biological nitrogen fixation was discovered by the German agronomist Hermann Hellriegel and Dutch microbiologist Martinus Beijerinck. Biological nitrogen fixation (BNF) occurs when atmospheric nitrogen is converted to ammonia by an enzyme called nitrogenase. The overall reaction for BNF is:
The process is coupled to the hydrolysis of 16 equivalents of ATP and is accompanied by the co-formation of one molecule of H2. The conversion of N2 into ammonia occurs at a cluster called FeMoco, an abbreviation for the iron-molybdenum cofactor. The mechanism proceeds via a series of protonation and reduction steps wherein the FeMoco active site hydrogenates the N2 substrate.
In free-living diazotrophs, the nitrogenase-generated ammonium is assimilated into glutamate through the glutamine synthetase/glutamate synthase pathway.
The microbial genes required for nitrogen fixation are widely distributed in diverse environments.
Enzymes responsible for nitrogenase action are very susceptible to destruction by oxygen. For this reason, many bacteria cease production of the enzyme in the presence of oxygen. Many nitrogen-fixing organisms exist only in anaerobic conditions, respiring to draw down oxygen levels, or binding the oxygen with a protein such as leghemoglobin.
Microorganisms that fix nitrogen.
Diazotrophs are a diverse group of prokaryotes that includes cyanobacteria (e.g. the highly significant "Trichodesmium" and "Cyanothece"), green sulfur bacteria, Azotobacteraceae, rhizobia and "Frankia".
Cyanobacteria inhabit nearly all illuminated environments on Earth and play key roles in the carbon and nitrogen cycle of the biosphere. In general, cyanobacteria are able to utilize a variety of inorganic and organic sources of combined nitrogen, like nitrate, nitrite, ammonium, urea, or some amino acids. Several cyanobacterial strains are also capable of diazotrophic growth, an ability that may have been present in their last common ancestor in the Archean eon. 
Nitrogen fixation by cyanobacteria in coral reefs can fix twice the amount of nitrogen than on land—around 1.8 kg of nitrogen is fixed per hectare per day. The colonial marine cyanobacterium "Trichodesmium" is thought to fix nitrogen on such a scale that it accounts for almost half of the nitrogen-fixation in marine systems on a global scale.
Root nodule symbioses.
Legume family.
Plants that contribute to nitrogen fixation include the legume family – Fabaceae – with taxa 
such as kudzu, clovers, soybeans, alfalfa, lupines, peanuts, and rooibos. They contain symbiotic bacteria called rhizobia within nodules in their root systems, producing nitrogen compounds that help the plant to grow and compete with other plants. When the plant dies, the fixed 
nitrogen is released, making it available to other plants and this helps to fertilize the soil. The great majority of legumes have this association, but a few genera (e.g., "Styphnolobium") do not. In many traditional and organic farming practices, fields are rotated through various types of crops, which usually includes one consisting mainly or entirely of clover or buckwheat (non-legume family Polygonaceae), which are often referred to as "green manure".
Inga alley farming relies on the leguminous genus "Inga", a small tropical, tough-leaved, nitrogen-fixing tree.
Non-leguminous.
Although by far the majority of plants able to form nitrogen-fixing root nodules are in the legume family Fabaceae, there are a few exceptions:
The ability to fix nitrogen is far from universally present in these families. For instance, of 122 genera in the Rosaceae, only 4 genera are capable of fixing nitrogen. All these families belong to the orders Cucurbitales, Fagales, and Rosales, which together with the Fabales form a clade of eurosids. In this clade, Fabales were the first lineage to branch off; thus, the ability to fix nitrogen may be plesiomorphic and subsequently lost in most descendants of the original nitrogen-fixing plant; however, it may be that the basic genetic and physiological requirements were present in an incipient state in the last common ancestors of all these plants, but only evolved to full function in some of them:
There are also several nitrogen-fixing symbiotic associations that involve cyanobacteria (such as "Nostoc"):
Endosymbiosis in diatoms.
"Rhopalodia gibba", a diatom algae, is propably the only eukaryote with cyanobacterial N2-fixing endosymbiont organelles. The spheroid bodies reside in the cytoplasm of the diatoms and are inseparable from their hosts.
Industrial nitrogen fixation.
The possibility that atmospheric nitrogen reacts with certain chemicals was first observed by Desfosses in 1828. He observed that mixtures of alkali metal oxides and carbon react at high temperatures with nitrogen. With the use of barium carbonate as starting material the first commercially used process became available in the 1860s developed by Margueritte and Sourdeval. The resulting barium cyanide could be reacted with steam yielding ammonia. In 1898 Adolph Frank and Nikodem Caro decoupled the process and first produced calcium carbide and in a subsequent step reacted it with nitrogen to calcium cyanamide. The Ostwald process for the production of nitric acid was discovered in 1902. Frank-Caro process and Ostwald process dominated the industrial fixation of nitrogen until the discovery of the Haber process in 1909. 
Prior to 1900, Nikola Tesla also experimented with the industrial fixation of nitrogen "by using currents of extremely high frequency or rate of vibration".
Haber process.
Artificial fertilizer production is now the largest source of human-produced fixed nitrogen in the Earth's ecosystem. Ammonia is a required precursor to fertilizers, explosives, and other products. The most common method is the Haber process. The Haber process requires high pressures (around 200 atm) and high temperatures (at least 400 °C), routine conditions for industrial catalysis. This highly efficient process uses natural gas as a hydrogen source and air as a nitrogen source.
Much research has been conducted on the discovery of catalysts for nitrogen fixation, often with the goal of reducing the energy required for this conversion. However, such research has thus far failed to even approach the efficiency and ease of the Haber process. Many compounds react with atmospheric nitrogen to give dinitrogen complexes. The first dinitrogen complex to be reported was Ru(NH3)5(N2)2+.
Ambient nitrogen reduction.
Catalytic chemical nitrogen fixation at ambient conditions is an ongoing scientific endeavor. Guided by the example of nitrogenase, this area of homogeneous catalysis is ongoing, with particular emphasis on hydrogenation to give ammonia.
It has long been known that metallic Lithium burns in an atmosphere of nitrogen, converting to lithium nitride. Hydrolysis of the resulting nitride gives ammonia. In a related process, trimethylsilyl chloride, lithium, and nitrogen react in the presence of a catalyst to give tris(trimethylsilyl)amine. Tris(trimethylsilyl)amine can then be used for reaction with α,δ,ω-triketones to give tricyclic pyrroles. Processes involving Li metal are however of no practical interest since they are noncatalytic (since it is difficult to re-reduce Li+).
Beginning in the 1960s several homogeneous systems were identified that convert nitrogen to ammonia, sometimes even catalytically but often operating via ill-defined mechanisms. The original discovery is described in an early review, "Vol'pin and co-workers, using a non-protic Lewis acid, aluminium tribromide, were able to demonstrate the truly catalytic effect of titanium by treating dinitrogen with a mixture of titanium tetrachloride, metallic aluminium, and aluminium tribromide at 50 °C, either in the absence or in the presence of a solvent,
e.g. benzene. As much as 200 mol of ammonia per mol of TiCl4 was obtained after hydrolysis..."
The quest for well defined intermediates led to the characterization of many transition metal dinitrogen complexes. Few of these well defined complexes function catalytically, their behavior illuminated likely stages in nitrogen fixation. Most fruitful of all of these early studies focused on M(N2)2(dppe)2 (M = Mo, W). For example double protonation of such low valent complexes gave intermediates with the linkage M=N-NH2. In 1995, a molybdenum(III) amido complex was discovered that cleaved N2 to give the corresponding molybdenum(VI) nitride. This and related terminal nitrido complexes have been used to make nitriles.
In 2003 a related molybdenum amido complex was found to catalyze the reduction of N2. In addition to a source of protons, the catalyst requires a strong reducing agent. However, this catalytic reduction fixates only a few nitrogen molecules. In these systems, like the biological one, hydrogen is provided to the substrate heterolytically, by means of protons and reducing equivalents rather than with H2 itself.
In 2011 Arashiba et al. reported yet another system with a catalyst again based on molybdenum but with a diphosphorus pincer ligand. Photolytic nitrogen splitting is also considered.

</doc>
<doc id="21994" url="http://en.wikipedia.org/wiki?curid=21994" title="Navigation research">
Navigation research

Whereas originally the term Navigation applies to the process of directing a ship to a destination, Navigation research deals with fundamental aspects of navigation in general. It can be defined as "The process of determining and maintaining a course or trajectory to a goal location" (Franz, Mallot, 2000).
It concerns basically all moving agents, biological or artificial, or remote-controlled.
Franz and Mallot proposed a navigation hierarchy in "Robotics and Autonomous Systems 30" (2006):
There are two basic methods for navigation:
Human navigation.
In human navigation people visualize different routes in their minds to plan how to get from one place to another. The things which they rely on to plan these routes vary from person to person and are the basis of the differing navigational strategies.
Some people use measures of distance and absolute directional terms (north, south, east, and west) in order to visualize the best pathway from point to point. The use of these more general, external cues as directions is considered part of an allocentric navigation strategy. Allocentric navigation is typically seen in males and is beneficial primarily in large and/or unfamiliar environments. This likely has some basis in evolution when males would have to navigate through large and unfamiliar environments while hunting. The use of allocentric strategies when navigating primarily activates the hippocampus and parahippocampus in the brain. This navigation strategy relies more on a mental, spatial map than visible cues, giving it an advantage in unknown areas but a flexibility to be used in smaller environments as well. The fact that it is mainly males that favor this strategy is likely related to the generalization that males are better navigators than females as it is better able to be applied in a greater variety of settings.
Egocentric navigation relies on more local landmarks and personal directions (left/right) to navigate and visualize a pathway. This reliance on more local and well-known stimuli for finding their way makes it difficult to apply in new locations, but is instead most effective in smaller, familiar environments. Evolutionarily, egocentric navigation likely comes from our ancestors who would forage for their food and need to be able to return to the same places daily to find edible plants. This foraging usually occurred in relatively nearby areas and was most commonly done by the females in hunter-gatherer societies. Females, today, are typically better at knowing where various landmarks are and often rely on them when giving directions. Egocentric navigation causes high levels of activation in the right parietal lobe and prefrontal regions of the brain that are involved in visuospatial processing.
Robotic navigation.
Outdoor robots can use GPS in a similar way to automotive navigation systems.
Alternative systems can be used with floor plan instead of maps for indoor robots, combined with localization wireless hardware.

</doc>
<doc id="21995" url="http://en.wikipedia.org/wiki?curid=21995" title="Naguib Mahfouz">
Naguib Mahfouz

Naguib Mahfouz (Arabic: نجيب محفوظ‎ "Nagīb Maḥfūẓ", ]; December 11, 1911 – August 30, 2006) was an Egyptian writer who won the 1988 Nobel Prize for Literature. He is regarded as one of the first contemporary writers of Arabic literature, along with Tawfiq el-Hakim, to explore themes of existentialism. He published 34 novels, over 350 short stories, dozens of movie scripts, and five plays over a 70-year career. Many of his works have been made into Egyptian and foreign films.
Early life and education.
Born into a lower middle-class Muslim family in the Gamaleyya quarter of Cairo, Mahfouz was named after Professor Naguib Pasha Mahfouz (1882–1974), the renowned Coptic physician who delivered him. Mahfouz was the seventh and the youngest child in a family that had five boys and two girls. The family lived in two popular districts of the town, in el-Gamaleyya, from where they moved in 1924 to el-Abbaseyya, then a new Cairo suburb; both provided the backdrop for many of Mahfouz's writings. His father, whom Mahfouz described as having been "old-fashioned", was a civil servant, and Mahfouz eventually followed in his footsteps. In his early years, Mahfouz read extensively and was influenced by Hafiz Najib, Taha Hussein and Salama Moussa. His mother often took him to museums and Egyptian history later became a major theme in many of his books.
The Mahfouz family were devout Muslims and Mahfouz had a strict Islamic upbringing. In an interview, he elaborated on the stern religious climate at home during his childhood. He stated that "You would never have thought that an artist would emerge from that family."
The Egyptian Revolution of 1919 had a strong effect on Mahfouz, although he was at the time only seven years old. From the window he often saw British soldiers firing at the demonstrators, men and women. "You could say ... that the one thing which most shook the security of my childhood was the 1919 revolution", he later said. After completing his secondary education, Mahfouz was admitted to King Fouad I University (now the University of Cairo), where he studied philosophy, graduating in 1934. By 1936, having spent a year working on an M.A., he decided to become a professional writer. Mahfouz then worked as a journalist at er-Risala, and contributed to el-Hilal and Al-Ahram. The major Egyptian influence on Mahfouz's thoughts on science and socialism in the 1930s was Salama Moussa, the Fabian intellectual.
Civil service.
Mahfouz left academy and joined the Egyptian civil service, where he continued to work until 1972. He served in the Ministry of Mortmain Endowments, then as Director of Censorship in the Bureau of Art, as Director of the Foundation for the Support of the Cinema, and finally as a consultant to the Ministry of Culture.
Writing career.
Mahfouz published 34 novels, over 350 short stories, dozens of movie scripts and five plays over a 70-year career. Possibly his most famous work, "The Cairo Trilogy", depicts the lives of three generations of different families in Cairo from World War I until after the 1952 military coup that overthrew King Farouk. He was a board member of the publisher "Dar el-Ma'aref". Many of his novels were serialized in "Al-Ahram", and his writings also appeared in his weekly column, "Point of View". Before the Nobel Prize only a few of his novels had appeared in the West.
Writing style and themes.
Most of Mahfouz's early works were set in Cairo. "Abath Al-Aqdar (Mockery of the Fates)" (1939), "Rhadopis" (1943), and "Kifah Tibah (The Struggle of Thebes)" (1944), were historical novels, written as part of a larger unfulfilled project of 30 novels. Inspired by Sir Walter Scott (1771–1832), Mahfouz planned to cover the entire history of Egypt in a series of books. However, following the third volume, he shifted his interest to the present and the psychological impact of social change on ordinary people.
Mahfouz's prose is characterised by the blunt expression of his ideas. His written works covered a broad range of topics, including socialism, homosexuality, and God. Writing about some of these subjects was prohibited in Egypt. In his works, he described the development of his country in the 20th century and combined intellectual and cultural influences from East and West. His own exposure to the literature of non-Egyptian culture began in his youth with the enthusiastic consumption of Western detective stories, Russian classics, and such modernist writers as Marcel Proust, Franz Kafka and James Joyce. Mahfouz's stories are almost always set in the heavily populated urban quarters of Cairo, where his characters, mostly ordinary people, try to cope with the modernization of society and the temptations of Western values.
Mahfouz's central work in the 1950s was the "Cairo Trilogy", which he completed before the July Revolution. The novels were titled with the street names "Palace Walk", "Palace of Desire", and "Sugar Street". Mahfouz set the story in the parts of Cairo where he grew up. The novels depict the life of the patriarch el-Sayyed Ahmed Abdel Gawad and his family over three generations, from World War I to the 1950s, when King Farouk I was overthrown. Mahfouz stopped writing for some years after finishing the trilogy. Disappointed in the Nasser régime, which had overthrown the monarchy in 1952, he started publishing again in 1959, now prolifically pouring out novels, short stories, journalism, memoirs, essays, and screenplays. He stated in a 1998 interview, he "long felt that Nasser was one of the greatest political leaders in modern history. I only began to fully appreciate him after he nationalized the Suez Canal."
"Tharthara Fawq Al-Nīl" (Adrift on the Nile, 1966) is one of his most popular novels. It was later made into a film during the régime of Anwar al-Sadat. The story criticizes the decadence of Egyptian society during the Nasser era. It was banned by Sadat to avoid provoking Egyptians who still loved former president Nasser. Copies were hard to find prior to the late 1990s.
The "Children of Gebelawi" (1959, also known as "Children of the Alley") one of Mahfouz's best known works, portrayed the patriarch Gebelaawi and his children, average Egyptians living the lives of Cain and Abel, Moses, Jesus, and Mohammed. Gebelawi built a mansion in an oasis in the middle of a barren desert; his estate becomes the scene of a family feud that continues for generations. "Whenever someone is depressed, suffering or humiliated, he points to the mansion at the top of the alley at the end opening out to the desert, and says sadly, 'That is our ancestor's house, we are all his children, and we have a right to his property. Why are we starving? What have we done?'" The book was banned throughout the Arab world, except in Lebanon, until 2006, when it was first published in Egypt. The work was prohibited because of its alleged blasphemy through the allegorical portrayal of God and the monotheistic Abrahamic faiths of Judaism, Christianity, and Islam.
In the 1960s, Mahfouz further developed the theme that humanity is moving further away from God in his existentialist novels. In "The Thief and the Dogs" (1961) he depicted the fate of a Marxist thief, who has been released from prison and plans revenge.
In the 1960s and 1970s Mahfouz began to construct his novels more freely and to use interior monologues. In "Miramar" (1967) he developed a form of multiple first-person narration. Four narrators, among them a Socialist and a Nasserite opportunist, represent different political views. In the center of the story is an attractive servant girl. In "Arabian Nights and Days" (1981) and in "The Journey of Ibn Fatouma" (1983) he drew on traditional Arabic narratives as subtexts. "Akhenaten: Dweller in Truth" (1985) is about conflict between old and new religious truths. Many of his novels were first published in serialized form, including "Children of Gebelawi" and "Midaq Alley" which was adapted into a Mexican film starring Salma Hayek ("El callejón de los milagros").
Political influence.
Most of Mahfouz's writings deal mainly with politics, a fact he acknowledged: "In all my writings, you will find politics. You may find a story which ignores love or any other subject, but not politics; it is the very axis of our thinking".
He espoused Egyptian nationalism in many of his works, and expressed sympathies for the post-World-War era Wafd Party. He was also attracted to socialist and democratic ideals early on in his youth. The influence of socialist ideals is strongly reflected in his first two novels, Al-Khalili and New Cairo, and also in many of his latter works. Parallel to his sympathy for socialism and democracy was his antipathy towards Islamic extremism as expressed by the Muslim Brotherhood in Egypt. He strongly criticized radical Islam in his works and contrasted between the merits of socialism and the demerits of Islamic extremism in his first two novels. He perceived Islamism as critically delineated and rejected it as unsuitable for all times. In his memoirs, he purportedly stated that of all the forces active in Egyptian politics during his youth, he most despised the Muslim Brotherhood.
Mahfouz had personally known Sayyid Qutb in his youth, when the latter was showing a greater interest in literary criticism than in Islamic fundamentalism; Qutb later became a significant influence on the Muslim Brotherhood. In fact, Qutb was one of the first critics to recognize Mahfouz's talent in the mid-1940s. Mahfouz even visited Qutb when the latter was in the hospital, during the 1960s, near the end of his life. In his semi-autobiographical novel, "Mirrors", he drew a very negative portrait of Sayyid Qutb. He was disillusioned with the 1952 revolution and by Egypt's defeat in the 1967 Six-Day War. He supported the principles of the revolution but became disillusioned, saying that the practices failed to live up to them.
Naguib Mahfouz influenced a new generation of Egyptian lawyers, including Nabil Mounir and Reda Aslan.
Reception.
Mahfouz has received praise from American reviewers:
“The alleys, the houses, the palaces and mosques and the people who live among them are evoked as vividly in Mahfouz’s work as the streets of London were conjured by Dickens.”
—Newsweek
“Throughout Naguib Mahfouz’s fiction there is a pervasive sense of metaphor, of a literary artist who is using his fiction to speak directly and unequivocally to the condition of his country. His work is imbued with love for Egypt and its people, but it is also utterly honest and unsentimental.”
—Washington Post
“Mahfouz’s work is freshly nuanced and hauntingly lyrical. The Nobel Prize acknowledges the universal significance of [his] fiction.”
—Los Angeles Times
“Mr. Mahfouz embodied the essence of what makes the bruising, raucous, chaotic human anthill of Cairo possible.”
—The Economist
Nobel Prize for Literature.
Mahfouz was awarded the 1988 Nobel Prize in Literature, the only Arab writer to have won the award. Shortly after winning the prize Mahfouz was quoted as saying "The Nobel Prize has given me, for the first time in my life, the feeling that my literature could be appreciated on an international level. The Arab world also won the Nobel with me. I believe that international doors have opened, and that from now on, literate people will consider Arab literature also. We deserve that recognition." The Swedish letter to Mahfouz included the quotations "rich and complex work invites us to reconsider the fundamental things in life. Themes like the nature of time and love, society and norms, knowledge and faith recur in a variety of situations and are presented in thought-provoking, evocative, and clearly daring ways. And the poetic quality of your prose can be felt across the language barrier. In the prize citation you are credited with the forming of an Arabian narrative art that applies to all mankind." Because Mahfouz found traveling to Sweden difficult at his age, he did not attend the award ceremony.
Political involvement.
Mahfouz did not shrink from controversy outside of his work. As a consequence of his outspoken support for Sadat's Camp David peace treaty with Israel in 1978, his books were banned in many Arab countries until after he won the Nobel Prize. Like many Egyptian writers and intellectuals, Mahfouz was on an Islamic fundamentalist "death list". He defended Salman Rushdie after Ayatollah Ruhollah Khomeini condemned Rushdie to death in 1989, but also criticized his "The Satanic Verses" as "insulting" to Islam. Mahfouz believed in freedom of expression and, although he did not personally agree with Rushdie's work, he did not believe that there should be a "fatwa" condemning him to death for it. In 1989, after Ayatollah Ruhollah Khomeini's "fatwa" calling for Salman Rushdie and his publishers to be killed, Mahfouz called Khomeini a terrorist. Shortly after Mahfouz joined 80 other intellectuals in declaring that "no blasphemy harms Islam and Muslims so much as the call for murdering a writer."
Attempted assassination.
The appearance of "The Satanic Verses" brought back up the controversy surrounding Mahfouz's novel "Children of Gebelawi". Death threats against Mahfouz followed, including one from the "blind sheikh," Egyptian theologian Omar Abdul-Rahman. Mahfouz was given police protection, but in 1994 Islamic extremists succeeded in attacking the 82-year-old novelist by stabbing him in the neck outside his Cairo home.
He survived, permanently affected by damage to nerves of his right upper limb. After the incident Mahfouz was unable to write for more than a few minutes a day and consequently produced fewer and fewer works. Subsequently, he lived under constant bodyguard protection. Finally, in the beginning of 2006, the novel was published in Egypt with a preface written by Ahmad Kamal Aboul-Magd. After the threats, Mahfouz stayed in Cairo with his lawyer Nabil Mounir Habib. Mahfouz and Mounir would spend most of their time in Mounir's office; Mahfouz used Mounir's library as a reference for most of his books. Mahfouz stayed with Mounir until his death.
Personal Life.
Mahfouz remained a bachelor until age 43 because he believed that with its numerous restrictions and limitations, marriage would hamper his literary future.[2] "I was afraid of marriage . . . especially when I saw how busy my brothers and sisters were with social events because of it. This one went to visit people, that one invited people. I had the impression that married life would take up all my time. I saw myself drowning in visits and parties. No freedom."[14]
In 1954, he married an Egyptian woman, Atiya, with whom he had two daughters, Fatima and Umm Kalthum. Mahfouz avoided public exposure, especially inquiries into his private life, which might have become, as he put it, “a silly topic in journals and radio programs.”[6]

</doc>
<doc id="21999" url="http://en.wikipedia.org/wiki?curid=21999" title="Nomenklatura">
Nomenklatura

The nomenklatura (Russian: номенклату́ра, ], Latin: "nomenclatura") were a category of people within the Soviet Union and other Eastern Bloc countries who held various key administrative positions in all spheres of those countries' activity: government, industry, agriculture, education, etc., whose positions were granted only with approval by the communist party of each country or region.
Virtually all were members of the Communist Party. Critics of Stalin, such as Milovan Đilas, critically defined them as a new class. Trotskyism uses the term "caste" rather than "class", because it sees the Soviet Union as a degenerated workers' state, not a new class society. Later developments of Trotsky's theories, notably Tony Cliff's theory of State Capitalism, did refer to the nomenklatura as a new "class".
Etymology.
The Russian term is derived from the Latin "nomenclatura", meaning a list of names.
The term was popularized in the West by the Soviet dissident Michael Voslenski, who in 1970 wrote a book titled "Nomenklatura: The Soviet Ruling Class" (Russian: Номенклатура. Господствующий класс Сове́тского Сою́за).
Description.
The nomenklatura referred to the Communist Party's governance to make appointments to key positions throughout the governmental system, as well as throughout the party's own hierarchy. Specifically, the nomenklatura consisted of two separate lists: one was for key positions, appointments to which were made by authorities within the party; the other was for persons who were potential candidates for appointment to those positions. The Politburo, as part of its nomenklatura authority, maintained a list of ministerial and ambassadorial positions that it had the power to fill, as well as a separate list of potential candidates to occupy those positions.
Coextensive with the nomenklatura were patron-client relations. Officials who had the authority to appoint individuals to certain positions cultivated loyalties among those whom they appointed. The patron (the official making the appointment) promoted the interests of clients in return for their support. Powerful patrons, such as the members of the Politburo, had many clients. Moreover, an official could be both a client (in relation to a higher-level patron) and a patron (to other, lower-level officials).
Because a client was beholden to his patron for his position, the client was eager to please his patron by carrying out his policies. The Soviet power structure essentially consisted (according to its critics) of groups of vassals (clients) who had an overlord (the patron). The higher the patron, the more clients the patron had. Patrons protected their clients and tried to promote their careers. In return for the patron's efforts to promote their careers, the clients remained loyal to their patron. Thus, by promoting his clients' careers, the patron could advance his own power.
Party's appointment authority.
The nomenklatura system arose early in Soviet history. Vladimir Lenin wrote that appointments were to take the following criteria into account: reliability, political attitude, qualifications, and administrative ability. Joseph Stalin, who was the first general secretary of the party, also was known as "Comrade File Cabinet" (Tovarishch Kartotekov) for his assiduous attention to the details of the party's appointments. Seeking to make appointments in a more systematic fashion, Stalin built the party's patronage system and used it to distribute his clients throughout the party bureaucracy.
Under Stalin's direction in 1922, the party created departments of the Central Committee and other organs at lower levels that were responsible for the registration and appointment of party officials. Known as uchraspred, these organs supervised appointments to important party posts. According to American sovietologist Seweryn Bialer, after Leonid Brezhnev's accession to power in October 1964, the party considerably expanded its appointment authority. However, in the late 1980s some official statements indicated that the party intended to reduce its appointment authority, particularly in the area of economic management, in line with Mikhail Gorbachev's reform efforts.
At the all-union level, the Party Building and Cadre Work Department supervised party nomenklatura appointments. This department maintained records on party members throughout the country, made appointments to positions on the all-union level, and approved nomenklatura appointments on the lower levels of the hierarchy. The head of this department sometimes was a member of the Secretariat and was often a protégé of the general secretary.
Every party committee and party organizational department, from the all-union level in Moscow to the district and city levels, prepared two lists according to their needs. The basic (osnovnoi) list detailed positions in the political, administrative, economic, military, cultural, and educational bureaucracies that the committee and its department had responsibility for filling. The registered (uchetnyi) list enumerated the persons suitable for these positions.
Patron–client relations.
An official in the party or government bureaucracy could not advance in the nomenklatura without the assistance of a patron. In return for this assistance in promoting his career, the client carried out the policies of the patron. Patron–client relations thus help to explain the ability of party leaders to generate widespread support for their policies. The presence of patron–client relations between party officials and officials in other bureaucracies also helped to account for the large-scale control the party exercised over the Soviet society. All of the 2 million members of the nomenklatura system understood that they held their positions only as a result of a favor bestowed on them by a superior official in the party and that they could easily be replaced if they manifested disloyalty to their patron. Self-interest dictated that members of the nomenklatura submit to the control of their patrons in the party.
Clients sometimes could attempt to supplant their patron. For example, Nikita Khrushchev, one of Lazar M. Kaganovich's former protégés, helped to oust the latter in 1957. Seven years later, Leonid Brezhnev, a client of Khrushchev, helped to remove his boss from power. The power of the general secretary was consolidated to the extent that he placed his clients in positions of power and influence. The ideal for the general secretary, writes Soviet émigré observer Michael Voslensky, "is to be overlord of vassals selected by oneself."
Several factors explain the entrenchment of patron–client relations. Firstly, in a centralized government system, promotion in the bureaucratic-political hierarchy was the only path to power. Secondly, the most important criterion for promotion in this hierarchy was approval from one's supervisors, who evaluated their subordinates on the basis of political criteria and their ability to contribute to the fulfillment of the economic plan. Thirdly, political rivalries were present at all levels of the party and state bureaucracies but were especially prevalent at the top. Power and influence decided the outcomes of these struggles, and the number and positions of one's clients were critical components of that power and influence. Fourthly, because fulfillment of the economic plan was decisive, systemic pressures led officials to conspire together and use their ties to achieve that goal.
The faction led by Brezhnev provides a good case study of patron–client relations in the Soviet system. Many members of the Brezhnev faction came from Dnipropetrovsk, where Brezhnev had served as first secretary of the provincial party organization. Andrei P. Kirilenko, a Politburo member and Central Committee secretary under Brezhnev, was first secretary of the regional committee of Dnipropetrovsk. Volodymyr Shcherbytsky, named as first secretary of the Ukrainian apparatus under Brezhnev, succeeded Kirilenko in that position. Nikolai Alexandrovich Tikhonov, appointed by Brezhnev as first deputy chairman of the Soviet Union's Council of Ministers, graduated from the Dnipropetrovsk College of Metallurgy, and presided over the economic council of Dnipropetrovsk Oblast. Finally, Nikolai Shchelokov, minister of internal affairs under Brezhnev, was a former chairman of the Dnipropetrovsk soviet.
Patron–client relations had implications for policy making in the party and government bureaucracies. Promotion of trusted subordinates into influential positions facilitated policy formation and policy execution. A network of clients helped to ensure that a patron's policies could be carried out. In addition, patrons relied on their clients to provide an accurate flow of information on events throughout the country. This information assisted policymakers in ensuring that their programs were being implemented.
The New Class.
Milovan Đilas, a critic of Stalin, wrote of the nomenklatura as the new class in his book "", and he claimed that it was seen by ordinary citizens as a bureaucratic élite that enjoyed special privileges and had supplanted the earlier wealthy capitalist élites.

</doc>
<doc id="22000" url="http://en.wikipedia.org/wiki?curid=22000" title="Neural Darwinism">
Neural Darwinism

Neural Darwinism, a large scale theory of brain function by Gerald Edelman, was initially published in 1978, in a book called "The Mindful Brain" (MIT Press). It was extended and published in the 1989 book "Neural Darwinism – The Theory of Neuronal Group Selection".
Edelman was awarded the Nobel Prize in 1972 for his work in immunology showing how the population of lymphocytes capable of binding to a foreign antigen is increased by differential clonal multiplication following antigen discovery. Essentially, this proved that the human body is capable of creating complex adaptive systems as a result of local events with feedback. Edelman's interest in selective systems expanded into the fields of neurobiology and neurophysiology, and in "Neural Darwinism", Edelman puts forth a theory called "neuronal group selection". It contains three major parts:
Degeneracy.
With neuronal heterogeneity (by Edelman called "degeneracy"), it is possible to test the many circuits (on the order of 30 billion neurons with an estimated one quadrillion connections between them in the human brain) with a diverse set of inputs, to see which neuronal groups respond "appropriately" statistically. Functional "distributed" (widespread) brain circuits thus emerge as a result.
Edelman goes into some detail about how brain development depends on a variety of cell adhesion molecules (CAMs) and substrate adhesion molecules (SAMs) on cell surfaces which allow cells to dynamically control their intercellular binding properties. This surface modulation allows cell collectives to effectively "signal" as the group aggregates, which helps govern morphogenesis. So morphology depends on CAM and SAM function. And CAM and SAM function also depend on developing morphology.
Edelman theorized that cell proliferation, cell migration, cell death, neuron arbor distribution, and neurite branching are also governed by similar selective processes.
Synaptic modification.
Once the basic variegated anatomical structure of the brain is laid down during early development, it is more or less fixed. But given the numerous and diverse collection of available circuitry, there are bound to be functionally equivalent albeit anatomically non-isomorphic neuronal groups capable of responding to certain sensory input. This creates a competitive environment where circuit groups proficient in their responses to certain inputs are "chosen" through the enhancement of the synaptic efficacies of the selected network. This leads to an increased probability that the same network will respond to similar or identical signals at a future time. This occurs through the strengthening of neuron-to-neuron synapses. And these adjustments allow for neural plasticity along a fairly quick timetable.
Reentry.
The last part of the theory attempts to explain how we experience spatiotemporal consistency in our interaction with environmental stimuli. Edelman called it "reentry" and proposes a model of reentrant signaling whereby a disjunctive, multimodal sampling of the same stimulus event correlated in time leads to self-organizing intelligence. Put another way, multiple neuronal groups can be used to sample a given stimulus set in parallel and communicate between these disjunctive groups with incurred latency.
Support for the theory.
It has been suggested that Friedrich Hayek had earlier proposed a similar idea in his book "The Sensory Order: An Inquiry into the Foundations of Theoretical Psychology", published in 1952 (Herrmann-Pillath, 1992). Other leading proponents include Jean-Pierre Changeux, Daniel Dennett, William H. Calvin, and Linda B. Smith. However, William Calvin proposes true replication in the brain, whereas Edelman's Neural Darwinism opposes the idea that there are true replicators in the brain.
Criticism of the theory.
Criticism of Neural "Darwinism" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended in a later paper by Chrisantha Fernando. In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.

</doc>
<doc id="22003" url="http://en.wikipedia.org/wiki?curid=22003" title="Neil Peart">
Neil Peart

Neil Ellwood Peart, OC (; born September 12, 1952), is a Canadian musician and author. He is the drummer and lyricist for the rock band Rush. Peart has received numerous awards for his musical performances, and is known for his technical proficiency and stamina.
Peart grew up in Port Dalhousie, Ontario (now part of St. Catharines). During adolescence, he floated between regional bands in pursuit of a career as a full-time drummer. After a discouraging stint in England to concentrate on his music, Peart returned home, where he joined a local Toronto band, Rush, in the summer of 1974.
Early in his career, Peart's performance style was deeply rooted in hard rock. He drew most of his inspiration from drummers such as Keith Moon and John Bonham, players who were at the forefront of the British hard rock scene. As time passed, he began to emulate jazz and big band musicians Gene Krupa and Buddy Rich. In 1994, Peart became a friend and pupil of jazz instructor Freddie Gruber. It was during this time that Peart decided to revamp his playing style by incorporating jazz and swing components. Gruber was also responsible for introducing him to the products of Drum Workshop, the company whose products Peart currently endorses.
In addition to being a musician, Peart has published several memoirs about his travels. Peart is also Rush's primary lyricist. In writing lyrics for Rush, Peart addresses universal themes and diverse subject matter including science fiction, fantasy, and philosophy, as well as secular, humanitarian and libertarian themes. All five of his books are travel-based non-fiction, though they diverge into his life and these subjects as well. Peart currently resides in Santa Monica, California, with his wife, photographer Carrie Nuttall, and daughter, Olivia Louise. He also has a home in the Laurentian Mountains of Quebec, Canada, and spends time in Toronto for recording purposes.
Biography.
Early childhood.
Peart was born in a Hamilton hospital to Glen and Betty Peart and lived his early years on his family's farm in Hagersville, on the outskirts of Hamilton. The first child of four, his brother Danny and sisters Judy and Nancy were born after the family moved to St. Catharines when Peart was two. At this time, his father became parts manager for Dalziel Equipment, an International Harvester farm machinery dealer. In 1956 the family moved to the Port Dalhousie area of the town. Peart attended Gracefield School and later Lakeport Secondary School, and describes his childhood as happy and says he experienced a warm family life. By early adolescence he became interested in music and acquired a transistor radio, which he would use to tune into pop music stations broadcasting from Toronto, Hamilton and Welland, Ontario and Buffalo, New York.
His first exposure to musical training came in the form of piano lessons, which he later said in his instructional video "A Work in Progress" did not have much impact on him. He had a penchant for drumming on various objects around the house with a pair of chopsticks, so for his 13th birthday, his parents bought him a pair of drum sticks, a practice drum and some lessons, with the promise that if he stuck with it for a year, they would buy him a kit.
His parents bought him a drum kit for his 14th birthday and he began taking lessons from Don George at the Peninsula Conservatory of Music. His stage debut took place that year at the school's Christmas pageant in St. Johns Anglican Church Hall in Port Dalhousie. His next appearance was at Lakeport High School with his first group, The Eternal Triangle. This performance contained an original number titled "LSD Forever". At this show he performed his first solo.
Peart got a job in Lakeside Park, in Port Dalhousie on the shores of Lake Ontario, which later inspired a song of the same name on the Rush album "Caress of Steel". He worked on the Bubble Game and Ball Toss, but his tendency to take it easy when business was slack resulted in his termination. By his late teens, Peart had played in local bands such as Mumblin’ Sumpthin’, the Majority, and JR Flood. These bands practiced in basement recreation rooms and garages and played church halls, high schools and roller rinks in towns across Southern Ontario such as Mitchell, Seaforth, and Elmira. They also played in the northern Ontario city of Timmins. Tuesday nights were filled with jam sessions at the Niagara Theatre Centre.
Career before joining Rush.
At eighteen years of age, after struggling to achieve success as a drummer in Canada, Peart travelled to London, England hoping to further his career as a professional musician. Despite playing in several bands and picking up occasional session work, he was forced to support himself by selling trinkets to tourists in a souvenir shop called The Great Frog on Carnaby Street.
While in London he came across the writings of novelist and objectivist Ayn Rand. Rand's writings became a significant early philosophical influence on Peart, as he found many of her writings on individualism and Objectivism inspiring. References to Rand's philosophy can be found in his early lyrics, most notably "Anthem" from 1975's "Fly by Night" and "2112" from 1976's "2112".
After eighteen months of dead-end musical gigs, and disillusioned by his lack of progress in the music business, Peart placed his aspiration of becoming a professional musician on hold and returned to Canada. Upon returning to St. Catharines, he worked for his father selling tractor parts at Dalziel Equipment.
Joining Rush.
After returning to Canada, Peart was recruited to play drums for the St. Catharines band Hush, who played on the South Ontario bar circuit. Soon after, a mutual acquaintance convinced Peart to audition for the Toronto-based band Rush, which needed a replacement for its original drummer John Rutsey. Geddy Lee and Alex Lifeson oversaw the audition. His future band mates describe his arrival that day as somewhat humorous, as he arrived in shorts, driving a battered old Ford Pinto with his drums stored in trashcans. Peart felt the entire audition was a complete disaster. While Lee and Peart hit it off on a personal level (both sharing similar tastes in books and music), Lifeson had a less favourable impression of Peart. After some discussion, Lee and Lifeson accepted Peart's maniacal British style of drumming, reminiscent of The Who's Keith Moon.
Peart officially joined the band on July 29, 1974, two weeks before the group's first US tour. Peart procured a silver Slingerland kit which he played at his first gig with the band, opening for Uriah Heep and Manfred Mann in front of over 11,000 people at the Civic Arena, Pittsburgh, Pennsylvania on August 14, 1974.
Early career with Rush.
Peart soon settled into his new position, also becoming the band's primary lyricist. Before joining Rush, he had written few songs, but, with the other members largely uninterested in writing lyrics, Peart's previously underutilized writing became as noticed as his musicianship. The band was working hard to establish themselves as a recording act, and Peart, along with the rest of the band, began to undertake extensive touring.
His first recording with the band, 1975's "Fly by Night," was fairly successful, winning the Juno Award for most promising new act, but the follow-up, "Caress of Steel," for which the band had high hopes, was greeted with hostility by both fans and critics. In response to this negative reception, most of which was aimed at the B side-spanning epic "The Fountain of Lamneth", Peart responded by penning "2112" on their next album of the same name in 1976. The album, despite record company indifference, became their breakthrough and gained a following in the United States. The supporting tour culminated in a three-night stand at Massey Hall in Toronto, a venue Peart had dreamed of playing in his days on the Southern Ontario bar circuit and where he was introduced as "The Professor on the drum kit" by Lee.
Peart returned to England for Rush's Northern European Tour and the band stayed in the United Kingdom to record the next album, 1977's "A Farewell to Kings" in Rockfield Studios in Wales. They returned to Rockfield to record the follow-up, "Hemispheres", in 1978, which they wrote entirely in the studio. The recording of five studio albums in four years, coupled with as many as 300 gigs a year, convinced the band to take a different approach thereafter. Peart has described his time in the band up to this point as "a dark tunnel."
Playing style reinvention.
In 1992, Peart was invited by Buddy Rich's daughter, Cathy Rich, to play at the Buddy Rich Memorial Scholarship Concert in New York City. Peart accepted and performed for the first time with the Buddy Rich Big Band. Peart remarked that he had little time to rehearse, and noted that he was embarrassed to find the band played a different arrangement of the song than the one he had learned. Feeling that his performance left much to be desired, Peart decided to produce and play on two Buddy Rich tribute albums titled "" in 1994 and 1997 in order to regain his aplomb.
While producing the first Buddy Rich tribute album, Peart was struck by the tremendous improvement in ex-Journey drummer Steve Smith's playing, and asked him his "secret." Smith responded he had been studying with drum teacher Freddie Gruber. As a result, Peart would put Rush-related activities on hold while he regularly met with Gruber.
In early 2007, Peart and Cathy Rich again began discussing yet another Buddy tribute concert. At the recommendation of bassist Jeff Berlin, Peart decided to once again augment his swing style with formal drum lessons, this time under the tutelage of another pupil of Freddie Gruber, Peter Erskine, himself an instructor of Steve Smith. On October 18, 2008, Peart once again performed at the Buddy Rich Memorial Concert at New York's Hammerstein Ballroom. The concert has since been released on DVD.
Family tragedy and recovery.
On August 10, 1997, soon after the conclusion of Rush's Test for Echo Tour on July 4, 1997, Peart's first daughter and then-only child, 19-year-old Selena Taylor, was killed in a single-car accident on Highway 401 near the town of Brighton, Ontario. His common-law wife of 22 years, Jacqueline Taylor, succumbed to cancer only 10 months later on June 20, 1998. Peart attributes her death to the result of a "broken heart" and called it "a slow suicide by apathy. She just didn't care."
In his book "", Peart writes that he told his bandmates at Selena's funeral, "consider me retired." Peart took a long sabbatical to mourn and reflect, and travelled extensively throughout North and Central America on his motorcycle, covering 88,000 km. After his journey, Peart decided to return to the band. Peart wrote the book as a chronicle of his geographical and emotional journey.
Peart was introduced to photographer Carrie Nuttall in Los Angeles by long-time Rush photographer Andrew MacNaughtan. They married on September 9, 2000. In early 2001, Peart announced to his bandmates that he was ready to return to recording and performing. The product of the band's return was the 2002 album "Vapor Trails". At the start of the ensuing tour in support of the album, it was decided amongst the band members that Peart would not take part in the daily grind of press interviews and "meet and greet" sessions upon their arrival in a new city that typically monopolize a touring band's daily schedule. Peart has always shied away from these types of in-person encounters, and it was decided that exposing him to an endless stream of questions about the tragic events of his life was not necessary.
Since the release of "Vapor Trails" and his reunion with bandmates, Peart has returned to work as a full-time musician. Rush released an all-covers EP, "Feedback" in June 2004 and their 18th studio album "Snakes & Arrows" in May 2007, supported by tours in 2004, 2007, and 2008.
In the June 2009 edition of Peart's website's "News, Weather, and Sports", titled "Under the Marine Layer", he announced that he and Nuttall were expecting their first child. Olivia Louise Peart was born later that year.
Peart and the rest of the band toured North America, South America and Europe on the Time Machine Tour, which concluded on July 2, 2011, at The Gorge Amphitheatre in George, Washington. Following this tour, the band released their most recent studio album, "Clockwork Angels" on June 12, 2012.
Musicianship.
Style and influences.
Peart's drumming skill and technique are well-regarded by fans, fellow musicians, and music journalists. His influences are eclectic, ranging from Pete Thomas, John Bonham, Michael Giles, Ginger Baker, Phil Collins, Steve Gadd, Michael Shrieve and Keith Moon, to fusion and jazz drummers Billy Cobham, Buddy Rich, Bill Bruford and Gene Krupa. The Who was the first group that inspired him to write songs and play the drums. Peart is distinguished for playing "butt-end out", reversing stick orientation for greater impact and increased rimshot capacity. "When I was starting out", Peart said, "if I broke the tips off my sticks I couldn't afford to buy new ones, so I would just turn them around and use the other end. I got used to it, and continue to use the heavy end of lighter sticks – it gives me a solid impact, but with less 'dead weight' to sling around."
Peart had long played matched grip, but shifted to traditional as part of his style reinvention in the mid-1990s under the tutelage of jazz coach Freddie Gruber. He played traditional grip throughout his first instructional DVD "A Work in Progress", and on the entire T4E album. Peart went back to using primarily matched, though he does switch to traditional from time to time when playing songs from "Test for Echo" and during moments when he feels traditional grip is more appropriate, such as the rudimentary snare drum section of his drum solo. He discusses the details of these switches in the DVD "Anatomy of a Drum Solo".
Equipment.
With Rush, Peart has played Slingerland, Tama, Ludwig, and Drum Workshop drums, in that order.
Peart played Zildjian A-series cymbals and Wuhan china cymbals until the early 2000s when he switched to Paragon, a line created for him by Sabian. In concert, Peart uses an elaborate 360-degree drum kit, with a large acoustic set in front and electronic drums to the rear.
During the late 1970s, Peart augmented his acoustic setup with diverse percussion instruments including orchestra bells, tubular bells, wind chimes, crotales, timbales, timpani, gong, temple blocks, bell tree, triangle, and melodic cowbells. Since the mid-1980s, Peart has replaced several of these pieces with MIDI trigger pads. This was done in order to trigger sounds sampled from various pieces of acoustic percussion that would otherwise consume far too much stage area. Some purely electronic non-instrumental sounds are also used. One classic MIDI pad used is the Malletkat Express which is a two-octave electronic MIDI device that resembles a xylophone or piano. The Malletkat Express is composed of rubber pads for the "keys" so that any stick can be used. Beginning with 1984's "Grace Under Pressure", he used Simmons electronic drums in conjunction with Akai digital samplers. Peart has performed several songs primarily using the electronic portion of his drum kit. (e.g. "Red Sector A", "Closer to the Heart" on "A Show of Hands" and "Mystic Rhythms" on "".) Peart's drum solos also feature sections performed primarily on the electronic portion of his kit.
Shortly after making the choice to include electronic drums and triggers, Peart added what has become another trademark of his kit: a rotating drum riser. During live Rush shows the riser allows Peart to swap the prominent portions of the kit (traditional acoustic in front, electronic in back). A staple of Peart's live drum solos has been the in-performance rotation-and-swap of the front and back kits as part of the solo itself. This special effect provides both a symbolic transition of drum styles within the solo and a visual treat for the audience.
In the early 2000s, Peart began taking full advantage of the advances in electronic drum technology, primarily incorporating Roland V-Drums and continued use of samplers with his existing set of acoustic percussion. His digitally-sampled library of both traditional and exotic sounds has expanded over the years with his music.
In April 2006, Peart took delivery of his third DW set, configured similarly to the R30 set, in a Tobacco Sunburst finish over curly maple exterior ply, with chrome hardware. He refers to this set, which he uses primarily in Los Angeles, as the "West Coast kit". Besides using it on recent recordings with Vertical Horizon, he played it while composing parts for Rush's album, "Snakes & Arrows". It features a custom 23-inch bass drum; all other sizes remain the same as the R30 kit.
On March 20, 2007 Peart revealed that Drum Workshop prepared a new set of red-painted DW maple shells with black hardware and gold "Snakes & Arrows" logos for him to play on the Snakes & Arrows Tour.
Peart has also designed his own signature series drumstick with Pro-Mark. The Promark PW747W, Neil Peart Signature drumsticks, are made of Japanese White Oak, which adds more weight than a standard hickory stick. They have the thickness of a standard 5A (0.551", 1.4 cm) but are longer (16.25", 41.3 cm) with a thicker taper and larger olive shaped wooden tip. When Rush held their Time Machine Tour, Pro-Mark released three limited edition sticks, each with one of the three designs selected from Peart's "Steam Punk" tour kit printed with copper ink. Some other artists who use the Neil Peart Signature series include Ben Johnston of Biffy Clyro, Richie Hayward of Little Feat and Paul Garred of The Kooks.
During the 2010 Time Machine Tour Peart used a new DW kit which was outfitted with copper-plated hardware and time machine designs to match the tour's steampunk themes. Matching Paragon cymbals with clock imagery were also used.
Solos.
Peart is noted for his distinctive in-concert drum solos, characterized by exotic percussion instruments and long, intricate passages in odd time signatures. His complex arrangements sometimes result in complete separation of upper- and lower-limb patterns; an ostinato dubbed "The Waltz" is a typical example. His solos have been featured on every live album released by the band. On the early live albums ("All the World's a Stage" & "Exit...Stage Left"), the drum solo was included as part of a song. On all subsequent live albums through "", the drum solo has been included as a separate track. The "Clockwork Angels Tour" album includes three short solos instead of a single long one: two interludes played during other songs and one standalone.
Peart's instructional DVD "Anatomy of a Drum Solo" is an in-depth examination of how he constructs a solo that is musical rather than indulgent, using his solo from the 2004 R30 30th anniversary tour as an example.
Lyrics.
Peart is also the main lyricist for Rush. Literature has always heavily influenced his writings and, as such, he has tackled a wide range of subjects. In his early days with Rush, much of his lyrical output was influenced by fantasy, science fiction, mythology and philosophy. However, nearly as much would deal with real world or personal issues such as life on the road and parts of his adolescence.
The song "2112" focuses on the struggle of an individual against the collectivist forces of a totalitarian state. This became the band's breakthrough release, but also brought unexpected criticism, mainly because of the credit of inspiration Peart gave to Ayn Rand in the liner notes. "There was a remarkable backlash, especially from the English press, this being the late seventies, when collectivism was still in style, especially among journalists," Peart said. "They were calling us 'Junior fascists' and 'Hitler lovers.' It was a total shock to me".
Weary of ideological fealty to Rand's philosophy of Objectivism, Peart has sought to remind listeners of his eclecticism and independence in interviews. He did not try to argue in defence of Rand's views: "For a start, the extent of my influence by the writings of Ayn Rand should not be overstated. I am no one's disciple."
The 1980 album "Permanent Waves" saw Peart cease to use fantasy literature or ancient mythology in his writing. 1981's "Moving Pictures" showed that Peart was still interested in heroic, mythological figures, but would now place them firmly in a modern and reality-based context. The song "Limelight" from the same album is an autobiographical account of Peart's reservations regarding his own popularity and the pressures with fame. From "Permanent Waves" onward, most of Peart's lyrics began to revolve around social, emotional, and humanitarian issues, usually from an objective standpoint and employing the use of metaphors and symbolic representation.
1984's "Grace Under Pressure" strings together such despondent topics as the Holocaust ("Red Sector A") and the death of close friends ("Afterimage"). Starting with 1987's "Hold Your Fire" and including 1989's "Presto", 1991's "Roll the Bones", and 1993's "Counterparts", Peart would continue to explore diverse lyrical motifs, even addressing the topic of love and relationships ("Open Secrets", "Ghost of a Chance", "Speed of Love", "Cold Fire", "Alien Shore") a subject which he purposefully eschewed in the past out of fear that he wouldn't be able to write about it without using clichés. However, 2002's "Vapor Trails" was heavily devoted to speaking about Peart's personal issues, combined with other humanitarian topics such as the 9/11 terrorist attacks ("Peaceable Kingdom"). The band's album "Snakes & Arrows" dealt primarily and vociferously with Peart's opinions regarding faith and religion. The lyrics of one of the songs on that album, "Faithless", exhibit Peart's life stance, which has been inferred to closely identified with secular humanism. He has only identified his religious views explicitly in the book "The Masked Rider: Cycling in West Africa" by stating "I'm a linear thinking agnostic, but not an atheist folks."
Opinions of Peart's lyrics have always been divided. While fans have lauded them as thoughtful and intelligent, some critics have called them overwrought and bombastic. In 2007, he was named No. 2 on "Blender" magazine's list of "worst lyricists in rock". In contrast, Allmusic has called Peart "one of rock's most accomplished lyricists." 
Political views.
Peart has never publicly identified with any political party or organization in Canada or the United States. Even so, his political and philosophical views have often been analyzed through his work with Rush and through other sources. In October 1993, shortly before that year's Canadian federal election, Peart appeared with then-Liberal Party leader Jean Chrétien in an interview broadcast in Canada on MuchMusic. In that interview, Peart stated he was an undecided voter who supported Quebec federalism.
Peart has often been categorized as an Objectivist and an admirer of Ayn Rand. This is largely based on his work with Rush in the 1970s, particularly the song "Anthem" and the album "2112"; the latter specifically credited Rand's work. However, in his 1994 Rush Backstage Club Newsletter, while contending the "individual is paramount in matters of justice and liberty," Peart specifically distanced himself from a strictly Objectivist line. In a June 2012 Rolling Stone interview, when asked if Rand's words still speak to him, Peart replied, "Oh, no. That was forty years ago. But it was important to me at the time in a transition of finding myself and having faith that what I believed was worthwhile." Peart has also ascribed to a philosophy that he has called "Tryism," which means that anything that one tries to attain will be attained if one tries hard enough.
Although Peart is sometimes regarded as a "conservative" and "Republican" rock star, he has criticized the Republican Party by stating that the philosophy of the party is "absolutely opposed" to Christ's teachings. In 2005 he described himself as a "left-leaning libertarian," and is often cited as a libertarian celebrity. In July 2011, Peart reiterated those views, calling himself a bleeding-heart libertarian.
Books.
Peart is the author of five non-fiction books, the latest released in September 2012. His growth as an author predates the published work by several years (not including his work as Rush's primary lyricist), through private letters and short travelogues sent out to a small circle of friends and family.
Peart's first book, titled "", was written in 1996 about a month-long bicycling tour through Cameroon in November 1988. The book details Peart's travels through towns and villages with four fellow riders. The original had a limited print run, but after the critical and commercial success of Peart's second book, "Masked Rider" was re-issued and remains in print as of 2006.
After losing his wife and (at the time) only daughter, Peart embarked on a lengthy motorcycle road trip spanning North America. His experiences were penned in "". Peart and the rest of the band were always able to keep his private life at a distance from his public image in Rush. However, "Ghost Rider" is a first-person narrative of Peart on the road, on a BMW R1100GS motorcycle, in an effort to put his life back together as he embarked on an extensive journey across North America.
Years later, after his marriage to Nuttall, Peart took another road trip, this time by car. In his third book, "Traveling Music: The Soundtrack of my Life and Times", he reflects on his life, his career, his family and music. As with his previous two books, it is a first person narrative.
Thirty years after Peart joined Rush, the band found itself on its . Released in September 2006, "Roadshow: Landscape With Drums, A Concert Tour By Motorcycle" details the tour both from behind Neil's drumkit and on his BMW R1150GS and R1200GS motorcycles.
Peart's next book, "Far and Away: A Prize Every Time", was published by ECW Press in May 2011. This book, which he worked on for two years, is based around his travelling in North and South America. It tells how he found in a Brazilian town a unique combination of West African and Brazilian music. In 2014, a followup book, "Far and Near: On Days like These", was published by ECW. It covers travels in North America and Europe.
Peart worked with science fiction author Kevin J. Anderson to develop a novelization of Rush's 2012 album "Clockwork Angels"; the book was published by ECW Press. Snippets of the band's lyrics can be found throughout the story.
Non-Rush music projects.
Peart had a brief cameo in the 2007 film "Aqua Teen Hunger Force Colon Movie Film for Theaters", in which samples of his drumming were played.
Peart also had a brief cameo in the 2008 film "Adventures of Power" and in the DVD extra does a drum-off competition.
DVDs.
Apart from Rush's video releases as a band, Peart has released the following DVDs as an individual:
Awards and honours.
Peart has received the following awards in the "Modern Drummer" magazine
reader's poll:
Peart has received the following awards from DRUM! magazine for 2007:
Peart received the following awards from DRUM! magazine for 2008:
Peart received the following awards from DRUM! magazine for 2009:
Peart received the following awards from DRUM! magazine for 2010:
Along with his bandmates Lee and Lifeson, Peart was made an Officer of the Order of Canada on May 9, 1996. The trio was the first rock band to be so honoured, as a group. Peart was inducted into the Canadian Songwriter Hall of Fame along with bandmates Lifeson and Lee.
On April 18, 2013, Rush was inducted into the Rock And Roll Hall Of Fame.

</doc>
<doc id="22007" url="http://en.wikipedia.org/wiki?curid=22007" title="North Atlantic Treaty">
North Atlantic Treaty

The North Atlantic Treaty, signed in Washington, D.C. on 4 April 1949, is the treaty establishing the North Atlantic Treaty Organization (NATO).
Background.
The treaty was drafted by a committee in talks in Washington which was chaired by Theodore Achilles. Earlier secret talks had been held at the Pentagon between 22 March and 1 April 1948, of which Achilles said:
The talks lasted about two weeks and by the time they finished, it had been secretly agreed that there would be a treaty, and I had a draft of one in the bottom drawer of my safe. It was never shown to anyone except Jack [Hickerson]. I wish I had kept it, but when I left the Department in 1950, I dutifully left it in the safe and I have never been able to trace it in the archives. It drew heavily on the Rio Treaty, and a bit of the Brussels Treaty, which had not yet been signed, but of which we were being kept heavily supplied with drafts. The eventual North Atlantic Treaty had the general form, and a good bit of the language of my first draft, but with a number of important differences.
According to Achilles, another important author of the treaty was John D. Hickerson:More than any human being Jack was responsible for the nature, content, and form of the Treaty...It was a one-man Hickerson treaty.
The treaty was created with an armed attack by the Soviet Union against Western Europe in mind, but the mutual self-defense clause was never invoked during the Cold War. Rather, it was invoked for the first time in 2001 in response to the 11 September 2001 attacks against the World Trade Center and The Pentagon in Operation Eagle Assist.
Members.
Founding members.
The following twelve nations signed the treaty and thus became the founding members of NATO. The following leaders signed the agreement as plenipotentiaries of their countries in Washington D.C.:
Later members.
The following 16 nations joined the treaty after the 12 founding countries:
Content.
Article 4.
The treaty includes Article 4, which triggers not military intervention but merely consultation over military matters. It has been invoked three times by Turkey: once in 2003 over the Second Persian Gulf War (Iraq War), once in June 2012 after the shooting down of a Turkish military jet, and once again in October 2012 after Syrian attacks on Turkey and their counterattacks. It was also invoked by Latvia, Lithuania, and Poland in March 2014 in response to the 2014 Crimean crisis.
Article 5.
The key section of the treaty is Article 5. Its commitment clause defines the casus foederis. It commits each member state to consider an armed attack against "one" member state to be an armed attack against "them all". This article has been invoked only once in NATO history: by the United States after the September 11 attacks in 2001.
Article 6.
Article 6 states that the treaty only covers member nations' territories north of the Tropic of Cancer, plus French Algeria. An attack on the US state of Hawaii, for example, would not trigger the treaty, but an attack on the other 49 would.

</doc>
<doc id="22009" url="http://en.wikipedia.org/wiki?curid=22009" title="Nitronium ion">
Nitronium ion

The nitronium ion, or sometimes the nitryl ion (incorrect because it is not a radical), NO, is a generally reactive cation created by the removal of an electron from the paramagnetic nitrogen dioxide molecule, or the protonation of nitric acid.
It is stable enough to exist in normal conditions, but it is used extensively as an electrophile in the nitration of other substances. The ion is generated "in situ" for this purpose by mixing concentrated sulfuric acid and concentrated nitric acid according to the equilibrium:
Historically, the nitronium ion was detected by Raman Spectroscopy, since its symmetric stretch is Raman active but Infrared inactive. (The nitronium ion is linear like carbon dioxide and their vibrational spectra are similar). The nitronium ion also exists in the solid form of dinitrogen pentoxide, N2O5, which is an ionic solid formed from nitronium and nitrate ions. Its liquid and gaseous forms, however, are molecular and do not contain nitronium ions.
A few stable nitronium salts with anions of weak nucleophilicity can be isolated. These include nitronium perchlorate (NOClO), nitronium tetrafluoroborate (NOBF), nitronium hexafluorophosphate (NOPF), nitronium hexafluoroarsenate (NOAsF), and nitronium hexafluoroantimonate (NOSbF). These are all very hygroscopic compounds.
The nitronium ion is isoelectronic with carbon dioxide and nitrous oxide, and like those molecules has a linear structure with an ONO bond angle of 180°. For this reason it has a similar vibrational spectrum to carbon dioxide: the Raman active symmetrical stretch was first used to identify the ion in nitrating mixtures.
Related species.
The compounds nitryl fluoride, NO2F, and nitryl chloride, NO2Cl, are not nitronium salts but rather molecular compounds, as shown by their low boiling points (−72 °C and −6 °C respectively) and short N-X bond lengths (N-F 135 pm, N-Cl 184 pm).
Addition of one electron forms the neutral nitryl radical, ·NO2; in fact, this is fairly stable and known as the compound nitrogen dioxide.
The related negatively charged species is NO, the nitrite ion.

</doc>
<doc id="22011" url="http://en.wikipedia.org/wiki?curid=22011" title="Neo Geo (system)">
Neo Geo (system)

The Neo Geo (Japanese: ネオジオ, Hepburn: Neojio) is a cartridge-based arcade system board and home video game console released on January 31, 1990 by Japanese game company SNK. Although it is a member of the fourth generation of video game consoles, it is the first system in the Neo Geo family, which ran throughout the 1990s before being revived in December 2012 with the Neo Geo X handheld and home system.
The MVS (Multi Video System), as the Neo Geo is known to the coin-operated arcade game industry, offers owners the ability to put up to six different arcade titles into a single cabinet, a key economic consideration for operators with limited floorspace. With its games stored on self-contained cartridges, a game cabinet can be exchanged for a different game title by swapping the game's ROM-cartridge and cabinet artwork. The platform's popular series include "Fatal Fury", "The King of Fighters", "Metal Slug" and "Samurai Shodown".
The Neo Geo system is also a notably costly and technologically uncompromised home console, commonly referred to today as the AES (Advanced Entertainment System). The Neo Geo was marketed as 24-bit, though it is technically a parallel processing 16-bit 68000-based system with an 8-bit Z80 coprocessor.
Neo Geo hardware production lasted seven years, discontinued in 1997; and game software production lasted fourteen years, discontinued in 2004. As of March 1997, the Neo Geo had sold 980,000 units worldwide. In 2009, the Neo Geo was ranked 19th out of the 25 best video game consoles of all time by the video game website IGN. There is an amateur and professional commercial homebrew market for the system.
History.
Initially, the (AES) home system was only available for rent to commercial establishments, such as hotel chains, bars and restaurants, and other venues. When customer response indicated that some gamers were willing to buy a US$ console, SNK expanded sales and marketing into the home console market. The Neo Geo console was officially launched on January 31, 1990 in Osaka, Japan. Neo Geo's graphics and sound are largely superior to other contemporary home consoles, and the MVS is one of the most powerful arcade units at the time. Furthermore, since the AES was identical to its arcade counterpart, the MVS, arcade titles released for the home market are perfect translations. Although its high price tag kept it out of the mainstream gaming market, it outlasted the market lifespan of the more popular Sega Mega Drive/Genesis.
In the United States, the console's debut price was planned to be US$ and included two joystick controllers and a game: either "Baseball Stars Professional" or "NAM-1975". However, the price was raised and its American launch debuted as the Gold System at US$. Later, the Gold System was bundled with "Magician Lord" and "Fatal Fury". The Silver System package, launched at US$, includes one joystick controller and does not include a game. Other games were launched at about US$ and up. At double or quadruple the competition, these premium prices made the console accessible only to a niche market.
Several home console systems were created based on the same hardware as the arcade games, as well as a series of handheld systems under the Neo Geo brand. These systems include the following: Hyper Neo Geo 64 arcade system, Neo Geo CD, Neo Geo Pocket, Neo Geo Pocket Color, and Neo Geo X. The most recent, the Neo Geo X, is an officially licensed device with a collection of Neo Geo AES games pre-installed.
When realtime 3D graphics became the norm in the arcade industry, the Neo Geo's 2D hardware was unable to do likewise. The last official game by SNK for the Neo Geo system, "Samurai Shodown V Special", was released in 2004. SNK ceased to manufacture home consoles by the end of 1997, but continued to release games for both arcade and home for another eight years.
Measured from the introduction of the arcade hardware in 1990 to the release of the last official home cartridge in 2004, the Neo Geo enjoyed a primary software production lifespan of fourteen years, and a hardware production lifespan of seven years. On August 31, 2007, SNK stopped offering maintenance and repairs to Neo Geo home consoles, handhelds, and games.
Reception.
In a 1993 review, "GamePro" gave the Neo Geo a "thumbs up". Though they voiced several criticisms, noting that the system was not as powerful as the soon-to-launch 3DO and had few releases which were not fighting games, they generally praised both the hardware and games library, and recommended that gamers who could not afford the console (which was still priced at $649.99) play the games in the arcade.
In 2009, the Neo Geo was ranked 19th out of the 25 best video game consoles of all time by the video game website IGN.
Technical details.
Each joystick controller is 280mm (width) × 190mm (depth) × 95mm (height) ( 11 × 8 × 2.5 in.) and contains the same four-button layout as the arcade MVS cabinet.
The arcade machines have a memory card system by which a player could save a game to return to at a later time and could also be used to continue play on the SNK home console of the same name.
The arcade version of the video game hardware is often referred to as the "MVS," or Multi Video System (available in 1-slot, 2-slot, 4-slot, and 6-slot variations, differing in the amount of game cartridges loaded into the machine at the time), with its console counterpart referred to as the "AES", or Advanced Entertainment System. Early motherboard revisions contain daughterboards, used to enhance the clarity of the video output.
The MVS and AES hardware can execute identical machine code. Owners can move EPROMs from one type to the other, and the game will still run. The program specifics for both MVS and AES game options are contained on every game ROM, whether the cartridge is intended for home or arcade use. However, the arcade and home cartridges do have a different pinout. They were designed this way to prevent arcade operators from buying the cheaper home carts and then using them in arcades. It has been found that in a few home version games, one could unlock the arcade version of the game by inputting a special code.
The original specification for ROM size is up to 330 megabits, hence the system displaying "MAX 330 MEGA PRO-GEAR SPEC" upon startup. While no technical advances were required to achieve it, some games over 100 megabits, such as Top Hunter, followed this screen by displaying an animation proclaiming "THE 100MEGA SHOCK!". The original ROM size specification was later enhanced on cartridges with bank switching memory technology, increasing the maximum cartridge size to around 716 megabits. These new cartridges also cause the system to display "GIGA POWER PRO-GEAR SPEC" upon startup or during attract mode, indicating this enhancement.
Specifications.
RAM.
RAM: 214 KB SRAM
On-board ROM: 512 KB
Display.
The SNK custom video chipset allows the system to draw sprites in vertical strips which are 16 pixels wide, and can be 16 to 512 pixels tall; it can draw up to 96 sprites per scanline for a total of 380 sprites on the screen at a time. Unlike most other video game consoles of its time, the Neo Geo does not use scrolling tilemap background layers. Instead, it has a single non-scrolling tilemap layer called the fix layer, while any scrolling layers rely exclusively on drawing sprites to create the scrolling backgrounds (like the Sega Y Board). By laying multiple sprites side by side, the system can simulate a tilemap background layer. The Neo Geo sprite system represents a step between conventional sprites and tilemaps.
Sound.
The onboard Yamaha YM2610 sound chip gives the system 15 channels of sound with seven channels reserved specifically for digital sound effects.
Legacy.
The Neo Geo is the first home game console to feature a removable memory card for saved games.
By the mid-1990s, SNK was trying to move onto a new platform, notably the Hyper-64. A Hyper-64 game called "" was ported to the PlayStation.
The GameTap subscription service has included a Neo Geo emulator and a small library of Neo Geo games. In 2007 Nintendo announced that Neo Geo games would appear on the Wii's Virtual Console, starting with ', "Art of Fighting", and "World Heroes". Neo Geo games are also available through Xbox Live Arcade and PlayStation Network, including "Fatal Fury Special", "Samurai Shodown II", "Metal Slug 3", ' and "The King of Fighters '98".
Homebrew activity exists even since the console's discontinuation, both by noncommercial hobbyists and commercially.
Neo Geo has a community of collectors. Because of the limited production runs received by cartridges amongst the sizable available arcade library, some of the rarest Neo Geo games can sell for well over $1,000. The most valuable game is the European AES version of "Kizuna Encounter". The MVS market provides a cheaper alternative to the expensive and rare home cartridges, and complete arcade kits are priced at a premium.

</doc>
<doc id="22012" url="http://en.wikipedia.org/wiki?curid=22012" title="Neo Geo CD">
Neo Geo CD

Neo Geo CD (Japanese: ネオジオCD, Hepburn: Neo Jio Shī Dī) is the second home video game console of SNK's Neo Geo family, released in September 1994, four years after its cartridge-based equivalent. This is the same platform, converted to the cheaper CD format retailing at about $50 per title, compared to the $300 cartridges. The system was originally priced at US$300, or £399 in the UK. The unit's 1X CD-ROM drive is slow, with very long loading times of up to 56 Mbit of data per load. The system can also play Audio CDs. All three versions of the system have no region-lock.
The Neo Geo CD was launched bundled with a control pad instead of a joystick like the AES version. However, the original AES joystick can be used with all three Neo Geo CD models.
As of March 1997, the Neo Geo CD had sold 570,000 units worldwide.
Overview.
The Neo Geo CD was first unveiled at the 1994 Tokyo Toy Show. The console uses the same CPU set-up as the arcade and cartridge-based Neo Geo systems, facilitating conversions, and SNK stated that they planned to release Neo Geo CD versions of every Neo Geo game still in the arcades.
Three versions of the Neo Geo CD were released: a front-loading version, only distributed in Japan, with 25,000 total units built; a top-loading version, marketed worldwide, as the most common model; the Neo Geo CDZ, an upgraded, faster-loading version, released in Japan only.
The front-loading version was the original console design, with the top-loading version developed shortly before the Neo Geo CD launch as a scaled-down, cheaper alternative model. The CDZ was released on December 29, 1995 as the Japanese market replacement for SNK's previous efforts (the "front loader" and the "top loader"). The Neo Geo CD had met with limited success due to it being plagued with slow loading times that could vary from 30 to 60 seconds between loads, depending on the game. Although SNK's American home entertainment division quickly acknowledged that the system simply was incapable to compete with the 3D-able powerhouse systems of the day like Sega's Saturn and Sony's PlayStation, SNK corporate of Japan felt they could continue to maintain profitable sales in the Japanese home market by shortening the previous system's load-times. 
The CDZ was only officially sold in Japan during its production. However, its faster loading times, lack of a "region lock", and the fact that it could play older CD software, made it a popular import item for enthusiasts in both Europe and North America.
Reception.
Criticism of the system's generally long loading times began even before launch; a report in "Electronic Gaming Monthly" on the Neo Geo CD's unveiling noted, "At the show, they were showing a demo of "Fatal Fury 2". The prototype of the machine that they showed was single speed, and the load time was 14-28 seconds between rounds. You can see that the screen[shot] on the right is a load screen."
Roughly a month after launch, SNK reported that they had sold the Neo Geo CD's entire initial shipment of 50,000 units.
Technical specifications.
The system is also capable of reading Redbook standard compact disc audio.
In addition to the multi-AV port (almost same one as used on the Sega Genesis model 1, though they are not interchangeable), all Neo Geo CD models had composite RCA A/V and S-Video out jacks on the rear of the console.
The CD system's 58 Mbit / 7 MB of RAM was split accordingly:
Game library.
While the Neo Geo CD library consists primarily of ports of MVS and AES titles, there are a few MVS arcade games which were not officially released for the Neo Geo AES and ported instead to the Neo Geo CD. These include "Puzzle Bobble", "Janshin Densetsu: Quest of Jongmaster" (a Mahjong game also released for the PC Engine), "Power Spikes II", "Neo Drift Out: New Technology", and "" ("Futsal: 5-on-5 Mini Soccer").
A few games which were unreleased in MVS and AES formats were also released exclusively for the Neo Geo CD. These include "Ironclad: Tesshō Rusha" ("Chōtetsu Burikingā", "BRIKIN'GER"), "Crossed Swords II", "ZinTrick" ("Oshidashi Zintorikku"), "ADK World", "Neo Geo CD Special", "The King of Fighters '96 Neo Collection", "Samurai Shodown RPG" ("Shinsetsu Samurai Spirits: Bushidō Retsuden"; an RPG spin-off of the "Samurai Shodown" series that was also released for the PlayStation and Sega Saturn), and "Idol-Mahjong Final Romance 2" (an arcade game which is not an MVS game, but was ported directly to the Neo Geo CD).

</doc>
<doc id="22015" url="http://en.wikipedia.org/wiki?curid=22015" title="Neopets">
Neopets

Neopets (originally "NeoPets") is a virtual pet website. Visitors can create accounts, own virtual pets ("Neopets"), and buy virtual items for them using one of two virtual currencies, one of which can be earned within the site and the other of which can purchased with real-world money, and given as a free NC Gift when you first create an account.
The website was launched by Adam Powell and Donna Williams in late 1999. Two years later, a consortium of investors led by Doug Dohring bought a controlling interest in the company and in June 2005, Viacom bought "Neopets", Inc. for US$160 million. On March 17, 2014, Neopets was sold to Knowledge Adventure for an unannounced amount.
Gameplay.
"Neopets" allows users to create and care for virtual pets called Neopets and explore the virtual world of Neopia. There is no set objective for the users, but they are expected to feed and care for their Neopets when they grow hungry or ill. Neopets will not die if neglected, but their health can limit their gameplay. Neopets come in a variety of species and colors and users can create or adopt their own. Users can obtain items to interact with their Neopet, such as books to read and toys to play with them. Neopets can be customised with certain clothing items, paint brushes, transformation potions, and accessories. Users can build a customisable Neohome for their Neopets and furnish it with furniture, wallpaper, and flooring. Neopets can battle against other Neopets or non-player characters in the Battledome but they cannot die there.
Neopia is a virtual planet with fantasy lands inhabited by Neopets and other virtual creatures. Each land has a different theme, such as pirates or prehistoric times, and their own shops, games, and attractions. Neopia follows its own calendar and time zone, which runs concurrent with real-world Pacific Time, and has tie-ins with certain real world holidays such as Halloween and Christmas. It has its own economy and stock market based on Neopoints. Users can earn Neopoints through various means including playing games and selling items, which can be invested or used to buy various virtual goods and services. While there is no set objective for users, interactive storylines are sometimes released that introduce changes to the planet such as new lands.
The site is updated almost on a daily basis with set of new games, items and content. In addition to the site content updated by the "Neopets" staff members, users also contribute content to the site. User contributions come in the form of prescreened submissions and readily editable content that is automatically filtered, such as the site's weekly electronic newspaper "The Neopian Times". There are different types of submissions that will be accepted.
Games.
Users can earn Neopoints from playing games. Games come in many different genres, which include action, puzzles, and luck & chance. Most games have a set maximum earnings or playtime. Players may also earn trophies and other awards from games if they score high enough or perform better than other users. Many single-player and multi-player browser games are available. Users can also participate in contests and spotlights judged by staff to showcase the users' talents. Quests to retrieve items may also be performed for specific NPCs. Challenges may be made against other players or random players in a "World Challenge" for a prize piece and Neopoints from the jackpot for certain Flash games. Monthly competitions also exist for multiplayer games with four week-long elimination rounds.
Economy.
The economy is based on Neopoints. Users can also exchange real money for Neocash, used exclusively for the NC Mall; however, Neopoints cannot be exchanged for Neocash and vice versa to keep "Neopets" fair. Users can earn Neopoints through playing games, selling items, and other transactions. Once earned, they can be saved in the bank, used to buy items from other users or non-player character (NPC) shops, used to buy and sell stocks in the Neopian stock market called the Neodaq, or used to buy various other things. Items can be bought from shops found throughout the world of Neopia that are run by NPCs who may allow haggling. Users can open their own shops to sell items, sometimes after obtaining those items at a lower price from sources such as other shops or charities. Items may also be exchanged through trade or auction.
Community.
"Neopets" has a community for users to chat with and contact other users. Each user has their own profile they can edit with HTML and CSS and are represented by avatars provided by the website, as users cannot upload their own. Users may request other users to be "Neofriends" or block other users from contacting them. To comply with COPPA, users under 13 years of age cannot access any of the site's communication features without sending in parental consent via fax. The main features include:
Discussions through these features are restricted and may not involve topics such as dating and romance or controversial topics like politics and religion. Continuous moderation is performed by paid "Neopets" staff members, and users can help moderate the site by reporting messages they believe are inappropriate or offensive. Messages are also automatically filtered to prevent users from posting messages with profanity or lewd content.
History and background.
"Neopets" was conceived in 1997 by Adam Powell, a British student at the University of Nottingham at the time. He shared this idea with Donna Williams and the two started work on the site in September 1999, with Powell responsible for the programming and the database and Williams the web design and art. The site launched on November 15, 1999 from offices in Portsmouth Road, Guildford, a location still commemorated on the site. Powell stated that the original goal was to "keep university students entertained, and possibly make some cash from banner advertising". The site contained popular culture references, such as a Neopet that was simply a picture of Bruce Forsyth.
The user base grew by word of mouth and by Christmas 1999, "Neopets" was logging 600,000 page views daily and sought investors to cover the high cost of running the site. Later in the month, American businessman Doug Dohring was introduced to the creators of the site and, along with other investors, bought a majority share in January of the following year. Dohring founded "Neopets, Inc." in February 2000 and began business April 28. Intellectual property that did not belong to "Neopets", such as Bruce Forsyth were removed, but the site kept the British spellings. The website made money from the first paying customers using an advertising method trademarked as "immersive advertising" and touted as "an evolutionary step forward in the traditional marketing practice of product placement" in television and film. In 2004, "Neopets" released a premium version and started showing advertisements on the basic site that, as a perk of premium membership, are not visible to premium members.
Media conglomerate Viacom purchased "Neopets, Inc." on June 20, 2005 for $160 million and announced plans to focus more on the use of banner ads over the site's existing immersive advertising. The website was redesigned on April 27, 2007 and included changes to the user interface and the ability to customise Neopets. In June, Viacom promoted "Neopets" through minishows on Nickelodeon. Promotions included the second Altador Cup and lead to an increase in traffic through the site. The first Altador Cup was released as an international online gaming event to coincide with the 2006 FIFA World Cup to improve interactivity between users and had 10.4 million participants the first year. On July 17, the NC Mall was launched in a partnership with Korean gaming company Nexon Corporation. It allowed users to use real money to purchase Neocash to buy exclusive virtual items. On June 17, 2008, Viacom formed the Nickelodeon Kids & Family Virtual Worlds Group to "encompass all paid and subscription gaming initiatives across all relevant platforms", including "Neopets". By June 2011, "Neopets" announced that the website had logged 1 trillion page views since its creation.
In July 2009, the "Neopets" site was the target of an identity theft hacking scheme that attempted to trick users into clicking a link that would allow them to gain items or Neopoints. Upon doing so, malware was installed onto the user's computer. According to reports, the hack was aimed not at child player's "Neopets" accounts, but at using the malware to steal the financial data and identities of their parents. Viacom stated that it was investigating the issue, and that the hack was a version of social engineering rather than an "indictment of Neopets security practices". In an on-site newsletter for players, "Neopets" denied the report and claimed that the site's security measures prevented the posting of such links.
Reception.
"Neopets" is consistently one of the "stickiest" sites for children's entertainment. Stickiness is a measure of the average amount of time spent on a website. A press release from "Neopets" in 2001 stated that Neopets.com led in site "stickiness" in May and June, with the average user spending 117 minutes a week. "Neopets" also led in the average number of hours spent per user per month in December 2003 with an average of 4 hours and 47 minutes. A 2004 article stated that Nielsen//NetRatings reported that people were spending around three hours a month on "Neopets", more than any other site in its Nielsen category. By May 2005, a "Neopets"-affiliated video game producer cited about 35 million unique users, 11 million unique IP addresses per month, and 4 billion web page views per month. This producer also described 20% of the users as 18 or older, with the median of the remaining 80% at about 14. "Neopets" was consistently ranked among the top ten "stickiest" sites by both Nielsen//NetRatings and comScore Media Metrix in 2005 and 2006. According to Nielsen//NetRatings, in 2007, "Neopets" lost about 15% of its audience over the previous year. In February 2008, comScore ranked it as the stickiest kids entertainment site with the average user spending 2 hours and 45 minutes per month.
Described as an online cross of "Pokémon" and "Tamagotchi", "Neopets" has received both praise and criticism. It has been praised for having educational content. Children can learn HTML to edit their own pages. They can also learn how to handle money by participating in the economy. Reviews from About.com and MMO Hut considered the multitude of possible activities a positive aspect. Most of the users are female, higher than in other massively multiplayer online games (MMOGs) but equivalent to social-networking-driven communities. Lucy Bradshaw, a vice president of Electronic Arts, attributes the popularity among girls to the openness of the site and said, "Games that have a tendency to satisfy on more than one dimension have a tendency to have a broader appeal and attract girls".
Luck & chance games draw criticism from parents as they introduce children to gambling. In Australia, a cross-promotion with McDonald's led to controversy with "Neopets"' luck/chance games in October 2004. Australian tabloid television show "Today Tonight" featured a nine-year-old boy who claimed the site requires one to gamble in order to earn enough Neopoints to feed one's Neopet or else it would be sent to the pound. While gambling is not required, nor are pets sent to the pound if unfed, the website includes games of chance based on real games such as blackjack and lottery scratchcards. After this incident, "Neopets" prohibited users under the age of 13 from playing most games that involve gambling.
Immersive advertising.
Immersive advertising is a trademarked term for the way "Neopets" displayed advertisements to generate profit after Doug Dohring bought the site. Unlike pop-up and banner ads, immersive ads integrate advertisements into the site's content in interactive forms, including games and items. Players could earn Neopoints from them by playing advergames and taking part in online marketing surveys. Prior to the arrival of the NC Mall, it contributed to 60% of the revenue from the site with paying Fortune 1000 companies including Disney, General Mills, and McDonald's.
It was a contentious issue with the site with regard to the ethics of marketing to children. It drew criticism from parents, psychologists, and consumer advocates who argued that children may not know that they are trying to be sold something, as it blurred the line between site content and advertisement. Children under eight had difficulty recognizing ads and half a million of the 25 million users were under the age of eight in 2005. Dohring responded to such criticism stating that of the 40 percent of users twelve and younger were seven or eight years old and older and that preschoolers were not their target audience.
Others criticised the functionality of the site. Susan Linn, another psychologist and author of "Consuming Kids: The Hostile Takeover of Childhood" considered the purpose of this site was to keep children in front of advertisements. Kalle Lasn, editor-in-chief and co-founder of "Adbusters" magazine, said the site was encouraged kids to spend hours in front of a screen and recruited them to consumerism. "Neopets" executives stated that paid content comprised less than 1% of the site's total content. Children were not required to play or use sponsor games and items, and all ads were marked as such.
Merchandise.
The popularity of "Neopets" spawned real-world merchandise. Merchandise existed as stickers, books, cereals, video games and other forms, sold at mainstream outlets and online retailers. Each piece of merchandise has a code which can be redeemed at the site for an in-game reward. The merchandise has now been discontinued. "Neopets, Inc." had always planned to "bring the online and offline worlds together in ways that have never been done before". An investment banker at Allen & Company in New York said that "Neopets" was the only online media he had seen "that might have the ability to capture market share in the offline world". Neopets, Inc. had signed various a licensing deals with companies such as Viacom Consumer Products, Thinkway Toys, and Jakks Pacific. Wizards of the Coast released the "Neopets Trading Card Game" in September 2003, which has been promoted in three of General Mills "Big G" cereals and ten Simon Property Group malls. and received two different nominations for "Toy of the Year" and two other recognitions. "Neopets: The Official Magazine" was a bi-monthly magazine released the same month but was replaced in 2008 by "Beckett Plushie Pals", which featured "Neopets" news as well as other companies' products. In 2005, "Neopets" expanded to film and video game deals. The first movie was to be written by Ron Lieber and produced by Dylan Sellers and John A. Davis, but the project has since been cancelled with no other projects announced. Two video games were released by Sony Computer Entertainment, ' for the PlayStation 2 in 2005 and ' for the PlayStation Portable in 2006.
The JumpStart Era.
JumpStart acquired Neopets from Viacom in April 2014. Server migration began in September. JumpStart-owned Neopets was immediately characterized by glitches and site lag. On March 6, 2015, much of the Neopets Team remaining from Viacom were laid off for as-yet unknown reasons.

</doc>
<doc id="22018" url="http://en.wikipedia.org/wiki?curid=22018" title="Nashville, Tennessee">
Nashville, Tennessee

Nashville is the capital of the U.S. state of Tennessee and the county seat of Davidson County. Nashville is the second largest city in Tennessee, after Memphis, and is the fourth largest city in the Southeastern United States. It is located on the Cumberland River in the north-central part of the state. The city is a center for the music, healthcare, publishing, banking and transportation industries, and is home to numerous colleges and universities. Reflecting the city's position in state government, Nashville is home to the Tennessee Supreme Court's courthouse for Middle Tennessee. It is known as a center of the music industry, earning it the nickname "Music City".
Since 1963, Nashville has had a consolidated city-county government which includes six smaller municipalities in a two-tier system. Thirty-five of 40 members are elected from single-member districts; five are elected at-large. According to the 2013 American Community Survey estimates, the consolidated county population stood at 658,602; not including the semi-independent municipalities, the population was 634,464. The 2013 population of the entire 13-county Nashville metropolitan area was 1,757,912, making it the largest metropolitan statistical area in the state. The 2013 population of the Nashville-Davidson–Murfreesboro–Columbia combined statistical area, a larger trade area, was 1,876,933.
History.
The town of Nashville was founded by James Robertson, John Donelson, and a party of Overmountain Men in 1779, near the original Cumberland settlement of Fort Nashborough. It was named for Francis Nash, the American Revolutionary War hero. Nashville quickly grew because of its strategic location, accessibility as a port on the Cumberland River, a tributary of the Ohio River; and its later status as a major railroad center. By 1800, the city had 345 residents, including 136 African American slaves and 14 free blacks. In 1806, Nashville was incorporated as a city and became the county seat of Davidson County, Tennessee. In 1843, the city was named the permanent capital of the state of Tennessee.
By 1860, when the first rumblings of secession began to be heard across the South, antebellum Nashville was a very prosperous city. The city's significance as a shipping port made it a desirable prize as a means of controlling important river and railroad transportation routes. In February 1862, Nashville became the first state capital to fall to Union troops. The state was occupied by Union troops for the duration of the war. The Battle of Nashville (December 15–16, 1864) was a significant Union victory and perhaps the most decisive tactical victory gained by either side in the war.
Within a few years after the Civil War, the city had reclaimed its important shipping and trading position and also developed a solid manufacturing base. The post–Civil War years of the late 19th century brought new prosperity to Nashville and Davidson County. These healthy economic times left the city with a legacy of grand classical-style buildings, which can still be seen around the downtown area.
20th century to present.
About 1950 the state legislature approved a new city charter that provided for election of city council members from single-member districts, rather than at-large voting. This change was supported because at-large voting diluted the minority population's political power in the city. They could seldom gain a majority of the population to support a candidate of their choice. In 1946, whites numbered about five thousand, and blacks about three thousand in the city. 
Apportionment under the single-member districts meant that some districts had black majorities. In 1952 after passage of the new charter, two African Americans were elected to the city council, the first to gain office since 1911, after disenfranchisement had been achieved by the state government, although before that Tennessee had competitive politics. Both men were attorneys.
The years after World War II were a time of rapid suburbanization as new housing was built outside the city limits. This resulted in a demand for many new schools and other support facilities, which the county found difficult to provide. At the same time, suburbanization had resulted in a declining tax base in the city, although many suburban residents used unique city amenities and services which were supported only by city taxpayers. After years of discussion, a referendum was held in 1958 on the issue of consolidating city and county government. It failed to gain approval although it was supported by elected leaders of both jurisdictions: County Judge Beverly Briley of Davidson and Mayor Ben West of Nashville.
Following the failure of the referendum, Nashville annexed some 42 square miles of suburban jurisdictions in order to expand its tax base. This increased uncertainty among residents, and created resentment among many suburban communities. Under the second charter for metropolitan government, which was approved in 1962, two levels of service provision were proposed: the General Services District and the Urban Services District, to provide for a differential in tax levels. Residents of the Urban Services District had a full range of city services. The areas that comprised the General Services District, however, had a lower tax rate until full services were provided. This helped reconcile aspects of services and taxation among the differing jurisdictions within the large metro region.
In 1963, Nashville consolidated its government with Davidson County, forming a metropolitan government. The membership on the Metro Council, the legislative body, was increased from 21 to 40 seats, compared to the first proposed charter. Of these, 5 members are elected at-large and 35 are elected from single-member districts, each to serve a term of four years.
Since the 1970s, the city and county have experienced tremendous growth, particularly during the economic boom of the 1990s under the leadership of then-Mayor and later-Tennessee Governor, Phil Bredesen. He made urban renewal a priority, and fostered the construction or renovation of several city landmarks, including the Country Music Hall of Fame and Museum, the downtown Nashville Public Library, the Bridgestone Arena, and LP Field.
LP Field (formerly Adelphia Coliseum) was built after the National Football League's (NFL) Houston Oilers agreed to move to the city in 1995. The NFL team debuted in Nashville in 1998 at Vanderbilt Stadium, and LP Field opened in the summer of 1999. The Oilers changed their name to the Tennessee Titans and finished the season with the Music City Miracle and a close Super Bowl game in which the St. Louis Rams‍ '​ win was secured in the last play.
In 1997 Nashville was awarded a National Hockey League NHL expansion team; this was named the Nashville Predators. Since the 2003/04 season, the Nashville Predators have made the playoffs every season except for two.
Today, the city along the Cumberland River is a crossroads of American culture, and one of the fastest-growing areas of the Upland South.
Geography.
Topography.
Nashville lies on the Cumberland River in the northwestern portion of the Nashville Basin. Nashville's topography ranges from 385 ft above sea level at the Cumberland River to 1160 ft above sea level at its highest point.
According to the United States Census Bureau, the city has a total area of 527.9 sqmi, of which 504.0 sqmi of it is land and 23.9 sqmi of it (4.53%) is water.
Climate.
Nashville has a humid subtropical climate (Köppen "Cfa"), with generally cool to moderately cold winters, and hot, humid summers. Monthly averages range from 37.7 F in January to 79.4 F in July, with a diurnal temperature variation of 18.2 to. In the winter months, snowfall does occur in Nashville, but is usually not heavy. Average annual snowfall is about 6.3 in, falling mostly in January and February and occasionally March and December. The largest snow event since 2000 was on January 16, 2003, when Nashville received 7 in of snow in a single storm; the largest on record was 17 in, received on March 17, 1892. Rainfall is typically greater in November and December, and spring, while August to October are the driest months on average. Spring and fall are generally warm but prone to severe thunderstorms, which occasionally bring tornadoes—with recent major events on April 16, 1998; April 7, 2006; February 5, 2008; April 10, 2009; and May 1–2, 2010. Relative humidity in Nashville averages 83% in the mornings and 60% in the afternoons, which is considered moderate for the Southeastern United States. In recent decades, due to urban development, Nashville has developed an urban heat island (UHI); especially on cool, clear nights, temperatures are up to 10 F-change warmer in the heart of the city than in rural outlying areas. The entire Nashville region lies within USDA Plant Hardiness Zone 7a.
Nashville's long springs and autumns combined with a diverse array of trees and grasses can often make it uncomfortable for allergy sufferers. In 2008, Nashville was ranked as the 18th-worst spring allergy city in the U.S. by the Asthma and Allergy Foundation of America.
The coldest temperature ever recorded in Nashville was -17 °F on January 21, 1985, and the highest was 109 °F on June 29, 2012.
Cityscape.
The downtown area of Nashville features a diverse assortment of entertainment, dining, cultural and architectural attractions. The Broadway and 2nd Avenue areas feature entertainment venues, night clubs and an assortment of restaurants. North of Broadway lie Nashville's central business district, Legislative Plaza, Capitol Hill and the Tennessee Bicentennial Mall. Cultural and architectural attractions can be found throughout the city.
Three major interstate highways (I-40, I-65 and I-24) converge near the core area of downtown, and many regional cities are within a day's driving distance.
Nashville's first skyscraper, the Life & Casualty Tower, was completed in 1957 and started the construction of high rises in downtown Nashville. After the construction of the AT&T Building (commonly known to locals as the "Batman Building") in 1994, the downtown area saw little construction until the mid-2000s. Many new residential developments have been constructed or are planned for various neighborhoods in the city. A new high rise office building, The Pinnacle, was recently opened in 2010.
Many civic and infrastructure projects are either being planned, in progress, or recently completed. A new MTA bus hub was recently completed in downtown Nashville, as was the Music City Star pilot project. Several public parks have been constructed, such as the Public Square. Riverfront Park is scheduled to be extensively updated. The Music City Center, a convention center project, is a 1,200,000 square foot (110,000 m2) convention center with 370,000 square feet (34,000 m2) of exhibit space. It opened in May 2013.
Demographics.
The data below is for all of Metropolitan Nashville-Davidson County, including other incorporated cities within the consolidated city–county (such as Belle Meade and Berry Hill). See Nashville-Davidson (balance) for demographic data on Nashville-Davidson County excluding separately incorporated cities.
According to the 2009 American Community Survey, there were 628,434 people residing in the city. The population density was 1204.2 PD/sqmi. There were 282,452 housing units at an average density of 560.4 /sqmi.
At the 2010 census, the racial makeup of the city was 60.5% White (56.3% non-Hispanic white), 28.4% African American, 0.3% American Indian and Alaska Native, 3.1% Asian, 0.1% Native Hawaiian and Other Pacific Islander, 2.5% from two or more races. 10.0% of the total population was of Hispanic or Latino origin (they may be of any race). The non-Hispanic White population was 79.5% in 1970.
There were 254,651 households and 141,469 families (55.6% of households). Of households with families, 37.2% had married couples living together, 14.1% had a female householder with no husband present, and 4.2% had a male householder with no wife present. 27.9% of all households had children under the age of 18, and 18.8% had at least one member 65 years of age or older. Of the 44.4% of households that are non-families, 36.2% were individuals and 8.2% had someone living alone who was 65 years of age or older. The average household size was 2.38 and the average family size was 3.16.
The age distribution was 22% under 18, 10% from 18 to 24, 33% from 25 to 44, 24% from 45 to 64, and 11% who were 65 or older. The median age was 34.2 years. For every 100 females there were 94.1 males. For every 100 females age 18 and over, there were 91.7 males.
The median income for a household in the city was $46,141, and the median income for a family was $56,377. Males with a year-round, full-time job had a median income of $41,017 versus $36,292 for females. The per capita income for the city was $27,372. About 13.9% of families and 18.2% of the population were below the poverty line, including 29.5% of those under age 18 and 9.9% of those age 65 or over. Of residents 25 or older, 33.4% have a bachelor's degree or higher.
Because of its relatively low cost of living and large job market, Nashville has become a popular city for immigrants. Nashville's foreign-born population more than tripled in size between 1990 and 2000, increasing from 12,662 to 39,596. The city's largest immigrant groups include Mexicans, Kurds, Vietnamese, Laotians, Arabs, and Somalis . There are also smaller communities of Pashtuns from Afghanistan and Pakistan concentrated primarily in Antioch. Nashville has the largest Kurdish community in the United States, numbering approximately 11,000. In 2009, about 60,000 Bhutanese refugees were being admitted to the U.S., and some were expected to resettle in Nashville. During the Iraqi election of 2005, Nashville was one of the few international locations where Iraqi expatriates could vote. The American Jewish community in Nashville dates back over 150 years, and numbered about 7,800 in 2002.
Metropolitan area.
s of 2013[ [update]], Nashville has the largest metropolitan area in the state of Tennessee, spanning 13 counties and an estimated population of 1,757,912. The Nashville metropolitan statistical area encompasses 13 of 41 Middle Tennessee counties: Cannon, Cheatham, Davidson, Dickson, Hickman, Macon, Robertson, Rutherford, Smith, Sumner, Trousdale, Williamson, and Wilson. The 2013 population of the Nashville-Davidson–Murfreesboro–Columbia combined statistical area was estimated at 1,876,933.
Economy.
As the "home of country music", Nashville has become a major music recording and production center. All of the Big Four record labels, as well as numerous independent labels, have offices in Nashville, mostly in the Music Row area. Nashville has been home to the headquarters of guitar company Gibson since 1984. Since the 1960s, Nashville has been the second-largest music production center (after New York) in the U.S. As of 2006, Nashville's music industry is estimated to have a total economic impact of $6.4 billion per year and to contribute 19,000 jobs to the Nashville area.
Although Nashville is renowned as a music recording center and tourist destination, its largest industry is health care. Nashville is home to more than 300 health care companies, including Hospital Corporation of America (HCA), the largest private operator of hospitals in the world. s of 2012[ [update]], it is estimated that the health care industry contributes US$ per year and 200,000 jobs to the Nashville-area economy.
The automotive industry is also becoming increasingly important for the entire Middle Tennessee region. Nissan North America moved its corporate headquarters in 2006 from Gardena, California (Los Angeles County) to Franklin, southwest of Nashville. Nissan also has its largest North American manufacturing plant in Smyrna, Tennessee. Largely as a result of the increased development of Nissan and other Japanese economic interests in the region, Japan moved its New Orleans consulate-general to Nashville's Palmer Plaza.
Other major industries in Nashville include insurance, finance, and publishing (especially religious publishing). The city hosts headquarters operations for several Protestant denominations, including the United Methodist Church, Southern Baptist Convention, National Baptist Convention USA, and the National Association of Free Will Baptists.
Nashville is also known for some of their famously popular Southern confections, including Goo Goo Clusters (which have been made in Nashville since 1912).
Fortune 500 companies with offices within Nashville include Dell, HCA and Dollar General.
In 2013, the city ranked No. 5 on "Forbes"‍ '​ list of the Best Places for Business and Careers.
Top employers.
According to the city's 2010 Comprehensive Annual Financial Report, the top employers in the city are:
Culture.
Much of the city's cultural life has revolved around its large university community. Particularly significant in this respect were two groups of critics and writers who were associated with Vanderbilt University in the early twentieth century: the Fugitives and the Agrarians.
Popular destinations include Fort Nashborough and Fort Negley, the former being a reconstruction of the original settlement, the latter being a semi-restored Civil War battle fort; the Tennessee State Museum; and The Parthenon, a full-scale replica of the original Parthenon in Athens. The Tennessee State Capitol is one of the oldest working state capitol buildings in the nation, while The Hermitage is one of the older presidential homes open to the public.
Dining.
Although best known for its music, Nashville is a city filled with many dining destinations. Some of the more popular types of local cuisine include hot chicken, hot fish, barbecue, and meat and three. Thanks, in part, to Nashville's foodie culture, the city was ranked as the 13th "snobbiest" city in America according to "Travel + Leisure" magazine.
Entertainment and performing arts.
Nashville has a vibrant music and entertainment scene spanning a variety of genres. The Tennessee Performing Arts Center is the major performing arts center of the city. It is the home of the Tennessee Repertory Theatre, the Nashville Opera, the Music City Drum and Bugle Corps, and the Nashville Ballet. In September 2006, the Schermerhorn Symphony Center opened as the home of the Nashville Symphony.
As the city's name itself is a metonym for the country music industry, many popular tourist sites involve country music, including the Country Music Hall of Fame and Museum, Belcourt Theatre, and Ryman Auditorium. Ryman was home to the "Grand Ole Opry" until 1974 when the show moved to the Grand Ole Opry House, 9 mi east of downtown. The "Opry" plays there several times a week, except for an annual winter run at the Ryman.
Numerous music clubs and honky-tonk bars can be found in downtown Nashville, especially the area encompassing Lower Broadway, Second Avenue, and Printer's Alley, which is often referred to as "the District".
Each year, the CMA Music Festival (formerly known as Fan Fair) brings thousands of country fans to the city. The Tennessee State Fair is also held annually in September.
Nashville was once home of television shows such as "Hee Haw" and "Pop! Goes the Country", as well as The Nashville Network. Country Music Television, RFD TV, and Great American Country currently operate from Nashville. The city was also home to the Opryland USA theme park, which operated from 1972 to 1997 before being closed by its owners (Gaylord Entertainment Company) and soon after demolished to make room for the Opry Mills mega-shopping mall.
The Contemporary Christian music industry is based along Nashville's Music Row, with a great influence in neighboring Williamson County. The Christian record companies include EMI Christian Music Group, Provident Label Group and Word Records.
Music Row houses many gospel music, and Contemporary Christian music companies centered around 16th and 17th Avenues South.
Although Nashville was never known as a jazz town, it did have many great jazz bands, including The Nashville Jazz Machine led by Dave Converse and its current version, the Nashville Jazz Orchestra, led by Jim Williamson, as well as The Establishment, led by Billy Adair. The Francis Craig Orchestra entertained Nashvillians from 1929 to 1945 from the Oak Bar and Grille Room in the Hermitage Hotel. Craig's orchestra was also the first to broadcast over local radio station WSM-AM and enjoyed phenomenal success with a 12-year show on the NBC Radio Network. In the late 1930s, he introduced a newcomer, Dinah Shore, a local graduate of Hume Fogg High School and Vanderbilt University.
Radio station WMOT-FM in nearby Murfreesboro has aided significantly in the recent revival of the city's jazz scene, as has the non-profit Nashville Jazz Workshop, which holds concerts and classes in a renovated building in the north Nashville neighborhood of Germantown. Fisk University also maintains a jazz station.
Nashville has an active theatre scene, and is home to several professional and community theatre companies. Nashville Children's Theatre, Tennessee Repertory Theatre, the Nashville Shakespeare Festival, the Dance Theatre of Tennessee and the Tennessee Women's Theater Project are among the most prominent professional companies. One community theatre, Circle Players, has been in operation for over 60 years.
Tourism.
Perhaps the biggest factor in drawing visitors to Nashville is its association with country music. Many visitors to Nashville attend live performances of the Grand Ole Opry, the world's longest running live radio show. The Country Music Hall of Fame and Museum is another major attraction relating to the popularity of country music. The Gaylord Opryland Resort & Convention Center, the Opry Mills regional shopping mall and the "General Jackson" showboat, are all located in what is known as Music Valley.
Civil War history is important to the city's tourism industry. Sites pertaining to the Battle of Nashville and the nearby Battle of Franklin and Battle of Stones River can be seen, along with several well-preserved antebellum plantation houses such as Belle Meade Plantation, Carnton plantation in Franklin, and Belmont Mansion.
Nashville has several arts centers and museums, including the Frist Center for the Visual Arts, Cheekwood Botanical Garden and Museum of Art, the Tennessee State Museum, the Johnny Cash Museum, Fisk University's Van Vechten and Aaron Douglas Galleries, Vanderbilt University's Fine Art Gallery and Sarratt Gallery, and the full-scale replica of the Parthenon.
Nicknames.
Nashville is a colorful, well-known city in several different arenas. As such, it has earned various sobriquets, including:
Nashville has additionally earned the moniker "The Hot Chicken Capital", becoming known for the local specialty cuisine hot chicken. The Music City Hot Chicken Festival is hosted annually in Nashville and several restaurants make this spicy version of southern fried chicken.
Sports.
Nashville hosted a team called the Nashville Rebels which participated in the 1938 American Football League, and two Arena Football League teams named the Nashville Kats: one that ran from 1997 to 2001 until they were sold to Atlanta and renamed as the Georgia Force; and another expansion franchise that competed from 2005 to 2007. Nashville also hosts the second longest continually operating race track in the United States, the Fairgrounds Speedway. Three Little League baseball teams from Nashville (one in 1970; one in 2013; and, one in 2014) have qualified for the Little League World Series. A team from neighboring Goodlettsville qualifed for the 2012 series, giving the metropolitan area teams in three consecutive years to so qualify.
Professional.
Nashville has several professional sports teams, of which two, the Nashville Predators of the NHL and the Tennessee Titans of the NFL, play at the highest professional level of their respective sports. Nashville is also home to the NCAA college football Music City Bowl and the Fairgrounds Speedway, a NASCAR Whelen All-American Series racetrack.
College.
Nashville is also home to four Division I athletic programs.
Parks and gardens.
Metro Board of Parks and Recreation owns and manages 10200 acre of land and 99 parks and greenways (comprising more than 3% of the total area of the county).
Warner Parks, situated on 2684 acre of land, consists of a 5000 sqft learning center, 20 mi of scenic roads, 12 mi of hiking trails, and 10 mi of horse trails. It is also the home of the annual Iroquois Steeplechase.
The United States Army Corps of Engineers maintains parks on Old Hickory Lake and Percy Priest Lake. These parks are used for activities such as fishing, waterskiing, sailing and boating. The Harbor Island Yacht Club makes its headquarters on Old Hickory Lake, and Percy Priest Lake is home to the Vanderbilt Sailing Club.
Other parks in Nashville include Centennial Park, Shelby Park, Cumberland Park, and Radnor Lake State Natural Area.
On August 27, 2013, Nashville mayor Karl Dean revealed plans for two new riverfront parks on the east and west banks of the Cumberland River downtown. Construction on the east bank park will begin fall of 2013, and the projected completion date for the west bank park is 2015. Among many exciting benefits of this Cumberland River re-development project is the construction of a highly anticipated outdoor amphitheater. Located on the west bank, this music venue will be surrounded by a new 12 acre park and will replace the previous thermal plant site. It will include room for 6,500 spectators with 2,500 removable seats and additional seating on an overlooking grassy knoll. In addition, the 4.5 acre east bank park will include a river landing, providing people access to the river. In regard to the parks' benefits for Nashvillian civilians, Mayor Dean remarked that "if done right, the thermal site can be an iconic park that generations of Nashvillians will be proud of and which they can enjoy" (WKRN-TV Nashville).
Law and government.
The city of Nashville and Davidson County merged in 1963 as a way for Nashville to combat the problems of urban sprawl. The combined entity is officially known as "the Metropolitan Government of Nashville and Davidson County", and is popularly known as "Metro Nashville" or simply "Metro". It offers services such as police, fire, electricity, water and sewage treatment. When the Metro government was formed in 1963, the government was split into two service districts—the "urban services district" and the "general services district." The urban services district encompasses the 1963 boundaries of the former City of Nashville, approximately 72 sqmi, and the general services district includes the remainder of Davidson County. There are six smaller municipalities within the consolidated city-county: Belle Meade, Berry Hill, Forest Hills, Oak Hill, Goodlettsville (partially), and Ridgetop (partially). These municipalities use a two-tier system of government, with the smaller municipality typically providing police services and the Metro Nashville government providing most other services. Previously, the city of Lakewood also had a separate charter. However, Lakewood residents voted in 2010 and 2011 to dissolve its city charter and join the metropolitan government, with both votes passing.
Nashville is governed by a mayor, vice-mayor, and 40-member Metropolitan Council. It uses the strong-mayor form of the mayor–council system. The current mayor of Nashville is Karl Dean. The Metropolitan Council is the legislative body of government for Nashville and Davidson County. There are five council members who are elected at large and 35 council members that represent individual districts. The Metro Council has regular meetings that are presided over by the vice-mayor, who is currently Diane Neighbors. The Metro Council meets on the first and third Tuesday of each month at 6:00 pm, according to the Metropolitan Charter.
Nashville is home to the Tennessee Supreme Court's courthouse for Middle Tennessee.
Politics.
Nashville has been a Democratic stronghold since at least the end of Reconstruction, and has remained staunchly Democratic even as the state as a whole has trended strongly Republican. Pockets of Republican influence exist in the wealthier portions of the city, but they are usually no match for the overwhelming Democratic trend in the rest of the city. While local elections are officially nonpartisan, nearly all of the city's elected officials are known to be Democrats. The city is split between 10 state house districts, all but two of which are held by Democrats. Three state senate districts and part of a fourth are within the city; two are held by Democrats and two by Republicans.
Democrats are no less dominant at the federal level. Democratic presidential candidates have only failed to carry Davidson County three times since the end of Reconstruction. In 1968, third-party candidate George C. Wallace carried Nashville with a plurality of 35.1 percent. In 1972, Richard Nixon became the first Republican to carry Nashville since Reconstruction, winning it with 61 percent of the vote as part of his 49-state landslide that year; as part of it, Nixon carried 90 of Tennessee's 95 counties. In 1988, George H. W. Bush narrowly won Nashville with 52 percent of the vote.
In most years, Democrats have carried Nashville at the presidential level with relatively little difficulty, even in years when they lose Tennessee as a whole. This has been especially true in recent elections. In the 2000 presidential election, Al Gore carried Nashville with over 59% of the vote even as he narrowly lost his home state. In the 2004 election, John Kerry carried Nashville with 55% of the vote even as George W. Bush won the state by 14 points. In 2008, Barack Obama carried Nashville with 60% of the vote even as John McCain won Tennessee by 15 points.
Despite its large size, Nashville has been in a single congressional district for most of the time since Reconstruction; it is currently numbered as the 5th District, represented by Democrat Jim Cooper. A Republican has not represented a significant portion of Nashville since 1874. Republicans made a few spirited challenges in the mid-1960s and early 1970s. The Republicans almost won it in 1968; only a strong showing by a candidate from Wallace's American Independent Party kept the seat in Democratic hands. However, they have not made a serious bid for the district since 1972, when the Republican candidate gained only 38% of the vote even as Nixon carried the district in the presidential election by a large margin. The district's best-known congressman was probably Jo Byrns, who represented the district from 1909 to 1936 and was Speaker of the House for much of Franklin Roosevelt's first term as President. Another nationally prominent congressman from Nashville was Percy Priest, who represented the district from 1941 to 1956 and was House Majority Whip from 1949 to 1953. Former mayors Richard Fulton and Bill Boner also sat in the U.S. House before assuming the Metro mayoral office.
From 2003 to 2013, a sliver of southwestern Nashville was located in the 7th District, represented by Republican Marsha Blackburn. This area was roughly coextensive with the portion of Nashville she'd represented in the state senate from 1998 to 2002. However, the 5th regained all of Nashville after the 2010 census.
Education.
Public schools.
The city is served by Metropolitan Nashville Public Schools.
Colleges and universities.
Nashville is often labeled the "Athens of the South" due to the many colleges and universities in the city and the metropolitan area. The colleges and universities in Nashville include:
Within 30 mi of Nashville in Murfreesboro is Middle Tennessee State University (MTSU), a full-sized public university with Tennessee's largest undergraduate population. Enrollment in post-secondary education in Nashville is around 43,000. Within the Nashville Metropolitan Statistical Area—which includes MTSU, Cumberland University (Lebanon), Volunteer State Community College (Gallatin), Daymar Institute, and O'More College of Design (Franklin)—total enrollment exceeds 74,000. Within a 40 mi radius are Austin Peay State University (Clarksville) and Columbia State Community College (Columbia), enrolling an additional 13,600.
Nashville is home to four historically black institutions of higher education: Fisk University, Tennessee State University, Meharry Medical College, and American Baptist College.
Media.
The daily newspaper in Nashville is "The Tennessean", which until 1998 competed with the "Nashville Banner", another daily paper that was housed in the same building under a joint-operating agreement. "The Tennessean" is the city's most widely circulated newspaper, while a smaller free daily called "The City Paper" shares the Nashville market. Online news service "NashvillePost.com" competes with the printed dailies to break local and state news. Several weekly papers are also published in Nashville, including "The Nashville Pride", "Nashville Business Journal", "Nashville Scene" and "The Tennessee Tribune". Historically, "The Tennessean" was associated with a broadly liberal editorial policy, while "The Banner" carried staunchly conservative views in its editorial pages; "The Banner"‍ '​s heritage is carried on these days by "The City Paper". The "Nashville Scene" is the area's alternative weekly broadsheet. "The Nashville Pride" is aimed towards community development and serves Nashville's entrepreneurial population.
Nashville is home to eleven broadcast television stations, although most households are served by direct cable network connections. Comcast Cable has a monopoly on terrestrial cable service in Davidson County (but not throughout the entire media market). Nashville is ranked as the 29th largest television market in the United States.
Nashville is also home to cable networks Country Music Television (CMT), among others. CMT's master control facilities are located in New York City with the other Viacom properties. The Top 20 Countdown and CMT Insider are taped in their Nashville studios. Shop at Home Network was once based in Nashville, but the channel signed off in 2008.
Several dozen FM and AM radio stations broadcast in the Nashville area, including five college stations and one LPFM community radio station. Nashville is ranked as the 44th largest radio market in the United States. WSM-FM is owned by Cumulus Media and is 95.5 FM. WSM-AM, owned by Gaylord Entertainment Company, can be heard nationally on 650 AM or online at from its studios located inside the Gaylord Opryland Resort & Convention Center. WSM is famous for carrying live broadcasts of the Grand Ole Opry, through which it helped spread the popularity of country music in America, and continues to broadcast country music throughout its broadcast day. WLAC, whose over-the-air signal is heard at 1510 AM, is a Clear Channel-owned talk station which was originally sponsored by the Life and Casualty Insurance Company of Tennessee, and its competitor WWTN is owned by Cumulus.
Several major motion pictures have been filmed in Nashville, including "The Green Mile", "The Last Castle", "Gummo", "The Thing Called Love", "Two Weeks", "Coal Miner's Daughter", "Nashville", and "Country Strong".
Transportation.
Road.
Nashville is centrally located at the crossroads of three Interstate Highways: I-40, I-24, and I-65. Interstate 440 is a bypass route connecting I-40, I-65, and I-24 south of downtown Nashville. Briley Parkway connects the north side of the city and its interstates. A number of arterial surface roads called "pikes" radiate from the city center; many carry the names of nearby towns to which they lead. Among these are Clarksville Pike, Gallatin Pike, Lebanon Pike, Murfreesboro Pike, Nolensville Pike, and Franklin Pike.
Bus.
The Metropolitan Transit Authority provides bus transit within the city, out of a newly built hub station downtown. Routes utilize a hub and spoke method. Expansion plans include use of Bus rapid transit for new routes, with the possibility for local rail service at some point in the future.
Nashville is considered a gateway city for rail and air traffic for the Piedmont Atlantic Megaregion.
Air.
The city is served by Nashville International Airport (BNA), which was a hub for American Airlines between 1986 and 1995 and is now a focus city for Southwest Airlines. During 2011, Nashville International was the 34th busiest passenger airport in the U.S. with a total of 4,673,047 passenger boardings. Major airlines serving Nashville include American Airlines, Delta Air Lines, Frontier Airlines, Southwest Airlines, United Airlines, US Airways, and AirCanada. AirTran Airways offered limited routing to the airport until it was deemed unprofitable.
Rail.
Amtrak.
Although a major freight hub for CSX Transportation, Nashville is not currently served by Amtrak, the second-largest metropolitan area in the U.S. to have this distinction. Amtrak's "Floridian" (Chicago-Florida via Louisville and Nashville) served Nashville until its cancellation on October 9, 1979 due to poor track conditions resulting in late trains and low ridership.
While there have been no proposals to restore Amtrak service to Nashville, there have been repeated calls from residents. However, Tennessee state officials have advised it will not be happening anytime soon due to scarce federal funding. "It would be wonderful to say I can be in Memphis and jump on a train to Nashville, but the volume of people who would do that isn't anywhere close to what the cost would be to provide the service," said Ed Cole, chief of environment and planning with the Tennessee Department of Transportation. Ross Capon, executive director of the National Association of Railroad Passengers, said rail trips would catch on if routes were expanded, but conceded that it would be nearly impossible to resume Amtrak service to Nashville without a substantial investment from the state because federal money has dried up.
Commuter.
Nashville launched a passenger commuter rail system called the Music City Star on September 18, 2006. The only currently operational leg of the system connects the city of Lebanon to downtown Nashville at the Nashville Riverfront. Legs to Clarksville, Murfreesboro and Gallatin are currently in the feasibility study stage. The system plan includes seven legs connecting Nashville to surrounding suburbs.
Bridges within the city include:
Sister cities.
Nashville is an active participant in the Sister Cities program and has relationships with the following towns:
 Caen (France)
 Crouy (France)

</doc>
<doc id="22024" url="http://en.wikipedia.org/wiki?curid=22024" title="Novial">
Novial

Novial ["nov-" ("new") + "IAL, International Auxiliary Language"] is a constructed international auxiliary language (IAL) for universal communication between speakers of different native languages. It was devised by Otto Jespersen, a Danish linguist who was previously involved in the Ido movement, and subsequently in the development of Interlingua.
Its vocabulary is based largely on the Germanic and Romance languages and its grammar is influenced by English.
Novial was first introduced in Jespersen's book "An International Language" in 1928. It was updated in his dictionary "Novial Lexike" in 1930, and further modifications were proposed in the 1930s, but the language became dormant with Jespersen's death in 1943. In the 1990s, with the revival of interest in constructed languages brought on by the Internet, some people rediscovered Novial.
An international language.
Novial was first described in Jespersen’s book "An International Language" (1928). Part One of the book discusses the need for an IAL, the disadvantages of ethnic languages for that purpose, and common objections to constructed IALs. He also provides a critical overview of the history of constructed IALs with sections devoted to Volapük, Esperanto, Idiom Neutral, Ido, Latino sine Flexione and Occidental (Interlingue). The author makes it clear that he draws on a wealth of earlier work on the problem of a constructed IAL, not only the aforementioned IALs.
Part Two of "An International Language" describes Novial in detail. Alternative possible solutions for problems in the phonology, orthography, grammar and vocabulary are considered. The choices made are explained by comparison with ethnic languages and previously constructed IALs.
Alphabet and pronunciation.
Jespersen suggested that it might be possible instead of the digraph SH to use the phonetic symbol ʃ.
For more details, see the of the Novial Wikibook.
Grammar.
Personal pronouns, subject and object.
Note that in Novial the Nominative and Oblique pronouns are the same.
The standard word order is subject-verb-object, as in English. Therefore, the object need not be marked to distinguish it from the subject:
E.g.:
The accusative (direct object) is therefore most often identical to the nominative (subject). However, in case of an ambiguity problem, an optional accusative ending, -m (-em after a consonant), is available but is rarely used. The preposition em is equivalent to this ending.
The personal possessive adjectives are formed from the pronouns by adding -n or after a consonant -en. This is in fact the genitive (possessive) of the pronoun so "men" means both "my" and "mine" ("of me"):
E.g.:
Possession may also be expressed with the preposition de: "de me", "de vu", and so on.
Verbs.
Verb forms never change with person or number. Most verb tenses, moods and voices are expressed with auxiliary verbs preceding the root form of the main verb. The auxiliaries follow the same word order as the English equivalent. The pronouns are indicated with parentheses and are given for example purposes.
Novial clearly distinguishes the passive of becoming and the passive of being. In English the forms are often the same, using the auxiliary verb "to be" followed by the past participle. However, the passive of becoming is also often expressed with the verb "to get" which is used in the examples below.
The passive voice of becoming is formed with the auxiliary bli followed by the root verb form.
The passive voice of being is formed with the auxiliary es followed by the past passive participle (stem + -t).
Articles.
The definite article is "li" which is invariant. It is used as in English.
There is no indefinite article, although "un" (one) can be used.
Nouns.
The plural noun is formed by adding –s to the singular (-es after a consonant).
The accusative case is generally identical to the nominative but can optionally be marked with the ending -m (-em after a consonant) with the plural being -sem (-esem after a consonant) or with the preposition em.
The genitive is formed with the ending -n (-en after a consonant) with the plural being -sen (-esen after a consonant) or with the preposition de.
Other cases are formed with prepositions.
Adjectives.
All adjectives end in -i, but this may be dropped if it is easy enough to pronounce and no confusion will be caused. Adjectives precede the noun qualified. Adjectives do not agree with the noun but may be given noun endings if there is no noun present to receive them.
Adverbs.
An adjective is converted to a corresponding adverb by adding -m after the -i ending of the adjective.
Vocabulary.
Affixes.
See the and at the Novial Wikibook.
Novial compared to Esperanto and Ido.
Jespersen was a professional linguist, unlike Esperanto's creator. He disliked the arbitrary and artificial character that he found in Esperanto and Ido. Additionally, he objected to those languages' inflectional systems, which he found needlessly complex. He sought to make Novial at once euphonious and regular while also preserving useful structures from natural languages.
In Novial:
A major difference between Novial and Esperanto/Ido concerns noun endings. Jespersen rejected a single vowel to terminate all nouns (-o in Esperanto/Ido), finding it unnatural and potentially confusing. Instead, Novial nouns may end in "-o", "-a", "-e", or "-u" or "-um". These endings may be taken to indicate natural sex according to the custom in Romance languages. Also there is no grammatical gender or requirement for adjectives to agree with nouns.
Language sample for comparison.
Here is the Lord's Prayer in Novial and several related languages:
Criticism.
As Jespersen relates in his autobiography, in 1934 he proposed an orthographic reform to Novial, which displeased a part of the users. Jespersen abandoned the essential principle of "one sound, one letter" :
 I proposed some not inconsiderable amendments, especially by introducing an "orthographic" Novial alongside the original phonetically written language. (...) Thus the sound [k], besides being represented by the letters "k" and "q" and the first part of "x", also acquired the new sign "c" (before "a, o, u" and consonants), a practice with which nearly all Europeans, Americans, and Australians are familiar from childhood. (...) I know that this orthographic form has displeased several of Novial's old and faithful friends, but it is my impression that many others have applauded it.
 — Otto Jespersen (1995 [1938], pp. 227–8)

</doc>
<doc id="22026" url="http://en.wikipedia.org/wiki?curid=22026" title="Musical note">
Musical note

In music, the term note has two primary meanings:
Notes are the "atoms" of much written music: discretizations of musical phenomena that facilitate performance, comprehension, and analysis.
The term "note" can be used in both generic and specific senses: one might say either "the piece 'Happy Birthday to You' begins with two notes having the same pitch," or "the piece begins with two repetitions of the same note." In the former case, one uses "note" to refer to a specific musical event; in the latter, one uses the term to refer to a class of events sharing the same pitch. (See also: Key signature names and translations).
Two notes with fundamental frequencies in a ratio equal to any power of two (e.g. half, twice, or four times) are perceived as very similar. Because of that, all notes with these kinds of relations can be grouped under the same pitch class.
In traditional music theory, most countries in the world use the naming convention Do-Re-Mi-Fa-Sol-La-Si, including for instance Italy, Spain, France, Romania, most Latin American countries, Greece, Bulgaria, Turkey, Russia, and all the Arabic-speaking or Persian-speaking countries. However, within the English-speaking and Dutch-speaking world, pitch classes are typically represented by the first seven letters of the Latin alphabet (A, B, C, D, E, F and G). A few European countries, including Germany, adopt an almost identical notation, in which H is substituted for B (see below for details). 
The eighth note, or octave, is given the same name as the first, but has double its frequency. The name octave is also used to indicate the span between a note and another with double frequency. To differentiate two notes that have the same pitch class but fall into different octaves, the system of scientific pitch notation combines a letter name with an Arabic numeral designating a specific octave. For example, the now-standard tuning pitch for most Western music, 440 Hz, is named a′ or A4.
There are two formal systems to define each note and octave, the Helmholtz pitch notation and the Scientific pitch notation.
Accidentals.
Letter names are modified by the accidentals. A sharp ♯ raises a note by a semitone or half-step, and a flat ♭ lowers it by the same amount. In modern tuning a half step has a frequency ratio of formula_1, approximately 1.059. The accidentals are written after the note name: so, for example, F♯ represents F-sharp, B♭ is B-flat.
Additional accidentals are the double-sharp , raising the frequency by two semitones, and double-flat , lowering it by that amount.
In musical notation, accidentals are placed before the note symbols. Systematic alterations to the seven lettered pitches in the scale can be indicated by placing the symbols in the key signature, which then apply implicitly to all occurrences of corresponding notes. Explicitly noted accidentals can be used to override this effect for the remainder of a bar. A special accidental, the natural symbol ♮, is used to indicate an unmodified pitch. Effects of key signature and local accidentals do not cumulate. If the key signature indicates G-sharp, a local flat before a G makes it G-flat (not G natural), though often this type of rare accidental is expressed as a natural, followed by a flat (♮♭) to make this clear. Likewise (and more commonly), a double sharp sign on a key signature with a single sharp ♯ indicates only a double sharp, not a triple sharp.
Assuming enharmonicity, many accidentals will create equivalences between pitches that are written differently. For instance, raising the note B to B♯ is equal to the note C. Assuming all such equivalences, the complete chromatic scale adds five additional pitch classes to the original seven lettered notes for a total of 12 (the 13th note completing the octave), each separated by a half-step.
Notes that belong to the diatonic scale relevant in the context are sometimes called "diatonic notes"; notes that do not meet that criterion are then sometimes called "chromatic notes".
Another style of notation, rarely used in English, uses the suffix "is" to indicate a sharp and "es" (only "s" after A and E) for a flat, e.g. Fis for F♯, Ges for G♭, Es for E♭. This system first arose in Germany and is used in almost all European countries whose main language is not English, Greek, or a Romance language.
In most countries using these suffixes, the letter H is used to represent what is B natural in English, the letter B is used instead of B♭, and Heses (i.e., H) is used instead of B (not Bes, which would also have fit into the system). Dutch-speakers in Belgium and the Netherlands use the same suffixes, but applied throughout to the notes A to G, so that B, B♭ and B have the same meaning as in English, although they are called B, Bes, and Beses instead of B, B flat and B double flat. Denmark also uses H, but uses Bes instead of Heses for B.
12-tone chromatic scale.
The following chart lists the names used in different countries for the 12 notes of a chromatic scale built on C. The corresponding symbols are shown within parenthesis. Differences between German and English notation are highlighted in bold typeface. Although the English and Dutch names are different, the corresponding symbols are identical.
Note designation in accordance with octave name.
The table of each octave and the frequencies for every note of pitch class A is shown below. The traditional (Helmholtz) system centers on the great octave (with capital letters) and small octave (with lower case letters). Lower octaves are named "contra" (with primes before), higher ones "lined" (with primes after). Another system (scientific) suffixes a number (starting with 0, or sometimes -1). In this system A4 is nowadays standardised to 440 Hz, lying in the octave containing notes from C4 (middle C) to B4. The lowest note on most pianos is A0, the highest C8. The MIDI system for electronic musical instruments and computers uses a straight count starting with note 0 for C-1 at 8.1758 Hz up to note 127 for G9 at 12,544 Hz.
Written notes.
A written note can also have a note value, a code that determines the note's relative duration. In order of halving duration, we have: double note (breve); whole note (semibreve); half note (minim); quarter note (crotchet); eighth note (quaver); sixteenth note (semiquaver). Smaller still are the thirty-second note (demisemiquaver), sixty-fourth note (hemidemisemiquaver), and hundred twenty-eighth note (semihemidemisemiquaver) or note, note, note, note, note, note, and note.
When notes are written out in a score, each note is assigned a specific vertical position on a staff position (a line or a space) on the staff, as determined by the clef. Each line or space is assigned a note name. These names are memorized by musicians and allow them to know at a glance the proper pitch to play on their instruments for each note-head marked on the page.
<score lang="ABC">
X:1
M:none
L:1/4
K:C
C4 D4 E4 F4 G4 A4 B4 c4 B4 A4 G4 F4 E4 D4 C4
</score>
The staff above shows the notes C, D, E, F, G, A, B, C    and then in reverse order, with no key signature or accidentals.
Note frequency (hertz).
In all technicality, "music" can be composed of notes at any arbitrary physical frequency. Since the physical causes of music are vibrations of mechanical systems, they are often measured in hertz (Hz), with 1 Hz = one vibration per second. For historical and other reasons, especially in Western music, only twelve notes of fixed frequencies are used. These fixed frequencies are mathematically related to each other, and are defined around the central note, "A4". The current "standard pitch" or modern "concert pitch" for this note is 440 Hz, although this varies in actual practice (see History of pitch standards).
The note-naming convention specifies a letter, any accidentals, and an octave number. Any note is an integer of half-steps away from middle A (A4). Let this distance be denoted "n". If the note is above A4, then "n" is positive; if it is below A4, then "n" is negative. The frequency of the note ("f") (assuming equal temperament) is then:
For example, one can find the frequency of C5, the first C above A4. There are 3 half-steps between A4 and C5 (A4 → A♯4 → B4 → C5), and the note is above A4, so "n" = +3. The note's frequency is:
To find the frequency of a note below A4, the value of "n" is negative. For example, the F below A4 is F4. There are 4 half-steps (A4 → A♭4 → G4 → G♭4 → F4), and the note is below A4, so "n" = −4. The note's frequency is:
Finally, it can be seen from this formula that octaves automatically yield powers of two times the original frequency, since "n" is therefore a multiple of 12 (12"k", where "k" is the number of octaves up or down), and so the formula reduces to:
yielding a factor of 2. In fact, this is the means by which this formula is derived, combined with the notion of equally-spaced intervals.
The distance of an equally tempered semitone is divided into 100 cents. So 1200 cents are equal to one octave — a frequency ratio of 2:1. This means that a cent is precisely equal to the 1200th root of 2, which is approximately 1.000578.
For use with the MIDI (Musical Instrument Digital Interface) standard, a frequency mapping is defined by:
Where p is the MIDI note number.
And in the opposite direction, to obtain the frequency from a MIDI note p, the formula is defined as:
For notes in an A440 equal temperament, this formula delivers the standard MIDI note number (p). Any other frequencies fill the space between the whole numbers evenly. This allows MIDI instruments to be tuned very accurately in any microtuning scale, including non-western traditional tunings.
History of note names.
Music notation systems have used letters of the alphabet for centuries. The 6th century philosopher Boethius is known to have used the first fourteen letters of the classical Latin alphabet,
to signify the notes of the two-octave range that was in use at the time, and which in modern scientific pitch notation is represented as
Though it is not known whether this was his devising or common usage at the time, this is nonetheless called "Boethian notation". Although Boethius is the first author which is known to have used this nomenclature in the literature, the above-mentioned two-octave range was already known five centuries before by Ptolemy, who called it the "perfect system" or "complete system", as opposed to other systems of notes of smaller range, which did not contain all the possible species of octave (i.e., the seven octaves starting from A, B, C, D, E, F, and G).
Following this, the range (or compass) of used notes was extended to three octaves, and the system of repeating letters A-G in each octave was introduced, these being written as lower case for the second octave (a-g) and double lowercase letters for the third (aa-gg). When the range was extended down by one note, to a G, that note was denoted using the Greek G (Γ), gamma. (It is from this that the French word for scale, "gamme" is derived, and the English word gamut, from "Gamma-Ut", the lowest note in Medieval music notation.)
The remaining five notes of the chromatic scale (the black keys on a piano keyboard) were added gradually; the first being B♭, since B was flattened in certain modes to avoid the dissonant tritone interval. This change was not always shown in notation, but when written, B♭ (B-flat) was written as a Latin, round "b", and B♮ (B-natural) a Gothic or "hard-edged" b. These evolved into the modern flat (♭) and natural (♮) symbols respectively. The sharp symbol arose from a barred b, called the "cancelled b".
In parts of Europe, including Germany, the Czech Republic, Slovakia, Poland, Hungary, Norway, Denmark, Serbia, Croatia, Finland, Iceland and Sweden, the Gothic b transformed into the letter H (possibly for "hart", German for "hard", or just because the Gothic b resembled an H). Therefore, in German music notation, H is used in lieu of B♮ (B-natural), and B in lieu of B♭ (B-flat). Occasionally, music written in German for international use will use H for B-natural and Bb for B-flat (with a modern-script lowercase b instead of a flat sign). Since a Bes or B♭ in Northern Europe (i.e. a B elsewhere) is both rare and unorthodox (more likely to be expressed as Heses), it is generally clear what this notation means.
In Italian, Portuguese, Spanish, French, Romanian, Greek, Russian, Mongolian, Flemish, Persian, Arabic, Hebrew, Bulgarian and Turkish notation the notes of scales are given in terms of Do-Re-Mi-Fa-Sol-La-Si rather than C-D-E-F-G-A-B. These names follow the original names reputedly given by Guido d'Arezzo, who had taken them from the first syllables of the first six musical phrases of a Gregorian Chant melody "Ut queant laxis", which began on the appropriate scale degrees. These became the basis of the solfege system. "Do" later replaced the original "Ut" for ease of singing (most likely from the beginning of "Dominus", Lord), though "Ut" is still used in some places. "Si" or "Ti" was added as the seventh degree (from "Sancte Johannes", St. John, to whom the hymn is dedicated). The use of 'Si' versus 'Ti' varies regionally.
The two notation systems most commonly used nowadays are the Helmholtz pitch notation system and the Scientific pitch notation system. As shown in the table above, they both include several octaves, each starting from C rather than A. The reason is that the most commonly used scale in Western music is the major scale, and the sequence C-D-E-F-G-A-B (the C-major scale) is the simplest example of a major scale. Indeed, it is the only major scale which can be obtained using natural notes (the white keys on the piano keyboard), and typically the first musical scale taught in music schools.
In a newly developed system, primarily in use in the United States, notes of scales become independent to the music notation. In this system the natural symbols C-D-E-F-G-A-B refer to the absolute notes, while the names Do-Re-Mi-Fa-So-La-Ti are relativized and show only the relationship between pitches, where Do is the name of the base pitch of the scale, Re is the name of the second pitch, etc. The idea of so-called movable-do, originally suggested by John Curwen in the 19th century, was fully developed and involved into a whole educational system by Zoltán Kodály in the middle of the 20th century, which system is known as the Kodály Method or Kodály Concept.

</doc>
<doc id="22028" url="http://en.wikipedia.org/wiki?curid=22028" title="Nephrology">
Nephrology

Nephrology (from Greek νεφρός "nephros" "kidney", combined with the suffix "-logy", "the study of") is a specialty of medicine and pediatrics that concerns itself with the study of normal kidney function, kidney problems, the treatment of kidney problems and renal replacement therapy (dialysis and kidney transplantation). Systemic conditions that affect the kidneys (such as diabetes and autoimmune disease) and systemic problems that occur as a result of kidney problems (such as renal osteodystrophy and hypertension) are also studied in nephrology. A physician who has undertaken additional training to become an expert in nephrology may call themselves a nephrologist or renal physician.
Scope.
Nephrology concerns the diagnosis and treatment of kidney diseases, including electrolyte disturbances and hypertension, and the care of those requiring renal replacement therapy, including dialysis and renal transplant patients. Many diseases affecting the kidney are systemic disorders not limited to the organ itself, and may require special treatment. Examples include acquired conditions such as systemic vasculitides (e.g. ANCA vasculitis) and autoimmune diseases (e.g., lupus), as well as congenital or genetic conditions such as polycystic kidney disease.
Patients are referred to nephrology specialists for various reasons, such as acute kidney failure, chronic kidney disease, hematuria, proteinuria, kidney stones, hypertension, and disorders of acid/base or electrolytes.
Nephrologist.
A nephrologist is a medical doctor (MD) who specializes in care and treatment of kidney disease. Nephrology requires additional training to become an expert with advanced skills. Nephrologists may provide care to people without kidney problems and may work in general/internal medicine, transplant medicine, immunosuppression management, intensive care medicine, clinical pharmacology, perioperative medicine, or pediatric nephrology.
Nephrologists may further sub-specialise in dialysis, kidney transplantation, chronic kidney disease, cancer-related kidney diseases (Onconephrology), procedural nephrology or other non-nephrology areas as described above.
Procedures a nephrologist may perform include native kidney and transplant kidney biopsy, dialysis access insertion (temporary vascular access lines, tunnelled vascular access lines, peritoneal dialysis access lines), fistula management (angiographic or surgical fistulogram and plasty), and .
Training.
Australia and New Zealand.
Nephrology training in Australia and New Zealand typically includes completion of a medical degree (Bachelor of Medicine, Bachelor of Surgery: 4–6 years), internship (1 year), Basic Physician Training (3 years minimum), successful completion of the Royal Australasian College of Physicians written and clinical examinations, and Advanced Physician Training in Nephrology (2–3 years). The training pathway is overseen and accredited by the Royal Australasian College of Physicians. Increasingly, nephrologists may additionally complete of a post-graduate degree (usually a PhD) in a nephrology research interest (3–4 years). Finally, all Australian and New Zealand nephrologists participate in career-long professional and personal development through the Royal Australasian College of Physicians and other bodies such as the and the .
United Kingdom.
In the United Kingdom, nephrology (or often renal medicine) is a subspecialty of general medicine. A nephrologist has completed medical school, foundation year posts (FY1 and FY2) and core medical training (CMT) and passed the Membership of the Royal College of Physicians (MRCP) exam before competing for a National Training Number (NTN) in renal medicine. The typical subspecialty training is five years and leads to a Certificate of Completion of Training (CCT) in both renal medicine and general (internal) medicine. Subspecialty trainees often interrupt their clinical training to obtain research degrees (MD/PhD).
United States.
Nephrology training can be accomplished through one of two routes. The first being through an internal medicine pathway leading to an Internal Medicine/Nephrology specialty, and sometimes known as "adult nephrology". The second pathway being through Pediatrics leading to a speciality in Pediatric Nephrology. In the United States, after medical school adult nephrologists complete a three-year residency in internal medicine followed by a two-year (or longer) fellowship in nephrology. Complementary to an adult nephrologist, a pediatric nephrologist will complete a three-year pediatric residency after medical school or a four-year Combined Internal Medicine and Pediatrics residency. This is followed by a three-year fellowship in Pediatic Nephrology. Once training is satisfactorily completed, the physician is eligible to take the American Board of Internal Medicine (ABIM) or American Osteopathic Board of Internal Medicine (AOBIM) nephrology examination. Nephrologists must be approved by one of these boards. To be approved, the physician must fulfill the requirements for education and training in nephrology in order to qualify to take the board's examination. If a physician passes the examination, then he or she can become a nephrology specialist. Typically, nephrologists also need two to three years of training in an ACGME or AOA accredited fellowship in nephrology. Nearly all programs train nephrologists in continuous renal replacement therapy; fewer than half in the United States train in the provision of plasmapheresis. Only pediatric trained physicians are able to train in pediatric nephrology, and internal medicine (adult) trained physicians may enter general (adult) nephrology fellowships.
Diagnosis.
History and physical examination are central to the diagnostic workup in nephrology. This may include inquires regarding family history, general medical history, diet, medication use, drug use and occupation. Examination typically includes an assessment of volume state, blood pressure, skin, joints, abdomen and flank.
Examination of the urine (urinalysis) allows a direct assessment for possible kidney problems, which may be suggested by appearance of blood in the urine (haematuria), protein in the urine (proteinuria), pus cells in the urine (pyuria) or cancer cells in the urine. A 24-hour urine collection can be used to quantify daily protein loss (see proteinuria), urine output, creatinine clearance or electrolyte handling by the renal tubules.
Basic blood tests can be used to check the concentration of hemoglobin, platelets, sodium, potassium, chloride, bicarbonate, urea, creatinine, calcium, magnesium or phosphate in the blood. All of these may be affected by kidney problems. The serum creatinine concentration can be used to estimate the function of the kidney, called the creatinine clearance or estimated glomerular filtration rate (GFR). More specialized tests can be ordered to discover or link certain systemic diseases to kidney failure such as infections (hepatitis B, hepatitis C), autoimmune conditions (systemic lupus erythematosus, ANCA vasculitis), paraproteinemias (amyloidosis, multiple myeloma) and metabolic diseases (diabetes, cystinosis).
Structural abnormalities of the kidneys are identified with imaging tests. These may include Medical ultrasonography/ultrasound, computed axial tomography (CT), scintigraphy (nuclear medicine), angiography or magnetic resonance imaging (MRI).
In certain circumstances, less invasive testing may not provide a certain diagnosis. Where definitive diagnosis is required, a biopsy of the kidney (renal biopsy) may be performed. This typically involves the insertion, under local anaesthetic and ultrasound or CT guidance, of a core biopsy needle into the kidney to obtain a small sample of kidney tissue. The kidney tissue is then examined under a microscope, allowing direct visualization of the changes occurring within the kidney. Additionally, the pathology may also stage a problem affecting the kidney, allowing some degree of prognostication. In some circumstances, kidney biopsy will also be used to monitor response to treatment and identify early relapse.
Treatment.
Treatments in nephrology can include medications, blood products, surgical interventions (urology, vascular or surgical procedures), renal replacement therapy (dialysis or kidney transplantation) and plasma exchange. Kidney problems can have significant impact on quality and length of life, and so psychological support, health education and advanced care planning play key roles in nephrology.
Chronic kidney disease is typically managed with treatment of causative conditions (such as diabetes), avoidance of substances toxic to the kidneys (nephrotoxins like radiologic contrast and non-steroidal anti-inflammatory drugs), antihypertensives, diet and weight modification and planning for end-stage kidney failure. Impaired kidney function has systemic effects on the body. An erythropoetin stimulating agent may be required to ensure adequate production of red blood cells, activated vitamin D supplements and phosphate binders may be required to counteract the effects of kidney failure on bone metabolism, and blood volume and electrolyte disturbance may need correction.
Auto-immune and inflammatory kidney disease, such as vasculitis or transplant rejection, may be treated with immunosuppression. Commonly used agents are prednisone, mycophenolate, cyclophosphamide, cyclosporine, tacrolimus, everolimus, thymoglobulin and sirolimus. Newer, so-called "biologic drugs" or monoclonal antibodies, are also used in these conditions and include rituximab, basiliximab and eculizumab. Blood products including intravenous immunoglobulin and a process known as plasma exchange can also be employed.
When the kidneys are no longer able to sustain the demands of the body, end-stage kidney failure is said to have occurred. Without renal replacement therapy, death from kidney failure will eventually result. Dialysis is an artificial method of replacing some kidney function to prolong life. Renal transplantation replaces kidney function by inserting into the body a healthier kidney from an organ donor and inducing immunologic tolerance of that organ with immunosuppression. At present, renal transplantation is the most effective treatment for end-stage kidney failure although its world-wide availability is limited by lack of availability of donor organs.
Most kidney conditions are chronic conditions and so long term followup with a nephrologist is usually necessary.
Organizations.
The world's first society of nephrology was the French 'Societe de Pathologie Renale'. Its first president was Jean Hamburger, and its first meeting was in Paris in February 1949. In 1959, Hamburger also founded the 'Société de Néphrologie', as a continuation of the older society. The UK's Renal Association was founded in 1950; the second society of nephrologists. Its first president was Arthur Osman. Its first meeting was on 30th March 1950 in London. The Società di Nefrologia Italiana was founded in 1957 and was the first national society to incorporate the phrase nephrologia (or nephrology) into its name.
The word 'nephrology' appeared for the first time in a conference, on 1st-4th September 1960 at the "Premier Congrès International de Néphrologie" in Evian and Geneva, the first meeting of the International Society of Nephrology (ISN, International Society of Nephrology). The first day (1.9.60) was in Geneva and the next three (2-4.9.60) were in Evian, France. The early history of the ISN is described by Robinson and Richet in 2005 and the later history by Barsoum in 2011. The ISN is the largest global society representing medical professionals engaged in advancing kidney care worldwide.
In the USA, founded in 1964, the National Kidney Foundation is a national organization representing patients and professionals who treat kidney diseases. Founded in 1966, the American Society of Nephrology (ASN) is the world’s largest professional society devoted to the study of kidney disease. The American Nephrology Nurses' Association (ANNA), founded in 1969, promotes excellence in and appreciation of nephrology nursing to make a positive difference for patients with kidney disease. The American Association of Kidney Patients (AAKP) is a non-profit, patient-centric group focused on improving the health and well-being of CKD and dialysis patients. The American Kidney Fund directly provides financial support to patients in need, as well as participating in health education and prevention efforts. ASDIN (American Society of Diagnostic and Interventional Nephrology) is the main organization of interventional nephrologists. Other organizations include CIDA, VASA etc. which deal with dialysis vascular access. The Renal Support Network (RSN) is a nonprofit, patient-focused, patient-run organization that provides non-medical services to those affected by chronic kidney disease (CKD).
In the United Kingdom, the UK National Kidney Federation and British Kidney Patient Association (BKPA) represents patients, and the Renal Association represents renal physicians and works closely with the National Service Framework for kidney disease.

</doc>
<doc id="22031" url="http://en.wikipedia.org/wiki?curid=22031" title="Native Esperanto speakers">
Native Esperanto speakers

Native Esperanto speakers (Esperanto: "denaskuloj" or "denaskaj esperantistoj") are people who have acquired Esperanto as one of their native languages. As of 1996, there were 350 or so attested cases of families with native Esperanto speakers. Estimates from associations indicate that there are currently around 1,000 Esperanto-speaking families, involving perhaps 2,000 children. In all known cases, speakers are natively bilingual, raised in both Esperanto and either the local national language or the native language of their parents. In all but a handful of cases, it was the father who used Esperanto with the child. In the majority of such families, the parents had the same native language, though in many the parents had different native languages, and only Esperanto in common.
Raising children in Esperanto occurred early in the history of the language, notably with the five children of Montagu Butler (1884–1970). Because of this, some families have passed Esperanto on to their children for several generations. A famous native speaker of Esperanto is businessman George Soros, son of Tivadar Soros, publisher and writer in Esperanto. Also notable are young Holocaust victim Petr Ginz, whose drawing of the planet Earth as viewed from the moon was carried aboard the Space Shuttle Columbia, and Daniel Bovet, the recipient of the 1957 Nobel Prize in Physiology or Medicine.
Esperanto is not the primary language of any geographic region, outside of temporary gatherings (such as conventions like the World Congress of Esperanto) and isolated offices (such as the World Esperanto Association's central office in Rotterdam). Consequently, native speakers have limited opportunity to meet one another except where meetings are specially arranged. For this reason, many parents consider it important to bring their children regularly to Esperanto conventions such as the annual "Renkontiĝo de Esperanto-familioj" (or "Esperantistaj familioj"; REF, since 1979). Similarly, the annual Children's Congress of Esperanto happens alongside the largest Esperanto convention, the World Congress of Esperanto ("Universala Kongreso").
List of noted native speakers.
Below is a list of noted native Esperanto speakers.
Grammatical characteristics.
The Esperanto of native-speaking children differs from the standard Esperanto spoken by their parents. In some cases this is due to interference from their other native language (the adstrate), but in others it appears to be an effect of acquisition.
Bergen (2001) found the following patterns in a study of eight native-speaking children, aged 6 to 14, who were bilingual in Hebrew (two siblings), Slovak (two siblings), French, Swiss German, Russian, and Croatian.
Among children that do use the accusative, its usage may be regularized from adult usage, at least at young ages. For example, when a screw dropped out of a lock, a young (≤ 5-year-old) child said it "malvenis la pordon." Besides the novel use of "mal-" with "veni" 'to come' to mean 'come away from', the accusative is not used in adult speech for motion away, but only motion towards. However, in this case the child generalized the usage of the accusative for direct objects.
Lindstedt, on the other hand, referencing Bergen's study, contends that "it is difficult to find convincing examples of changes introduced by the process of nativisation. All examples proposed seem rather to be due to (1) transfers from the children’s other native languages, (2) differences between the spoken and written register of Esperanto and, in some cases, (3) incomplete acquisition." Some of the features, such as phonological reduction, can be found in the speech of some fluent non-native speakers, while some other, such as the attrition of the accusative, are completely absent from the speech of some native-speaking children.
Word derivation.
Native-speaking children, especially at a young age, may coin words that do not exist in the speech of their parents, often for concepts for which Esperanto has a word they do not yet know, by exploiting the morphology of the language. This is analogous to what adult speakers do for concepts where Esperanto lacks a word, and indicates that some of the grammatical alterations that adult learners may find difficult come easily to native-speaking children. For example,

</doc>
<doc id="22033" url="http://en.wikipedia.org/wiki?curid=22033" title="Nicaragua v. United States">
Nicaragua v. United States

The Republic of Nicaragua v. The United States of America (1986) is a public international law case decided by the International Court of Justice (ICJ). The ICJ ruled in favor of Nicaragua and against the United States and awarded reparations to Nicaragua. The ICJ held that the U.S. had violated international law by supporting the Contras in their rebellion against the Nicaraguan government and by mining Nicaragua's harbors. The United States refused to participate in the proceedings after the Court rejected its argument that the ICJ lacked jurisdiction to hear the case. The U.S. later blocked enforcement of the judgment by the United Nations Security Council and thereby prevented Nicaragua from obtaining any actual compensation. The Nicaraguan government finally withdrew the complaint from the court in September 1992 (under the later, post-FSLN, government of Violeta Chamorro), following a repeal of the law requiring the country to seek compensation.
The Court found in its verdict that the United States was "in breach of its obligations under customary international law not to use force against another State", "not to intervene in its affairs", "not to violate its sovereignty", "not to interrupt peaceful maritime commerce", and "in breach of its obligations under Article XIX of the Treaty of Friendship, Commerce and Navigation between the Parties signed at Managua on 21 January 1956."
The Court had 16 final decisions upon which it voted. In Statement 9, the Court stated that while the U.S. encouraged human rights violations by the Contras by the manual entitled "Psychological Operations in Guerrilla Warfare", this did not, however, make such acts attributable to the U.S.
Background and History of US Intervention in Nicaragua.
The first armed intervention by the United States in Nicaragua occurred under President Taft. In 1909, he ordered the overthrow of Nicaraguan President José Santos Zelaya. During August and September 1912, a contingent of 2300 U.S. Marines landed at the port of Corinto and occupied León and the railway line to Granada. A pro-U.S. government was formed under the occupation. The 1914 Bryan-Chamorro Treaty granted perpetual canal rights to the U.S. in Nicaragua and was signed ten days before the U.S.-operated Panama Canal opened for use, thus preventing anyone from building a competing canal in Nicaragua without U.S. permission.
In 1927, under Augusto César Sandino, a major peasant uprising was launched against both the U.S. occupation and the Nicaraguan establishment. In 1933, the Marines withdrew and left the National Guard in charge of internal security and elections. In 1934, Anastasio Somoza García, the head of the National Guard, ordered his forces to capture and murder Sandino. In 1937, Somoza assumed the presidency, while still in control of the National Guard, and established a dictatorship that his family controlled until 1979.
The downfall of the regime is attributed to its embezzlement of millions of dollars in foreign aid that was given to the country in response to the devastating 1972 earthquake. Many moderate supporters of the dictatorship began abandoning it in the face of growing revolutionary sentiment. The Sandinista (FLSN) movement organized relief, began to expand its influence and assumed the leadership of the revolution. A popular uprising brought the FSLN to power in 1979. The United States had long been opposed to the socialist FSLN, and after the revolution the Carter administration moved quickly to support the Somocistas with financial and material aid. When Ronald Reagan took office, he augmented the direct support to an anti-Sandinista group, called the Contras, which included factions loyal to the former dictatorship. When Congress prohibited further funding to the Contras, Reagan continued the funding through arms sales that were also prohibited by Congress.
Nicaragua's submissions.
Nicaragua charged:
Nicaragua demanded that all such actions cease and that the United States had an obligation to pay reparations to the government for damage to their people, property, and economy.
It is noteworthy that the United States, the defaulting party, was the only member that put forward arguments against the validity of the judgment of the court, arguing that it passed a decision that it "had neither the jurisdiction nor the competence to render." Members that sided with the United States in opposing Nicaragua's claims did not challenge the court's jurisdiction, its findings, nor the substantive merits of the case.
Pursuant to general and customary international law, the United States has an obligation to Nicaragua to respect the sovereignty of Nicaragua.
ii. Pursuant to general and customary international law, the United States has an obligation to Nicaragua not to use force or the threat of force against Nicaragua. 
iii. Pursuant to general and customary international law, the United States has an obligation to Nicaragua not to intervene in the internal affairs of Nicaragua.
Judgment.
The very long judgment first listed 291 points, among them that the United States had been involved in the "unlawful use of force." The alleged violations included attacks on Nicaraguan facilities and naval vessels, the mining of Nicaraguan ports, the invasion of Nicaraguan air space, and the training, arming, equipping, financing and supplying of forces (the "Contras") and seeking to overthrow Nicaragua's Sandinista government. This was followed by the statements that the judges voted on.
Findings.
The court found evidence of an arms flow between Nicaragua and insurgents in El Salvador between 1979-81. However, there was not enough evidence to show that the Nicaraguan government was imputable for this or that the US response was proportional. The court also found that certain transborder incursions into the territory of Guatemala and Costa Rica, in 1982, 1983 and 1984, were imputable to the Government of Nicaragua. However, neither Guatemala nor Costa Rica had made any request for US intervention; El Salvador did in 1984, well after the US had intervened unilaterally.
"As regards El Salvador, the Court considers that in customary international law the provision of arms to the opposition in another State does not constitute an armed attack on that State. As regards Honduras and Costa Rica, the Court states that, in the absence of sufficient information as to the transborder incursions into the territory of those two States from Nicaragua, it is difficult to decide whether they amount, singly or collectively, to an armed attack by Nicaragua. The Court finds that neither these incursions nor the alleged supply of arms may be relied on as justifying the exercise of the right of collective self-defence."
Regarding human rights violations by the Contras, "The Court has to determine whether the relationship of the contras to the United States Government was such that it would be right to equate the Contras, for legal purposes, with an organ of the United States Government, or as acting on behalf of that Government. The Court considers that the evidence available to it is insufficient to demonstrate the total dependence of the contras on United States aid. A partial dependency, the exact extent of which the Court cannot establish, may be inferred from the fact that the leaders were selected by the United States, and from other factors such as the organisation, training and equipping of the force, planning of operations, the choosing of targets and the operational support provided. There is no clear evidence that the United States actually exercised such a degree of control as to justify treating the contras as acting on its behalf... Having reached the above conclusion, the Court takes the view that the Contras remain responsible for their acts, in particular the alleged violations by them of humanitarian law. For the United States to be legally responsible, it would have to be proved that that State had effective control of the operations in the course of which the alleged violations were committed."
The Court concluded that the United States, despite its objections, was subject to the Court's jurisdiction. The Court had ruled on 26 November by 11 votes to one that it had jurisdiction in the case on the basis of either Article 36 (i.e. compulsory jurisdiction) or the 1956 Treaty of Friendship, Commerce and Navigation between the United States and Nicaragua. The Charter provides that, in case of doubt, it is for the Court itself to decide whether it has jurisdiction, and that each member of the United Nations undertakes to comply with the decision of the Court. The Court also ruled by unanimity that the present case was admissible. The United States then announced that it had "decided not to participate in further proceedings in this case." About a year after the Court's jurisdictional decision, the United States took the further, radical step of withdrawing its consent to the Court's compulsory jurisdiction, ending its previous 40 year legal commitment to binding international adjudication. The Declaration of acceptance of the general compulsory jurisdiction of the International Court of Justice terminated after a 6-month notice of termination delivered by the Secretary of State to the United Nations on October 7, 1985.
Although the Court called on the United States to "cease and to refrain" from the unlawful use of force against Nicaragua and stated that the US was "in breach of its obligation under customary international law not to use force against another state" and ordered it to pay reparations, the United States refused to comply. As a permanent member of the Security Council, the U.S. has been able to block any enforcement mechanism attempted by Nicaragua. On November 3, 1986 the United Nations General Assembly passed, by a vote of 94-3 (El Salvador, Israel and the US voted against), a non-binding urging the US to comply.
The ruling.
On June 27, 1986, the Court made the following ruling:
The Court
Legal clarification and importance.
The ruling did in many ways clarify issues surrounding prohibition of the use of force and the right of self-defence. Arming and training the Contra was found to be in breach with principles of non-intervention and prohibition of use of force, as was laying mines in Nicaraguan territorial waters.
Nicaragua's dealings with the armed opposition in El Salvador, although it might be considered a breach with the principle of non-intervention and the prohibition of use of force, did not constitute "an armed attack", which is the wording in article 51 justifying the right of self-defence.
The Court considered also the United States claim to be acting in collective self-defence of El Salvador and found the conditions for this not reached as El Salvador never requested the assistance of the United States on the grounds of self-defence.
In regards to laying mines, "...the laying of mines in the waters of another State without any warning or notification is not only an unlawful act but also a breach of the principles of humanitarian law underlying the Hague Convention No. VIII of 1907."
How the judges voted.
Votes of Judges - Nicaragua v. United States
Dissent.
Judge Schwebel’s dissent was twice as long as the actual judgment. Judge Schwebel argued that the Sandinista government came to power with support of foreign intervention similar to what it was now complaining about. He argued that the Sandinista government achieved international recognition and received large amounts of foreign aid in exchange for commitments they subsequently violated. He cited evidence that the Sandinista government had indeed supported the rebels in El Salvador and noted that Nicaragua’s own CIA witness contradicted their assertions that they had never at any point supported the rebels in El Salvador. The CIA witness said that there was no evidence of weapon shipments since early 1981, but Schwebel argued that he could not credibly explained why opponents of Contra aid such as congressman Boland, who also saw the evidence, believed that weapon shipments were ongoing. He further argued that Daniel Ortega publicly admitted such shipments in statements in 1985 and 1986. Furthermore, there was no dispute that the leadership of the rebels operated in Nicaragua from time to time.
He stated that in August 1981 the U.S. offered to resume aid to Nicaragua and to not support regime change in exchange for Nicaraguan commitments to not support the rebels in El Salvador. These proposals were rejected by the Sandinistas, and judge Schwebel argued that the U.S. was entitled to take action in collective self-defense with El Salvador by authorizing contra aid in December 1981. He stated that further U.S. proposals to resolve the issue made in early 1982 were also ignored by the Sandinistas. The Sandinista government in 1983 began advancing proposals in which it would undertake not to support the rebels, but Schwebel noted that these were coupled with demands that the U.S. cease supporting the lawful government of El Salvador. The judge noted that since early 1985 the U.S. had increasingly made regime change a primary objective but argued this was not inconsistent with self-defense because it was reasonable to believe that Nicaragua would not maintain any commitments unless Sandinista power was diluted.
The judge said that both sides of the wars in Nicaragua and El Salvador had committed atrocities. He said the U.S. mining of Nicaraguan harbors was unlawful in regard to third parties, but not Nicaragua.
Certain witnesses against the US.
First witness: Commander Luis Carrion.
The first witness called by Nicaragua was Nicaragua's first Vice Minister of the Interior, Commander Luis Carrion. Commander Carrion had overall responsibility for state security and was in charge of all government operations in the "principal war zone". He was responsible for monitoring United States involvement in military and paramilitary activities against Nicaragua, directing Nicaragua's military and intelligence efforts against the contra guerrillas.
Commander Carrion began by explaining the condition of the contras prior to United States' aid in December 1981. Commander Carrion stated that the contras consisted of insignificant bands of poorly armed and poorly organized members of Somoza's National Guard, who carried out uncoordinated border raids and rustled cattle (presumably for food).
In December 1981, the U.S. Congress authorized an initial appropriation of 19 million dollars to finance paramilitary operations in Nicaragua and elsewhere in Central America. Because of this aid, Commander Carrion stated that the contras began to become centralized and received both training and weapons from the CIA. During 1982 the contra guerrillas engaged the Sandinista armed forces in a series of hit and run border raids and carried out a number of sabotage operations including:
The United States Central Intelligence Agency, and Argentine military officers financed by the CIA, were engaged in the training of the contra forces. The guerrillas received both basic infantry training as well as training in specialized sabotage and demolition for "special operation groups".
The U.S. Congress apportioned new funds for the contras to the amount of $30 million at the end of 1982. This made it possible for the contra forces to launch a military offensive against Nicaragua. According to Commander Carrion, the offensive known as "C Plan" had the objective of capturing the Nicaraguan border town of Jalapa in order to install a provisional government, which could receive international recognition. This plan failed.
After the failure of the Jalapa offensive the contras changed their tactics from frontal assaults to economic warfare against State farms, coffee plantations, grain storage centers, road junctions, etc.
The CIA began to support the contras by setting up and coordinating a communications and logistical system. The CIA supplied aircraft and the construction of airfields in the Honduran border area next to Nicaragua. This allowed the contras to carry out deep penetration raids into the more developed and populated areas of the Nicaraguan interior. U.S. Army engineers created this airfield. The purpose of these deep penetration attacks upon economic targets was to weaken the Nicaraguan economy, causing a shortages of goods.
As a part of its training program for the contras, the CIA prepared and distributed a manual entitled Psychological Operations in Guerrilla Warfare. This manual included instructions in the "use of implicit and explicit terror", and in the "selective use of violence for propaganda effects". Commander Carrion explained that the manual was given to the Contras, "All of these terrorist instructions have the main purpose of alienating the population from the Government through creating a climate of terror and fear, so that nobody would dare support the Government". The manual calls for the "neutralization" (i.e. assassination) of Sandinista local government officials, judges, etc. for purposes of intimidation. It was openly admitted by the President Reagan in a press conference that the manual had been prepared by a CIA contract employee.
After the United States Congress approved an additional $24 million aid to the contras in December 1983, a new offensive was launched, named Plan Sierra. This offensive involved approximately 7000 members of the contra forces. As in earlier attacks, the initial objective of this offensive was to capture the border town of Jalapa to install a provisional government, which the CIA informed the contras would be immediately recognized by the United States Government. But this contra offensive was also repulsed by the Nicaraguan government forces.
In the beginning of 1984, the contras made a major effort to prevent the harvesting of the coffee crop, which is one of Nicaragua's most important export products. Coffee plantations and state farms where coffee is grown were attacked, vehicles were destroyed, and coffee farmers were killed.
Commander Carrion testified that the ability of the contras to carry out military operations was completely dependent upon United States funding, training and logistical support. Carrion stated that the U.S. Government supplied the contras with uniforms, weapons, communications equipment, intelligence, training, and coordination in using this material aid.
In September 1983, CIA operatives blew up Nicaragua's only oil pipeline, which was used to transport oil from off-loading facilities to storage tanks on shore. The United States was also directly involved in a large scale sabotage operation directed against Nicaragua's oil storage facilities. This last attack was carried out by CIA contract employees termed by that organization as "Unilaterally Controlled Latin Assets" (UCLAs). The CIA personnel were also directly involved in a helicopter attack on a Nicaraguan army training camp. One of the helicopters was shot down by Nicaraguan ground fire resulting in the death of two U.S. citizens.
Commander Carrion testified that the United States was involved in the mining of Nicaragua's ports between February - April 1984. The mining operation was carried out by CIA ships directing the operation from international waters, while the actual mining was carried out by CIA employees on board speedboats operating inshore. After the mine-laying was completed the speedboats returned to the mother vessel.
Carrion stated that 3,886 people had been killed and 4,731 wounded in the four years since the contras began their attacks. Carrion estimated property damage at $375 million.
Commander Carrion stated if the United States stopped aid, support and training, this would result in the end of the contras military activities within three months. Asked why he was so sure of this, Commander Carrion answered, "Well, because the contras are an artificial force, artificially set up by the United States, that exists only because it counts on United States direction, on United States training, on United States assistance, on United States weapons, on United States everything...Without that kind of support and direction the contras would simply disband, disorganize, and thus lose their military capacity in a very short time".
Second witness: Dr. David MacMichael.
David MacMichael was an expert on counter-insurgency, guerrilla warfare, and Latin American affairs, he was also a witness because he was closely involved with U.S. intelligence activities as a contract employee from March 1981 - April 1983. MacMichael worked for Stanford Research Institute, which was contracted by the U.S. Department of Defense. After this he worked two years for the CIA as a "senior estimates officer", preparing the National Intelligence Estimate. Dr. MacMichael's responsibility was centered upon Central America. He had top-secret clearance. He was qualified and authorized to have access to all relevant U.S. intelligence concerning Central America, including intelligence relating to alleged Nicaraguan support for, and arms shipments to the anti-Government insurgents in El Salvador. He took part in high level meetings of the Latin American affairs office of the CIA. Including a fall 1981 meeting, which submitted the initial plan to set up a 1500-man covert force on the Nicaraguan border, shipping arms from Nicaragua to the El Salvador insurgents. This plan was approved by President Reagan.
"The overall purpose (for the creation of the contras) was to weaken, even destabilize the Nicaraguan Government and thus reduce the menace it allegedly posed to the United States' interests in Central America..."
Contra paramilitary actions would "hopefully provoke cross-border attacks by Nicaraguan forces and thus serve to demonstrate Nicaragua's aggressive nature and possibly call into play the Organization of American States' provisions (regarding collective self-defense). It was hoped that the Nicaraguan Government would clamp down on civil liberties within Nicaragua itself, arresting its opposition, so demonstrating its allegedly inherent totalitarian nature and thus increase domestic dissent within the country, and further that there would be reaction against United States citizens, particularly against United States diplomatic personnel within Nicaragua and thus to demonstrate the hostility of Nicaragua towards the United States".
 In response to repeated questions as to whether there was any substantial evidence of the supply of weapons to the guerrilla movement in El Salvador- either directly by the Nicaraguan Government itself-or with the knowledge, approval or authorization of the Nicaraguan Government of either non-official Nicaraguan sources, or by third country nationals inside or outside Nicaragua, using Nicaraguan territory for this purpose, Dr. MacMichael answered that there was no such evidence. In the opinion of the witness it would not have been possible for Nicaragua to send arms to the insurgents in El Salvador in significant amounts (as alleged by the U.S. Government) and over a prolonged period, without this being detected by the U.S. intelligence network in the area...Counsel for Nicaragua, asked the witness several times whether any detection of arms shipments by or through Nicaragua had taken place during the period he was employed by the CIA. (MacMichael) answered repeatedly that there was no such evidence. He also stated that after his employment had terminated, nothing had occurred that would cause him to change his opinion. He termed the evidence that had been publicly disclosed by the U.S. Government concerning Nicaraguan arms deliveries to the El Salvadoran insurgents as both "scanty" and "unreliable". The witness did however state that based on evidence, which had been gathered immediately prior to his employment with the CIA, evidence he had already actually seen, there was substantial evidence that arms shipments were reaching El Salvador from Nicaragua - with the probable involvement and complicity of the Nicaraguan Government - through late 1980 up until the spring of 1981...But this evidence, which most importantly had included actual seizures of weapons, which could be traced to Nicaragua, as well as documentary evidence and other sources, had completely ceased by early 1981. Since then, no evidence linking Nicaragua to shipments of arms in any substantial quantities had resumed coming in. 
Third witness: Professor Michael Glennon.
Mr. Glennon testified about a fact-finding mission he had conducted in Nicaragua to investigate alleged human rights violations committed by the contra guerrillas, sponsored by the International Human Rights Law Group, and the Washington Office on Latin America. Glennon conducted the investigation with Mr. Donald T. Fox who is a New York attorney and a member of the International Commission of Jurists.
They traveled to Nicaragua, visiting the northern region where the majority of contra military operations took place. The two lawyers interviewed around 36 northern frontier residents who had direct experience with the contras. They also spoke with the U.S. Ambassador to Nicaragua, and with senior officials of the U.S. Department of State in Washington after returning to the United States.
No hearsay evidence was accepted. Professor Glennon stated that those interviewed were closely questioned and their evidence was carefully cross-checked with available documentary evidence. Doubtful "testimonies" were rejected, and the results were published in April 1985.
The conclusions of the report were summarized by Glennon in Court:
"We found that there is substantial credible evidence that the contras were engaged with some frequency in acts of terroristic violence directed at Nicaraguan civilians. These are individuals who have no connection with the war effort-persons with no economic, political or military significance. These are Individuals who are not caught in the cross-fire between Government and contra forces, but rather individuals who are deliberately targeted by the contras for acts of terror. "Terror" was used in the same sense as in recently enacted United States law, i.e. "an activity that involves a violent act or an act dangerous to human life that Is a violation or the criminal law, and appears to be intended to intimidate or coerce a civilian population, to Influence the policy of a government by intimidation or coercion, or to affect the conduct of a government by assassination or kidnapping".
In talks with U.S. State Department officials, at those in Managua U.S. Embassy, and with officials in Washington, Professor Glennon had inquired whether the U.S. Government had ever investigated human rights abuses by the contras. Professor Glennon testified that no such investigation had ever been conducted, because in the words of a ranking State Department official who he could not name, the U.S. Government maintained a policy of "intentional ignorance" on the matter. State Department officials in Washington- had admitted to Glennon that "it was clear that the level of atrocities was enormous". Those words "enormous" and "atrocities" were the ranking State Department official's words.
Fourth witness: Father Jean Loison.
Father Jean Loison was a French priest who worked as a nurse in a hospital in the northern frontier region close to Honduras.
Asked whether the contras engaged in acts of violence directed against the civilian population, Father Loison answered:
"Yes, I could give you several examples. Near Quilali, at about 30 kilometers east of Quilali, there was a little village called El Coco. The contras arrived, they devastated it, they destroyed and burned everything. They arrived in front of a little house and turned their machinegun fire on it, without bothering to check if there were any people inside. Two children, who had taken fright and hidden under a bed, were hit. I could say the same thing of a man and woman who were hit, this was in the little co-operative of Sacadias Olivas. It was just the same. They too had taken fright and got into bed. Unlike El Coco, the contras had just been on the attack, they had encountered resistance and were now in flight. During their flight they went into a house, and seeing that there were people there, they threw grenade. The man and the woman were killed and one of the children was injured."
About contra kidnappings: 
"I would say that kidnappings are one of the reasons why some of the peasants have formed themselves into groups. Here (indicates a point on the map) is Quilali. Between Quilali and Uilili, in this region to the north, there are hardly any peasants left of any age to bear arms, because they have all been carried off"."
Father Loison described many examples of violence, mostly indiscriminate, directed at the civilian population in the region where he resides. The picture that emerges from his testimony is that the contras engage in brutal violation of minimum standards of humanity. He described murders of unarmed civilians, including women and children, rape followed in many instances by torture or murder, and indiscriminate terror designed to coerce the civilian population. His testimony was similar to various reports including the International Human Rights Law Group, Amnesty International, and others.
Fifth witness: William Hüper.
William Hüper was Nicaragua's Minister of Finance. He testified about Nicaragua economic damage, including the loss of fuel as a result of the attack in the oil storage facilities at Corinto, the damage to Nicaragua's commerce as a result of the mining of its ports, and other economic damage.
UN voting.
After five vetoes in the Security Council between 1982 and 1985 of resolutions concerning the situation in Nicaragua , the United States made one final veto on 28 October 1986 (France, Thailand, and United Kingdom abstaining) of a resolution calling for full and immediate compliance with the judgment.
Nicaragua brought the matter to the U.N. Security Council, where the United States vetoed a resolution (11 to 1, 3 abstentions) calling on all states to observe international law. Nicaragua also turned to the General Assembly, which passed a resolution 94 to 3 calling for compliance with the World Court ruling. Two states, Israel and El Salvador, joined the United States in opposition. At that time, El Salvador was receiving substantial funding and military advisement from the U.S., which was aiming to crush a Sandinista-like revolutionary movement by the FMLN. At the same session, Nicaragua called upon the U.N. to send an independent fact-finding mission to the border to secure international monitoring of the borders after a conflict there; the proposal was rejected by Honduras with U.S. backing. A year later, on November 12, 1987, the General Assembly again called for "full and immediate compliance" with the World Court decision. This time only Israel joined the United States in opposing adherence to the ruling.
U.S. defense and response.
The United States refused to participate in the merits phase of the proceedings, but the Court found that the US refusal did not prevent it from deciding the case. The Court also rejected the United States defense that its action constituted collective self-defense. 
The United States argued that the Court did not have jurisdiction, with U.S. ambassador to the United Nations Jeane Kirkpatrick dismissing the Court as a "semi-legal, semi-juridical, semi-political body, which nations sometimes accept and sometimes don't." 
The United States had signed the treaty accepting the Court's decision as binding, but with the exception that the court would not have the power to hear cases based on multilateral treaty obligations unless it involved all parties to the treaty affected by that decision or the United States specially agreed to jurisdiction. The court found that it was obliged to apply this exception and refused to take on claims by Nicaragua based on the United Nations Charter and Organization of American States charter, but concluded that it could still decide the case based on customary international law obligations with 11-4 majority.
After five vetoes in the Security Council between 1982 and 1985 of resolutions concerning the situation in Nicaragua , the United States made one final veto on 28 October 1986 (France, Thailand, and United Kingdom abstaining) of a resolution calling for full and immediate compliance with the Judgement.
When the same resolution was brought before the United Nations General Assembly on 3 November it was passed. Only El Salvador and Israel voted with the U.S. against it. El Salvador's ruling junta was at that time receiving substantial funding and military advisement from the U.S., which was aiming to crush a Sandinista-like revolutionary movement by the FMLN. In spite of this resolution, the U.S. still chose not to pay the fine.
Significance.
Third-party interpretations.
Professor of International Law, Anthony D'Amato, writing for the American Journal of International Law, Vol. 80, 1986, commented on this case, stating that "...law would collapse if defendants could only be sued when they agreed to be sued, and the proper measurement of that collapse would be not just the drastically diminished number of cases but also the necessary restructuring of a vast system of legal transactions and relations predicated on the availability of courts as a last resort. There would be talk of a return to the law of the jungle." The author also notes that the case resulted in an unusual candor. A month after the announced withdrawal, Secretary of State Shultz suggested, and President Reagan later confirmed in a press conference, that the goal of U.S. policy was to overthrow the Sandinista Government of Nicaragua (see N.Y. Times, Feb. 22, 1985, at A10, cols. 1, 3). Although this was what Nicaragua had alleged to be the U.S. goal, while the case was actively pending, the United States could not concede that goal without serious risk of undermining its litigating position.
References.
</dl>

</doc>
<doc id="22035" url="http://en.wikipedia.org/wiki?curid=22035" title="No-cloning theorem">
No-cloning theorem

In physics, the no-cloning theorem states that it is impossible to create an identical copy of an arbitrary unknown quantum state. This no-go theorem of quantum mechanics was articulated by Wootters and Zurek and Dieks in 1982, and has profound implications in quantum computing and related fields.
The state of one system can be entangled with the state of another system. For instance, one can use the controlled NOT gate and the Walsh–Hadamard gate to entangle two qubits. This is not cloning. No well-defined state can be attributed to a subsystem of an entangled state. Cloning is a process whose result is a separable state with identical factors.
According to Asher Peres and David Kaiser, the publication of the no-cloning theorem was prompted by a proposal of Nick Herbert for a superluminal communication device using quantum entanglement.
The no-cloning theorem is normally stated and proven for pure states; the no-broadcast theorem generalizes this result to mixed states.
The no-cloning theorem has a time-reversed dual, the no-deleting theorem. Together, these underpin the interpretation of quantum mechanics in terms of category theory, and, in particular, as a dagger compact category. This formulation, known as categorical quantum mechanics, allows, in turn, a connection to be made from quantum mechanics to linear logic as the logic of quantum information theory (in the same sense that classical logic arises from Cartesian closed categories).
Theorem and proof.
Suppose the state of a quantum system A, which we wish to copy, is formula_1 (see bra–ket notation). In order to make a copy, we take a system B with the same state space and initial state formula_2. The initial, or blank, state must be independent of formula_1, of which we have no prior knowledge. The state of the composite system is then described by the following tensor product:
There are only two permissible quantum operations with which we may manipulate the composite system. 
We could perform an observation, which irreversibly collapses the system into some eigenstate of an observable, corrupting the information contained in the qubit(s). This is obviously not what we want. Alternatively, we could control the Hamiltonian of the system, and thus the time-evolution operator "U" (for a time independent Hamiltonian, formula_6, where formula_7 is called the generator of translations in time) up to some fixed time interval, which yields a unitary operator "U". Then "U" acts as a copier provided that
for all possible states formula_9 in the state space. We now select an arbitrary pair of states
formula_1 and formula_11 drawn from the Hilbert space. Because "U" is unitary, it preserves the inner product:
and because quantum mechanical states are assumed to be normalized, it follows that
This implies that either formula_14 or formula_15, so we obtain either formula_16 or formula_17 is orthogonal to formula_18. However, this cannot be the case for two "arbitrary" states. Therefore a single universal "U" cannot clone a "general" quantum state. This proves the no-cloning theorem.
Note that is possible to find specific pairs that satisfy the algebraic requirement above. An example is given by the orthogonal states
and one verifies that formula_20 in this special case. But this relationship does not hold for more-general quantum states.
Generalizations.
Mixed states and nonunitary operations.
In the statement of the theorem, two assumptions were made: the state to be copied is a pure state and the proposed copier acts via unitary time evolution. These assumptions cause no loss of generality. If the state to be copied is a mixed state, it can be purified. Alternately, a different proof can be given that works directly with mixed states; in this case, the theorem is often known as the no-broadcast theorem. Similarly, an arbitrary quantum operation can be implemented via introducing an ancilla and performing a suitable unitary evolution. Thus the no-cloning theorem holds in full generality.
Arbitrary sets of states.
Non-clonability can be seen as a property of arbitrary sets of quantum states. If we know that a system's state is one of the states in some set S, but we do not know which one, can we prepare another system in the same state? If the elements of S are pairwise orthogonal, the answer is always yes: for any such set there exists a measurement which will ascertain the exact state of the system without disturbing it, and once we know the state we can prepare another system in the same state. 
On the other hand, if S contains a pair of elements that are not pairwise orthogonal, then an argument like that given above shows that the answer is no. So even if we can narrow down the state of a quantum system to just "two" possibilities, we still cannot clone it in general (unless the states happen to be orthogonal).
Another way of stating the no-cloning theorem is that amplification of a quantum signal can only happen with respect to some orthogonal basis. This is related to the emergence of the rules of classical probability via quantum decoherence.
No-cloning in a classical context.
There is a classical analogue to the quantum no-cloning theorem, which might be stated as follows: given only the result of one flip of a (possibly biased) coin, we cannot simulate a second, independent toss of the same coin. The proof of this statement uses the linearity of classical probability, and has exactly the same structure as the proof of the quantum no-cloning theorem. Thus, in order to claim that no-cloning is a uniquely quantum result, some care is necessary in stating the theorem. One way of restricting the result to quantum mechanics is to restrict the states to pure states, where a pure state is defined to be one that is not a convex combination of other states. The classical pure states are pairwise orthogonal, but quantum pure states are not.
Imperfect cloning.
Even though it is impossible to make perfect copies of an unknown quantum state, it is possible to produce imperfect copies. This can be done by coupling a larger auxiliary system to the system that is to be cloned, and applying a unitary transformation to the combined system. If the unitary transformation is chosen correctly, several components of the combined system will evolve into approximate copies of the original system. In 1996, V. Buzek and M. Hillery showed that a universal cloning machine can make a clone of an unknown state with the surprisingly high fidelity of 5/6.
Imperfect cloning can be used as an eavesdropping attack on quantum cryptography protocols, among other uses in quantum information science.

</doc>
<doc id="22036" url="http://en.wikipedia.org/wiki?curid=22036" title="Norman Hackerman">
Norman Hackerman

Norman Hackerman (March 2, 1912 – June 16, 2007) was an American chemist, internationally known as an expert in metal corrosion, and a former president of both the University of Texas at Austin (1967–1970) and Rice University (1970–1985).
Biography.
Born in Baltimore, Maryland, he was the only son of Jacob Hackerman and Anna Raffel, immigrants from the Baltic regions of the Russian Empire that later became Estonia and Latvia, respectively.
Hackerman earned his bachelor's degree in 1932 and his doctor's degree in chemistry in 1935 from Johns Hopkins University. He taught at Johns Hopkins, Loyola College in Baltimore and the Virginia Polytechnic Institute and State University in Blacksburg, Virginia, before working on the Manhattan Project in World War II.
He joined the University of Texas in 1945 as an assistant professor of chemistry, became an associate professor in 1946, a full professor in 1950, a department chair in 1952, dean of research in 1960, vice president and provost in 1961, and vice chancellor for academic affairs for the University of Texas System in 1963. Hackerman left the University of Texas in 1970 for Rice, where he retired 15 years later. He was named professor emeritus of chemistry at the University of Texas in 1985 and taught classes until the end of his life. 
He was a member of the National Academy of Sciences and the American Academy of Arts and Sciences. Among his many honors are the Palladium Medal of the Electrochemical Society, the Gold Medal of the American Institute of Chemists (1978), the Charles Lathrop Parsons Award, the Vannevar Bush Award and the National Medal of Science.
Hackerman served on advisory committees and boards of several technical societies and government agencies, including the National Science Board, the Texas Governor's Task Force on Higher Education and the Scientific Advisory Board of the Welch Foundation.
He also served as editor of the "Journal of The Electrochemical Society" and as president of the Electrochemical Society.
Family.
Hackerman's wife of 61 years, Gene Coulbourn, died in 2002; they had three daughters and one son.
Legacy.
In 2000 the Welch Foundation created the Norman Hackerman Award in Chemical Research to recognize the work of young researchers in Texas. The Rice Board of Trustees established the Norman Hackerman Fellowship in Chemistry in honor of Hackerman's 90th birthday in 2002. In 2008, the original Experimental Science Building at the University of Texas at Austin campus was demolished and rebuilt as the Norman Hackerman Experimental Science Building in his name and honor. The building was completed in late-2010, with the opening and dedication ceremony on March 2, 2011, which was both Hackerman's 99th Birthday and the 175th Anniversary of Texas Independence. The main building at the J. Erik Jonsson Center of the National Academy of Sciences is Hackerman House, named in his honor. Hackerman House overlooks Quissett Harbor in Woods Hole MA, on Cape Cod.

</doc>
<doc id="22037" url="http://en.wikipedia.org/wiki?curid=22037" title="N ray">
N ray

 
N rays (or N-rays) were a hypothesized form of radiation, described by Prosper-René Blondlot in 1903, and initially confirmed by others, but subsequently found to be illusory.
History.
Context.
The N ray affair occurred shortly after a series of major breakthroughs in experimental physics. Victor Schumann discovered vacuum ultraviolet radiation in 1893, Wilhelm Röntgen discovered X-rays in 1895, Henri Becquerel discovered radioactivity in 1896 and in 1897 J. J. Thomson discovered electrons, showing that they were the constituents of cathode rays. This created an expectation within the scientific community that other forms of radiation might be discovered.
At this time, Prosper-René Blondlot was a professor of physics at the University of Nancy studying electromagnetic radiation. Blondlot was a respected member of the scientific community: he was one of eight physicists who were corresponding members of the French Academy of Sciences and was awarded the Academy's Gaston Planté prize in 1893 and the LaCaze prize in 1899. His attempts to measure the speed of electromagnetic waves were commended by Thomson and Henri Poincaré. After the discovery of X rays, Blondlot began investigating the nature of X rays, trying to determine whether they behaved as particles or electromagnetic waves. (This was before wave-particle duality became widely accepted among scientists.)
Initial discovery.
In 1903, Blondlot announced his discovery while working at the University of Nancy and attempting to polarize X-rays. He had perceived changes in the brightness of an electric spark in a spark gap placed in an X-ray beam which he photographed, and he later attributed to the novel form of radiation, naming this the "N rays" for the University of Nancy. Blondlot, Augustin Charpentier, Arsène d'Arsonval and approximately 120 other scientists in 300 published articles claimed to be able to detect N rays emanating from most substances, including the human body with the peculiar exceptions that they were not emitted by green wood and by some treated metals. Most researchers of the subject at the time used the perceived light of a dim phosphorescent surface as "detectors", although work in the period clearly showed the change in brightness to be a physiological phenomenon rather than some actual change in the level of illumination. Physicists Gustave le Bon and P. Audollet and spiritualist Carl Huter even claimed the discovery as their own, leading to a commission of the Académie des sciences to decide priority.
Response to results.
The "discovery" excited international interest and many physicists worked to replicate the effects. However, the notable physicists Lord Kelvin, William Crookes, Otto Lummer, and Heinrich Rubens failed to do so. Following his own failure, self-described as "wasting a whole morning", the American physicist Robert W. Wood, who had a reputation as a popular "debunker" of nonsense during the period, was prevailed upon by the British journal "Nature" to travel to Blondlot's laboratory in France to investigate further. Wood suggested that Rubens should go since he had been the most embarrassed when Kaiser Wilhelm II of Germany asked him to repeat the French experiments, and then after two weeks Rubens had to report his failure to do so. Rubens, however, felt it would look better if Wood went, since Blondlot had been most polite in answering his many questions.
In the darkened room, Wood surreptitiously removed an essential prism from the experimental apparatus, yet the experimenters still said that they observed N rays. Wood also stealthily swapped a large file that was supposed to be giving off N rays with an inert piece of wood, yet the N rays were still "observed". His report on these investigations were published in "Nature",
and they suggested that the N rays were a purely subjective phenomenon, with the scientists involved having recorded data that matched their expectations. By 1905, no one outside of Nancy believed in N rays, but Blondlot himself is reported to have still been convinced of their existence in 1926. Martin Gardner, referencing Wood's biographer William Seabrook's account of the affair, attributed a subsequent decline in mental health and eventual death of Blondlot to the resulting scandal, but there is evidence that this is at least some exaggeration of the facts.
Dictionary addition.
The term "N rays" was added to dictionaries upon discovery and could be found described in dictionaries as real phenomena until at least as late as the 1940s. For instance Webster's Dictionary copyright 1946 defined N rays as "An emanation or radiation from certain hot bodies which increases the luminosity without increasing the temperature: as yet, not fully determined."
Significance.
The incident is used as a cautionary tale among scientists on the dangers of error introduced by experimenter bias. N rays were cited as an example of pathological science by Irving Langmuir. Nearly identical properties of an equally unknown radiation had been recorded about 50 years before in another country by Carl Reichenbach in his treatise "Researches on Magnetism, Electricity, Heat, Light, Crystallization, and Chemical Attraction in their relations to the Vital Force" in 1850, and before that in Vienna by Franz Mesmer in his "Mémoire on the Discovery of Animal-Magnetism" in 1779. It is clear that Reichenbach was aware of Mesmer's work and that researchers in Paris working with Blondlot were aware of Reichenbach's work, although there is no proof that Blondlot was personally aware of it.
A park in central Nancy is named after Blondlot. He left his house and garden to the city which transformed it into a public park. James Randi reported that many citizens of Nancy and members of the faculty at the university did not remember having heard about N-rays or of Blondlot.

</doc>
<doc id="22039" url="http://en.wikipedia.org/wiki?curid=22039" title="Nikolay Kuznetsov (officer)">
Nikolay Kuznetsov (officer)

Nikolay Gerasimovich Kuznetsov (Russian: Никола́й Гера́симович Кузнецо́в) (July 24, 1904 – December 6, 1974) was a Soviet naval officer who achieved the rank of Admiral of the Fleet of the Soviet Union and served as People's Commissar of the Navy during The Second World War. The Russian aircraft carrier "Admiral Kuznetsov" is named in his honor.
Biography.
Early years and advancement.
Nikolay Gerasimovich Kuznetsov was born into a peasant family of Serbian descent in the village of Medvedki in the Kotlassky District of Arkhangelsk Oblast.
In 1919, Kuznetsov joined the Northern Dvina Naval Flotilla, after adding two years to his age in order to be eligible to serve. In 1920 he was stationed at Petrograd and in 1924, as a member of a naval unit, he attended the funeral ceremony of Vladimir Lenin. That same year he joined the Bolshevik Party.
Upon graduation from the Frunze Higher Naval School in 1926, Kuznetsov served on the cruiser "Chervona Ukraina", first as watch officer and then as First Lieutenant. In 1932 he graduated from the Naval College after studying operational tactics. Upon graduation he was offered one of two options – a desk job with the general staff or a command post on a ship.
Kuznetsov successfully applied for the post of executive officer on the cruiser "Krasnyi Kavkaz". Within a year the young officer earned his next promotion. In 1934 he returned to the "Chervona Ukraina", this time as her commander. Under Kuznetsov, the ship became an outstanding example of discipline and organization, quickly drawing attention to her young captain.
From September 5, 1936 to August 15, 1937, Kuznetsov was the naval attaché and chief naval advisor to Republican Spain. Serving during the early stages of the Spanish Civil War, he developed a strong dislike of fascism.
On returning home, on January 10, 1938, he was promoted to the rank of flag officer, 2nd rank, and given command of the Pacific Fleet. While in this position, he came face to face with Stalin's purge of the military. Kuznetsov himself was never implicated, but many of the officers under his command were. Kuznetsov resisted the purges at every step, and his intervention saved the lives of many Soviet officers.
On April 28, 1939, Kuznetsov, still only thirty-four, was appointed the People's Commissar (Minister) of the Navy, a post he would hold for the duration of the Second World War. In 1939, despite the Stalin policy against the Nikolaevsky Engineering Academy, Nikolay Gerasimovich Kuznetsov ordered the return of the Naval Engineering faculty from Moscow, and created the Military Engineering-Technical University to educate engineers for the construction of naval bases. In 1941, he ordered the creation near Leningrad of a special forces company RON, the first Soviet underwater special force.
The Second World War.
Kuznetsov played a crucial role during the first hours of the war – at this pivotal moment, his resolve and blatant disregard for orders averted the destruction of the Soviet Navy. By June 21, 1941, Kuznetzov was convinced of the inevitability of war with Nazi Germany. On the same day Semyon Timoshenko and Georgy Zhukov issued a directive prohibiting Soviet commanders from responding to "German provocations". The Navy, however, constituted a distinct ministry (narkomat), and thus Kuznetsov held a position which was technically outside the direct chain of command. He utilized this fact in a very bold move.
Shortly after midnight on the morning of June 22, Kuznetsov ordered all Soviet fleets to battle readiness. At 4:45am that same morning, the Wehrmacht began Operation Barbarossa. The Soviet Navy was the only branch of the military in the highest state of combat readiness at the start of the initial German push.
In the following two years, Kuznetsov's primary concern was the protection of the Caucasus from a German invasion. Throughout the war, the Black Sea remained the primary theater of operations for the Soviet Navy. During the war years Kuznetsov honed Soviet methods of amphibious assault. In May 1944 he was given the rank of Admiral of the Fleet – a newly created position initially equated to that of a four-star general. In the same year, Kuznetsov was given the title of Hero of the Soviet Union. On May 31, 1945, his rank was equated to the rank of Marshal of the Soviet Union with a similar insignia.
The first fall.
From 1946 to 1947 he was the Deputy Minister of the USSR Armed Forces and Commander-in-Chief of the Naval Forces.
In 1947 he was removed from his post on Stalin's orders and in 1948 he, as well as several other admirals were put on trial by the Naval Tribunal. Kuznetsov was demoted to vice-admiral, while the other admirals received prison sentences of varying length.
In 1951 Stalin ended Kuznetsov's pariah status, once again placing him in command of the Navy (as the Minister of the Navy of the USSR), but without restoring his military rank, which was returned to him upon Stalin's death in 1953. In the same year, he became the First Deputy Minister of Defense of the USSR. In 1955, Kuznetsov was made Commander-in-Chief of the Naval Forces. His rank was raised to Admiral of the Fleet of the Soviet Union and he was awarded the Marshal's Star.
The second fall and retirement.
His newfound prominence brought him into direct conflict with Marshal Zhukov, with whom he had clashed during the war years. On December 8, 1955, using the loss of the battleship "Novorossiysk" as a pretext, Zhukov removed the Admiral from his post; in February 1956 Kuznetsov was again demoted to the rank of vice-admiral, retired and expressly forbidden "any and all work connected with the navy."
During his retirement he wrote and published many essays and articles, as well as several longer works, including his memoirs and an officially sanctioned book, "With a Course for Victory", which dealt with the Patriotic War. His memoirs, unlike those of many other prominent leaders, were written by him personally and are noted for their style.
Kuznetsov also authored several books on the war, on Stalin's repressions, and on the navy which were published posthumously. In these he was highly critical of the Party's interference in the internal affairs of the military, and insisted that "the state must be ruled by law."
Rehabilitation and legacy.
After the retirement of Zhukov in 1957, and of Khrushchev in 1964, a group of naval veterans began a campaign to restore Kuznetsov's rank, with all benefits, and to make him one of the General Inspectors of the Ministry of Defence. Invariably, these requests fell on deaf ears, particularly on those of Kuznetsov's successor, Admiral Gorshkov. Not until July 26, 1988 did the Presidium of the Supreme Soviet of the USSR reinstate Kuznetsov to his former rank of Admiral of the Fleet of the Soviet Union. Kuznetzov is now recognized as one of the most prominent men in the history of the Soviet and, today, of the Russian Navy.
Dates of rank.
"Personal ranks for the Russian Navy were abolished in 1918, and were only restored in 1935, excepting the various ranks of admiral which were not restored until 1940."

</doc>
<doc id="22042" url="http://en.wikipedia.org/wiki?curid=22042" title="Nuon (DVD technology)">
Nuon (DVD technology)

Nuon was a technology developed by VM Labs that adds features to a DVD player. In addition to viewing DVDs, one can play 3D video games and use enhanced DVD navigational tools such as zoom and smooth scanning of DVD playback. One could also play CDs while the Nuon graphics processor generates synchronized graphics on the screen. There were plans to provide Internet access capability in the next generation of Nuon-equipped DVD players.
History.
Nuon originally started off as "Project X," and was featured in "Electronic Gaming Monthly"‍ '​s 1999 Video Game Buyer's Guide. One of the Nuon's main software developers was Jeff Minter, who created a version of "Tempest" entitled "Tempest 3000" for the system and the built-in VLM-2 audio visualizer. However, the Nuon platform was primarily marketed as an expanded DVD format. A large majority of Nuon players that were sold in fact resembled typical consumer DVD players with the only noticeable difference being a Nuon logo. Nuon players offered a number of features that were not available on other DVD players when playing standard DVD-formatted titles. These included very smooth forward and reverse functionality and the ability to smoothly zoom in and out of sections of the video image. In addition, Nuon provided a software platform to DVD authors to provide interactive software like features to their titles.
In North America, Nuon was used in the Samsung DVD-N501 and DVD-N2000 models; they also released several models in other parts of the world: DVD-N504 (Europe), DVD N505 (Europe), and DVD-N591 (Korea). Toshiba released the SD-2300 DVD player, and there are two RCA models, the DRC300N and DRC480N. The Nuon was also used in Motorola's Streamaster 5000 "Digital DNA" set-top box. However, the format has appeared to have died off. Nuon was created by VM Labs, whose assets were sold to Genesis Microchip in April 2002. As of November 2004, there were no Nuon-enabled DVD players shipping and no new Nuon software titles.
Peripherals and accessories.
Peripherals for Nuon-enhanced DVD players included the following:
Released movies.
Only four DVD releases utilized Nuon technology. All of them were released by 20th Century Fox Home Entertainment:
Released games.
Eight games were released for the Nuon:
Homebrew development.
During late 2001, VM Labs released a homebrew SDK which allowed people to be able to program apps/games for their Nuon system. Only the Samsung DVD-N501/​DVDN504/​DVDN505 and RCA DRC300N/​DRC480N can load homebrew games. The Samsung DVDN-2000 and the Toshiba cannot. The RCA DRC300N and RCA DRC480N cannot play commercial Nuon games..
Homebrew releases.
Several homebrew titles have been created for or ported to Nuon. They are not commercially available and require the user to burn the material to a Nuon-compatible CD-R.

</doc>
<doc id="22045" url="http://en.wikipedia.org/wiki?curid=22045" title="Nashville (disambiguation)">
Nashville (disambiguation)

Nashville is the capital of the U.S. state of Tennessee.
Nashville may also refer to:

</doc>
<doc id="22048" url="http://en.wikipedia.org/wiki?curid=22048" title="Cuisine of New England">
Cuisine of New England

New England cuisine is an American cuisine which originated in the northeastern region of the United States known as New England. It is characterized by extensive use of seafood and dairy products, which results from its historical reliance on its seaports and fishing industry, as well as extensive dairy farming in inland regions. Many of New England's earliest Puritan settlers were from eastern England, where baking foods such as pies, beans, and turkey were more common than frying as was the tradition elsewhere. Two prominent characteristic foodstuffs native to New England are maple syrup and cranberries. The traditional standard starch is potato, though rice has a somewhat increased popularity in modern cooking. Although known for limited spices aside from ground black pepper, parsley and sage are common, with a few Caribbean additions like nutmeg. Due to the reliance on dairy, creams are standard. The favored cooking techniques are stewing, steaming, and baking.
History.
Native American foods and cooking methods such as corn meal johnny cakes, oysters, succotash, and New England clam bakes were adopted by early immigrants to New England, as were many staples of their diet such as the nuts of the black walnut tree, the nuts of the shagbark hickory, popcorn, blueberries, blackberries and beach plums. Many of the animals they hunted were avidly adopted by the early settlers of Plymouth Colony and later Massachusetts Bay Colony. In England during this period, carrying weapons (especially guns) was forbidden to any but the upper classes. Upon reaching the New World, these Englishmen found themselves in a land where they could feast on venison from the white tailed deer and the Eastern moose and shoot pigeons for their meat (these were likely featured at the first Thanksgiving feast in 1621.)
Many of New England's earliest Puritan settlers were from eastern England and also brought with them traditions of dairy products and baking pies and other foods. Baked beans, apple pies, baked or roast turkey, pease porridge, and steamed puddings became common Yankee dishes; some are now common nationally during Thanksgiving dinners. Other foods they would have prized would include dishes like roast duck and roast goose, lamb, and hams, and all of the above were brought to the New World as soon as the colonies began to prosper as farmyard stock.
Due to New England's involvement in the Triangle Trade in the 18th century, molasses and rum were common in New England cuisine. Well into the 19th century, molasses from the Caribbean and honey were staple sweeteners for all but the upper class. Prior to Prohibition, some of the finest rum distilleries were located in New England.
Many herbs were uncommon, particularly Mediterranean herbs, which are not hardy in much of New England away from the coast. As a result, most savory New England dishes do not have much strong seasoning aside from salt and ground black pepper, nor are there many particularly spicy staple items. Other dishes meant as desserts often contain ingredients like nutmeg, cinnamon, allspice, cloves, and ground ginger which are a legacy of trade with the Caribbean Sea that began in the 17th Century and lasted well into the 19th.
Even today, traditional cuisine remains a strong part of New England's identity. Some of its plates are now enjoyed by the entire United States, including clam chowder, baked beans, and homemade ice cream. In the past two centuries, New England cooking was strongly influenced and transformed by Irish Americans, the Portuguese fishermen of coastal New England, and Italian Americans.
The oldest operating restaurant in the United States, the Union Oyster House (1826), is located in Boston.
State dishes and staples.
Connecticut is known for its apizza (particularly the white clam pie), shad and shadbakes, grinders (including the state-based Subway chain), and New Haven's claim as the birthplace of the hamburger sandwich at Louis' Lunch in 1900. Italian-inspired cuisine is dominant in the New Haven area, while southeastern Connecticut relies heavily on the fishing industry. Irish American influences are common in the interior portions of the state, including the Hartford area. Hasty pudding is sometimes found in rural communities, particularly around Thanksgiving.
Maine is known for its lobster. Relatively inexpensive lobster rolls (lobster meat mixed with mayonnaise and other ingredients, served in a grilled hot dog roll) are often available in the summer, particularly on the coast. Northern Maine produces potato crops, second only to Idaho in the United States. Moxie, America's first mass-produced soft drink and the official state soft drink, is known for its strong aftertaste and is found throughout New England. Although originally from New Jersey, wax-wrapped salt water taffy is a popular item sold in tourist areas. Wild blueberries are a common ingredient or garnish, and blueberry pie (when made with wild Maine blueberries) is the official state dessert. Red snappers — natural casing frankfurters colored bright red — are considered the most popular type of hot dog in Maine. The whoopie pie is the official state treat. Finally, the Italian sandwich is popular in Portland and southern Maine—Portland restaurant Amato's claims to have invented the Italian sandwich (specifically, a submarine sandwich made with ham, cheese, tomato, raw peppers and pickles, served with or without oil, salt and pepper) in 1902. The city of Portland, Maine, known for its numerous nationally renowned restaurants, was ranked as Bon Appétit magazine's "America's Foodiest Small Town" in 2009.
Coastal Massachusetts is known for its clams, haddock, and cranberries, and previously cod. Boston is known for, among other things, baked beans (hence the nickname "Beantown"), bulkie rolls, and various pastries. Hot roast beef sandwiches served with a sweet barbecue sauce and usually on an onion roll is popular in Boston's surrounding area. The North Shore area is locally known for its roast beef establishments, which slice tender roast beef extremely thin. Apples are grown commercially throughout the Commonwealth. Because of the landlocked, hilly terrain common plant foods in Massachusetts are similar to those of interior northern New England- including potatoes, maple syrup, and wild blueberries. Dairy production is also prominent in this central and western area. Cuisine in western Massachusetts had similar immigrant influences as the coastal regions, though historically strong Eastern European populations instilled kielbasa and pierogi as common dishes.
Southern New Hampshire cuisine is similar to that of the Boston area, featuring fish, shellfish and local apples. As with Maine and Vermont, French-Canadian dishes are popular, including tourtière, which is traditionally served on Christmas Eve, and poutine. Corn chowder, which is similar to clam chowder but with corn and bacon replacing the clams, is also common. Portsmouth is known for its orange cake.
Rhode Island and bordering Bristol County, Massachusetts are known for Rhode Island clam chowder (clear chowder), quahog (hard clams), johnny cakes, coffee milk, celery salt, milkshakes known as "cabinets" (called "frappes" elsewhere in New England), grinders, pizza strips, clam cakes, the chow mein sandwich, and Del's Frozen Lemonade. Another food item popular in Rhode Island and southern Massachusetts is called a "hot wiener" or "New York System wiener," although it is unknown in New York (including Coney Island). Portuguese influences are becoming increasingly popular in the region, with Italian cooking already long established.
Vermont produces Cheddar cheese and other dairy products. It is known in and outside of New England for its maple syrup. Maple syrup is used as an ingredient in some Vermont dishes, including baked beans. Rhubarb pie is a common dessert and has been combined with strawberries in late spring.
Typical foods.
New England also has its own food language. In New England, hot and cold sandwiches in elongated rolls are called subs or grinders. This is opposed to the appellations hoagies or heroes in other sections of the country, in particular those closer to the New York City and Philadelphia areas, where the aforementioned names are more common. Sub is short for submarine sandwich, for which Boston, Massachusetts is one of three main claimants for inventing. In Maine, the Italian sandwich—a variation specifically made up of ham or salami, cheese, peppers, pickles, tomatoes and optional oil—is popular, though usually kept distinct from other subs.
New England hot dog rolls are split on top instead of on the side, and have a more rectangular shape. While overall smaller, when separated they have a larger soft surface area because of the way they are baked which allows for buttering and toasting, which are also commonly used for convenient serving of seafood like lobster or fried clams. Regional bread makers often differentiate between these and the more traditional-style American hot dog rolls by referring to the New England variation as "Frankfurt Rolls" on packaging, with both commonly available next to each other on store shelves (though when purchasing a cooked hot dog or seafood "roll" from a restaurant or food stand, the Frankfurt style is almost exclusively used).
Like many of its sister cities of the East Coast, Boston shares a love of a sandwich that is made up of a long soft roll and filled with grilled sweet peppers, sweet onion, a little olive oil, and Italian style pork sausage, called a sausage and pepper sandwich: it is a nod to the Italian immigrants who settled in Boston a century ago, invented it as a quick snack, and now is part of the fabric of local cuisine: it is only served in the spring and summer and is a staple at Fenway Park, home of the Boston Red Sox baseball team.
New England has many local lagers and ales. Notable examples include Samuel Adams of the Boston Beer Company in Boston (even though the recipe for the beer does not come from New England); Sea Dog Brewing Company of Bangor; Shipyard Brewing Company of Portland; and Smuttynose Brewing Company of Portsmouth, New Hampshire. Vermont-based Woodchuck Draft Cider is a popular alcoholic cider.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="22049" url="http://en.wikipedia.org/wiki?curid=22049" title="Neil Simon">
Neil Simon

Marvin Neil Simon (born July 4, 1927) is an American playwright and screenwriter. He has written more than thirty plays and nearly the same number of movie screenplays, mostly adaptations of his plays. He has received more combined Oscar and Tony nominations than any other writer.
Simon grew up in New York during the Great Depression, with his parents' financial hardships affecting their marriage, and giving him a mostly unhappy and unstable childhood. He often took refuge in movie theaters where he enjoyed watching the early comedians like Charlie Chaplin, which inspired him to become a comedy writer. After a few years in the Army Air Force Reserve after graduating from high school, he began writing comedy scripts for radio and some popular early television shows. Among them were "The Phil Silvers Show" and Sid Caesar's "Your Show of Shows" in 1950, where he worked alongside other young writers including Carl Reiner, Mel Brooks and Selma Diamond.
He began writing his own plays beginning with "Come Blow Your Horn" (1961), which took him three years to complete and ran for 678 performances on Broadway. It was followed by two more successful plays, "Barefoot in the Park" (1963) and "The Odd Couple" (1965), for which he won a Tony Award, making him a national celebrity and "the hottest new playwright on Broadway." His style ranged from romantic comedy to farce to more serious dramatic comedy. Overall, he has garnered seventeen Tony nominations and won three. During one season, he had four successful plays showing on Broadway at the same time, and in 1983 became the only living playwright to have a New York theatre, the Neil Simon Theatre, named in his honor. During the time between the
1960s, 70s, and 80s, he wrote both original screenplays and stage plays, with some films actually based on his plays.
After winning the Pulitzer Prize for drama in 1991 for "Lost in Yonkers", critics began to take notice of the depths, complexity and issues of universal interest in his stories, which expressed serious concerns of most average people. His comedies were based around subjects such as marital conflict, infidelity, sibling rivalry, adolescence, and fear of aging. Most of his plays were also partly autobiographical, portraying his troubled childhood and different stages of his life, creating characters who were typically New Yorkers and often Jewish, like himself. Simon's facility with dialogue gives his stories a rare blend of realism, humor and seriousness which audiences find easy to identify with.
Early years.
Neil Simon was born on July 4, 1927, in The Bronx, New York, to Jewish parents. His father, Irving Simon, was a garment salesman, and his mother, Mamie (Levy) Simon, was mostly a homemaker. Simon had one older brother by eight years, Danny Simon. He grew up in Washington Heights, Manhattan during the period of the Great Depression, graduating from DeWitt Clinton High School when he was sixteen, where he was nicknamed "Doc" and described as extremely shy in the school yearbook.:39
Simon's childhood was difficult and mostly unhappy due to his parents' "tempestuous marriage", and ongoing financial hardship caused by the Depression.:1 His father often abandoned the family for months at a time, causing them further financial and emotional hardship. As a result, Simon and his brother Danny were sometimes forced to live with different relatives, or else their parents took in boarders for some income.:2
During an interview with writer Lawrence Grobel, Simon stated: "To this day I never really knew what the reason for all the fights and battles were about between the two of them ... She'd hate him and be very angry, but he would come back and she would take him back. She really loved him.":378 Simon states that among the reasons he became a writer was to fulfill his need to be independent of such emotional family issues, a need he recognized when he was seven or eight: "I'd better start taking care of myself somehow . . . It made me strong as an independent person.:378
To escape difficulties at home he often took refuge in movie theaters, where he especially enjoyed comedies with silent stars like Charlie Chaplin, Buster Keaton, and Laurel and Hardy. Simon recalls: "I was constantly being dragged out of movies for laughing too loud."
I think part of what made me a comedy writer is the blocking out of some of the really ugly, painful things in my childhood and covering it up with a humorous attitude ... do something to laugh until I was able to forget what was hurting.:2
Simon attributes these childhood movies for inspiring him to some day write comedy: "I wanted to make a whole audience fall onto the floor, writhing and laughing so hard that some of them pass out.":1 He appreciated Chaplin's ability to make people laugh and made writing comedy his long-term goal, and also saw it as a way to connect with people. "I was never going to be an athlete or a doctor.":379 He began creating comedy for which he got paid while still in high school, when at the age of fifteen, Simon and his brother created a series of comedy sketches for employees at an annual department store event. And to help develop his writing skill, he often spent three days a week at the library reading books by famous humorists such as Mark Twain, Robert Benchley, George S. Kaufman and S. J. Perelman.:218
Soon after graduating high school he signed up with the Army Air Force Reserve at New York University, eventually being sent to Colorado as a corporal. It was during those years in the Reserve that Simon began writing, starting as a sports editor. He was assigned to Lowry Air Force Base during 1945 and attended the University of Denver from 1945 to 1946.:2
Writing career.
Television comedy.
Two years later, he quit his job as a mailroom clerk in the Warner Brothers offices in Manhattan to write radio and television scripts with his brother Danny Simon, including tutelage by radio humourist Goodman Ace when Ace ran a short-lived writing workshop for CBS. They wrote for the radio series "The Robert Q. Lewis Show", which led to other writing jobs, including "The Phil Silvers Show". Sid Caesar hired the duo for his popular television comedy series "Your Show of Shows", for which he earned two Emmy Award nominations. 
Simon credits these two latter writing jobs for their importance to his career, stating that "between the two of them, I spent five years and learned more about what I was eventually going to do than in any other previous experience.":381 He adds, "I knew when I walked into "Your Show of Shows", that this was the most talented group of writers that up until that time had ever been assembled together." Simon describes a typical writing session with the show:
There were about seven writers, plus Sid, Carl Reiner, and Howie Morris. Mel Brooks and maybe Woody Allen would write one of the other sketches ... everyone would pitch in and rewrite, so we all had a part of it ... It was probably the most enjoyable time I ever had in writing with other people.:382
Simon incorporated some of their experiences into his play "Laughter on the 23rd Floor" (1993). The play won him two Emmy Award nominations and the appreciation of Phil Silvers, who, in 1959, hired him to write scripts for his character as Sergeant Bilko on "The Phil Silvers Show". The first Broadway show Simon wrote was "Catch a Star!" (1955), collaborating on sketches with his brother, Danny.
Playwright.
During 1961, Simon's first Broadway play, "Come Blow Your Horn", ran for 678 performances at the Brooks Atkinson Theatre. Simon took three years to write that first play, partly because he was also working on writing television scripts at the same time. He rewrote the play at least twenty times from beginning to end::384 "It was the lack of belief in myself. I said, 'This isn't good enough. It's not right. . . It was the equivalent of three years of college.":384 That play, besides being a "monumental effort" for Simon, was a turning point in his career: "The theater and I discovered each other.":3
After "Barefoot in the Park" (1963) and "The Odd Couple" (1965), for which he won a Tony Award, he became a national celebrity and was considered "the hottest new playwright on Broadway", writes Susan Koprince in her book on Simon.:3 Those successful productions were followed by others, including "The Good Doctor", "God's Favorite", "Chapter Two", "They're Playing Our Song", "I Ought to Be in Pictures", "Brighton Beach Memoirs", "Biloxi Blues", "Broadway Bound", "Jake's Women", "The Goodbye Girl", and "Laughter on the 23rd Floor". His subjects ranged from serious to romantic comedy to more serious drama and less humor. Overall, he has garnered seventeen Tony nominations and won three.
During 1966 Simon had four shows playing at Broadway theaters at the same time: "Sweet Charity", "The Star-Spangled Girl", "The Odd Couple", and "Barefoot in the Park". His professional association with producer Emanuel Azenberg began with "The Sunshine Boys" during 1972 and continued with "The Good Doctor", "God's Favorite", "Chapter Two", "They're Playing Our Song", "I Ought to Be in Pictures", "Brighton Beach Memoirs", "Biloxi Blues", "Broadway Bound", "Jake's Women", "The Goodbye Girl", and "Laughter on the 23rd Floor", among others.
Simon also adapted material written by others for his plays, such as the musical "Little Me" (1962) from the novel by Patrick Dennis, "Sweet Charity" (1966) from a screenplay by Federico Fellini, and "Promises, Promises" (1968) from a film by Billy Wilder, "The Apartment". Simon has occasionally been brought in as an uncredited "script doctor" to help hone the book for Broadway-bound plays or musicals under development such as A Chorus Line. During the 1970s he wrote a string of successful plays, sometimes having more than one playing at the same time to standing room only audiences. And while he was by then recognized as one of the country's leading playwrights, his inner drive kept him writing:
Did I relax and watch my boyhood ambitions being fulfilled before my eyes? Not if you were born in the Bronx, in the Depression and Jewish, you don't.:47
Simon has also drawn "extensively on his own life and experience" for his stories, with settings typically in working-class New York neighborhoods, similar to ones he grew up in. In 1983 he began writing the first of three autobiographical plays, "Brighton Beach Memoirs" (1983), "Biloxi Blues" (1985), and "Broadway Bound" (1986). With them, he received his greatest critical acclaim. After his follow-up play, "Lost in Yonkers" (1991), Simon was awarded a Pulitzer Prize.
Screenwriter.
Simon has also written screenplays for more than twenty films, and he has received four Academy Award nominations for his screenplays. Some of his screenplays are adaptations of his own plays, along with some original work, including "The Out-of-Towners", "Murder by Death" and "The Goodbye Girl". But although most of his films have been successful, movies were always secondary in importance to his plays::372
I always feel more like a writer when I'm writing a play because of the tradition of the theater ... there is no tradition of the screenwriter, unless he is also the director, which makes him an "auteur". So I really feel that I'm writing for posterity with plays, which have been around since the Greek times.:375
Simon chose not to write the screenplay for his first film adaptation, "Come Blow Your Horn", preferring to focus on his playwriting. However, he was disappointed with the film, and tried to control his film screenplays thereafter. Many of his earlier screenplays were similar to the play, a characteristic Simon observed in hindsight: "I really didn't have an interest in films then", he explains. "I was mainly interested in continuing writing for the theater ... The plays never became cinematic.":153 "The Odd Couple", however, was a highly successful early adaptation, both faithful to the stage play but also more like a traditional film, having more scenic variety.
Themes and genres.
Theater critic John Lahr describes Simon's primary theme as being about "the silent majority", many of whom are "frustrated, edgy, and insecure". Simon's characters are also portrayed as "likable" and easy for audiences to identify with, often having difficult relationships in marriage, friendship or business, as they "struggle to find a sense of belonging".:5 There is always "an implied seeking for solutions to human problems through relationships with other people [and] Simon is able to deal with serious topics of universal and enduring concern", writes biographer Edythe McGovern, while still making people laugh.:11 
She adds that one of Simon's hallmarks is his "great compassion for his fellow human beings,":188 an opinion similar to that of author Alan Cooper, who states that Simon's plays "are essentially about friendships, even when they are about marriage or siblings or crazy aunts ...":46
With regard to places, all of Simon's plays except for two are set in New York, which gives them an urban flavor. Within that setting, Simon's themes, besides marital conflict, sometimes include infidelity, sibling rivalry, adolescences, bereavement, and fear of aging. And despite the serious nature of the themes, Simon has continually managed to tell the stories with humor, developing the theme to include both realism and comedy.:11 Simon said he would tell aspiring comedy playwrights "not to try to make it funny. . . try and make it real and then the comedy will come.":232
"When I was writing plays," he says, "I was almost always (with some exceptions) writing a drama that was funny ... I wanted to tell a story about real people.":219 Simon explains how he manages this combination:
My view is, "how sad and funny life is." I can't think of a humorous situation that does not involve some pain. I used to ask, "What is a funny situation?" Now I ask, "What is a sad situation and how can I tell it humorously?":14
In marriage relationships, his comedies often portray these struggles with plots of marital difficulties or fading love, sometimes leading to separation, divorce and child custody battles. Their endings would typically conclude, after many twists in the plot, to renewal of the relationships.:7
Politics seldom have any overt role in Simon's stories, and his characters avoid confronting society despite their personal problems. "Simon is simply interested in showing human beings as they are—with their foibles, eccentricities, and absurdities.":9 Drama critic Richard Eder notes that Simon's popularity relies on his ability to portray a "painful comedy," where characters say and do funny things in extreme contrast to the unhappiness they are feeling.:14
Simon's plays are generally semi-autobiographical, often portraying aspects of his troubled childhood and first marriages. According to Koprince, Simon's plays also "invariably depict the plight of white middle-class Americans, most of whom are New Yorkers and many of whom are Jewish, like himself.":5 He states, "I suppose you could practically trace my life through my plays.":10 In plays such as "Lost in Yonkers", Simon suggests the necessity of a loving marriage, opposite to that of his parents', and when children are deprived of it in their home, "they end up emotionally damaged and lost".:13
One of the key influences on Simon is his Jewish heritage, says Koprince, although he is unaware of it when writing. For example, in the "Brighton Beach" trilogy, she explains, the lead character is a "master of self-deprecating humor, cleverly poking fun at himself and at his Jewish culture as a whole.":9 Simon himself has said that his characters are people who "often self-deprecating and [who] usually see life from the grimmest point of view.":9 And according to Koprince, this theme in writing "belongs to a tradition of Jewish humor ... a tradition which values laughter as a defense mechanism and which sees humor as a healing, life-giving force.":9
Characters.
Simon's characters are typically portrayed as "imperfect, unheroic figures who are at heart decent human beings", according to Koprince, and she traces Simon's style of comedy to that of Menander, a playwright of ancient Greece. Menander, like Simon, also used average people in domestic life settings, the stories also blending humor and tragedy into his themes.:6 Many of Simon's most memorable plays are built around two-character scenes, as in segments of "California Suite" and "Plaza Suite".
Before writing, Simon tries to create an image of his characters. He says that the play, "Star Spangled Girl" which was a box-office failure, was "the only play I ever wrote where I did not have a clear visual image of the characters in my mind as I sat down at the typewriter.":4 Simon considers "character building" as an obligation, stating that the "trick is to do it skillfully".:4 While other writers have created vivid characters, they have not created nearly as many as Simon: "Simon has no peers among contemporary comedy playwrights," states biographer Robert Johnson.:141
Simon's characters often amuse the audience with sparkling "zingers," believable due to Simon's skill with writing dialogue. He reproduces speech so "adroitly" that his characters are usually plausible and easy for audiences to identify with and laugh at.:190 His characters may also express "serious and continuing concerns of mankind ... rather than purely topical material".:10 McGovern notes that his characters are always impatient "with phoniness, with shallowness, with amorality", adding that they sometimes express "implicit and explicit criticism of modern urban life with its stress, its vacuity, and its materialism.":11 However, Simon's characters will never be seen thumbing his or her nose at society.":141
Style and subject matter.
The key aspect most consistent in Simon's writing style is comedy, situational and verbal, and presents serious subjects in a way that makes audiences "laugh to avoid weeping.":192 He achieves this with rapid-fire jokes and wisecracks,:150 in a wide variety of urban settings and stories.:139 This creates a "sophisticated, urban humor", says editor Kimball King, and results in plays that represent "middle America.":1 Simon creates everyday, apparently simple conflicts with his stories, which become comical premises for problems which need be be solved.:2–3
Another feature of his writing is his adherence to traditional values regarding marriage and family.:150 McGovern states that this thread of the monogamous family runs though most of Simon's work, and is one he feels is necessary to give stability to society.:189 Some critics have therefore described his stories as somewhat old fashioned, although Johnson points out that most members of his audiences "are delighted to find Simon upholding their own beliefs.":142 And where infidelity is the theme in a Simon play, rarely, if ever, do those characters gain happiness: "In Simon's eyes, adds Johnson, "divorce is never a victory.":142
Another aspect of Simon's style is his ability to combine both comedy and drama. "Barefoot in the Park", for example, was a light romantic comedy, while portions of "Plaza Suite" were written as "farce", and portions of "California Suite" are "high comedy".:149
Simon was willing to experiment and take risks, often moving his plays in new and unexpected directions. In "The Gingerbread Lady", he combines comedy with tragedy; "Rumors" (1988) was a full-length farce; in "Jake's Women" and "Brighton Beach Memoirs" he uses dramatic narration; in "The Good Doctor", he created a "pastiche of sketches" around various stories by Chekhov; and "Fools" (1981), was written as a fairy-tale romance similar to stories by Sholem Aleichem.:150 Although some of these efforts failed to win approval by many critics, Koprince claims that they nonetheless demonstrate Simon's "seriousness as a playwright and his interest in breaking new ground.":150
Critical response.
For most of his career Simon's work has received mixed reviews, with many critics admiring his comedy skills, much of it a blend of "humor and pathos".:4 Other critics were less complimentary, noting that much of his dramatic structure was weak and sometimes relied too heavily on gags and one-liners. As a result, notes Kopince, "literary scholars had generally ignored Simon's early work, regarding him as a commercially successful playwright rather than a serious dramatist.":4 Clive Barnes, theater critic for the "New York Times", wrote that like his British counterpart Noël Coward, Simon was "destined to spend most of his career underestimated", but nonetheless very "popular".:foreword
Simon towers like a Colossus over the American Theater. When Neil Simon's time comes to be judged among successful playwrights of the twentieth century, he will definitely be first among equals. No other playwright in history has had the run he has: fifteen "Best Plays" of their season.
Lawrence Grobel:371
This attitude changed after 1991, when he won a Pulitzer Prize for drama with "Lost in Yonkers". McGovern writes that "seldom has even the most astute critic recognized what depths really exist in the plays of Neil Simon.":foreword Although, when "Lost in Yonkers" was considered by the Pulitzer Advisory Board, board member Douglas Watt noted that it was the only play nominated by all five jury members, and that they judged it "a mature work by an enduring (and often undervalued) American playwright.":1
McGovern compares Simon with noted earlier playwrights, including Ben Jonson, Molière, and George Bernard Shaw, pointing out that those playwrights had "successfully raised fundamental and sometimes tragic issues of universal and therefore enduring interest without eschewing the comic mode." She concludes, "It is my firm conviction that Neil Simon should be considered a member of this company ... an invitation long overdue.":foreword McGovern attempts to explain the response of many critics:
Above all, his plays which may appear simple to those who never look beyond the fact that they are amusing are, in fact, frequently more perceptive and revealing of the human condition than many plays labeled complex dramas.:192
Similarly, literary critic Robert Johnson explains that Simon's plays have given us a "rich variety of entertaining, memorable characters" who portray the human experience, often with serious themes. Although his characters are "more lifelike, more complicated and more interesting" than most of the characters audiences see on stage, Simon has "not received as much critical attention as he deserves.":preface Lawrence Grobel, in fact, calls him "the Shakespeare of his time", and possibly the "most successful playwright in history.":371 He states:
Broadway critic Walter Kerr tries to rationalize why Simon's work has been underrated:
Because Americans have always tended to underrate writers who make them laugh, Neil Simon's accomplishment have not gained as much serious critical praise as they deserve. His best comedies contain not only a host of funny lines, but numerous memorable characters and an incisively dramatized set of beliefs that are not without merit. Simon is, in fact, one of the finest writers of comedy in American literary history.:144
Personal life.
Simon has been married five times, to dancer Joan Baim (1953–1973), actress Marsha Mason (1973–1981), twice to actress Diane Lander (1987–1988 and 1990–1998), and currently actress Elaine Joyce. He is the father of Nancy and Ellen, from his first marriage, and Bryn, Lander's daughter from a previous relationship whom he adopted. His nephew is U.S. District Judge Michael H. Simon and niece-in-law is U.S. Congresswoman Suzanne Bonamici.
Simon is on the Board of Selectors of Jefferson Awards for Public Service.
Honors and recognition.
Simon has been conferred with two "honoris causa" degrees; a Doctor of Humane Letters from Hofstra University and a Doctor of Laws from Williams College. In 1983 Simon became the only living playwright to have a New York theatre named after him. The legitimate Broadway theater the Neil Simon Theatre, formerly the Alvin Theatre, was named in his honor, and he is an honorary member of the Walnut Street Theatre's board of trustees. Also in 1983, Simon was inducted into the American Theater Hall of Fame.
In 1965 he won the Tony Award for Best Playwright ("The Odd Couple"), and in 1975, a special Tony Award for his overall contribution to American theater. For "Brighton Beach Memoirs" (1983) he was awarded the New York Drama Critics Circle Award, followed by another Tony Award for Best Play of 1985, "Biloxi Blues". In 1991 he won the Pulitzer Prize along with the Tony Award for "Lost in Yonkers" (1991).

</doc>
<doc id="22050" url="http://en.wikipedia.org/wiki?curid=22050" title="North American Free Trade Agreement">
North American Free Trade Agreement

The North American Free Trade Agreement (NAFTA; Spanish: "Tratado de Libre Comercio de América del Norte", "TLCAN"; French: "Accord de libre-échange nord-américain", "ALÉNA") is an agreement signed by Canada, Mexico, and the United States, creating a trilateral trade bloc in North America. The agreement came into force on January 1, 1994. It superseded the Canada–United States Free Trade Agreement between the U.S. and Canada.
NAFTA has two supplements: the North American Agreement on Environmental Cooperation (NAAEC) and the North American Agreement on Labor Cooperation (NAALC).
Negotiation and U.S. ratification.
Following diplomatic negotiations dating back to 1990 among the three nations, U.S. President George H. W. Bush, Canadian Prime Minister Brian Mulroney and Mexican President Carlos Salinas, each responsible for spearheading and promoting the agreement, ceremonially signed the agreement in their respective capitals on December 17, 1992. The signed agreement then needed to be ratified by each nation's legislative or parliamentary branch.
The Canada–United States Free Trade Agreement had been very controversial and divisive in Canada, and the 1988 Canadian election was fought almost exclusively on that issue. In that election, more Canadians voted for anti-free trade parties (the Liberals and the New Democrats) but the split caused more seats in parliament to be won by the pro-free trade Progressive Conservatives (PCs). Mulroney and the PCs had a parliamentary majority and were easily able to pass the 1987 Canada-US FTA and NAFTA bills. However, he was replaced as Conservative leader and prime minister by Kim Campbell. Campbell led the PC party into the 1993 election where they were decimated by the Liberal Party under Jean Chrétien, who had campaigned on a promise to renegotiate or abrogate NAFTA; however, Chrétien subsequently negotiated two supplemental agreements with the new US president. In the US, Bush, who had worked to "fast track" the signing prior to the end of his term, ran out of time and had to pass the required ratification and signing of the implementation law to incoming president Bill Clinton. Prior to sending it to the United States Senate Clinton added two side agreements, The North American Agreement on Labor Cooperation (NAALC) and the North American Agreement on Environmental Cooperation (NAAEC), to protect workers and the environment, plus allay the concerns of many House members. It also required US partners to adhere to environmental practices and regulations similar to its own.
After much consideration and emotional discussion, the House of Representatives passed the North American Free Trade Agreement Implementation Act on November 17, 1993, 234-200. The agreement's supporters included 132 Republicans and 102 Democrats. The bill passed the Senate on November 20, 1993, 61-38. Senate supporters were 34 Republicans and 27 Democrats. Clinton signed it into law on December 8, 1993; the agreement went into effect on January 1, 1994. Clinton, while signing the NAFTA bill, stated that "NAFTA means jobs. American jobs, and good-paying American jobs. If I didn't believe that, I wouldn't support this agreement."
Provisions.
The goal of NAFTA was to eliminate barriers to trade and investment between the U.S., Canada and Mexico. The implementation of NAFTA on January 1, 1994 brought the immediate elimination of tariffs on more than one-half of Mexico's exports to the U.S. and more than one-third of U.S. exports to Mexico. Within 10 years of the implementation of the agreement, all U.S.-Mexico tariffs would be eliminated except for some U.S. agricultural exports to Mexico that were to be phased out within 15 years. Most U.S.-Canada trade was already duty-free. NAFTA also seeks to eliminate non-tariff trade barriers and to protect the intellectual property right of the products.
Chapter 52 provides a procedure for the interstate resolution of disputes over the application and interpretation of NAFTA. It was modeled after Chapter 69 of the Canada-United States Free Trade Agreement. The includes many retired judges, such as Alice Desjardins, John Maxwell Evans, Constance Hunt, John Richard, Arlin M. Adams, Susan Getzendanner, George C. Pratt, Charles B. Renfrew and Sandra Day O'Connor.
Intellectual Property.
The North American Free Trade Agreement Implementation Act made some changes to the Copyright law of the United States, foreshadowing the Uruguay Round Agreements Act of 1994 by restoring copyright (within NAFTA) on certain motion pictures which had entered the public domain.
Environment.
Securing U.S. congressional approval for NAFTA would have been impossible without addressing public concerns about NAFTA’s environmental impact. The Clinton administration negotiated a side agreement on the environment with Canada and Mexico, the North American Agreement on Environmental Cooperation (NAAEC), which led to the creation of the Commission for Environmental Cooperation (CEC) in 1994. To alleviate concerns that NAFTA, the first regional trade agreement between a developing country and two developed countries, would have negative environmental impacts, the CEC was given a mandate to conduct ongoing "ex post" environmental assessment of NAFTA.
In response to this mandate, the CEC created a framework for conducting environmental analysis of NAFTA, one of the first "ex post" frameworks for the environmental assessment of trade liberalization. The framework was designed to produce a focused and systematic body of evidence with respect to the initial hypotheses about NAFTA and the environment, such as the concern that NAFTA would create a "race to the bottom" in environmental regulation among the three countries, or the hope that NAFTA would pressure governments to increase their environmental protection mechanisms. The CEC has held four symposia using this framework to evaluate the environmental impacts of NAFTA and has commissioned 47 papers on this subject. In keeping with the CEC’s overall strategy of transparency and public involvement, the CEC commissioned these papers from leading independent experts.
Agriculture.
From the earliest negotiation, agriculture was (and still remains) a controversial topic within NAFTA, as it has been with almost all free trade agreements that have been signed within the WTO framework. Agriculture is the only section that was not negotiated trilaterally; instead, three separate agreements were signed between each pair of parties. The Canada–U.S. agreement contains significant restrictions and tariff quotas on agricultural products (mainly sugar, dairy, and poultry products), whereas the Mexico–U.S. pact allows for a wider liberalization within a framework of phase-out periods (it was the first North–South FTA on agriculture to be signed).
Transportation infrastructure.
NAFTA established the CANAMEX Corridor for road transport between Canada and Mexico, also proposed for use by rail, pipeline, and fiber optic telecommunications infrastructure. This became a High Priority Corridor under the U.S. Intermodal Surface Transportation Efficiency Act of 1991.
Impact.
NAFTA's effects, both positive and negative, have been quantified by several economists, whose findings have been reported in publications such as the World Bank's "Lessons from NAFTA for Latin America and the Caribbean", "NAFTA's Impact on North America", and "NAFTA Revisited" by the Institute for International Economics.
Canada.
Like Mexico and the U.S., Canada received a modest positive economic benefit as measured by GDP. Many feared declines failed to materialize, and some industries, like the furniture industry, were expected to suffer but grew instead. Canadian manufacturing employment held steady despite an international downward trend in developed countries. One of NAFTA's biggest economic effects on U.S.-Canada trade has been to boost bilateral agricultural flows. In the year 2008 alone, Canada exports to the United States and Mexico were at $381.3 billion, and imports from NAFTA were at $245.1 billion.
A book written by Mel Hurtig published in 2002 called "The Vanishing Country" charged that since NAFTA's ratification more than 10,000 Canadian companies had been taken over by foreigners, and that 98% of all foreign direct investments in Canada were for foreign takeovers.
Mexico.
Maquiladoras (Mexican factories that take in imported raw materials and produce goods for export) have become the landmark of trade in Mexico. These are plants that moved to this region from the United States, hence the debate over the loss of American jobs. Hufbauer's (2005) book shows that income in the maquiladora sector has increased 15.5% since the implementation of NAFTA in 1994. Other sectors now benefit from the free trade agreement, and the share of exports from non-border states has increased in the last five years while the share of exports from maquiladora-border states has decreased. This has allowed for the rapid growth of non-border metropolitan areas, such as Toluca, León and Puebla; all three larger in population than Tijuana, Ciudad Juárez, and Reynosa.
The overall effect of the Mexico–U.S. agricultural agreement is a matter of dispute. Mexico did not invest in the infrastructure necessary for competition, such as efficient railroads and highways, which resulted in more difficult living conditions for the country's poor. Mexico's agricultural exports increased 9.4 percent annually between 1994 and 2001, while imports increased by only 6.9 percent a year during the same period.
One of the most affected agricultural sectors is the meat industry. Mexico has gone from a small player in the pre-1994 U.S. export market to the 2nd largest importer of U.S. agricultural products in 2004, and NAFTA may be credited as a major catalyst for this change. The allowance of free trade removed the hurdles that impeded business between the two countries. As a result, Mexico has provided a growing market for meat for the U.S., leading to an increase in sales and profits for the U.S. meat industry. This coincides with a noticeable increase in Mexican per capita GDP that has created large changes in meat consumption patterns, implying that Mexicans can now afford to buy more meat and thus per capita meat consumption has grown.
Production of corn in Mexico has increased since NAFTA's implementation. However, internal corn demand has increased beyond Mexico's sufficiency, and imports have become necessary, far beyond the quotas Mexico had originally negotiated. Zahniser & Coyle have also pointed out that corn prices in Mexico, adjusted for international prices, have drastically decreased, yet through a program of subsidies expanded by former president Vicente Fox, production has remained stable since 2000.
United States.
The U.S. Chamber of Commerce credits NAFTA with increasing US trade in goods and services with Canada and Mexico from $337 billion in 1993 to $1.2 trillion in 2011, while the AFL-CIO blames the agreement for sending 700,000 American manufacturing jobs to Mexico over that time.
Trade balances.
The US goods trade deficit with NAFTA was $94.6 billion in 2010, a 36.4% increase ($25 billion) over 2009. The US goods trade deficit with NAFTA accounted for 26.8% of the overall U.S. goods trade deficit in 2010. The US had a services trade surplus of $28.3 billion with NAFTA countries in 2009 (the latest data available).
In a study published in the August 2008 issue of the "American Journal of Agricultural Economics", NAFTA has increased U.S. agricultural exports to Mexico and Canada even though most of this increase occurred a decade after its ratification. The study focused on the effects that gradual "phase-in" periods in regional trade agreements, including NAFTA, have on trade flows. Most of the increase in members’ agricultural trade, which was only recently brought under the purview of the World Trade Organization, was due to very high trade barriers before NAFTA or other regional trade agreements.
Investment.
The US foreign direct investment (FDI) in NAFTA countries (stock) was $327.5 billion in 2009 (latest data available), up 8.8% from 2008. The US direct investment in NAFTA countries is in nonbank holding companies, and in the manufacturing, finance/insurance, and mining sectors. The foreign direct investment of Canada and Mexico in the United States (stock) was $237.2 billion in 2009 (the latest data available), up 16.5% from 2008.
Jobs.
Many American small businesses depend on exporting their products to Canada or Mexico under NAFTA. According to the US Trade Representative, this trade supports over 140,000 small and medium-sized businesses in the US. 
According to the Economic Policy Institute, California, Texas, Michigan and other states with high concentrations of manufacturing jobs were most affected by job loss due to NAFTA.
Environment.
Overall, none of the initial hypotheses were confirmed. NAFTA did not inherently present a systemic threat to the North American environment, as was originally feared. NAFTA-related environmental threats instead occurred in specific areas where government environmental policy, infrastructure, or mechanisms were unprepared for the increasing scale of production under trade liberalization. In some cases, environmental policy was neglected in the wake of trade liberalization; in other cases, NAFTA's measures for investment protection, such as Chapter 11, and measures against non-tariff trade barriers threatened to discourage more vigorous environmental policy. The most serious overall increases in pollution due to NAFTA were found in the base metals sector, the Mexican petroleum sector, and the transportation equipment sector in the United States and Mexico, but not in Canada.
Mobility of persons.
According to the Department of Homeland Security Yearbook of Immigration Statistics, during fiscal year 2006 (i.e., October 2005 through September 2006), 73,880 foreign professionals (64,633 Canadians and 9,247 Mexicans) were admitted into the United States for temporary employment under NAFTA (i.e., in the TN status). Additionally, 17,321 of their family members (13,136 Canadians, 2,904 Mexicans, as well as a number of third-country nationals married to Canadians and Mexicans) entered the U.S. in the treaty national's dependent (TD) status. Because DHS counts the number of the new I-94 arrival records filled at the border, and the TN-1 admission is valid for three years, the number of non-immigrants in TN status present in the U.S. at the end of the fiscal year is approximately equal to the number of admissions during the year. (A discrepancy may be caused by some TN entrants leaving the country or changing status before their three-year admission period has expired, while other immigrants admitted earlier may change their status "to" TN or TD, or extend TN status granted earlier).
Canadian authorities estimated that, as of December 1, 2006, a total of 24,830 U.S. citizens and 15,219 Mexican citizens were present in Canada as "foreign workers". These numbers include both entrants under the NAFTA agreement and those who have entered under other provisions of the Canadian immigration law. New entries of foreign workers in 2006 were 16,841 (U.S. citizens) and 13,933 (Mexicans).
Disputes and controversies.
Legal disputes.
In 1996, the gasoline additive MMT was brought into Canada by Ethyl Corporation, an American company. At the time, the Canadian federal government banned the importation of the additive. The American company brought a claim under NAFTA Chapter 11 seeking US$201 million, from the Canadian government and the Canadian provinces under the Agreement on Internal Trade ("AIT"). The American company argued that their additive had not been conclusively linked to any health dangers, and that the prohibition was damaging to their company. Following a finding that the ban was a violation of the AIT, the Canadian federal government repealed the ban and settled with the American company for US$13 million. Studies by Health and Welfare Canada (now Health Canada) on the health effects of MMT in fuel found no significant health effects associated with exposure to these exhaust emissions. Other Canadian researchers and the U.S. Environmental Protection Agency disagree with Health Canada, and cite studies that include possible nerve damage.
Canada had filed numerous motions to have the duty eliminated and the collected duties returned to Canada. After the United States lost an appeal from a NAFTA panel, it responded by saying "We are, of course, disappointed with the [NAFTA panel's] decision, but it will have no impact on the anti-dumping and countervailing duty orders." (Nick Lifton, spokesman for U.S. Trade Representative Rob Portman) On July 21, 2006, the United States Court of International Trade found that imposition of the duties was contrary to U.S. law.
Change in income trust taxation not expropriation.
On October 30, 2007, American citizens filed a Notice of Intent to Submit a Claim to Arbitration under NAFTA, claiming thousands of U.S. investors lost a total of $5 billion in the fall-out from the Conservative Government's decision the previous year to change the tax rate on income trusts in the energy sector. On April 29, 2009, a determination was made that this change in tax law was not expropriation.
Impact on Mexican farmers.
In 2000, U.S. government subsidies to the corn sector totalled $10.1 billion. These subsidies have led to charges of dumping, which jeopardizes Mexican farms and the country's food self-sufficiency.
Other studies reject NAFTA as the force responsible for depressing the incomes of poor corn farmers, citing the trend's existence more than a decade before NAFTA's existence, an increase in maize production after NAFTA went into effect in 1994, and the lack of a measurable impact on the price of Mexican corn due to subsidized corn coming into Mexico from the United States, though they agree that the abolition of U.S. agricultural subsidies would benefit Mexican farmers. According to Graham Purchase in "Anarchism and Environmental Survival", NAFTA could cause "the destruction of the ejidos (peasant cooperative village holdings) by corporate interests, and threatens to completely reverse the gains made by rural peoples in the Mexican Revolution."
Zapatista Uprising in response to NAFTA in Chiapas.
The preparations for NAFTA included cancellation of Article 27 of Mexico's constitution, the cornerstone of Emiliano Zapata's revolution of 1910–1919. Under the historic Article 27, Indian communal landholdings were protected from sale or privatization. However, this barrier to investment was incompatible with NAFTA. With the removal of Article 27, Indian farmers feared the loss of their remaining lands, and also feared cheap imports (substitutes) from the US. Thus, the Zapatistas labeled NAFTA as a "death sentence" to Indian communities all over Mexico. Then EZLN declared war on the Mexican state on January 1, 1994, the day NAFTA came into force.
Chapter 11.
Another contentious issue is the impact of the investor state dispute settlement obligations contained in Chapter 11 of the NAFTA. Chapter 11 allows corporations or individuals to sue Mexico, Canada or the United States for compensation when actions taken by those governments (or by those for whom they are responsible at international law, such as provincial, state, or municipal governments) violate the international law.
This chapter has been criticized by groups in the U.S., Mexico, and Canada for a variety of reasons, including not taking into account important social and environmental considerations. In Canada, several groups, including the Council of Canadians, challenged the constitutionality of Chapter 11. They lost at the trial level and have subsequently appealed.
Methanex Corporation, a Canadian corporation, filed a US$970 million suit against the United States, claiming that a California ban on Methyl tert-butyl ether (MTBE), a substance that had found its way into many wells in the state, was hurtful to the corporation's sales of methanol. However, the claim was rejected, and the company was ordered to pay US$3 million to the U.S. government in costs. The tribunal based its decision namely on following reasoning: But as a matter of general international law, a non-discriminatory regulation for a public purpose, which is enacted in accordance with due process and, which affects, inter alios, a foreign investor or investment is not deemed expropriatory and compensable unless specific commitments had been given by the regulating government to the then putative foreign investor contemplating investment that the government would refrain from such regulation.
In another case, Metalclad, an American corporation, was awarded US$15.6 million from Mexico after a Mexican municipality refused a construction permit for the hazardous waste landfill it intended to construct in Guadalcázar, San Luis Potosí. The construction had already been approved by the federal government with various environmental requirements imposed (see paragraph 48 of the tribunal decision). The NAFTA panel found that the municipality did not have the authority to ban construction on the basis of the environmental concerns.
Eli Lilly and Company v. Government of Canada is a US$500mn claim for faulty drug patent legislation. Apotex is suing the US for US$520mn because of lost opportunity in a FDA generic drug decision.
 v. Government of Canada has filed a US$250mn claim against Canada, whom it accuses of "arbitrary, capricious and illegal" behaviour, because Quebec aims to prevent fracking exploration under the St. Lawrence Seaway. , the lawyer who represents Lone Pine, has decried attempts to portray his client as "another rapacious multinational challenging governments’ ability to regulate for health, safety and the environment". Lone Pine Resources is incorporated in Delaware but headquartered in Calgary, and had an initial public offering of stock on the NYSE on May 25, 2011, which offered 15M shares each for $13 and raised US$195mn. Barutciski acknowledged "that NAFTA and other investor-protection treaties create an anomaly in that Canadian companies that have also seen their permits rescinded by the very same Quebec legislation, which expressly forbids the paying of compensation, do not have the right pursue a NAFTA claim," and that winning "compensation in Canadian courts for domestic companies in this case would be more difficult since the Constitution puts property rights in provincial hands." A treaty with China would extend similar rights to Chinese investors, including SOEs.
Chapter 19.
Also contentious is NAFTA's Chapter 19, which subjects antidumping and countervailing duty (AD/CVD) determinations to binational panel review instead of, or in addition to, conventional judicial review. For example, in the United States, review of agency decisions imposing antidumping and countervailing duties are normally heard before the U.S. Court of International Trade, an Article III court. NAFTA parties, however, have the option of appealing the decisions to binational panels composed of five citizens from the two relevant NAFTA countries. The panelists are generally lawyers experienced in international trade law. Since the NAFTA does not include substantive provisions concerning AD/CVD, the panel is charged with determining whether final agency determinations involving AD/CVD conform with the country's domestic law. Chapter 19 can be considered as somewhat of an anomaly in international dispute settlement since it does not apply international law, but requires a panel composed of individuals from many countries to reexamine the application of one country's domestic law.
A Chapter 19 panel is expected to examine whether the agency's determination is supported by "substantial evidence." This standard assumes significant deference to the domestic agency.
Some of the most controversial trade disputes in recent years, such as the U.S.-Canada softwood lumber dispute, have been litigated before Chapter 19 panels.
Decisions by Chapter 19 panels can be challenged before a NAFTA extraordinary challenge committee. However, an extraordinary challenge committee does not function as an ordinary appeal. Under the NAFTA, it will only vacate or remand a decision if the decision involves a significant and material error that threatens the integrity of the NAFTA dispute settlement system. Since January 2006, no NAFTA party has successfully challenged a Chapter 19 panel's decision before an extraordinary challenge committee.

</doc>
<doc id="22051" url="http://en.wikipedia.org/wiki?curid=22051" title="National Lampoon (magazine)">
National Lampoon (magazine)

National Lampoon was a ground-breaking American humor magazine which ran from 1970 to 1998. The magazine started out as a spinoff from the "Harvard Lampoon". "National Lampoon" magazine reached its height of popularity and critical acclaim during the 1970s, when it had a far-reaching effect on American humor and comedy. The magazine spawned films, radio, live theatre, various kinds of recordings, and print products including books. Many members of the creative staff from the magazine subsequently went on to contribute creatively to successful media of all types.
During the magazine's most successful years, parody of every kind was a mainstay; surrealist content was also central to its appeal. Almost all the issues included long text pieces, shorter written pieces, a section of actual news items (dubbed "True Facts"), cartoons and comic strips. Most issues also included "Foto Funnies" or fumetti, which often featured nudity. The result was an unusual mix of intelligent, cutting-edge wit, combined with some crass, bawdy jesting.
In both cases, "National Lampoon" humor often pushed far beyond the boundaries of what was generally considered appropriate and acceptable. As co-founder Henry Beard described the experience years later: "There was this big door that said, 'Thou shalt not.' We touched it, and it fell off its hinges."
The magazine declined during the late 1980s and never recovered. It was kept alive minimally, but ceased publication altogether in 1998.
About the magazine.
"National Lampoon" was started by Harvard graduates and "Harvard Lampoon" alumni Doug Kenney, Henry Beard and Robert Hoffman in 1969, when they first licensed the "Lampoon" name for a monthly national publication. The magazine's first issue was dated April 1970. The company that owned the magazine was called Twenty First Century Communications.
After a shaky start for a few issues, the magazine rapidly grew in popularity. Like the "Harvard Lampoon", individual issues had themes, including such topics as "The Future", "Back to School", "Death", "Self-Indulgence", and "Blight". The magazine regularly reprinted material in "best-of" omnibus collections. Its writers joyfully targeted every kind of phoniness, and had no specific political stance, even though individual staff members had strong political views.
"National Lampoon" was a monthly magazine for most of its publication history. Numerous "special editions" were also published and sold simultaneously on newsstands. Some of the special editions were anthologies of reprinted material; others were entirely original. Additional projects included a calendar, a songbook, a collection of transfer designs for T-shirts, and a number of books. The magazine sold yellow binders with the Lampoon logo, designed to store a year's worth of issues.
Cover art.
The original art directors were cartoonist Peter Bramley and Bill Skurski, founders of New York's "Cloud Studio", an alternative-culture outfit known at the time for its eclectic style. Bramley created the Lampoon's first cover and induced successful cartoonists Arnold Roth and Gahan Wilson to become regular contributors. 
Beginning with the eighth issue, the art direction of the magazine was taken over by Michael C. Gross, who directed the look of the magazine until 1974. A number of the "National Lampoon"'s most acerbic and humorous covers were designed or overseen by Gross, including:
Michael Gross and Doug Kenney chose a young designer from "Esquire" Magazine named Peter Kleinman to succeed the team of Gross and David Kaestle. During his Lampoon tenure, Kleinman was also the art director of "Heavy Metal" magazine, published by the same company. The best known of Kleinman's Lampoon covers were "Stevie Wonder with 3-D Glasses," painted by Sol Korby; a photographed "Nose to The Grindstone" cover depicting a man's face being pressed against a spinning grinder wheel for the "Work" issue; the "JFK's First 6000 Days Issue," featuring a portrait of an old John F. Kennedy; the "Fat Elvis" Cover which appeared a year before Elvis Presley died, and many of the Mara McAfee covers done in a classic Norman Rockwell style. Kleinman designed the logos for "Animal House" and "Heavy Metal." 
Kleinman left in 1979 to open an ad agency.
He was succeeded by Skip Johnson, the designer responsible for the "Sunday Newspaper Parody" and the "Arab Getting Punched in the Face" cover of the "Revenge" Issue. Johnson went on to "The New York Times." He was followed by Michael Grossman, who changed the logo and style of the magazine.
In 1984, Kleinman returned as Creative Director and went back to the 1970s logo and style, bringing back many of the artists and writers from the magazine's heyday. He left four years later to pursue a career in corporate marketing. At that time, the "National Lampoon" magazine entered a period of precipitous decline.
Editorial.
Every regular monthly issue of the magazine had an editorial at the front of the magazine. This often appeared to be straightforward, but was always a parody. It was written by whoever was the editor of that particular issue, since that role rotated among the staff. A few issues were guest-edited.
Staff.
The magazine was an outlet for some notable writing talents, including Kenney, Beard, George W. S. Trow, Chris Miller, P. J. O'Rourke, Michael O'Donoghue, Chris Rush, Sean Kelly, Tony Hendra, Brian McConnachie, Gerald Sussman, Ellis Weiner, Danny Abelson, Ted Mann, Chris Cluess, Al Jean, Mike Reiss, Jeff Greenfield, and John Hughes.
The work of many important cartoonists, photographers and illustrators appeared in the magazine's pages, including Neal Adams, Gahan Wilson, Michael Sullivan, Ron Barrett, Peter Bramley, Vaughn Bode, Bruce McCall, Rick Meyerowitz, M. K. Brown, Shary Flenniken, Bobby London, Edward Gorey, Jeff Jones, Joe Orlando, Arnold Roth, Rich Grote, Ed Subitzky, Mara McAfee, Sam Gross, Charles Rodrigues, Buddy Hickerson, B. K. Taylor, Birney Lettick, Frank Frazetta, Boris Vallejo, Marvin Mattelson, Stan Mack, Chris Callis, John E. Barrett, Raymond Kursar and Andy Lackow.
Comedy stars John Belushi, Chevy Chase, Gilda Radner, Bill Murray, Brian Doyle Murray, Harold Ramis, and Richard Belzer first gained national attention for their performances in the National Lampoon's stage show and radio show. The first three subsequently went on to become part of "Saturday Night Live"'s original wave of Not Ready for Primetime Players, Bill Murray replaced Chase when Chase left "SNL" after the first season, and Brian Doyle Murray later appeared as an "SNL" regular. Harold Ramis went on to be a prolific director and writer working on such films as "Animal House", "Caddyshack", "Ghostbusters", and many more. Brian Doyle Murray has had roles in dozens of films, and Belzer is an Emmy-award-winning TV actor.
Jerry Taylor aka Gerald L. Taylor was the Publisher, followed by William T. Lippe. The business side of the magazine was controlled by Matty Simmons, who was Chairman of the Board and CEO of Twenty First Century Communications, a publishing company.
True Facts.
"True Facts" was a section near the front of the magazine which contained true but ridiculous items from real life. Together with the masthead, it was one of the few parts of the magazine that was factual. "True Facts" included photographs of unintentionally funny signage, extracts from ludicrous newspaper reports, strange headlines, and so on. For many years John Bendel was in charge of the "True Facts" section of the magazine. Steven Brykman edited the "True Facts" section of the National Lampoon website. Several "True Facts" compilation books were published in the 1980s and early 90s, and several all-True-Facts issues of the magazine were published during the 1980s.
Foto Funnies.
Most issues of the magazine featured one or more "Foto Funny" or fumetti, comic strips that use photographs instead of drawings as illustrations. The characters who appeared in the Lampoon's Foto Funnies were usually editors or contributing editors of the magazine, often cast alongside nude or semi-nude models. In 1980, a paperback compilation book, "National Lampoon Foto Funnies", was published.
Funny Pages.
The "Funny Pages" was a large section at the back of the magazine that was composed entirely of comic strips of various kinds. These included work from a number of artists who also had pieces published in the main part of the magazine, including Gahan Wilson, Ed Subitzky and Vaughn Bode, as well as artists whose work was only published in this section. The regular strips included "Dirty Duck" by Bobby London, "Trots and Bonnie" by Shary Flenniken, "The Appletons" by B. K. Taylor, and "Politeness Man" by Ron Barrett, and many other strips. A compilation of Gahan Wilson's "Nuts" strip was published in 2011. The Funny Pages logo header art residing above Gahan Wilson's "Nuts" in each issue, showing a comfortable, old-fashioned family reading newspaper-sized funny papers, was by Michael Wm Kaluta.
Other merchandise.
From time to time the magazine advertised Lampoon-related merchandise for sale, including tee-shirts that had been especially designed.
Chronology.
The magazine existed from 1970 to 1998. Many consider its finest period was 1971 to 1975, although it continued to be produced on a monthly schedule throughout the 1970s and the early 1980s, and did quite well during that time.
However, during the late 1980s, a much more serious decline set in. In 1989, the company that controlled the magazine and its related projects (which was part of "Twenty First Century Communications") was the subject of a hostile takeover. In 1991 it was sold outright to another company, "J2 Communications".
At that point "National Lampoon" was considered valuable only as a brand name that could be licensed out to other companies. The magazine was issued erratically and rarely from 1991 onwards. 1998 saw the last issue.
1970.
The first issue was April 1970. By November of that year Michael Gross had become the art director. He achieved a unified, sophisticated and integrated look for the magazine, which enhanced its humorous appeal.
1973–1975.
National Lampoon's most successful sales period was 1973–75. Its national circulation peaked at 1,000,096 copies sold of the October 1974 "Pubescence" issue. The 1974 monthly average was 830,000, which was also a peak. Former "Lampoon" editor Tony Hendra's book "Going Too Far" includes a series of precise circulation figures.
The magazine was considered by many to be at its creative zenith during this time. It should however be noted that the publishing industry's newsstand sales were excellent for many other titles during that time: there were sales peaks for "Mad" (more than 2 million), "Playboy" (more than 7 million), and "TV Guide" (more than 19 million).
1975.
Some fans consider the glory days of National Lampoon to be from 1972 to 1975, although the magazine remained popular and profitable after that point. During 1975, the three founders (Kenney, Beard and Hoffman) took advantage of a buyout clause in their contracts for $7.5 million. And, at about the same time, writers Michael O'Donoghue and Anne Beatts left to join the NBC comedy show "Saturday Night Live" ("SNL"). At the same time, the National Lampoon Show's John Belushi and Gilda Radner left the troupe to join the original septet of SNL's "Not Ready for Primetime Players."
The magazine was a springboard to Hollywood for a generation of comedy writers, directors, and performers. Various alumni went on to create and write for "Saturday Night Live," "The David Letterman Show," SCTV, "The Simpsons", "Married... with Children", "Night Court", and various films including "National Lampoon's Animal House", "Caddyshack", "National Lampoon's Vacation", and "Ghostbusters".
As some of the original creators departed, the magazine remained popular and profitable as it saw the emergence of John Hughes and editor-in-chief P.J. O'Rourke, along with artists and writers such as Gerry Sussman, Ellis Weiner, Tony Hendra, Ted Mann, Peter Kleinman, Chris Cleuss, Stu Kreisman, John Weidman, Jeff Greenfield, Bruce McCall, and Rick Meyerowitz.
1985.
In 1985, Matty Simmons (who had been working only on the business end of the Lampoon up to that point) took over as Editor-in-Chief. He fired the entire editorial staff, and appointed his two sons, Michael Simmons and Andy Simmons, as editors, Peter Kleinman as Creative Director and Editor, and Larry "Ratso" Sloman as Executive Editor.
The magazine was on an increasingly shaky financial footing, and beginning in November 1986, the magazine was published six times a year instead of every month.
1989.
In 1989, the magazine was acquired in a hostile takeover by a business partnership of producer Daniel Grodnik and actor Tim Matheson (who played "Otter" in the 1978 film "National Lampoon's Animal House"). Grodnik and Matheson became the co-Chairmen/co CEOS. During their tenure, the stock went up from under $2 dollars to $6 and the magazine and doubled the monthly ad pages from 7 to 15. The Company moved its' head quarters from New York to Los Angeles to focus on film and television. The publishing operation stayed in New York.
1991.
In 1991 the magazine (and more importantly, the rights to the brand name "National Lampoon") were bought by a company called J2 Communications, headed by James P. Jimirro. (J2 was previously known for marketing Tim Conway's "Dorf" videos.)
J2 Communications' focus was to make money by licensing out the brand name "National Lampoon". The company was contractually obliged to publish at least one new issue of the magazine per year in order to retain the rights to the Lampoon name. However, the company had very little interest in the magazine itself; throughout the 1990s the number of issues per year declined precipitously and erratically. In 1991 there was an attempt at monthly publication: nine issues were produced that year. Only two issues were released in 1992. This was followed by one issue in 1993, five in 1994, and three in 1995. For the last three years of its existence, the magazine was published only once a year.
1998, last issue.
The magazine's final print publication was November 1998, after which the contract was renegotiated, and in a sharp reversal, J2 Communications was then "prohibited" from publishing issues of the magazine. J2, however, still owned the rights to the brand name, which it continued to franchise out to other users. In 2002 the use of the brand name and the rights to republish old material were sold to a new, and otherwise unrelated, company which chose to call itself National Lampoon, Incorporated.
Related media.
During its most active period, the magazine spun off numerous productions in a wide variety of media.
"National Lampoon" released books, special issues, anthologies, and other print pieces, including:
Books.
"True Facts" special editions and books
Recordings.
Vinyl.
Vinyl record albums
Vinyl singles
CDs.
Many of the older albums that were originally on vinyl have been re-issued as CDs and a number of tracks from certain albums are available as MP3s.
Films.
There is considerable ambiguity about what actually constitutes a "National Lampoon" film.
During the 1970s and early 1980s, a few films were made as spin-offs from the original "National Lampoon" magazine, using its creative staff. The first theatrical release, and by far the most successful "National Lampoon" film was "National Lampoon's Animal House" (1978). Starring John Belushi and written by Doug Kenney, Harold Ramis and Chris Miller, it became the highest grossing comedy film of all time. Produced on a low budget, it was so enormously profitable that, from that point on for the next two decades, the name "National Lampoon" applied to the title of a movie was considered to be a valuable selling point in and of itself.
Numerous movies were subsequently made that had "National Lampoon" as part of the title. Many of these were unrelated projects, because by that point in time, the name "National Lampoon" could simply be licensed on a one-time basis, by any company, for a fee. Critics such as the "Orlando Sentinel"′s Roger Moore and the "New York Times"′ Andrew Adam Newman have written about the cheapening of the "National Lampoon"′s movie imprimatur; in 2006, an Associated Press review said: “The National Lampoon, once a brand name above nearly all others in comedy, has become shorthand for pathetic frat boy humor."
The first of the "National Lampoon" movies was a not-very-successful made-for-TV movie: 
"National Lampoon's Animal House".
In 1978, "National Lampoon's Animal House" was released. Made on a small budget, it did phenomenally well at the box office. In 2001, the United States Library of Congress considered the film "culturally significant", and preserved it in the National Film Registry.
The script had its origins in a series of short stories that had been previously published in the magazine. These included Chris Miller's "Night of the Seven Fires," which dramatized a frat initiation and included the characters Pinto and Otter, which contained prose versions of the toga party, the "road trip", and the dead horse incident. Another source was Doug Kenney's "First Lay Comics," which included the angel and devil scene and the grocery-cart affair. According to the authors, most of these elements were based on real incidents.
"National Lampoon's Class Reunion".
This 1982 movie was an attempt by John Hughes to make something similar to "Animal House". "National Lampoon's Class Reunion" was not successful however.
"National Lampoon's Vacation".
Released in 1983, the movie "National Lampoon's Vacation" was based upon John Hughes' "National Lampoon" story "Vacation '58". The movie's financial success gave rise to several follow-up films, including "National Lampoon's European Vacation", "National Lampoon's Christmas Vacation" (1989), based on John Hughes' "Christmas '59", and "Vegas Vacation", starring Chevy Chase.
Similar films.
The Robert Altman film "O.C. and Stiggs" (1987) was based on two characters who had been featured in several written pieces in "National Lampoon" magazine, including an issue-long story from October 1982 entitled "The Utterly Monstrous, Mind-Roasting Summer of O.C. and Stiggs." Completed in 1984, the film was not released until 1987, when it was shown in a small number of theaters and without the "National Lampoon" name. It was not a success.
Following the success of "Animal House", "MAD" magazine lent its name to a 1980 comedy titled "Up the Academy". But whereas two of "Animal House"′s co-writers were the "Lampoon"′s Doug Kenney and Chris Miller, "Up The Academy" was strictly a licensing maneuver, with no creative input from "MAD"′s staff or contributors. It was a critical and commercial failure.

</doc>
<doc id="22052" url="http://en.wikipedia.org/wiki?curid=22052" title="Non-disclosure agreement">
Non-disclosure agreement

A non-disclosure agreement (NDA), also known as a confidentiality agreement (CA), confidential disclosure agreement (CDA), proprietary information agreement (PIA), or secrecy agreement (SA), is a legal contract between at least two parties that outlines confidential material, knowledge, or information that the parties wish to share with one another for certain purposes, but wish to restrict access to or by third parties. It is a contract through which the parties agree not to disclose information covered by the agreement. An NDA creates a confidential relationship between the parties to protect any type of confidential and proprietary information or trade secrets. As such, an NDA protects nonpublic business information.
NDAs are commonly signed when two companies, individuals, or other entities (such as partnerships, societies, etc.) are considering doing business and need to understand the processes used in each other's business for the purpose of evaluating the potential business relationship. NDAs can be "mutual", meaning both parties are restricted in their use of the materials provided, or they can restrict the use of material by a single party.
It is also possible for an employee to sign an NDA or NDA-like agreement with an employer. In fact, some employment agreements will include a clause restricting employees' use and dissemination of company-owned confidential information.
Types.
A non-disclosure agreement may be unilateral or bilateral, that is, it may bind only one party or multiple parties (typically two):
Unilateral NDA.
A unilateral, or a one-way, agreement is where one party wants to disclose certain information to another party but needs the information to remain secret for some reason, perhaps due to secrecy requirements required to satisfy patent laws or to make sure that the other party does not take and use the disclosed information without compensating the discloser.
Bilateral NDA.
A bilateral, or mutual, agreement is where both parties will be supplying information that is intended to remain secret. This type of agreement is common when businesses are considering some kind of joint venture or merger.
Some practitioners insist on a mutual NDA in all cases, to encourage the drafter to make the provisions "fair and balanced" in case the drafter's receiving-party client later ends up as a disclosing party, or vice versa (not an uncommon occurrence).
Content.
A nondisclosure agreement can protect any type of information that is not generally known. However, nondisclosure agreements may also contain clauses that will protect the person receiving the information so that if they lawfully obtained the information through other sources they would not be obligated to keep the information secret. In other words, the nondisclosure agreement typically only requires the receiving party to maintain information in confidence when that information has been directly supplied by the disclosing party. Ironically, however, it is sometimes easier to get a receiving party to sign a simple agreement that is shorter, less complex and does not contain safety provisions protecting the receiver.
Some common issues addressed in an NDA include:
California.
In California (and some other states), there are some special circumstances relating to non-disclosure agreements and non-compete clauses. California's courts and legislature have signaled that they generally value an employee's mobility and entrepreneurship more highly than they do protectionist doctrine.
India.
Use of non-disclosure agreements are on the rise in India and is governed by the Indian Contract Act 1872. Use of an NDA is crucial in many circumstances, such as to tie in employees who are developing patentable technology if the employer intends to apply for a patent. Non-disclosure agreements have become very important in light of India's burgeoning outsourcing industry. In India, an NDA must be stamped to be a valid enforceable document.

</doc>
<doc id="22053" url="http://en.wikipedia.org/wiki?curid=22053" title="Network effect">
Network effect

In economics and business, a network effect (also called network externality or demand-side economies of scale) is the effect that one user of a good or service has on the value of that product to other people. When a network effect is present, the value of a product or service is dependent on the number of others using it.
The classic example is the telephone. The more people who own telephones, the more valuable the telephone is to each owner. This creates a positive externality because a user may purchase a telephone without intending to create value for other users, but does so in any case. Online social networks work in the same way, with sites like Twitter and Facebook becoming more attractive as more users join.
The expression "network effect" is applied most commonly to positive network externalities as in the case of the telephone. Negative network externalities can also occur, where more users make a product less valuable, but are more commonly referred to as "congestion" (as in traffic congestion or network congestion).
Over time, positive network effects can create a bandwagon effect as the network becomes more valuable and more people join, in a positive feedback loop.
Origins.
Network effects were a central theme in the arguments of Theodore Vail, the first post patent president of Bell Telephone, in gaining a monopoly on US telephone services. In 1908, when he presented the concept in Bell's annual report, there were over 4000 local and regional telephone exchanges, most of which were eventually merged into the Bell System. 
The economic theory of the network effect was advanced significantly between 1985 and 1995 by researchers Michael L. Katz, Carl Shapiro, Joseph Farrell and Garth Saloner.
Network effects were popularized by Robert Metcalfe, stated as Metcalfe's law. Metcalfe was one of the co-inventors of Ethernet and a co-founder of the company 3Com. In selling the product, Metcalfe argued that customers needed Ethernet cards to grow above a certain critical mass if they were to reap the benefits of their network.
According to Metcalfe, the rationale behind the sale of networking cards was that (1) the cost of the network was directly proportional to the number of cards installed, but (2) the value of the network was proportional to the square of the number of users. This was expressed algebraically as having a cost of N, and a value of N². While the actual numbers behind this definition were never firm, the concept allowed customers to share access to expensive resources like disk drives and printers, send e-mail, and access the Internet.
Rod Beckstrom presented a mathematical model for describing networks that are in a state of positive network effect at BlackHat and Defcon in 2009 and also presented the "inverse network effect" with an economic model for defining it as well.
Benefits.
Network effects become significant after a certain subscription percentage has been achieved, called critical mass. At the critical mass point, the value obtained from the good or service is greater than or equal to the price paid for the good or service. As the value of the good is determined by the user base, this implies that after a certain number of people have subscribed to the service or purchased the good, additional people will subscribe to the service or purchase the good due to the value exceeding the price.
A key business concern must then be how to attract users prior to reaching critical mass. One way is to rely on extrinsic motivation, such as a payment, a fee waiver, or a request for friends to sign up. A more natural strategy is to build a system that has enough value "without" network effects, at least to early adopters. Then, as the number of users increases, the system becomes even more valuable and is able to attract a wider user base.
Beyond critical mass, the increasing number of subscribers generally cannot continue indefinitely. After a certain point, most networks become either congested or saturated, stopping future uptake. Congestion occurs due to overuse. The applicable analogy is that of a telephone network. While the number of users is below the congestion point, each additional user adds additional value to every other customer. However, at some point the addition of an extra user exceeds the capacity of the existing system. After this point, each additional user decreases the value obtained by every other user. In practical terms, each additional user increases the total system load, leading to busy signals, the inability to get a dial tone, and poor customer support. The next critical point is where the value obtained again equals the price paid. The network will cease to grow at this point, and the system must be enlarged. The congestion point may be larger than the market size. New Peer-to-peer technological models may always defy congestion. Peer-to-peer systems, or "P2P," are networks designed to distribute load among their user pool. This theoretically allows true P2P networks to scale indefinitely. The P2P based telephony service Skype benefits greatly from this effect (though market saturation will still occur).
Network effects are commonly mistaken for economies of scale, which result from business size rather than interoperability. To help clarify the distinction, people speak of demand side vs. supply side economies of scale. Classical economies of scale are on the production side, while network effects arise on the demand side. Network effects are also mistaken for economies of scope.
The network effect has a lot of similarities with the description of phenomenon in reinforcing positive feedback loops described in system dynamics. System dynamics could be used as a modelling method to describe phenomena such as word of mouth and Bass model of marketing.
Network effect is a benefit to society as a whole because it positively relates to and affects the Intellectual Commons, Property Rights, and Cultural Commons of the world. One form of network externality is social media, which is a peer-to-peer network ran by a privately held for profit business. Although the creation of a large network creates a barrier to entry according to Porters five forces and may prevent a few from creating a new form of P2P networking, it largely benefits society as whole and provides a new form of a common-pool resource solargely scalable that the entire world has the ability to use it. Although the barrier to entry may be high, there is no true form of monopoly in the P2P social sharing market. For example, Facebook holds a large stake in the P2P social sharing market, but it is not mutually exclusive, meaning users can have an account on Facebook and also have an account on Twitter. Furthermore, there becomes no true critical mass in this space due to the ability for technology and innovation to constantly adapt to different environments, market for underdeveloped countries to integrate with social sharing is unlimited.
Network effect relates to the intellectual commons in a positive way. Through P2P networks users are able to share their intellectual property in a way that can benefit society as a whole.The sharing of intellectual property ultimately relates to, economic growth due to the ability for creators to share information and still possibly benefit financially from it. Through P2P networks people are able to share types of education like scholarly articles, becoming a new form of public commons. Network externality like Ted.com is an example of how intellectual commons with the use of network externality benefits society as a whole. Those who present intellectual property at Ted conferences are sharing their education on a public forum that benefits whoever will listen. Therefore, the larger Ted.com network becomes positively correlates to those who benefit from its common-pool resources.
P2P networks positively affect property rights. In reference to property rights, it enables those who create the intellectual property: The right to use the good, The right to earn income from the good, The right to transfer the good to others, The right to enforcement of property rights. Through P2P networks those who provide intellectual property not only have these rights, but they also possess the right to claim their information on a public forum. Due to these rights sharing benefits the intellectual property holders and promotes P2P sharing in a positive way. Those who consume the intellectual property also benefit positively from the sharing of it because they are able to use the information freely with respect to the person who created it. An example of this system in effect is a company called . Music Vault operates on the P2P network Facebook, enabling users who create music to openly and freely collaborate with other artists content. This is a form of remixing that benefits both parties. This is an example of how a P2P network positively affects the sharing of property rights. In Joseph E. Stiglitz essay "Prizes, Not Patents", he suggests that the creation of intellectual property should be rewarded with by social gratification and rewards instead of patents preventing others from duplicating the creation and sharing it as a common-pool resource. This can be related to P2P networking because it creates a greater incentive for those who create intellectual property to share it is a common-pool resource. As a P2P sharing network becomes larger the gratification of being rewarded on a global public forum would compete with a patent. It is through large P2P networks and network externality that humans can create a reward system large enough to deter seekers of patents to be rewarded in different ways.
Network Externality positively affects the cultural commons in many ways. The reward for being part of a group, society, and even the world through a P2P network is one of the greatest benefits that a modern common-pool resource can provide.The ability to connect and create with people from different cultures, ethnicities, and beliefs is something thought to be impossible 100 years ago. Without network externality this form of communication would have been impossible. Through P2P sharing the world as a culture are able to learn and teach each other through public forums. In Sugata Mitra’s Ted talk, “The child-driven education” he placed a computer in the a third world town and left it there to see what would happen. To his amazement children were able to quickly figure out how to use the computer and educate themselves on its inner workings. This example is a benefit to society for several reasons. The first is the relationship between Sugata Mitra and the P2P network which led him to place the computer in a third world town, along with the ability to present his findings on a public forum. Secondly, it is those who consumed his ted talk and benefited from the knowledge that those in third world countries just need a chance to learn and they will take it. This experiment as a whole brings the culture of the world together and connects us with those we thought impossible due to the P2P network and network externality that led individuals to the Ted talk.
Technology lifecycle.
If some existing technology or company whose benefits are largely based on network effects starts to lose market share against a challenger such as a disruptive technology or open standards based competition, the benefits of network effects will reduce for the incumbent, and increase for the challenger. In this model, a tipping point is eventually reached at which the network effects of the challenger dominate those of the former incumbent, and the incumbent is forced into an accelerating decline, whilst the challenger takes over the incumbent's former position.
Lock-in.
Not surprisingly network economics became a hot topic after the diffusion of the Internet across academia. Most people know only of Metcalfe's law as part of network effects. Network effects are notorious for causing lock-in with the most-cited examples being Microsoft products and the QWERTY keyboard.
Vendor lock-in can be mitigated by opening the standards upon which users depend, allowing competition between implementations. This does not, however, mitigate industry-wide lock-in to the standard itself. Indeed, as there are now multiple vendors driving down the price and increasing the quality, more users are likely to adopt the standard thereby creating greater industry-wide lock-in to the standard.
Types of network effects.
There are many ways to classify networks effects. One popular segmentation views network effects as being of four kinds:
Additionally, there are two sources of economic value that are relevant when analyzing products that display network effects:
Negative network effects.
Negative network effects, in the mathematical sense, are those that have the opposite effect on stability compared to normal (positive) network effects. Just like positive network effects cause positive feedback loops and exponential growth, negative network effects create negative feedback and exponential decay. In nature, negative network effects are the forces that pull towards equilibrium, are responsible for stability, and are the physical limitations preventing states from reaching infinity.
Open versus closed standards.
In communication and information technologies, open standards and interfaces are often developed through the participation of multiple companies and are usually perceived to provide mutual benefit. But, in cases in which the relevant communication protocols or interfaces are closed standards the network effect can give the company controlling those standards monopoly power. The Microsoft corporation is widely seen by computer professionals as maintaining its monopoly through these means. One observed method Microsoft uses to put the network effect to its advantage is called Embrace, extend and extinguish.
Mirabilis is an Israeli start-up which pioneered instant messaging (IM) and was bought by America Online. By giving away their ICQ product for free and preventing interoperability between their client software and other products, they were able to temporarily dominate the market for instant messaging. Because of the network effect, new IM users gained much more value by choosing to use the Mirabilis system (and join its large network of users) than they would using a competing system. As was typical for that era, the company never made any attempt to generate profits from their dominant position before selling the company.
Examples.
Financial exchanges.
Stock exchanges and derivatives exchanges feature a network effect. Market liquidity is a major determinant of transaction cost in the sale or purchase of a security, as a bid-ask spread exists between the price at which a purchase can be done versus the price at which the sale of the same security can be done. As the number of buyers and sellers on an exchange increases, liquidity increases, and transaction costs decrease. This then attracts a larger number of buyers and sellers to the exchange. See, for example, the work of Steve Wunsch (1999).
The network advantage of financial exchanges is apparent in the difficulty that startup exchanges have in dislodging a dominant exchange. For example, the Chicago Board of Trade has retained overwhelming dominance of trading in US Treasury bond futures despite the startup of Eurex US trading of identical futures contracts. Similarly, the Chicago Mercantile Exchange has maintained a dominance in trading of Eurobond interest rate futures despite a challenge from Euronext.Liffe.
Software.
There are very strong network effects operating in the market for widely used computer software.
Take, for example, Microsoft Office. For many people choosing an office suite, prime considerations include how valuable having learned that office suite will prove to potential employers, and how well the software interoperates with other users. That is, since learning to use an office suite takes many hours, they want to invest that time learning the office suite that will make them most attractive to potential employers and clients, and they also want to be able to share documents. (Additionally, an example of an indirect network effect in this case is the notable similarity in user-interfaces and operability menus of most new software – since that similarity directly translates into less time spent learning new environments, therefore potentially greater acceptance and adoption of those products.)
Similarly, finding already-trained employees is a big concern for employers when deciding which office suite to purchase or standardize on. The lack of cross-platform user-interface standards results in a situation in which one firm is in control of almost 100% of the market.
Microsoft Windows is a further example of network effect. The most-vaunted advantage of Windows, and that most publicised by Microsoft, is that Windows is compatible with the widest range of computer hardware and software. Although this claim is justified, it is in reality the result of network effect: hardware and software manufacturers ensure that their products are compatible with Windows in order to have access to the large market of Windows users. Thus, Windows is popular because it is well supported, but is well supported because it is popular.
However, network effects need not lead to market dominance by one firm, when there are standards which allow multiple firms to interoperate, thus allowing the network externalities to benefit the entire market. This is true for the case of x86-based personal computer hardware, in which there are extremely strong market pressures to interoperate with pre-existing standards, but in which no one firm dominates in the market. Also, it is true for the development of enterprise software applications where the Internet (HTTP), databases (SQL), and to a moderate degree, service-oriented message buses (SOA) have become common interfaces. Further up the development chain there are network effects as well in language back-end base platforms (JVM, CLR, LLVM), programming models (FP, OOP) and languages themselves.
In 2007 Apple released the iPhone followed by the app store. Most iPhone apps rely heavily on the existence of strong network effects. This enables the software to grow in popularity very quickly and spread to a large userbase with very limited marketing needed. The Free-mium business model has evolved to take advantage of these network effects by releasing a free version that will not limit the adoption or any users and then charge for "premium" features as the primary source of revenue.
Telecommunications.
The same holds true for the market for long-distance telephone service within the United States. In fact, the existence of these types of networks discourages dominance of the market by one company, as it creates pressures which work against one company attempting to establish a proprietary protocol or to even distinguish itself by means of product differentiation.
Web sites.
Many web sites also feature a network effect. One example is web marketplaces and exchanges, in that the value of the marketplace to a new user is proportional to the number of other users in the market. For example, eBay would not be a particularly useful site if auctions were not competitive. However, as the number of users grows on eBay, auctions grow more competitive, pushing up the prices of bids on items. This makes it more worthwhile to sell on eBay and brings more sellers onto eBay, which drives prices down again as this increases supply, while bringing more people onto eBay because there are more things being sold that people want. Essentially, as the number of users of eBay grows, prices fall and supply increases, and more and more people find the site to be useful.
Social networking websites are also good examples. The more people register onto a social networking website, the more useful the website is to its registrants.
By contrast, the value of a news site is primarily proportional to the quality of the articles, not to the number of other people using the site. Similarly, the first generation of search sites experienced little network effect, as the value of the site was based on the value of the search results. This allowed Google to win users away from Yahoo! without much trouble, once users believed that Google's search results were superior. Some commentators mistook the value of the Yahoo! brand (which does increase as more people know of it) for a network effect protecting its advertising business.
Alexa Internet uses a technology that tracks users' surfing patterns; thus Alexa's Related Sites results improve as more users use the technology. Alexa's network relies heavily on a small number of browser software relationships, which makes the network more vulnerable to competition.
Google has also attempted to create a network effect in its advertising business with its Google AdSense service. Google AdSense places ads on many small sites, such as blogs, using Google technology to determine which ads are relevant to which blogs. Thus, the service appears to aim to serve as an exchange (or ad network) for matching many advertisers with many small sites (such as blogs). In general, the more blogs Google AdSense can reach, the more advertisers it will attract, making it the most attractive option for more blogs, and so on, making the network more valuable for all participants.
Network effects were used as justification for some of the dot-com business models in the late 1990s. These firms operated under the belief that when a new market comes into being which contains strong network effects, firms should care more about growing their market share than about becoming profitable. This was believed because market share will determine which firm can set technical and marketing standards and thus determine the basis of future competition.
Rail gauge.
There are strong network effects in the initial choice of rail gauge, and in gauge conversion decisions.
Even when placing isolated rails not connected to any other lines, track layers usually choose a standard rail gauge so they can use off-the-shelf rolling stock.
Although a few manufacturers make rolling stock that can adjust to different rail gauges, most manufacturers make rolling stock that only works with one of the standard rail gauges.

</doc>
<doc id="22054" url="http://en.wikipedia.org/wiki?curid=22054" title="Nuclear fission">
Nuclear fission

In nuclear physics and nuclear chemistry, nuclear fission is either a nuclear reaction or a radioactive decay process in which the nucleus of an atom splits into smaller parts (lighter nuclei). The fission process often produces free neutrons and photons (in the form of gamma rays), and releases a very large amount of energy even by the energetic standards of radioactive decay.
Nuclear fission of heavy elements was discovered on December 17, 1938 by German Otto Hahn and his assistant Fritz Strassmann, and explained theoretically in January 1939 by Lise Meitner and her nephew Otto Robert Frisch. Frisch named the process by analogy with biological fission of living cells. It is an exothermic reaction which can release large amounts of energy both as electromagnetic radiation and as kinetic energy of the fragments (heating the bulk material where fission takes place). In order for fission to produce energy, the total binding energy of the resulting elements must be less negative (higher energy) than that of the starting element.
Fission is a form of nuclear transmutation because the resulting fragments are not the same element as the original atom. The two nuclei produced are most often of comparable but slightly different sizes, typically with a mass ratio of products of about 3 to 2, for common fissile isotopes. Most fissions are binary fissions (producing two charged fragments), but occasionally (2 to 4 times per 1000 events), "three" positively charged fragments are produced, in a ternary fission. The smallest of these fragments in ternary processes ranges in size from a proton to an argon nucleus.
Apart from fission induced by a neutron, harnessed and exploited by humans, a natural form of spontaneous radioactive decay (not requiring a neutron) is also referred to as fission, and occurs especially in very high-mass-number isotopes. Spontaneous fission was discovered in 1940 by Flyorov, Petrzhak and Kurchatov in Moscow, when they decided to confirm that, without bombardment by neutrons, the fission rate of uranium was indeed negligible, as predicted by Niels Bohr; it wasn't.
The unpredictable composition of the products (which vary in a broad probabilistic and somewhat chaotic manner) distinguishes fission from purely quantum-tunnelling processes such as proton emission, alpha decay and cluster decay, which give the same products each time. Nuclear fission produces energy for nuclear power and drives the explosion of nuclear weapons. Both uses are possible because certain substances called nuclear fuels undergo fission when struck by fission neutrons, and in turn emit neutrons when they break apart. This makes possible a self-sustaining nuclear chain reaction that releases energy at a controlled rate in a nuclear reactor or at a very rapid uncontrolled rate in a nuclear weapon.
The amount of free energy contained in nuclear fuel is millions of times the amount of free energy contained in a similar mass of chemical fuel such as gasoline, making nuclear fission a very dense source of energy. The products of nuclear fission, however, are on average far more radioactive than the heavy elements which are normally fissioned as fuel, and remain so for significant amounts of time, giving rise to a nuclear waste problem. Concerns over nuclear waste accumulation and over the destructive potential of nuclear weapons may counterbalance the desirable qualities of fission as an energy source, and give rise to ongoing political debate over nuclear power.
Physical overview.
Mechanism.
Nuclear fission can occur without neutron bombardment as a type of radioactive decay. This type of fission (called spontaneous fission) is rare except in a few heavy isotopes. In engineered nuclear devices, essentially all nuclear fission occurs as a "nuclear reaction" — a bombardment-driven process that results from the collision of two subatomic particles. In nuclear reactions, a subatomic particle collides with an atomic nucleus and causes changes to it. Nuclear reactions are thus driven by the mechanics of bombardment, not by the relatively constant exponential decay and half-life characteristic of spontaneous radioactive processes.
Many types of nuclear reactions are currently known. Nuclear fission differs importantly from other types of nuclear reactions, in that it can be amplified and sometimes controlled via a nuclear chain reaction (one type of general chain reaction). In such a reaction, free neutrons released by each fission event can trigger yet more events, which in turn release more neutrons and cause more fissions.
The chemical element isotopes that can sustain a fission chain reaction are called nuclear fuels, and are said to be "fissile". The most common nuclear fuels are 235U (the isotope of uranium with an atomic mass of 235 and of use in nuclear reactors) and 239Pu (the isotope of plutonium with an atomic mass of 239). These fuels break apart into a bimodal range of chemical elements with atomic masses centering near 95 and 135 u (fission products). Most nuclear fuels undergo spontaneous fission only very slowly, decaying instead mainly via an alpha/beta decay chain over periods of millennia to eons. In a nuclear reactor or nuclear weapon, the overwhelming majority of fission events are induced by bombardment with another particle, a neutron, which is itself produced by prior fission events.
Nuclear fissions in fissile fuels are the result of the nuclear excitation energy produced when a fissile nucleus captures a neutron. This energy, resulting from the neutron capture, is a result of the attractive nuclear force acting between the neutron and nucleus. It is enough to deform the nucleus into a double-lobed "drop," to the point that nuclear fragments exceed the distances at which the nuclear force can hold two groups of charged nucleons together, and when this happens, the two fragments complete their separation and then are driven further apart by their mutually repulsive charges, in a process which becomes irreversible with greater and greater distance. A similar process occurs in fissionable isotopes (such as uranium-238), but in order to fission, these isotopes require additional energy provided by fast neutrons (such as those produced by nuclear fusion in thermonuclear weapons).
The liquid drop model of the atomic nucleus predicts equal-sized fission products as an outcome of nuclear deformation. The more sophisticated nuclear shell model is needed to mechanistically explain the route to the more energetically favorable outcome, in which one fission product is slightly smaller than the other. A theory of the fission based on shell model has been formulated by Maria Goeppert Mayer.
The most common fission process is binary fission, and it produces the fission products noted above, at 95±15 and 135±15 u. However, the binary process happens merely because it is the most probable. In anywhere from 2 to 4 fissions per 1000 in a nuclear reactor, a process called ternary fission produces three positively charged fragments (plus neutrons) and the smallest of these may range from so small a charge and mass as a proton (Z=1), to as large a fragment as argon (Z=18). The most common small fragments, however, are composed of 90% helium-4 nuclei with more energy than alpha particles from alpha decay (so-called "long range alphas" at ~ 16 MeV), plus helium-6 nuclei, and tritons (the nuclei of tritium). The ternary process is less common, but still ends up producing significant helium-4 and tritium gas buildup in the fuel rods of modern nuclear reactors.
Energetics.
Input.
The fission of a heavy nucleus requires a total input energy of about 7 to 8 million electron volts (MeV) to initially overcome the nuclear force which holds the nucleus into a spherical or nearly spherical shape, and from there, deform it into a two-lobed ("peanut") shape in which the lobes are able to continue to separate from each other, pushed by their mutual positive charge, in the most common process of binary fission (two positively charged fission products + neutrons). Once the nuclear lobes have been pushed to a critical distance, beyond which the short range strong force can no longer hold them together, the process of their separation proceeds from the energy of the (longer range) electromagnetic repulsion between the fragments. The result is two fission fragments moving away from each other, at high energy.
About 6 MeV of the fission-input energy is supplied by the simple binding of an extra neutron to the heavy nucleus via the strong force; however, in many fissionable isotopes, this amount of energy is not enough for fission. Uranium-238, for example, has a near-zero fission cross section for neutrons of less than one MeV energy. If no additional energy is supplied by any other mechanism, the nucleus will not fission, but will merely absorb the neutron, as happens when U-238 absorbs slow and even some fraction of fast neutrons, to become U-239. The remaining energy to initiate fission can be supplied by two other mechanisms: one of these is more kinetic energy of the incoming neutron, which is increasingly able to fission a fissionable heavy nucleus as it exceeds a kinetic energy of one MeV or more (so-called fast neutrons). Such high energy neutrons are able to fission U-238 directly (see thermonuclear weapon for application, where the fast neutrons are supplied by nuclear fusion). However, this process cannot happen to a great extent in a nuclear reactor, as too small a fraction of the fission neutrons produced by any type of fission have enough energy to efficiently fission U-238 (fission neutrons have a mode energy of 2 MeV, but a median of only 0.75 MeV, meaning half of them have less than this insufficient energy).
Among the heavy actinide elements, however, those isotopes that have an odd number of neutrons (such as U-235 with 143 neutrons) bind an extra neutron with an additional 1 to 2 MeV of energy over an isotope of the same element with an even number of neutrons (such as U-238 with 146 neutrons). This extra binding energy is made available as a result of the mechanism of neutron pairing effects. This extra energy results from the Pauli exclusion principle allowing an extra neutron to occupy the same nuclear orbital as the last neutron in the nucleus, so that the two form a pair. In such isotopes, therefore, no neutron kinetic energy is needed, for all the necessary energy is supplied by absorption of any neutron, either of the slow or fast variety (the former are used in moderated nuclear reactors, and the latter are used in fast neutron reactors, and in weapons). As noted above, the subgroup of fissionable elements that may be fissioned efficiently with their own fission neutrons (thus potentially causing a nuclear chain reaction in relatively small amounts of the pure material) are termed "fissile." Examples of fissile isotopes are U-235 and plutonium-239.
Output.
Typical fission events release about two hundred million eV (200 MeV) of energy for each fission event. The exact isotope which is fissioned, and whether or not it is fissionable or fissile, has only a small impact on the amount of energy released. This can be easily seen by examining the curve of binding energy (image below), and noting that the average binding energy of the actinide nuclides beginning with uranium is around 7.6 MeV per nucleon. Looking further left on the curve of binding energy, where the fission products cluster, it is easily observed that the binding energy of the fission products tends to center around 8.5 MeV per nucleon. Thus, in any fission event of an isotope in the actinide's range of mass, roughly 0.9 MeV is released per nucleon of the starting element. The fission of U235 by a slow neutron yields nearly identical energy to the fission of U238 by a fast neutron. This energy release profile holds true for thorium and the various minor actinides as well.
By contrast, most chemical oxidation reactions (such as burning coal or TNT) release at most a few eV per event. So, nuclear fuel contains at least ten million times more usable energy per unit mass than does chemical fuel. The energy of nuclear fission is released as kinetic energy of the fission products and fragments, and as electromagnetic radiation in the form of gamma rays; in a nuclear reactor, the energy is converted to heat as the particles and gamma rays collide with the atoms that make up the reactor and its working fluid, usually water or occasionally heavy water or molten salts.
When a uranium nucleus fissions into two daughter nuclei fragments, about 0.1 percent of the mass of the uranium nucleus appears as the fission energy of ~200 MeV. For uranium-235 (total mean fission energy 202.5 MeV), typically ~169 MeV appears as the kinetic energy of the daughter nuclei, which fly apart at about 3% of the speed of light, due to Coulomb repulsion. Also, an average of 2.5 neutrons are emitted, with a mean kinetic energy per neutron of ~2 MeV (total of 4.8 MeV). The fission reaction also releases ~7 MeV in prompt gamma ray photons. The latter figure means that a nuclear fission explosion or criticality accident emits about 3.5% of its energy as gamma rays, less than 2.5% of its energy as fast neutrons (total of both types of radiation ~ 6%), and the rest as kinetic energy of fission fragments (this appears almost immediately when the fragments impact surrounding matter, as simple heat). In an atomic bomb, this heat may serve to raise the temperature of the bomb core to 100 million kelvin and cause secondary emission of soft X-rays, which convert some of this energy to ionizing radiation. However, in nuclear reactors, the fission fragment kinetic energy remains as low-temperature heat, which itself causes little or no ionization.
So-called neutron bombs (enhanced radiation weapons) have been constructed which release a larger fraction of their energy as ionizing radiation (specifically, neutrons), but these are all thermonuclear devices which rely on the nuclear fusion stage to produce the extra radiation. The energy dynamics of pure fission bombs always remain at about 6% yield of the total in radiation, as a prompt result of fission.
The total "prompt fission" energy amounts to about 181 MeV, or ~ 89% of the total energy which is eventually released by fission over time. The remaining ~ 11% is released in beta decays which have various half-lives, but begin as a process in the fission products immediately; and in delayed gamma emissions associated with these beta decays. For example, in uranium-235 this delayed energy is divided into about 6.5 MeV in betas, 8.8 MeV in antineutrinos (released at the same time as the betas), and finally, an additional 6.3 MeV in delayed gamma emission from the excited beta-decay products (for a mean total of ~10 gamma ray emissions per fission, in all). Thus, about 6.5% of the total energy of fission is released some time after the event, as non-prompt or delayed ionizing radiation, and the delayed ionizing energy is about evenly divided between gamma and beta ray energy.
In a reactor that has been operating for some time, the radioactive fission products will have built up to steady state concentrations such that their rate of decay is equal to their rate of formation, so that their fractional total contribution to reactor heat (via beta decay) is the same as these radioisotopic fractional contributions to the energy of fission. Under these conditions, the 6.5% of fission which appears as delayed ionizing radiation (delayed gammas and betas from radioactive fission products) contributes to the steady-state reactor heat production under power. It is this output fraction which remains when the reactor is suddenly shut down (undergoes scram). For this reason, the reactor decay heat output begins at 6.5% of the full reactor steady state fission power, once the reactor is shut down. However, within hours, due to decay of these isotopes, the decay power output is far less. See decay heat for detail.
The remainder of the delayed energy (8.8 MeV/202.5 MeV = 4.3% of total fission energy) is emitted as antineutrinos, which as a practical matter, are not considered "ionizing radiation." The reason is that energy released as antineutrinos is not captured by the reactor material as heat, and escapes directly through all materials (including the Earth) at nearly the speed of light, and into interplanetary space (the amount absorbed is minuscule). Neutrino radiation is ordinarily not classed as ionizing radiation, because it is almost entirely not absorbed and therefore does not produce effects (although the very rare neutrino event is ionizing). Almost all of the rest of the radiation (6.5% delayed beta and gamma radiation) is eventually converted to heat in a reactor core or its shielding.
Some processes involving neutrons are notable for absorbing or finally yielding energy — for example neutron kinetic energy does not yield heat immediately if the neutron is captured by a uranium-238 atom to breed plutonium-239, but this energy is emitted if the plutonium-239 is later fissioned. On the other hand, so-called delayed neutrons emitted as radioactive decay products with half-lives up to several minutes, from fission-daughters, are very important to reactor control, because they give a characteristic "reaction" time for the total nuclear reaction to double in size, if the reaction is run in a "delayed-critical" zone which deliberately relies on these neutrons for a supercritical chain-reaction (one in which each fission cycle yields more neutrons than it absorbs). Without their existence, the nuclear chain-reaction would be prompt critical and increase in size faster than it could be controlled by human intervention. In this case, the first experimental atomic reactors would have run away to a dangerous and messy "prompt critical reaction" before their operators could have manually shut them down (for this reason, designer Enrico Fermi included radiation-counter-triggered control rods, suspended by electromagnets, which could automatically drop into the center of Chicago Pile-1). If these delayed neutrons are captured without producing fissions, they produce heat as well.
Product nuclei and binding energy.
In fission there is a preference to yield fragments with even proton numbers, which is called the odd-even effect on the fragments charge distribution. However, no odd-even effect is observed on fragment mass number distribution. This result is attributed to nucleon pair breaking.
In nuclear fission events the nuclei may break into any combination of lighter nuclei, but the most common event is not fission to equal mass nuclei of about mass 120; the most common event (depending on isotope and process) is a slightly unequal fission in which one daughter nucleus has a mass of about 90 to 100 u and the other the remaining 130 to 140 u. Unequal fissions are energetically more favorable because this allows one product to be closer to the energetic minimum near mass 60 u (only a quarter of the average fissionable mass), while the other nucleus with mass 135 u is still not far out of the range of the most tightly bound nuclei (another statement of this, is that the atomic binding energy curve is slightly steeper to the left of mass 120 u than to the right of it).
Origin of the active energy and the curve of binding energy.
Nuclear fission of heavy elements produces energy because the specific binding energy (binding energy per mass) of intermediate-mass nuclei with atomic numbers and atomic masses close to 62Ni and 56Fe is greater than the nucleon-specific binding energy of very heavy nuclei, so that energy is released when heavy nuclei are broken apart. The total rest masses of the fission products (Mp) from a single reaction is less than the mass of the original fuel nucleus (M). The excess mass Δm = M – Mp is the invariant mass of the energy that is released as photons (gamma rays) and kinetic energy of the fission fragments, according to the mass-energy equivalence formula "E" = "mc"2.
The variation in specific binding energy with atomic number is due to the interplay of the two fundamental forces acting on the component nucleons (protons and neutrons) that make up the nucleus. Nuclei are bound by an attractive nuclear force between nucleons, which overcomes the electrostatic repulsion between protons. However, the nuclear force acts only over relatively short ranges (a few nucleon diameters), since it follows an exponentially decaying Yukawa potential which makes it insignificant at longer distances. The electrostatic repulsion is of longer range, since it decays by an inverse-square rule, so that nuclei larger than about 12 nucleons in diameter reach a point that the total electrostatic repulsion overcomes the nuclear force and causes them to be spontaneously unstable. For the same reason, larger nuclei (more than about eight nucleons in diameter) are less tightly bound per unit mass than are smaller nuclei; breaking a large nucleus into two or more intermediate-sized nuclei releases energy. The origin of this energy is the nuclear force, which intermediate-sized nuclei allows to act more efficiently, because each nucleon has more neighbors which are within the short range attraction of this force. Thus less energy is needed in the smaller nuclei and the difference to the state before is set free.
Also because of the short range of the strong binding force, large stable nuclei must contain proportionally more neutrons than do the lightest elements, which are most stable with a 1 to 1 ratio of protons and neutrons. Nuclei which have more than 20 protons cannot be stable unless they have more than an equal number of neutrons. Extra neutrons stabilize heavy elements because they add to strong-force binding (which acts between all nucleons) without adding to proton–proton repulsion. Fission products have, on average, about the same ratio of neutrons and protons as their parent nucleus, and are therefore usually unstable to beta decay (which changes neutrons to protons) because they have proportionally too many neutrons compared to stable isotopes of similar mass.
This tendency for fission product nuclei to beta-decay is the fundamental cause of the problem of radioactive high level waste from nuclear reactors. Fission products tend to be beta emitters, emitting fast-moving electrons to conserve electric charge, as excess neutrons convert to protons in the fission-product atoms. See Fission products (by element) for a description of fission products sorted by element.
Chain reactions.
Several heavy elements, such as uranium, thorium, and plutonium, undergo both spontaneous fission, a form of radioactive decay and "induced fission", a form of nuclear reaction. Elemental isotopes that undergo induced fission when struck by a free neutron are called fissionable; isotopes that undergo fission when struck by a thermal, slow moving neutron are also called fissile. A few particularly fissile and readily obtainable isotopes (notably 233U, 235U and 239Pu) are called nuclear fuels because they can sustain a chain reaction and can be obtained in large enough quantities to be useful.
All fissionable and fissile isotopes undergo a small amount of spontaneous fission which releases a few free neutrons into any sample of nuclear fuel. Such neutrons would escape rapidly from the fuel and become a free neutron, with a mean lifetime of about 15 minutes before decaying to protons and beta particles. However, neutrons almost invariably impact and are absorbed by other nuclei in the vicinity long before this happens (newly created fission neutrons move at about 7% of the speed of light, and even moderated neutrons move at about 8 times the speed of sound). Some neutrons will impact fuel nuclei and induce further fissions, releasing yet more neutrons. If enough nuclear fuel is assembled in one place, or if the escaping neutrons are sufficiently contained, then these freshly emitted neutrons outnumber the neutrons that escape from the assembly, and a "sustained nuclear chain reaction" will take place.
An assembly that supports a sustained nuclear chain reaction is called a critical assembly or, if the assembly is almost entirely made of a nuclear fuel, a critical mass. The word "critical" refers to a cusp in the behavior of the differential equation that governs the number of free neutrons present in the fuel: if less than a critical mass is present, then the amount of neutrons is determined by radioactive decay, but if a critical mass or more is present, then the amount of neutrons is controlled instead by the physics of the chain reaction. The actual mass of a "critical mass" of nuclear fuel depends strongly on the geometry and surrounding materials.
Not all fissionable isotopes can sustain a chain reaction. For example, 238U, the most abundant form of uranium, is fissionable but not fissile: it undergoes induced fission when impacted by an energetic neutron with over 1 MeV of kinetic energy. However, too few of the neutrons produced by 238U fission are energetic enough to induce further fissions in 238U, so no chain reaction is possible with this isotope. Instead, bombarding 238U with slow neutrons causes it to absorb them (becoming 239U) and decay by beta emission to 239Np which then decays again by the same process to 239Pu; that process is used to manufacture 239Pu in breeder reactors. In-situ plutonium production also contributes to the neutron chain reaction in other types of reactors after sufficient plutonium-239 has been produced, since plutonium-239 is also a fissile element which serves as fuel. It is estimated that up to half of the power produced by a standard "non-breeder" reactor is produced by the fission of plutonium-239 produced in place, over the total life-cycle of a fuel load.
Fissionable, non-fissile isotopes can be used as fission energy source even without a chain reaction. Bombarding 238U with fast neutrons induces fissions, releasing energy as long as the external neutron source is present. This is an important effect in all reactors where fast neutrons from the fissile isotope can cause the fission of nearby 238U nuclei, which means that some small part of the 238U is "burned-up" in all nuclear fuels, especially in fast breeder reactors that operate with higher-energy neutrons. That same fast-fission effect is used to augment the energy released by modern thermonuclear weapons, by jacketing the weapon with 238U to react with neutrons released by nuclear fusion at the center of the device. But the explosive effects of nuclear fission chain reactions can be reduced by using substances like moderators which slow down the speed of secondary neutrons.
Fission reactors.
Critical fission reactors are the most common type of nuclear reactor. In a critical fission reactor, neutrons produced by fission of fuel atoms are used to induce yet more fissions, to sustain a controllable amount of energy release. Devices that produce engineered but non-self-sustaining fission reactions are subcritical fission reactors. Such devices use radioactive decay or particle accelerators to trigger fissions.
Critical fission reactors are built for three primary purposes, which typically involve different engineering trade-offs to take advantage of either the heat or the neutrons produced by the fission chain reaction:
While, in principle, all fission reactors can act in all three capacities, in practice the tasks lead to conflicting engineering goals and most reactors have been built with only one of the above tasks in mind. (There are several early counter-examples, such as the Hanford N reactor, now decommissioned). Power reactors generally convert the kinetic energy of fission products into heat, which is used to heat a working fluid and drive a heat engine that generates mechanical or electrical power. The working fluid is usually water with a steam turbine, but some designs use other materials such as gaseous helium. Research reactors produce neutrons that are used in various ways, with the heat of fission being treated as an unavoidable waste product. Breeder reactors are a specialized form of research reactor, with the caveat that the sample being irradiated is usually the fuel itself, a mixture of 238U and 235U.
For a more detailed description of the physics and operating principles of critical fission reactors, see nuclear reactor physics. For a description of their social, political, and environmental aspects, see nuclear power.
Fission bombs.
One class of nuclear weapon, a "fission bomb" (not to be confused with the "fusion bomb"), otherwise known as an "atomic bomb" or "atom bomb", is a fission reactor designed to liberate as much energy as possible as rapidly as possible, before the released energy causes the reactor to explode (and the chain reaction to stop). Development of nuclear weapons was the motivation behind early research into nuclear fission: the Manhattan Project of the U.S. military during World War II carried out most of the early scientific work on fission chain reactions, culminating in the Trinity test bomb and the Little Boy and Fat Man bombs that were exploded over the cities Hiroshima, and Nagasaki, Japan in August 1945.
Even the first fission bombs were thousands of times more explosive than a comparable mass of chemical explosive. For example, Little Boy weighed a total of about four tons (of which 60 kg was nuclear fuel) and was 11 ft long; it also yielded an explosion equivalent to about 15 kilotons of TNT, destroying a large part of the city of Hiroshima. Modern nuclear weapons (which include a thermonuclear "fusion" as well as one or more fission stages) are hundreds of times more energetic for their weight than the first pure fission atomic bombs (see nuclear weapon yield), so that a modern single missile warhead bomb weighing less than 1/8 as much as Little Boy (see for example W88) has a yield of 475,000 tons of TNT, and could bring destruction to about 10 times the city area.
While the fundamental physics of the fission chain reaction in a nuclear weapon is similar to the physics of a controlled nuclear reactor, the two types of device must be engineered quite differently (see nuclear reactor physics). A nuclear bomb is designed to release all its energy at once, while a reactor is designed to generate a steady supply of useful power. While overheating of a reactor can lead to, and has led to, meltdown and steam explosions, the much lower uranium enrichment makes it impossible for a nuclear reactor to explode with the same destructive power as a nuclear weapon. It is also difficult to extract useful power from a nuclear bomb, although at least one rocket propulsion system, Project Orion, was intended to work by exploding fission bombs behind a massively padded and shielded spacecraft.
The strategic importance of nuclear weapons is a major reason why the technology of nuclear fission is politically sensitive. Viable fission bomb designs are, arguably, within the capabilities of many being relatively simple from an engineering viewpoint. However, the difficulty of obtaining fissile nuclear material to realize the designs, is the key to the relative unavailability of nuclear weapons to all but modern industrialized governments with special programs to produce fissile materials (see uranium enrichment and nuclear fuel cycle).
History.
Discovery of nuclear fission.
The discovery of nuclear fission occurred in 1938 in the buildings of Kaiser Wilhelm Society for Chemistry, today part of the Free University of Berlin, following nearly five decades of work on the science of radioactivity and the elaboration of new nuclear physics that described the components of atoms. In 1911, Ernest Rutherford proposed a model of the atom in which a very small, dense and positively charged nucleus of protons (the neutron had not yet been discovered) was surrounded by orbiting, negatively charged electrons (the Rutherford model). Niels Bohr improved upon this in 1913 by reconciling the quantum behavior of electrons (the Bohr model). Work by Henri Becquerel, Marie Curie, Pierre Curie, and Rutherford further elaborated that the nucleus, though tightly bound, could undergo different forms of radioactive decay, and thereby transmute into other elements. (For example, by alpha decay: the emission of an alpha particle—two protons and two neutrons bound together into a particle identical to a helium nucleus.)
Some work in nuclear transmutation had been done. In 1917, Rutherford was able to accomplish transmutation of nitrogen into oxygen, using alpha particles directed at nitrogen 14N + α → 17O + p.  This was the first observation of a nuclear reaction, that is, a reaction in which particles from one decay are used to transform another atomic nucleus. Eventually, in 1932, a fully artificial nuclear reaction and nuclear transmutation was achieved by Rutherford's colleagues Ernest Walton and John Cockcroft, who used artificially accelerated protons against lithium-7, to split this nucleus into two alpha particles. The feat was popularly known as "splitting the atom", although it was not the modern nuclear fission reaction later discovered in heavy elements, which is discussed below. Meanwhile, the possibility of "combining" nuclei—nuclear fusion—had been studied in connection with understanding the processes which power stars. The first artificial fusion reaction had been achieved by Mark Oliphant in 1932, using two accelerated deuterium nuclei (each consisting of a single proton bound to a single neutron) to create a helium nucleus.
After English physicist James Chadwick discovered the neutron in 1932, Enrico Fermi and his colleagues in Rome studied the results of bombarding uranium with neutrons in 1934. Fermi concluded that his experiments had created new elements with 93 and 94 protons, which the group dubbed ausonium and hesperium. However, not all were convinced by Fermi's analysis of his results. The German chemist Ida Noddack notably suggested in print in 1934 that instead of creating a new, heavier element 93, that "it is conceivable that the nucleus breaks up into several large fragments." However, Noddack's conclusion was not pursued at the time.
After the Fermi publication, Otto Hahn, Lise Meitner, and Fritz Strassmann began performing similar experiments in Berlin. Meitner, an Austrian Jew, lost her citizenship with the "Anschluss", the occupation and annexation of Austria into Nazi Germany in March 1938, but she fled in July 1938 to Sweden and started a correspondence by mail with Hahn in Berlin. By coincidence, her nephew Otto Robert Frisch, also a refugee, was also in Sweden when Meitner received a letter from Hahn dated 19 December describing his chemical proof that some of the product of the bombardment of uranium with neutrons was barium. Hahn suggested a "bursting" of the nucleus, but he was unsure of what the physical basis for the results were. Barium had an atomic mass 40% less than uranium, and no previously known methods of radioactive decay could account for such a large difference in the mass of the nucleus. Frisch was skeptical, but Meitner trusted Hahn's ability as a chemist. Marie Curie had been separating barium from radium for many years, and the techniques were well-known. According to Frisch:
Was it a mistake? No, said Lise Meitner; Hahn was too good a chemist for that. But how could barium be formed from uranium? No larger fragments than protons or helium nuclei (alpha particles) had ever been chipped away from nuclei, and to chip off a large number not nearly enough energy was available. Nor was it possible that the uranium nucleus could have been cleaved right across. A nucleus was not like a brittle solid that can be cleaved or broken; George Gamow had suggested early on, and Bohr had given good arguments that a nucleus was much more like a liquid drop. Perhaps a drop could divide itself into two smaller drops in a more gradual manner, by first becoming elongated, then constricted, and finally being torn rather than broken in two? We knew that there were strong forces that would resist such a process, just as the surface tension of an ordinary liquid drop tends to resist its division into two smaller ones. But nuclei differed from ordinary drops in one important way: they were electrically charged, and that was known to counteract the surface tension.
The charge of a uranium nucleus, we found, was indeed large enough to overcome the effect of the surface tension almost completely; so the uranium nucleus might indeed resemble a very wobbly unstable drop, ready to divide itself at the slightest provocation, such as the impact of a single neutron. But there was another problem. After separation, the two drops would be driven apart by their mutual electric repulsion and would acquire high speed and hence a very large energy, about 200 MeV in all; where could that energy come from? ...Lise Meitner... worked out that the two nuclei formed by the division of a uranium nucleus together would be lighter than the original uranium nucleus by about one-fifth the mass of a proton. Now whenever mass disappears energy is created, according to Einstein's formula "E" = "mc"2, and one-fifth of a proton mass was just equivalent to 200 MeV. So here was the source for that energy; it all fitted!
In short, Meitner and Frisch had correctly interpreted Hahn's results to mean that the nucleus of uranium had split roughly in half. Frisch suggested the process be named "nuclear fission," by analogy to the process of living cell division into two cells, which was then called binary fission. Just as the term nuclear "chain reaction" would later be borrowed from chemistry, so the term "fission" was borrowed from biology.
On 22 December 1938, Hahn and Strassmann sent a manuscript to "Naturwissenschaften" reporting that they had discovered the element barium after bombarding uranium with neutrons. Simultaneously, they communicated these results to Meitner in Sweden. She and Frisch correctly interpreted the results as evidence of nuclear fission. Frisch confirmed this experimentally on 13 January 1939. For proving that the barium resulting from his bombardment of uranium with neutrons was the product of nuclear fission, Hahn was awarded the Nobel Prize for Chemistry in 1944 (the sole recipient) "for his discovery of the fission of heavy nuclei". (The award was actually given to Hahn in 1945, as "the Nobel Committee for Chemistry decided that none of the year's nominations met the criteria as outlined in the will of Alfred Nobel." In such cases, the Nobel Foundation's statutes permit that year's prize be reserved until the following year.)
News spread quickly of the new discovery, which was correctly seen as an entirely novel physical effect with great scientific—and potentially practical—possibilities. Meitner’s and Frisch’s interpretation of the discovery of Hahn and Strassmann crossed the Atlantic Ocean with Niels Bohr, who was to lecture at Princeton University. I.I. Rabi and Willis Lamb, two Columbia University physicists working at Princeton, heard the news and carried it back to Columbia. Rabi said he told Enrico Fermi; Fermi gave credit to Lamb. Bohr soon thereafter went from Princeton to Columbia to see Fermi. Not finding Fermi in his office, Bohr went down to the cyclotron area and found Herbert L. Anderson. Bohr grabbed him by the shoulder and said: “Young man, let me explain to you about something new and exciting in physics.” It was clear to a number of scientists at Columbia that they should try to detect the energy released in the nuclear fission of uranium from neutron bombardment. On 25 January 1939, a Columbia University team conducted the first nuclear fission experiment in the United States, which was done in the basement of Pupin Hall; the members of the team were Herbert L. Anderson, Eugene T. Booth, John R. Dunning, Enrico Fermi, G. Norris Glasoe, and Francis G. Slack. The experiment involved placing uranium oxide inside of an ionization chamber and irradiating it with neutrons, and measuring the energy thus released. The results confirmed that fission was occurring and hinted strongly that it was the isotope uranium 235 in particular that was fissioning. The next day, the Fifth Washington Conference on Theoretical Physics began in Washington, D.C. under the joint auspices of the George Washington University and the Carnegie Institution of Washington. There, the news on nuclear fission was spread even further, which fostered many more experimental demonstrations.
During this period the Hungarian physicist Leó Szilárd, who was residing in the United States at the time, realized that the neutron-driven fission of heavy atoms could be used to create a nuclear chain reaction. Such a reaction using neutrons was an idea he had first formulated in 1933, upon reading Rutherford's disparaging remarks about generating power from his team's 1932 experiment using protons to split lithium. However, Szilárd had not been able to achieve a neutron-driven chain reaction with neutron-rich light atoms. In theory, if in a neutron-driven chain reaction the number of secondary neutrons produced was greater than one, then each such reaction could trigger multiple additional reactions, producing an exponentially increasing number of reactions. It was thus a possibility that the fission of uranium could yield vast amounts of energy for civilian or military purposes (i.e., electric power generation or atomic bombs).
Szilard now urged Fermi (in New York) and Frédéric Joliot-Curie (in Paris) to refrain from publishing on the possibility of a chain reaction, lest the Nazi government become aware of the possibilities on the eve of what would later be known as World War II. With some hesitation Fermi agreed to self-censor. But Joliot-Curie did not, and in April 1939 his team in Paris, including Hans von Halban and Lew Kowarski, reported in the journal "Nature" that the number of neutrons emitted with nuclear fission of 235U was then reported at 3.5 per fission. (They later corrected this to 2.6 per fission.) Simultaneous work by Szilard and Walter Zinn confirmed these results. The results suggested the possibility of building nuclear reactors (first called "neutronic reactors" by Szilard and Fermi) and even nuclear bombs. However, much was still unknown about fission and chain reaction systems.
Fission chain reaction realized.
"Chain reactions" at that time were a known phenomenon in "chemistry", but the analogous process in nuclear physics, using neutrons, had been foreseen as early as 1933 by Szilárd, although Szilárd at that time had no idea with what materials the process might be initiated. Szilárd considered that neutrons would be ideal for such a situation, since they lacked an electrostatic charge.
With the news of fission neutrons from uranium fission, Szilárd immediately understood the possibility of a nuclear chain reaction using uranium. In the summer, Fermi and Szilard proposed the idea of a nuclear reactor (pile) to mediate this process. The pile would use natural uranium as fuel. Fermi had shown much earlier that neutrons were far more effectively captured by atoms if they were of low energy (so-called "slow" or "thermal" neutrons), because for quantum reasons it made the atoms look like much larger targets to the neutrons. Thus to slow down the secondary neutrons released by the fissioning uranium nuclei, Fermi and Szilard proposed a graphite "moderator," against which the fast, high-energy secondary neutrons would collide, effectively slowing them down. With enough uranium, and with pure-enough graphite, their "pile" could theoretically sustain a slow-neutron chain reaction. This would result in the production of heat, as well as the creation of radioactive fission products.
In August 1939, Szilard and fellow Hungarian refugees physicists Teller and Wigner thought that the Germans might make use of the fission chain reaction and were spurred to attempt to attract the attention of the United States government to the issue. Towards this, they persuaded German-Jewish refugee Albert Einstein to lend his name to a letter directed to President Franklin Roosevelt. The Einstein–Szilárd letter suggested the possibility of a uranium bomb deliverable by ship, which would destroy "an entire harbor and much of the surrounding countryside." The President received the letter on 11 October 1939 — shortly after World War II began in Europe, but two years before U.S. entry into it. Roosevelt ordered that a scientific committee be authorized for overseeing uranium work and allocated a small sum of money for pile research.
In England, James Chadwick proposed an atomic bomb utilizing natural uranium, based on a paper by Rudolf Peierls with the mass needed for critical state being 30–40 tons. In America, J. Robert Oppenheimer thought that a cube of uranium deuteride 10 cm on a side (about 11 kg of uranium) might "blow itself to hell." In this design it was still thought that a moderator would need to be used for nuclear bomb fission (this turned out not to be the case if the fissile isotope was separated). In December, Werner Heisenberg delivered a report to the German Ministry of War on the possibility of a uranium bomb. Most of these models were still under the assumption that the bombs would be powered by slow neutron reactions—and thus be similar to a reactor undergoing a meltdown.
In Birmingham, England, Frisch teamed up with Peierls, a fellow German-Jewish refugee. They had the idea of using a purified mass of the uranium isotope 235U, which had a cross section just determined, and which was much larger than that of 238U or natural uranium (which is 99.3% the latter isotope). Assuming that the cross section for fast-neutron fission of 235U was the same as for slow neutron fission, they determined that a pure 235U bomb could have a critical mass of only 6 kg instead of tons, and that the resulting explosion would be tremendous. (The amount actually turned out to be 15 kg, although several times this amount was used in the actual uranium (Little Boy) bomb). In February 1940 they delivered the Frisch–Peierls memorandum. Ironically, they were still officially considered "enemy aliens" at the time. Glenn Seaborg, Joseph W. Kennedy, Arthur Wahl and Italian-Jewish refugee Emilio Segrè shortly thereafter discovered 239Pu in the decay products of 239U produced by bombarding 238U with neutrons, and determined it to be a fissile material, like 235U.
The possibility of isolating uranium-235 was technically daunting, because uranium-235 and uranium-238 are chemically identical, and vary in their mass by only the weight of three neutrons. However, if a sufficient quantity of uranium-235 could be isolated, it would allow for a fast neutron fission chain reaction. This would be extremely explosive, a true "atomic bomb." The discovery that plutonium-239 could be produced in a nuclear reactor pointed towards another approach to a fast neutron fission bomb. Both approaches were extremely novel and not yet well understood, and there was considerable scientific skepticism at the idea that they could be developed in a short amount of time.
On June 28, 1941, the Office of Scientific Research and Development was formed in the U.S. to mobilize scientific resources and apply the results of research to national defense. In September, Fermi assembled his first nuclear "pile" or reactor, in an attempt to create a slow neutron-induced chain reaction in uranium, but the experiment failed to achieve criticality, due to lack of proper materials, or not enough of the proper materials which were available.
Producing a fission chain reaction in natural uranium fuel was found to be far from trivial. Early nuclear reactors did not use isotopically enriched uranium, and in consequence they were required to use large quantities of highly purified graphite as neutron moderation materials. Use of ordinary water (as opposed to heavy water) in nuclear reactors requires enriched fuel — the partial separation and relative enrichment of the rare 235U isotope from the far more common 238U isotope. Typically, reactors also require inclusion of extremely chemically pure neutron moderator materials such as deuterium (in heavy water), helium, beryllium, or carbon, the latter usually as graphite. (The high purity for carbon is required because many chemical impurities such as the boron-10 component of natural boron, are very strong neutron absorbers and thus poison the chain reaction and end it prematurely.)
Production of such materials at industrial scale had to be solved for nuclear power generation and weapons production to be accomplished. Up to 1940, the total amount of uranium metal produced in the USA was not more than a few grams, and even this was of doubtful purity; of metallic beryllium not more than a few kilograms; and concentrated deuterium oxide (heavy water) not more than a few kilograms. Finally, carbon had never been produced in quantity with anything like the purity required of a moderator.
The problem of producing large amounts of high purity uranium was solved by Frank Spedding using the thermite or "Ames" process. Ames Laboratory was established in 1942 to produce the large amounts of natural (unenriched) uranium metal that would be necessary for the research to come. The critical nuclear chain-reaction success of the Chicago Pile-1 (December 2, 1942) which used unenriched (natural) uranium, like all of the atomic "piles" which produced the plutonium for the atomic bomb, was also due specifically to Szilard's realization that very pure graphite could be used for the moderator of even natural uranium "piles". In wartime Germany, failure to appreciate the qualities of very pure graphite led to reactor designs dependent on heavy water, which in turn was denied the Germans by Allied attacks in Norway, where heavy water was produced. These difficulties—among many others— prevented the Nazis from building a nuclear reactor capable of criticality during the war, although they never put as much effort as the United States into nuclear research, focusing on other technologies (see German nuclear energy project for more details).
Manhattan Project and beyond.
In the United States, an all-out effort for making atomic weapons was begun in late 1942. This work was taken over by the U.S. Army Corps of Engineers in 1943, and known as the Manhattan Engineer District. The top-secret Manhattan Project, as it was colloquially known, was led by General Leslie R. Groves. Among the project's dozens of sites were: Hanford Site in Washington state, which had the first industrial-scale nuclear reactors; Oak Ridge, Tennessee, which was primarily concerned with uranium enrichment; and Los Alamos, in New Mexico, which was the scientific hub for research on bomb development and design. Other sites, notably the Berkeley Radiation Laboratory and the Metallurgical Laboratory at the University of Chicago, played important contributing roles. Overall scientific direction of the project was managed by the physicist J. Robert Oppenheimer.
In July 1945, the first atomic bomb, dubbed "Trinity", was detonated in the New Mexico desert. It was fueled by plutonium created at Hanford. In August 1945, two more atomic bombs—"Little Boy", a uranium-235 bomb, and "Fat Man", a plutonium bomb—were used against the Japanese cities of Hiroshima and Nagasaki.
In the years after World War II, many countries were involved in the further development of nuclear fission for the purposes of nuclear reactors and nuclear weapons. The UK opened the first commercial nuclear power plant in 1956. In 2013, there are 437 reactors in 31 countries.
Natural fission chain-reactors on Earth.
Criticality in nature is uncommon. At three ore deposits at Oklo in Gabon, sixteen sites (the so-called Oklo Fossil Reactors) have been discovered at which self-sustaining nuclear fission took place approximately 2 billion years ago. Unknown until 1972 (but postulated by Paul Kuroda in 1956), when French physicist Francis Perrin discovered the Oklo Fossil Reactors, it was realized that nature had beaten humans to the punch. Large-scale natural uranium fission chain reactions, moderated by normal water, had occurred far in the past and would not be possible now. This ancient process was able to use normal water as a moderator only because 2 billion years before the present, natural uranium was richer in the shorter-lived fissile isotope 235U (about 3%), than natural uranium available today (which is only 0.7%, and must be enriched to 3% to be usable in light-water reactors).

</doc>
<doc id="22055" url="http://en.wikipedia.org/wiki?curid=22055" title="Neil Gaiman">
Neil Gaiman

Neil Richard MacKinnon Gaiman (; born Neil Richard Gaiman; 10 November 1960) is an English author of short fiction, novels, comic books, graphic novels, audio theatre and films. His notable works include the comic book series "The Sandman" and novels "Stardust", "American Gods", "Coraline", and "The Graveyard Book". He has won numerous awards, including the Hugo, Nebula, and Bram Stoker awards, as well as the Newbery and Carnegie medals. He is the first author to win both the Newbery and the Carnegie medals for the same work, "The Graveyard Book" (2008). In 2013, "The Ocean at the End of the Lane" was voted Book of the Year in the British National Book Awards.
Early life.
Gaiman's family is of Polish and other Eastern European-Jewish origins; his great-grandfather emigrated to the UK from Antwerp, Belgium before 1914 and his grandfather eventually settled in the south of England in the Hampshire city of Portsmouth and established a chain of grocery stores. His father, David Bernard Gaiman, worked in the same chain of stores; his mother, Sheila Gaiman ("née" Goldman), was a pharmacist. He has two younger sisters, Claire and Lizzy. After living for a period in the nearby town of Portchester, Hampshire, where Neil was born in 1960, the Gaimans moved in 1965 to the West Sussex town of East Grinstead where his parents studied Dianetics at the Scientology centre in the town; one of Gaiman's sisters works for the Church of Scientology in Los Angeles. His other sister, Lizzy Calcioli, has said, "Most of our social activities were involved with Scientology or our Jewish family. It would get very confusing when people would ask my religion as a kid. I’d say, 'I’m a Jewish Scientologist.'" Gaiman says that he is not a Scientologist, and that like Judaism, Scientology is his family's religion. About his personal views, Gaiman has stated, "I think we can say that God exists in the DC Universe. I would not stand up and beat the drum for the existence of God in this universe. I don't know, I think there's probably a 50/50 chance. It doesn't really matter to me."
Gaiman was able to read at the age of four. He said, "I was a reader. I loved reading. Reading things gave me pleasure. I was very good at most subjects in school, not because I had any particular aptitude in them, but because normally on the first day of school they'd hand out schoolbooks, and I'd read them—which would mean that I'd know what was coming up, because I'd read it." When he was about ten years old, he read his way through the works of Dennis Wheatley, where especially "The Ka of Gifford Hillary" and "The Haunting of Toby Jugg" made an impact on him. One work that made a particular impression on him was J. R. R. Tolkien's "The Lord of the Rings" from his school library, although it only had the first two volumes of the novel. He consistently took them out and read them. He would later win the school English prize and the school reading prize, enabling him finally to acquire the third volume.
For his seventh birthday, Gaiman received C. S. Lewis's "The Chronicles of Narnia" series. He later recalled that "I admired his use of parenthetical statements to the reader, where he would just talk to you ... I'd think, 'Oh, my gosh, that is so cool! I want to do that! When I become an author, I want to be able to do things in parentheses.' I liked the power of putting things in brackets." "Narnia" also introduced him to literary awards, specifically the 1956 Carnegie Medal won by the concluding volume. When Gaiman won the 2010 Medal himself, the press reported him recalling, "... it had to be the most important literary award there ever was" and observing, "if you can make yourself aged seven happy, you're really doing well – it's like writing a letter to yourself aged seven."
Lewis Carroll's "Alice's Adventures in Wonderland" was another childhood favourite, and "a favourite forever. Alice was default reading to the point where I knew it by heart." He also enjoyed Batman comics as a child.
Gaiman was educated at several Church of England schools, including Fonthill School in East Grinstead, Ardingly College (1970–74), and Whitgift School in Croydon (1974–77). His father's position as a public relations official of the Church of Scientology was the cause of the seven-year-old Gaiman being blocked from entering a boys' school, forcing him to remain at the school that he had previously been attending. He lived in East Grinstead for many years, from 1965 to 1980 and again from 1984 to 1987. He met his first wife, Mary McGrath, while she was studying Scientology and living in a house in East Grinstead that was owned by his father. The couple were married in 1985 after having their first child, Michael.
Career.
Journalism, early writings, and literary influences.
As a child and a teenager, Gaiman read the works of C. S. Lewis, J. R. R. Tolkien, Lewis Carroll, Mary Shelley, Rudyard Kipling, Edgar Allan Poe, Michael Moorcock, Alan Moore, Ursula K. Le Guin, Harlan Ellison, Lord Dunsany and G. K. Chesterton.
In the early 1980s, Gaiman pursued journalism, conducting interviews and writing book reviews, as a means to learn about the world and to make connections that he hoped would later assist him in getting published. He wrote and reviewed extensively for the British Fantasy Society. His first professional short story publication was "Featherquest", a fantasy story, in "Imagine Magazine" in May 1984.
When waiting for a train at London's Victoria Station in 1984, Gaiman noticed a copy of "Swamp Thing" written by Alan Moore, and carefully read it. Moore's fresh and vigorous approach to comics had such an impact on Gaiman that he would later write "that was the final straw, what was left of my resistance crumbled. I proceeded to make regular and frequent visits to London's Forbidden Planet shop to buy comics".
In 1984, he wrote his first book, a biography of the band Duran Duran, as well as "Ghastly Beyond Belief", a book of quotations, with Kim Newman. Even though Gaiman thought he had done a terrible job, the book's first edition sold out very quickly. When he went to relinquish his rights to the book, he discovered the publisher had gone bankrupt. After this, he was offered a job by "Penthouse". He refused the offer.
He also wrote interviews and articles for many British magazines, including "Knave." During this he sometimes wrote under pseudonyms, including Gerry Musgrave, Richard Grey, and "a couple of house names". Gaiman has said he ended his journalism career in 1987 because British newspapers regularly publish untruths as fact.
In the late 1980s, he wrote "" in what he calls a "classic English humour" style. Following this he wrote the opening of what would become his collaboration with fellow English author Terry Pratchett on the comic novel "Good Omens", about the impending apocalypse.
Comics and graphic novels.
After forming a friendship with comic-book writer Alan Moore, Gaiman started writing comic-books, picking up "Marvelman" after Moore finished his run on the series. Gaiman and artist Mark Buckingham collaborated on several issues of the series before its publisher, Eclipse Comics, collapsed, leaving the series unfinished. His first published comic strips were four short "Future Shocks" for "2000 AD" in 1986–7. He wrote three graphic novels with his favourite collaborator and long-time friend Dave McKean: "Violent Cases", "Signal to Noise", and "The Tragical Comedy or Comical Tragedy of Mr. Punch". Impressed with his work, DC Comics hired him in February 1987, and he wrote the limited series "Black Orchid". Karen Berger, who later became head of DC Comics's Vertigo, read "Black Orchid" and offered Gaiman a job: to re-write an old character, The Sandman, but to put his own spin on him.
"The Sandman" tells the tale of the ageless, anthropomorphic personification of Dream that is known by many names, including Morpheus. The series began in January 1989 and concluded in March 1996. In the eighth issue of "The Sandman", Gaiman and artist Mike Dringenberg introduced Death, the older sister of Dream, who would become as popular as the series' title character. The ' limited series launched DC's Vertigo line in 1993. The 75 issues of the regular series, along with an illustrated prose text and a special containing seven short stories, have been collected into 12 volumes that remain in print, 14 if the "Death: The High Cost of Living" and ' spin-offs are included. Artists include Sam Kieth, Mike Dringenberg, Jill Thompson, Shawn McManus, Marc Hempel and Michael Zulli, lettering by Todd Klein, colours by Daniel Vozzo, and covers by Dave McKean. The series became one of DC's top selling titles, eclipsing even "Batman" and "Superman". Comics historian Les Daniels called Gaiman's work "astonishing" and noted that "The Sandman" was "a mixture of fantasy, horror, and ironic humor such as comic books had never seen before". DC Comics writer and executive Paul Levitz observed that ""The Sandman" became the first extraordinary success as a series of graphic novel collections, reaching out and converting new readers to the medium, particularly young women on college campuses, and making Gaiman himself into an iconic cultural figure."
Gaiman and Jamie Delano were to become co-writers of the "Swamp Thing" series following Rick Veitch. An editorial decision by DC to censor Veitch's final storyline caused both Gaiman and Delano to withdraw from the title.
In 1990, Gaiman wrote "The Books of Magic", a four-part mini-series that provided a tour of the mythological and magical parts of the DC Universe through a frame story about an English teenager who discovers that he is destined to be the world's greatest wizard. The miniseries was popular, and sired an ongoing series written by John Ney Rieber.
Gaiman's adaptation of "Sweeney Todd", illustrated by Michael Zulli for Stephen R. Bissette's publication "Taboo", was stopped when the anthology itself was discontinued.
In the mid-1990s, he also created a number of new characters and a setting that was to be featured in a title published by Tekno Comix. The concepts were then altered and split between three titles set in the same continuity: "Lady Justice", "Mr. Hero the Newmatic Man", and "Teknophage". They were later featured in "" and "Wheel of Worlds". Although Gaiman's name appeared prominently on all titles, he was not involved in writing of any of the above-mentioned books.
Gaiman wrote a semi-autobiographical story about a boy's fascination with Michael Moorcock's anti-hero Elric of Melniboné for Ed Kramer's anthology "Tales of the White Wolf." In 1996, Gaiman and Ed Kramer co-edited "". Nominated for the British Fantasy Award, the original fiction anthology featured stories and contributions by Tori Amos, Clive Barker, Gene Wolfe, Tad Williams, and others.
Asked why he likes comics more than other forms of storytelling, Gaiman said: "One of the joys of comics has always been the knowledge that it was, in many ways, untouched ground. It was virgin territory. When I was working on "Sandman", I felt a lot of the time that I was actually picking up a machete and heading out into the jungle. I got to write in places and do things that nobody had ever done before. When I’m writing novels I’m painfully aware that I’m working in a medium that people have been writing absolutely jaw-droppingly brilliant things for, you know, three-four thousand years now. You know, you can go back. We have things like "The Golden Ass". And you go, well, I don’t know that I’m as good as that and that's two and a half thousand years old. But with comics I felt like – I can do stuff nobody has ever done. I can do stuff nobody has ever thought of. And I could and it was enormously fun."
Gaiman wrote two series for Marvel Comics. "Marvel 1602" was an eight-issue limited series published from November 2003 to June 2004 with art by Andy Kubert and Richard Isanove. "The Eternals" was a seven-issue limited series drawn by John Romita, Jr. which was published from August 2006 to March 2007.
In 2009, Gaiman wrote a two-part Batman story for DC Comics to follow "Batman R.I.P." titled "" a play-off of the classic Superman story "" by Alan Moore. He contributed a twelve-part Metamorpho serial drawn by Mike Allred for "Wednesday Comics", a weekly newspaper-style series. Gaiman and Paul Cornell co-wrote "Action Comics" #894 (Dec. 2010) which featured an appearance by Death. In 2012, DC announced that Gaiman would write a "Sandman" prequel series, "The Sandman: Overture" with art by J. H. Williams III to be released 30 October 2013. Gaiman's Angela character was introduced into the Marvel Universe in the last issue of the "Age of Ultron" miniseries in 2013.
Novels.
In a collaboration with author Terry Pratchett best known for his series of "Discworld" novels, Gaiman's first novel "Good Omens" was published in 1990. In recent years Pratchett has said that while the entire novel was a collaborative effort and most of the ideas could be credited to both of them, Pratchett did a larger portion of writing and editing if for no other reason than Gaiman's scheduled involvement with "Sandman".
The 1996 novelisation of Gaiman's teleplay for the BBC mini-series "Neverwhere" was his first solo novel. The novel was released in tandem with the television series though it presents some notable differences from the television series. Gaiman has since revised the novel twice, the first time for an American audience unfamiliar with the London Underground, the second time because he felt unsatisfied with the original.
In 1999 first printings of his fantasy novel "Stardust" were released. The novel has been released both as a standard novel and in an illustrated text edition.
"American Gods" became one of Gaiman's best-selling and multi-award winning novels upon its release in 2001. A special 10th Anniversary edition was released, with the "author's preferred text" 12,000 words longer than the original mass-market editions.
Gaiman has not written a direct sequel to "American Gods" but he has revisited the characters. A glimpse at Shadow's travels in Europe is found in a short story which finds him in Scotland, applying the same concepts developed in "American Gods" to the story of "Beowulf". The 2005 novel "Anansi Boys" deals with Anansi ('Mr. Nancy'), tracing the relationship of his two sons, one semi-divine and the other an unassuming Englishman, as they explore their common heritage. It debuted at number one on "The New York Times" Best Seller list.
In late 2008, Gaiman released a new children's book, "The Graveyard Book". It follows the adventures of a boy named Bod after his family is murdered and he is left to be brought up by a graveyard. It is heavily influenced by Rudyard Kipling's "The Jungle Book". As of late January 2009[ [update]], it had been on "The New York Times" Bestseller children's list for fifteen weeks.
As of 2008, Gaiman has several books planned. After a tour of China, he decided to write a non-fiction book about his travels and the general mythos of China. Following that, will be a new 'adult' novel (his first since 2005's "Anansi Boys"). After that, another 'all-ages' book (in the same vein as "Coraline" and "The Graveyard Book"). Following that, Gaiman says that he will release another non-fiction book called "The Dream Catchers". In December 2011, Gaiman announced that in January 2012 he would begin work on what is essentially, "American Gods 2".
Film and screenwriting.
Gaiman wrote the 1996 BBC dark fantasy television series "Neverwhere". He cowrote the screenplay for the movie "MirrorMask" with his old friend Dave McKean for McKean to direct. In addition, he wrote the localised English language script to the anime movie "Princess Mononoke", based on a translation of the Japanese script.
He cowrote the script for Robert Zemeckis's "Beowulf" with Roger Avary, a collaboration that has proved productive for both writers. Gaiman has expressed interest in collaborating on a film adaptation of the Epic of Gilgamesh.
He was the only person other than J. Michael Straczynski to write a "Babylon 5" script in the last three seasons, contributing the season five episode "Day of the Dead".
Gaiman has also written at least three drafts of a screenplay adaptation of Nicholson Baker's novel "The Fermata" for director Robert Zemeckis, although the project was stalled while Zemeckis made "The Polar Express" and the Gaiman-Roger Avary written "Beowulf" film.
Neil Gaiman was featured in the "History Channel" documentary "Comic Book Superheroes Unmasked".
Several of Gaiman's original works have been optioned or greenlighted for film adaptation, most notably "Stardust", which premiered in August 2007 and stars Robert De Niro, Michelle Pfeiffer and Claire Danes, directed by Matthew Vaughn. A stop-motion version of "Coraline" was released on 6 February 2009, with Henry Selick directing and Dakota Fanning and Teri Hatcher in the leading voice-actor roles.
In 2007, Gaiman announced that after ten years in development, the feature film of "" would finally begin production with a screenplay by Gaiman that he would direct for Warner Independent. Don Murphy and Susan Montford are the producers, and Guillermo del Toro is the film's executive producer.
Seeing Ear Theatre performed two of Gaiman's audio theatre plays, "Snow, Glass, Apples", Gaiman's retelling of Snow White and "Murder Mysteries", a story of heaven before the Fall in which the first crime is committed. Both audio plays were published in the collection "Smoke and Mirrors" in 1998.
Gaiman's 2009 Newbery Medal winning book "The Graveyard Book" will be made into a movie, with Ron Howard as the director.
Gaiman wrote an episode of the long-running BBC science fiction series "Doctor Who", broadcast in 2011 during Matt Smith's second series as the Doctor. Shooting began in August 2010 for this episode, the original title of which was "The House of Nothing" but which was eventually transmitted as "The Doctor's Wife". The episode won the 2012 Hugo Award for Best Dramatic Presentation (Short Form). Gaiman made his return to "Doctor Who" with an episode titled "Nightmare in Silver", broadcast on 11 May 2013.
In 2011, it was announced that Gaiman would be writing the script to a new film version of "Journey to the West".
Gaiman appeared as himself on "The Simpsons" episode "The Book Job" broadcast on 20 November 2011.
Radio.
A six part radio play of "Neverwhere" was broadcast in March 2013, adapted by Dirk Maggs for BBC Radio 4 and Radio 4 Extra. Featured stars include James McAvoy as Richard, Natalie Dormer, Benedict Cumberbatch, Christopher Lee, Bernard Cribbens and Johnny Vegas.
In September 2014, Gaiman and Terry Pratchett joined forces with BBC Radio 4 to make the first ever dramatisation of their co-penned novel "Good Omens", which was broadcast in December in five half-hour episodes and culminated in an hour-long final apocalyptic showdown.
Public performances.
Gaiman frequently performs public readings from his stories and poetry, and has toured with his wife, musician Amanda Palmer. In some of these performances he has also sung songs, in "a novelist's version of singing", despite having "no kind of singing voice".
Blog and Twitter.
In February 2001, when Gaiman had completed writing "American Gods", his publishers set up a promotional web site featuring a weblog in which Gaiman described the day-to-day process of revising, publishing, and promoting the novel. After the novel was published, the web site evolved into a more general Official Neil Gaiman Website.
Gaiman generally posts to the blog describing the day-to-day process of being Neil Gaiman and writing, revising, publishing, or promoting whatever the current project is. He also posts reader emails and answers questions, which gives him unusually direct and immediate interaction with fans. One of his answers on why he writes the blog is "because writing is, like death, a lonely business."
The original "American Gods" blog was extracted for publication in the NESFA Press collection of Gaiman miscellany, "Adventures in the Dream Trade".
To celebrate the seventh anniversary of the blog, the novel "American Gods" was provided free of charge online for a month.
Gaiman is an active user of the social networking site Twitter with over 1.8 million followers as of February 2013, using the username "@neilhimself". In 2013, Gaiman was named by IGN as one of "The Best Tweeters in Comics", describing his posts as "sublime." Gaiman also runs a Tumblr account on which he primarily answers fan questions.
Personal life.
Home and family.
Gaiman lives near Menomonie, Wisconsin, United States and has lived there since 1992. Gaiman moved there to be close to the family of his then-wife, Mary McGrath, with whom he has three children: Michael, Holly, and Madeleine. s of 2013[ [update]], Gaiman also resides in Cambridge, Massachusetts. In 2014, he took up a five-year appointment as professor in the arts at Bard College, in Red Hook, New York.
Gaiman is married to songwriter and performer Amanda Palmer. The couple publicly announced that they were dating in June 2009, announced their engagement on Twitter on 1 January 2010. On 16 November 2010, Amanda Palmer hosted a non-legally binding flash mob wedding for Gaiman's birthday in New Orleans. They were legally married on 2 January 2011. The wedding took place in the parlour of writers Ayelet Waldman and Michael Chabon. On marrying Palmer, he took her middle name, MacKinnon, as one of his names. On 18 March 2015, they announced through their Facebook and Twitter accounts that Palmer is pregnant with their first child, due in September.
Friendship with Tori Amos.
One of Gaiman's most commented-upon friendships is with the musician Tori Amos, a "Sandman" fan who became friends with Gaiman after making a reference to "Neil and the Dream King" on her 1991 demo tape. He included her in turn as a character (a talking tree) in his novel "Stardust". Amos also mentions Gaiman in her songs, "Tear in Your Hand" ("If you need me, me and Neil'll be hangin' out with the dream king. Neil says hi by the way"), "Space Dog" ("Where's Neil when you need him?"), "Horses" ("But will you find me if Neil makes me a tree?"), "Carbon" ("Get me Neil on the line, no I can't hold. Have him read, 'Snow, Glass, Apples' where nothing is what it seems"), "Sweet Dreams" ("You're forgetting to fly, darling, when you sleep"), and "Not Dying Today" ("Neil is thrilled he can claim he's mammalian, 'but the bad news,' he said, 'girl you're a dandelion'"). He also wrote stories for the tour book of "Boys for Pele" and "Scarlet's Walk", a letter for the tour book of "American Doll Posse", and the stories behind each girl in her album "Strange Little Girls". Amos penned the introduction for his novel "Death: the High Cost of Living", and posed for the cover. She also wrote a song called "Sister Named Desire" based on his "Sandman" character, which was included on his anthology, "Where's Neil When You Need Him?".
Gaiman is godfather to Tori Amos's daughter Tash, and wrote a poem called "Blueberry Girl" for Tori and Tash. The poem has been turned into a book by the illustrator Charles Vess. Gaiman read the poem aloud to an audience at the Sundance Kabuki Theater in San Francisco on 5 October 2008 during his book reading tour for "The Graveyard Book". It was published in March 2009 with the title, "Blueberry Girl".
Litigation.
In 1993, Gaiman was contracted by Todd McFarlane to write a single issue of "Spawn", a popular title at the newly created Image Comics company. McFarlane was promoting his new title by having guest authors Gaiman, Alan Moore, Frank Miller, and Dave Sim each write a single issue.
In issue No. 9 of the series, Gaiman introduced the characters Angela, Cogliostro, and Medieval Spawn. Prior to this issue, Spawn was an assassin who worked for the government and came back as a reluctant agent of Hell but had no direction. In Angela, a cruel and malicious angel, Gaiman introduced a character who threatened Spawn's existence, as well as providing a moral opposite. Cogliostro was introduced as a mentor character for exposition and instruction, providing guidance. Medieval Spawn introduced a history and precedent that not all Spawns were self-serving or evil, giving additional character development to Malebolgia, the demon that creates Hellspawn.
As intended, all three characters were used repeatedly throughout the next decade by Todd McFarlane within the wider Spawn universe. In papers filed by Gaiman in early 2002, however, he claimed that the characters were jointly owned by their scripter (himself) and artist (McFarlane), not merely by McFarlane in his role as the creator of the series. Disagreement over who owned the rights to a character was the primary motivation for McFarlane and other artists to form Image Comics (although that argument related more towards disagreements between writers and artists as character creators). As McFarlane used the characters without Gaiman's permission or royalty payments, Gaiman believed his copyrighted work was being infringed upon, which violated their original, oral, agreement. McFarlane initially agreed that Gaiman had not signed away any rights to the characters, and negotiated with Gaiman to effectively 'swap' McFarlane's interest in the character Marvelman (McFarlane believes he purchased interest in the character when Eclipse Comics was liquidated; Gaiman is interested in being able to continue his aborted run on that title) but later claimed that Gaiman's work had been work-for-hire and that McFarlane owned all of Gaiman's creations entirely. The presiding judge, however, ruled against their agreement being work for hire, based in large part on the legal requirement that "copyright assignments must be in writing."
The Seventh Circuit Court of Appeals upheld the district court ruling in February 2004 granting joint ownership of the characters to Gaiman and McFarlane. On the specific issue of Cogliostro, presiding Judge John C. Shabaz proclaimed, "The expressive work that is the comic-book character Count Nicholas Cogliostro was the joint work of Gaiman and McFarlane—their contributions strike us as quite equal—and both are entitled to ownership of the copyright". Similar analysis led to similar results for the other two characters, Angela and Medieval Spawn.
This legal battle was brought by Gaiman and the specifically formed Marvels and Miracles, LLC, which Gaiman created to help sort out the legal rights surrounding Marvelman. Gaiman wrote "Marvel 1602 "in 2003 to help fund this project. All of Marvel Comics' profits for the original issues of the series went to Marvels and Miracles. In 2009, Marvel Comics purchased Marvelman.
Gaiman returned to court over three more Spawn characters, Dark Ages Spawn, Domina and Tiffany, that are claimed to be "derivative of the three he co-created with McFarlane." The judge ruled that Gaiman was right in his claims and gave McFarlane until the start of September 2010 to settle matters.
Gaiman is a major supporter and board member of the Comic Book Legal Defense Fund.
Literary allusions.
Gaiman's work is known for a high degree of allusiveness. Meredith Collins, for instance, has commented upon the degree to which his novel "Stardust" depends on allusions to Victorian fairy tales and culture. Particularly in "The Sandman", literary figures and characters appear often; the character of Fiddler's Green is modelled visually on G. K. Chesterton, both William Shakespeare and Geoffrey Chaucer appear as characters, as do several characters from within "A Midsummer Night's Dream" and "The Tempest". The comic also draws from numerous mythologies and historical periods.
Clay Smith has argued that this sort of allusiveness serves to situate Gaiman as a strong authorial presence in his own works, often to the exclusion of his collaborators. However, Smith's viewpoint is in the minority: to many, if there is a problem with Gaiman scholarship and intertextuality it is that "... his literary merit and vast popularity have propelled him into the nascent comics canon so quickly that there is not yet a basis of critical scholarship about his work."
David Rudd takes a more generous view in his study of the novel "Coraline", where he argues that the work plays and riffs productively on Sigmund Freud's notion of the Uncanny, or the "Unheimlich".
Though Gaiman's work is frequently seen as exemplifying the monomyth structure laid out in Joseph Campbell's "The Hero with a Thousand Faces", Gaiman says that he started reading "The Hero with a Thousand Faces" but refused to finish it: "I think I got about half way through "The Hero with a Thousand Faces" and found myself thinking if this is true – I don’t want to know. I really would rather not know this stuff. I’d rather do it because it’s true and because I accidentally wind up creating something that falls into this pattern than be told what the pattern is."
Selected awards and honours.
</dl>

</doc>
<doc id="22058" url="http://en.wikipedia.org/wiki?curid=22058" title="Nymph">
Nymph

A nymph (Greek: νύμφη, "nymphē") in Greek mythology and in Latin mythology is a minor female nature deity typically associated with a particular location or landform. Different from other goddesses, nymphs are generally regarded as divine spirits who animate nature, and are usually depicted as beautiful, young nubile maidens who love to dance and sing; their amorous freedom sets them apart from the restricted and chaste wives and daughters of the Greek "polis". They are beloved by many and dwell in mountainous regions and forests by lakes and streams. Although they would never die of old age nor illness, and could give birth to fully immortal children if mated to a god, they themselves were not necessarily immortal, and could be beholden to death in various forms. Charybdis and Scylla were once nymphs. Nymphs are also found in lots of popular culture such as fantasy video games.
Other nymphs, always in the shape of young maidens, were part of the retinue of a god, such as Dionysus, Hermes, or Pan, or a goddess, generally the huntress Artemis. Nymphs were the frequent target of satyrs.
Etymology.
Nymphs are personifications of the creative and fostering activities of nature, most often identified with the life-giving outflow of springs: as Walter Burkert (Burkert 1985:III.3.3) remarks, "The idea that rivers are gods and springs divine nymphs is deeply rooted not only in poetry but in belief and ritual; the worship of these deities is limited only by the fact that they are inseparably identified with a specific locality."
The Greek word νύμφη has "bride" and "veiled" among its meanings: hence a marriageable young woman. Other readers refer the word (and also Latin "nubere" and German "Knospe") to a root expressing the idea of "swelling" (according to Hesychius, one of the meanings of νύμφη is "rose-bud").
Adaptations.
The Greek nymphs were spirits invariably bound to places, not unlike the Latin "genius loci", and the difficulty of transferring their cult may be seen in the complicated myth that brought Arethusa to Sicily. In the works of the Greek-educated Latin poets, the nymphs gradually absorbed into their ranks the indigenous Italian divinities of springs and streams (Juturna, Egeria, Carmentis, Fontus), while the Lymphae (originally Lumpae), Italian water-goddesses, owing to the accidental similarity of their names, could be identified with the Greek Nymphae. The mythologies of classicizing Roman poets were unlikely to have affected the rites and cult of individual nymphs venerated by country people in the springs and clefts of Latium. Among the Roman literate class, their sphere of influence was restricted, and they appear almost exclusively as divinities of the watery element.
In modern Greek folklore.
The ancient Greek belief in nymphs survived in many parts of the country into the early years of the twentieth century, when they were usually known as "nereids". At that time, John Cuthbert Lawson wrote: "...there is probably no nook or hamlet in all Greece where the womenfolk at least do not scrupulously take precautions against the thefts and malice of the nereids, while many a man may still be found to recount in all good faith stories of their beauty, passion and caprice.
Nor is it a matter of faith only; more than once I have been in villages where certain Nereids were known by sight to several persons (so at least they averred); and there was a wonderful agreement among the witnesses in the description of their appearance and dress."
Nymphs tended to frequent areas distant from humans but could be encountered by lone travelers outside the village, where their music might be heard, and the traveler could spy on their dancing or bathing in a stream or pool, either during the noon heat or in the middle of the night. They might appear in a whirlwind. Such encounters could be dangerous, bringing dumbness, besotted infatuation, madness or stroke to the unfortunate human. When parents believed their child to be nereid-struck, they would pray to Saint Artemidos.
Modern sexual connotations.
Due to the depiction of the mythological nymphs as females who mate with men or women at their own volition, and are completely outside of male control, the term is often used for women who are perceived as behaving similarly. (For example, the title of the Perry Mason detective novel "The Case of the Negligent Nymph" (1956) by Erle Stanley Gardner is derived from this meaning of the word.)
The term "nymphomania" was created by modern psychology as referring to a "desire to engage in human sexual behavior at a level high enough to be considered clinically significant", "nymphomaniac" being the person suffering from such a disorder. Due to widespread use of the term among lay persons (often shortened to "nympho") and stereotypes attached, professionals nowadays prefer the term "hypersexuality", which can refer to males and females alike.
The word "nymphet" is used to identify a sexually precocious girl. The term was made famous in the novel "Lolita" by Vladimir Nabokov. The main character, Humbert Humbert, uses the term many times, usually in reference to the title character.
Classification.
As H.J. Rose states, all the names for various classes of nymphs are plural feminine adjectives agreeing with the substantive "nymphai", and there was no single classification that could be seen as canonical and exhaustive. Thus the classes of nymphs tend to overlap, which complicates the task of precise classification. Rose mentions dryads and hamadryads as nymphs of trees generally, "meliai" as nymphs of ash trees, and naiads as nymphs of water, but no others specifically.
Classification by type of dwelling.
The following is not the authentic Greek classification, but is intended simply as a guide:
Location-specific groupings of nymphs.
The following is a list of groups of nymphs associated with this or that particular location. Nymphs in such groupings could belong to any of the classes mentioned above (Naiades, Oreades, and so on).
Individual names of some of the nymphs.
The following is a selection of names of the nymphs whose class was not specified in the source texts. For lists of Naiads, Oceanids, Dryades etc. see respective articles.

</doc>
<doc id="22059" url="http://en.wikipedia.org/wiki?curid=22059" title="Norse">
Norse

Norse may refer to:

</doc>
<doc id="22063" url="http://en.wikipedia.org/wiki?curid=22063" title="Natural law">
Natural law

Natural law, or the law of nature (Latin: "lex naturalis; ius naturale"), is a philosophy of law that is supposedly determined by nature, and so is universal. Classically, natural law refers to the use of reason to analyze human nature — both social and personal — and deduce binding rules of moral behavior from it. Natural law is often contrasted with the positive law of a given political community, society, or state. In legal theory, on the other hand, the interpretation of positive law requires some reference to natural law. On this understanding of natural law, natural law can be invoked to criticize judicial decisions about what the law says but not to criticize the best interpretation of the law itself. Some scholars use natural law synonymously with natural justice or natural right (Latin "ius naturale"), while others distinguish between natural law and natural right.
Although natural law is often conflated with common law, the two are distinct in that natural law is a view that certain rights or values are inherent in or universally cognizable by virtue of human reason or human nature, while common law is the legal tradition whereby certain rights or values are legally cognizable by virtue of judicial recognition or articulation. Natural law theories have, however, exercised a profound influence on the development of English common law, and have featured greatly in the philosophies of Thomas Aquinas, Francisco Suárez, Richard Hooker, Thomas Hobbes, Hugo Grotius, Samuel von Pufendorf, John Locke, Francis Hutcheson, Jean Jacques Burlamaqui, and Emmerich de Vattel. Because of the intersection between natural law and natural rights, it has been cited as a component in the United States Declaration of Independence and the Constitution of the United States, as well as in the Declaration of the Rights of Man and of the Citizen. Declarationism states that the founding of the United States is based on Natural law.
Natural Law and consent of the governed (John Locke) are the Foundation of the American Declaration of Independence, Constitution and Bill of Rights. (See "Laws of Nature" First Paragraph Declaration of Independence) Consent of the Governed, derived from the John Locke's Natural Law Social Contract, replaced the Old World Governance Doctrine of the Divine Right of Kings.
History.
The use of natural law, in its various incarnations, has varied widely through its history. There are a number of different theories of natural law, differing from each other with respect to the role that morality plays in determining the authority of legal norms. This article deals with its usages separately rather than attempt to unify them into a single theory.
Plato.
Although Plato does not have an explicit theory of natural law (he rarely used the phrase 'natural law' except in "Gorgias" 484 and "Timaeus" 83e), his concept of nature, according to John Wild, contains some of the elements found in many natural law theories. According to Plato we live in an orderly universe. At the basis of this orderly universe or nature are the forms, most fundamentally the Form of the Good, which Plato describes as "the brightest region of Being". The Form of the Good is the cause of all things and when it is seen it leads a person to act wisely. In the "Symposium", the Good is closely identified with the Beautiful. Also in the "Symposium", Plato describes how the experience of the Beautiful by Socrates enables him to resist the temptations of wealth and sex. In the "Republic", the ideal community is, "...a city which would be established in accordance with nature."
Aristotle.
Greek philosophy emphasized the distinction between "nature" ("physis", "φúσις") on the one hand and "law", "custom", or "convention" ("nomos", "νóμος") on the other. What the law commanded varied from place to place, but what was "by nature" should be the same everywhere. A "law of nature" would therefore have had the flavor more of a paradox than something that obviously existed. Against the conventionalism that the distinction between nature and custom could engender, Socrates and his philosophic heirs, Plato and Aristotle, posited the existence of natural justice or natural right ("dikaion physikon", "δικαιον φυσικον", Latin "ius naturale"). Of these, Aristotle is often said to be the father of natural law.
Aristotle's association with natural law may be due to the interpretation given to his works by Thomas Aquinas. But whether Aquinas correctly read Aristotle is a disputed question. According to some, Aquinas conflates the natural law and natural right, the latter of which Aristotle posits in Book V of the "Nicomachean Ethics" (Book IV of the "Eudemian Ethics"). According to this interpretation, Aquinas's influence was such as to affect a number of early translations of these passages in an unfortunate manner, though more recent translations render them more literally. Aristotle notes that natural justice is a species of political justice, viz. the scheme of distributive and corrective justice that would be established under the best political community; were this to take the form of law, this could be called a natural law, though Aristotle does not discuss this and suggests in the "Politics" that the best regime may not rule by law at all.
The best evidence of Aristotle's having thought there was a natural law comes from the "Rhetoric", where Aristotle notes that, aside from the "particular" laws that each people has set up for itself, there is a "common" law that is according to nature. Specifically, he quotes Sophocles and Empedocles:
Universal law is the law of Nature. For there really is, as every one to some extent divines, a natural justice and injustice that is binding on all men, even on those who have no association or covenant with each other. It is this that Sophocles' Antigone clearly means when she says that the burial of Polyneices was a just act in spite of the prohibition: she means that it was just by nature:
 "Not of to-day or yesterday it is,
 But lives eternal: none can date its birth."
And so Empedocles, when he bids us kill no living creature, says that doing this is not just for some people while unjust for others:
 "Nay, but, an all-embracing law, through the realms of the sky
 Unbroken it stretcheth, and over the earth's immensity."
Some critics believe that the context of this remark suggests only that Aristotle advised that it could be rhetorically advantageous to appeal to such a law, especially when the "particular" law of one's own city was averse to the case being made, not that there actually was such a law; Moreover, they claim that Aristotle considered two of the three candidates for a universally valid, natural law provided in this passage to be wrong. Aristotle's theoretical paternity of the natural law tradition is consequently disputed.
Stoic natural law.
The development of this tradition of natural justice into one of natural law is usually attributed to the Stoics. The rise of natural law as a universal system coincided with the rise of large empires and kingdoms in the Greek world. Whereas the "higher" law Aristotle suggested one could appeal to was emphatically natural, in contradistinction to being the result of divine positive legislation, the Stoic natural law was indifferent to the divine or natural source of the law: the Stoics asserted the existence of a rational and purposeful order to the universe (a divine or eternal law), and the means by which a rational being lived in accordance with this order was the natural law, which spelled out action that accorded with virtue.
As the English historian A. J. Carlyle (1861–1943) notes:
There is no change in political theory so startling in its completeness as the change from the theory of Aristotle to the later philosophical view represented by Cicero and Seneca... We think that this cannot be better exemplified than with regard to the theory of the equality of human nature." Charles H. McIlwain likewise observes that "the idea of the equality of men is the profoundest contribution of the Stoics to political thought" and that "its greatest influence is in the changed conception of law that in part resulted from it.
Natural law first appeared among the stoics who believed that God is everywhere and in everyone. Within humans is a "divine spark" which helps them to live in accordance with nature.
The stoics felt that there was a way in which the universe had been designed and natural law helped us to harmonise with this.
Cicero.
Cicero wrote in his De Legibus that both justice and law derive their origin from what nature has given to man, from what the human mind embraces, from the function of man, and from what serves to unite humanity. For Cicero, natural law obliges us to contribute to the general good of the larger society. The purpose of positive laws is to provide for "the safety of citizens, the preservation of states, and the tranquility and happiness of human life." In this view, "wicked and unjust statutes" are "anything but 'laws,'" because "in the very definition of the term 'law' there inheres the idea and principle of choosing what is just and true." Law, for Cicero, "ought to be a reformer of vice and an incentive to virtue." Cicero expressed the view that "the virtues which we ought to cultivate, always tend to our own happiness, and that the best means of promoting them consists in living with men in that perfect union and charity which are cemented by mutual benefits."
Cicero influenced the discussion of natural law for many centuries to come, up through the era of the American Revolution. The jurisprudence of the Roman Empire was rooted in Cicero, who held "an extraordinary grip ... upon the imagination of posterity" as "the medium for the propagation of those ideas which informed the law and institutions of the empire." Cicero's conception of natural law "found its way to later centuries notably through the writings of Saint Isidore of Seville and the Decretum of Gratian." Thomas Aquinas, in his summary of medieval natural law, quoted Cicero's statement that "nature" and "custom" were the sources of a society's laws.
The Renaissance Florentine chancellor Leonardo Bruni praised Cicero as the man "who carried philosophy from Greece to Italy, and nourished it with the golden river of his eloquence." The legal culture of Elizabethan England, exemplified by Sir Edward Coke, was "steeped in Ciceronian rhetoric." The Scottish moral philosopher Francis Hutcheson, as a student at Glasgow, "was attracted most by Cicero, for whom he always professed the greatest admiration." More generally in eighteenth-century Great Britain, Cicero's name was a household word among educated people. Likewise, "in the admiration of early Americans Cicero took pride of place as orator, political theorist, stylist, and moralist."
The British polemicist Thomas Gordon "incorporated Cicero into the radical ideological tradition that travelled from the mother country to the colonies in the course of the eighteenth century and decisively shaped early American political culture." Cicero's description of the immutable, eternal, and universal natural law was quoted by Burlamaqui and later by the American revolutionary legal scholar James Wilson. Cicero became John Adams's "foremost model of public service, republican virtue, and forensic eloquence." Adams wrote of Cicero that "as all the ages of the world have not produced a greater statesman and philosopher united in the same character, his authority should have great weight." Thomas Jefferson "first encountered Cicero as a schoolboy learning Latin, and continued to read his letters and discourses as long as he lived. He admired him as a patriot, valued his opinions as a moral philosopher, and there is little doubt that he looked upon Cicero's life, with his love of study and aristocratic country life, as a model for his own." Jefferson described Cicero as "the father of eloquence and philosophy."
Some early Church Fathers, especially those in the West, sought to incorporate natural law into Christianity. The most notable among these was Augustine of Hippo, who equated natural law with man's prelapsarian state; as such, a life according to nature was no longer possible and men needed instead to seek salvation through the divine law and grace of Jesus Christ.
In the twelfth century, Gratian equated the natural law with divine law. A century later, St. Thomas Aquinas in his "Summa Theologica" I-II qq. 90–106, restored Natural Law to its independent state, asserting natural law as the rational creature's participation in the eternal law. Yet, since human reason could not fully comprehend the Eternal law, it needed to be supplemented by revealed Divine law. (See also Biblical law in Christianity.) Meanwhile, Aquinas taught that all human or positive laws were to be judged by their conformity to the natural law. An unjust law is not a law, in the full sense of the word. It retains merely the 'appearance' of law insofar as it is duly constituted and enforced in the same way a just law is, but is itself a 'perversion of law.' At this point, the natural law was not only used to pass judgment on the moral worth of various laws, but also to determine what the law said in the first place. This principle laid the seed for possible societal tension with reference to tyrants.
The natural law was inherently teleological and deontological in that although it is aimed at goodness, it is entirely focused on the ethicalness of actions, rather than the consequence. The specific content of the natural law was therefore determined by a conception of what things constituted happiness, be they temporal satisfaction or salvation. The state, in being bound by the natural law, was conceived as an institution directed at bringing its subjects to true happiness.
In the 16th century, the School of Salamanca (Francisco Suárez, Francisco de Vitoria, etc.) further developed a philosophy of natural law. After the Church of England broke from Rome, the English theologian Richard Hooker adapted Thomistic notions of natural law to Anglicanism. There are five important principles: to live, to learn, to reproduce, to worship God, and to live in an ordered society.
Those who see biblical support for the doctrine of natural law often point to Paul's Epistle to the Romans: "For when the Gentiles, which have not the law, do by nature the things contained in the law, these, having not the law, are a law unto themselves: Which shew the work of the law written in their hearts, their conscience also bearing witness, and their thoughts the mean while accusing or else excusing one another. (). The intellectual historian A. J. Carlyle has commented on this passage, "There can be little doubt that St Paul's words imply some conception analogous to the 'natural law' in Cicero, a law written in men's hearts, recognized by man's reason, a law distinct from the positive law of any State, or from what St Paul recognized as the revealed law of God. It is in this sense that St Paul's words are taken by the Fathers of the fourth and fifth centuries like St Hilary of Poitiers, St Ambrose, and St Augustine, and there seems no reason to doubt the correctness of their interpretation."
English jurisprudence.
Heinrich A. Rommen remarked upon "the tenacity with which the spirit of the English common law retained the conceptions of natural law and equity which it had assimilated during the Catholic Middle Ages, thanks especially to the influence of Henry de Bracton (d. 1268) and Sir John Fortescue (d. cir. 1476)." Bracton's translator notes that Bracton "was a trained jurist with the principles and distinctions of Roman jurisprudence firmly in mind"; but Bracton adapted such principles to English purposes rather than copying slavishly. In particular, Bracton turned the imperial Roman maxim that "the will of the prince is law" on its head, insisting that the king is "under" the law. The legal historian Charles F. Mullett has noted Bracton's "ethical definition of law, his recognition of justice, and finally his devotion to natural rights." Bracton considered justice to be the "fountain-head" from which "all rights arise." For his definition of justice, Bracton quoted the twelfth-century Italian jurist Azo: "'Justice is the constant and unfailing will to give to each his right.'" Bracton's work was the second legal treatise studied by the young apprentice lawyer Thomas Jefferson.
Fortescue stressed "the supreme importance of the law of God and of nature" in works that "profoundly influenced the course of legal development in the following centuries." The legal scholar Ellis Sandoz has noted that "the historically ancient and the ontologically higher law—eternal, divine, natural—are woven together to compose a single harmonious texture in Fortescue's account of English law." As the legal historian Norman Doe explains: "Fortescue follows the general pattern set by Aquinas. The objective of every legislator is to dispose people to virtue. It is by means of law that this is accomplished. Fortescue's definition of law (also found in Accursius and Bracton), after all, was 'a sacred sanction commanding what is virtuous ["honesta"] and forbidding the contrary.'" Fortescue cited Leonardo Bruni for his statement that "virtue alone produces happiness."
Christopher St. Germain's "Doctor and Student" was a classic of English jurisprudence, and it was thoroughly annotated by Thomas Jefferson. St. Germain informs his readers that English lawyers generally don't use the phrase "law of nature," but rather use "reason" as the preferred synonym. Norman Doe notes that St. Germain's view "is essentially Thomist," quoting Thomas Aquinas's definition of law as "an ordinance of reason made for the common good by him who has charge of the community, and promulgated."
Sir Edward Coke was the preeminent jurist of his time. Coke's preeminence extended across the ocean: "For the American revolutionary leaders, 'law' meant Sir Edward Coke's custom and right reason."
 Coke defined law as "perfect reason, which commands those things that are proper and necessary and which prohibits contrary things." For Coke, human nature determined the purpose of law; and law was superior to any one man's reason or will. Coke's discussion of natural law appears in his report of "Calvin's Case" (1608): "The law of nature is that which God at the time of creation of the nature of man infused into his heart, for his preservation and direction." In this case the judges found that "the ligeance or faith of the subject is due unto the King by the law of nature: secondly, that the law of nature is part of the law of England: thirdly, that the law of nature was before any judicial or municipal law: fourthly, that the law of nature is immutable." To support these findings, the assembled judges (as reported by Coke, who was one of them) cited as authorities Aristotle, Cicero, and the Apostle Paul; as well as Bracton, Fortescue, and St. Germain.
As early as the thirteenth century, it was held that "the law of nature...is the ground of all laws" and by the Chancellor and Judges that "it is required by the law of nature that every person, before he can be punish'd, ought to be present; and if absent by contumacy, he ought to be summoned and make default.". Further, in 1824, we find it held that "proceedings in our Courts are founded upon the law of England, and that law is again founded upon the law of nature and the revealed law of God. If the right sought to be enforced is inconsistent with either of these, the English municipal courts cannot recognize it."
American jurisprudence.
The U.S. Declaration of Independence states that it has become necessary for the people of the United States to assume "the separate and equal station to which the Laws of Nature and of Nature's God entitle them". Some early American lawyers and judges perceived natural law as too tenuous, amorphous and evanescent a legal basis for grounding concrete rights and governmental limitations. Natural law did, however, serve as authority for legal claims and rights in some judicial decisions, legislative acts, and legal pronouncements. Robert Lowry Clinton argues that the U.S. Constitution rests on a common law foundation and the common law, in turn, rests on a classical natural law foundation.
Islamic natural law.
Abū Rayhān al-Bīrūnī, an Islamic scholar and polymath scientist, understood natural law as the survival of the fittest. He argued that the antagonism between human beings can only be overcome through a divine law, which he believed to have been sent through prophets. This is also the position of the Ashari school, the largest school of Sunni theology. Averroes (Ibn Rushd), in his treatise on "Justice and Jihad" and his commentary on Plato's "Republic", writes that the human mind can know of the unlawfulness of killing and stealing and thus of the five maqasid or higher intents of the Islamic sharia or to protect religion, life, property, offspring, and reason. The concept of natural law entered the mainstream of Western culture through his Aristotelian commentaries, influencing the subsequent Averroist movement and the writings of Thomas Aquinas.
The Maturidi school, the second largest school of Sunni theology, posits the existence of a form of natural law. Abu Mansur al-Maturidi stated that the human mind could know of the existence of God and the major forms of 'good' and 'evil' without the help of revelation. Al-Maturidi gives the example of stealing, which is known to be evil by reason alone due to man's working hard for his property. Killing, fornication, and drinking alcohol were all 'evils' the human mind could know of according to al-Maturidi. The concept of "Istislah" in Islamic law bears some similarities to the natural law tradition in the West, as exemplified by Thomas Aquinas. However, whereas natural law deems good what is self-evidently good, according as it tends towards the fulfilment of the person, "istislah" calls good whatever is connected to one of five "basic goods". Al-Ghazali abstracted these "basic goods" from the legal precepts in the Qur'an and Sunnah: they are religion, life, reason, lineage and property. Some add also "honour". Ibn Qayyim Al-Jawziyya also posited that human reason could discern between 'great sins' and good deeds.
Hobbes.
By the 17th Century, the Medieval teleological view came under intense criticism from some quarters. Thomas Hobbes instead founded a contractualist theory of legal positivism on what all men could agree upon: what they sought (happiness) was subject to contention, but a broad consensus could form around what they feared (violent death at the hands of another). The natural law was how a rational human being, seeking to survive and prosper, would act. Natural law, therefore, was discovered by considering humankind's natural rights, whereas previously it could be said that natural rights were discovered by considering the natural law. In Hobbes' opinion, the only way natural law could prevail was for men to submit to the commands of the sovereign. Because the ultimate source of law now comes from the sovereign, and the sovereign's decisions need not be grounded in morality, legal positivism is born. Jeremy Bentham's modifications on legal positivism further developed the theory.
As used by Thomas Hobbes in his treatises "Leviathan" and "De Cive", natural law is "a precept, or general rule, found out by reason, by which a man is forbidden to do that which is destructive of his life, or takes away the means of preserving the same; and to omit that by which he thinks it may best be preserved."
According to Hobbes, there are nineteen Laws. The first two are expounded in chapter XIV of Leviathan ("of the first and second natural laws; and of contracts"); the others in chapter XV ("of other laws of nature").
Hobbes's philosophy includes a frontal assault on the founding principles of the earlier natural legal tradition, disregarding the traditional association of virtue with happiness, and likewise re-defining "law" to remove any notion of the promotion of the common good. Hobbes has no use for Aristotle's association of nature with human perfection, inverting Aristotle's use of the word "nature." Hobbes posits a primitive, unconnected state of nature in which men, having a "natural proclivity...to hurt each other" also have "a Right to every thing, even to one anothers body"; and "nothing can be Unjust" in this "warre of every man against every man" in which human life is "solitary, poore, nasty, brutish, and short." Rejecting Cicero's view that men join in society primarily through "a certain social spirit which nature has implanted in man," Hobbes declares that men join in society simply for the purpose of "getting themselves out from that miserable condition of Warre, which is necessarily consequent...to the naturall Passions of men, when there is no visible Power to keep them in awe." As part of his campaign against the classical idea of natural human sociability, Hobbes inverts that fundamental natural legal maxim, the Golden Rule. Hobbes's version is "Do not that to another, which thou wouldst not have done to thy selfe."
Cumberland's rebuttal of Hobbes.
The English cleric Richard Cumberland wrote a lengthy and influential attack on Hobbes's depiction of individual self-interest as the essential feature of human motivation. Historian Knud Haakonssen has noted that in the eighteenth century, Cumberland was commonly placed alongside Hugo Grotius and Samuel Pufendorf "in the triumvirate of seventeenth-century founders of the 'modern' school of natural law." The eighteenth-century philosophers Shaftesbury and Hutcheson "were obviously inspired in part by Cumberland." Historian Jon Parkin likewise describes Cumberland's work as "one of the most important works of ethical and political theory of the seventeenth century." Parkin observes that much of Cumberland's material "is derived from Roman Stoicism, particularly from the work of Cicero, as "Cumberland deliberately cast his engagement with Hobbes in the mould of Cicero's debate between the Stoics, who believed that nature could provide an objective morality, and Epicureans, who argued that morality was human, conventional and self-interested." In doing so, Cumberland de-emphasized the overlay of Christian dogma (in particular, the doctrine of "original sin" and the corresponding presumption that humans are incapable of "perfecting" themselves without divine intervention) that had accreted to natural law in the Middle Ages.
By way of contrast to Hobbes's multiplicity of laws, Cumberland states in the very first sentence of his "Treatise of the Laws of Nature" that "all the Laws of Nature are reduc'd to that one, of Benevolence toward all Rationals." He later clarifies: "By the name "Rationals" I beg leave to understand, as well "God" as "Man"; and I do it upon the Authority of Cicero." Cumberland argues that the mature development ("perfection") of human nature involves the individual human willing and acting for the common good. For Cumberland, human interdependence precludes Hobbes's natural right of each individual to wage war against all the rest for personal survival. However, Haakonssen warns against reading Cumberland as a proponent of "enlightened self-interest." Rather, the "proper moral love of humanity" is "a disinterested love of God through love of humanity in ourselves as well as others." Cumberland concludes that actions "principally conducive to our Happiness" are those that promote "the Honour and Glory of God" and also "Charity and Justice towards men." Cumberland emphasizes that desiring the well-being of our fellow humans is essential to the "pursuit of our own Happiness." He cites "reason" as the authority for his conclusion that happiness consists in "the most extensive Benevolence," but he also mentions as "Essential Ingredients of Happiness" the "Benevolent Affections," meaning "Love and Benevolence towards others," as well as "that Joy, which arises from their Happiness."
Liberal natural law.
Liberal natural law grew out of the medieval Christian natural law theories and out of Hobbes' revision of natural law, sometimes in an uneasy balance of the two.
Hugo Grotius based his philosophy of international law on natural law. In particular, his writings on freedom of the seas and just war theory directly appealed to natural law. About natural law itself, he wrote that "even the will of an omnipotent being cannot change or abrogate" natural law, which "would maintain its objective validity even if we should assume the impossible, that there is no God or that he does not care for human affairs." ("De iure belli ac pacis", Prolegomeni XI). This is the famous argument "etiamsi daremus" ("non esse Deum"), that made natural law no longer dependent on theology. However, German church-historians Ernst Wolf and M. Elze disagreed and claimed that Grotius' concept of natural law did have a theological basis. In Grotius' view, the Old Testament contained moral precepts (e.g. the Decalogue) which Christ confirmed and therefore were still valid. Moreover, they were useful in explaining the content of natural law. Both biblical revelation and natural law originated in God and could therefore not contradict each other.
In a similar way, Samuel Pufendorf gave natural law a theological foundation and applied it to his concepts of government and international law.
John Locke incorporated natural law into many of his theories and philosophy, especially in "Two Treatises of Government". There is considerable debate about whether his conception of natural law was more akin to that of Aquinas (filtered through Richard Hooker) or Hobbes' radical reinterpretation, though the effect of Locke's understanding is usually phrased in terms of a revision of Hobbes upon Hobbesean contractualist grounds. Locke turned Hobbes' prescription around, saying that if the ruler went against natural law and failed to protect "life, liberty, and property," people could justifiably overthrow the existing state and create a new one.
While Locke spoke in the language of natural law, the content of this law was by and large protective of natural rights, and it was this language that later liberal thinkers preferred. Political philosopher Jeremy Waldron has pointed out that Locke's political thought was based on "a particular set of Protestant Christian assumptions." To Locke, the content of natural law was identical with biblical ethics as laid down especially in the Decalogue, Christ's teaching and exemplary life, and St. Paul's admonitions. Locke derived the concept of basic human equality, including the equality of the sexes ("Adam and Eve"), from , the starting-point of the theological doctrine of Imago Dei. One of the consequences is that as all humans are created equally free, governments need the consent of the governed. Thomas Jefferson, arguably echoing Locke, appealed to unalienable rights in the "Declaration of Independence", "We hold these truths to be self-evident, that all men are "created" equal, that they are endowed by their "Creator" with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness." The Lockean idea that governments need the consent of the governed was also fundamental to the Declaration of Independence, as the American Revolutionaries used it as justification for their separation from the British crown.
The Belgian philosopher of law Frank van Dun is one among those who are elaborating a secular conception of natural law in the liberal tradition. Libertarian theorist Murray Rothbard argues that "the very existence of a natural law discoverable by reason is a potentially powerful threat to the status quo and a standing reproach to the reign of blindly traditional custom or the arbitrary will of the State apparatus." Ludwig von Mises states that he relaid the general sociological and economic foundations of the liberal doctrine upon utilitarianism, rather than natural law, but R.A. Gonce argues that "the reality of the argument constituting his system overwhelms his denial." David Gordon notes, "When most people speak of natural law, what they have in mind is the contention that morality can be derived from human nature. If human beings are rational animals of such-and-such a sort, then the moral virtues are...(filling in the blanks is the difficult part)."
However, a secular critique of the natural law doctrine was stated by Pierre Charron in his "De la sagesse" (1601): "The sign of a natural law must be the universal respect in which it is held, for if there was anything that nature had truly commanded us to do, we would undoubtedly obey it universally: not only would every nation respect it, but every individual. Instead there is nothing in the world that is not subject to contradiction and dispute, nothing that is not rejected, not just by one nation, but by many; equally, there is nothing that is strange and (in the opinion of many) unnatural that is not approved in many countries, and authorized by their customs."
Contemporary Christian understanding.
The Roman Catholic Church holds the view of natural law provided by St. Thomas Aquinas, particularly in his "Summa Theologiae", and often as filtered through the School of Salamanca. This view is also shared by some Protestant churches, and was delineated by C.S. Lewis in his works "Mere Christianity" and "The Abolition of Man.
The Catholic Church understands human beings to consist of body and mind, the physical and the non-physical (or soul perhaps), and that the two are inextricably linked. Humans are capable of discerning the difference between good and evil because they have a conscience. There are many manifestations of the good that we can pursue. Some, like procreation, are common to other animals, while others, like the pursuit of truth, are inclinations peculiar to the capacities of human beings.
To know what is right, one must use one's reason and apply it to Aquinas' precepts. This reason is believed to be embodied, in its most abstract form, in the concept of a primary precept: "Good is to be sought, evil avoided." St. Thomas explains that:
there belongs to the natural law, first, certain most general precepts, that are known to all;
 and secondly, certain secondary and more detailed precepts, which are, as it were,
 conclusions following closely from first principles. As to those general principles, the
 natural law, in the abstract, can nowise be blotted out from men's hearts. But it is blotted
 out in the case of a particular action, insofar as reason is hindered from applying the
 general principle to a particular point of practice, on account of concupiscence or some
 other passion, as stated above (77, 2). But as to the other, i.e., the secondary precepts, the
 natural law can be blotted out from the human heart, either by evil persuasions, just as in
 speculative matters errors occur in respect of necessary conclusions; or by vicious customs
 and corrupt habits, as among some men, theft, and even unnatural vices, as the Apostle
 states (Rm. i), were not esteemed sinful.
However, while the primary and immediate precepts cannot be "blotted out", the secondary precepts can be. Therefore, for a deontological ethical theory they are open to a surprisingly large amount of interpretation and flexibility. Any rule that helps man to live up to the primary or subsidiary precepts can be a secondary precept, for example:
Natural moral law is concerned with both exterior and interior acts, also known as action and motive. Simply doing the right thing is not enough; to be truly moral one's motive must be right as well. For example, helping an old lady across the road (good exterior act) to impress someone (bad interior act) is wrong. However, good intentions don't always lead to good actions. The motive must coincide with the cardinal or theological virtues. Cardinal virtues are acquired through reason applied to nature; they are:
The theological virtues are:
According to Aquinas, to lack any of these virtues is to lack the ability to make a moral choice. For example, consider a man who possesses the virtues of justice, prudence, and fortitude, yet lacks temperance. Due to his lack of self-control and desire for pleasure, despite his good intentions, he will find himself swaying from the moral path.
In contemporary jurisprudence.
In jurisprudence, "natural law" can refer to the several doctrines:
Whereas legal positivism would say that a law can be unjust without it being any less a law, a natural law jurisprudence would say that there is something legally deficient about an unjust law. Legal interpretivism, famously defended in the English-speaking world by Ronald Dworkin, claims to have a position different from both natural law and positivism.
Besides utilitarianism and Kantianism, natural law jurisprudence has in common with virtue ethics that it is a live option for a first principles ethics theory in analytic philosophy.
The concept of natural law was very important in the development of the English common law. In the struggles between Parliament and the monarch, Parliament often made reference to the Fundamental Laws of England, which were at times said to embody natural law principles since time immemorial and set limits on the power of the monarchy. According to William Blackstone, however, natural law might be useful in determining the content of the common law and in deciding cases of equity, but was not itself identical with the laws of England. Nonetheless, the implication of natural law in the common law tradition has meant that the great opponents of natural law and advocates of legal positivism, like Jeremy Bentham, have also been staunch critics of the common law.
Natural law jurisprudence is currently undergoing a period of reformulation (as is legal positivism). The most prominent contemporary natural law jurist, Australian John Finnis, is based in Oxford, but there are also Americans Germain Grisez, Robert P. George, and Canadian Joseph Boyle. All have tried to construct a new version of natural law. The 19th-century anarchist and legal theorist, Lysander Spooner, was also a figure in the expression of modern natural law.
"New Natural Law" as it is sometimes called, originated with Grisez. It focuses on "basic human goods," such as human life, knowledge, and aesthetic experience, which are self-evidently and intrinsically worthwhile, and states that these goods reveal themselves as being incommensurable with one another.
The tensions between the natural law and the positive law have played, and continue to play a key role in the development of international law.
References.
</dl>

</doc>
<doc id="22065" url="http://en.wikipedia.org/wiki?curid=22065" title="Nestorianism">
Nestorianism

Nestorianism is a Christological doctrine that emphasizes the disunion between the human and divine natures of Jesus. It was advanced by Nestorius (386–450), Patriarch of Constantinople from 428–431, influenced by Nestorius' studies under Theodore of Mopsuestia at the School of Antioch. Nestorius's teachings brought him into conflict with other prominent church leaders, most notably Cyril of Alexandria, who criticized especially his rejection of the title "Theotokos" ("Bringer forth of God") for the Virgin Mary. Nestorius and his teachings were eventually condemned as heretical at the First Council of Ephesus in 431 and the Council of Chalcedon in 451, leading to the Nestorian Schism, in which churches supporting Nestorius broke with the rest of the Christian Church. Following that, many of Nestorius's supporters relocated to the Sasanian Empire, where they affiliated with the local Christian community, known as the Church of the East. Over the next decades the Church of the East became increasingly Nestorian in doctrine, leading to it becoming known alternately as the Nestorian Church.
Nestorianism is a form of dyophysitism, and can be seen as the antithesis to monophysitism, which emerged in reaction to Nestorianism. Where Nestorianism holds that Christ had two loosely-united natures, divine and human, monophysitism holds that he had but a single nature, his human nature being absorbed into his divinity. A brief definition of Nestorian Christology can be given as: "Jesus Christ, who is not identical with the Son but personally united with the Son, who lives in him, is one hypostasis and one nature: human." Both Nestorianism and monophysitism were condemned as heretical at the Council of Chalcedon. Monophysitism survived and developed into the Miaphysitism of the modern Oriental Orthodox churches.
Following the exodus to Persia, scholars expanded on the teachings of Nestorius and his mentors, particularly after the relocation of the School of Edessa to the Persian city of Nisibis in 489 (where it became known as the School of Nisibis). Nestorianism never again became prominent in the Roman Empire or later Europe, though the diffusion of the Church of the East in and after the 7th century spread it widely across Asia. But not all churches affiliated with the Church of the East appear to have followed Nestorian Christology; indeed, the modern Assyrian Church of the East, which reveres Nestorius, does not follow all historically Nestorian doctrine.
Despite this initial Eastern expansion, the Nestorians' missionary success was eventually deterred. David J Bosch observes
By the end of the fourteenth century, however, the Nestorian and other churches—which at one time had dotted the landscape of all of Central and even parts of East Asia—were all but wiped out. Isolated pockets of Christianity survived only in India. The religious victors on the vast Central Asian mission field of the Nestorians were Islam and Buddhism...
Nestorian doctrine.
Nestorius developed his Christological views as an attempt to rationally explain and understand the incarnation of the divine Logos, the Second Person of the Holy Trinity as the man Jesus Christ. He had studied at the School of Antioch where his mentor had been Theodore of Mopsuestia; Theodore and other Antioch theologians had long taught a literalist interpretation of the Bible and stressed the distinctiveness of the human and divine natures of Jesus. Nestorius took his Antiochene leanings with him when he was appointed Patriarch of Constantinople by Eastern Roman Emperor Theodosius II in 428.
Nestorius' teachings became the root of controversy when he publicly challenged the long-used title "Theotokos" (Bringer forth of God) for the Virgin Mary. He suggested that the title denied Christ's full humanity, arguing instead that Jesus had two persons, the divine Logos and the human Jesus. As such he proposed "Christotokos" (Bringer forth of Christ) as a more suitable title for Mary.
Nestorius' opponents found his teaching too close to the heresy of adoptionism – the idea that Christ had been born a man who had later been "adopted" as God's son. Nestorius was especially criticized by Cyril, Pope (Patriarch) of Alexandria, who argued that Nestorius' teachings undermined the unity of Christ's divine and human natures at the Incarnation. Some of Nestorius' opponents argued that he put too much emphasis on the human nature of Christ, and others debated that the difference that Nestorius implied between the human nature and the divine nature created a fracture in the singularity of Christ, thus creating two Christ figures. Nestorius himself always insisted that his views were orthodox, though they were deemed heretical at the First Council of Ephesus in 431, leading to the Nestorian Schism, when churches supportive of Nestorius and the rest of the Christian Church separated. A more elaborate Nestorian theology developed from there, which came to see Christ as having two natures united, or hypostases, the divine Logos and the human Christ. However, this formulation was never adopted by all churches termed "Nestorian". Indeed, the modern Assyrian Church of the East, which reveres Nestorius, does not fully subscribe to Nestorian doctrine, though it does not employ the title "Theotokos".
Nestorian Schism and early history.
Nestorianism became a distinct sect following the Nestorian Schism, beginning in the 430s. Nestorius had come under fire from Western theologians, most notably Cyril of Alexandria. Cyril had both theological and political reasons for attacking Nestorius; on top of feeling that Nestorianism was an error against true belief, he also wanted to denigrate the head of a competing patriarchate. Cyril and Nestorius asked Pope Celestine I to weigh in on the matter. Celestine found that the title "Theotokos" was orthodox, and authorized Cyril to ask Nestorius to recant. Cyril, however, used the opportunity to further attack Nestorius, who pleaded with Emperor Theodosius II to call a council so that all grievances could be aired.
In 431 Theodosius called the First Council of Ephesus. However, the council ultimately sided with Cyril, holding that Christ is one subsistence and nature, and that the Virgin Mary is the mother of God. The council accused Nestorius of heresy, and deposed him as patriarch. Nestorianism was officially anathematized, a ruling reiterated at the Council of Chalcedon in 451. However, a number of churches, particularly those associated with the School of Edessa, supported Nestorius – though not necessarily his doctrine – and broke with the churches of the West. Many of Nestorius' supporters relocated to Sassanid Persia, home to a vibrant but persecuted Christian minority.
Nestorianism and the Persian Church.
Persia had long been home to a Christian community that had been persecuted by the Zoroastrian majority, which had accused it of Roman leanings. In 424, the Persian Church declared itself independent of the Byzantine and all other churches, in order to ward off allegations of foreign allegiance. Following the Nestorian Schism, the Persian Church increasingly aligned itself with the Nestorians, a measure encouraged by the Zoroastrian ruling class. The Persian Church became increasingly Nestorian in doctrine over the next decades, furthering the divide between Chalcedonian Christianity and the Nestorians. In 486 the Metropolitan of Nisibis, Barsauma, publicly accepted Nestorius' mentor, Theodore of Mopsuestia, as a spiritual authority. In 489 when the School of Edessa in Mesopotamia was closed by Byzantine Emperor Zeno for its Nestorian teachings, the school relocated to its original home of Nisibis, becoming again the School of Nisibis, leading to a wave of Nestorian immigration into Persia. The Persian patriarch Mar Babai I (497–502) reiterated and expanded upon the church's esteem for Theodore, solidifying the church's adoption of Nestorianism.
Now firmly established in Persia, with centers in Nisibis, Ctesiphon, and Gundeshapur, and several metropolitan sees, the Nestorian Persian Church began to branch out beyond the Persian Sassanid Empire. However, through the 6th century the church was frequently beset with internal strife and persecution from the Zoroastrians. The infighting led to a schism, which lasted from 521 until around 539, when the issues were resolved. However, immediately afterward Roman-Persian conflict led to the persecution of the church by the Sassanid King Khosrau I; this ended in 545. The church survived these trials under the guidance of Patriarch Mar Abba I, who had converted to Christianity from Zoroastrianism.
The church emerged stronger after this period of ordeal, and increased missionary efforts farther afield. Missionaries established dioceses in the Arabian Peninsula and India (the Saint Thomas Christians). They made some advances in Egypt, despite the strong Miaphysite (Oriental Orthodox) presence there. Missionaries entered Central Asia and had significant success converting local Turkic tribes. Nestorian missionaries were firmly established in China during the early part of the Tang Dynasty (618–907); the Chinese source known as the Nestorian Stele records a mission under a Persian proselyte named Alopen as introducing Nestorian Christianity to China in 635. Following the Muslim conquest of Persia, completed in 644, the Persian Church became a "dhimmi" community under the Rashidun Caliphate. The church and its communities abroad grew larger under the Caliphate; by the 10th century it had fifteen metropolitan sees within the Caliphate's territories, and another five elsewhere, including in China and India.

</doc>
<doc id="22066" url="http://en.wikipedia.org/wiki?curid=22066" title="NCR">
NCR

NCR may refer to:

</doc>
<doc id="22068" url="http://en.wikipedia.org/wiki?curid=22068" title="Naomi Klein">
Naomi Klein

Naomi Klein (born May 8, 1970) is a Canadian author, social activist, and filmmaker known for her political analyses and criticism of corporate globalization and of corporate capitalism. She is best known for "No Logo", a book that went on to become an international bestseller; "The Take", a documentary film about Argentina’s occupied factories that was written by Klein and directed by her husband Avi Lewis; and "The Shock Doctrine," a bestselling critical analysis of the history of neoliberal economics that was adapted into a six-minute companion film by Alfonso and Jonás Cuarón, as well as a feature length documentary by Michael Winterbottom. Her latest book is "This Changes Everything: Capitalism vs. the Climate", a New York Times non-fiction bestseller and the 2014 winner of the Hilary Weston Writers' Trust Prize for Nonfiction. Klein frequently appears on global and national lists of top influential thinkers, most recently including the 2014 Thought Leaders ranking compiled by the Gottlieb Duttweiler Institute, Prospect magazine's world thinkers 2014 poll, and Maclean's 2014 Power List. She is a member of the board of directors of the climate activist group 350.org.
Family.
Naomi Klein was born in Montreal, Quebec, and brought up in a Jewish family with a history of peace activism. Her parents were self-described "hippies" who moved to Montreal from the U.S. in 1967 as war resisters to the Vietnam War. Her mother, documentary film-maker Bonnie Sherr Klein, is best known for her anti-pornography film "Not a Love Story". Her father, Michael Klein, is a physician and a member of Physicians for Social Responsibility. Her brother, Seth Klein, is director of the British Columbia office of the Canadian Centre for Policy Alternatives.
Her paternal grandparents were communists who began to turn against the Soviet Union after the Molotov-Ribbentrop Pact and had abandoned communism by 1956. In 1942, her grandfather Phil Klein, an animator at Disney, was fired after the Disney animators' strike, and went to work at a shipyard instead. Klein's father grew up surrounded by ideas of social justice and racial equality, but found it "difficult and frightening to be the child of Communists", a so-called red diaper baby.
Klein's husband, Avi Lewis, works as a TV journalist and documentary filmmaker. The couple's first child, son Toma, was born on June 13, 2012.
Early life.
Klein spent much of her teenage years in shopping malls, obsessed with designer labels. As a child and teenager, she found it "very oppressive to have a very public feminist mother" and she rejected politics, instead embracing "full-on consumerism".
She has attributed her change in worldview to two events. One was when she was 17 and preparing for the University of Toronto, her mother had a stroke and became severely disabled. Naomi, her father, and her brother took care of Bonnie through the period in hospital and at home, making educational sacrifices to do so. That year off prevented her "from being such a brat". The next year, after beginning her studies at the University of Toronto, the second event occurred: the 1989 École Polytechnique massacre of female engineering students, which proved to be a wake-up call to feminism.
Klein's writing career started with contributions to "The Varsity", a student newspaper, where she served as editor-in-chief. After her third year at the University of Toronto, she dropped out of university to take a job at the "Toronto Globe and Mail", followed by an editorship at "This Magazine". In 1995, she returned to the University of Toronto with the intention of finishing her degree but left academia for a journalism internship before acquiring the final credits required to complete her degree.
Works.
"No Logo".
In 2000, Klein published the book "No Logo", which for many became a manifesto of the anti-corporate globalization movement. In it, she attacks brand-oriented consumer culture and the operations of large corporations. She also accuses several such corporations of unethically exploiting workers in the world's poorest countries in pursuit of greater profits. In this book, Klein criticized Nike so severely that Nike published a point-by-point response. "No Logo" became an international bestseller, selling over one million copies in over 28 languages.
"Fences and Windows".
In 2002, Klein published "Fences and Windows", a collection of her articles and speeches written on behalf of the anti-globalization movement (all proceeds from the book go to benefit activist organizations through The Fences and Windows Fund).
"The Take".
In 2004, Klein and her husband, Avi Lewis, released a documentary film called "The Take" about factory workers in Argentina who took over a closed plant and resumed production, operating as a collective. The first African screening was in the Kennedy Road shack settlement in the South African city of Durban, where the Abahlali baseMjondolo movement began.
At least one article in "Z Communications" criticized "The Take" for its portrayal of the Argentine general and politician Juan Domingo Perón, which they felt portrayed him as a social democrat.
"The Shock Doctrine".
Klein's third book, "The Shock Doctrine: The Rise of Disaster Capitalism", was published on September 4, 2007, becoming an international and New York Times bestseller translated into 28 languages. The book argues that the free market policies of Nobel Laureate Milton Friedman and the Chicago School of Economics have risen to prominence in countries such as Chile, under Pinochet, Poland, Russia, under Yeltsin, and the United States (for example, the privatization of the New Orleans Public Schools after Hurricane Katrina). The book also argues that policy initiatives (for instance, the privatization of Iraq's economy under the Coalition Provisional Authority) were rushed through while the citizens of these countries were in shock from disasters, upheavals, or invasion.
Central to the book's thesis is the contention that those who wish to implement unpopular free market policies now routinely do so by taking advantage of certain features of the aftermath of major disasters, be they economic, political, military or natural. The suggestion is that when a society experiences a major 'shock' there is a widespread desire for a rapid and decisive response to correct the situation; this desire for bold and immediate action provides an opportunity for unscrupulous actors to implement policies which go far beyond a legitimate response to disaster. The book suggests that when the rush to act means the specifics of a response will go unscrutinized, that is the moment when unpopular and unrelated policies will intentionally be rushed into effect. The book appears to claim that these shocks are in some cases intentionally encouraged or even manufactured.
Klein identifies the "shock doctrine", elaborating on Joseph Schumpeter, as the latest in capitalism's phases of "creative destruction".
"The Shock Doctrine" was adapted into a short film of the same name, released onto YouTube. The film was directed by Jonás Cuarón, produced and co-written by his father Alfonso Cuarón. The video has been viewed over one million times.
The publication of "The Shock Doctrine" increased Klein's prominence, with the "New Yorker" judging her "the most visible and influential figure on the American left—what Howard Zinn and Noam Chomsky were thirty years ago." On February 24, 2009, the book was awarded the inaugural Warwick Prize for Writing from the University of Warwick in England. The prize carried a cash award of £50,000.
"This Changes Everything: Capitalism vs. the Climate".
Klein's fourth book, "This Changes Everything: Capitalism vs. the Climate" was published in September 2014. The book puts forth the argument that the hegemony of neoliberal market fundamentalism is blocking any serious reforms to halt climate change and protect the environment. The book won the 2014 Hilary Weston Writers' Trust Prize for Nonfiction, and was a shortlisted nominee for the 2015 Shaughnessy Cohen Prize for Political Writing.
Iraq War criticism.
Klein has written on various current issues, such as the Iraq War. In a September 2004 article for "Harper's Magazine", she argues that, contrary to popular belief, the Bush administration did have a clear plan for post-invasion Iraq, which was to build a completely unconstrained free market economy. She describes plans to allow foreigners to extract wealth from Iraq, and the methods used to achieve those goals. The 2008 film "War, Inc." was partially inspired by her article, "Baghdad Year Zero".
Klein's August 2004 "Bring Najaf to New York", published in "The Nation", argued that Muqtada Al Sadr's Mahdi Army "represents the overwhelmingly mainstream sentiment in Iraq." She went on to say "Yes, if elected Sadr would try to turn Iraq into a theocracy like Iran, but for now his demands are for direct elections and an end to foreign occupation". Marc Cooper, a former "Nation" columnist, attacked the assertion that Al Sadr represented mainstream Iraqi sentiment and that American forces had brought the fight to the holy city of Najaf. Cooper wrote that "Klein should know better. All enemies of the U.S. occupation she opposes are not her friends. Or ours. Or those of the Iraqi people. I don’t think that Mullah Al Sadr, in any case, is much desirous of support issuing from secular Jewish feminist-socialists."
Criticism of Israeli policies.
In March 2008, Klein was the keynote speaker at the first national conference of the Alliance of Concerned Jewish Canadians. In January 2009, during the Gaza War, Klein supported the Boycott, Divestment and Sanctions (BDS) campaign against Israel, arguing that "the best strategy to end the increasingly bloody occupation is for Israel to become the target of the kind of global movement that put an end to apartheid in South Africa."
In summer 2009, on the occasion of the publication of the Hebrew translation of her book "The Shock Doctrine", Klein visited Israel, the West Bank, and Gaza, combining the promotion of her book and the BDS campaign. In an interview to the Israeli newspaper "Ha'aretz" she emphasized that it is important to her "not to boycott Israelis but rather to boycott the normalization of Israel and the conflict." In a speech in Ramallah on 27 June, she apologized to the Palestinians for not joining the BDS campaign earlier. Her remarks, particularly that "[Some Jews] even think we get one get-away-with-genocide-free-card" were characterized by an op-ed columnist in the "Jerusalem Post" as "violent" and "unethical", and as the "most perverse of aspersions on Jews, an age-old stereotype of Jews as intrinsically evil and malicious."
Klein was also a spokesperson for the protest against the spotlight on Tel Aviv at the 2009 Toronto International Film Festival, a spotlight that Klein said was a very selective and misleading portrait of Israel.
Environmentalism.
Since 2009, Klein’s attention has turned to environmentalism, with particular focus on climate change, the subject of her 2014 book "This Changes Everything". According to her website, the book and a new film will be about "how the climate crisis can spur economic and political transformation." She sits on the board of directors of campaign group 350.org and took part in their 'Do the Math' tour in 2013, encouraging a divestment movement.
She has encouraged the Occupy movement to join forces with the environmental movement, saying the financial crisis and the climate crisis have the same root – unrestrained corporate greed. She gave a speech at Occupy Wall Street where she described the world as ‘upside down’, where we act as if ‘there is no end to what is actually finite—fossil fuels and the atmospheric space to absorb their emissions’, and as if there are ‘limits to what is actually bountiful—the financial resources to build the kind of society we need.
She has been a particularly vocal critic of the Athabasca oil sands in Alberta, describing it in a TED talk as a form of ‘terrestrial skinning’. On September 2, 2011, she attended the demonstration against the Keystone XL pipeline outside the White House and was arrested. Klein celebrated Obama’s decision to postpone a decision on the Keystone pipeline until 2013 pending an environmental review as a victory for the environmental movement.
She attended the Copenhagen Climate Summit of 2009. She put the blame for the failure of Copenhagen on Barack Obama, and described her own country, Canada, as a ‘climate criminal’. She presented the Angry Mermaid Award (a satirical award designed to recognise the corporations who have best sabotaged the climate negotiations) to Monsanto.
Writing in the wake of Hurricane Sandy she warned that the climate crisis constitutes a massive opportunity for disaster capitalists and corporations seeking to profit from crisis. But equally, the climate crisis 'can be a historic moment to usher in the next great wave of progressive change', or a so-called 'People's Shock'.
Other activities.
Klein contributes to "The Nation", "In These Times", "The Globe and Mail", "This Magazine", "Harper's Magazine", and "The Guardian".
She once lectured as a Miliband Fellow at the London School of Economics as an award-winning journalist, writer on the anti-globalisation movement. Klein ranked 11th in an internet poll of the top global intellectuals of 2005, a list of the world's top 100 public intellectuals compiled by the "Prospect" magazine in conjunction with "Foreign Policy" magazine. She was involved in 2010 G-20 Toronto summit protests, condemning police force and brutality. She spoke to a rally seeking the release of protesters in front of police headquarters on June 28, 2010.
In May 2011, Klein received an honorary degree from Saint Thomas University. On October 6, 2011, she visited Occupy Wall Street and gave a speech declaring the protest movement "the most important thing in the world". On November 10, 2011, she participated in a panel discussion about the future of Occupy Wall Street with four other panelists, including Michael Moore, William Greider, and Rinku Sen, in which she stressed the crucial nature of the evolving movement.

</doc>
<doc id="22071" url="http://en.wikipedia.org/wiki?curid=22071" title="Nonsteroidal anti-inflammatory drug">
Nonsteroidal anti-inflammatory drug

Nonsteroidal anti-inflammatory drugs (usually abbreviated to NSAIDs ), also called nonsteroidal anti-inflammatory agents/analgesics (NSAIAs) or nonsteroidal anti-inflammatory medicines (NSAIMs), are a class of drugs that provides analgesic (pain-killing) and antipyretic (fever-reducing) effects, and, in higher doses, anti-inflammatory effects.
The term "nonsteroidal" distinguishes these drugs from steroids, which, among a broad range of other effects, have a similar eicosanoid-depressing, anti-inflammatory action. As analgesics, NSAIDs are unusual in that they are non-narcotic and thus are used as a non-addictive alternative to narcotics.
The most prominent members of this group of drugs, aspirin, ibuprofen and naproxen, are all available over the counter in most countries. Paracetamol (acetaminophen) is generally not considered an NSAID because it has only little anti-inflammatory activity. It treats pain mainly by blocking COX-2 mostly in the central nervous system, but not much in the rest of the body.
NSAIDs inhibit the activity of both cyclooxygenase-1 (COX-1) and cyclooxygenase-2 (COX-2), and thereby, the synthesis of prostaglandins and thromboxanes. It is thought that inhibiting COX-2 leads to the anti-inflammatory, analgesic and antipyretic effects and that those NSAIDs also inhibiting COX-1, particularly aspirin, may cause gastrointestinal bleeding and ulcers.
Medical uses.
NSAIDs are usually used for the treatment of acute or chronic conditions where pain and inflammation are present. Research continues into their potential for prevention of colorectal cancer.
NSAIDs are generally used for the symptomatic relief of the following conditions:
Aspirin, the only NSAID able to irreversibly inhibit COX-1, is also indicated for inhibition of platelet aggregation. This is useful in the management of arterial thrombosis and prevention of adverse cardiovascular events. Aspirin inhibits platelet aggregation by inhibiting the action of thromboxane A2.
Contraindications.
NSAIDs may be used with caution by people with the following conditions:
NSAIDs should usually be avoided by people with the following conditions:
Adverse effects.
The widespread use of NSAIDs has meant that the adverse effects of these drugs have become increasingly prevalent. Use of NSAIDs increases risk of having a range of gastrointestinal (GI) problems. When NSAIDs are used for pain management after surgery they cause increased risk of kidney problems.
An estimated 10-20% of NSAID patients experience dyspepsia. In the 1990s high doses of prescription NSAIDs were associated with serious upper gastrointestinal adverse events, including bleeding. Over the past decade, deaths associated with gastric bleeding have declined.
NSAIDs, like all drugs, may interact with other medications. For example, concurrent use of NSAIDs and quinolones may increase the risk of quinolones' adverse central nervous system effects, including seizure.
Combinational risk.
If a COX-2 inhibitor is taken, a traditional NSAID (prescription or over-the-counter) should not be taken at the same time. In addition, people on daily aspirin therapy (e.g., for reducing cardiovascular risk) must be careful if they also use other NSAIDs, as these may inhibit the cardioprotective effects of aspirin.
Rofecoxib was shown to produce significantly fewer gastrointestinal ADRs compared with naproxen. This study, the VIGOR trial, raised the issue of the cardiovascular safety of the coxibs. A statistically insignificant increase in the incidence of myocardial infarctions was observed in patients on rofecoxib. Further data, from the APPROVe trial, showed a statistically significant relative risk of cardiovascular events of 1.97 versus placebo—which caused a worldwide withdrawal of rofecoxib in October 2004.
Use of methotrexate together with NSAIDS in rheumatoid arthritis is safe, if adequate monitoring is done.
Cardiovascular.
NSAIDs aside from aspirin, both newer selective COX-2 inhibitors and traditional anti-inflammatories, increase the risk of myocardial infarction and stroke. They are not recommended in those who have had a previous heart attack as they increase the risk of death and / or recurrent MI. Evidence indicates that naproxen may be the least harmful out of these.
NSAIDs aside from (low-dose) aspirin are associated with a doubled risk of heart failure in people without a history of cardiac disease. In people with such a history, use of NSAIDs (aside from low-dose aspirin) was associated with a more than 10-fold increase in heart failure. If this link is proven causal, researchers estimate that NSAIDs would be responsible for up to 20 percent of hospital admissions for congestive heart failure. In people with heart failure, NSAIDs increase mortality risk (hazard ratio) by approximately 1.2-1.3 for naproxen and ibuprofen, 1.7 for rofecoxib and celecoxib, and 2.1 for diclofenac.
Possible erectile dysfunction risk.
A 2005 Finnish study linked long term (over 3 months) use of NSAIDs with an increased risk of erectile dysfunction. This study was correlational only, and depended solely on self-reports (questionnaires).
A 2011 publication in the Journal of Urology received widespread publicity. According to this study, men who used NSAIDs regularly were at significantly increased risk of erectile dysfunction. A link between NSAID use and erectile dysfunction still existed after controlling for several conditions. However, the study was observational and not controlled, with low original participation rate, potential participation bias, and other uncontrolled factors. The authors warned against drawing any conclusion regarding cause.
Gastrointestinal.
The main adverse drug reactions (ADRs) associated with NSAID use relate to direct and indirect irritation of the gastrointestinal (GI) tract. NSAIDs cause a dual assault on the GI tract: the acidic molecules directly irritate the gastric mucosa, and inhibition of COX-1 and COX-2 reduces the levels of protective prostaglandins. Inhibition of prostaglandin synthesis in the GI tract causes increased gastric acid secretion, diminished bicarbonate secretion, diminished mucus secretion and diminished trophic effects on epithelial mucosa.
Common gastrointestinal ADRs include:
Clinical NSAID ulcers are related to the systemic effects of NSAID administration. Such damage occurs irrespective of the route of administration of the NSAID (e.g., oral, rectal, or parenteral) and can occur even in patients with achlorhydria.
Ulceration risk increases with therapy duration, and with higher doses. To minimise GI ADRs, it is prudent to use the lowest effective dose for the shortest period of time—a practice that studies show is often not followed. Recent studies show that over 50% of patients who take NSAIDs have sustained some mucosal damage to their small intestine.
There are also some differences in the propensity of individual agents to cause gastrointestinal ADRs. Indomethacin, ketoprofen and piroxicam appear to have the highest prevalence of gastric ADRs, while ibuprofen (lower doses) and diclofenac appear to have lower rates.
Certain NSAIDs, such as aspirin, have been marketed in enteric-coated formulations that manufacturers claim reduce the incidence of gastrointestinal ADRs. Similarly, some believe that rectal formulations may reduce gastrointestinal ADRs. However, considering the mechanism of such ADRs, and in clinical practice, these formulations have not demonstrated a reduced risk of GI ulceration.
Commonly, gastric (but not necessarily intestinal) adverse effects can be reduced through suppressing acid production, by concomitant use of a proton pump inhibitor, e.g., omeprazole, esomeprazole; or the prostaglandin analogue misoprostol. Misoprostol is itself associated with a high incidence of gastrointestinal ADRs (diarrhea). While these techniques may be effective, they are expensive for maintenance therapy.
Inflammatory bowel disease.
NSAIDs should be used with caution in individuals with inflammatory bowel disease (e.g., Crohn's disease or ulcerative colitis) due to their tendency to cause gastric bleeding and form ulceration in the gastric lining. Pain relievers such as paracetamol (also known as acetaminophen) or drugs containing codeine (which slows down bowel activity) are safer medications for pain relief in IBD.
Renal.
NSAIDs are also associated with a relatively high incidence of renal adverse drug reactions (ADRs). The mechanism of these renal ADRs is due to changes in renal haemodynamics (kidney blood flow), ordinarily mediated by prostaglandins, which are affected by NSAIDs. Prostaglandins normally cause vasodilation of the afferent arterioles of the glomeruli. This helps maintain normal glomerular perfusion and glomerular filtration rate (GFR), an indicator of renal function. This is particularly important in renal failure where the kidney is trying to maintain renal perfusion pressure by elevated angiotensin II levels. At these elevated levels, angiotensin II also constricts the afferent arteriole into the glomerulus in addition to the efferent arteriole it normally constricts. Prostaglandins serve to dilate the afferent arteriole; by blocking this prostaglandin-mediated effect, particularly in renal failure, NSAIDs cause unopposed constriction of the afferent arteriole and decreased renal perfusion pressure.
Common ADRs associated with altered renal function include:
These agents may also cause renal impairment, especially in combination with other nephrotoxic agents. Renal failure is especially a risk if the patient is also concomitantly taking an ACE inhibitor (which removes angiotensin II's vasoconstriction of the efferent arteriole) and a diuretic (which drops plasma volume, and thereby RPF)—the so-called "triple whammy" effect.
In rarer instances NSAIDs may also cause more severe renal conditions:
NSAIDs in combination with excessive use of phenacetin and/or paracetamol (acetaminophen) may lead to analgesic nephropathy.
Photosensitivity.
Photosensitivity is a commonly overlooked adverse effect of many of the NSAIDs. The 2-arylpropionic acids are the most likely to produce photosensitivity reactions, but other NSAIDs have also been implicated including piroxicam, diclofenac and benzydamine.
Benoxaprofen, since withdrawn due to its hepatotoxicity, was the most photoactive NSAID observed. The mechanism of photosensitivity, responsible for the high photoactivity of the 2-arylpropionic acids, is the ready decarboxylation of the carboxylic acid moiety. The specific absorbance characteristics of the different chromophoric 2-aryl substituents, affects the decarboxylation mechanism. While ibuprofen has weak absorption, it has been reported as a weak photosensitising agent.
During pregnancy.
NSAIDs are not recommended during pregnancy, particularly during the third trimester. While NSAIDs as a class are not direct teratogens, they may cause premature closure of the fetal ductus arteriosus and renal ADRs in the fetus. Additionally, they are linked with premature birth and miscarriage. Aspirin, however, is used together with heparin in pregnant women with antiphospholipid antibodies. Additionally, Indomethacin is used in pregnancy to treat polyhydramnios by reducing fetal urine production via inhibiting fetal renal blood flow.
In contrast, paracetamol (acetaminophen) is regarded as being safe and well-tolerated during pregnancy, but Leffers et al. released a study in 2010 indicating that there may be associated male infertility in the unborn. Doses should be taken as prescribed, due to risk of hepatotoxicity with overdoses.
In France, the country's health agency contraindicates the use of NSAIDs, including aspirin, after the sixth month of pregnancy.
Other.
Common adverse drug reactions (ADR), other than listed above, include: raised liver enzymes, headache, dizziness. Uncommon ADRs include: hyperkalaemia, confusion, bronchospasm, rash. Rapid and severe swelling of the face and/or body. Ibuprofen may also rarely cause irritable bowel syndrome symptoms. NSAIDs are also implicated in some cases of Stevens–Johnson syndrome.
Most NSAIDs penetrate poorly into the central nervous system (CNS). However, the COX enzymes are expressed constitutively in some areas of the CNS, meaning that even limited penetration may cause adverse effects such as somnolence and dizziness.
In very rare cases, ibuprofen can cause aseptic meningitis.
As with other drugs, allergies to NSAIDs might exist. While many allergies are specific to one NSAID, up to 1 in 5 people may have unpredictable cross-reactive allergic responses to other NSAIDs as well.
Drug interactions.
NSAIDs reduce renal blood flow and thereby decrease the efficacy of diuretics, and inhibit the elimination of lithium and methotrexate.
NSAIDs cause hypocoagulability, which may be serious when combined with other drugs that also decrease blood clotting, such as warfarin.
NSAIDs may aggravate hypertension (high blood pressure) and thereby antagonize the effect of antihypertensives, such as ACE Inhibitors.
NSAIDs may interfere and reduce efficiency of SSRI antidepressants
Various widely used nonsteroidal anti-inflammatory drugs (NSAIDs) enhance endocannabinoid signaling by blocking the anandamide-degrading membrane enzyme fatty acid amide hydrolase (FAAH).
Mechanism of action.
Most NSAIDs act as nonselective inhibitors of the enzyme cyclooxygenase (COX), inhibiting both the cyclooxygenase-1 (COX-1) and cyclooxygenase-2 (COX-2) isoenzymes. This inhibition is competitively reversible (albeit at varying degrees of reversibility), as opposed to the mechanism of aspirin, which is irreversible inhibition. COX catalyzes the formation of prostaglandins and thromboxane from arachidonic acid (itself derived from the cellular phospholipid bilayer by phospholipase A2). Prostaglandins act (among other things) as messenger molecules in the process of inflammation. This mechanism of action was elucidated by John Vane (1927–2004), who received a Nobel Prize for his work (see Mechanism of action of aspirin).
COX-1 is a constitutively expressed enzyme with a "house-keeping" role in regulating many normal physiological processes. One of these is in the stomach lining, where prostaglandins serve a protective role, preventing the stomach mucosa from being eroded by its own acid. COX-2 is an enzyme facultatively expressed in inflammation, and it is inhibition of COX-2 that produces the desirable effects of NSAIDs.
When nonselective COX-1/COX-2 inhibitors (such as aspirin, ibuprofen, and naproxen) lower stomach prostaglandin levels, ulcers of the stomach or duodenum internal bleeding can result.
NSAIDs have been studied in various assays to understand how they affect each of these enzymes. While the assays reveal differences, unfortunately different assays provide differing ratios.
The discovery of COX-2 led to research to development of selective COX-2 inhibiting drugs that do not cause gastric problems characteristic of older NSAIDs.
Paracetamol (acetaminophen) is not considered an NSAID because it has little anti-inflammatory activity. It treats pain mainly by blocking COX-2 mostly in the central nervous system, but not much in the rest of the body.
However, many aspects of the mechanism of action of NSAIDs remain unexplained, and for this reason further COX pathways are hypothesized. The COX-3 pathway was believed to fill some of this gap but recent findings make it appear unlikely that it plays any significant role in humans and alternative explanation models are proposed.
NSAIDs are also used in the acute pain caused by gout because they inhibit urate crystal phagocytosis besides inhibition of prostaglandin synthase.
Antipyretic activity.
NSAIDS have antipyretic activity and can be used to treat fever. Fever is caused by elevated levels of prostaglandin E2, which alters the firing rate of neurons within the hypothalamus that control thermoregulation. Antipyretics work by inhibiting the enzyme COX, which causes the general inhibition of prostanoid biosynthesis (PGE2) within the hypothalamus. PGE2 signals to the hypothalamus to increase the body's thermal set point. Ibuprofen has been shown more effective as an antipyretic than paracetamol (acetaminophen).
Arachidonic acid is the precursor substrate for cyclooxygenase leading to the production of prostaglandins F, D & E.
Classification.
NSAIDs can be classified based on their chemical structure or mechanism of action. Older NSAIDs were known long before their mechanism of action was elucidated and were for this reason classified by chemical structure or origin. Newer substances are more often classified by mechanism of action.
Chirality.
Most NSAIDs are chiral molecules (diclofenac is a notable exception). However, the majority are prepared in a racemic mixture. Typically, only a single enantiomer is pharmacologically active. For some drugs (typically profens), an isomerase enzyme "in vivo" converts the inactive enantiomer into the active form, although its activity varies widely in individuals. This phenomenon is likely responsible for the poor correlation between NSAID efficacy and plasma concentration observed in older studies, when specific analysis of the active enantiomer was not performed.
Ibuprofen and ketoprofen are now available in single, active enantiomer preparations (dexibuprofen and dexketoprofen), which purport to offer quicker onset and an improved side-effect profile. Naproxen has always been marketed as the single active enantiomer.
Main practical differences.
NSAIDs within a group tend to have similar characteristics and tolerability. There is little difference in clinical efficacy among the NSAIDs when used at equivalent doses. Rather, differences among compounds usually relate to dosing regimens (related to the compound's elimination half-life), route of administration, and tolerability profile.
Regarding adverse effects, selective COX-2 inhibitors have lower risk of gastrointestinal bleeding, but a substantially more increased risk of myocardial infarction than the increased risk from nonselective inhibitors. Some data also supports that the partially selective nabumetone is less likely to cause gastrointestinal events. The nonselective naproxen appears risk-neutral with regard to cardiovascular events.
A consumer report noted that ibuprofen, naproxen, and salsalate are less expensive than other NSAIDs, and essentially as effective and safe when used appropriately to treat osteoarthritis and pain.
Pharmacokinetics.
Most nonsteroidal anti-inflammatory drugs are weak acids, with a pKa of 3-5. They are absorbed well from the stomach and intestinal mucosa. They are highly protein-bound in plasma (typically >95%), usually to albumin, so that their volume of distribution typically approximates to plasma volume. Most NSAIDs are metabolised in the liver by oxidation and conjugation to inactive metabolites that typically are excreted in the urine, though some drugs are partially excreted in bile. Metabolism may be abnormal in certain disease states, and accumulation may occur even with normal dosage.
Ibuprofen and diclofenac have short half-lives (2–3 hours). Some NSAIDs (typically oxicams) have very long half-lives (e.g. 20–60 hours).
History.
From the era of Greek medicine to the mid-19th century, the discovery of medicinal agents was classed as an empirical art; folklore and mythological guidance were combined in deploying the vegetable and mineral products that made up the expansive pharmacopoeia of the time. Myrtle leaves were in use by 1500 BCE. Hippocrates (460 – 377 BCE) first reported using willow bark and in 30 BCE Celsus described the signs of inflammation and also used willow bark to mitigate them. On April 25, 1763, Edward Stone wrote to the Royal Society describing his observations on the use of willow bark-based medicines in febrile patients. The active ingredient of willow bark, a glycoside called salicin, was first isolated by Johann Andreas Buchner in 1827. By 1829, French chemist Henri Leroux had improved the extraction process to obtain about 30g of purified salicin from 1.5 kg of bark.
By hydrolysis, salicin releases glucose and salicylic alcohol which can be converted into salicylic acid, both in vivo and through chemical methods. The acid is more effective than salicin and, in addition to its fever-reducing properties, is anti-inflammatory and analgesic. In 1869, Hermann Kolbe synthesised salicylate, although it was too acidic for the gastric mucosa. The reaction used to synthesise aromatic acid from a phenol in the presence of CO2 is known as the Kolbe-Schmitt reaction.
By 1897 the German chemist Felix Hoffmann and the Bayer company prompted a new age of pharmacology by converting salicylic acid into acetylsalicylic acid—named aspirin by Heinrich Dreser. Other NSAIDs were developed from the 1950s forward.
In 2001, NSAIDs accounted for 70,000,000 prescriptions and 30 billion over-the-counter doses sold annually in the United States.
Veterinary use.
Research supports the use of NSAIDs for the control of pain associated with veterinary procedures such as dehorning and castration of calves. The best effect is obtained by combining a short-term local anesthetic such as lidocaine with an NSAID acting as a longer term analgesic. However, as different species have varying reactions to different medications in the NSAID family, little of the existing research data can be extrapolated to animal species other than those specifically studied, and the relevant government agency in one area sometimes prohibits uses approved in other jurisdictions.
For example, ketoprofen's effects have been studied in horses more than in ruminants but, due to controversy over its use in racehorses, veterinarians who treat livestock in the United States more commonly prescribe flunixin meglumine, which, while labeled for use in such animals, is not indicated for post-operative pain.
In the United States, meloxicam is approved for use only in canines, whereas (due to concerns about liver damage) it carries warnings against its use in cats except for one-time use during surgery. In spite of these warnings, meloxicam is frequently prescribed "off-label" for non-canine animals including cats and livestock species. In other countries, for example The European Union (EU), there is a label claim for use in cats.

</doc>
<doc id="22073" url="http://en.wikipedia.org/wiki?curid=22073" title="NC (complexity)">
NC (complexity)

In complexity theory, the class NC (for "Nick's Class") is the set of decision problems decidable in polylogarithmic time on a parallel computer with a polynomial number of processors. In other words, a problem is in NC if there exist constants "c" and "k" such that it can be solved in time "O"(log"c" "n") using "O"("n""k") parallel processors. Stephen Cook coined the name "Nick's class" after Nick Pippenger, who had done extensive research on circuits with polylogarithmic depth and polynomial size.
Just as the class P can be thought of as the tractable problems (Cobham's thesis), so NC can be thought of as the problems that can be efficiently solved on a parallel computer. NC is a subset of P because polylogarithmic parallel computations can be simulated by polynomial-time sequential ones. It is unknown whether NC = P, but most researchers suspect this to be false, meaning that there are probably some tractable problems that are "inherently sequential" and cannot significantly be sped up by using parallelism. Just as the class NP-complete can be thought of as "probably intractable", so the class P-complete, when using NC reductions, can be thought of as "probably not parallelizable" or "probably inherently sequential".
The parallel computer in the definition can be assumed to be a "parallel, random-access machine" (PRAM). That is a parallel computer with a central pool of memory, and any processor can access any bit of memory in constant time. The definition of NC is not affected by the choice of how the PRAM handles simultaneous access to a single bit by more than one processor. It can be CRCW, CREW, or EREW. See PRAM for descriptions of those models.
Equivalently, NC can be defined as those decision problems decidable by a uniform Boolean circuit (which can be calculated from the length of the input) with polylogarithmic depth and a polynomial number of gates.
RNC is a class extending NC with access to randomness.
Problems in NC.
As with P, by a slight abuse of language, one might classify function problems and search problems as being in NC. NC is known to include many problems, including
Often algorithms for those problems had to be separately invented and could not be naïvely adapted from well-known algorithms – Gaussian elimination and Euclidean algorithm rely on operations performed in sequence. One might contrast ripple carry adder with a carry-lookahead adder.
The NC hierarchy.
NC"i" is the class of decision problems decidable by uniform boolean circuits with a polynomial number of gates of at most two inputs and depth "O"(log"i" "n"), or the class of decision problems solvable in time "O"(log"i" "n") on a parallel computer with a polynomial number of processors. Clearly, we have
which forms the NC-hierarchy.
We can relate the NC classes to the space classes L and NL and AC.
The NC classes are related to the AC classes, which are defined similarly, but with gates having unbounded fanin. For each "i", we have
As an immediate consequence of this, we have that NC = AC.
It is known that both inclusions are strict for "i" = 0.
Similarly, we have that NC is equivalent to the problems solvable on an alternating Turing machine restricted to at most two options at each step with "O"(log "n") space and formula_4 alternations.
Open problem: Is NC proper?
One major open question in complexity theory is whether or not every containment in the NC hierarchy is proper. It was observed by Papadimitriou that, if NC"i" = NC"i"+1 for some "i", then NC"i" = NC"j" for all "j" ≥ "i", and as a result, NC"i" = NC. This observation is known as NC-hierarchy collapse because even a single equality in the chain of containments
implies that the entire NC hierarchy "collapses" down to some level "i". Thus, there are 2 possibilities:
It is widely believed that (1) is the case, although no proof as to the truth of either statement has yet been discovered.
Barrington's theorem.
A branching program with "n" variables of width "k" and length "m" consists of a sequence of "m" instructions. Each of the instructions is a tuple ("i", "p", "q") where "i" is the index of variable to check (1 ≤ "i" ≤ "n"), and "p" and "q" are functions from {1, 2, ..., "k"} to {1, 2, ..., "k"}. Numbers 1, 2, ..., "k" are called states of the branching program. The program initially starts in state 1, and each instruction ("i", "p", "q") changes the state from "x" to "p"("x") or "q"("x"), depending on whether the "i"th variable is 0 or 1.
A family of branching programs consists of a branching program with "n" variables for each "n".
It is easy to show that every language "L" on {0,1} can be recognized by a family of branching programs of width 4 and exponential length, or by a family of exponential width and linear length.
Every regular language on {0,1} can be recognized by a family of branching programs of constant width and linear number of instructions (since a DFA can be converted to a branching program). BWBP denotes the class of languages recognizable by a family of branching programs of bounded width and polynomial length.
Barrington's theorem says that is exactly nonuniform NC1. The proof uses the nonsolvability of the symmetric group S5.
The theorem is rather surprising. For instance, it implies that the majority function can be computed by a family of branching programs of constant width and polynomial size, while intuition might suggest that to achieve polynomial size, one needs a linear number of states. 
Proof of Barrington's theorem.
A branching program of constant width and polynomial size can be easily converted (via divide-and-conquer) to a circuit in NC1.
Conversely, suppose a circuit in NC1 is given. Without loss of generality, assume it uses only AND and NOT gates.
Lemma 1: If there exists a branching program that sometimes works as a permutation "P" and sometimes as "Q", by right-multiplying permutations in the first instruction by α, and in the last instruction left-multiplying by β, we can make a circuit of the same length that behaves as β"P"α or β"Q"α, respectively.
Call a branching program α-computing a circuit "C" if it works as identity when C's output is 0, and as α when C's output is 1.
As a consequence of lemma 1 and the fact that all cycles of length 5 are conjugate, for any two 5-cycles α, β, if there exists a branching program α-computing a circuit "C", then there exists a branching program β-computing the circuit "C", of the same length.
Lemma 2: There exist 5-cycles γ, δ such that their commutator formula_8 is a 5-cycle. For example, γ = (1 2 3 4 5), δ = (1 3 5 4 2).
We will now prove Barrington's theorem by induction.
Assume that for all subcircuits "D" of "C" and 5-cycles α, there exists a branching program α-computing "D". We will show that for all 5-cycles α, there exists a branching program α-computing "C".
The size of the branching program is at most formula_18, where "d" is the depth of the circuit. If the circuit has logarithmic depth, the branching program has polynomial length.

</doc>
<doc id="22076" url="http://en.wikipedia.org/wiki?curid=22076" title="Nori">
Nori

Nori (海苔) is the Japanese name for edible seaweed species of the red algae genus "Porphyra", including "P. yezoensis" and "P. tenera". Nori is familiar in the United States and other countries as an ingredient of sushi, being referred to as "nori" (as the Japanese do) or simply as seaweed. Finished products are made by a shredding and rack-drying process that resembles papermaking.
Porphyra is also called "laver" in Wales and other English-speaking countries.
History.
Originally, the term "nori" was generic and referred to seaweeds including "hijiki". One of the oldest descriptions of nori is dated to around the 8th century. In the Taihō Code enacted in 701, "nori" was already included in the form of taxation. Local people have been described as drying nori in Hitachi Province Fudoki (721–721), and nori was harvested in Izumo Province Fudoki (713–733), showing that nori was used as food from ancient times. In "Utsubo Monogatari", written around 987, "nori" was recognized as a common food. The original "nori" was formed as a paste, and the sheet form was invented in Asakusa, Edo (contemporary Tokyo), in the Edo period through the method of Japanese paper-making.
The word "nori" first appeared in an English-language publication in "C. P. Thunberg's Trav.", published in 1796. It was used in conjugation as "Awa nori", probably referring to what is now called aonori.
The Japanese nori industry was in decline after WWII, when Japan was in need of all food which could be produced. The decline was due to a lack of understanding of the plant's three stage life cycle so that local people did not understand why traditional cultivation methods were not effective. The industry was rescued by knowledge deriving from the work of British phycologist, Kathleen Mary Drew-Baker who had been researching the organism porphyria umbilicalis, which grew in the seas around Wales and was harvested for food, as in Japan. Her work was discovered by Japanese scientists who applied it to artificial methods of seeding and growing the plants, rescuing the industry. Kathleen Baker was hailed as the 'Mother of the Sea' in Japan and a statue erected in her memory.
The word "nori" started to be used widely in the United States, and the product (imported in dry form from Japan) became widely available at natural food stores and Asian-American grocery stores in the 1960s due to the macrobiotic movement, and in the 1970s with the increase of sushi bars and Japanese restaurants.
In one study by Jan-Hendrik Hehemann, subjects of Japanese descent have been shown to be able to digest the polysaccharide of the seaweed, after gut microbes developed the enzyme from marine bacteria. Gut microbes from the North American subjects lacked these enzymes.
Production.
Production and processing of "nori" is an advanced form of agriculture. The biology of "Porphyra", although complicated, is well understood, and this knowledge is used to control the production process. Farming takes place in the sea where the "Porphyra" plants grow attached to nets suspended at the sea surface and where the farmers operate from boats. The plants grow rapidly, requiring about 45 days from "seeding" until the first harvest. Multiple harvests can be taken from a single seeding, typically at about ten-day intervals. Harvesting is accomplished using mechanical harvesters of a variety of configurations. Processing of raw product is mostly accomplished by highly automated machines that accurately duplicate traditional manual processing steps, but with much improved efficiency and consistency. The final product is a paper-thin, black, dried sheet of approximately 18 × and 3 g in weight.
Several grades of "nori" are available in the United States. The most common, and least expensive, grades are imported from China, costing about six cents per sheet. At the high end, ranging up to 90 cents per sheet, are "delicate "shin-nori"" ("nori" from the first of the year's several harvests) cultivated in Ariake Sea, off the island of Kyushu in Japan".
In Japan, over 600 km2 of Japanese coastal waters are given to producing 350000 t of "nori", worth over a billion dollars. China produces about a third of this amount.
Use.
"Nori" is commonly used as a wrap for sushi and "onigiri". It is also a garnish or flavoring in noodle preparations and soups. It is most typically toasted prior to consumption ("yaki-nori"). A common secondary product is toasted and flavored "nori" ("ajitsuke-nori"), in which a flavoring mixture (variable, but typically soy sauce, sugar, sake, mirin, and seasonings) is applied in combination with the toasting process. It is also eaten by making it into a soy sauce-flavored paste, "nori no tsukudani" (海苔の佃煮).
"Nori" is sometimes used as a form of food decoration.
A related product, prepared from the unrelated green algae "Monostroma" and "Enteromorpha", is called "aonori" (青海苔 literally blue/green "nori") and is used like herbs on everyday meals, such as "okonomiyaki" and "yakisoba".
Since "nori" sheets easily absorb water from the air and degrade, a desiccant is indispensable when storing it for any significant time.
Nutrition.
"Porphyra yezoensis" has been found to contain sufficient vitamin B#redirect 
 to prevent vitamin B#redirect 
 deficiency in rats. Though Nori has long been considered to be an important source of vitamin B#redirect 
 for vegans, its vitamin B#redirect 
 may actually not be biologically available to humans. It may contain cobalamin analogues which block absorption of B#redirect 
. However, recent studies have shown otherwise, that Nori (Porphyra yezoensis) contains a significant amount of bioactive vitamin B12, not the inactive analogues.
Controversy.
There is controversy in plant-based vegan communities regarding the composition of nori. Nori (the sea vegetable), often used to make sushi, grows as a lettuce in the ocean. The nets used in the harvesting process of nori are believed to scoop up small fish. Following this, seaweed, shrimp, crabs, snails, barnacles and small fish are ground together and made into nori sheets. The industry is so large that it often does not take the time to remove the sea animals. Japan is the world’s largest nori supplier. However, Nagata and Yamamoto, two suppliers of nori, claim that animal contamination is minimal. Vegan communities are divided among this issue and often seek kosher certified kibbutz who offer fish-free nori to create vegetable sushi dishes. 
Additionally, mechanical seaweed harvesting is considered highly unethical by some environmentalists because of the over-harvesting taking place in the Earth’s oceans. Over-harvesting takes away from the ecology of an ocean, putting all sea life and marine animal lives at risk. Seaweed plays a vital role in marine ecology. Nitrogen and carbon dioxide is removed from the sea by seaweed. It is argued that mechanical harvesting of seaweed may never be sustainable. For example, the mechanical harvesting on the Norwegian coast is so extensive, that if all seaweed harvesting procedures followed their practices, many fish and other marine life would be dead. In one square metre of the ocean's floor, there are up to 100,000 organisms living there, potentially being harvested with the seaweed. With 90% of fish already being taken out of the ocean due to overfishing, over-harvesting practices are of significant concern for the environment. 
Individuals on a plant-based diet may choose not to consume nori for ethical reasons, however it is unfortunate as seaweed provides omega-3s, iodine, calcium and magnesium for vegans who may otherwise not get these nutrients.
Not only can nori contain small amounts of fish and other marine life, but it catches and filters a lot of pollution. Seaweed does for the ocean what plants do for the air, therefore, heavy metals, toxic materials and other pollution that is dumped into ocean water has a strong possibility of being harvested and sold in nori and seaweed products.

</doc>
<doc id="22080" url="http://en.wikipedia.org/wiki?curid=22080" title="Netwar">
Netwar

Netwar is a form of low intensity conflict, crime, and activism waged by networked actors. Typical netwar actors might include transnational terrorists, criminal organizations, activist groups, and social movements that employ decentralized, flexible network structures.
The term "netwar" was developed by RAND researchers John Arquilla and David Ronfeldt.
Terminology.
The term is proposed in order to focus specifically on the spread of network based organizational structures throughout the low intensity spectrum of societal conflict. It is argued that other terms applied to information age conflict, such as ‘information warfare’, are inadequate, focusing too narrowly on technological issues while missing the broader social transformation enabled by technological advances.
‘Cyberwar’ is a corresponding term which Arquilla and Ronfeldt propose to describe high-intensity information age conflicts.
Network structures.
Arquilla and Ronfeldt point to three basic types of networks that may be used by netwar actors:
Netwar actors may also take on hybrid forms as well, blending different types of networks and hierarchies. For instance, a node in the network may be hierarchical, an organization may shift between hierarchy and networked autonomy depending on operational demands, or various members of the same group may be networked to each other through different types of network structures.
All-channel networks.
Arquilla and Ronfeldt argue that it is the all-channel model that is becoming increasingly significant as a source of organizational collaborative power. The all-channel network has no central leadership and no key node whose removal might disrupt the entire organization. Instead, the network is completely decentralized, “allowing for local initiative and autonomy” in an organization that may at times appear “acephalous (headless), and at other times polycephalous (hydra-headed).”
The all-channel network is one of the most difficult to maintain because it requires a strong communications capacity to maintain ties between nodes. Moreover, nodal autonomy results in a distributed, consensus style of decision making which is necessarily dependent on back-and-forth communication. As such, this form of organization has only recently become feasible on a greater scale with the dawn of the information age.
Historical context.
The theory of netwar rests on the prediction that networks, enabled by information technology, will become the dominant form of organization following past eras of tribal, institutional/hierarchical, and market-based societies.
Proponents of netwar argue that globalization has set the stage for the rise of networks. National borders in the 21st century have become more permeable to flows of people, capital, and information; non-state actors have gained power vis-à-vis states; and the information revolution has empowered both individuals and dispersed groups. States have begun experimenting with networking and cooperation to tackle transnational issues, non-governmental organizations have formed transnational advocacy networks around shared goals, multinational corporations have distributed and networked their operations around the globe, and criminal and terrorist organizations have shifted to more agile and resilient network forms.
Advances in communications technologies have played a large part in enabling globalization, and likewise play a crucial role in enabling netwar. Networks, especially global or transnational networks, require "rapid, dense, multidirectional communications to function well and endure". This level of communication has only become easily attainable and affordable with the spread of the Internet, satellite communications, cellular phones, digitization, wireless communications, fax, email, etc., all of which allow "diverse, dispersed, autonomous actors […] to consult, coordinate, and act jointly across great distances on the basis of more, better, and faster information than ever before".
Netwar and network dynamics.
The high flexibility and reconfigurability inherent in the network structure creates a challenge in maintaining its effectiveness. Arquilla and Ronfeldt identify four areas that affect the strength of a network:
With this rubric, the strength of a netwar actor corresponds to how highly networked it is, whether its doctrine sustains the network and guides its members, how effectively technology is used to maintain the network, and how much interpersonal trust there is between nodes in the network.
Networks with many leaders, or no leader, may maintain coordination through a combination of powerful doctrine, ideology, shared beliefs, and/or common interests. This allows all the members of the network to maintain a common objective despite great personal or group autonomy. In other words, this provides an “ideational, strategic, and operational centrality that allows for tactical decentralization.”
Examples.
The following are several examples used to support the argument that there is in fact an emergent netwar.
Terrorism.
Terrorist groups, in the Middle East especially, seem to be adopting flexible, decentralized network structures as part of a shift away from “formally organized, state-sponsored groups to privately financed, loose networks of individuals and subgroups that may have strategic guidance but that, nonetheless, enjoy tactical independence”.
Past terrorist groups did incorporate autonomous cells, but they were largely coordinated in a non-networked manner. Newer terrorist movements, such as al-Qaeda, employ less hierarchical, loosely interlinked organizational models. Rather than the rigid bureaucratic structures and nationalist agendas of old terror groups, these new operatives are networked, relying on decentralized decision making with flexible ties between other individuals and radical groups sharing common values.
Zapatistas.
The Zapatista movement began as a seemingly traditional, hierarchical insurgency, but was transformed into an information-age conflict. It has benefited from a diverse network of actors, made up of indigenous communities, non-indigenous middle-class guerilla leaders, and a range of local and transnational NGOs sympathetic to the Zapatista cause. Numerous transnational NGOs networked with local Mexican NGOs that were involved with the marginalized indigenous community and the Zapatista guerillas.
Following setbacks in battle, the guerillas switched tactics and began to exploit the network form, taking advantage of the NGOs connections to mobilize global awareness and support for their reform movement, while putting pressure on the Mexican government. These diverse groups of activists and issue organizations were united by common values and shared goals. The internet, which was in its infancy at the time, also became a key space for networking various groups from around the globe with the Zapatista movement.
Transnational Criminal Organizations.
Transnational Criminal organizations (TCOs), such as Extremist and Terrorist Groups, Italian, Russian, and Jewish Mafias, Mexican and Colombian Drug Cartels, Japanese Yakuza and Chinese Triad Crime syndicates or some Street Gangs, Outlaw Motorcycle Gangs and Cyber-criminals are empowered by the network form in the sense that it heightens their mobility, adaptability, and their ability to operate transnationally. These transnational networks pose a problem for states operating in a conventional, inwardly focused manner. For instance, cartels in Colombia draw power from their extended transnational network resources, making it difficult for the Colombian government to fight the cartels within the confines of its national boundaries. Thus, networking allows TCOs to easily operate across jurisdictions, evading national law enforcement agencies. Networks also make it more difficult to dismantle a criminal operation, given that there is less emphasis on rigid, central leadership.

</doc>
<doc id="22081" url="http://en.wikipedia.org/wiki?curid=22081" title="Normative ethics">
Normative ethics

Normative ethics is the study of ethical action. It is the branch of philosophical ethics that investigates the set of questions that arise when considering how one ought to act, morally speaking. Normative ethics is distinct from meta-ethics because it examines standards for the rightness and wrongness of actions, while meta-ethics studies the meaning of moral language and the metaphysics of moral facts. Normative ethics is also distinct from descriptive ethics, as the latter is an empirical investigation of people’s moral beliefs. To put it another way, descriptive ethics would be concerned with determining what proportion of people believe that killing is always wrong, while normative ethics is concerned with whether it is correct to hold such a belief. Hence, normative ethics is sometimes called prescriptive, rather than descriptive. However, on certain versions of the meta-ethical view called moral realism, moral facts are both descriptive and prescriptive at the same time.
Most traditional moral theories rest on principles that determine whether an action is right or wrong. Classical theories in this vein include utilitarianism, Kantianism, and some forms of contractarianism. These theories mainly offered overarching moral principles to use to resolve difficult moral decisions.
Normative ethical theories.
There are disagreements about what precisely gives an action, rule, or disposition its ethical force. Broadly speaking, there are three competing views on how moral questions should be answered, along with hybrid positions that combine some elements of each. Virtue ethics focuses on the character of those who are acting, while both deontological ethics and consequentialism focus on the status of the action, rule, or disposition itself. The latter two conceptions of ethics themselves come in various forms.
Binding force.
It can be unclear what it means to say that a person "ought to do X because it is moral, whether they like it or not". Morality is sometimes presumed to have some kind of special binding force on behaviour, but some philosophers think that, used this way, the word "ought" seems to wrongly attribute magic powers to morality. For instance, G. E. M. Anscombe worries that "ought" has become "a word of mere mesmeric force". British ethicist Philippa Foot elaborates that morality does not seem to have any special binding force, and she clarifies that people only behave morally when motivated by other factors.
If he is an amoral man he may deny that he has any reason to trouble his head over this or any other moral demand. Of course, he may be mistaken, and his life as well as others' lives may be most sadly spoiled by his selfishness. But this is not what is urged by those who think they can close the matter by an emphatic use of 'ought'. My argument is that they are relying on an illusion, as if trying to give the moral 'ought' a magic force.
”
-Philippa Foot 
Foot says "People talk, for instance, about the 'binding force' of morality, but it is not clear what this means if not that we feel ourselves unable to escape." The idea is that, faced with an opportunity to steal a book because we can get away with it, moral obligation itself has no power to stop us unless we "feel" an obligation. Morality may therefore have no binding force beyond regular human motivations, and people must be motivated to behave morally. The question then arises: what role does reason play in motivating moral behaviour?
Motivating morality.
The categorical imperative perspective suggests that proper reason always leads to particular moral behaviour. As mentioned above, Foot instead believes that humans are actually motivated by desires. Proper reason, on this view, allows humans to discover actions that get them what they want (i.e., hypothetical imperatives)—not necessarily actions that are moral.
Social structure and motivation can make morality binding in a sense, but only because it makes moral norms feel inescapable, according to Foot.
John Stuart Mill adds that external pressures, to please others for instance, also influence this felt binding force, which he calls human "conscience". Mill says that humans must first reason about what is moral, then try to bring the feelings of our conscience in line with our reason. At the same time, Mill says that a good moral system (in his case, utilitarianism) ultimately appeals to aspects of human nature — which, must themselves be nurtured during upbringing. Mill explains:
This firm foundation is that of the social feelings of mankind; the desire to be in unity with our fellow creatures, which is already a powerful principle in human nature, and happily one of those which tend to become stronger, even without express inculcation, from the influences of advancing civilisation.
Mill thus believes that it is important to appreciate that it is feelings that drive moral behavior, but also that they may not be present in some people (e.g. psychopaths). Mill goes on to describe factors that help ensure people develop a conscience and behave morally, and thinkers like Joseph Daleiden describe how societies can use science to figure out how to make people more likely to be good.

</doc>
<doc id="22083" url="http://en.wikipedia.org/wiki?curid=22083" title="Negotiation">
Negotiation

Negotiation is a dialogue between two or more people or parties intended to reach a mutually beneficial outcome, resolve points of difference, to gain advantage for an individual or collective, or to craft outcomes to satisfy various interests.
Negotiation occurs in business, non-profit organizations, government branches, legal proceedings, among nations and in personal situations such as marriage, divorce, parenting, and everyday life. The study of the subject is called "negotiation theory". Professional negotiators are often specialized, such as "union negotiators", "leverage buyout negotiators", "peace negotiators", "hostage negotiators", or may work under other titles, such as diplomats, legislators or brokers.
Strategies.
Negotiation can take a wide variety of forms, from a trained negotiator acting on behalf of a particular organization or position in a formal setting, to an informal negotiation between friends. Negotiation can be contrasted with mediation, where a neutral third party listens to each side's arguments and attempts to help craft an agreement between the parties. It can also be compared with arbitration, which resembles a legal proceeding. In arbitration, both sides make an argument as to the merits of their case and the arbitrator decides the outcome. This negotiation is also sometimes called positional or hard-bargaining negotiation.
Negotiation theorists generally distinguish between two types of negotiation. Different theorists use different labels for the two general types and distinguish them in different ways.
Distributive negotiation.
Distributive negotiation is also sometimes called positional or hard-bargaining negotiation. It tends to approach negotiation on the model of haggling in a market. In a distributive negotiation, each side often adopts an extreme position, knowing that it will not be accepted, and then employs a combination of guile, bluffing, and brinkmanship in order to cede as little as possible before reaching a deal. Distributive bargainers conceive of negotiation as a process of distributing a fixed amount of value.
The term distributive implies that there is a finite amount of the thing being distributed or divided among the people involved. Sometimes this type of negotiation is referred to as the distribution of a "fixed pie." There is only so much to go around, but the proportion to be distributed is variable. Distributive negotiation is also sometimes called "win-lose" because of the assumption that one person's gain results in another person's loss. A distributive negotiation often involves people who have never had a previous interactive relationship, nor are they likely to do so again in the near future. Simple everyday examples would be buying a car or a house.
Integrative negotiation.
Integrative negotiation is also sometimes called interest-based or principled negotiation. It is a set of techniques that attempts to improve the quality and likelihood of negotiated agreement by providing an alternative to traditional distributive negotiation techniques. While distributive negotiation assumes there is a fixed amount of value (a "fixed pie") to be divided between the parties, integrative negotiation often attempts to create value in the course of the negotiation ("expand the pie"). It focuses on the underlying interests of the parties rather than their arbitrary starting positions, approaches negotiation as a shared problem rather than a personalized battle, and insists upon adherence to objective, principled criteria as the basis for agreement.
Integrative negotiation often involves a higher degree of trust and the forming of a relationship. It can also involve creative problem-solving that aims to achieve mutual gains. It is also sometimes called "win-win" negotiation. ("See Win-win game.")
Tactics.
There are many different ways to categorize the essential elements of negotiation.
One view of negotiation involves three basic elements: "process", "behavior" and "substance". The process refers to how the parties negotiate: the context of the negotiations, the parties to the negotiations, the tactics used by the parties, and the sequence and stages in which all of these play out. Behavior refers to the relationships among these parties, the communication between them and the styles they adopt. The substance refers to what the parties negotiate over: the agenda, the issues (positions and - more helpfully - interests), the options, and the agreement(s) reached at the end.
Another view of negotiation comprises four elements: "strategy", "process", "tools", and "tactics". Strategy comprises the top level goals - typically including relationship and the final outcome. Processes and tools include the steps that will be followed and the roles taken in both preparing for and negotiating with the other parties. Tactics include more detailed statements and actions and responses to others' statements and actions. Some add to this "persuasion and influence", asserting that these have become integral to modern day negotiation success, and so should not be omitted.
Adversary or partner?
The two basically different approaches to negotiating will require different tactics. In the distributive approach each negotiator is battling for the largest possible piece of the pie, so it may be quite appropriate - within certain limits - to regard the other side more as an adversary than a partner and to take a somewhat harder line. This would however be less appropriate if the idea were to hammer out an arrangement that is in the best interest of both sides. A good agreement is not one with maximum gain, but optimum gain. This does not by any means suggest that we should give up our own advantage for nothing. But a cooperative attitude will regularly pay dividends. What is gained is not at the expense of the other, but with him.
Employing an advocate.
A skilled negotiator may serve as an advocate for one party to the negotiation. The advocate attempts to obtain the most favorable outcomes possible for that party. In this process the negotiator attempts to determine the minimum outcome(s) the other party is (or parties are) willing to accept, then adjusts their demands accordingly. A "successful" negotiation in the advocacy approach is when the negotiator is able to obtain all or most of the outcomes their party desires, but without driving the other party to permanently break off negotiations, unless the best alternative to a negotiated agreement (BATNA) is acceptable.
Skilled negotiators may use a variety of tactics ranging from negotiation hypnosis, to a straightforward presentation of demands or setting of preconditions, to more deceptive approaches such as cherry picking. Intimidation and salami tactics may also play a part in swaying the outcome of negotiations.
Another negotiation tactic is bad guy/good guy. Bad guy/good guy is when one negotiator acts as a bad guy by using anger and threats. The other negotiator acts as a good guy by being considerate and understanding. The good guy blames the bad guy for all the difficulties while trying to get concessions and agreement from the opponent.
Perspective taking for integrative negotiation.
Perspective taking can be helpful for two reasons: that it can help self-centered negotiators to seek mutually beneficial solutions, and it increases the likelihood of logrolling (when a favor is traded for another i.e. quid pro quo). Social motivation can increase the chances of a party conceding to a negotiation. While concession is mandatory for negotiations, research shows that people who concede more quickly, are less likely to explore all integrative and mutually beneficial solutions. Therefore conceding reduces the chance of an integrative negotiation.
Negotiation styles.
Kenneth W. Thomas identified 5 styles/responses to negotiation. These ﬁve strategies have been frequently described in the literature and are based on the dual-concern model. The dual concern model of conflict resolution is a perspective that assumes individuals' preferred method of dealing with conflict is based on two themes or dimensions 
Based on this model, individuals balance the concern for personal needs and interests with the needs and interests of others. The following five styles can be used based on individuals’ preferences depending on their pro-self or pro-social goals.These styles can change over time, and individuals can have strong dispositions towards numerous styles.
1. Accommodating: Individuals who enjoy solving the other party's problems and preserving personal relationships. Accommodators are sensitive to the emotional states, body language, and verbal signals of the other parties. They can, however, feel taken advantage of in situations when the other party places little emphasis on the relationship.
2. Avoiding: Individuals who do not like to negotiate and don't do it unless warranted. When negotiating, avoiders tend to defer and dodge the confrontational aspects of negotiating; however, they may be perceived as tactful and diplomatic.
3. Collaborating: Individuals who enjoy negotiations that involve solving tough problems in creative ways. Collaborators are good at using negotiations to understand the concerns and interests of the other parties. They can, however, create problems by transforming simple situations into more complex ones.
4. Competing: Individuals who enjoy negotiations because they present an opportunity to win something. Competitive negotiators have strong instincts for all aspects of negotiating and are often strategic. Because their style can dominate the bargaining process, competitive negotiators often neglect the importance of relationships.
5. Compromising: Individuals who are eager to close the deal by doing what is fair and equal for all parties involved in the negotiation. Compromisers can be useful when there is limited time to complete the deal; however, compromisers often unnecessarily rush the negotiation process and make concessions too quickly.
Types of negotiators.
Three basic kinds of negotiators have been identified by researchers involved in The Harvard Negotiation Project. These types of negotiators are: Soft bargainers, hard bargainers, and principled bargainers.
Researchers from The Harvard Negotiation Project recommend that negotiators explore a number of alternatives to the problems they are facing in order to come to the best overall conclusion/solution, but this is often not the case (as when you may be dealing with an individual utilizing soft or hard bargaining tactics) (Forsyth, 2010).
Bad faith negotiation.
When a party pretends to negotiate, but secretly has no intention of compromising, the party is considered to be negotiating in bad faith. Bad faith is a concept in negotiation theory whereby parties pretend to reason to reach settlement, but have no intention to do so, for example, one political party may pretend to negotiate, with no intention to compromise, for political effect.
Inherent bad faith model in international relations and political psychology.
Bad faith in political science and political psychology refers to negotiating strategies in which there is no real intention to reach compromise, or a model of information processing. The "inherent bad faith model" of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. It is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. Examples are John Foster Dulles' position regarding the Soviet Union, or Hamas's position on the state of Israel.
Emotion.
Emotions play an important part in the negotiation process, although it is only in recent years that their effect is being studied. Emotions have the potential to play either a positive or negative role in negotiation. During negotiations, the decision as to whether or not to settle rests in part on emotional factors. Negative emotions can cause intense and even irrational behavior, and can cause conflicts to escalate and negotiations to break down, but may be instrumental in attaining concessions. On the other hand, positive emotions often facilitate reaching an agreement and help to maximize joint gains, but can also be instrumental in attaining concessions. Positive and negative discrete emotions can be strategically displayed to influence task and relational outcomes and may play out differently across cultural boundaries.
Affect effect.
Dispositional affects affect the various stages of the negotiation process: which strategies are planned to be used, which strategies are actually chosen, the way the other party and his or her intentions are perceived, their willingness to reach an agreement and the final negotiated outcomes. Positive affectivity (PA) and negative affectivity (NA) of one or more of the negotiating sides can lead to very different outcomes.
Positive affect.
Even before the negotiation process starts, people in a positive mood have more confidence, and higher tendencies to plan to use a cooperative strategy. During the negotiation, negotiators who are in a positive mood tend to enjoy the interaction more, show less contentious behavior, use less aggressive tactics and more cooperative strategies. This in turn increases the likelihood that parties will reach their instrumental goals, and enhance the ability to find integrative gains. Indeed, compared with negotiators with negative or natural affectivity, negotiators with positive affectivity reached more agreements and tended to honor those agreements more.
Those favorable outcomes are due to better decision making processes, such as flexible thinking, creative problem solving, respect for others' perspectives, willingness to take risks and higher confidence.
Post negotiation positive affect has beneficial consequences as well. It increases satisfaction with achieved outcome and influences one's desire for future interactions. The PA aroused by reaching an agreement facilitates the dyadic relationship, which result in affective commitment that sets the stage for subsequent interactions.
PA also has its drawbacks: it distorts perception of self performance, such that performance is judged to be relatively better than it actually is. Thus, studies involving self reports on achieved outcomes might be biased.
Negative affect.
Negative affect has detrimental effects on various stages in the negotiation process. Although various negative emotions affect negotiation outcomes, by far the most researched is anger. Angry negotiators plan to use more competitive strategies and to cooperate less, even before the negotiation starts. These competitive strategies are related to reduced joint outcomes.
During negotiations, anger disrupts the process by reducing the level of trust, clouding parties' judgment, narrowing parties' focus of attention and changing their central goal from reaching agreement to retaliating against the other side. Angry negotiators pay less attention to opponent's interests and are less accurate in judging their interests, thus achieve lower joint gains. Moreover, because anger makes negotiators more self-centered in their preferences, it increases the likelihood that they will reject profitable offers. Opponents who get really angry (or cry, or otherwise lose control) are more likely to make errors: make sure they are in your favor.
Anger does not help in achieving negotiation goals either: it reduces joint gains and does not help to boost personal gains, as angry negotiators do not succeed in claiming more for themselves. Moreover, negative emotions lead to acceptance of settlements that are not in the positive utility function but rather have a negative utility. However, expression of negative emotions during negotiation can sometimes be beneficial: legitimately expressed anger can be an effective way to show one's commitment, sincerity, and needs. Moreover, although NA reduces gains in integrative tasks, it is a better strategy than PA in distributive tasks (such as zero-sum). In his work on negative affect arousal and white noise, Seidner found support for the existence of a negative affect arousal mechanism through observations regarding the devaluation of speakers from other ethnic origins." Negotiation may be negatively affected, in turn, by submerged hostility toward an ethnic or gender group.
Conditions for emotion affect.
Research indicates that negotiator's emotions do not necessarily affect the negotiation process.
Albarracın et al. (2003) suggested that there are two conditions for emotional affect, both related to the ability (presence of environmental or cognitive disturbances) and the motivation:
According to this model, emotions are expected to affect negotiations only when one is high and the other is low. When both ability and motivation are low the affect will not be identified, and when both are high the affect will be identify but discounted as irrelevant for judgment.
A possible implication of this model is, for example, that the positive effects PA has on negotiations (as described above) will be seen only when either motivation or ability are low.
The effect of the partner's emotions.
Most studies on emotion in negotiations focus on the effect of the negotiator's own emotions on the process. However, what the other party feels might be just as important, as group emotions are known to affect processes both at the group and the personal levels.
When it comes to negotiations, trust in the other party is a necessary condition for its emotion to affect, and visibility enhances the effect.
Emotions contribute to negotiation processes by signaling what one feels and thinks and can thus prevent the other party from engaging in destructive behaviors and to indicate what steps should be taken next: PA signals to keep in the same way, while NA points that mental or behavioral adjustments are needed.
Partner's emotions can have two basic effects on negotiator's emotions and behavior: mimetic/ reciprocal or complementary. For example, disappointment or sadness might lead to compassion and more cooperation. In a study by Butt et al. (2005) which simulated real multi-phase negotiation, most people reacted to the partner's emotions in reciprocal, rather than complementary, manner.
Specific emotions were found to have different effects on the opponent's feelings and strategies chosen:
Problems with laboratory studies.
Negotiation is a rather complex interaction. Capturing all its complexity is a very difficult task, let alone isolating and controlling only certain aspects of it. For this reason most negotiation studies are done under laboratory conditions, and focus only on some aspects. Although lab studies have their advantages, they do have major drawbacks when studying emotions:
Team negotiations.
Due to globalization and growing business trends, negotiation in the form of teams is becoming widely adopted. Teams can effectively collaborate to break down a complex negotiation. There is more knowledge and wisdom dispersed in a team than in a single mind. Writing, listening, and talking, are specific roles team members must satisfy. The capacity base of a team reduces the amount of blunder, and increases familiarity in a negotiation.
Etymology.
The word "negotiation" originated in the early 15th century from the Old French and Latin expressions “negociacion” and “negotiationem.” These terms mean “business, trade and traffic.” By the late 1590s negotiation had the definition, "to communicate in search of mutual agreement." With this new introduction and this meaning, it showed a shift in “doing business” to “bargaining about” business.
Tactics.
Tactics are always an important part of the negotiating process. But tactics don't often jump up and down shouting "Here I am, look at me." If they did, the other side would see right through them and they would not be effective. More often than not they are subtle, difficult to identify and used for multiple purposes. Tactics are more frequently used in distributive negotiations and when the focus in on taking as much value off the table as possible. Many negotiation tactics exist. Below are a few commonly used tactics.
Auction:
The bidding process is designed to create competition. When multiple parties want the same thing, pit them against one another. When people know that they may lose out on something, they will want it even more. Not only do they want the thing that is being bid on, they also want to win, just to win. Taking advantage of someone's competitive nature can drive up the price.
Brinksmanship:
One party aggressively pursues a set of terms to the point at which the other negotiating party must either agree or walk away. Brinkmanship is a type of "hard nut" approach to bargaining in which one party pushes the other party to the "brink" or edge of what that party is willing to accommodate. Successful brinksmanship convinces the other party they have no choice but to accept the offer and there is no acceptable alternative to the proposed agreement.
Bogey:
Negotiators use the bogey tactic to pretend that an issue of little or no importance to him or her is very important. Then, later in the negotiation, the issue can be traded for a major concession of actual importance.
Chicken:
Negotiators propose extreme measures, often bluffs, to force the other party to chicken out and give them what they want. This tactic can be dangerous when parties are unwilling to back down and go through with the extreme measure.
Defence in Depth:
Several layers of decision-making authority is used to allow further concessions each time the agreement goes through a different level of authority. In other words, each time the offer goes to a decision maker, that decision maker asks to add another concession in order to close the deal.
Deadlines:
Give the other party a deadline forcing them to make a decision. This method uses time to apply pressure to the other party. Deadlines given can be actual or artificial.
Flinch:
Flinching is showing a strong negative physical reaction to a proposal. Common examples of flinching are gasping for air, or a visible expression of surprise or shock. The flinch can be done consciously or unconsciously. The flinch signals to the opposite party that you think the offer or proposal is absurd in hopes the other party will lower their aspirations. Seeing a physical reaction is more believable than hearing someone saying, "I'm shocked."
Good Guy/Bad Guy:
The good guy/bad guy approach is typically used in team negotiations where one member of the team makes extreme or unreasonable demands, and the other offers a more rational approach. This tactic is named after a police interrogation technique often portrayed in the media. The "good guy" will appear more reasonable and understanding, and therefore, easier to work with. In essence, it is using the law of relativity to attract cooperation. The good guy will appear more agreeable relative to the "bad guy." This tactic is easy to spot because of its frequent use.
Highball/Lowball:
Depending on whether selling or buying, sellers or buyers use a ridiculously high, or ridiculously low opening offer that will never be achieved. The theory is that the extreme offer will cause the other party to reevaluate his or her own opening offer and move close to the resistance point (as far as you are willing to go to reach an agreement). Another advantage is that the person giving the extreme demand appears more flexible he or she makes concessions toward a more reasonable outcome. A danger of this tactic is that the opposite party may think negotiating is a waste of time.
The Nibble:
Nibbling is asking for proportionally small concessions that haven't been discussed previously just before closing the deal. This method takes advantage of the other party's desire to close by adding "just one more thing."
Snow Job:
Negotiators overwhelm the other party with so much information that he or she has difficulty determining which facts are important, and which facts are diversions. Negotiators may also use technical language or jargon to mask a simple answer to a question asked by a non-expert.
Nonverbal communication.
Communication is a key element of negotiation. Effective negotiation requires that participants effectively convey and interpret information. Participants in a negotiation will communicate information not only verbally but non-verbally through body language and gestures. By understanding how nonverbal communication works, a negotiator is better equipped to interpret the information other participants are leaking non-verbally while keeping secret those things that would inhibit his/her ability to negotiate.
Examples in negotiation.
Non-verbal "anchoring"
In a negotiation, a person can gain the advantage by verbally expressing his or her position first. By "anchoring" your position, one establishes the position from which the negotiation will proceed. In a like manner, one can "anchor" and gain advantage with nonverbal (body language) cues.
Reading non-verbal communication
Being able to read the non-verbal communication of another person can significantly aid in the communication process. By being aware of inconsistencies between a person's verbal and non-verbal communication and reconciling them, negotiators will be able to come to better resolutions. Examples of incongruity in body language include:
Conveying receptivity
The way negotiation partners position their bodies relative to each other may influence how receptive each is to the other person's message and ideas.
Receptive negotiators tend to appear relaxed with their hands open and palms visibly displayed.

</doc>
<doc id="22085" url="http://en.wikipedia.org/wiki?curid=22085" title="Fertility awareness">
Fertility awareness

Fertility awareness (FA) refers to a set of practices used to determine the fertile and infertile phases of a woman's menstrual cycle. Fertility awareness methods may be used to avoid pregnancy, to achieve pregnancy, or as a way to monitor gynecological health.
Methods of identifying infertile days have been known since antiquity, but scientific knowledge gained during the past century has increased the number and variety of methods.
Systems of fertility awareness rely on observation of changes in one or more of the primary fertility signs (basal body temperature, cervical mucus, and cervical position), tracking menstrual cycle length and identifying the fertile window based on this information, or both. Other signs may also be observed: these include breast tenderness and mittelschmerz (ovulation pains), urine analysis strips known as ovulation predictor kits (OPKs), and microscopic examination of saliva or cervical fluid. Also available are computerized fertility monitors.
Terminology.
Symptoms-based methods involve tracking one or more of the three primary fertility signs: basal body temperature, cervical mucus, and cervical position. Systems relying exclusively on cervical mucus include the Billings Ovulation Method, , the Creighton Model, and the Two-Day Method. Symptothermal methods combine observations of basal body temperature (BBT), cervical mucus, and sometimes cervical position. Calendar-based methods rely on tracking a woman's cycle and identifying her fertile window based on the lengths of her cycles. The best known of these methods is the Standard Days Method. The Calendar-Rhythm method is also considered a calendar-based method, though it is not well defined and has many different meanings to different people.
Systems of fertility awareness may be referred to as fertility awareness–based methods (FAB methods); the term Fertility Awareness Method (FAM) refers specifically to the system taught by Toni Weschler. The term natural family planning (NFP) is sometimes used to refer to any use of FA methods. However, : the Lactational amenorrhea method and periodic abstinence during fertile times. A method of FA may be used by NFP users to identify these fertile times.
Women who are breastfeeding a child and wish to avoid pregnancy may be able to practice the lactational amenorrhea method (LAM). LAM is distinct from fertility awareness, but because it also does not involve devices or chemicals, it is often presented alongside FA as a method of "natural" birth control.
History.
Development of calendar-based methods.
It is not known exactly when it was first discovered that women have predictable periods of fertility and infertility. St. Augustine wrote about periodic abstinence to avoid pregnancy in the year 388 (the Manichaeans attempted to use this method to remain childfree, and Augustine condemned their use of periodic abstinence). One book states that periodic abstinence was recommended "by a few secular thinkers since the mid-nineteenth century," but the dominant force in the twentieth century popularization of fertility awareness-based methods was the Roman Catholic Church.
In 1905 Theodoor Hendrik van de Velde, a Dutch gynecologist, showed that women only ovulate once per menstrual cycle. In the 1920s, Kyusaku Ogino, a Japanese gynecologist, and Hermann Knaus, from Austria, independently discovered that ovulation occurs about fourteen days before the next menstrual period. Ogino used his discovery to develop a formula for use in aiding infertile women to time intercourse to achieve pregnancy. In 1930, John Smulders, Roman Catholic physician from the Netherlands, used this discovery to create a method for "avoiding" pregnancy. Smulders published his work with the Dutch Roman Catholic medical association, and this was the first formalized system for periodic abstinence: the rhythm method.
Introduction of temperature and cervical mucus signs.
In the 1930s, Reverend Wilhelm Hillebrand, a Catholic priest in Germany, developed a system for avoiding pregnancy based on basal body temperature. This temperature method was found to be more effective at helping women avoid pregnancy than were calendar-based methods. Over the next few decades, both systems became widely used among Catholic women. Two speeches delivered by Pope Pius XII in 1951 gave the highest form of recognition to the Catholic Church's approval—for couples who needed to avoid pregnancy—of these systems. In the early 1950s, Dr. John Billings discovered the relationship between cervical mucus and fertility while working for the Melbourne Catholic Family Welfare Bureau. Billings and several other physicians, including his wife, Dr. Evelyn Billings, studied this sign for a number of years, and by the late 1960s had performed clinical trials and begun to set up teaching centers around the world.
First symptoms-based teaching organizations.
While Dr. Billings initially taught both the temperature and mucus signs, they encountered problems in teaching the temperature sign to largely illiterate populations in developing countries. In the 1970s they modified the method to rely on only mucus. The international organization founded by Dr. Billings is now known as the World Organization Ovulation Method Billings (WOOMB).
The first organization to teach a symptothermal method was founded in 1971. John and Sheila Kippley, lay Catholics, joined with Dr. Konald Prem in teaching an observational method that relied on all three signs: temperature, mucus, and cervical position. Their organization is now called Couple to Couple League International. The next decade saw the founding of other now-large Catholic organizations, Family of the Americas (1977), teaching the Billings method, and the Pope Paul VI Institute (1985), teaching a new mucus-only system called the Creighton Model.
Up until the 1980s, information about fertility awareness was only available from Catholic sources. The first secular teaching organization was the Fertility Awareness Center in New York, founded in 1981. Toni Weschler started teaching in 1982 and published the bestselling book "Taking Charge of Your Fertility" in 1995. Justisse was founded in 1987 in Edmonton, Canada. These secular organizations all teach symptothermal methods. Although the Catholic organizations are significantly larger than the secular fertility awareness movement, independent secular teachers have become increasingly common since the 1990s.
Ongoing development.
Development of fertility awareness methods is ongoing. In the late 1990s, the Institute for Reproductive Health at Georgetown University introduced two new methods. The Two-Day Method, a mucus-only system, and CycleBeads and iCycleBeads (the digital version), based on the Standard Days Method, are designed to be both effective and simple to teach, learn, and use.
Fertility signs.
Most menstrual cycles have several days at the beginning that are infertile (pre-ovulatory infertility), a period of fertility, and then several days just before the next menstruation that are infertile (post-ovulatory infertility). The first day of red bleeding is considered day one of the menstrual cycle. Different systems of fertility awareness calculate the fertile period in slightly different ways, using primary fertility signs, cycle history, or both.
Primary fertility signs.
The three primary signs of fertility are "basal body temperature" (BBT), "cervical mucus", and "cervical position". A woman practicing symptoms-based fertility awareness may choose to observe one sign, two signs, or all three. Many women experience secondary fertility signs that correlate with certain phases of the menstrual cycle, such as abdominal pain and heaviness, back pain, breast tenderness, and mittelschmerz (ovulation pains).
Basal body temperature.
This classifies a temperature reading collected when a person first wakes up in the morning (or after their longest sleep period of the day). In women, ovulation will trigger a rise in BBT between 0.3 and 0.9 °C (0.5 and 1.6 °F) that lasts approximately until the next menstruation. This temperature shift may be used to determine the onset of post-ovulatory infertility.
Cervical mucus.
The appearance of cervical mucus and vulvar sensation are generally described together as two ways of observing the same sign. Cervical mucus is produced by the cervix, which connects the uterus to the vaginal canal. Fertile cervical mucus promotes sperm life by decreasing the acidity of the vagina, and also helps guide sperm through the cervix and into the uterus. The production of fertile cervical mucus is caused by estrogen, the same hormone that prepares a woman's body for ovulation. By observing her cervical mucus and paying attention to the sensation as it passes the vulva, a woman can detect when her body is gearing up for ovulation, and also when ovulation has passed. When ovulation occurs, estrogen production drops slightly and progesterone starts to rise. The rise in progesterone causes a distinct change in the quantity and quality of mucus observed at the vulva.
Cervical position.
The cervix changes position in response to the same hormones that cause cervical mucus to be produced and to dry up. When a woman is in an infertile phase of her cycle, the cervix will be low in the vaginal canal; it will feel firm to the touch (like the tip of a person's nose); and the os—the opening in the cervix—will be relatively small, or "closed". As a woman becomes more fertile, the cervix will rise higher in the vaginal canal, it will become softer to the touch (more like a person's lips), and the os will become more open. After ovulation has occurred, the cervix will revert to its infertile position.
Cycle history.
Calendar-based systems determine both pre-ovulatory and post-ovulatory infertility based on cycle history. When used to avoid pregnancy, these systems have higher perfect-use failure rates than symptoms-based systems, but are still comparable with barrier methods, such as diaphragms and cervical caps.
Mucus- and temperature-based methods used to determine post-ovulatory infertility, when used to avoid conception, result in very low perfect-use pregnancy rates. However, mucus and temperature systems have certain limitations in determining pre-ovulatory infertility. A temperature record alone provides no guide to fertility or infertility before ovulation occurs. Determination of pre-ovulatory infertility may be done by observing the absence of fertile cervical mucus; however, this results in a higher failure rate than that seen in the period of post-ovulatory infertility. Relying only on mucus observation also means that unprotected sexual intercourse is not allowed during menstruation, since any mucus would be obscured.
Use of certain calendar rules to determine the length of the pre-ovulatory infertile phase allows unprotected intercourse during the first few days of the menstrual cycle while maintaining a very low risk of pregnancy. With mucus-only methods, there is a possibility of incorrectly identifying mid-cycle or anovulatory bleeding as menstruation. Keeping a BBT chart enables accurate identification of menstruation, when pre-ovulatory calendar rules may be reliably applied. In temperature-only systems, a calendar rule may be relied on alone to determine pre-ovulatory infertility. In symptothermal systems, the calendar rule is cross-checked by mucus records: observation of fertile cervical mucus overrides any calendar-determined infertility.
Calendar rules may set a standard number of days, specifying that (depending on a woman's past cycle lengths) the first three to six days of each menstrual cycle are considered infertile. Or, a calendar rule may require calculation, for example holding that the length of the pre-ovulatory infertile phase is equal to the length of a woman's shortest cycle minus 21 days. Rather than being tied to cycle length, a calendar rule may be determined from the cycle day on which a woman observes a thermal shift. One system has the length of the pre-ovulatory infertile phase equal to a woman's earliest historical day of temperature rise minus seven days.
Other techniques.
Ovulation predictor kits (OPKs) can detect imminent ovulation from the concentration of lutenizing hormone (LH) in a woman's urine. A positive OPK is usually followed by ovulation within 12–36 hours.
Saliva microscopes, when correctly used, can detect ferning structures in the saliva that precede ovulation. Ferning is usually detected beginning three days before ovulation, and continuing until ovulation has occurred. During this window, ferning structures occur in cervical mucus as well as saliva.
Computerized fertility monitors, such as Lady-Comp, are available under various brand names. These monitors may use BBT-only systems, they may analyze urine test strips, they may use symptothermal observations, they may monitor the electrical resistance of saliva and vaginal fluids, or a combination of any of these factors.
Benefits and drawbacks.
Fertility awareness has a number of unique characteristics:
As birth control.
By restricting unprotected sexual intercourse to the infertile portion of the menstrual cycle, a woman and her partner can prevent pregnancy. During the fertile portion of the menstrual cycle, the couple may use barrier contraception or abstain from sexual intercourse. Or the couple may attempt to control birth in the opposite sense by timing intercourse to achieve pregnancy.
Effectiveness.
The effectiveness of fertility awareness, as of most forms of contraception, can be assessed two ways. "Perfect use" or "method" effectiveness rates only include people who follow all observational rules, correctly identify the fertile phase, and refrain from unprotected intercourse on days identified as fertile. "Actual use" or "typical use" effectiveness rates include all women relying on fertility awareness to avoid pregnancy, including those who fail to meet the "perfect use" criteria. Rates are generally presented for the first year of use. Most commonly, the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.
The failure rate of fertility awareness varies widely depending on the system used to identify fertile days, the instructional method, and the population being studied. Some studies have found actual failure rates of 25% per year or higher. At least one study has found a failure rate of less than 1% per year with continuous intensive coaching and monthly review, and several studies have found actual failure rates of 2%–3% per year.
When used correctly and consistently (i.e., with perfect use) with ongoing coaching, under study conditions some studies have found some forms of FA to be 99% effective.
From "Contraceptive Technology":
Reasons for lower typical-use effectiveness.
Several factors account for typical-use effectiveness being lower than perfect-use effectiveness:
The most common reason for the lower actual effectiveness is not mistakes on the part of instructors or users, but conscious user non-compliance—that is, the couple knowing that the woman is likely to be fertile at the time but engaging in sexual intercourse nonetheless. This is similar to failures of barrier methods, which are primarily caused by non-use of the method.
To achieve pregnancy.
Intercourse timing.
A study by Barrett and Marshall has shown that random acts of intercourse achieve a 24% pregnancy rate per cycle. That study also found that timed intercourse based on information from a BBT-only method of FA increased pregnancy rates to 31%–68%.
Studies of cervical-mucus methods of fertility awareness have found pregnancy rates of 67%-81% in the first cycle if intercourse occurred on the Peak Day of the mucus sign.
Because of high rates of very early miscarriage (25% of pregnancies are lost within the first six weeks since the woman's last menstrual period, or LMP), the methods used to detect pregnancy may lead to bias in conception rates. Less-sensitive methods will detect lower conception rates, because they miss the conceptions that resulted in early pregnancy loss. A Chinese study of couples practicing random intercourse to achieve pregnancy used very sensitive pregnancy tests to detect pregnancy. It found a 40% conception rate per cycle over the 12-month study period.
Problem diagnosis.
Regular menstrual cycles are sometimes taken as evidence that a woman is ovulating normally, and irregular cycles as evidence she is not. However, many women with irregular cycles do ovulate normally, and some with regular cycles are actually annovulatory or have a luteal phase defect. Records of basal body temperatures, especially, but also of cervical mucus and position, can be used to accurately determine if a woman is ovulating, and if the length of the post-ovulatory (luteal) phase of her menstrual cycle is sufficient to sustain a pregnancy.
Fertile cervical mucus is important in creating an environment that allows sperm to pass through the cervix and into the fallopian tubes where they wait for ovulation. Fertility charts can help diagnose hostile cervical mucus, a common cause of infertility. If this condition is diagnosed, some sources suggest taking guaifenesin in the few days before ovulation to thin out the mucus.
Pregnancy testing and gestational age.
Pregnancy tests are not accurate until 1–2 weeks after ovulation. Knowing an estimated date of ovulation can prevent a woman from getting false negative results due to testing too early. Also, 18 consecutive days of elevated temperatures means a woman is almost certainly pregnant.
Estimated ovulation dates from fertility charts are a more accurate method of estimating gestational age than the traditional pregnancy wheel or last menstrual period (LMP) method of tracking menstrual periods.

</doc>
