<doc id="23529" url="http://en.wikipedia.org/wiki?curid=23529" title="Proverb">
Proverb

A proverb (from Latin: "proverbium") is a simple and concrete saying, popularly known and repeated, that expresses a truth based on common sense or the practical experience of humanity. They are often metaphorical. A proverb that describes a basic rule of conduct may also be known as a maxim.
Proverbs are often borrowed from similar languages and cultures, and sometimes come down to the present through more than one language. Both the Bible (including, but not limited to the Book of Proverbs) and medieval Latin (aided by the work of Erasmus) have played a considerable role in distributing proverbs across Europe. Mieder has concluded that cultures that treat the Bible as their "major spiritual book contain between three hundred and five hundred proverbs that stem from the Bible." However, almost every culture has examples of its own unique proverbs.
Paremiology.
The study of proverbs is called paremiology which has a variety of uses in the study of such topics as philosophy, linguistics, and folklore.
There are several types and styles of proverbs which are analyzed within Paremiology as is the use and misuse of familiar expressions which are not strictly 'proverbial' in the dictionary definition of being fixed sentences.
Definitions of "proverb".
Defining a “proverb” is a difficult task. Proverb scholars often quote Archer Taylor’s classic “The definition of a proverb is too difficult to repay the undertaking... An incommunicable quality tells us this sentence is proverbial and that one is not. Hence no definition will enable us to identify positively a sentence as proverbial”. Another common definition is from Lord John Russell (c. 1850) “A proverb is the wit of one, and the wisdom of many.” 
More constructively, Mieder has proposed the following definition, “A proverb is a short, generally known sentence of the folk which contains wisdom, truth, morals, and traditional views in a metaphorical, fixed, and memorizable form and which is handed down from generation to generation.” Norrick created a table of distinctive features to distinguish proverbs from idioms, cliches, etc. Prahlad distinguishes proverbs from some other, closely related types of sayings, “True proverbs must further be distinguished from other types of proverbial speech, e.g. proverbial phrases, Wellerisms, maxims, quotations, and proverbial comparisons.” Based on Persian proverbs, Zolfaghari and Ameri propose the following definition: "A proverb is a short sentence, which is well-known and at times rhythmic, including advice, sage themes and ethnic experiences, comprising simile, metaphor or irony which is well-known among people for its fluent wording, clarity of expression, simplicity, expansiveness and generality and is used either with or without change"
There are many sayings in English that are commonly referred to as “proverbs”, such as weather sayings. Alan Dundes, however, rejects including such sayings among truly proverbs: “Are weather proverbs proverbs? I would say emphatically 'No!'” The definition of “proverb” has also changed over the years. For example, the following was labeled “A Yorkshire proverb” in 1883, but would not be categorized as a proverb by most today, “as throng as Throp's wife when she hanged herself with a dish-cloth.” The changing of the definition of "proverb" is also noted in Turkish.
In other languages and cultures, the definition of “proverb” also differs from English. In the Chumburung language of Ghana, ""aŋase" are literal proverb and "akpare" are metaphoric ones.” Among the Bini of Nigeria, there are three words that are used to translate "proverb": "ere, ivbe", and "itan". The first relates to historical events, the second relates to current events, and the third was “linguistic ornamentation in formal discourse”. Among the Balochi of Pakistan and Afghanistan, there is a word "batal" for ordinary proverbs and "bassīttuks" for "proverbs with background stories".
All of this makes it difficult to come up with a definition of "proverb" that is universally applicable, which brings us back to Taylor's observation, "An incommunicable quality tells us this sentence is proverbial and that one is not.".
Grammatical structures of proverbs.
Proverbs in various languages are found with a wide variety of grammatical structures. In English, for example, we find the following structures (in addition to others):
However, people will often quote only a fraction of a proverb to invoke an entire proverb, e.g. "All is fair" instead of "All is fair in love and war", and "A rolling stone" for "A rolling stone gathers no moss."
The grammar of proverbs is not always the typical grammar of the spoken language, often elements are moved around, to achieve rhyme or focus.
Use in conversation.
Proverbs are used in conversation by adults more than children, partially because adults have learned more proverbs than children. Also, using proverbs well is a skill that is developed over years. Additionally, children have not mastered the patterns of metaphorical expression that are invoked in proverb use. Proverbs, because they are indirect, allow a speaker to disagree or give advice in a way that may be less offensive. Studying actual proverb use in conversation, however, is difficult since the researcher must wait for proverbs to happen. An Ethiopian researcher, Tadesse Jaleta Jirata, made headway in such research by attending and taking notes at events where he knew proverbs were expected to be part of the conversations.
Use in literature.
Many authors have used proverbs in their writings. Probably the most famous user of proverbs in novels is J. R. R. Tolkien in his "The Hobbit" and "The Lord of the Rings" series. Also, C. S. Lewis created a dozen proverbs in "The Horse and His Boy". These books are notable for not only using proverbs as integral to the development of the characters and the story line, but also for creating proverbs.
Among medieval literary texts, Geoffrey Chaucer's Troilus and Criseyde plays a special role because Chaucer's usage seems to challenge the truth value of proverbs by exposing their epistemological unreliability.
Proverbs (or portions of them) have been the inspiration for titles of books: "The Bigger they Come" by Erle Stanley Gardner, and "Birds of a Feather" (several books with this title), "Devil in the Details" (multiple books with this title). Sometimes a title alludes to a proverb, but does not actually quote it, such as "The Gift Horse's Mouth" by Robert Campbell. Some stories have been written with a proverb overtly as an opening, such as "A stitch in time saves nine" at the beginning of "Kitty's Class Day", one of Louisa May Alcott's "Proverb Stories". Other times, a proverb appears at the end of a story, summing up a moral to the story, frequently found in Aesop's Fables, such as "Heaven helps those who help themselves" from "Hercules and the Wagoner".
Proverbs have also been used strategically by poets. Sometimes proverbs (or portions of them or anti-proverbs) are used for titles, such as "A bird in the bush" by Lord Kennet and his stepson Peter Scott and "The blind leading the blind" by Lisa Mueller. Sometimes, multiple proverbs are important parts of poems, such as Paul Muldoon's "Symposium", which begins "You can lead a horse to water but you can't make it hold its nose to the grindstone and hunt with the hounds. Every dog has a stitch in time..." The Turkish poet Refiki wrote an entire poem by stringing proverbs together, which has been translated into English poetically yielding such verses as "Be watchful and be wary, / But seldom grant a boon; / The man who calls the piper / Will also call the tune."
Because proverbs are familiar and often pointed, they have been used by a number of hip-hop poets. This has been true not only in the USA, birthplace of hip-hop, but also in Nigeria. Since Nigeria is so multilingual, hip-hop poets there use proverbs from various languages, mixing them in as it fits there need, sometimes translating the original. For example, 
"They forget say ogbon ju agbaralo
They forget that wisdom is greater than power"
Some authors have bent and twisted proverbs, creating anti-proverbs, for a variety of literary effects. For example, in the Harry Potter novels, J. K. Rowling reshapes a standard English proverb into “It’s no good crying over spilt potion” and Dumbledore
advises Harry not to “count your owls before they are delivered”. In a slightly different use of reshaping proverbs, in the Aubrey–Maturin series of historical naval novels by Patrick O'Brian, Capt. Jack Aubrey humorously mangles and mis-splices proverbs, such as “Never count the bear’s skin before it is hatched” and “There’s a good deal to be said for making hay while the iron is hot.”
Because proverbs are so much a part of the language and culture, authors have sometimes used proverbs in historical fiction effectively, but anachronistically, before the proverb was actually known. For example, the novel "Ramage and the Rebels", by Dudley Pope is set in approximately 1800. Captain Ramage reminds his adversary "You are supposed to know that it is dangerous to change horses in midstream" (p. 259), with another allusion to the same proverb three pages later. However, the proverb about changing horses in midstream is reliably dated to 1864, so the proverb could not have been known or used by a character from that period.
Some authors have used so many proverbs that there have been entire books written cataloging their proverb usage, such as Charles Dickens, Agatha Christie, George Bernard Shaw, and Friedrich Nietzsche.
On the non-fiction side, proverbs have also been used by authors. Some have been used as the basis for book titles, e.g. "I Shop, Therefore I Am: Compulsive Buying and the Search for Self" by April Lane Benson. Some proverbs been used as the basis for article titles, "All our eggs in a broken basket: How the Human Terrain System is undermining sustainable military cultural competence." Many authors have cited proverbs as epigrams at the beginning of their articles, e.g. "'If you want to dismantle a hedge, remove one thorn at a time' Somali proverb" in an article on peacemaking in Somalia.
Interpretations of proverbs.
Interpreting proverbs is often complex. Interpreting proverbs from other cultures is much more difficult than interpreting proverbs in ones own culture. Even within English-speaking cultures, there is difference of opinion on how to interpret the proverb A rolling stone gathers no moss. Some see it as condemning a person that keeps moving, seeing moss as a positive thing, such as profit; others see it the proverb as praising people that keep moving and developing, seeing moss as a negative thing, such as negative habits.
In an extreme example, one researcher working in Ghana found that for a single Akan proverb, twelve different interpretations were given. Though this is extreme, proverbs can often have multiple interpretations.
Children will sometimes interpret proverbs in a literal sense, not yet knowing how to understand the conventionalized metaphor. Interpretation of proverbs is also affected by injuries and diseases of the brain, "A hallmark of schizophrenia is impaired proverb interpretation."
Counter proverbs.
There are often proverbs that contradict each other, such as "Look before you leap" and "He who hesitates is lost." These have been labeled "counter proverbs" When there are such counter proverbs, each can be used in its own appropriate situation, and neither is intended to be a universal truth.
The concept of "counter proverb" is more about pairs of contradictory proverbs than about the use of proverbs to counter each other in an argument. For example, the following pair are counter proverbs from Ghana "It is the patient person who will milk a barren cow" and "The person who would milk a barren cow must prepare for a kick on the forehead" The two contradict each other, whether they are used in an argument or not (though indeed they were used in an argument). But the same work contains an appendix with many examples of proverbs used in arguing for contrary positions, but proverbs that are not inherently contradictory, such as "One is better off with hope of a cow's return than news of its death" countered by "If you don't know a goat [before its death] you mock at its skin". Though this pair was used in a contradictory way in a conversation, they are not a set of "counter proverbs".
Discussing counter proverbs in the Badaga language, Hockings explained that in his large collection "a few proverbs are mutually contradictory... we can be sure that the Badagas do not see the matter that way, and would explain such apparent contradictions by reasoning that proverb "x" is used in one context, while "y" is used in quite another." Comparing Korean proverbs, "when you compare two proverbs, often they will be contradictory." They are used for "a particular situation".
"Counter proverbs" are not the same as a "paradoxical proverb", a proverb that contains a seeming paradox.
Proverbs in drama and film.
Similarly to other forms of literature, proverbs have also been used as important units of language in drama and films. This is true from the days of classical Greek works to old French to Shakespeare, to 19th Century Spanish, to today. The use of proverbs in drama and film today is still found in languages around the world, such as Yorùbá.
A film that makes rich use of proverbs is "Forrest Gump", known for both using and creating proverbs. Other studies of the use of proverbs in film include work by Kevin McKenna on the Russian film "Aleksandr Nevsky", Haase's study of an adaptation of Little Red Riding Hood, Elias Dominguez Barajas on the film "Viva Zapata!", and Aboneh Ashagrie on "The Athlete" (a movie in Amharic about Abebe Bikila).
In the case of "Forrest Gump", the screenplay by Eric Roth had more proverbs than the novel by Winston Groom, but for "The Harder They Come", the reverse is true, where the novel derived from the movie by Michael Thelwell has many more proverbs than the movie.
Éric Rohmer, the French film director, directed a series of films, the "Comedies and Proverbs", where each film was based on a proverb: "The Aviator's Wife", "The Perfect Marriage", "Pauline at the Beach", "Full Moon in Paris" (the film's proverb was invented by Rohmer himself: "The one who has two wives loses his soul, the one who has two houses loses his mind."), "The Green Ray", "Boyfriends and Girlfriends".
Movie titles based on proverbs include "Murder Will Out (1939 film)", "Try, Try Again", and "The Harder They Fall". The title of an award-winning Turkish film, Three Monkeys, also invokes a proverb, though the title does not fully quote it.
They have also been used as the titles of plays: "Baby with the Bathwater" by Christopher Durang, "Dog Eat Dog" by Mary Gallagher, and "The Dog in the Manger" by Charles Hale Hoyt. The use of proverbs as titles for plays is not, of course, limited to English plays: "Il faut qu'une porte soit ouverte ou fermée" (A door must be open or closed) by Paul de Musset. Proverbs have also been used in musical dramas, such as "The Full Monty", which has been shown to use proverbs in clever ways.
Proverbs and music.
Proverbs are often poetic in and of themselves, making them ideally suited for adapting into songs. Proverbs have been used in music from opera to country to hip-hop. Proverbs have also been used in music in other languages, such as the Akan language the Igede language, and Spanish.
English examples of using proverbs in music include Elvis Presley's "Easy come, easy go", Harold Robe's "Never swap horses when you're crossing a stream", Arthur Gillespie's "Absence makes the heart grow fonder", Bob Dylan's "Like a rolling stone", Cher's "Apples don't fall far from the tree". Lynn Anderson made famous a song full of proverbs, "I never promised you a rose garden" (written by Joe South). In choral music, we find Michael Torke's "Proverbs" for female voice and ensemble. A number of Blues musicians have also used proverbs extensively. The frequent use of proverbs in Country music has led to published studies of proverbs in this genre. The Reggae artist Jahdan Blakkamoore has recorded a piece titled "Proverbs Remix". The opera "Maldobrìe" contains careful use of proverbs. An extreme example of many proverbs used in composing songs is a song consisting almost entirely of proverbs performed by Bruce Springsteen, "My best was never good enough". The Mighty Diamonds recorded a song called simply "Proverbs".
The band Fleet Foxes used the proverb painting Netherlandish Proverbs for the cover of their eponymous album Fleet Foxes.
In addition to proverbs being used in songs themselves, some rock bands have used parts of proverbs as their names, such as the Rolling Stones, Bad Company, The Mothers of Invention, Feast or Famine, Of Mice and Men. There have been at least two groups that called themselves "The Proverbs". In addition, many albums have been named with allusions to proverbs, such as "Spilt milk" (a title used by Jellyfish and also Kristina Train), "The more things change" by Machine Head, "Silk purse" by Linda Rondstadt, "Another day, another dollar" by DJ Scream Roccett, "The blind leading the naked" by Vicious Femmes, "What's good for the goose is good for the gander" by Bobby Rush, "Resistance is Futile" by Steve Coleman, "Murder will out" by Fan the Fury. The proverb "Feast or famine" has been used as an album title by Chuck Ragan, Reef the Lost Cauze, Indiginus, and DaVinci. Whitehorse mixed two proverbs for the name of their album "Leave no bridge unburned". The band Splinter Group released an album titled "When in Rome, Eat Lions". The band Downcount used a proverb for the name of their tour, "Come and take it".
Sources of proverbs.
Proverbs come from a variety of sources. Some are, indeed, the result of people pondering and crafting language, such as some by Confucius, Plato, Baltasar Gracián, etc. Others are taken from such diverse sources as poetry, songs, commercials, advertisements, movies, literature, etc. A number of the well known sayings of Jesus, Shakespeare, and others have become proverbs, though they were original at the time of their creation, and many of these sayings were not seen as proverbs when they were first coined. Many proverbs are also based on stories, often the end of a story. For example, the proverb "Who will bell the cat?" is from the end of a story about the mice planning how to be safe from the cat.
Some authors have created proverbs in their writings, such a J.R.R. Tolkien, and some of these proverbs have made their way into broader society, such as the bumper sticker pictured here. Similarly, C.S. Lewis' created proverb about a lobster in a pot, from the "Chronicles of Narnia", has also gained currency. In cases like this, deliberately created proverbs for fictional societies have become proverbs in real societies.
Though many proverbs are ancient, they were all newly created at some point by somebody. Sometimes it is easy to detect that a proverb is newly coined by a reference to something recent, such as the Haitian proverb "The fish that is being microwaved doesn't fear the lightning". Also, there is a proverb in the Kafa language of Ethiopia that refers to the forced military conscription of the 1980s, "...the one who hid himself lived to have children." A Mongolian proverb also shows evidence of recent origin, "A beggar who sits on gold; Foam rubber piled on edge." A political candidate in Kenya popularised a new proverb in his campaign, "Chuth ber" "Immediacy is best". Over 1,400 new English proverbs are said to have been coined in the 20th century. This process of creating proverbs is always ongoing, so that possible new proverbs are being created constantly. Those sayings that are adopted and used by an adequate number of people become proverbs in that society.
Paremiological minimum.
Grigorii Permjakov developed the concept of the core set of proverbs that full members of society know, what he called the "paremiological minimum" (1979). For example, an adult American is expected to be familiar with "Birds of a feather flock together", part of the American paremiological minimum. However, an average adult American is not expected to know "Fair in the cradle, foul in the saddle", an old English proverb that is not part of the current American paremiological minimum. Thinking more widely than merely proverbs, Permjakov observed "every adult Russian language speaker (over 20 years of age) knows no fewer than 800 proverbs, proverbial expressions, popular literary quotations and other forms of cliches". Studies of the paremiological minimum have been done for a limited number of languages, including Russian, Hungarian, Czech, Somali, Nepali, Gujarati, Spanish, and Esperanto. Two noted examples of attempts to establish a paremiological minimum in America are by Haas (2008) and Hirsch, Kett, and Trefil (1988), the latter more prescriptive than descriptive. There is not yet a recognized standard method for calculating the paremiological minimum, as seen by comparing the various efforts to establish the paremiological minimum in a number of languages.
Proverbs in visual form.
From ancient times, people around the world have recorded proverbs in visual form. This has been done in two ways. First, proverbs have been "written" to be displayed, often in a decorative manner, such as on pottery, cross-stitch, murals, kangas (East African women's wraps), and quilts.
Secondly, proverbs have often been visually depicted in a variety of media, including paintings, etchings, and sculpture. Jakob Jordaens painted a plaque with a proverb about drunkenness above a drunk man wearing a crown, titled "The King Drinks". Probably the most famous examples of depicting proverbs are the different versions of the paintings "Netherlandish Proverbs" by the father and son Pieter Bruegel the Elder and Pieter Brueghel the Younger, the proverbial meanings of these paintings being the subject of a 2004 conference, which led to a published volume of studies (Mieder 2004a). The same father and son also painted versions of The Blind Leading the Blind, a Biblical proverb. These and similar paintings inspired another famous painting depicting some proverbs and also idioms (leading to a series of additional paintings) "Proverbidioms" by T. E. Breitenbach. Another painting inspired by Bruegel's work is by the Chinese artist, Ah To, who created a painting illustrating 81 Cantonese sayings. Corey Barksdale has produced a book of paintings with specific proverbs and pithy quotations. The British artist Chris Gollon has painted a major work entitled "Big Fish Eat Little Fish", a title echoing Bruegel's painting Big Fishes Eat Little Fishes.
Sometimes well-known proverbs are pictured on objects, without a text actually quoting the proverb, such as the three wise monkeys who remind us "Hear no evil, see no evil, speak no evil". When the proverb is well known, viewers are able to recognize the proverb and understand the image appropriately, but if viewers do not recognize the proverb, much of the effect of the image is lost. For example, there is a Japanese painting in the Bonsai museum in Saitama city that depicted flowers on a dead tree, but only when the curator learned the ancient (and no longer current) proverb "Flowers on a dead tree" did the curator understand the deeper meaning of the painting.
A bibliography on proverbs in visual form has been prepared by Mieder and Sobieski (1999). Interpreting visual images of proverbs is subjective, but familiarity with the depicted proverb helps.
In an abstract non-representational visual work, sculptor Mark di Suvero has created a sculpture titled "Proverb", which is located in Dallas, TX, near the Morton H. Meyerson Symphony Center.
Some artists have used proverbs and anti-proverbs for titles of their paintings, alluding to a proverb rather than picturing it. For example, Vivienne LeWitt painted a piece titled "If the shoe doesn’t fit, must we change the foot?", which shows neither foot nor shoe, but a woman counting her money as she contemplates different options when buying vegetables.
Proverbs in cartoons.
Cartoonists, both editorial and pure humorists, have often used proverbs, sometimes primarily building on the text, sometimes primarily on the situation visually, the best cartoons combining both. Not surprisingly, cartoonists often twist proverbs, such as visually depicting a proverb literally or twisting the text as an anti-proverb. An example with all of these traits is a cartoon showing a waitress delivering two plates with worms on them, telling the customers, "Two early bird specials... here ya go."
The traditional Three wise monkeys were depicted in Bizarro with different labels. Instead of the negative imperatives, the one with ears covered bore the sign “See and speak evil”, the one with eyes covered bore the sign “See and hear evil”, etc. The caption at the bottom read “The power of positive thinking.” Another cartoon showed a customer in a pharmacy telling a pharmacist, “I'll have an ounce of prevention.” The comic strip The Argyle Sweater showed an Egyptian archeologist loading a mummy on the roof of a vehicle, refusing the offer of a rope to tie it on, with the caption “A fool and his mummy are soon parted.” The comic One Big Happy showed a conversation where one person repeatedly posed part of various proverb and the other tried to complete each one, resulting in such humorous results as “Don't change horses... unless you can lift those heavy diapers.”
Editorial cartoons can use proverbs to make their points with extra force as they can invoke the wisdom of society, not just the opinion of the editors. In an example that invoked a proverb only visually, when a US government agency (GSA) was caught spending money extravagantly, a cartoon showed a black pot labeled “Congress” telling a black kettle labeled “GSA”, “Stop wasting the taxpayers' money!” It may have taken some readers a moment of pondering to understand it, but the impact of the message was the stronger for it.
Cartoons with proverbs are so common that Wolfgang Mieder has published a collected volume of them, many of them editorial cartoons. For example, a German editorial cartoon linked a current politician to the Nazis, showing him with a bottle of swastika-labeled wine and the caption “In vino veritas.” 
One cartoonist very self-consciously drew and wrote cartoons based on proverbs for the University of Vermont student newspaper "The Water Tower", under the title "Proverb place".
Applications of proverbs.
There is a growing interest in deliberately using proverbs to achieve goals, usually to support and promote changes in society. On the negative side, this was deliberately done by the Nazis. On the more positive side, proverbs have also been used for constructive purposes. For example, proverbs have been used for teaching foreign languages at various levels. In addition, proverbs have been used for public health promotion, such as promoting breast feeding with a shawl bearing a Swahili proverb “Mother’s milk is sweet”. Proverbs have also been applied for helping people manage diabetes, to combat prostitution, and for community development., to resolve conflicts, and to slow the transmission of HIV.
The most active field deliberately using proverbs is Christian ministry, where Joseph G. Healey and others have deliberately worked to catalyze the collection of proverbs from smaller languages and the application of them in a wide variety of church-related ministries, resulting in publications of collections and applications. This attention to proverbs by those in Christian ministries is not new, many pioneering proverb collections having been collected and published by Christian workers.
U.S. Navy Captain Edward Zellem pioneered the use of Afghan proverbs as a positive relationship-building tool during the war in Afghanistan, and in 2012 he published two bilingual collections of Afghan proverbs in Dari and English, part of an effort of nationbuilding.
Borrowing and spread of proverbs.
Proverbs are often and easily translated and transferred from one language into another. “There is nothing so uncertain as the derivation of proverbs, the same proverb being often found in all nations, and it is impossible to assign its paternity.”
Proverbs are often borrowed across lines of language, religion, and even time. For example, a proverb of the approximate form “No flies enter a mouth that is shut” is currently found in Spain, France, Ethiopia, and many countries in between. It is embraced as a true local proverb in many places and should not be excluded in any collection of proverbs because it is shared by the neighbors. However, though it has gone through multiple languages and millennia, the proverb can be traced back to an ancient Babylonian proverb (Pritchard 1958:146).
In the Alaaba and Gurage languages of south central Ethiopia, there is a proverb, “The she-dog [bitch], because she is in extreme hurry gives birth to blind (ones).” It is also found in Pashto language of Afghanistan. Erasmus also gave a Latin form of it in his "Adagia", "Canis festinans caecos parit catulos". This proverb is also well attested in ancient Greek and even Akkadian texts, where Moran gives it as “The bitch by her acting too hastily brought forth the blind”. Alster, documenting an Akkadian inscription, classified this proverb as having “a longer history than any other recorded proverb in the world”, going back to “around 1800 BC”.
Another example of a widely spread proverb is “A drowning person clutches at [frogs] foam”, found in Peshai of Afghanistan and Orma of Kenya, and presumably places in between.
Proverbs about one hand clapping are common across Asia, from Dari in Afghanistan to Japan.
Some studies have been done devoted to the spread of proverbs in certain regions, such as India and her neighbors and Europe.
An extreme example of the borrowing and spread of proverbs was the work done to create a corpus of proverbs for Esperanto, where all the proverbs were translated from other languages.
It is often not possible to trace the direction of borrowing a proverb between languages. This is complicated by the fact that the borrowing may have been through plural languages. In some cases, it is possible to make a strong case for discerning the direction of the borrowing based on an artistic form of the proverb in one language, but a prosaic form in another language. For example, in Ethiopia there is a proverb “Of mothers and water, there is none evil.” It is found in Amharic, Alaaba language, and Oromo, three languages of Ethiopia:
The Oromo version uses poetic features, such as the initial "ha" in both clauses with the final "-aa" in the same word, and both clauses ending with "-an". Also, both clauses are built with the vowel "a" in the first and last words, but the vowel "i" in the one syllable central word. In contrast, the Amharic and Alaaba versions of the proverb show little evidence of sound-based art. Based on the verbal artistry of the Oromo, it appears that the Oromo form is prior to the Alaaba or Amharic, though it could be borrowed from yet another language.
Are cultural values reflected in proverbs?
There is a longstanding debate among proverb scholars as to whether the cultural values of specific language communities are reflected (to varying degree) in their proverbs. Many claim that the proverbs of a particular culture reflect the values of that specific culture, at least to some degree. Many writers have asserted that the proverbs of their cultures reflect their culture and values; this can be seen in such titles as the following: "An introduction to Kasena society and culture through their proverbs", Prejudice, power, and poverty in Haiti: a study of a nation's culture as seen through its proverbs, Proverbiality and worldview in Maltese and Arabic proverbs, Fatalistic traits in Finnish proverbs, "Vietnamese cultural patterns and values as expressed in proverbs", and "The Wisdom and Philosophy of the Gikuyu proverbs: The Kihooto worldview".
However, a number of scholars argue that such claims are not valid. They have used a variety of arguments. Grauberg argues that since many proverbs are so widely circulated they are reflections of broad human experience, not any one culture's unique viewpoint. Related to this line of argument, from a collection of 199 American proverbs, Jente showed that only 10 were coined in the USA, so that most of these proverbs would not reflect uniquely American values. Giving another line of reasoning that proverbs should not be trusted as a simplistic guide to cultural values, Mieder once observed “proverbs come and go, that is, antiquated proverbs with messages and images we no longer relate to are dropped from our proverb repertoire, while new proverbs are created to reflect the mores and values of our time”, so old proverbs still in circulation might reflect past values of a culture more than its current values. Also, within any language’s proverb repertoire, there may be “counter proverbs”, proverbs that contradict each other on the surface (see section above). When examining such counter proverbs, it is difficult to discern an underlying cultural value. With so many barriers to a simple calculation of values directly from proverbs, some feel "one cannot draw conclusions about values of speakers simply from the texts of proverbs".
Many outsiders have studied proverbs to discern and understand cultural values and world view of cultural communities. These outsider scholars are confident that they have gained insights into the local cultures by studying proverbs, but this is not universally accepted.
Seeking empirical evidence to evaluate the question of whether proverbs reflect a culture’s values, some have counted the proverbs that support various values. For example, Moon lists what he sees as the top ten core cultural values of the Builsa society of Ghana, as exemplified by proverbs. He found that 18% of the proverbs he analyzed supported the value of being a member of the community, rather than being independent. This was corroboration to other evidence that collective community membership is an important value among the Builsa. In studying Tajik proverbs, Bell notes that the proverbs in his corpus “Consistently illustrate Tajik values” and “The most often observed proverbs reflect the focal and specific values” discerned in the thesis 
There are many examples where cultural values have been explained and illustrated by proverbs. For example, from India, the concept that birth determines one's nature "is illustrated in the oft-repeated proverb: there can be no friendship between grass-eaters and meat-eaters, between a food and its eater". Proverbs have been used to explain and illustrate the Fulani cultural value of "pulaaku". But using proverbs to "illustrate" a cultural value is not the same as using a collection of proverbs to "discern" cultural values.
Some scholars have adopted a cautious approach, acknowledging at least a genuine, though limited, link between cultural values and proverbs: “The cultural portrait painted by proverbs may be fragmented, contradictory, or otherwise at variance with reality... but must be regarded not as accurate renderings but rather as tantalizing shadows of the culture which spawned them.” There is not yet agreement on the issue of whether, and how much, cultural values are reflected in a culture's proverbs.
It is clear that the Soviet Union believed that proverbs had a direct link to the values of a culture, as they used them to try to create changes in the values of cultures within their sphere of domination. Sometimes they took old Russian proverbs and altered them into socialist forms. These new proverbs promoted Socialism and its attendant values, such as atheism and collectivism, e.g. “Bread is given to us not by Christ, but by machines and collective farms” and “A good harvest is had only by a collective farm.” They did not limit their efforts to Russian, but also produced “newly coined proverbs that conformed to socialist thought” in Tajik and other languages of the USSR.
Proverbs and religion.
Many proverbs from around the world address matters of ethics and expected of behavior. Therefore, it is not surprising that proverbs are often important texts in religions. The most obvious example is the Book of Proverbs in the Bible. Additional proverbs have also been coined to support religious values, such as the following from Dari of Afghanistan: "In childhood you're playful, In youth you're lustful, In old age you're feeble, So when will you before God be worshipful?"
Clearly proverbs in religion are not limited to monotheists; among the Badaga of India (Sahivite Hindus), there is a traditional proverb "Catch hold of and join with the man who has placed sacred ash [on himself]." Proverbs are widely associated with large religions that draw from sacred books, but they are also used for religious purposes among groups with their own traditional religions, such as the Guji Oromo. The broadest comparative study of proverbs across religions is "The eleven religions and their proverbial lore, a comparative study. A reference book to the eleven surviving major religions of the world" by Selwyn Gurney Champion, from 1945. Some sayings from sacred books also become proverbs, even if they were not obviously proverbs in the original passage of the sacred book. For example, many quote "Be sure your sin will find you out" as a proverb from the Bible, but there is no evidence it was proverbial in its original usage (Numbers 32:23).
Not all religious references in proverbs are positive, some are cynical, such as the Tajik, "Do as the mullah says, not as he does." Also, note the Italian proverb, "One barrel of wine can work more miracles than a church full of saints". An Indian proverb is cynical about devotees of Hinduism, "[Only] When in distress, a man calls on Rama". In the context of Tibetan Buddhism, some Ladakhi proverbs mock the lamas, e.g. "If the lama's own head does not come out cleanly, how will he do the drawing upwards of the dead?... used for deriding the immoral life of the lamas."
Dammann thought "The influence of Islam manifests itself in African proverbs... Christian influences, on the contrary, are rare." If widely true in Africa, this is likely due to the longer presence of Islam in many parts of Africa. Reflection of Christian values is common in Amharic proverbs of Ethiopia, an area that has had a presence of Christianity for well over 1,000 years.
Proverbs and psychology.
Though much proverb scholarship is done by literary scholars, those studying the human mind have used proverbs in a variety of studies. One of the earliest studies in this field is the "Proverbs Test" by Gorham, developed in 1956. A similar test is being prepared in German. Proverbs have been used to evaluate dementia, study the cognitive development of children, measure the results of brain injuries, and study how the mind processes figurative language.
Proverbs in advertising.
Proverbs are frequently used in advertising, often in slightly modified form.
Ford once advertised its Thunderbird with, "One drive is worth a thousand words" (Mieder 2004b: 84). This is doubly interesting since the underlying proverb behind this, "One picture is worth a thousand words," was originally introduced into the English proverb repertoire in an ad for televisions (Mieder 2004b: 83).
A few of the many proverbs adapted and used in advertising include:
The GEICO company has created a series of television ads that are built around proverbs, such as "A bird in the hand is worth two in the bush", and "The pen is mightier than the sword", "Pigs may fly/When pigs fly", "If a tree falls in the forest...", and "Words can never hurt you".
Use of proverbs in advertising is not limited to the English language. Seda Başer Çoban has studied the use of proverbs in Turkish advertising. Tatira has given a number of examples of proverbs used in advertising in Zimbabwe. However, unlike the examples given above in English, all of which are anti-proverbs, Tatira's examples are standard proverbs. Where the English proverbs above are meant to make a potential customer smile, in one of the Zimbabwean examples "both the content of the proverb and the fact that it is phrased as a proverb secure the idea of a secure time-honored relationship between the company and the individuals". When newer buses were imported, owners of older buses compensated by painting a traditional proverb on the sides of their buses, "Going fast does not assure safe arrival".
Conservative language.
Because many proverbs are both poetic and traditional, they are often passed down in fixed forms. Though spoken language may change, many proverbs are often preserved in conservative, even archaic, form. In English, for example, "betwixt" is not used by many, but a form of it is still heard (or read) in the proverb "There is many a slip 'twixt the cup and the lip." The conservative form preserves the meter and the rhyme. This conservative nature of proverbs can result in archaic linguistic structures being preserved in individual proverbs, as has been documented in Amharic, Greek. and Nsenga.
In addition, proverbs may still be used in languages which were once more widely known in a society, but are now no longer so widely known. For example, English speakers use some non-English proverbs that are drawn from languages that used to be widely understood by the educated class, e.g. "C'est la vie" from French and "Carpe diem" from Latin.
Proverbs are often handed down through generations. Therefore, it is common that they preserve words that become less common and archaic in broader society. For example, English has a proverb "The cobbler's children have no shoes". The word "cobbler", meaning a maker of shoes, is now unknown among many English speakers, but it is preserved in the proverb.
Sources for proverb study.
A seminal work in the study of proverbs is Archer Taylor's "The Proverb" (1931), later republished by Wolfgang Mieder with Taylor's Index included (1985/1934). A good introduction to the study of proverbs is Mieder's 2004 volume, "Proverbs: A Handbook". Mieder has also published a series of bibliography volumes on proverb research, as well as a large number of articles and other books in the field. Stan Nussbaum has edited a large collection on proverbs of Africa, published on a CD, including reprints of out-of-print collections, original collections, and works on analysis, bibliography, and application of proverbs to Christian ministry (1998). Paczolay has compared proverbs across Europe and published a collection of similar proverbs in 55 languages (1997). Mieder edits an academic journal of proverb study, "Proverbium" (ISSN: 0743-782X), many back issues of which are available online. A volume containing articles on a wide variety of topics touching on proverbs was edited by Mieder and Alan Dundes (1994/1981). "Paremia" is a Spanish-language journal on proverbs, with articles available online. There are also papers on proverbs published in conference proceedings volumes from the annual Interdisciplinary Colloquium on Proverbs in Tavira, Portugal. Mieder has published a two-volume "International Bibliography of Paremiology and Phraseology", with a topical, language, and author index. Mieder has published a bibliography of collections of proverbs from around the world.
External links.
Serious websites related to the study of proverbs, and some that list regional proverbs:

</doc>
<doc id="23531" url="http://en.wikipedia.org/wiki?curid=23531" title="Portability (social security)">
Portability (social security)

The portability of social security benefits is the ability of workers to preserve, maintain, and transfer acquired social security rights and social security rights in the process of being acquired from one private, occupational, or public social security scheme to another. Social security rights refer to rights stemming from pension schemes (old age, survivor, disability), unemployment insurance, health insurance, workers' compensation, and sickness benefits. 
Hence, if social security benefits are portable, contributors to, for example, old-age pension schemes do not experience any disadvantage such as the loss of contributions and benefits associated with these contributions when moving from one job to another, from one occupation to another, or from the public to the private sector or vice versa.
International portability of social security rights allows international migrants, who have contributed to a social security scheme for some time in a particular country, to maintain acquired benefits or benefits in the process of being acquired when moving to another country. International portability of social security benefits is therefore understood as the migrant's ability to preserve, maintain, and transfer acquired social security rights independent of nationality and country of residence.
International portability of social security benefits is achieved through bilateral or multilateral social security agreements between countries. These agreements guarantee the totalization of periods of contribution to the social security systems of both countries and the extraterritorial payment of benefits. Currently it is estimated that approximately 23 per cent of migrants worldwide are covered by bilateral social security agreements.

</doc>
<doc id="23534" url="http://en.wikipedia.org/wiki?curid=23534" title="Percopsiformes">
Percopsiformes

The Percopsiformes are a small order of ray-finned fishes, comprising the trout-perch and its allies. It contains just ten extant species, grouped into seven genera and three families. Five of these genera are monotypic
They are generally small fish, ranging from 5 to in adult body length. They inhabit freshwater habitats in North America. They are grouped together because of technical characteristics of their internal anatomy, and the different species may appear quite different externally.

</doc>
<doc id="23535" url="http://en.wikipedia.org/wiki?curid=23535" title="Photon">
Photon

A photon is an elementary particle, the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level, because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles. For example, a single photon may be refracted by a lens or exhibit wave interference with itself, but also act as a particle giving a definite result when its position is measured.
The modern photon concept was developed gradually by Albert Einstein to explain experimental observations that did not fit the classical wave model of light. In particular, the photon model accounted for the frequency dependence of light's energy, and explained the ability of matter and radiation to be in thermal equilibrium. It also accounted for anomalous observations, including the properties of black-body radiation, that other physicists, most notably Max Planck, had sought to explain using "semiclassical models", in which light is still described by Maxwell's equations, but the material objects that emit and absorb light do so in amounts of energy that are "quantized" (i.e., they change energy only by certain particular discrete amounts and cannot change energy in any arbitrary way). Although these semiclassical models contributed to the development of quantum mechanics, many further experiments starting with Compton scattering of single photons by electrons, first observed in 1923, validated Einstein's hypothesis that "light itself" is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name "photon" for these particles, and after 1927, when Arthur H. Compton won the Nobel Prize for his scattering studies, most scientists accepted the validity that quanta of light have an independent existence, and the term "photon" for light quanta was accepted.
In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass and spin, are determined by the properties of this gauge symmetry.
The photon concept has led to momentous advances in experimental and theoretical physics, such as lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers and for applications in optical imaging and optical communication such as quantum cryptography.
Nomenclature.
In 1900, Max Planck was working on black-body radiation and suggested that the energy in electromagnetic waves could only be released in "packets" of energy. In his 1901 article in Annalen der Physik he called these packets "energy elements". The word "quanta" (singular "quantum") was used even before 1900 to mean particles or amounts of different quantities, including electricity. Later, in 1905, Albert Einstein went further by suggesting that electromagnetic waves could only exist in these discrete wave-packets. He called such a wave-packet "the light quantum" (German: "das Lichtquant"). The name "photon" derives from the Greek word for light, "φῶς" (transliterated "phôs"). Arthur Compton used "photon" in 1928, referring to Gilbert N. Lewis. The same name was used earlier, by the American physicist and psychologist Leonard T. Troland, who coined the word in 1916, in 1921 by the Irish physicist John Joly and in 1926 by the French physiologist René Wurmser (1890-1993) and by the French physicist Frithiof Wolfers (ca. 1890-1971). The name was suggested initially as a unit related to the illumination of the eye and the resulting sensation of light and was used later on in a physiological context. Although Wolfers's and Lewis's theories were never accepted, as they were contradicted by many experiments, the new name was adopted very soon by most physicists after Compton used it.
In physics, a photon is usually denoted by the symbol "γ" (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by "hν", the energy of a photon, where "h" is Planck's constant and the Greek letter "ν" (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by "hf", where its frequency is denoted by "f".
Physical properties.
A photon is massless, has no electric charge, and is stable. A photon has two possible polarization states. In the momentum representation, which is preferred in quantum field theory, a photon is described by its wave vector, which determines its wavelength "λ" and its direction of propagation. A photon's wave vector may not be zero and can be represented either as a spatial 3-vector or as a (relativistic) four-vector; in the latter case it belongs to the light cone (pictured). Different signs of the four-vector denote different circular polarizations, but in the 3-vector representation one should account for the polarization state separately; it actually is a spin quantum number. In both cases the space of possible wave vectors is three-dimensional.
The photon is the gauge boson for electromagnetism,:29-30 and therefore all other quantum numbers of the photon (such as lepton number, baryon number, and flavour quantum numbers) are zero. Also, the photon does not obey the Pauli exclusion principle.:1221
Photons are emitted in many natural processes. For example, when a charge is accelerated it emits synchrotron radiation. During a molecular, atomic or nuclear transition to a lower energy level, photons of various energy will be emitted, from radio waves to gamma rays. A photon can also be emitted when a particle and its corresponding antiparticle are annihilated (for example, electron–positron annihilation).:572, 1114, 1172
In empty space, the photon moves at "c" (the speed of light) and its energy and momentum are related by , where "p" is the magnitude of the momentum vector p. This derives from the following relativistic relation, with :
The energy and momentum of a photon depend only on its frequency ("ν") or inversely, its wavelength ("λ"):
where k is the wave vector (where the wave number ), is the angular frequency, and is the reduced Planck constant.
Since p points in the direction of the photon's propagation, the magnitude of the momentum is
The photon also carries spin angular momentum that does not depend on its frequency. The magnitude of its spin is formula_5 and the component measured along its direction of motion, its helicity, must be ±ħ. These two possible helicities, called right-handed and left-handed, correspond to the two possible circular polarization states of the photon.
To illustrate the significance of these formulae, the annihilation of a particle with its antiparticle in free space must result in the creation of at least "two" photons for the following reason. In the center of momentum frame, the colliding antiparticles have no net momentum, whereas a single photon always has momentum (since it is determined, as we have seen, only by the photon's frequency or wavelength—which cannot be zero). Hence, conservation of momentum (or equivalently, translational invariance) requires that at least two photons are created, with zero net momentum. (However, it is possible if the system interacts with another particle or field for annihilation to produce one photon, as when a positron annihilates with a bound atomic electron, it is possible for only one photon to be emitted, as the nuclear Coulomb field breaks translational symmetry.):64-65 The energy of the two photons, or, equivalently, their frequency, may be determined from conservation of four-momentum. Seen another way, the photon can be considered as its own antiparticle. The reverse process, pair production, is the dominant mechanism by which high-energy photons such as gamma rays lose energy while passing through matter. That process is the reverse of "annihilation to one photon" allowed in the electric field of an atomic nucleus.
The classical formulae for the energy and momentum of electromagnetic radiation can be re-expressed in terms of photon events. For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.
Experimental checks on photon mass.
Current commonly accepted physical theories imply or assume the photon to be strictly massless, but this should be also checked experimentally. If the photon is not a strictly massless particle, it would not move at the exact speed of light in vacuum, "c". Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, "c", would then not be the actual speed at which light moves, but a constant of nature which is the maximum speed that any object could theoretically attain in space-time. Thus, it would still be the speed of space-time ripples (gravitational waves and gravitons), but it would not be the speed of photons.
If a photon did have non-zero mass, there would be other effects as well. Coulomb's law would be modified and the electromagnetic field would have an extra physical degree of freedom. These effects yield more sensitive experimental probes of the photon mass than the frequency dependence of the speed of light. If Coulomb's law is not exactly valid, then that would cause the presence of an electric field inside a hollow conductor when it is subjected to an external electric field. This thus allows one to test Coulomb's law to very high precision. A null result of such an experiment has set a limit of "m" ≲ 10−14 eV/c2.
Sharper upper limits have been obtained in experiments designed to detect effects caused by the galactic vector potential. Although the galactic vector potential is very large because the galactic magnetic field exists on very long length scales, only the magnetic field is observable if the photon is massless. In case of a massive photon, the mass term formula_6 would affect the galactic plasma. The fact that no such effects are seen implies an upper bound on the photon mass of "m" < . The galactic vector potential can also be probed directly by measuring the torque exerted on a magnetized ring. Such methods were used to obtain the sharper upper limit of 10−18eV/c2 (the equivalent of ) given by the Particle Data Group.
These sharp limits from the non-observation of the effects caused by the galactic vector potential have been shown to be model dependent. If the photon mass is generated via the Higgs mechanism then the upper limit of "m"≲10−14 eV/c2 from the test of Coulomb's law is valid.
Photons inside superconductors do develop a nonzero effective rest mass; as a result, electromagnetic forces become short-range inside superconductors.
Historical development.
In most theories up to the eighteenth century, light was pictured as being made up of particles. Since particle models cannot easily account for the refraction, diffraction and birefringence of light, wave theories of light were proposed by René Descartes (1637), Robert Hooke (1665), and Christiaan Huygens (1678); however, particle models remained dominant, chiefly due to the influence of Isaac Newton. In the early nineteenth century, Thomas Young and August Fresnel clearly demonstrated the interference and diffraction of light and by 1850 wave models were generally accepted. In 1865, James Clerk Maxwell's prediction that light was an electromagnetic wave—which was confirmed experimentally in 1888 by Heinrich Hertz's detection of radio waves—seemed to be the final blow to particle models of light.
The Maxwell wave theory, however, does not account for "all" properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.
At the same time, investigations of blackbody radiation carried out over four decades (1860–1900) by various researchers culminated in Max Planck's hypothesis that the energy of "any" system that absorbs or emits electromagnetic radiation of frequency "ν" is an integer multiple of an energy quantum "E" = "hν". As shown by Albert Einstein, some form of energy quantization "must" be assumed to account for the thermal equilibrium observed between matter and electromagnetic radiation; for this explanation of the photoelectric effect, Einstein received the 1921 Nobel Prize in physics.
Since the Maxwell theory of light allows for all possible energies of electromagnetic radiation, most physicists assumed initially that the energy quantization resulted from some unknown constraint on the matter that absorbs or emits the radiation. In 1905, Einstein was the first to propose that energy quantization was a property of electromagnetic radiation itself. Although he accepted the validity of Maxwell's theory, Einstein pointed out that many anomalous experiments could be explained if the "energy" of a Maxwellian light wave were localized into point-like quanta that move independently of one another, even if the wave itself is spread continuously over space. In 1909 and 1916, Einstein showed that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum "p" = "h"/"λ", making them full-fledged particles. This photon momentum was observed experimentally by Arthur Compton, for which he received the Nobel Prize in 1927. The pivotal question was then: how to unify Maxwell's wave theory of light with its experimentally observed particle nature? The answer to this question occupied Albert Einstein for the rest of his life, and was solved in quantum electrodynamics and its successor, the Standard Model (see Second quantization and The photon as a gauge boson, below).
Einstein's light quantum.
Unlike Planck, Einstein entertained the possibility that there might be actual physical quanta of light—what we now call photons. He noticed that a light quantum with energy proportional to its frequency would explain a number of troubling puzzles and paradoxes, including an unpublished law by Stokes, the ultraviolet catastrophe, and of course the photoelectric effect. Stokes's law said simply that the frequency of fluorescent light cannot be greater than the frequency of the light (usually ultraviolet) inducing it. He eliminated the ultraviolet catastrophe by imagining a gas of photons behaving like a gas of electrons that he had previously considered. He was advised by a colleague to be careful how he wrote up this paper, in order to not challenge Planck too directly, as he was a powerful figure, and indeed the warning was justified, as Planck never forgave him for writing it.
Early objections.
Einstein's 1905 predictions were verified experimentally in several ways in the first two decades of the 20th century, as recounted in Robert Millikan's Nobel lecture. However, before Compton's experiment showing that photons carried momentum proportional to their wave number (or frequency) (1922), most physicists were reluctant to believe that electromagnetic radiation itself might be particulate. (See, for example, the Nobel lectures of Wien, Planck and Millikan.) Instead, there was a widespread belief that energy quantization resulted from some unknown constraint on the matter that absorbs or emits radiation. Attitudes changed over time. In part, the change can be traced to experiments such as Compton scattering, where it was much more difficult not to ascribe quantization to light itself to explain the observed results.
Even after Compton's experiment, Niels Bohr, Hendrik Kramers and John Slater made one last attempt to preserve the Maxwellian continuous electromagnetic field model of light, the so-called BKS model. To account for the data then available, two drastic hypotheses had to be made:
However, refined Compton experiments showed that energy–momentum is conserved extraordinarily well in elementary processes; and also that the jolting of the electron and the generation of a new photon in Compton scattering obey causality to within 10 ps. Accordingly, Bohr and his co-workers gave their model "as honorable a funeral as possible". Nevertheless, the failures of the BKS model inspired Werner Heisenberg in his development of matrix mechanics.
A few physicists persisted in developing semiclassical models in which electromagnetic radiation is not quantized, but matter appears to obey the laws of quantum mechanics. Although the evidence for photons from chemical and physical experiments was overwhelming by the 1970s, this evidence could not be considered as "absolutely" definitive; since it relied on the interaction of light with matter, a sufficiently complicated theory of matter could in principle account for the evidence. Nevertheless, "all" semiclassical theories were refuted definitively in the 1970s and 1980s by photon-correlation experiments. Hence, Einstein's hypothesis that quantization is a property of light itself is considered to be proven.
Wave–particle duality and uncertainty principles.
Photons, like all quantum objects, exhibit wave-like and particle-like properties. Their dual wave–particle nature can be difficult to visualize. The photon displays clearly wave-like phenomena such as diffraction and interference on the length scale of its wavelength. For example, a single photon passing through a double-slit experiment lands on the screen exhibiting interference phenomena but only if no measure was made on the actual slit being run across. To account for the particle interpretation that phenomenon is called probability distribution but behaves according to Maxwell's equations. However, experiments confirm that the photon is "not" a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted "as a whole" by arbitrarily small systems, systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron. Nevertheless, the photon is "not" a point-like particle whose trajectory is shaped probabilistically by the electromagnetic field, as conceived by Einstein and others; that hypothesis was also refuted by the photon-correlation experiments cited above. According to our present understanding, the electromagnetic field itself is produced by photons, which in turn result from a local gauge symmetry and the laws of quantum field theory (see the Second quantization and Gauge boson sections below).
A key element of quantum mechanics is Heisenberg's uncertainty principle, which forbids the simultaneous measurement of the position and momentum of a particle along the same direction. Remarkably, the uncertainty principle for charged, material particles "requires" the quantization of light into photons, and even the frequency dependence of the photon's energy and momentum. An elegant illustration is Heisenberg's thought experiment for locating an electron with an ideal microscope. The position of the electron can be determined to within the resolving power of the microscope, which is given by a formula from classical optics
where formula_8 is the aperture angle of the microscope. Thus, the position uncertainty formula_9 can be made arbitrarily small by reducing the wavelength λ. The momentum of the electron is uncertain, since it received a "kick" formula_10 from the light scattering from it into the microscope. If light were "not" quantized into photons, the uncertainty formula_10 could be made arbitrarily small by reducing the light's intensity. In that case, since the wavelength and intensity of light can be varied independently, one could simultaneously determine the position and momentum to arbitrarily high accuracy, violating the uncertainty principle. By contrast, Einstein's formula for photon momentum preserves the uncertainty principle; since the photon is scattered anywhere within the aperture, the uncertainty of momentum transferred equals
giving the product formula_13, which is Heisenberg's uncertainty principle. Thus, the entire world is quantized; both matter and fields must obey a consistent set of quantum laws, if either one is to be quantized.
The analogous uncertainty principle for photons forbids the simultaneous measurement of the number formula_14 of photons (see Fock state and the Second quantization section below) in an electromagnetic wave and the phase formula_15 of that wave
See coherent state and squeezed coherent state for more details.
Both (photons and material) particles such as electrons create analogous interference patterns when passing through a double-slit experiment. For photons, this corresponds to the interference of a Maxwell light wave whereas, for material particles, this corresponds to the interference of the Schrödinger wave equation. Although this similarity might suggest that Maxwell's equations are simply Schrödinger's equation for photons, most physicists do not agree. For one thing, they are mathematically different; most obviously, Schrödinger's one equation solves for a complex field, whereas Maxwell's four equations solve for real fields. More generally, the normal concept of a Schrödinger probability wave function cannot be applied to photons. Being massless, they cannot be localized without being destroyed; technically, photons cannot have a position eigenstate formula_17, and, thus, the normal Heisenberg uncertainty principle formula_18 does not pertain to photons. A few substitute wave functions have been suggested for the photon, but they have not come into general use. Instead, physicists generally accept the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.
Another interpretation, that avoids duality, is the De Broglie–Bohm theory: knowned also as the "pilot-wave model", the photon in this theory is both, wave and particle. "This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored", J.S.Bell.
Bose–Einstein model of a photon gas.
In 1924, Satyendra Nath Bose derived Planck's law of black-body radiation without using any electromagnetism, but rather a modification of coarse-grained counting of phase space. Einstein showed that this modification is equivalent to assuming that photons are rigorously identical and that it implied a "mysterious non-local interaction", now understood as the requirement for a symmetric quantum mechanical state. This work led to the concept of coherent states and the development of the laser. In the same papers, Einstein extended Bose's formalism to material particles (bosons) and predicted that they would condense into their lowest quantum state at low enough temperatures; this Bose–Einstein condensation was observed experimentally in 1995. It was later used by Lene Hau to slow, and then completely stop, light in 1999 and 2001.
The modern view on this is that photons are, by virtue of their integer spin, bosons (as opposed to fermions with half-integer spin). By the spin-statistics theorem, all bosons obey Bose–Einstein statistics (whereas all fermions obey Fermi–Dirac statistics).
Stimulated and spontaneous emission.
In 1916, Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a relation between the rates at which atoms emit and absorb photons. The condition follows from the assumption that light is emitted and absorbed by atoms independently, and that the thermal equilibrium is preserved by interaction with atoms. Consider a cavity in thermal equilibrium and filled with electromagnetic radiation and atoms that can emit and absorb that radiation. Thermal equilibrium requires that the energy density formula_19 of photons with frequency formula_20 (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are "emitted" must equal the rate of "absorbing" them.
Einstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate formula_21 for a system to "absorb" a photon of frequency formula_20 and transition from a lower energy formula_23 to a higher energy formula_24 is proportional to the number formula_25 of atoms with energy formula_23 and to the energy density formula_19 of ambient photons with that frequency,
where formula_29 is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, and a return to the lower-energy state that is initiated by the interaction with a passing photon. Following Einstein's approach, the corresponding rate formula_30 for the emission of photons of frequency formula_20 and transition from a higher energy formula_24 to a lower energy formula_23 is
where formula_35 is the rate constant for emitting a photon spontaneously, and formula_36 is the rate constant for emitting it in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state i and that of atoms in state j must, on average, be constant; hence, the rates formula_21 and formula_30 must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of formula_39 and formula_25 is formula_41 where formula_42 are the degeneracy of the state i and that of j, respectively, formula_43 their energies, k the Boltzmann constant and T the system's temperature. From this, it is readily derived that
formula_44 and
The A and Bs are collectively known as the "Einstein coefficients".
Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients formula_35, formula_29 and formula_36 once physicists had obtained "mechanics and electrodynamics modified to accommodate the quantum hypothesis". In fact, in 1926, Paul Dirac derived the formula_36 rate constants in using a semiclassical approach, and, in 1927, succeeded in deriving "all" the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called "second quantization" or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.
Einstein was troubled by the fact that his theory seemed incomplete, since it did not determine the "direction" of a spontaneously emitted photon. A probabilistic nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which path it would follow. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's probabilistic interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.
Second quantization and high energy photon interactions.
In 1910, Peter Debye derived Planck's law of black-body radiation from a relatively simple assumption. He correctly decomposed the electromagnetic field in a cavity into its Fourier modes, and assumed that the energy in any mode was an integer multiple of formula_50, where formula_20 is the frequency of the electromagnetic mode. Planck's law of black-body radiation follows immediately as a geometric sum. However, Debye's approach failed to give the correct formula for the energy fluctuations of blackbody radiation, which were derived by Einstein in 1909.
In 1925, Born, Heisenberg and Jordan reinterpreted Debye's concept in a key way. As may be shown classically, the Fourier modes of the electromagnetic field—a complete set of electromagnetic plane waves indexed by their wave vector k and polarization state—are equivalent to a set of uncoupled simple harmonic oscillators. Treated quantum mechanically, the energy levels of such oscillators are known to be formula_52, where formula_20 is the oscillator frequency. The key new step was to identify an electromagnetic mode with energy formula_52 as a state with formula_14 photons, each of energy formula_50. This approach gives the correct energy fluctuation formula.
Dirac took this one step further. He treated the interaction between a charge and an electromagnetic field as a small perturbation that induces transitions in the photon states, changing the numbers of photons in the modes, while conserving energy and momentum overall. Dirac was able to derive Einstein's formula_35 and formula_36 coefficients from first principles, and showed that the Bose–Einstein statistics of photons is a natural consequence of quantizing the electromagnetic field correctly (Bose's reasoning went in the opposite direction; he derived Planck's law of black-body radiation by "assuming" B–E statistics). In Dirac's time, it was not yet known that all bosons, including photons, must obey Bose–Einstein statistics.
Dirac's second-order perturbation theory can involve virtual photons, transient intermediate states of the electromagnetic field; the static electric and magnetic interactions are mediated by such virtual photons. In such quantum field theories, the probability amplitude of observable events is calculated by summing over "all" possible intermediate steps, even ones that are unphysical; hence, virtual photons are not constrained to satisfy formula_59, and may have extra polarization states; depending on the gauge used, virtual photons may have three or four polarization states, instead of the two states of real photons. Although these transient virtual photons can never be observed, they contribute measurably to the probabilities of observable events. Indeed, such second-order and higher-order perturbation calculations can give apparently infinite contributions to the sum. Such unphysical results are corrected for using the technique of renormalization.
Other virtual particles may contribute to the summation as well; for example, two photons may interact indirectly through virtual electron–positron pairs. In fact, such photon-photon scattering (see two-photon physics), as well as electron-photon scattering, is meant to be one of the modes of operations of the planned particle accelerator, the International Linear Collider.
In modern physics notation, the quantum state of the electromagnetic field is written as a Fock state, a tensor product of the states for each electromagnetic mode
where formula_61 represents the state in which formula_62 photons are in the mode formula_63. In this notation, the creation of a new photon in mode formula_63 (e.g., emitted from an atomic transition) is written as formula_65. This notation merely expresses the concept of Born, Heisenberg and Jordan described above, and does not add any physics.
The hadronic properties of the photon.
Measurements of the interaction between energetic photons and hadrons show that the interaction is much more intense than expected by the interaction of merely photons with the hadron's electric charge. Furthermore, the interaction of energetic photons with protons is similar to the interaction of photons with neutrons in spite of the fact that the electric charge structures of protons and neutrons are substantially different.
A theory called Vector Meson Dominance (VMD) was developed to explain this effect. According to VMD, the photon is a superposition of the pure electromagnetic photon (which interacts only with electric charges) and vector meson.
However, if experimentally probed at very short distances, the intrinsic structure of the photon is recognized as a flux of quark and gluon components, quasi-free according to asymptotic freedom in QCD and described by the photon structure function. A comprehensive comparison of data with theoretical predictions is presented in a recent review.
The photon as a gauge boson.
The electromagnetic field can be understood as a gauge field, i.e., as a field that results from requiring that a gauge symmetry holds independently at every position in spacetime. For the electromagnetic field, this gauge symmetry is the Abelian U(1) symmetry of a complex number, which reflects the ability to vary the phase of a complex number without affecting observables or real valued functions made from it, such as the energy or the Lagrangian.
The quanta of an Abelian gauge field must be massless, uncharged bosons, as long as the symmetry is not broken; hence, the photon is predicted to be massless, and to have zero electric charge and integer spin. The particular form of the electromagnetic interaction specifies that the photon must have spin ±1; thus, its helicity must be formula_66. These two spin components correspond to the classical concepts of right-handed and left-handed circularly polarized light. However, the transient virtual photons of quantum electrodynamics may also adopt unphysical polarization states.
In the prevailing Standard Model of physics, the photon is one of four gauge bosons in the electroweak interaction; the other three are denoted W+, W− and Z0 and are responsible for the weak interaction. Unlike the photon, these gauge bosons have mass, owing to a mechanism that breaks their SU(2) gauge symmetry. The unification of the photon with W and Z gauge bosons in the electroweak interaction was accomplished by Sheldon Glashow, Abdus Salam and Steven Weinberg, for which they were awarded the 1979 Nobel Prize in physics. Physicists continue to hypothesize grand unified theories that connect these four gauge bosons with the eight gluon gauge bosons of quantum chromodynamics; however, key predictions of these theories, such as proton decay, have not been observed experimentally.
Contributions to the mass of a system.
The energy of a system that emits a photon is "decreased" by the energy formula_67 of the photon as measured in the rest frame of the emitting system, which may result in a reduction in mass in the amount formula_68. Similarly, the mass of a system that absorbs a photon is "increased" by a corresponding amount. As an application, the energy balance of nuclear reactions involving photons is commonly written in terms of the masses of the nuclei involved, and terms of the form formula_68 for the gamma photons (and for other relevant energies, such as the recoil energy of nuclei).
This concept is applied in key predictions of quantum electrodynamics (QED, see above). In that theory, the mass of electrons (or, more generally, leptons) is modified by including the mass contributions of virtual photons, in a technique known as renormalization. Such "radiative corrections" contribute to a number of predictions of QED, such as the magnetic dipole moment of leptons, the Lamb shift, and the hyperfine structure of bound lepton pairs, such as muonium and positronium.
Since photons contribute to the stress–energy tensor, they exert a gravitational attraction on other objects, according to the theory of general relativity. Conversely, photons are themselves affected by gravity; their normally straight trajectories may be bent by warped spacetime, as in gravitational lensing, and their frequencies may be lowered by moving to a higher gravitational potential, as in the Pound–Rebka experiment. However, these effects are not specific to photons; exactly the same effects would be predicted for classical electromagnetic waves.
Photons in matter.
Any 'explanation' of how photons travel through matter has to explain why different arrangements of matter are transparent or opaque at different wavelengths (light through carbon as diamond or not, as graphite) and why individual photons behave in the same way as large groups. Explanations that invoke 'absorption' and 're-emission' have to provide an explanation for the directionality of the photons (diffraction, reflection) and further explain how entangled photon pairs can travel through matter without their quantum state collapsing.
The simplest explanation is that light that travels through transparent matter does so at a lower speed than "c", the speed of light in a vacuum. In addition, light can also undergo scattering and absorption. There are circumstances in which heat transfer through a material is mostly radiative, involving emission and absorption of photons within it. An example would be in the core of the Sun. Energy can take about a million years to reach the surface. However, this phenomenon is distinct from scattered radiation passing diffusely through matter, as it involves local equilibrium between the radiation and the temperature. Thus, the time is how long it takes the "energy" to be transferred, not the "photons" themselves. Once in open space, a photon from the Sun takes only 8.3 minutes to reach Earth. The factor by which the speed of light is decreased in a material is called the refractive index of the material. In a classical wave picture, the slowing can be explained by the light inducing electric polarization in the matter, the polarized matter radiating new light, and the new light interfering with the original light wave to form a delayed wave. In a particle picture, the slowing can instead be described as a blending of the photon with quantum excitation of the matter (quasi-particles such as phonons and excitons) to form a polariton; this polariton has a nonzero effective mass, which means that it cannot travel at "c".
Alternatively, photons may be viewed as "always" traveling at "c", even in matter, but they have their phase shifted (delayed or advanced) upon interaction with atomic scatters: this modifies their wavelength and momentum, but not speed. A light wave made up of these photons does travel slower than the speed of light. In this view the photons are "bare", and are scattered and phase shifted, while in the view of the preceding paragraph the photons are "dressed" by their interaction with matter, and move without scattering or phase shifting, but at a lower speed.
Light of different frequencies may travel through matter at different speeds; this is called dispersion. In some cases, it can result in extremely slow speeds of light in matter. The effects of photon interactions with other quasi-particles may be observed directly in Raman scattering and Brillouin scattering.
Photons can also be absorbed by nuclei, atoms or molecules, provoking transitions between their energy levels. A classic example is the molecular transition of retinal C20H28O, which is responsible for vision, as discovered in 1958 by Nobel laureate biochemist George Wald and co-workers. The absorption provokes a cis-trans isomerization that, in combination with other such transitions, is transduced into nerve impulses. The absorption of photons can even break chemical bonds, as in the photodissociation of chlorine; this is the subject of photochemistry. Analogously, gamma rays can in some circumstances dissociate atomic nuclei in a process called photodisintegration.
Technological applications.
Photons have many applications in technology. These examples are chosen to illustrate applications of photons "per se", rather than general optical devices such as lenses, etc. that could operate under a classical theory of light. The laser is an extremely important application and is discussed above under stimulated emission.
Individual photons can be detected by several methods. The classic photomultiplier tube exploits the photoelectric effect: a photon landing on a metal plate ejects an electron, initiating an ever-amplifying avalanche of electrons. Charge-coupled device chips use a similar effect in semiconductors: an incident photon generates a charge on a microscopic capacitor that can be detected. Other detectors such as Geiger counters use the ability of photons to ionize gas molecules, causing a detectable change in conductivity.
Planck's energy formula formula_70 is often used by engineers and chemists in design, both to compute the change in energy resulting from a photon absorption and to predict the frequency of the light emitted for a given energy transition. For example, the emission spectrum of a fluorescent light bulb can be designed using gas molecules with different electronic energy levels and adjusting the typical energy with which an electron hits the gas molecules within the bulb.
Under some conditions, an energy transition can be excited by "two" photons that individually would be insufficient. This allows for higher resolution microscopy, because the sample absorbs energy only in the region where two beams of different colors overlap significantly, which can be made much smaller than the excitation volume of a single beam (see two-photon excitation microscopy). Moreover, these photons cause less damage to the sample, since they are of lower energy.
In some cases, two energy transitions can be coupled so that, as one system absorbs a photon, another nearby system "steals" its energy and re-emits a photon of a different frequency. This is the basis of fluorescence resonance energy transfer, a technique that is used in molecular biology to study the interaction of suitable proteins.
Several different kinds of hardware random number generator involve the detection of single photons. In one example, for each bit in the random sequence that is to be produced, a photon is sent to a beam-splitter. In such a situation, there are two possible outcomes of equal probability. The actual outcome is used to determine whether the next bit in the sequence is "0" or "1".
Recent research.
Much research has been devoted to applications of photons in the field of quantum optics. Photons seem well-suited to be elements of an extremely fast quantum computer, and the quantum entanglement of photons is a focus of research. Nonlinear optical processes are another active research area, with topics such as two-photon absorption, self-phase modulation, modulational instability and optical parametric oscillators. However, such processes generally do not require the assumption of photons "per se"; they may often be modeled by treating atoms as nonlinear oscillators. The nonlinear process of spontaneous parametric down conversion is often used to produce single-photon states. Finally, photons are essential in some aspects of optical communication, especially for quantum cryptography.
Additional references.
By date of publication:
Education with single photons:
</dl>

</doc>
<doc id="23537" url="http://en.wikipedia.org/wiki?curid=23537" title="Philipp Franz von Siebold">
Philipp Franz von Siebold

Philipp Franz Balthasar von Siebold (February 17, 1796 – October 18, 1866) was a German physician, botanist, and traveler. He taught some pupils Western medicine in Japan. He achieved prominence by his studies of Japanese flora and fauna and was the father of the female Japanese doctor Kusumoto Ine.
Career.
Early life.
Born into a family of doctors and professors of medicine in Würzburg (then in the Bishopric of Würzburg, later part of Bavaria), Siebold initially studied medicine at University of Würzburg from November 1815, where he became a member of the Corps Moenania Würzburg. One of his professors was Franz Xaver Heller (1775–1840), author of the "Flora Wirceburgensis" ("Flora of the Grand Duchy of Würzburg", 1810–1811). Ignaz Döllinger (1770–1841), his professor of anatomy and physiology, however, most influenced him. Döllinger was one of the first professors to understand and treat medicine as a natural science. Siebold stayed with Döllinger, where he came in regular contact with other scientists. He read the books of Humboldt, a famous naturalist and explorer, which probably raised his desire to travel to distant lands. Philipp Franz von Siebold became a physician by earning his M.D. degree in 1820. He initially practiced medicine in Heidingsfeld, in the Kingdom of Bavaria, now part of Würzburg.
Invited to Holland by an acquaintance of his family, Siebold applied for a position as a military physician, which would enable him to travel to the Dutch colonies. He entered the Dutch military service on June 19, 1822, and was appointed as ship's surgeon on the frigate "Adriana", sailing from Rotterdam to Batavia (present-day Jakarta) in the Dutch East Indies (now called Indonesia). On his trip to Batavia on the frigate "Adriana", Siebold practiced his knowledge of the Dutch language and also rapidly learned Malay, and during the long voyage he began a collection of marine fauna. He arrived in Batavia on February 18, 1823.
As an army medical officer, Siebold was posted to an artillery unit. However, he was given a room for a few weeks at the residence of the Governor-General of the Dutch East Indies, Baron Godert van der Capellen, to recover from an illness. With his erudition, he impressed the Governor-General, and also the director of the botanical garden at Buitenzorg (now Bogor), Caspar Georg Carl Reinwardt. These men sensed in Siebold a worthy successor to Engelbert Kaempfer and Carl Peter Thunberg, two former resident physicians at Dejima, a Dutch trading post in Japan, the latter of whom was the author of "Flora Japonica". The Batavian Academy of Arts and Sciences soon elected Siebold as a member.
Arrival in Japan.
On June 28, 1823, after only a few months in the Dutch East Indies, Siebold was posted as resident physician and scientist to Dejima, a small artificial island and trading post at Nagasaki, and arrived there on August 11, 1823. During an eventful voyage to Japan he only just escaped drowning during a typhoon in the East China Sea. As only a very small number of Dutch personnel were allowed to live on this island, the posts of physician and scientist had to be combined. Dejima had been in the possession of the Dutch East India Company (known as the VOC) since the 17th century, but the Company had gone bankrupt in 1798, after which a trading post was operated there by the Dutch state for political considerations, with notable benefits to the Japanese.
The European tradition of sending doctors with botanical training to Japan was a long one. Sent on a mission by the Dutch East India Company, Engelbert Kaempfer (1651–1716), a German physician and botanist who lived in Japan from 1690 until 1692, ushered in this tradition of a combination of physician and botanist. The Dutch East India Company did not, however, actually employ the Swedish botanist and physician Carl Peter Thunberg (1743–1828), who had arrived in Japan in 1775.
Medical practise.
Japanese scientists invited Siebold to show them the marvels of western science, and he learned in return through them much about the Japanese and their customs. After curing an influential local officer, Siebold gained the permission to leave the trade post. He used this opportunity to treat Japanese patients in the greater area around the trade post. Siebold is credited with the introduction of vaccination and pathological anatomy for the first time in Japan.
In 1824 Siebold started a medical school in Nagasaki, the "Narutaki-juku", that grew into a meeting place for around fifty "students". They helped him in his botanical and naturalistic studies. The Dutch language became the "lingua franca" (common spoken language) for these academic and scholarly contacts for a generation, until the Meiji Restoration.
His patients paid him in kind with a variety of objects and artifacts that would later gain historical significance. These everyday objects later became the basis of his large ethnographic collection, which consisted of everyday household goods, woodblock prints, tools and hand-crafted objects used by the Japanese people.
Japanese family.
During his stay in Japan, Siebold "lived together" with Kusumoto Taki (楠本滝), who gave birth to their daughter O-Ine in 1827. Siebold used to call his wife "Otakusa" (probably derived from O-Taki-san) and named a "Hydrangea" after her. O-Ine eventually became the first Japanese woman known to have received a physician's training and became a highly regarded practicing physician. She died in 1903.
Studies of Japanese fauna and flora.
His main interest, however, focused on the study of Japanese fauna and flora. He collected as much material as he could. Starting a small botanical garden behind his home (there was not much room on the small island) Siebold amassed over 1,000 native plants. In a specially built glasshouse he cultivated the Japanese plants to endure the Dutch climate. Local Japanese artists drew images of these plants, creating botanical illustrations and images of the daily life in Japan, which complemented his ethnographic collection. He hired Japanese hunters to track rare animals and collect specimens. Many specimens were collected with the help of his Japanese collaborators Keisuke Ito (1803–1901), Mizutani Sugeroku (1779–1833), Ōkochi Zonshin (1796–1882) and Katsuragawa Hoken (1797–1844), a physician to the Shogun. As well, Siebold's assistant and later successor, Heinrich Bürger (1806–1858), proved to be indispensable in carrying on Siebold's work in Japan.
Siebold first introduced to Europe such familiar garden-plants as the "Hosta" and the "Hydrangea otaksa". Unknown to the Japanese, he was also able to smuggle out germinative seeds of tea plants to the botanical garden "Buitenzorg" in Batavia. Through this single act, he started the tea culture in Java, a Dutch colony at the time. Until then Japan had strictly guarded the trade in tea plants. Remarkably, in 1833, Java already could boast a half million tea plants.
He also introduced Japanese knotweed ("Fallopia japonica"), which has become a highly invasive weed in Europe and North America. All derive from a single female plant collected by Siebold.
During his stay at Dejima, Siebold sent three shipments with an unknown number of herbarium specimens to Leiden, Ghent, Brussels and Antwerp. The shipment to Leiden contained the first specimens of the Japanese giant salamander ("Andrias japonicus") to be sent to Europe.
In 1825 the Indian government provided him with two assistants: apothecary and mineralogist Heinrich Bürger (his later successor) and the painter Carl Hubert de Villeneuve. Each would prove to be useful to Siebold's efforts that ranged from ethnographical to botanical to horticultural, when attempting to document the exotic Eastern Japanese experience.
Reportedly, Siebold was not the easiest man to deal with; he was in continuous conflict with his Dutch superiors, who felt he was arrogant. This threat of conflict resulted in his recall in July 1827 back to Batavia. But the ship, the "Cornelis Houtman", sent to carry him back to Batavia, was thrown ashore by a typhoon in Nagasaki bay. The same storm badly damaged Dejima and destroyed Siebold's botanical garden. Repaired, the "Cornelis Houtman" was refloated. It left for Batavia with 89 crates of Siebold's salvaged botanical collection, but Siebold himself remained behind in Dejima.
Siebold Incident.
In 1826 Siebold made the court journey to Edo. During this long trip he collected many plants and animals. But he also obtained from the court astronomer Takahashi Kageyasu several detailed maps of Japan and Korea (written by Inō Tadataka), an act strictly forbidden by the Japanese government. When the Japanese discovered, by accident, that Siebold had a map of the northern parts of Japan, the government accused him of high treason and of being a spy for Russia.
The Japanese placed Siebold under house arrest and expelled him from Japan on October 22, 1829. Satisfied that his Japanese collaborators would continue his work, he journeyed back on the frigate "Java" to his former residence, Batavia, in possession of his enormous collection of thousands of animals and plants, his books and his maps. The botanical garden of "Buitenzorg" would soon house Siebold's surviving, living flora collection of 2,000 plants. He arrived in the Netherlands on July 7, 1830. His stay in Japan and Batavia had lasted for a period of eight years.
Return to Europe.
Philipp Franz von Siebold arrived in the Netherlands in 1830, just at a time when political troubles erupted in Brussels, leading soon to Belgian independence. Hastily he salvaged his ethnographic collections in Antwerp and his herbarium specimens in Brussels and took them to Leiden. He left behind his botanical collections of living plants that were sent to the University of Ghent. The consequent expansion of this collection of rare and exotic plants led to the horticultural fame of Ghent. In gratitude the University of Ghent presented him in 1841 with specimens of every plant from his original collection.
Siebold settled in Leiden, taking with him the major part of his collection. The "Philipp Franz von Siebold collection", containing many type specimens, was the earliest botanical collection from Japan. Even today, it still remains a subject of ongoing research, a testimony to the depth of work undertaken by Siebold. It contained about 12,000 specimens, from which he could describe only about 2,300 species. The whole collection was purchased for a handsome amount by the Dutch government. Siebold was also granted a substantial annual allowance by the Dutch King William II and was appointed "Advisor to the King for Japanese Affairs". In 1842 the King even raised Siebold to the nobility as an esquire.
The "Siebold collection" opened to the public in 1831. He founded a museum in his home in 1837. This small, private museum would eventually evolve into the National Museum of Ethnology in Leiden. Siebold's successor in Japan, Heinrich Bürger, sent Siebold three more shipments of herbarium specimens collected in Japan. This flora collection formed the basis of the Japanese collections of the National Herbarium of the Netherlands in Leiden and the natural history museum Naturalis ("National Natuurhistorisch Museum").
In 1845 Siebold married Helene von Gagern (1820–1877), they had three sons and two daughters.
Writings.
During his stay in Leiden, Siebold wrote "Nippon" in 1832, the first part of a volume of a richly illustrated ethnographical and geographical work on Japan. The 'Archiv zur Beschreibung Nippons' also contained a report of his journey to the Shogunate Court at Edo. He wrote six further parts, the last ones published posthumously in 1882; his sons published an edited and lower-priced reprint in 1887.
The "Bibliotheca Japonica" appeared between 1833 and 1841. This work was co-authored by Joseph Hoffmann and Kuo Cheng-Chang, a Javanese of Chinese extraction, who had journeyed along with Siebold from Batavia. It contained a survey of Japanese literature and a Chinese, Japanese and Korean dictionary.
The zoologists Coenraad Temminck (1777–1858), Hermann Schlegel (1804–1884), and Wilhem de Haan (1801–1855) scientifically described and documented Siebold's collection of Japanese animals. The "Fauna Japonica", a series of monographs published between 1833 and 1850, was mainly based on Siebold's collection, making the Japanese fauna the best-described non-European fauna – "a remarkable feat". A significant part of the "Fauna Japonica" was also based on the collections of Siebold's successor on Dejima, Heinrich Bürger.
Siebold wrote his "Flora Japonica" in collaboration with the German botanist Joseph Gerhard Zuccarini (1797–1848). It first appeared in 1835, but the work was not completed until after his death, finished in 1870 by F. A. W. Miquel (1811–1871), director of the Rijksherbarium in Leiden. This work expanded Siebold's scientific fame from Japan to Europe.
From the Hortus Botanicus Leiden – the botanical garden of Leiden – many of Siebold's plants spread to Europe and from there to other countries. "Hosta" and "Hortensia", "Azalea", and the Japanese butterbur and the coltsfoot as well as the Japanese larch began to inhabit gardens across the world.
International reputation.
After his return to Europe, Siebold tried to exploit his knowledge of Japan. Whilst living in Boppard, from 1852 he corresponded with Russian diplomats such as Baron von Budberg-Bönninghausen, the Russian ambassador to Prussia, which resulted in an invitation to go to St Petersburg to advise the Russian government how to open trade relations with Japan. Though still employed by the Dutch government he did not inform the Dutch of this voyage until after his return. American Naval Commodore Matthew C. Perry consulted Siebold in advance of his voyage to Japan in 1854.
In 1858 the Japanese government lifted the banishment of Siebold. He returned to Japan in 1859 as an adviser to the Agent of the Dutch Trading Society (Nederlandsche Handel-Maatschappij) in Nagasaki, Albert Bauduin. After two years the connection with the Trading Society was severed as the advice of Siebold was of no value. In Nagasaki he fathered another child with one of his female servants. In 1861 Siebold organised his appointment as an adviser to the Japanese government and went in that function to Edo. There he tried to obtain a position between the foreign representatives and the Japanese government. As he had been specially admonished by the Dutch authorities before going to Japan that he was to abstain from all interference in politics, the Dutch Consul General in Japan, J. K. de Wit, was ordered to ask Siebold's removal. Siebold was ordered to return to Batavia and from there he returned to Europe. After his return he asked the Dutch government to employ him as Consul General in Japan but the Dutch government severed all relations with Siebold who had a huge debt because of loans given to him, except for the payment of his pension.
Siebold kept trying to organise an other voyage to Japan. After he did not succeed in gaining employment with the Russian government, he went to Paris in 1865 to try to interest the French government in funding another expedition to Japan, but failed. He died in Munich on October 18, 1866.
Plants named after Siebold.
The botanical and horticultural spheres of influence have honored Philipp Franz von Siebold by naming some of the very garden-worthy plants that he studied after him. Examples include:
Also a type of abalone, "Nordotis gigantea", is known as Siebold's abalone, and is prized for sushi.
Though he is well known in Japan, where he is called "Shiboruto-san", and although mentioned in the relevant schoolbooks, Siebold is almost unknown elsewhere, except among gardeners who admire the many plants whose names incorporate "sieboldii" and "sieboldiana". The Hortus Botanicus in Leiden has recently laid out the "Von Siebold Memorial Garden", a Japanese garden with plants sent by Siebold. The garden was laid out under a 150-year old "Zelkova serrata" tree dating from Siebold's lifetime. Japanese visitors come and visit this garden, to pay their respect for him.
Siebold museums.
Although he was disillusioned by what he perceived as a lack of appreciation for Japan and his contributions to its understanding, a testimony of the remarkable character of Siebold is found in museums that honor him.
His collections laid the foundation for the ethnographic museums of Munich and Leiden. Alexander von Siebold, his son by his European wife, donated much of the material left behind after Siebold's death in Würzburg to the British Museum in London. The Royal Scientific Academy of St. Petersburg purchased 600 colored plates of the "Flora Japonica".
Another son, Heinrich (or Henry) von Siebold (1852–1908), continued part of his father's research. He is recognized, together with Edward S. Morse, as one of the founders of modern archaeological efforts in Japan.
Published works.
The standard author abbreviation Siebold is used to indicate Philipp Franz von Siebold as the author when citing a botanical name.
References.
</dl>

</doc>
<doc id="23538" url="http://en.wikipedia.org/wiki?curid=23538" title="Probability interpretations">
Probability interpretations

The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory.
There are two broad categories of probability interpretations which can be called "physical" and "evidential" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as the dice yielding a six) tends to occur at a persistent rate, or "relative frequency", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. Thus talking about physical probability makes sense only when dealing with well defined random experiments. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer).
Evidential probability, also called Bayesian probability (or subjectivist probability), can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap).
Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of "frequentist" statistical methods, such as R. A. Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference.
The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word "frequentist" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, "frequentist probability" is just another name for physical (or objective) probability. Those who promote Bayesian inference view "frequentist statistics" as an approach to statistical inference that recognises only physical probabilities. Also the word "objective", as applied to probability, sometimes means exactly what "physical" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.
It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.—(Savage, 1954, p 2)
Philosophy.
The philosophy of probability presents problems chiefly in matters of epistemology and the uneasy interface between mathematical concepts and ordinary language as it is used by non-mathematicians.
Probability theory is an established field of study in mathematics. It has its origins in correspondence discussing the mathematics of games of chance between Blaise Pascal and Pierre de Fermat in the seventeenth century, and was formalized and rendered axiomatic as a distinct branch of mathematics by Andrey Kolmogorov in the twentieth century. In its axiomatic form, mathematical statements about probability theory carry the same sort of epistemological confidence shared by other mathematical statements in the philosophy of mathematics.
The mathematical analysis originated in observations of the behaviour of game equipment such as playing cards and dice, which are designed specifically to introduce random and equalized elements; in mathematical terms, they are subjects of indifference. This is not the only way probabilistic statements are used in ordinary human language: when people say that "it will probably rain", they typically do not mean that the outcome of rain versus not-rain is a random factor that the odds currently favor; instead, such statements are perhaps better understood as qualifying their expectation of rain with a degree of confidence. Likewise, when it is written that "the most probable explanation" of the name of Ludlow, Massachusetts "is that it was named after Roger Ludlow", what is meant here is not that Roger Ludlow is favored by a random factor, but rather that this is the most plausible explanation of the evidence, which admits other, less likely explanations.
Thomas Bayes attempted to provide a logic that could handle varying degrees of confidence; as such, Bayesian probability is an attempt to recast the representation of probabilistic statements as an expression of the degree of confidence by which the beliefs they express are held.
Though probability initially had somewhat mundane motivations, its modern influence and use is widespread ranging from Evidence based medicine, through Six sigma, all the way to the Probabilistically checkable proof and the String theory landscape.
 (p 1132)
Classical definition.
The first attempt at mathematical rigour in the field of probability, championed by Pierre-Simon Laplace, is now known as the classical definition. Developed from studies of games of chance (such as rolling dice) it states that probability is shared equally between all the possible outcomes, provided these outcomes can be deemed equally likely. (3.1)
 The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.
 — Pierre-Simon Laplace, "A Philosophical Essay on Probabilities"
This can be represented mathematically as follows:
If a random experiment can result in N mutually exclusive and equally likely outcomes and if NA of these outcomes result in the occurrence of the event A, the probability of A is defined by
There are two clear limitations to the classical definition. Firstly, it is applicable only to situations in which there is only a 'finite' number of possible outcomes. But some important random experiments, such as tossing a coin until it rises heads, give rise to an infinite set of outcomes. And secondly, you need to determine in advance that all the possible outcomes are equally likely without relying on the notion of probability to avoid circularity—for instance, by symmetry considerations.
Frequentism.
Frequentists posit that the probability of an event is its relative frequency over time, (3.4) i.e., its relative frequency of occurrence after repeating a process a large number of times under similar conditions. This is also known as aleatory probability. The events are assumed to be governed by some random physical phenomena, which are either phenomena that are predictable, in principle, with sufficient information (see Determinism); or phenomena which are essentially unpredictable. Examples of the first kind include tossing dice or spinning a roulette wheel; an example of the second kind is radioactive decay. In the case of tossing a fair coin, frequentists say that the probability of getting a heads is 1/2, not because there are two equally likely outcomes but because repeated series of large numbers of trials demonstrate that the empirical frequency converges to the limit 1/2 as the number of trials goes to infinity.
If we denote by formula_2 the number of occurrences of an event formula_3 in formula_4 trials, then if formula_5 we say that "formula_6
The frequentist view has its own problems. It is of course impossible to actually perform an infinity of repetitions of a random experiment to determine the probability of an event. But if only a finite number of repetitions of the process are performed, different relative frequencies will appear in different series of trials. If these relative frequencies are to define the probability, the probability will be slightly different every time it is measured. But the real probability should be the same every time. If we acknowledge the fact that we only can measure a probability with some error of measurement attached, we still get into problems as the error of measurement can only be expressed as a probability, the very concept we are trying to define. This renders even the frequency definition circular.
Logical, epistemic, and inductive probability.
It is widely recognised that the term "probability" is sometimes used in contexts where it has nothing to do with physical randomness. Consider, for example, the claim that the extinction of the dinosaurs was probably caused by a large meteorite hitting the earth. Statements such as "Hypothesis H is probably true" have been interpreted to mean that the (presently available) empirical evidence (E, say) supports H to a high degree. This degree of support of H by E has been called the logical probability of H given E, or the epistemic probability of H given E, or the inductive probability of H given E.
The differences between these interpretations are rather small, and may seem inconsequential. One of the main points of disagreement lies in the relation between probability and belief. Logical probabilities are conceived (for example in Keynes' Treatise on Probability) to be objective, logical relations between propositions (or sentences), and hence not to depend in any way upon belief. They are degrees of (partial) entailment, or degrees of logical consequence, not degrees of belief. (They do, nevertheless, dictate proper degrees of belief, as is discussed below.) Frank P. Ramsey, on the other hand, was skeptical about the existence of such objective logical relations and argued that (evidential) probability is "the logic of partial belief". (p 157) In other words, Ramsey held that epistemic probabilities simply "are" degrees of rational belief, rather than being logical relations that merely "constrain" degrees of rational belief.
Another point of disagreement concerns the "uniqueness" of evidential probability, relative to a given state of knowledge. Rudolf Carnap held, for example, that logical principles always determine a unique logical probability for any statement, relative to any body of evidence. Ramsey, by contrast, thought that while degrees of belief are subject to some rational constraints (such as, but not limited to, the axioms of probability) these constraints usually do not determine a unique value. Rational people, in other words, may differ somewhat in their degrees of belief, even if they all have the same information.
Propensity.
Propensity theorists think of probability as a physical propensity, or disposition, or tendency of a given type of physical situation to yield an outcome of a certain kind or to yield a long run relative frequency of such an outcome. This kind of objective probability is sometimes called 'chance'.
Propensities, or chances, are not relative frequencies, but purported causes of the observed stable relative frequencies. Propensities are invoked to explain why repeating a certain kind of experiment will generate given outcome types at persistent rates, which are known as propensities or chances. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives. In contrast, a propensitist is able to use the law of large numbers to explain the behaviour of long-run frequencies. This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, in such a way that its probability of landing heads is the same on each toss, and the outcomes are probabilistically independent, then the relative frequency of heads will be close to the probability of heads on each single toss. This law allows that stable long-run frequencies are a manifestation of invariant "single-case" probabilities. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time.
The main challenge facing propensity theories is to say exactly what propensity means. (And then, of course, to show that propensity thus defined has the required properties.) At present, unfortunately, none of the well-recognised accounts of propensity comes close to meeting this challenge.
A propensity theory of probability was given by Charles Sanders Peirce. A later propensity theory was proposed by philosopher Karl Popper, who had only slight acquaintance with the writings of C. S. Peirce, however. Popper noted that the outcome of a physical experiment is produced by a certain set of "generating conditions". When we repeat an experiment, as the saying goes, we really perform another experiment with a (more or less) similar set of generating conditions. To say that a set of generating conditions has propensity "p" of producing the outcome "E" means that those exact conditions, if repeated indefinitely, would produce an outcome sequence in which "E" occurred with limiting relative frequency "p". For Popper then, a deterministic experiment would have propensity 0 or 1 for each outcome, since those generating conditions would have same outcome on each trial. In other words, non-trivial propensities (those that differ from 0 and 1) only exist for genuinely indeterministic experiments.
A number of other philosophers, including David Miller and Donald A. Gillies, have proposed propensity theories somewhat similar to Popper's.
Other propensity theorists (e.g. Ronald Giere) do not explicitly define propensities at all, but rather see propensity as defined by the theoretical role it plays in science. They argue, for example, that physical magnitudes such as electrical charge cannot be explicitly defined either, in terms of more basic things, but only in terms of what they do (such as attracting and repelling other electrical charges). In a similar way, propensity is whatever fills the various roles that physical probability plays in science.
What roles does physical probability play in science? What are its properties? One central property of chance is that, when known, it constrains rational belief to take the same numerical value. David Lewis called this the "Principal Principle", (3.3 & 3.5) a term that philosophers have mostly adopted. For example, suppose you are certain that a particular biased coin has propensity 0.32 to land heads every time it is tossed. What is then the correct price for a gamble that pays $1 if the coin lands heads, and nothing otherwise? According to the Principal Principle, the fair price is 32 cents.
Subjectivism.
Subjectivists, also known as Bayesians or followers of epistemic probability, give the notion of probability a subjective status by regarding it as a measure of the 'degree of belief' of the individual assessing the uncertainty of a particular situation. Epistemic or subjective probability is sometimes called credence, as opposed to the term chance for a propensity probability.
Some examples of epistemic probability are to assign a probability to the proposition that a proposed law of physics is true, and to determine how probable it is that a suspect committed a crime, based on the evidence presented.
Gambling odds don't reflect the bookies' belief in a likely winner, so much as the other bettors' belief, because the bettors are actually betting against one another. The odds are set based on how many people have bet on a possible winner, so that even if the high odds players always win, the bookies will always make their percentages anyway.
The use of Bayesian probability raises the philosophical debate as to whether it can contribute valid justifications of belief.
Bayesians point to the work of Ramsey (p 182) and de Finetti (p 103) as proving that subjective beliefs must follow the laws of probability if they are to be coherent. Evidence casts doubt on human coherence.
The use of Bayesian probability involves specifying a prior probability. This may be obtained from consideration of whether the required prior probability is greater or lesser than a reference probability associated with an urn model or a thought experiment. The issue is that for a given problem, multiple thought experiments could apply, and choosing one is a matter of judgement: different people may assign different prior probabilities, known as the reference class problem.
The "sunrise problem" provides an example.
Prediction.
An alternative account of probability emphasizes the role of "prediction" – predicting future observations on the basis of past observations, not on unobservable parameters. In its modern form, it is mainly in the Bayesian vein. This was the main function of probability before the 20th century,
but fell out of favor compared to the parametric approach, which modeled phenomena as a physical system that was observed with error, such as in celestial mechanics.
The modern predictive approach was pioneered by Bruno de Finetti, with the central idea of exchangeability – that future observations should behave like past observations. This view came to the attention of the Anglophone world with the 1974 translation of de Finetti's book, and has
since been propounded by such statisticians as Seymour Geisser.
Axiomatic probability.
The mathematics of probability can be developed on an entirely axiomatic basis that is independent of any interpretation: see the articles on probability theory and probability axioms for a detailed treatment.

</doc>
<doc id="23539" url="http://en.wikipedia.org/wiki?curid=23539" title="Probability axioms">
Probability axioms

In Kolmogorov's probability theory, the probability "P" of some event "E", denoted formula_1, is usually defined such that "P" satisfies the Kolmogorov axioms, named after the famous Russian mathematician Andrey Kolmogorov, which are described below.
These assumptions can be summarised as follows: Let (Ω, "F", "P") be a measure space with "P"(Ω)=1. Then (Ω, "F", "P") is a probability space, with sample space Ω, event space "F" and probability measure "P".
An alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem.
Axioms.
First axiom.
The probability of an event is a non-negative real number:
where formula_3 is the event space. In particular, formula_1 is always finite, in contrast with more general measure theory. Theories which assign negative probability relax the first axiom.
Second axiom.
This is the assumption of unit measure: that the probability that some elementary event in the entire sample space will occur is 1. More specifically, there are no elementary events outside the sample space.
This is often overlooked in some mistaken probability calculations; if you cannot precisely define the whole sample space, then the probability of any subset cannot be defined either.
Third axiom.
This is the assumption of σ-additivity:
Some authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets, rather than a σ-algebra. Quasiprobability distributions in general relax the third axiom.
Consequences.
From the Kolmogorov axioms, one can deduce other useful rules for calculating probabilities.
The numeric bound.
It immediately follows from the monotonicity property that
Proofs.
The proofs of these properties are both interesting and insightful. They illustrate the power of the third axiom,
and its interaction with the remaining two axioms. When studying axiomatic probability theory, many deep consequences follow from merely these three axioms.
In order to verify the monotonicity property, we set formula_11 and formula_12,
where formula_13 for formula_14. It is easy to see that the sets formula_15
are pairwise disjoint and formula_16. Hence,
we obtain from the third axiom that
Since the left-hand side of this equation is a series of non-negative numbers, and that it converges to
formula_18 which is finite, we obtain both formula_19 and formula_20.
The second part of the statement is seen by contradiction: if formula_21 then the left hand side is not less than
If formula_23 then we obtain a contradiction, because the sum does not exceed formula_18 which is finite. Thus, formula_25. We have shown as a byproduct of the proof of monotonicity that formula_20.
Further consequences.
Another important property is:
This is called the addition law of probability, or the sum rule.
That is, the probability that "A" "or" "B" will happen is the sum of the
probabilities that "A" will happen and that "B" will happen, minus the
probability that both "A" "and" "B" will happen. The proof of this is as follows:
now, formula_29.
Eliminating formula_30 from both equations gives us the desired result.
This can be extended to the inclusion-exclusion principle.
That is, the probability that any event will "not" happen (or the event's complement) is 1 minus the probability that it will.
Simple example: coin toss.
Consider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both). No assumption is made as to whether the coin is fair.
We may define:
Kolmogorov's axioms imply that:
The probability of "neither" heads "nor" tails, is 0.
The probability of "either" heads "or" tails, is 1.
The sum of the probability of heads and the probability of tails, is 1.

</doc>
<doc id="23542" url="http://en.wikipedia.org/wiki?curid=23542" title="Probability theory">
Probability theory

Probability theory is the branch of mathematics concerned with probability, the analysis of random phenomena. The central objects of probability theory are random variables, stochastic processes, and events: mathematical abstractions of non-deterministic events or measured quantities that may either be single occurrences or evolve over time in an apparently random fashion. If an individual coin toss or the roll of dice is considered to be a random event, then if repeated many times the sequence of random events will exhibit certain patterns, which can be studied and predicted. Two representative mathematical results describing such patterns are the law of large numbers and the central limit theorem.
As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of large sets of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.
History.
The mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the "problem of points"). Christiaan Huygens published a book on the subject in 1657 and in the 19th century a big work was done by Laplace in what can be considered today as the classic interpretation.
Initially, probability theory mainly considered discrete events, and its methods were mainly combinatorial. Eventually, analytical considerations compelled the incorporation of continuous variables into the theory.
This culminated in modern probability theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. Fairly quickly this became the mostly undisputed axiomatic basis for modern probability theory but alternatives exist, in particular the adoption of finite rather than countable additivity by Bruno de Finetti.
Treatment.
Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The more mathematically advanced measure theory based treatment of probability covers both the discrete, the continuous, any mix of these two and more.
Motivation.
Consider an experiment that can produce a number of outcomes. The set of all outcomes is called the "sample space" of the experiment. The "power set" of the sample space is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of die rolls. These collections are called "events". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.
Probability is a way of assigning every "event" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that one of the events will occur is given by the sum of the probabilities of the individual events.
The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.
Discrete probability distributions.
Discrete probability theory deals with events that occur in countable sample spaces.
Examples: Throwing dice, experiments with decks of cards, random walk, and tossing coins
Classical definition:
Initially the probability of an event to occur was defined as number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: see Classical definition of probability.
For example, if the event is "occurrence of an even number when a die is rolled", the probability is given by formula_1, since 3 faces out of the 6 have even numbers and each face has the same probability of appearing.
Modern definition:
The modern definition starts with a finite or countable set called the sample space, which relates to the set of all "possible outcomes" in classical sense, denoted by formula_2. It is then assumed that for each element formula_3, an intrinsic "probability" value formula_4 is attached, which satisfies the following properties:
That is, the probability function "f"("x") lies between zero and one for every value of "x" in the sample space "Ω", and the sum of "f"("x") over all values "x" in the sample space "Ω" is equal to 1. An event is defined as any subset formula_7 of the sample space formula_8. The probability of the event formula_7 is defined as
So, the probability of the entire sample space is 1, and the probability of the null event is 0.
The function formula_4 mapping a point in the sample space to the "probability" value is called a probability mass function abbreviated as pmf. The modern definition does not try to answer how probability mass functions are obtained; instead it builds a theory that assumes their existence.
Continuous probability distributions.
Continuous probability theory deals with events that occur in a continuous sample space.
Classical definition:
The classical definition breaks down when confronted with the continuous case. See Bertrand's paradox.
Modern definition:
If the outcome space of a random variable "X" is the set of real numbers (formula_12) or a subset thereof, then a function called the cumulative distribution function (or cdf) formula_13 exists, defined by formula_14. That is, "F"("x") returns the probability that "X" will be less than or equal to "x".
The cdf necessarily satisfies the following properties.
If formula_13 is absolutely continuous, i.e., its derivative exists and integrating the derivative gives us the cdf back again, then the random variable "X" is said to have a probability density function or pdf or simply density formula_19
For a set formula_20, the probability of the random variable "X" being in formula_7 is
In case the probability density function exists, this can be written as
Whereas the "pdf" exists only for continuous random variables, the "cdf" exists for all random variables (including discrete random variables) that take values in formula_24
These concepts can be generalized for multidimensional cases on formula_25 and other continuous sample spaces.
Measure-theoretic probability theory.
The "raison d'être" of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.
An example of such distributions could be a mix of discrete and continuous distributions—for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a pdf of formula_26, where formula_27 is the Dirac delta function.
Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems using measure theory to define the probability space:
Given any set formula_8, (also called sample space) and a σ-algebra formula_29 on it, a measure formula_30 defined on formula_29 is called a probability measure if formula_32
If formula_29 is the Borel σ-algebra on the set of real numbers, then there is a unique probability measure on formula_29 for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf. This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.
The "probability" of a set formula_7 in the σ-algebra formula_29 is defined as
where the integration is with respect to the measure formula_38 induced by formula_39
Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside formula_25, as in the theory of stochastic processes. For example to study Brownian motion, probability is defined on a space of functions.
Classical probability distributions.
Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions therefore have gained "special importance" in probability theory. Some fundamental "discrete distributions" are the discrete uniform, Bernoulli, binomial, negative binomial, Poisson and geometric distributions. Important "continuous distributions" include the continuous uniform, normal, exponential, gamma and beta distributions.
Convergence of random variables.
In probability theory, there are several notions of convergence for random variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions.
As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true.
Law of large numbers.
Common intuition suggests that if a fair coin is tossed many times, then "roughly" half of the time it will turn up "heads", and the other half it will turn up "tails". Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number of "heads" to the number of "tails" will approach unity. Modern probability provides a formal version of this intuitive idea, known as the law of large numbers. This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence.
The law of large numbers (LLN) states that the sample average
of a sequence of independent and
identically distributed random variables formula_57 converges towards their common expectation formula_58, provided that the expectation of formula_59 is finite.
It is in the different forms of convergence of random variables that separates the "weak" and the "strong" law of large numbers
It follows from the LLN that if an event of probability "p" is observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towards "p".
For example, if formula_61 are independent Bernoulli random variables taking values 1 with probability "p" and 0 with probability 1-"p", then formula_62 for all "i", so that formula_63 converges to "p" almost surely.
Central limit theorem.
"The central limit theorem (CLT) is one of the great results of mathematics." (Chapter 18 in)
It explains the ubiquitous occurrence of the normal distribution in nature.
The theorem states that the average of many independent and identically distributed random variables with finite variance tends towards a normal distribution "irrespective" of the distribution followed by the original random variables. Formally, let formula_48 be independent random variables with mean formula_65 and variance formula_66 Then the sequence of random variables
converges in distribution to a standard normal random variable.
Notice that for some classes of random variables the classic central limit theorem works rather fast (see Berry–Esseen theorem), for example the distributions with finite first, second and third moment from the exponential family, on the other hand for some random variables of the heavy tail and fat tail variety, it works very slow or may not work at all: in such cases one may use the Generalized Central Limit Theorem (GCLT).

</doc>
<doc id="23543" url="http://en.wikipedia.org/wiki?curid=23543" title="Probability distribution">
Probability distribution

In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.
In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience:
A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector—a set of two or more random variables—taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
Introduction.
To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous random variables. In the discrete case, one can easily assign a probability to each possible value: for example, when throwing a fair die, each of the six values "1" to "6" has the probability 1/6. In contrast, when a random variable takes values from a continuum then, typically, probabilities can be nonzero only if they refer to intervals: in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%.
If the random variable is real-valued (or more generally, if a total order is defined for its possible values), the cumulative distribution function (CDF) gives the probability that the random variable is no larger than a given value; in the real-valued case, the CDF is the integral of the probability density function (pdf) provided that this function exists.
Terminology.
As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:
The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:
Finally,
Cumulative distribution function.
Because a probability distribution Pr on the real line is determined by the probability of a scalar random variable "X" being in a half-open interval (-∞, "x"], the probability distribution is completely characterized by its cumulative distribution function:
Discrete probability distribution.
A discrete probability distribution is a "probability distribution" characterized by a probability mass function. Thus, the distribution of a random variable "X" is discrete, and "X" is called a discrete random variable, if
as "u" runs through the set of all possible values of "X". Hence, a random variable can assume only a finite or countably infinite number of values—the random variable is a discrete variable. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. for example, if formula_3 for "n" = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.
Well-known discrete probability distributions used in statistical modeling include the Poisson distribution, the Bernoulli distribution, the binomial distribution, the geometric distribution, and the negative binomial distribution. Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.
Measure theoretic formulation.
A measurable function formula_4 between a probability space formula_5 and a measurable space formula_6 is
called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e., formula_7 for all formula_8.
The latter requirement induces a probability mass function formula_9 via formula_10. Since the pre-images of disjoint sets
are disjoint
This recovers the definition given above.
Cumulative density.
Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities—that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.
Delta-function representation.
Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions, which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.
Indicator-function representation.
For a discrete random variable "X", let "u"0, "u"1, ... be the values it can take with non-zero probability. Denote
These are disjoint sets, and by formula (1)
It follows that the probability that "X" takes any value except for "u"0, "u"1, ... is zero, and thus one can write "X" as
except on a set of probability zero, where formula_15 is the indicator function of "A". This may serve as an alternative definition of discrete random variables.
Continuous probability distribution.
A continuous probability distribution is a "probability distribution" that has a probability density function. Mathematicians also call such a distribution absolutely continuous, since its cumulative distribution function is absolutely continuous with respect to the Lebesgue measure "λ". If the distribution of "X" is continuous, then "X" is called a continuous random variable. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.
Intuitively, a continuous random variable is the one which can take a continuous range of values—as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable. While for a discrete distribution an event with probability zero is impossible (e.g., rolling 3 1⁄2 on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval (3 cm, 4 cm) is nonzero. This apparent paradox is resolved by the fact that the probability that "X" attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.
Formally, if "X" is a continuous random variable, then it has a probability density function "ƒ"("x"), and therefore its probability of falling into a given interval, say ["a", "b"] is given by the integral
In particular, the probability for "X" to take any single value "a" (that is "a" ≤ "X" ≤ "a") is zero, because an integral with coinciding upper and lower limits is always equal to zero.
The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions, which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution. Such singular distributions however are never encountered in practice.
Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.
By one convention, a probability distribution formula_17 is called "continuous" if its cumulative distribution function formula_18 is continuous and, therefore, the probability measure of singletons formula_19 for all formula_20.
Another convention reserves the term "continuous probability distribution" for absolutely continuous distributions. These distributions can be characterized by a probability density function: a non-negative Lebesgue integrable function formula_21 defined on the real numbers such that
Discrete distributions and some continuous distributions (like the Cantor distribution) do not admit such a density.
Kolmogorov definition.
In the measure-theoretic formalization of probability theory, a random variable is defined as a measurable function "X" from a probability space formula_23 to measurable space formula_24. A probability distribution of "X" is the pushforward measure "X"*P  of "X" , which is a probability measure on formula_24 satisfying "X"*P = P"X" −1.
Random number generation.
A frequent problem in statistical simulations (the Monte Carlo method) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers "X" that are uniformly distributed in the interval [0,1). These random variates "X" are then transformed via some algorithm to create a new random variate having the required probability distribution.
Applications.
The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically,from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.
As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.
Common probability distributions.
The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)
Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.

</doc>
<doc id="23545" url="http://en.wikipedia.org/wiki?curid=23545" title="Psychological statistics">
Psychological statistics

Psychological statistics is the application of formulas, theorems, numbers and laws to psychology. Some of the more common applications include:
Some of the more commonly used statistical tests in psychology are:
Parametric tests
Non-parametric tests

</doc>
<doc id="23547" url="http://en.wikipedia.org/wiki?curid=23547" title="Peter Cook">
Peter Cook

Peter Edward Cook (17 November 1937 – 9 January 1995) was an English actor, satirist, writer and comedian.
An extremely influential figure in modern British comedy, Cook is regarded as the leading light of the British satire boom of the 1960s. He was closely associated with the anti-establishment comedy that emerged in Britain and the United States in the late 1950s.
Early life.
Cook was born at his parents' house, "Shearbridge", in Middle Warberry Road, Torquay, Devon. He was the only son and eldest of the three children of Alexander Edward "Alec" Cook (1906–1984), a colonial civil servant, and his wife Ethel Catherine Margaret, "née" Mayo (1908–1994). He was educated at Radley College and Pembroke College, Cambridge, where he studied French and German. As a student, Cook initially intended to become a career diplomat like his father, but Britain "had run out of colonies", as he put it. Although politically largely apathetic, particularly in later life when he displayed a deep distrust of politicians of all hues, he did join the Cambridge University Liberal Club.
It was at Pembroke that Cook performed and wrote comedy sketches as a member of the Cambridge Footlights Club, of which he became president in 1960. His hero was fellow Footlights writer and Cambridge magazine writer David Nobbs.
Whilst still at university, Cook wrote for Kenneth Williams, providing several sketches for Williams' West End comedy revue "Pieces of Eight" and much of the follow-up, "One Over the Eight", before finding prominence in his own right in a four-man group satirical stage show, "Beyond the Fringe", with Jonathan Miller, Alan Bennett and Dudley Moore.
"Beyond the Fringe" became a great success in London after being first performed at the Edinburgh Festival and included Cook impersonating the prime minister, Harold Macmillan. This was one of the first occasions satirical political mimicry had been attempted in live theatre and it shocked audiences. During one performance, Macmillan was in the theatre and Cook departed from his script and attacked him verbally.
Career.
1960s.
In 1961, Cook opened The Establishment, a club at 18 Greek Street in Soho in central London, presenting fellow comedians in a nightclub setting, including American Lenny Bruce. Cook said it was a satirical venue modelled on "those wonderful Berlin cabarets ... which did so much to stop the rise of Hitler and prevent the outbreak of the Second World War"; as a members-only venue it was outside the censorship restrictions. Cook befriended and supported Australian comedian and actor Barry Humphries, who began his British solo career at the club. Humphries said in his autobiography, "My Life As Me", that he found Cook's lack of interest in art and literature off-putting. Cook's chiselled looks and languid manner led Humphries to observe that whereas most people take after their father or mother, Cook seemed more like an aunt. Dudley Moore's jazz trio played in the basement of the club during the early 1960s.
In 1962, the BBC commissioned a pilot for a television series of satirical sketches based on the Establishment Club, but it was not picked up straight away and Cook went to New York City for a year to perform in "Beyond The Fringe" on Broadway. When he returned, the pilot had been refashioned as "That Was the Week That Was" and had made a star of David Frost, something Cook resented. The 1960s satire boom was closing and Cook said "England was about to sink giggling into the sea". He complained that Frost's success was based on copying Cook's own stage persona and Cook dubbed him "the bubonic plagiarist", and said that his only regret in life, recalled Alan Bennett at Cook's memorial service, had been saving Frost from drowning. This incident occurred in the summer of 1963, when the rivalry between the two men was at its height. Cook had realised that Frost's potential drowning would have looked deliberate if he had not been rescued.
Around this time, Cook provided financial backing for the satirical magazine "Private Eye", supporting it through difficult periods, particularly in libel trials. Cook invested his own money and solicited investment from his friends. For a time, the magazine was produced from the premises of the Establishment Club. In 1963, Cook married Wendy Snowden; the couple had two daughters, Lucy and Daisy, but the marriage ended in 1970.
Cook expanded television comedy with Eleanor Bron, John Bird and John Fortune. His first regular television spot was on Granada Television's "Braden Beat" with Bernard Braden, where he featured his most enduring character: the static, dour and monotonal E. L. Wisty, whom Cook had conceived for Radley College's Marionette Society.
Cook's comedy partnership with Dudley Moore led to "Not Only... But Also". This was intended by the BBC for Moore's music, but Moore invited Cook to write sketches and appear with him. Using few props, they created dry and absurd television that lasted for three series between 1965 and 1970. Cook played characters such as Sir Arthur Streeb-Greebling and the two men created their Pete and Dud alter egos. Other sketches included "Superthunderstingcar", a parody of the Gerry Anderson marionette TV shows, and Cook's pastiche of 1960s trendy arts documentaries – satirised in a parodic TV segment on Greta Garbo.
When Cook learned a few years later that the videotapes of the series were to be wiped, a common practice at the time, he offered to buy the recordings from the BBC but was refused because of copyright issues. He suggested he could purchase new tapes so that the BBC would have no need to erase the originals, but this was also turned down. Of the original programmes, only eight of the twenty-two episodes still survive complete. A compilation of six half-hour programmes, "The Best of What's Left of Not Only...But Also" was shown on television and has been released on both VHS and DVD.
With "The Wrong Box" (1966) and "Bedazzled" (1967) Cook and Moore began to act in films together. Directed by Stanley Donen, the underlying story of "Bedazzled" is credited to Cook and Moore and its screenplay to Cook. A comic parody of Faust, it stars Cook as George Spigott (The Devil) who tempts Stanley Moon (Moore), a frustrated, short-order chef, with the promise of gaining his heart's desire – the unattainable beauty and waitress at his cafe, Margaret Spencer (Eleanor Bron) – in exchange for his soul, but repeatedly tricks him. The film features cameo appearances by Barry Humphries as Envy and Raquel Welch as Lust. Moore composed the soundtrack music and co-wrote (with Cook) the songs performed in the film. His jazz trio backed Cook on the theme, a parodic anti-love song, which Cook delivered in a monotonous deadpan voice and included his familiar put-down, "You fill me with inertia."
In 1968, Cook and Moore briefly switched to ATV for four one-hour programmes entitled "Goodbye Again", based on the Pete and Dud characters. They ignored suggestions from the director and cast. Sketches were drawn out to fill the running time. With no interest in the show, Cook's increasing alcoholism led him to become reliant on cue cards and he ended up garbling the script, forcing Moore to ad-lib. The show was not a popular success, owing in part to the publication of the ITV listings magazine, "TV Times", being suspended because of a strike. John Cleese was a cast member.
1970s.
In 1970, Cook took over a project initiated by David Frost for a satirical film about an opinion pollster who rises to become President of Great Britain. Under Cook's guidance, the character became modelled on Frost. The film, "The Rise and Rise of Michael Rimmer", was not a success, although the cast contained notable names.
Cook became a favourite of the chat show circuit but his own effort at hosting one for the BBC in 1971, "Where Do I Sit?", was said by the critics to have been a disappointment. He was replaced after only two episodes by Michael Parkinson, the start of Parkinson's career as a chat show host. Parkinson later asked Cook what his ambitions were, Cook replied jocularly "[...] in fact, my ambition is to shut you up altogether you see!"
Cook and Moore fashioned sketches from "Not Only...But Also" and "Goodbye Again" with new material into the stage revue called "Behind the Fridge". This show toured Australia in 1972 before transferring to New York City in 1973, re-titled as "Good Evening". Cook frequently appeared on and off stage the worse for drink. Nonetheless, the show proved very popular and it won Tony and Grammy Awards. When it finished, Moore stayed in the U.S. to pursue his film acting ambitions in Hollywood. Cook returned to Britain and in 1973 he married the actress and model Judy Huxtable.
Later, the more risqué humour of Pete and Dud went farther on long-playing records as "Derek and Clive". The first recording was initiated by Cook to alleviate boredom during the Broadway run of "Good Evening" and used material conceived years before for the two characters but considered too outrageous. One of these audio recordings was also filmed and tensions between the duo are seen to rise. Chris Blackwell circulated bootleg copies to friends in the music business. The popularity of the recording convinced Cook to release it commercially, although Moore was initially reluctant, fearing that his rising fame as a Hollywood star would be undermined. Two further "Derek and Clive" albums were released, the last accompanied by a film.
Cook and Moore hosted Saturday Night Live on 24 January 1976 during the SNL first season. They did a number of their classic stage routines, including "One Legged Tarzan" and "Frog and Peach" among others, in addition to participating in some skits with the show's "not ready for prime-time" ensemble.
In 1978, Cook appeared on the British music series "Revolver" as the manager of a ballroom where emerging punk and new wave acts played. For some groups, these were their first appearances on television. Cook's acerbic commentary was a distinctive aspect of the programme.
In 1979, Cook recorded comedy-segments as B-sides to the Sparks 12-inch singles "Number One in Heaven" and "Tryouts for the Human Race". The main songwriter Ron Mael often started off a banal situation in his lyrics, and then went at surreal tangents in the style of Cook and S. J. Perelman.
"Consequences" album.
Cook played multiple roles on the 1977 concept album "Consequences", written and produced by former 10cc members Kevin Godley and Lol Creme. A mixture of spoken comedy and progressive rock with an environmental subtext, "Consequences" started as a single that Godley and Creme planned to make to demonstrate their invention, an electric guitar effect called the Gizmo, which they developed in 10cc. The project grew into a triple LP boxed set. The comedy sections were originally intended to be performed by a cast including Spike Milligan and Peter Ustinov, but Godley and Creme eventually settled on Cook once they realised he could perform most parts himself.
The storyline centres on the impending divorce of ineffectual Englishman Walter Stapleton (Cook) and his French wife Lulu (Judy Huxtable). While meeting their lawyers – the bibulous Mr Haig and overbearing Mr Pepperman (both played by Cook) – the encroaching global catastrophe interrupts proceedings with bizarre and mysterious happenings, which seem to centre on Mr Blint (Cook), a musician and composer living in the flat below Haig's office, to which it is connected by a large hole in the floor.
Although it has since developed a cult following due to Cook's presence, "Consequences" was released as punk was sweeping the UK and proved a resounding commercial failure, savaged by critics who found the music self-indulgent. The script and story have evident connections to Cook's own life – his then wife Judy Huxtable, plays Walter's wife. Cook's struggles with alcohol are mirrored in Haig's drinking, and there is a parallel between the fictional divorce of Walter and Lulu and Cook's own divorce from his first wife. The voice and accent Cook used for the character of Stapleton are similar to Cook's Beyond the Fringe colleague, Alan Bennett, and a book on Cook's comedy, "How Very Interesting", speculates that the characters Cook plays in "Consequences" are caricatures of the four "Beyond The Fringe" cast members – the alcoholic Haig represents Cook, the tremulous Stapleton is Bennett, the parodically Jewish Pepperman is Miller, and the pianist Blint represents Moore.
Amnesty International performances.
Cook appeared at the first three fund-raising galas staged by humourists John Cleese and Martin Lewis on behalf of Amnesty International. The benefits were dubbed "The Secret Policeman's Balls" though it wasn't until the third show in 1979 that the title was used. He performed on all three nights of the first show in April 1976, "A Poke in the Eye (With a Sharp Stick)", as an individual performer and as a member of the cast of "Beyond The Fringe", which reunited for the first time since the 1960s. He also appeared in a Monty Python sketch, taking the place of Eric Idle. Cook was on the cast album of the show and in the film, "Pleasure at Her Majesty's". He was in the second Amnesty gala in May 1977, "An Evening Without Sir Bernard Miles". It was retitled "The Mermaid Frolics" for the cast album and TV special. Cook performed monologues and skits with Terry Jones.
In June 1979, Cook performed all four nights of "The Secret Policeman's Ball" – teaming with John Cleese. Cook performed a couple of solo pieces and a sketch with Eleanor Bron. He also led the ensemble in the finale – the "End of the World" sketch from "Beyond The Fringe".
In response to a barb in "The Daily Telegraph" that the show was recycled material, Cook wrote a satire of the summing-up by Mr Justice Cantley in the trial of former Liberal Party leader Jeremy Thorpe, a summary thought by some to show bias in favour of Thorpe. Cook performed it that same night (Friday 29 June – the third of the four nights) and the following night. The nine-minute opus, "Entirely a Matter for You", is considered by many fans and critics to be one of the finest works of Cook's career. Cook and show producer Martin Lewis brought out an album on Virgin Records entitled "Here Comes the Judge: Live" of the live performance together with three studio tracks that further lampooned the Thorpe trial.
Although unable to take part in the 1981 gala, Cook supplied the narration over the animated opening title sequence of the 1982 film of the show. With Lewis, he wrote and voiced radio commercials to advertise the film in the UK. He also hosted a spoof film awards ceremony that was part of the world première of the film in London in March 1982.
Following Cook's 1987 stage reunion with Moore for the annual American benefit for the homeless, "Comic Relief" (not related to the UK "Comic Relief" benefits), Cook repeated the reunion for a British audience by performing with Moore at the 1989 Amnesty benefit "The Secret Policeman's Biggest Ball".
1980s.
In 1980, partly spurred by Moore's growing film star status, Cook moved to Hollywood and appeared as an uptight English butler to a wealthy American woman in a short-lived United States television sitcom, "The Two of Us", also making cameo appearances in a couple of undistinguished films. In 1980, Cook starred in the LWT special "Peter Cook & Co." The show included comedy sketches, including a "Tales of the Unexpected" parody "Tales of the Much As We Expected". This involved Cook as Roald Dahl, explaining his name had been Ronald before he dropped the "n". The cast included John Cleese, Rowan Atkinson, Beryl Reid, Paula Wilcox and Terry Jones.
In 1983 Cook played the role of Richard III in the first episode of "Blackadder", "The Foretelling", which parodies Laurence Olivier's portrayal. He narrated the short film "Diplomatix" by Norwegian comedy trio Kirkvaag, Lystad and Mjøen, which won the "Special Prize of the City of Montreux" at the Montreux Comedy Festival in 1985. In 1986 he partnered Joan Rivers on her UK talk show. He appeared as Mr Jolly in 1987 in The Comic Strip Presents' "Mr. Jolly Lives Next Door", playing an assassin who covers the sound of his murders by playing Tom Jones records. That same year, Cook made a big splash on American shores when he appeared in "The Princess Bride" as the "Impressive Clergyman" who officiates the wedding ceremony between Buttercup and Prince Humperdinck. Also that year he spent time working with Martin Lewis on a political satire about the 1988 U.S. presidential elections for HBO, but the script went unproduced. Lewis suggested Cook team with Moore for the U.S. Comic Relief telethon for the homeless. The duo reunited and performed their "One Leg Too Few" sketch.
In 1988, Cook appeared as a contestant on the improvisation comedy show, "Whose Line Is It Anyway?" Cook was declared the winner, his prize being to read the credits in the style of a New York cab driver – a character he'd portrayed in "Peter Cook & Co."
Cook occasionally called in to Clive Bull's night-time phone-in radio show on LBC in London. Using the name "Sven from Swiss Cottage", he mused on love, loneliness and herrings in a mock Norwegian accent. Jokes included Sven's attempts to find his estranged wife, in which he often claimed to be telephoning the show from all over the world, and his hatred of the Norwegian obsession with fish. While Bull was clearly aware that Sven was fictional he did not learn Sven's real identity until later.
Revival.
In late 1989, Cook married for the third time, to Malaysian-born property developer Chiew Lin Chong in Torbay, Devon. She provided him with some stability in his personal life and he reduced his drinking, to the extent that for a time he was teetotal. He lived alone in a small 18th century house in Perrins Lane, Hampstead, once owned by H. G. Wells, while his wife kept her own property only 100 yards away.
Cook returned to the BBC as Sir Arthur Streeb-Greebling for an appearance with Ludovic Kennedy in "A Life in Pieces". The 12 interviews saw Sir Arthur recount his life based on the Twelve Days of Christmas. Unscripted interviews with Cook as Streeb-Greebling and satirist Chris Morris were recorded in late 1993 and broadcast as "Why Bother?" on BBC Radio 3. Morris described them:It was a very different style of improvisation from what I'd been used to, working with people like Steve Coogan, Doon Mackichan and Rebecca Front, because those "On the Hour" and "The Day Today" things were about trying to establish a character within a situation, and Peter Cook was really doing 'knight's move' and 'double knight's move' thinking to construct jokes or ridiculous scenes flipping back on themselves, and it was amazing. I mean, I held out no great hopes that he wouldn't be a boozy old sack of lard with his hair falling out and scarcely able to get a sentence out, because he hadn't given much evidence that that wouldn't be the case. But, in fact, he stumbled in with a Safeways bag full of Kestrel lager and loads of fags and then proceeded to skip about mentally with the agility of a grasshopper. Really quite extraordinary.
On 17 December 1993, Cook appeared on "Clive Anderson Talks Back" as four characters – biscuit tester and alien abductee Norman House, football manager and motivational speaker Alan Latchley, judge Sir James Beauchamp and rock legend Eric Daley. The following day he appeared on BBC2 performing links for "Arena"'s "Radio Night". He also appeared, on 26 December, in the 1993 Christmas special of "One Foot in the Grave" ("One Foot in the Algarve"), playing a muckraking tabloid photographer. Before the end of the next year his mother died, and a grief-stricken Cook returned to heavy drinking. He made his last TV appearance on the show "Pebble Mill at One" in November 1994.
Death.
Cook died on 9 January 1995, aged 57, having suffered a gastrointestinal haemorrhage (a direct result of severe liver damage) in the intensive-care unit of the Royal Free Hospital in Hampstead, North London. Days earlier he had been taken in and announced, "I feel a bit poorly." He was cremated at Golders Green Crematorium and his ashes were buried in an unmarked plot behind St John's Church in Hampstead, not far from his house in Perrins Walk.
Dudley Moore attended Cook's memorial service in London in May 1995. He and Martin Lewis presented a two-night memorial for Cook in Los Angeles the following November to mark what would have been Cook's 58th birthday.
Legacy.
Cook is widely acknowledged as a strong influence on the many British comedians who followed him from the amateur dramatic clubs of British universities to the Edinburgh Festival Fringe, and then to radio and television. On his death some critics choose to see Cook's life as tragic, insofar as the brilliance of his youth had not been sustained in his later years. However, Cook himself always maintained he had no ambitions at all for sustained success. He assessed happiness by his friendships and his enjoyment of life. Eric Idle and Stephen Fry said Cook had not wasted his talent but rather that the newspapers had tried to waste him.
Several friends honoured him with a dedication in the closing credits of "Fierce Creatures" (1997), a comedy film written by John Cleese about a zoo in peril of being closed. It starred Cleese, Jamie Lee Curtis, Kevin Kline and Michael Palin. The dedication displays photos and the lifespan dates of Peter Cook and of British naturalist/humorist Gerald Durrell.
In 1999 the minor planet 20468 Petercook, in the main asteroid belt, was named after him.
Ten years after his death, Cook was ranked at number one in the "Comedians' Comedian", a poll of 300 comics, comedy writers, producers and directors throughout the English-speaking world. Channel 4 broadcast "Not Only But Always", a television film dramatising the relationship between Cook and Moore, with Rhys Ifans portraying Cook. At the 2005 Edinburgh Festival Fringe a play, written by Chris Bartlett and Nick Awde, examined the relationship from Moore's view, "", before transferring to London's West End at The Venue in 2006 and touring the UK the following year; Tom Goodman-Hill starred as Cook and Kevin Bishop as Moore in the West End.
At the 2007 Edinburgh Festival Fringe, "Goodbye – the (after)life of Cook & Moore" by Jonathan Hansler and Clive Greenwood was presented at The Gilded Balloon. The play imagined the newly dead Moore meeting Cook in Limbo, also inhabited by other comic actors with whom they had worked, including Peter Sellers, Tony Hancock, Frankie Howerd and Kenneth Williams. In May 2009 the play was seen again in London's West End at the Leicester Square Theatre (formerly "The Venue" and home to ) with Jonathan Hansler as Cook, Adam Bampton Smith as Moore and Clive Greenwood as everyone else.
A green plaque was unveiled by Westminster City Council and the Heritage Foundation at the site of the Establishment club on 15 February 2009.
An historic Blue Plaque was unveiled by Torbay Civic Society on 17 November 2014 at Cook's place of birth, "Shearbridge", Middle Warberry Road, Torquay, with his widow Lin and other members of the family in attendance. A further blue plaque has been commissioned to be erected at the home of his favourite football club, Torquay United, Plainmoor, Torquay, in 2015.
Filmography.
Amnesty
Discography.
UK chart singles:
Albums:

</doc>
<doc id="23549" url="http://en.wikipedia.org/wiki?curid=23549" title="Psychedelic rock">
Psychedelic rock

Psychedelic rock is a style of rock music that is inspired or influenced by psychedelic culture and attempts to replicate and enhance the mind-altering experiences of psychedelic drugs. It often uses new recording techniques and effects and sometimes draws on sources such as the ragas and drones of Indian music.
It was pioneered by musicians including the Beatles, the Byrds, and the Yardbirds, emerging as a genre during the mid-1960s among folk rock and blues rock bands in the United Kingdom and United States, such as Grateful Dead, Jefferson Airplane, the Jimi Hendrix Experience, Cream, the Doors and Pink Floyd. It reached a peak in between 1967 and 1969 with the Summer of Love and Woodstock Rock Festival, respectively, becoming an international musical movement and associated with a widespread counterculture, before beginning a decline as changing attitudes, the loss of some key individuals and a back-to-basics movement, led surviving performers to move into new musical areas.
Psychedelic rock influenced the creation of psychedelic pop and psychedelic soul. It also bridged the transition from early blues- and folk music-based rock to progressive rock, glam rock, hard rock and as a result influenced the development of subgenres such as heavy metal. Since the late 1970s it has been revived in various forms of neo-psychedelia.
Characteristics.
As a musical style, psychedelic rock attempted to replicate the effects of and enhance the mind-altering experiences of hallucinogenic drugs, incorporating new electronic and recording effects, extended solos, and improvisation, and it was particularly influenced by eastern mysticism, reflected in use of exotic instrumentation, particularly from Indian music or the incorporation of elements of eastern music. Major features include:
Etymology.
The term psychedelic was first coined in 1957 by psychiatrist Humphry Osmond as an alternative descriptor for hallucinogenic drugs in the context of psychedelic psychotherapy. The first musical use of the term psychedelic is thought to have been by the New York-based folk group The Holy Modal Rounders on their version of Lead Belly's "Hesitation Blues" in 1964. The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas, at the end of 1965. The term was first used in print in the "Austin American Statesman" in an article about the band titled "Unique Elevators shine with psychedelic rock", dated 10 February 1966, and theirs was the first album to use the term as part of its title, in "The Psychedelic Sounds of the 13th Floor Elevators", released in August that year.
As the countercultural scene developed in San Francisco the terms acid rock and psychedelic rock were used in 1966 to describe the new drug-influenced music and were being widely used by 1967. The terms psychedelic rock and acid rock are often used interchangeably, but some commentators have distinguished the former, which generally evoked the effects of psychedelic drugs, and acid rock, which can be seen as a more extreme subgenre that focused more directly on LSD, was often louder, made greater use of distortion and often consisted of long, improvised jams.
History.
Origins.
From the second half of the 1950s, Beat Generation writers like William Burroughs, Jack Kerouac and Allen Ginsberg wrote about and took drugs, including cannabis and Benzedrine, raising awareness and helping to popularise their use. In the same period Lysergic acid diethylamide, better known as LSD, or "acid" (at the time a legal drug), began to be used in the US and UK as an experimental treatment, initially promoted as a potential cure for mental illness.
In the early 1960s the use of LSD and other hallucinogens was advocated by proponents of the new "consciousness expansion", such as Timothy Leary, Alan Watts, Aldous Huxley and Arthur Koestler, their writings profoundly influenced the thinking of the new generation of youth. There had long been a culture of drug use among jazz and blues musicians, and, in the early 1960s, use of drugs (including cannabis, peyote, mescaline and LSD) had begun to grow among folk and rock musicians, who also began to include drug references in their songs.
Two of the most successful and influential acts of the era, Bob Dylan and the Beatles, were among the first to experiment with such references. Dylan's song "Subterranean Homesick Blues" (1965), which may have taken its title from a Kerouac novel, included the line, "Johnny's in the basement, mixing up the medicine", and his "Mr. Tambourine Man" (1965) requested "Take me on a trip upon your magic swirling ship". Whether this was intended as a drug reference was unclear, but the line would enter rock music when the song was a hit for the Byrds later in the year. Dylan indicated that he had smoked cannabis, but has denied using hard drugs. Nevertheless, his lyrics would continue to contain apparent drug references.
After being introduced to cannabis by Dylan, members of The Beatles began using LSD in 1965. The Beatles introduced audiences to many of the major elements of the psychedelic sound during this period, with guitar feedback in "I Feel Fine" (1964), "Norwegian Wood (This Bird Has Flown)" from their 1965 "Rubber Soul" album using a sitar, and the employment of reversed audio tapes on their 1966 B-side "Rain". Drug references began to appear in their songs, in "Day Tripper" (1965), and more explicitly in "Tomorrow Never Knows" (1966), from their album "Revolver".
The Beach Boys' "Pet Sounds" (1966) also contained many elements that would be incorporated into psychedelia, with its artful experiments, psychedelic lyrics based on emotional longings and self-doubts, elaborate sound effects and new sounds on both conventional and unconventional instruments.
By the mid-1960s, the psychedelic life-style had already developed in California. This was particularly true in San Francisco, due in part to the first major underground LSD factory, established there by Owsley Stanley. There was also an emerging music scene of folk clubs, coffee houses and independent radio stations catering to a population of students at nearby Berkeley, and to free thinkers that had gravitated to the city. From 1964, the Merry Pranksters, a loose group that developed around novelist Ken Kesey, sponsored the Acid Tests, a series of events based around the taking of LSD (supplied by Stanley), accompanied by light shows, film projection and discordant, improvised music known as the "psychedelic symphony". The Pranksters helped popularize LSD use through their road trips across America in a psychedelically-decorated school bus, which involved distributing the drug and meeting with major figures of the beat movement, and through publications about their activities such as Tom Wolfe's "The Electric Kool-Aid Acid Test" (1968).
The Byrds, emerging from the Californian folk scene, and the Yardbirds from the British blues scene, have been seen as particularly influential on the development of the genre. Drug use and attempts at psychedelic music moved out of acoustic folk-based music towards rock soon after The Byrds "plugged in" to produce a chart topping version of Dylan's "Mr. Tambourine Man" in the summer of 1965, which became a folk rock standard.
A number of Californian-based folk acts followed them into folk-rock, bringing their psychedelic influences with them, to produce the "San Francisco Sound". Particularly prominent products of the scene were The Grateful Dead (who had effectively become the house band of the Acid Tests), Country Joe and the Fish, The Great Society, Big Brother and the Holding Company, The Charlatans, Moby Grape, Quicksilver Messenger Service and Jefferson Airplane. In 1966, The Byrds moved rapidly away from folk rock with their single "Eight Miles High", which made use of free jazz and Indian ragas, and the lyrics of which were widely taken to refer to drug use. The result of this directness was limited airplay, and there was a similar reaction when Dylan, who had also electrified to produce his own brand of folk rock, released "Rainy Day Women ♯ 12 & 35", with its repeating chorus of "Everybody must get stoned!".
In Britain, the Yardbirds, with Jeff Beck as their guitarist, increasingly moved into psychedelic territory, adding up-tempo improvised "rave ups", Gregorian chant and world music (in particular Indian) influences to their songs, including "Still I'm Sad" (1965) and "Over Under Sideways Down" (1966), and singles such as "Heart Full of Soul" (1965), "Shapes of Things" (1966) and "Happenings Ten Years Time Ago" (1966). They were soon followed by bands such as Procol Harum, The Moody Blues and The Nice.
Development in the USA.
The San Francisco music scene continued to develop as The Fillmore, the Avalon Ballroom, and The Matrix began booking local rock bands on a nightly basis. The first Trips Festival, sponsored by the Merry Pranksters and held at the Longshoremen's Hall in January 1966, saw The Grateful Dead and Big Brother and the Holding Company play to an audience of 10,000, giving many their first encounter with both acid rock, with its long instrumentals and unstructured jams, and LSD. Also from San Francisco, Blue Cheer played psychedelic-influenced rock in a blues-rock style.
A major figure in the expansion of the genre was promoter Bill Graham, whose first rock concert in 1965 was a benefit that included Allen Ginsberg and the then unknown Jefferson Airplane on the bill. He produced shows attracting most of the major psychedelic rock bands and operated The Fillmore. When this proved too small he took over Winterland and then the Fillmore West (in San Francisco) and the Fillmore East (in New York City), where the major rock artists, from both the US and the UK, came to play.
Although San Francisco was the centre of American psychedelic music scene, many other American cities contributed significantly to the new genre. The first psychedelic single to reach the US top 10 was "Psychotic Reaction" by San Jose garage band Count Five in July 1966. Los Angeles boasted dozens of important psychedelic bands. Besides The Byrds, these included Iron Butterfly, Love, Spirit, Captain Beefheart and his Magic Band, The United States of America, The West Coast Pop Art Experimental Band, and the Electric Prunes; perhaps the most commercially successful were The Doors. Frank Zappa and his group The Mothers of Invention began to incorporate psychedelic influences in their first two albums "Freak Out!" (1966) and "Absolutely Free" (1967).
New York City produced its share of psychedelic bands, such as folk pioneers The Fugs, The Godz, and Pearls Before Swine, besides the Blues Magoos, the Blues Project, Lothar and the Hand People and the blues-influenced Vanilla Fudge. The Detroit area gave rise to psychedelic bands the Amboy Dukes, and the SRC, and Chicago produced H. P. Lovecraft. Texas (particularly Austin) is often cited for its contributions to psychedelic music: besides the 13th Floor Elevators it produced acts including Bubble Puppy, Lost and Found, The Golden Dawn, The Zakary Thaks, and Red Crayola.
Development in the UK.
In the UK before 1967 media outlets for psychedelic culture were limited to stations like Radio Luxembourg and pirate radio like Radio London, particularly the programmes hosted by DJ John Peel. The growth of underground culture was facilitated by the emergence of alternative weekly publications like IT (International Times) and OZ magazine which featured psychedelic and progressive music together with the counterculture lifestyle, which involved long hair, and the wearing of wild shirts from shops like Mr Fish, Granny Takes a Trip and old military uniforms from Carnaby Street (Soho) and Kings Road (Chelsea) boutiques.
Soon psychedelic rock clubs like the UFO Club in Tottenham Court Road, Middle Earth Club in Covent Garden, The Roundhouse in Chalk Farm, the Country Club (Swiss Cottage) and the Art Lab (also in Covent Garden) were drawing capacity audiences with psychedelic rock and ground-breaking liquid light shows. A major figure in the development of British psychedelia was the American promoter and record producer Joe Boyd, who moved to London in 1966. He co-founded venues including the UFO Club, produced Pink Floyd's first single, "Arnold Layne", and went on to manage folk and folk rock acts including Nick Drake, the Incredible String Band and Fairport Convention.
British psychedelic rock, like its American counterpart, had roots in the folk scene. Blues, drugs, jazz and eastern influences had featured since 1964 in the work of Davy Graham and Bert Jansch. However, the largest strand was a series of bands that emerged from 1966 from the British blues scene, but influenced by folk, jazz and psychedelia, including Pink Floyd, Traffic, Soft Machine, Cream, and The Jimi Hendrix Experience (led by an American, but initially produced and managed in Britain by Chas Chandler of The Animals). The Rolling Stones had drug references and psychedelic hints in their 1966 singles "19th Nervous Breakdown" and "Paint It, Black", the latter featuring drones and sitar. The Small Faces managed to get drug references past the censors with their first single "Here Come the Nice" (1967) and introduced phasing on "Itchycoo Park" (1967).
The Crazy World of Arthur Brown added surreal theatrical touches to its dark psychedelic sounds, such as the singer's flaming headdress. Existing "British Invasion" acts now joined the psychedelic revolution, including Eric Burdon (previously of The Animals) and The Who, whose "The Who Sell Out" (1967) included psychedelic influenced tracks "I Can See for Miles" and "Armenia City in the Sky".
Peak years.
Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release the double A-side "Strawberry Fields Forever" and "Penny Lane", opening a strain of British "pastoral" or "nostalgic" psychedelia, followed by the release of what is often seen as their definitive psychedelic statement in "Sgt. Pepper's Lonely Hearts Club Band", including the controversial track "Lucy in the Sky with Diamonds". They continued the psychedelic theme later in the year with the double EP "Magical Mystery Tour" and the number one single "Hello, Goodbye" with its B-side "I Am the Walrus". Also enigmatic and surreal was one of the most influential records of 1967, "A Whiter Shade of Pale" by Procol Harum, which reached number one in the UK Singles Chart on 8 June 1967, and stayed there for six weeks.
The Rolling Stones responded to "Sgt Pepper" later in the year with "Their Satanic Majesties Request", and Pink Floyd produced what is usually seen as their best psychedelic work "The Piper at the Gates of Dawn". In 1967 the Incredible String Band's "The 5000 Spirits or the Layers of the Onion" developed their folk music into full blown psychedelia, which would be a major influence on psychedelic rock. From 1967 Fairport Convention became a mainstay of the London Underground scene, producing their eponymous first album of American-inspired folk rock the following year. The Pretty Things' rock opera "S.F. Sorrow", released in December 1968, featured both heavy psychedelic songs such as "Old Man Going" and "I See You" and poppy numbers like "S.F.Sorrow Is Born" and "Baron Saturday". The Small Faces' "Ogdens' Nut Gone Flake" (1968), released soon after, also pioneered the concept album, with the tracks on LP telling a single story.
In America the Summer of Love of 1967 saw a huge number of young people from across America and the world travel to the Haight-Ashbury district of San Francisco, boosting the population from 15,000 to around 100,000. It was prefaced by the Human Be-In event in March and reached its peak at the Monterey Pop Festival in June, the latter helping to make major American stars of Janis Joplin, lead singer of Big Brother and the Holding Company, Jimi Hendrix and The Who. Key recordings included Jefferson Airplane's "Surrealistic Pillow", the first album to come out of San Francisco during this era, which sold well enough to bring the city's music scene to the attention of the record industry: from it they took two of the earliest psychedelic hit singles: "White Rabbit" (1967) and "Somebody to Love" (1967). The Doors' first hit single "Light My Fire" (1967), running for over seven minutes, became one of the defining records of the genre, although their follow up album "Strange Days" only enjoyed moderate success. Santana, led by guitarist Carlos Santana, used Latin rhythms as the basis for their psychedelic music.
These trends climaxed in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts, including Jimi Hendrix, Jefferson Airplane, and the Grateful Dead. Psychedelic rock was glamorized on screen in "Easy Rider" (1969), which used songs including Steppenwolf's "Born to be Wild" as part of its soundtrack.
International expansion.
The US and UK were the major centres of psychedelic music, but in the late 1960s scenes began to develop across the world, including continental Europe, Australasia, Asia and south and Central America.
In the later 1960s psychedelic scenes developed in a large number of countries in continental Europe, including the Netherlands with bands like The Outsiders, Denmark where it was pioneered by Steppeulvene, and Germany, where musicians began to fuse music of psychedelia and the electronic avant-garde. 1968 saw the first major German rock festival in , and the foundation of the Zodiak Free Arts Lab in Berlin by Hans-Joachim Roedelius, and Conrad Schnitzler, which helped bands like Tangerine Dream and Amon Düül achieve cult status.
The fledgling Australian and New Zealand rock scenes that formed in wake of Beatlemania were most influenced by British psychedelia, often with bands of first generation immigrants, who returned to further their musical careers. Among the most successful were The Easybeats, formed in Sydney but who recorded their international hit "Friday on My Mind" (1966) in London and remained there for their forays into psychedelic-tinged pop until they disbanded in 1970. A similar path was pursued by the Bee Gees, formed in Brisbane, but whose first album "Bee Gees' 1st" (1967), recorded in London, gave them three major hit singles and contained folk, rock and psychedelic elements, heavily influenced by the Beatles. The Twilights, formed in Adelaide, also made the trip to London, recording a series of minor hits, absorbing the psychedelic scene, to return home to produce covers of Beatles' songs, complete with sitar, and the concept album "Once upon a Twilight" (1968). The most successful New Zealand band, The La De Das, produced the psychedelic pop concept album "The Happy Prince" (1968), based on the Oscar Wilde children's classic, but failed to break through in Britain and the wider world.
A thriving psychedelic music scene in Cambodia, influenced by psychedelic rock and soul broadcast by US forces radio in Vietnam, was pioneered by artists such as Sinn Sisamouth and Ros Sereysothea. In South Korea, Shin Jung-Hyeon, often considered the godfather of Korean rock, played psychedelic-influenced music for the American soldiers stationed in the country. Following Shin Jung-Hyeon, the band San Ul Lim (Mountain Echo) often combined psychedelic rock with a more folk sound. In Turkey, Anatolian rock artist Erkin Koray blended classic Turkish music and Middle Eastern themes into his psychedelic-driven rock, helping to found the Turkish rock scene with artists such as Cem Karaca, Mogollar and Baris Manco.
Latin America proved a particularly fertile ground for psychedelic rock. The Brazilian psychedelic rock group Os Mutantes formed in 1966, although little known outside Brazil at the time, have since accrued a substantial international cult following. In the late 1960s, a wave of Mexican rock, heavily influenced by psychedelia and funk emerged, especially in northern border Mexican states, in particular, Tijuana, Baja California. Among the most recognized bands from this "Chicano Wave" (Onda Chicana in Spanish) were Three Souls in my Mind, Love Army, El Ritual and Los Dug Dug's. In Chile from 1967 to 1973, between the ending of the government of President Frei Montalva and the government of President Allende, a cultural movement was born from a few Chilean bands that emerged playing a unique fusion of folkloric music with heavy psychedelic influences. The 1967 release of Los Mac's album "Kaleidoscope Men" (1967) inspired bands such as Los Jaivas and Los Blops, the latter going on to collaborate with the iconic Chilean singer-songwriter Victor Jara on his 1971 album "El derecho de vivir en paz". Meanwhile in the Argentinian capital Buenos Aires, a burgeoning psychedelic scene gave birth to three of the most important bands in Argentine rock: Los Gatos, Manal and Almendra.
Decline.
By the end of the 1960s, psychedelic rock was in retreat. LSD had been made illegal in the US and UK in 1966. In 1969, the murders of Sharon Tate and Leno and Rosemary LaBianca by Charles Manson and his "family" of followers, claiming to have been inspired by Beatles' songs such as "Helter Skelter", has been seen as contributing to an anti-hippie backlash. At the end of the same year, the Altamont Free Concert in California, headlined by the Rolling Stones, became notorious for the fatal stabbing of black teenager Meredith Hunter by Hells Angel security guards. Brian Wilson of the Beach Boys (whose much anticipated "Smile" project would not emerge until 2004), Brian Jones of the Rolling Stones, Peter Green of Fleetwood Mac and Syd Barrett of Pink Floyd were early "acid casualties", helping to shift the focus of the respective bands of which they had been leading figures. Some groups, such as the Jimi Hendrix Experience and Cream, broke up. Jimi Hendrix died in London in September 1970, shortly after recording "Band of Gypsys" (1970), Janis Joplin died of a heroin overdose in October 1970 and they were closely followed by Jim Morrison of the Doors, who died in Paris in July 1971. Many surviving acts moved away from psychedelia into either more back-to-basics "roots rock", traditional-based, pastoral or whimsical folk, the wider experimentation of progressive rock, or riff-based heavy rock.
In 1966, even while psychedelic rock was becoming dominant, Bob Dylan spearheaded the back-to-basics roots revival when he went to Nashville to record the album "Blonde on Blonde". This, and the subsequent more clearly country-influenced albums, "John Wesley Harding" (1967) and "Nashville Skyline" (1969), have been seen as creating the genre of country folk. Dylan's lead was also followed by The Byrds, joined by Gram Parsons to record "Sweetheart of the Rodeo" (1968), helping to define the genre of country rock, which became a particularly popular style in the California music scene of the late 1960s, and was adopted by former folk rock artists including Hearts and Flowers, Poco and New Riders of the Purple Sage. Other acts that followed the back to basics trend in different ways were the Canadian group The Band and the Californian-based Creedence Clearwater Revival. The Grateful Dead also had major successes with the more reflective and stripped back "Workingman's Dead" and "American Beauty" in 1970. The super-group Crosby, Stills and Nash, formed in 1968 from members of The Byrds, Buffalo Springfield, and The Hollies, were joined by Neil Young for "Deja Vu" in 1970, which moved away from many of what had become the "clichés" of psychedelic rock and placed an emphasis on political commentary and vocal harmonies.
After the death of their manager Brian Epstein and the unpopular surreal television film, "Magical Mystery Tour", the Beatles returned to a raw style with "The Beatles" (1968), "Abbey Road" (1969) and "Let It Be" (1970), before their eventual break up. The back to basics trend was also evident in The Rolling Stones' albums starting from "Beggar's Banquet" (1968) to "Exile on Main St." (1972). Fairport Convention released "Liege and Lief" in 1969, turning away from American-influenced folk rock toward a sound based on traditional British music and founding the subgenre of electric folk, to be followed by bands like Steeleye Span and Fotheringay. The psychedelic-influenced and whimsical strand of British folk continued into the 1970s with acts including Comus, Mellow Candle, Nick Drake, The Incredible String Band, Forest and Trees and with Syd Barrett's two solo albums.
Influence.
Other genres.
As psychedelia emerged as a mainstream and commercial force, particularly through the work of the Beatles, it began to influence pop music, which incorporated hippie fashions, as well as the sounds of sitars, fuzz guitars, and tape effects. The Beach Boys' hit single "Good Vibrations" was one of the first pop songs to incorporate psychedelic lyrics and sounds. Scottish folk singer Donovan's transformation to 'electric' music gave him a series of pop hits, beginning with "Sunshine Superman", which reached number one in both Britain and the US, to be followed by "Mellow Yellow" (1966) and "Atlantis" (1968). American pop-oriented bands that followed in this vein included the Electric Prunes, the Blues Magoos and the Strawberry Alarm Clock. International acts such as the Bee Gees and the Easybeats were also prominent in the development of psychedelic pop. Psychedelic sounds were also incorporated into the output of early bubblegum pop acts like The Monkees and The Lemon Pipers.
Following the lead of Hendrix in rock, psychedelia began to have an impact on African American musicians, particularly the stars of the Motown label. This psychedelic soul was influenced by the civil rights movement, giving it a darker and more political edge than much acid rock. Building on the funk sound of James Brown, it was pioneered from about 1968 by Sly and the Family Stone and The Temptations. Acts that followed them into this territory included the Supremes, The Chambers Brothers, The 5th Dimension, Edwin Starr and the Undisputed Truth. George Clinton's interdependent Funkadelic and Parliament ensembles and their various spin-offs took the genre to its most extreme lengths making funk almost a religion in the 1970s, producing over forty singles, including three in the US top ten, and three platinum albums. While psychedelic rock began to waver at the end of the 1960s, psychedelic soul continued into the 1970s, peaking in popularity in the early years of the decade, and only disappearing in the late 1970s as tastes began to change. Acts like Earth, Wind and Fire, Kool and the Gang and Ohio Players, who began as psychedelic soul artists, incorporated its sounds into funk music and eventually the disco which partly replaced it.
Rock music.
Many of the British musicians and bands that had embraced psychedelia went on to create progressive rock in the 1970s, including Pink Floyd, Soft Machine and members of Yes. King Crimson's album "In the Court of the Crimson King" (1969) has been seen as an important link between psychedelia and progressive rock. While bands such as Hawkwind maintained an explicitly psychedelic course into the 1970s, most dropped the psychedelic elements in favour of wider experimentation. The incorporation of jazz into the music of bands like Soft Machine and Can also contributed to the development of the jazz rock of bands like Colosseum. As they moved away from their psychedelic roots and placed increasing emphasis on electronic experimentation, German bands like Kraftwerk, Tangerine Dream, Can and Faust developed a distinctive brand of electronic rock, known as kosmische musik, or in the British press as "Kraut rock". The adoption of electronic synthesisers, pioneered by Popol Vuh from 1970, together with the work of figures like Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent synth rock. In Japan, Osamu Kitajima's 1974 psychedelic rock album "Benzaiten" utilized electronic equipment such as a synthesizer and drum machine, and one of the record's contributors was Haruomi Hosono, who later started the electronic music band Yellow Magic Orchestra (as "Yellow Magic Band") in 1977.
Psychedelic rock, with its distorted guitar sound, extended solos and adventurous compositions, has been seen as an important bridge between blues-oriented rock and later heavy metal. American bands whose loud, repetitive psychedelic rock emerged as early heavy metal included the Amboy Dukes and Steppenwolf. From England, two former guitarists with the Yardbirds, Jeff Beck and Jimmy Page, moved on to form key acts in the genre, The Jeff Beck Group and Led Zeppelin respectively. Other major pioneers of the genre had begun as blues-based psychedelic bands, including Black Sabbath, Deep Purple, Judas Priest and UFO. Psychedelic music also contributed to the origins of glam rock, with Marc Bolan changing his psychedelic folk duo into rock band T. Rex and becoming the first glam rock star from 1970. From 1971 David Bowie moved on from his early psychedelic work to develop his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act.
Neo-psychedelia.
1970s and 1980s.
There were occasional mainstream acts that dabbled in neo-psychedelia, including Prince's mid-1980s work and some of Lenny Kravitz's 1990s output, but it has mainly been an influence on alternative and indie-rock bands. Psychedelic rock began to be revived in the late 1970s/early 1980s by bands of the post-punk scene, including the work of The Teardrop Explodes, Echo and the Bunnymen, The Church, the Soft Boys, Siouxsie and the Banshees, The Cure, The Glove, and The Legendary Pink Dots. In the US in the early 1980s these bands were joined by the Paisley Underground movement, based in Los Angeles, with acts like Dream Syndicate, The Bangles and Rain Parade. New wave band XTC published records under the pseudonym The Dukes of Stratosphear from 1985. Even Gothic rock band The Damned incorporated psychedelic music into their sound. The late 1980s saw the birth of shoegazing in the UK, which, among other influences, took inspiration from 1960s psychedelia. Critic Simon Reynolds described this movement as "a rash of blurry, neo-psychedelic bands". With loud walls of sound, where individual instruments and even vocals were often indistinguishable, they followed the lead of noise pop and dream pop bands such as My Bloody Valentine (often considered as the earliest shoegaze act), The Jesus and Mary Chain, and the Cocteau Twins. Major acts included Ride, Lush, Chapterhouse, and The Boo Radleys, who enjoyed considerable attention in the UK, but largely failed to break through in the US.
1990s to the present.
In the 1990s the Elephant 6 collective, including acts like The Apples in Stereo, The Olivia Tremor Control, Neutral Milk Hotel, Elf Power and of Montreal, produced eclectic psychedelic rock and folk. Other alternative acts to pursue psychedelia from the 1990s included The Brian Jonestown Massacre, Porno For Pyros and Super Furry Animals. Stoner rock also emerged, combining elements of psychedelic rock, blues-rock and doom metal. Typically using a slow-to-mid tempo and featuring low-tuned guitars in a bass-heavy sound, with melodic vocals, and 'retro' production, it was pioneered by the Californian bands Kyuss and Sleep. In the UK the Madchester scene influenced the early sound of 1990s Britpop bands like Blur. The Verve mixed on 1960s psychedelia with the shoegazing aesthetic. Oasis also drew on 1960s psychedelic pop and rock, particularly on the album "Standing on the Shoulder of Giants" (2000). In the immediate post-Britpop era Kula Shaker incorporated swirling, guitar-heavy sounds of late-'60s psychedelia with Indian mysticism and spirituality. In the new millennium neo-psychedelia was continued by bands directly emulating the sounds of the 60s such as The Black Angels, Tame Impala, Pond, and The Essex Green, while bands like Animal Collective applied an experimental approach that combined genres from the 1960s and the present. Modern festivals focusing on psychedelic music include Austin Psych Fest in Texas, founded in 2008 and Liverpool Psych Fest.

</doc>
<doc id="23550" url="http://en.wikipedia.org/wiki?curid=23550" title="Philips">
Philips

Koninklijke Philips N.V. (Royal Philips, commonly known as Philips) is a Dutch diversified technology company headquartered in Amsterdam with primary divisions focused in the areas of electronics, healthcare and lighting. It was founded in Eindhoven in 1891 by Gerard Philips and his father Frederik. It is one of the largest electronics companies in the world and employs around 122,000 people across more than 60 countries.
Philips is organized into three main divisions: Philips Consumer Lifestyle (formerly Philips Consumer Electronics and Philips Domestic Appliances and Personal Care), Philips Healthcare (formerly Philips Medical Systems) and Philips Lighting. As of 2012 Philips was the largest manufacturer of lighting in the world measured by applicable revenues. In 2013, the company announced the sale of the bulk of its remaining consumer electronics operations to Japan's Funai Electric Co, but in October 2013, the deal to Funai Electric Co was broken off and the consumer electronics operations remain under Philips. Philips said it would seek damages for breach of contract in the $200-million sale.
Philips has a primary listing on the Euronext Amsterdam stock exchange and is a constituent of the AEX index. It has a secondary listing on the New York Stock Exchange.
History.
The Philips Company was founded in 1891 by Gerard Philips and his father Frederik. Frederik, a banker based in Zaltbommel, financed the purchase and setup of a modest, empty factory building in Eindhoven, where the company started the production of carbon-filament lamps and other electro-technical products in 1892. This first factory has been adapted and is used as a museum.
In 1895, after a difficult first few years and near bankruptcy, the Philipses brought in Anton, Gerard's younger brother by sixteen years. Though he had earned a degree in engineering, Anton started work as a sales representative; soon, however, he began to contribute many important business ideas. With Anton's arrival, the family business began to expand rapidly, resulting in the founding of Philips Metaalgloeilampfabriek N.V. (Philips Metal Filament Lamp Factory Ltd.) in Eindhoven in 1907, followed in 1912 by the foundation of Philips Gloeilampenfabrieken N.V. (Philips Lightbulb Factories Ltd.). After Gerard and Anton Philips changed their family business by founding the Philips corporation, they laid the foundations for the later electronics multinational.
In the 1920s, the company started to manufacture other products, such as vacuum tubes. In 1939 they introduced their electric razor, the "Philishave" (marketed in the USA using the Norelco brand name). The "Chapel" is a radio with built-in loudspeaker, which was designed during the early 1930s.
Philips Radio.
On 11 March 1927 Philips went on the air with shortwave radio station PCJJ (later PCJ) which was joined in 1929 by sister station PHOHI (Philips Omroep Holland-Indië). PHOHI broadcast in Dutch to the Dutch East Indies (now Indonesia) while PCJJ broadcast in English, Spanish and German to the rest of the world.
The international program on Sundays commenced in 1928, with host Eddie Startz hosting the "Happy Station" show, which became the world's longest-running shortwave program. Broadcasts from the Netherlands were interrupted by the German invasion in May 1940. The Germans commandeered the transmitters in Huizen to use for pro-Nazi broadcasts, some originating from Germany, others concerts from Dutch broadcasters under German control.
Philips Radio was absorbed shortly after liberation when its two shortwave stations were nationalised in 1947 and renamed Radio Netherlands Worldwide, the Dutch International Service. Some PCJ programs, such as "Happy Station", continued on the new station.
Stirling engine.
Philips was instrumental in the revival of the Stirling engine when, in the early 1930s, the management decided that offering a low-power portable generator would assist in expanding sales of its radios into parts of the world where mains electricity was unavailable and the supply of batteries uncertain. Engineers at the company's research lab carried out a systematic comparison of various power sources and determined that the almost forgotten Stirling engine would be most suitable, citing its quiet operation (both audibly and in terms of radio interference) and ability to run on a variety of heat sources (common lamp oil – "cheap and available everywhere" – was favoured). They were also aware that, unlike steam and internal combustion engines, virtually no serious development work had been carried out on the Stirling engine for many years and asserted that modern materials and know-how should enable great improvements.
Encouraged by their first experimental engine, which produced 16 W of shaft power from a bore and stroke of 30 mm × 25 mm, various development models were produced in a programme which continued throughout World War II. By the late 1940s the 'Type 10' was ready to be handed over to Philips' subsidiary Johan de Witt in Dordrecht to be productionised and incorporated into a generator set as originally planned. The result, rated at 180/200 W electrical output from a bore and stroke of 55 mm x 27 mm, was designated MP1002CA (known as the "Bungalow set"). Production of an initial batch of 250 began in 1951, but it became clear that they could not be made at a competitive price, besides which the advent of transistor radios with their much lower power requirements meant that the original rationale for the set was disappearing. Approximately 150 of these sets were eventually produced.
In parallel with the generator set Philips developed experimental Stirling engines for a wide variety of applications and continued to work in the field until the late 1970s, though the only commercial success was the 'reversed Stirling engine' cryocooler. However, they filed a large number of patents and amassed a wealth of information, which they later licensed to other companies.
World War II.
On 9 May 1940, the Philips directors learned that the German invasion of the Netherlands was to take place the following day. Having prepared for this, Anton Philips and his son in law Frans Otten, as well as other Philips family members, fled to the United States, taking a large amount of the company capital with them. Operating from the U.S. as the North American Philips Company, they managed to run the company throughout the war. At the same time, the company was moved (on paper) to the Netherlands Antilles to keep it out of American hands.
Frits Philips, the son of Anton, was the only Philips family member to stay in the Netherlands. He saved the lives of 382 Jews by convincing the Nazis that they were indispensable for the production process at Philips. In 1943 he was held at the internment camp for political prisoners at Vught for several months because a strike at his factory reduced production. For his actions in saving the hundreds of Jews, he was recognized by Yad Vashem in 1995 as a "Righteous Among the Nations".
1945 to 2001.
After the war the company was moved back to the Netherlands, with their headquarters in Eindhoven. Many secret research facilities had been locked and successfully hidden from the invaders, which allowed the company to get up to speed again quickly after the war.
In 1949, the company began selling television sets. In 1950, it formed Philips Records.
Philips introduced the audio Compact Audio Cassette tape in 1963, and it was wildly successful. Compact cassettes were initially used for dictation machines for office typing stenographers and professional journalists. As their sound quality improved, cassettes would also be used to record sound and became the second mass media alongside vinyl records used to sell recorded music.
Philips introduced the first combination portable radio and cassette recorder, which was marketed as the "radiorecorder", and is now better known as the boom box. Later, the cassette was used in telephone answering machines, including a special form of cassette where the tape was wound on an endless loop. The C-cassette was used as the first mass storage device for early personal computers in the 1970s and 1980s. Philips reduced the cassette size for the professional needs with the Mini-Cassette, although it would not be as successful as the Olympus Microcassette. This became the predominant dictation medium up to the advent of fully digital dictation machines.
In 1972 Philips launched the world's first home video cassette recorder, in the UK, the N1500. Its relatively bulky video cassettes could record 30 minutes or 45 minutes. Later one-hour tapes were also offered. As competition came from Sony's Betamax and the VHS group of manufacturers, Philips introduced the N1700 system which allowed double-length recording. For the first time, a 2-hour movie could fit onto one video cassette. In 1977, the company unveiled a special promotional film for this system in the UK, featuring comedian Denis Norden. The concept was quickly copied by the Japanese makers, whose tapes were significantly cheaper. Philips made one last attempt at a new standard for video recorders with the Video 2000 system, with tapes that could be used on both sides and had 8 hours of total recording time. As Philips only sold its systems on the PAL standard and in Europe, and the Japanese makers sold globally, the scale advantages of the Japanese proved insurmountable and Philips withdrew the V2000 system and joined the VHS Coalition.
Philips had developed a LaserDisc early on for selling movies, but delayed its commercial launch for fear of cannibalizing its video recorder sales. Later Philips joined with MCA to launch the first commercial LaserDisc standard and players. In 1982, Philips teamed with Sony to launch the Compact Disc; this format evolved into the CD-R, CD-RW, DVD and later Blu-ray, which Philips launched with Sony in 1997 and 2006 respectively.
In 1984, Philips split off its activities on the field of photolithographic integrated circuit production equipment, the so-called wafer steppers, into a joint venture with ASM International, located in Veldhoven under the name ASML. Over the years, this new company has evolved into the world's leading manufacturer of chip production machines at the expense of competitors like Nikon and Canon.
In 1991, the company's name was changed from N.V. Philips Gloeilampenfabrieken to Philips Electronics N.V. At the same time, North American Philips was formally dissolved, and a new corporate division was formed in the U.S. with the name Philips Electronics North America Corp.
In 1997 the company officers decided to move the headquarters from Eindhoven to Amsterdam along with the corporate name change to Koninklijke Philips Electronics N.V. The move was completed in 2001. Initially, the company was housed in the Rembrandt Tower, but in 2002 they moved again, this time to the Breitner Tower. Philips Lighting, Philips Research, Philips Semiconductors (spun off as NXP in September 2006) and Philips Design, are still based in Eindhoven. Philips Healthcare is headquartered in both Best, Netherlands (near Eindhoven) and Andover, Massachusetts, United States (near Boston).
In 2000, Philips bought Optiva Corporation, the maker of Sonicare electric toothbrushes. The company was renamed Philips Oral Healthcare and made a subsidiary of Philips DAP.
2001 to 2011.
In 2004, Philips abandoned the slogan "Let's make things better" in favour of a new one: "Sense and simplicity".
In December 2005 Philips announced its intention to sell or demerge its semiconductor division. On 1 September 2006, it was announced in Berlin that the name of the new company formed by the division would be NXP Semiconductors. On 2 August 2006, Philips completed an agreement to sell a controlling 80.1% stake in NXP Semiconductors to a consortium of private equity investors consisting of Kohlberg Kravis Roberts & Co. (KKR), Silver Lake Partners and AlpInvest Partners. On 21 August 2006, Bain Capital and Apax Partners announced that they had signed definitive commitments to join the acquiring consortium, a process which was completed on 1 October 2006. In 2006 Philips bought out the company Lifeline Systems headquartered in Framingham, Massachusetts.
In August 2007 Philips acquired the company Ximis, Inc. headquartered in El Paso, Texas for their Medical Informatics Division. In October 2007, it purchased a Moore Microprocessor Patent (MPP) Portfolio license from The TPL Group.
On 21 December 2007 Philips and Respironics, Inc. announced a definitive agreement pursuant to which Philips acquired all of the outstanding shares of Respironics for US$66 per share, or a total purchase price of approximately €3.6 billion (US$5.1 billion) in cash.
On 21 February 2008 Philips completed the acquisition of VISICU Baltimore, Maryland through the merger of its indirect wholly owned subsidiary into VISICU. As a result of that merger, VISICU has become an indirect wholly owned subsidiary of Philips. VISICU was the creator of the eICU concept of the use of Telemedicine from a centralized facility to monitor and care for ICU patients.
The Philips physics laboratory was scaled down in the early 21st century, as the company ceased trying to be innovative in consumer electronics through fundamental research.
2011 to present.
In January 2011 Philips agreed to acquire the assets of Preethi, a leading India-based kitchen appliances company.
Because net profit slumped 85 percent in Q3 2011, Philips announced a cut of 4,500 jobs to match part of an €800 million ($1.1 billion) cost-cutting scheme to boost profits and meet its financial target.
In March 2012 Philips announced its intention to sell, or demerge its television manufacturing operations to TPV Technology.
In 2011, the company posted a loss of €1.3 billion, but earned a net profit in Q1 and Q2 2012, however the management wanted €1.1 billion cost-cutting which was an increase from €800 million and may cut another 2,200 jobs until end of 2014.
On 5 December 2012, the antitrust regulators of the European Union fined Philips and several other major companies for fixing prices of TV cathode-ray tubes in two cartels lasting nearly a decade.
On 29 January 2013, it was announced that Philips had agreed to sell its audio and video operations to the Japan-based Funai Electric for €150 million, with the audio business planned to transfer to Funai in the latter half of 2013, and the video business in 2017. As part of the transaction, Funai paid a regular licensing fee to Philips for the use of the Philips brand. The purchase agreement was terminated by Philips in October because of breach of contract.
In April 2013, Philips announced a collaboration with Paradox Engineering for the realization and implementation of a “pilot project” on network-connected street-lighting management solutions. This project was endorsed by the San Francisco Public Utilities Commission (SFPUC).
In 2013, Philips omitted the word "Electronics" from its name, which is now Royal Philips N.V.
On 13 November 2013 Philips unveiled its new brand line “Innovation and You” and a new design of its shield mark. The new brand positioning is cited by Philips to signify company’s evolution and emphasize that innovation is only meaningful if it is based on an understanding of people’s needs and desires.
On 28 April 2014 Philips agreed to sell their Woox Innovations subsidiary (consumer electronics) to Gibson Brands for $US135 million.
On 23 September 2014, Philips announced a plan to split the company into two, separating the lighting business from the healthcare and consumer lifestyle divisions.
In February 2015, Philips acquires Volcano Corporation to strengthen its position in non-invasive surgery and imaging.
Corporate affairs.
CEOs.
Past and present CEOs:
Acquisitions.
Companies acquired by Philips through the years include Amperex, Magnavox, Signetics, Mullard, VLSI, Agilent Healthcare Solutions Group, Marconi Medical Systems, ADAC Laboratories, ATL Ultrasound, portions of Westinghouse and the consumer electronics operations of Philco and Sylvania. Philips abandoned the Sylvania trademark which is now owned by Havells Sylvania except in Australia, Canada, Mexico, New Zealand, Puerto Rico and the USA where it is owned by Osram. Formed in November 1999 as an equal joint venture between Philips and Agilent Technologies, the light-emitting diode manufacturer Lumileds became a subsidiary of Phillips Lighting in August 2005 and a fully owned subsidiary in December 2006. On 20 January 2006, Philips Electronics NV said it would buy Lifeline Systems Inc in a deal valued at $750 million, its biggest move yet to expand its consumer-health business (M).
Operations.
Philips is registered in the Netherlands as a naamloze vennootschap and has its world headquarters in Amsterdam. At the end of 2013 Philips had 111 manufacturing facilities, 59 R&D Facilities across 26 countries and sales and service operations in around 100 countries.
Philips is organized into three main divisions: Philips Consumer Lifestyle (formerly Philips Consumer Electronics and Philips Domestic Appliances and Personal Care), Philips Healthcare (formerly Philips Medical Systems) and Philips Lighting. Philips achieved total revenues of €22.579 billion in 2011, of which €8.852 billion were generated by Philips Healthcare, €7.638 billion by Philips Lighting, €5.823 billion by Philips Consumer Lifestyle and €266 million from group activities. At the end of 2011 Philips had a total of 121,888 employees, of whom around 44% were employed in Philips Lighting, 31% in Philips Healthcare and 15% in Philips Consumer Lifestyle.
Philips invested a total of €1.61 billion in research and development in 2011, equivalent to 7.1% of sales. Philips Intellectual Property and Standards is the group-wide division responsible for licensing, trademark protection and patenting. Philips currently holds around 54,000 patent rights, 39,000 trademarks, 70,000 design rights and 4,400 domain name registrations.
Asia.
Pakistan.
Philips has been active in Pakistan since 1948 and has a wholly owned subsidiary, Philips Electrical industries of Pakistan Limited.
It has manufacturing in Karachi and two regional sales offices in Lahore and Rawalpindi.
The CEO of the Pakistani branch is Asad Jafar.
Hong Kong.
Philips Hong Kong began operation in 1948. Philips Hong Kong houses the global headquarters of Philips' Audio Business Unit. It also house Philips' Asia Pacific regional office and headquarters for its Design Division, Domestic Appliances & Personal Care Products Division, Lighting Products Division and Medical System Products Division.
China.
In early 2008 Philips Lighting, a division of Royal Philips Electronics, opened a small engineering center in Shanghai to adapt the company's products to vehicles in Asia.
India.
Philips began operations in India in 1930 with the establishment of Philips Electrical Co. (India) Pvt Ltd in Kolkata as a sales outlet for imported Philips lamps. In 1938, Philips established its first Indian lamp-manufacturing factory in Kolkata. In 1948, Philips started manufacturing radios in Kolkata. In 1959, a second radio factory was established near Pune. In 1957, the company converted into a public limited company, renamed "Philips India Ltd". In 1970 a new consumer electronics factory began operations in Pimpri near Pune; the factory was closed in 2006. In 1996, the Philips Software Centre was established in Bangalore, later renamed the Philips Innovation Campus. In 2008, Philips India entered the water purifier market. In 2014, Philip's was ranked 12th among India's most trusted brands according to the Brand Trust Report, a study conducted by Trust Research Advisory.
Israel.
Philips has been active in Israel since 1948 and in 1998 set-up a wholly owned subsidiary, Philips Electronics (Israel) Ltd. The company has 600 employees in Israel and generated sales of over $300 million in 2007.
Philips Medical Systems Technologies Ltd. (Haifa) is a developer and manufacturer of Computerized Tomography (CT), diagnostic and Medical Imaging systems. The company was founded in 1969 as Elscint by Elron Electronic Industries and was acquired by Marconi Medical Systems in 1998, which was itself acquired by Philips in 2001.
Philips Semiconductors formerly had major operations in Israel; these now form part of NXP Semiconductors.
Europe.
France.
Philips France has its headquarters in Suresnes. The company employs over 3600 people nationwide.
Philips Lighting has manufacturing facilities in Chalon-sur-Saône (fluorescent lamps), Chartres (automotive lighting), Lamotte-Beuvron (architectural lighting by LEDs and professional indoor lighting), Longvic (lamps), Miribel (outdoor lighting), Nevers (professional indoor lighting).
Greece.
Philips' Greece is headquartered in Marousi, Athens. As of 2012 Philips has no manufacturing plants in Greece, although there have been in the past.
Philips also has a Light Factory in Hong Kong, with 11 automatic production lines installed which is capable of producing 200 million pieces a year. The Philips Light Factory was established in 1974, now certified with ISO 9001:2000 & ISO 14001, its product portfolio ranges from Prefocus, Lensend to E10 miniature light bulbs.
Italy.
Philips founded its Italian headquarter in 1918, basing it in Monza (Milan) where it still operates, for commercial activities only.
Poland.
Philips' operations in Poland include: a European financial and accounting centre in Łódź; Philips Lighting facilities in Bielsko-Biała, Pabianice, Piła, and Kętrzyn; and a Philips Domestic Appliances facility in Białystok.
United Kingdom.
Philips UK has its headquarters in Guildford, Surrey. The company employs over 2500 people nationwide.
In the past, Philips UK also included
North America.
Canada.
Philips Canada was founded in 1934. It is well known in medical systems for diagnosis and therapy, lighting technologies, shavers, and consumer electronics.
The Canadian headquarters are located in Markham, Ontario.
For several years, Philips manufactured lighting products in two Canadian factories. The London, Ontario, plant opened in 1971. It produced A19 lamps (including the "Royale" long life bulbs), PAR38 lamps and T19 lamps (originally a Westinghouse lamp shape). Philips closed the factory in May 2003. The Trois-Rivières, Quebec plant was a Westinghouse facility which Philips continued to run it after buying Westinghouse's lamp division in 1983. Philips closed this factory a few years later, in the late 1980s.
Mexico.
Philips Mexicana SA de CV is headquartered in Mexico City. Philips Lighting has manufacturing facilities in:Monterrey, Nuevo León; Ciudad Juárez, Chihuahua; and Tijuana, Baja California. Philips Consumer Electronics has a manufacturing facility in Ciudad Juárez, Chihuahua. Philips Domestic Appliances formerly operated a large factory in the Industrial Vallejo sector of Mexico City but this was closed in 2004.
United States.
Philips' Electronics North American headquarters is based in Andover, Massachusetts. Philips Lighting has its corporate office in Somerset, New Jersey, with manufacturing plants in Danville, Kentucky, Bath, New York, Salina, Kansas and Paris, Texas and distribution centers in Mountain Top, Pennsylvania, Ontario, California and Memphis, Tennessee. Philips Healthcare is headquartered in Andover, Massachusetts. The North American sales organization is based in Bothell, Washington. There are also manufacturing facilities in Andover, Massachusetts, Bothell, Washington, Baltimore, Maryland, Cleveland, Ohio, Foster City, California, Melbourne, Florida, Milpitas, California and Reedsville, Pennsylvania. Philips Healthcare also formerly had a factory in Knoxville, Tennessee. Philips Consumer Lifestyle has its corporate office in Stamford, Connecticut. It has a manufacturing plant in Snoqualmie, Washington which makes Sonicare electric toothbrushes. Philips Lighting has a Color Kinetics office in Burlington, Massachusetts. Philips Research North American headquarters is in Briarcliff Manor, New York.
In 2007, Philips entered into a definitive merger agreement with North American luminaires company Genlyte Group Incorporated, which provides the company with a leading position in the North American luminaires (also known as ˜lighting fixtures"), controls and related products for a wide variety of applications, including solid state lighting. The company also acquired Respironics, which was a significant gain for its healthcare sector. On 21 February 2008 Philips completed the acquisition of VISICU Baltimore, Maryland. VISICU was the creator of the eICU concept of the use of Telemedicine from a centralized facility to monitor and care for ICU patients.
Oceania.
Australia and New Zealand.
Philips Australia was founded in 1927 and is headquartered in North Ryde, New South Wales and also manages the New Zealand operation from there. The company currently employs around 800 people. Regional sales and support offices are located in Melbourne, Brisbane, Adelaide, Perth and Auckland.
Current activities include: Philips Healthcare (also responsible for New Zealand operations); Philips Lighting (also responsible for New Zealand operations); Philips Consumer Lifestyle (also responsible for New Zealand operations); Philips Sleep & Respiratory Care (formerly Respironics), with its ever increasing national network of Sleepeasy Centres ; Philips Dynalite (Lighting Control systems, acquired in 2009, global design and manufacturing centre) and Philips Selecon NZ (Lighting Entertainment product design and manufacture).
South America.
Brazil.
Philips do Brasil (Brazilian Portuguese: "Philips do Brasil") was founded in 1924 in Rio de Janeiro. In 1929, Philips started to sell radio receivers. In the 1930s, Philips was making its light bulbs and radio receivers in Brazil. From 1939 to 1945, World War II forced Brazilian branch of Philips to sell bicycles, refrigerators and insecticides. After the war, Philips had a great industrial expansion in Brazil, and was among the first groups to establish in Manaus Free Zone. In the 1970s, Philips Records was a major player in Brazil recording industry. Nowadays, Philips do Brasil is one of the largest foreign-owned companies in Brazil. Philips uses the brand Walita for domestic appliances in Brazil.
Former operations.
Polymer Vision, the maker of The Readius, is a spin out from Philips Electronics. In May 2011, Polymer Vision designed and manufactured a 6-inch screen that displays black and white e-ink text and images at 800×600 pixels and can roll around a tube the circumference of a dime.
Philips also forayed into the pharmaceuticals market in a company best known as Philips-Duphar (Dutch Pharmaceuticals). Philips-Duphar made products for crop protection, veterinary medicine and products for human use. Duphar was sold to Solvay in 1990. In subsequent years Solvay sold off all divisions to other companies (crop protection to UniRoyal, now Chemtura, the veterinary division to Fort Dodge, a division of Wyeth, and the pharmaceutical division to Abbott Laboratories).
Philips' record division, PolyGram, was sold to Seagram in 1998 and was merged into Universal Music Group. Philips Records continues to operate as part of UMG, its name licenced from its former parent.
Origin, now part of Atos Origin, is a former division of Philips.
ASM Lithography is a spin-off from a division of Philips.
NXP Semiconductors, formerly known as Philips Semiconductors, was sold a consortium of private equity investors in 2006. On 6 August 2010, NXP completed its IPO, with shares trading on NASDAQ.
Philips used to sell major household appliances (whitegoods) under the name Philips. After selling the Major Domestic Appliances division to Whirlpool Corporation it changed via Philips Whirlpool and Whirlpool Philips to Whirlpool only. Whirlpool bought a 53% stake in Philips' major appliance operations to form Whirlpool International. Whirlpool bought Philips' remaining interest in Whirlpool International in 1991.
Philips Cryogenics was split off in 1990 to form the Stirling Cryogenics BV, Netherlands. This company is still active in the development and manufacturing of Stirling cryocoolers and cryogenic cooling systems.
North American Philips distributed AKG Acoustics products under the AKG of America, Philips Audio/Video, Norelco and AKG Acoustics Inc. branding until AKG set up its North American division in San Leandro, California in 1985. (AKG's North American division has since moved to Northridge, California.)
Products.
Philips' core products are consumer electronics and electrical products (including small domestic appliances, shavers, beauty appliances, mother and childcare appliances, electric toothbrushes and coffee makers [products like Smart Phones, audio equipment, Blu-ray players, computer accessories and televisions are sold under license]); healthcare products (including CT scanners, ECG equipment, mammography equipment, monitoring equipment, MRI scanners, radiography equipment, resuscitation equipment, ultrasound equipment and X-ray equipment); and lighting products (including indoor luminaires, outdoor luminaires, automotive lighting, lamps, lighting controls and lighting electronics).
Healthcare products.
Philips Healthcare products include:
Sponsorships.
In 1913, in celebration of the 100th anniversary of the independence of the Netherlands, Philips founded "Philips Sport Vereniging" (Philips Sports Club, now commonly known as PSV). The club is active in numerous sports, but is now best known for its football team, PSV Eindhoven, and swimming team. Philips owns the naming rights to Philips Stadion in Eindhoven, which is the home ground of PSV Eindhoven.
Outside of the Netherlands, Philips sponsors and has sponsored numerous sport clubs, sport facilities and events. In November 2008 Philips renewed and extended its F1 partnership with AT&T Williams. Philips owns the naming rights to the Philips Arena in Atlanta, Georgia and to the "Philips Championship", the premier basketball league in Australia, traditionally known as the National Basketball League. From 1988 to 1993 Philips was the principal sponsor of the Australian rugby league team The Balmain Tigers.
Outside of sports Philips sponsors the international "Philips Monsters of Rock festival".
Environmental record.
Green Initiatives.
Philips is running the EcoVision4 initiative in which it committed to a number of environmentally positive improvements by 2012.
Also Philips marks its "green" products with the Philips Green Logo, identifying them as products that have a significantly better environmental performance than their competitors or predecessors.
L-Prize competition.
In 2011, Philips won a $10 million cash prize from the US Department of Energy for winning its L-Prize competition, to produce a high-efficiency, long operating life replacement for a standard 60-W incandescent lightbulb. The winning LED lightbulb, which was made available to consumers in April 2012, produces slightly more than 900 lumens at an input power of only 10 W.
Greenpeace ranking.
In Greenpeace's 2012 Guide to Greener Electronics that ranks electronics manufacturers on sustainability, climate and energy and how green their products are, Philips ranks 10th place with a score of 3.8/10. The company was the top scorer in the Energy section due to its energy advocacy work calling upon the EU to adopt a 30% reduction for greenhouse gas emissions by 2020. It is also praised for its new products which are free from PVC plastic and BFRs. However, the guide criticizes Phillips' sourcing of fibres for paper, arguing it must develop a paper procurement policy which excludes suppliers involved in deforestation and illegal logging.
Philips have made some considerable progress since 2007 (when it was first ranked in this guide), in particular by supporting the Individual Producer Responsibility principle, which means that the company is accepting the responsibility for the toxic impacts of its products on e-waste dumps around the world.

</doc>
<doc id="23551" url="http://en.wikipedia.org/wiki?curid=23551" title="Perciformes">
Perciformes

Perciformes, also called the Percomorphi or Acanthopteri, are the largest order of vertebrates, containing about 41% of all bony fish. Perciformes means "perch-like". They belong to the class of ray-finned fish, and comprise over 10,000 species found in almost all aquatic ecosystems. 
The order contains about 160 families, which is the most of any order within the vertebrates. It is also the most variably sized order of vertebrates, ranging from the 7-mm (1/4-in) "Schindleria brevipinguis" to the marlins in the "Makaira" genus. They first appeared and diversified in the Late Cretaceous.
Among the well-known members of this group are cichlids, California sheephead, bluegill, damselfish, bass, and perch.
Characteristics.
The dorsal and anal fins are divided into anterior spiny and posterior soft-rayed portions, which may be partially or completely separated. The pelvic fins usually have one spine and up to five soft rays, positioned unusually far forward under the chin or under the belly. Scales are usually ctenoid, although sometimes they are cycloid or otherwise modified.
Taxonomy.
Classification is controversial. As traditionally defined, the Perciformes are almost certainly paraphyletic. Other orders that should possibly be included as suborders are the Scorpaeniformes, Tetraodontiformes, and Pleuronectiformes.
Of the presently recognized suborders, several may be paraphyletic, as well. These are grouped by suborder/superfamily, generally following the text "Fishes of the World".

</doc>
<doc id="23553" url="http://en.wikipedia.org/wiki?curid=23553" title="Asimina">
Asimina

Asimina is a genus of small trees or shrubs described as a genus in 1763.
"Asimina" has large simple leaves and large fruit. It is native to eastern North America and collectively referred to as Pawpaw. The genus includes the widespread common pawpaw "Asimina triloba," which bears the largest edible fruit indigenous to the continent. Pawpaws are native to 26 states of the U.S. and to Ontario in Canada. The common pawpaw is a patch-forming (clonal) understory tree found in well-drained, deep, fertile bottomland and hilly upland habitat. Pawpaws are in the same plant family (Annonaceae) as the custard-apple, cherimoya, sweetsop, ylang-ylang and soursop; the genus is the only member of that family not confined to the tropics.
Names.
The genus name "Asimina" was first described and named by Michel Adanson, a French naturalist of Scottish descent. The name is adapted from the Native American name "assimin" through the French colonial "asiminier."
The common name pawpaw, also spelled paw paw, paw-paw, and papaw, probably derives from the Spanish "papaya", perhaps because of the superficial similarity of their fruits.
Description.
Pawpaws are shrubs or small trees to 2–12 m (6–40 ft) tall. The northern, cold-tolerant common pawpaw ("Asimina triloba") is deciduous, while the southern species are often evergreen.
The leaves are alternate, obovate, entire, 20–35 cm (8–13 in) long and 10–15 cm (4–6 in) broad.
The fetid flowers of pawpaws are produced singly or in clusters of up to eight together; they are large, 4–6 cm across, perfect, with six sepals and petals (three large outer petals, three smaller inner petals). The petal color varies from white to purple or red-brown.
The fruit of the common pawpaw is a large edible berry, 5–16 cm (2–6 in) long and 3–7 cm (1.5–2.5 in) broad, weighing from 20–500 g (1–18 oz), with numerous seeds; it is green when unripe, maturing to yellow or brown. It has a flavor somewhat similar to both banana and mango, varying significantly by cultivar, and has more protein than most fruits.
Ecology.
The common pawpaw is native to shady, rich bottom lands, where it often forms a dense undergrowth in the forest, often appearing as a patch or thicket of individual small slender trees.
Pawpaw flowers are insect-pollinated, but fruit production is limited since few if any pollinators are attracted to the flower's faint, or sometimes non-existent scent. The flowers produce an odor similar to that of rotting meat to attract blowflies or carrion beetles for cross pollination. Other insects that are attracted to pawpaw plants include scavenging fruit flies, carrion flies and beetles. Because of difficult pollination, some believe the flowers are self-incompatible.
Pawpaw fruit may be eaten by foxes, opossums, squirrels and raccoons. However, pawpaw leaves and twigs are seldom consumed by rabbits or deer.
The leaves, twigs, and bark of the common pawpaw tree contain natural insecticides known as acetogenins.
Larvae of the Zebra Swallowtail Butterfly feed exclusively on young leaves of the various pawpaw species, but never occur in great numbers on the plants.
Cultivation and uses.
Wild-collected fruits of the common pawpaw "(Asimina triloba)" have long been a favorite treat throughout the tree's extensive native range in eastern North America. Fresh pawpaw fruits are commonly eaten raw; however, they do not store or ship well unless frozen. The fruit pulp is also often used locally in baked dessert recipes, with pawpaw often substituted in many banana-based recipes.
Pawpaws have never been cultivated for fruit on the scale of apples and peaches, but interest in pawpaw cultivation has increased in recent decades. However, only frozen fruit will store or ship well. Other methods of preservation include dehydration, production of jams or jellies, and pressure canning.
The pawpaw is also gaining in popularity among backyard gardeners because of the tree's distinctive growth habit, the appeal of its fresh fruit, and its relatively low maintenance needs once established. The common pawpaw is also of interest in ecological restoration plantings since this tree grows well in wet soil and has a strong tendency to form well-rooted colonial thickets.
The several other species of "Asimina" have few economic uses.
History.
The earliest documentation of pawpaws is in the 1541 report of the Spanish de Soto expedition, who found Native Americans cultivating it east of the Mississippi River. Chilled pawpaw fruit was a favorite dessert of George Washington, and Thomas Jefferson planted it at his home in Virginia, Monticello. The Lewis and Clark Expedition sometimes subsisted on pawpaws during their travels. The common pawpaw was designated as the Ohio state native fruit in 2009.

</doc>
<doc id="23555" url="http://en.wikipedia.org/wiki?curid=23555" title="Pentecostalism">
Pentecostalism

Pentecostalism or Classical Pentecostalism is a renewal movement within Protestant Christianity that places special emphasis on a direct personal experience of God through the baptism with the Holy Spirit. The term "Pentecostal" is derived from Pentecost, the Greek name for the Jewish Feast of Weeks. For Christians, this event commemorates the descent of the Holy Spirit upon the followers of Jesus Christ, as described in the second chapter of the Book of Acts.
Like other forms of evangelical Protestantism, Pentecostalism adheres to the inerrancy of scripture and the necessity of accepting Christ as personal Lord and Savior. It is distinguished by belief in the baptism with the Holy Spirit as an experience separate from conversion that enables a Christian to live a Holy Spirit–filled and empowered life. This empowerment includes the use of spiritual gifts such as speaking in tongues and divine healing—two other defining characteristics of Pentecostalism. Because of their commitment to biblical authority, spiritual gifts, and the miraculous, Pentecostals tend to see their movement as reflecting the same kind of spiritual power and teachings that were found in the Apostolic Age of the early church. For this reason, some Pentecostals also use the term "Apostolic" or "Full Gospel" to describe their movement.
Pentecostalism emerged in the early 20th century among radical adherents of the Holiness movement who were energized by revivalism and expectation for the imminent Second Coming of Christ. Believing that they were living in the end times, they expected God to spiritually renew the Christian Church thereby bringing to pass the restoration of spiritual gifts and the evangelization of the world. In 1900, Charles Parham, an American evangelist and faith healer, began teaching that speaking in tongues was the Bible evidence of Spirit baptism. The three-year-long Azusa Street Revival in Los Angeles, California, resulted in the spread of Pentecostalism throughout the United States and the rest of the world as visitors carried the Pentecostal experience back to their home churches or felt called to the mission field. While virtually all Pentecostal denominations trace their origins to Azusa Street, the movement has experienced a variety of divisions and controversies. An early dispute centered on challenges to the doctrine of the Trinity. As a result, the Pentecostal Movement is divided between trinitarian and non-trinitarian branches.
Comprising over 700 denominations and a large number of independent churches, there is no central authority governing Pentecostalism; however, many denominations are affiliated with the Pentecostal World Fellowship. There are over 279 million Pentecostals worldwide, and the movement is growing in many parts of the world, especially the global South. Since the 1960s, Pentecostalism has increasingly gained acceptance from other Christian traditions, and Pentecostal beliefs concerning Spirit baptism and spiritual gifts have been embraced by non-Pentecostal Christians in Protestant and Catholic churches through the Charismatic Movement. Together, Pentecostal and Charismatic Christianity numbers over 500 million adherents.
Beliefs.
Pentecostalism is an evangelical faith, emphasizing the reliability of the Bible and the need for the transformation of an individual's life through faith in Jesus. Like other evangelicals, Pentecostals generally adhere to the Bible's divine inspiration and inerrancy—the belief that the Bible, in the original manuscripts in which it was written, is infallible. Pentecostals emphasize the teaching of the "full gospel" or "foursquare gospel". The term "foursquare" refers to the four fundamental beliefs of Pentecostalism: Jesus saves according to ; baptizes with the Holy Spirit according to Acts 2:4; heals bodily according to James 5:15; and is coming again to receive those who are saved according to 1 Thessalonians 4:16–17.
Salvation.
The central belief of Pentecostalism is that through the death, burial, and resurrection of Jesus Christ, sins can be forgiven and humanity reconciled with God. This is the Gospel or "good news". The fundamental requirement of Pentecostalism is that one be born again. The new birth is received by the grace of God through faith in Christ as Lord and Savior. In being born again, the believer is regenerated, justified, adopted into the family of God, and the Holy Spirit's work of sanctification is initiated.
Pentecostal soteriology is generally Arminian rather than Calvinist. The security of the believer is a doctrine held within Pentecostalism; nevertheless, this security is conditional upon continual faith and repentance. Pentecostals believe in both a literal heaven and hell, the former for those who have accepted God's gift of salvation and the latter for those who have rejected it.
For most Pentecostals there is no other requirement to receive salvation. Baptism with the Holy Spirit and speaking in tongues are not generally required, though Pentecostal converts are usually encouraged to seek these experiences. A notable exception is Oneness Pentecostalism, most adherents of which believe both water baptism and Spirit baptism are integral components of salvation. For a more detailed explanation of Oneness Pentecostal beliefs, see below in the Statistics and denominations section of this article.
Baptism with the Holy Spirit.
To avoid confusion when studying Pentecostal beliefs, it should be noted that Pentecostals identify three distinct uses of the word "baptism" in the New Testament:
While the figure of Jesus Christ and his redemptive work are at the center of Pentecostal theology, that redemptive work is believed to provide for a fullness of the Holy Spirit of which believers in Christ may take advantage. The majority of Pentecostals believe that at the moment a person is born again, the new believer has the presence (indwelling) of the Holy Spirit. While the Spirit "dwells" in every Christian, Pentecostals believe that all Christians should seek to be "filled" with him. The Spirit's "filling", "falling upon", "coming upon", or being "poured out upon" believers is called the baptism with the Holy Spirit. Pentecostals define it as a definite experience occurring after salvation whereby the Holy Spirit comes upon the believer to anoint and empower him or her for special service. It has also been described as "a baptism into the love of God".
The main purpose of the experience is to grant power for Christian service. Other purposes include power for spiritual warfare (the Christian struggles against spiritual enemies and thus requires spiritual power), power for overflow (the believer's experience of the presence and power of God in his or her life flows out into the lives of others), and power for ability (to follow divine direction, to face persecution, to exercise spiritual gifts for the edification of the church, etc.).
Pentecostals believe that the baptism with the Holy Spirit is available to all Christians. Repentance from sin and being born again are fundamental requirements to receive it. There must also be in the believer a deep conviction of needing more of God in his or her life, and a measure of consecration by which the believer yields himself or herself to the will of God. Citing instances in the Book of Acts where believers were Spirit baptized before they were baptized with water, most Pentecostals believe a Christian need not have been baptized in water to receive Spirit baptism. However, Pentecostals do believe that the biblical pattern is "repentance, regeneration, water baptism, and then the baptism with the Holy Ghost". There are Pentecostal believers who have claimed to receive their baptism with the Holy Spirit while being water baptized.
It is received by having faith in God's promise to fill the believer and in yielding the entire being to Christ. Certain conditions, if present in a believer's life, could cause delay in receiving Spirit baptism, such as "weak faith, unholy living, imperfect consecration, and egocentric motives". In the absence of these, Pentecostals teach that seekers should maintain a persistent faith in the knowledge that God will fulfill his promise. For Pentecostals, there is no prescribed manner in which a believer will be filled with the Spirit. It could be expected or unexpected, during public or private prayer.
Pentecostals expect certain results following baptism with the Holy Spirit. Some of these are immediate while others are enduring or permanent. Some Pentecostal denominations teach that speaking in tongues is an immediate or initial physical evidence that one has received the experience. However, not all Pentecostals share this doctrinal position. It is most prominent among white Pentecostal denominations in the United States; elsewhere, beliefs are more varied. Some teach that any of the gifts of the Spirit can be evidence of having received Spirit baptism. Other immediate evidences include giving God praise, having joy, and desiring to testify about Jesus. Enduring or permanent results in the believer's life include Christ glorified and revealed in a greater way, a "deeper passion for souls", greater power to witness to nonbelievers, a more effective prayer life, greater love for and insight into the Bible, and the manifestation of the gifts of the Spirit.
While the baptism with the Holy Spirit is a definite experience in a believer's life, Pentecostals view it as just the beginning of living a Spirit-filled life. Pentecostal teaching stresses the importance of continually being filled with the Spirit. There is only one baptism with the Spirit, but there should be many infillings with the Spirit throughout the believer's life.
Divine healing.
Pentecostalism is a holistic faith, and the belief that Jesus is Healer is one quarter of the full gospel. Pentecostals cite four major reasons for believing in divine healing: 1) it is reported in the Bible, 2) Jesus' healing ministry is included in his atonement (thus divine healing is part of salvation), 3) "the whole gospel is for the whole person"—spirit, soul, and body, 4) sickness is a consequence of the Fall of Man and salvation is ultimately the restoration of the fallen world. In the words of Pentecostal scholar Vernon L. Purdy, "Because sin leads to human suffering, it was only natural for the Early Church to understand the ministry of Christ as the alleviation of human suffering, since he was God's answer to sin ... The restoration of fellowship with God is the most important thing, but this restoration not only results in spiritual healing but many times in physical healing as well." In the book "In Pursuit of Wholeness: Experiencing God's Salvation for the Total Person", Pentecostal writer and Church historian Wilfred Graves, Jr. describes the healing of the body as a physical expression of salvation.
For Pentecostals, spiritual and physical healing serves as a reminder and testimony to Christ's future return when his people will be completely delivered from all the consequences of the fall. However, not everyone receives healing when they pray. It is God in his sovereign wisdom who either grants or withholds healing. Common reasons that are given in answer to the question why are all not healed include: God teaches through suffering, healing is not always immediate, lack of faith on the part of the person needing healing, and personal sin in one's life (however, this does not mean that all illness is caused by personal sin). Regarding healing and prayer Purdy states:
On the other hand, it appears from Scripture that when we are sick we should be prayed for, and as we shall see later in this chapter, it appears that God's normal will is to heal. Instead of expecting that it is not God's will to heal us, we should pray with faith, trusting that God cares for us and that the provision He has made in Christ for our healing is sufficient. If He does not heal us, we will continue to trust Him. The victory many times will be procured in faith (see Heb. 10:35–36; 1 John 5:4–5).
Pentecostals believe that prayer is central in receiving healing. Pentecostals look to scriptures such as James 5:13–16 for direction regarding healing prayer. One can pray for one's own healing (verse 13) and for the healing of others (verse 16); no special gift or clerical status is necessary. Verses 14–16 supply the framework for congregational healing prayer. The sick person expresses his or her faith by calling for the elders of the church who pray over and anoint the sick with olive oil. The oil is a symbol of the Holy Spirit.
Besides prayer, there are other ways in which Pentecostals believe healing can be received. One way is based on Mark 16:17–18 and involves believers laying hands on the sick. This is done in imitation of Jesus who often healed in this manner. Another method that is found in some Pentecostal churches is based on the account in Acts 19:11–12 where people were healed when given handkerchiefs or aprons worn by the Apostle Paul. This practice is described by Duffield and Van Cleave in "Foundations of Pentecostal Theology":
Many Churches have followed a similar pattern and have given out small pieces of cloth over which prayer has been made, and sometimes they have been anointed with oil. Some most remarkable miracles have been reported from the use of this method. It is understood that the prayer cloth has no virtue in itself, but provides an act of faith by which one's attention is directed to the Lord, who is the Great Physician.
During the initial decades of the movement, Pentecostals thought it was sinful to take medicine or receive care from doctors. Over time, Pentecostals moderated their views concerning medicine and doctor visits; however, a minority of Pentecostal churches continues to rely exclusively on prayer and divine healing. For example, doctors in the United Kingdom reported that a minority of Pentecostal HIV patients was encouraged to stop taking their medicines and parents were told to stop giving medicine to their children, trends that placed lives at risk.
Eschatology.
The last element of the fourfold gospel is that Jesus is the "Soon Coming King". For Pentecostals, "every moment is eschatological" since at any time Christ may return. This "personal and imminent" Second Coming is for Pentecostals the motivation for practical Christian living including: personal holiness, meeting together for worship, faithful Christian service, and evangelism (both personal and worldwide). Many, if not the majority, of Pentecostals are premillennial dispensationalists believing in a pretribulation rapture.
Spiritual gifts.
Pentecostals are continuationists, meaning they believe that all of the spiritual gifts, including the miraculous or "sign gifts", found in , , , and continue to operate within the Church in the present time. Pentecostals place the gifts of the Spirit in context with the fruit of the Spirit. The fruit of the Spirit is the result of the new birth and continuing to abide in Christ. It is by the fruit exhibited that spiritual character is assessed. Spiritual gifts are received as a result of the baptism with the Holy Spirit. As gifts freely given by the Holy Spirit, they cannot be earned or merited, and they are not appropriate criteria with which to evaluate one's spiritual life or maturity. Pentecostals see in the biblical writings of Paul an emphasis on having both character and power, exercising the gifts in love.
Just as fruit should be evident in the life of every Christian, Pentecostals believe that every Spirit-filled believer is given some capacity for the manifestation of the Spirit. It is important to note that the exercise of a gift is a manifestation of the Spirit, not of the gifted person, and though the gifts operate through people, they are primarily gifts given to the Church. They are valuable only when they minister spiritual profit and edification to the body of Christ. Pentecostal writers point out that the lists of spiritual gifts in the New Testament do not seem to be exhaustive. It is generally believed that there are as many gifts as there are useful ministries and functions in the Church. A spiritual gift is often exercised in partnership with another gift. For example, in a Pentecostal church service, the gift of tongues might be exercised followed by the operation of the gift of interpretation.
According to Pentecostals, all manifestations of the Spirit are to be judged by the church. This is made possible, in part, by the gift of discerning of spirits, which is the capacity for discerning the source of a spiritual manifestation—whether from the Holy Spirit, an evil spirit, or from the human spirit. While Pentecostals believe in the current operation of all the spiritual gifts within the church, their teaching on some of these gifts has generated more controversy and interest than others. There are different ways in which the gifts have been grouped. W.R. Jones suggests three categories, illumination (Word of Wisdom, word of knowledge, discerning of spirits), action (Faith, working of miracles and gifts of healings) and communication (Prophecy, tongues and interpretation of tongues). Duffield and Van Cleave use two categories: the vocal and the power gifts.
Vocal gifts.
The gifts of prophecy, tongues, interpretation of tongues, and words of wisdom and knowledge are called the vocal gifts. Pentecostals look to for instructions on the proper use of the spiritual gifts, especially the vocal ones. Pentecostals believe that prophecy is the vocal gift of preference, a view derived from 1 Corinthians 14. Some teach that the gift of tongues is equal to the gift of prophecy when tongues are interpreted. Prophetic and glossolalic utterances are not to replace the preaching of the Word of God nor to be considered as equal to or superseding the written Word of God, which is the final authority for determining teaching and doctrine.
Word of wisdom and word of knowledge.
Pentecostals understand the word of wisdom and the word of knowledge to be supernatural revelations of wisdom and knowledge by the Holy Spirit. The word of wisdom is defined as a revelation of the Holy Spirit that applies scriptural wisdom to a specific situation that a Christian community faces. The word of knowledge is often defined as the ability of one person to know what God is currently doing or intends to do in the life of another person.
Prophecy.
Pentecostals agree with the Protestant principle of "sola Scriptura". The Bible is the "all sufficient rule for faith and practice"; it is "fixed, finished, and objective revelation". Alongside this high regard for the authority of scripture is a belief that the gift of prophecy continues to operate within the Church. Pentecostal theologians Duffield and van Cleave described the gift of prophecy in the following manner: "Normally, in the operation of the gift of prophecy, the Spirit heavily anoints the believer to speak forth to the body not premeditated words, but words the Spirit supplies spontaneously in order to uplift and encourage, incite to faithful obedience and service, and to bring comfort and consolation."
Any Spirit-filled Christian, according to Pentecostal theology, has the potential, as with all the gifts, to prophesy. Sometimes, prophecy can overlap with preaching "where great unpremeditated truth or application is provided by the Spirit, or where special revelation is given beforehand in prayer and is empowered in the delivery".
While a prophetic utterance at times might foretell future events, this is not the primary purpose of Pentecostal prophecy and is never to be used for personal guidance. For Pentecostals, prophetic utterances are fallible, i.e. subject to error. Pentecostals teach that believers must discern whether the utterance has edifying value for themselves and the local church. Because prophecies are subject to the judgement and discernment of other Christians, most Pentecostals teach that prophetic utterances should never be spoken in the first person (e.g. "I, the Lord") but always in the third person (e.g. "Thus saith the Lord" or "The Lord would have...").
Tongues and interpretation.
A Pentecostal believer in a spiritual experience may vocalize fluent, unintelligible utterances (glossolalia) or articulate a natural language previously unknown to them (xenoglossy). Commonly termed "speaking in tongues", this vocal phenomenon is believed by Pentecostals to include an endless variety of languages. According to Pentecostal theology, the language spoken (1) may be an unlearned human language, such as the Bible claims happened on the Day of Pentecost, or (2) it might be of heavenly (angelic) origin. In the first case, tongues could work as a sign by which witness is given to the unsaved. In the second case, tongues are used for praise and prayer when the mind is superseded and "the speaker in tongues speaks to God, speaks mysteries, and ... no one understands him".
Within Pentecostalism, there is a belief that speaking in tongues serves two functions. Tongues as the "initial evidence" of the baptism with the Holy Spirit and in individual prayer serves a different purpose than tongues as a spiritual gift. All Spirit-filled believers, according to initial evidence proponents, will speak in tongues when baptized in the Spirit and, thereafter, will be able to express prayer and praise to God in an unknown tongue. This type of tongue speaking forms an important part of many Pentecostals' personal daily devotions. When used in this way, it is referred to as a "prayer language" as the believer is speaking unknown languages not for the purpose of communicating with others but for "communication between the soul and God". Its purpose is for the spiritual edification of the individual. Pentecostals believe the private use of tongues in prayer (i.e. "prayer in the Spirit") "promotes a deepening of the prayer life and the spiritual development of the personality". From , Pentecostals believe that the Spirit intercedes for believers through tongues; in other words, when a believer prays in an unknown tongue, the Holy Spirit is supernaturally directing the believer's prayer.
Besides acting as a prayer language, tongues also function as the "gift of tongues". Not all Spirit-filled believers possess the gift of tongues. Its purpose is for gifted persons to publicly "speak with God in praise, to pray or sing in the Spirit, or to speak forth in the congregation". There is a division among Pentecostals on the relationship between the gifts of tongues and prophecy. One school of thought believes that the gift of tongues is always directed from man to God, in which case it is always prayer or praise spoken to God but in the hearing of the entire congregation for encouragement and consolation. Another school of thought believes that the gift of tongues can be prophetic, in which case the believer delivers a "message in tongues"—a prophetic utterance given under the influence of the Holy Spirit—to a congregation.
Whether prophetic or not, however, Pentecostals are agreed that all public utterances in an unknown tongue must be interpreted in the language of the gathered Christians. This is accomplished by the gift of interpretation, and this gift can be exercised by the same individual who first delivered the message (if he or she possesses the gift of interpretation) or by another individual who possesses the required gift. If a person with the gift of tongues is not sure that a person with the gift of interpretation is present and is unable to interpret the utterance him or herself, then the person should not speak. Pentecostals teach that those with the gift of tongues should pray for the gift of interpretation. Pentecostals do not require that an interpretation be a literal word-for-word translation of a glossolalic utterance. Rather, as the word "interpretation" implies, Pentecostals expect only an accurate explanation of the utterance's meaning.
Besides the gift of tongues, Pentecostals may also use glossolalia as a form of praise and worship in corporate settings. Pentecostals in a church service may pray aloud in tongues while others pray simultaneously in the common language of the gathered Christians. This use of glossolalia is seen as an acceptable form of prayer and therefore requires no interpretation. Congregations may also corporately sing in tongues, a phenomenon known as singing in the Spirit.
Speaking in tongues is not universal among Pentecostal Christians. In 2006, a 10-country survey by the Pew Forum on Religion and Public Life found that 49 percent of Pentecostals in the United States, 50 percent in Brazil, 41 percent in South Africa, and 54 percent in India said they "never" speak or pray in tongues.
Power gifts.
The gifts of power are distinct from the vocal gifts in that they do not involve utterance. Included in this category are the gift of faith, gifts of healing, and the gift of miracles. The gift of faith (sometimes called "special" faith) is different from "saving faith" and normal Christian faith in its degree and application. This type of faith is a manifestation of the Spirit granted only to certain individuals "in times of special crisis or opportunity" and endues them with "a divine certainty ... that triumphs over everything". It is sometimes called the "faith of miracles" and is fundamental to the operation of the other two power gifts.
Worship.
Traditional Pentecostal worship has been described as a "gestalt made up of prayer, singing, sermon, the operation of the gifts of the Spirit, altar intercession, offering, announcements, testimonies, musical specials, Scripture reading, and occasionally the Lord's supper". Russell P. Spittler identified five values that govern Pentecostal spirituality. The first was individual experience, which emphasizes the Holy Spirit's personal work in the life of the believer. Second was orality, a feature that might explain Pentecostalism's success in evangelizing nonliterate cultures. The third was spontaneity; members of Pentecostal congregations are expected to follow the leading of the Holy Spirit, sometimes resulting in unpredictable services. The fourth value governing Pentecostal spirituality was "otherworldliness" or asceticism, which was partly informed by Pentecostal eschatology. The final and fifth value was a commitment to biblical authority, and many of the distinctive practices of Pentecostals are derived from a literal reading of scripture.
Spontaneity is a characteristic element of Pentecostal worship. This was especially true in the movement's earlier history, when anyone could initiate a song, chorus, or spiritual gift. Even as Pentecostalism has become more organized and formal, with more control exerted over services, the concept of spontaneity has retained an important place within the movement and continues to inform stereotypical imagery, such as the derogatory "holy roller". The phrase "Quench not the Spirit", derived from 1 Thessalonians 5:19, is used commonly and captures the thought behind Pentecostal spontaneity.
Prayer plays an important role in Pentecostal worship. Collective oral prayer, whether glossolalic or in the vernacular or a mix of both, is common. While praying, individuals may lay hands on a person in need of prayer, or they may raise their hands in response to biblical commands (1 Timothy 2:8). The raising of hands (which itself is a revival of the ancient orans posture) is an example of some Pentecostal worship practices that have been widely adopted by the larger Christian world. Pentecostal musical and liturgical practice have also played an influential role in shaping contemporary worship trends, with Pentecostal churches such as Hillsong Church being the leading producers of congregational music.
Several spontaneous practices have become characteristic of Pentecostal worship. Being "slain in the Spirit" or "falling under the power" is a form of prostration in which a person falls backwards, as if fainting, while being prayed over. It is at times accompanied by glossolalic prayer; at other times, the person is silent. It is believed by Pentecostals to be caused by "an overwhelming experience of the presence of God", and Pentecostals sometimes receive the baptism in the Holy Spirit in this posture. Another spontaneous practice is "dancing in the Spirit". This is when a person leaves their seat "spontaneously 'dancing' with eyes closed without bumping into nearby persons or objects". It is explained as the worshipper becoming "so enraptured with God's presence that the Spirit takes control of physical motions as well as the spiritual and emotional being". Pentecostals derive biblical precedent for dancing in worship from 2 Samuel 6, where David danced before the Lord. A similar occurrence is often called "running the aisles". The "Jericho march" (inspired by Book of Joshua 6:1–27) is a celebratory practice occurring at times of high enthusiasm. Members of a congregation began to spontaneously leave their seats and walk in the aisles inviting other members as they go. Eventually, a full column is formed around the perimeter of the meeting space as worshipers march with singing and loud shouts of praise and jubilation. In some Pentecostal churches, these spontaneous expressions are primarily found in revival meetings or special prayer meetings, being rare or non-existent in the main services.
Ordinances.
Like other Christian churches, Pentecostals believe that certain rituals or ceremonies were instituted as a pattern and command by Jesus in the New Testament. Pentecostals commonly call these ceremonies ordinances. Many Christians call these sacraments, but this term is not generally used by Pentecostals and certain other Protestants as they do not see ordinances as imparting grace. Instead the term sacerdotal ordinance is used to denote the distinctive belief that grace is received directly from God by the congregant with the officiant serving only to facilitate rather than acting as a conduit or vicar.
The ordinance of water baptism is an outward symbol of an inner conversion that has already taken place. Therefore, most Pentecostal groups practice believer's baptism by immersion. The majority of Pentecostals do not view baptism as essential for salvation, and likewise, most Pentecostals are Trinitarian and use the traditional Trinitarian baptismal formula. However, Oneness Pentecostals view baptism as an essential and necessary part of the salvation experience and, as non-Trinitarians, reject the use of the traditional baptismal formula. For more information on Oneness Pentecostal baptismal beliefs, see the following section on Statistics and denominations.
The ordinance of Holy Communion, or the Lord's Supper, is seen as a direct command given by Jesus at the Last Supper, to be done in remembrance of him. Pentecostal denominations reject the use of wine as part of communion, using grape juice instead. Foot washing is also held as an ordinance by some Pentecostals. It is considered an "ordinance of humility" because Jesus showed humility when washing his disciples' feet in . Other Pentecostals do not consider it an ordinance; however, they may still recognize spiritual value in the practice.
Statistics and denominations.
In 1995, David Barrett estimated there were 217 million "Denominational Pentecostals" throughout the world. In 2011, a Pew Forum study of global Christianity found that there were an estimated 279 million classical Pentecostals, making 4 percent of the total world population and 12.8 percent of the world's Christian population Pentecostal. The study found "Historically pentecostal denominations" (a category that did not include independent Pentecostal churches) to be the largest Protestant denominational family. The largest percentage of Pentecostals are found in Sub-Saharan Africa (44 percent), followed by the Americas (37 percent) and Asia and the Pacific (16 percent). The movement is enjoying its greatest surge today in the global South, which includes Africa, Latin America, and most of Asia. There are 740 recognized Pentecostal denominations, but the movement also has a significant number of independent churches that are not organized into denominations.
Among the over 700 Pentecostal denominations, 240 are classified as part of Wesleyan, Holiness, or Methodistic Pentecostalism. Until 1910, Pentecostalism was universally Wesleyan in doctrine, and Holiness Pentecostalism continues to predominate in the Southern United States. Wesleyan Pentecostals teach that there are three crisis experiences within a Christian's life: conversion, sanctification, and Spirit baptism. They inherited the holiness movement's belief in entire sanctification. According to Wesleyan Pentecostals, entire sanctification is a definite event that occurs after salvation but before Spirit baptism. This experience cleanses and enables the believer to live a life of personal holiness. This personal cleansing prepares the believer to receive the baptism in the Holy Spirit. Holiness Pentecostal denominations include the Church of God in Christ, Church of God (Cleveland, Tennessee), and the International Pentecostal Holiness Church.
After William H. Durham began preaching his Finished Work doctrine in 1910, many Pentecostals rejected the Wesleyan doctrine of entire sanctification and began to teach that there were only two definite crisis experiences in the life of a Christian: conversion and Spirit baptism. These Finished Work Pentecostals (also known as "Baptistic" or "Reformed" Pentecostals because many converts were originally drawn from Baptist and Presbyterian backgrounds) teach that a person is initially sanctified at the moment of conversion. After conversion, the believer grows in grace through a lifelong process of progressive sanctification. There are 390 denominations that adhere to the finished work position. They include the Assemblies of God, the International Church of the Foursquare Gospel, and the Open Bible Churches.
The 1904-1905 Welsh Revival laid the foundation for British Pentecostalism and especially for a distinct family of denominations known as Apostolic Pentecostalism (not to be confused with Oneness Pentecostalism). These Pentecostals are led by a hierarchy of living apostles, prophets, and other charismatic offices. Apostolic Pentecostals are found worldwide in 30 denominations, including the Apostolic Church based in the United Kingdom.
There are 80 Pentecostal denominations that are classified as Jesus' Name or Oneness Pentecostalism (often self identifying as "Apostolic Pentecostals"). These differ from the rest of Pentecostalism in several significant ways. Oneness Pentecostals reject the doctrine of the Trinity. They do not describe God as three persons but rather as three manifestations of the one living God. Oneness Pentecostals practice Jesus' Name Baptism—water baptisms performed in the name of Jesus Christ, rather than that of the Trinity. Oneness Pentecostal adherents believe repentance, baptism in Jesus' name, and Spirit baptism are all essential elements of the conversion experience. Oneness Pentecostals hold that repentance is necessary before baptism to make the ordinance valid, and receipt of the Holy Spirit manifested by speaking in other tongues is necessary afterwards, to complete the work of baptism. This differs from other Pentecostals, along with evangelical Christians in general, who see only repentance and faith in Christ as essential to salvation. This has resulted in Oneness believers being accused by some (including other Pentecostals) of a "works-salvation" soteriology, a charge they vehemently deny. Oneness Pentecostals insist that salvation comes by grace through faith in Christ, coupled with obedience to his command to be "born of water and of the Spirit"; hence, no good works or obedience to laws or rules can save anyone. For them, baptism is not seen as a "work" but rather the indispensable means that Jesus himself provided to come into his kingdom. The major Oneness churches include the United Pentecostal Church International and the Pentecostal Assemblies of the World.
In addition to the denominational Pentecostal churches, there are many Pentecostal churches that choose to exist independently of denominational oversight. Some of these churches may be doctrinally identical to the various Pentecostal denominations, while others may adopt beliefs and practices that differ considerably from classical Pentecostalism, such as Word of Faith teachings or Kingdom Now theology. Some of these groups have been successful in utilizing the mass media, especially television and radio, to spread their message.
History.
Background.
The charismatic experiences found in Pentecostalism have precedents in earlier movements in Christianity. Church historian Dr. Curtis Ward proposes the existence of an unbroken Pentecostal lineage from the early church to the present, with glossolalia and gifts following. However, early Pentecostals considered the movement a latter day restoration of the church's apostolic power, and most historians of Pentecostalism write that the movement emerged from late 19th century radical evangelical revival movements in America and Great Britain.
Within this radical evangelicalism, expressed most strongly in the holiness and higher life movements, themes of restorationism, premillennialism, faith healing, and greater attention on the person and work of the Holy Spirit were central to emerging Pentecostalism. Evangelicals felt that modern Christianity was missing the power and authority of the New Testament church. Believing that the second coming of Christ was imminent, many evangelicals expected an endtime revival that would bring many people to Christ. Many leaders began to speak of an experience available to all Christians which would empower believers to evangelize the world, often termed "baptism with the Holy Spirit". The earliest Pentecostals understood their movement historically within the framework of a "Latter Rain motif"—a modified version of dispensationalism in which the return to prominence of the charismata within the church was a sign of the imminence of Christ's Second Coming.
Certain Christian leaders and movements had important influences on early Pentecostals. The essentially universal belief in the continuation of all the spiritual gifts in the Keswick and Faith Cure movement-the Higher Life for the soul and for the body-constituted crucial historical background for the rise of Pentecostalism. Albert Benjamin Simpson and his Christian and Missionary Alliance was very influential in the early years of Pentecostalism, especially on the development of the Assemblies of God. Another early influence on Pentecostals was John Alexander Dowie and his Christian Catholic Apostolic Church. The teachings of Simpson, Dowie, Adoniram Judson Gordon and Maria Woodworth-Etter (she later joined the Pentecostal movement) on healing were embraced by Pentecostals. Edward Irving's Catholic Apostolic Church also shared many characteristics later found in the Pentecostal revival.
There was no one founder of Pentecostalism. Instead, isolated Christian groups were experiencing charismatic phenomena such as divine healing and speaking in tongues. The Wesleyan holiness movement provided a theological explanation for what was happening to these Christians. They adapted Wesleyan soteriology to accommodate their new understanding. Pentecostalism's Wesleyan-holiness heritage distinguishes it from the rest of Evangelicalism, which has roots in Christian Fundamentalism.
Early revivals: 1900–29.
Charles Fox Parham, an independent holiness evangelist who believed strongly in divine healing, was an important figure to the emergence of Pentecostalism as a distinct Christian movement. In 1900, he started a school near Topeka, Kansas, which he named Bethel Bible School. There he taught that speaking in tongues was the scriptural evidence for the reception of the baptism with the Holy Spirit. On January 1, 1901, after a watch night service, the students prayed for and received the baptism with the Holy Spirit with the evidence of speaking in tongues. Parham received this same experience sometime later and began preaching it in all his services. Parham believed this was xenoglossia and that missionaries would no longer need to study foreign languages. After 1901, Parham closed his Topeka school and began a four-year revival tour throughout Kansas and Missouri. He taught that the baptism with the Holy Spirit was a third experience, subsequent to conversion and sanctification. Sanctification cleansed the believer, but Spirit baptism empowered for service.
At about the same time that Parham was spreading his doctrine of initial evidence in the Midwestern United States, news of the Welsh Revival of 1904–05 ignited intense speculation among radical evangelicals around the world and particularly in the United States of a coming move of the Spirit which would renew the entire Christian Church. This revival saw thousands of conversions and also exhibited speaking in tongues.
In 1905, Parham moved to Houston, Texas, where he started a Bible training school. One of his students was William J. Seymour, a one-eyed black preacher. Seymour traveled to Los Angeles where his preaching sparked the three-year-long Azusa Street Revival in 1906. Worship at the racially integrated Azusa Mission featured an absence of any order of service. People preached and testified as moved by the Spirit, spoke and sung in tongues, and fell in the Spirit. The revival attracted both religious and secular media attention, and thousands of visitors flocked to the mission, carrying the "fire" back to their home churches. Despite the work of various Wesleyan groups such as Parham's and D. L. Moody's revivals, the beginning of the widespread Pentecostal movement in the United States is generally considered to have begun with Seymour's Azusa Street Revival.
The crowds of African-Americans and whites worshiping together at William Seymour's Azusa Street Mission set the tone for much of the early Pentecostal movement. During the period of 1906–24, Pentecostals defied social, cultural and political norms of the time that called for racial segregation and the enactment of Jim Crow laws. The Church of God in Christ, the Church of God (Cleveland), the Pentecostal Holiness Church, and the Pentecostal Assemblies of the World were all interracial denominations before the 1920s. These groups, especially in the Jim Crow South were under great pressure to conform to segregation. Ultimately, North American Pentecostalism would divide into white and African-American branches. Though it never entirely disappeared, interracial worship within Pentecostalism would not reemerge as a widespread practice until after the Civil Rights Movement.
Women were vital to the early Pentecostal movement. Believing that whoever received the Pentecostal experience had the responsibility to use it towards the preparation for Christ's second coming, Pentecostal women held that the baptism in the Holy Spirit gave them empowerment and justification to engage in activities traditionally denied to them. The first person at Parham's Bible college to receive Spirit baptism with the evidence of speaking in tongues was a woman, Agnes Ozman. Women such as Florence Crawford, Ida Robinson, and Aimee Semple McPherson founded new denominations, and many women served as pastors, co-pastors, and missionaries. Women wrote religious songs, edited Pentecostal papers, and taught and ran Bible schools. The unconventionally intense and emotional environment generated in Pentecostal meetings dually promoted, and was itself created by, other forms of participation such as personal testimony and spontaneous prayer and singing. Women did not shy away from engaging in this forum, and in the early movement the majority of converts and church-goers were female. Nevertheless, there was considerable ambiguity surrounding the role of women in the church. The subsiding of the early Pentecostal movement allowed a socially more conservative approach to women to settle in, and, as a result, female participation was channeled into more supportive and traditionally accepted roles. Auxiliary women's organizations were created to focus women's talents on more traditional activities. Women also became much more likely to be evangelists and missionaries than pastors. When they were pastors, they often co-pastored with their husbands.
The majority of early Pentecostal denominations taught pacifism and adopted military service articles that advocated conscientious objection.
Spread and opposition.
Azusa participants returned to their homes carrying their new experience with them. In many cases, whole churches were converted to the Pentecostal faith, but many times Pentecostals were forced to establish new religious communities when their experience was rejected by the established churches. Because speaking in tongues was initially believed to always be actual foreign languages, it was believed that missionaries would no longer have to learn the languages of the peoples they evangelized because the Holy Spirit would provide whatever foreign language was required. (When the majority of missionaries, to their disappointment, learned that tongues speech was unintelligible on the mission field, Pentecostal leaders were forced to modify their understanding of tongues.) Thus, as the experience of speaking in tongues spread, a sense of the immediacy of Christ's return took hold and that energy would be directed into missionary and evangelistic activity. Early Pentecostals saw themselves as outsiders from mainstream society, dedicated solely to preparing the way for Christ's return.
An associate of Seymour's, Florence Crawford, brought the message to the Northwest, forming what would become the Apostolic Faith Church by 1908. After 1907, Azusa participant William Howard Durham, pastor of the North Avenue Mission in Chicago, returned to the Midwest to lay the groundwork for the movement in that region. It was from Durham's church that future leaders of the Pentecostal Assemblies of Canada would hear the Pentecostal message. One of the most well known Pentecostal pioneers was Gaston B. Cashwell (the "Apostle of Pentecost" to the South), whose evangelistic work led three Southeastern holiness denominations into the new movement.
The Pentecostal movement, especially in its early stages, was typically associated with the impoverished and marginalized of America, especially African Americans and Southern Whites. With the help of many healing evangelists such as Oral Roberts, Pentecostalism spread across America by the 1950s.
International visitors and Pentecostal missionaries would eventually export the revival to other nations. The first foreign Pentecostal missionaries were A. G. Garr and his wife, who were Spirit baptized at Azusa and traveled to India and later Hong Kong. The Norwegian Methodist pastor T. B. Barratt was influenced by Seymour during a tour of the United States. By December 1906, he had returned to Europe and is credited with beginning the Pentecostal movement in Sweden, Norway, Denmark, Germany, France and England. A notable convert of Barratt was Alexander Boddy, the Anglican vicar of All Saints' in Sunderland, England, who became a founder of British Pentecostalism. Other important converts of Barratt were German minister Jonathan Paul who founded the first German Pentecostal denomination (the Mülheim Association) and Lewi Pethrus, the Swedish Baptist minister who founded the Swedish Pentecostal movement.
Through Durham's ministry, Italian immigrant Luigi Francescon received the Pentecostal experience in 1907 and established Italian Pentecostal congregations in the United States (Christian Congregation in the United States), Argentina (Christian Assembly in Argentina), and Brazil (Christian Congregation of Brazil). In 1908, Giacomo Lombardi led the first Pentecostal services in Italy. In November 1910, two Swedish Pentecostal missionaries arrived in Belem, Brazil and established what would become the Assembleias de Deus (Assemblies of God of Brazil). In 1908, John G. Lake, a follower of Alexander Dowie who had experienced Pentecostal Spirit baptism, traveled to South Africa and founded what would become the Apostolic Faith Mission of South Africa and the Zion Christian Church. As a result of this missionary zeal, practically all Pentecostal denominations today trace their historical roots to the Azusa Street Revival.
The first generation of Pentecostal believers faced immense criticism and ostracism from other Christians, most vehemently from the Holiness movement from which they originated. Alma White, leader of the Pillar of Fire Church, wrote a book against the movement titled "Demons and Tongues" in 1910. She called Pentecostal tongues "satanic gibberish" and Pentecostal services "the climax of demon worship". Famous holiness preacher W. B. Godbey characterized those at Azusa Street as "Satan's preachers, jugglers, necromancers, enchanters, magicians, and all sorts of mendicants". To Dr. G. Campbell Morgan, Pentecostalism was "the last vomit of Satan", while Dr. R. A. Torrey thought it was "emphatically not of God, and founded by a Sodomite". Ironically, the Pentecostal Church of the Nazarene, one of the largest holiness groups, was strongly opposed to the new Pentecostal movement. To avoid confusion, the church changed its name in 1919 to the Church of the Nazarene. A. B. Simpson's Christian and Missionary Alliance negotiated a compromise position unique for the time. Simpson believed that Pentecostal tongues speaking was a legitimate manifestation of the Holy Spirit, but he did not believe it was a necessary evidence of Spirit baptism. This view on speaking in tongues ultimately led to what became known as the "Alliance position" articulated by A. W. Tozer as "seek not—forbid not".
Early controversies.
The first Pentecostal converts were mainly derived from the Holiness movement and adhered to a Wesleyan understanding of sanctification as a definite, instantaneous experience and "second work of grace". Problems with this view arose when large numbers of converts entered the movement from non-Wesleyan backgrounds, especially from Baptist churches. In 1910, William Durham of Chicago first articulated the Finished Work, a doctrine which located sanctification at the moment of salvation and held that after conversion the Christian would progressively grow in grace in a lifelong process. This teaching polarized the Pentecostal movement into two factions. The Wesleyan doctrine was strongest in the Southern denominations, such as the Church of God (Cleveland), Church of God in Christ, and the Pentecostal Holiness Church. The Finished Work, however, would ultimately gain ascendancy among Pentecostals. After 1911, most new Pentecostal denominations would adhere to Finished Work sanctification.
In 1914, a group of predominately 300 white Pentecostal ministers and laymen from all regions of the United States gathered in Hot Springs, Arkansas, to create a new, national Pentecostal fellowship—the General Council of the Assemblies of God. By 1911, many of these white ministers were distancing themselves from an existing arrangement under an African-American leader. Many of these white ministers were licensed by the African-American, C. H. Mason under the auspices of the Church of God in Christ,one of the few legally chartered Pentecostal organizations at the time credentialing and licensing ordained Pentecostal clergy. To further such distance, Bishop Mason and other African-American Pentecostal leaders were not invited to the initial 1914 fellowship of Pentecostal ministers. These predominately white ministers adopted a congregational polity (whereas the COGIC and other Southern groups remained largely episcopal) and rejected a Finished Work understanding of Sanctification. Thus, the creation of the Assemblies of God marked an official end of Pentecostal doctrinal unity and racial integration.
The new Assemblies of God would soon face a "new issue" which first emerged at a 1913 camp meeting. During a baptism service, the speaker, R. E. McAlister, mentioned that the Apostles baptized converts once in the name of Jesus Christ, and the words "Father, Son, and Holy Ghost" were never used in baptism. This inspired Frank Ewart who claimed to have received as a divine prophecy revealing a nontrinitarian conception of God. Ewart believed that there was only one personality in the Godhead—Jesus Christ. The terms "Father" and "Holy Ghost" were titles designating different aspects of Christ. Those who had been baptized in the Trinitarian fashion needed to submit to rebaptism in Jesus' name. Furthermore, Ewart believed that Jesus' name baptism and the gift of tongues were essential for salvation. Ewart and those who adopted his belief called themselves "oneness" or "Jesus' Name" Pentecostals, but their opponents called them "Jesus Only".
Amid great controversy, the Assemblies of God rejected the Oneness teaching, and a large number of its churches and pastors were forced to withdraw from the denomination in 1916. They organized their own Oneness groups. Most of these joined Garfield T. Haywood, an African-American preacher from Indianapolis, to form the Pentecostal Assemblies of the World. This church maintained an interracial identity until 1924 when the white ministers withdrew to form the Pentecostal Church, Incorporated. This church later merged with another group forming the United Pentecostal Church International.
1930–59.
While Pentecostals shared many basic assumptions with conservative Protestants, the earliest Pentecostals were rejected by Fundamentalist Christians who adhered to cessationism. In 1928, the World Christian Fundamentals Association labeled Pentecostalism "fanatical" and "unscriptural". By the early 1940s, this rejection of Pentecostals was giving way to a new cooperation between them and leaders of the "new evangelicalism", and American Pentecostals were involved in the founding of the 1942 National Association of Evangelicals. Pentecostal denominations also began to interact with each other both on national levels and international levels through the Pentecostal World Fellowship, which was founded in 1947.
Though Pentecostals began to find acceptance among evangelicals in the 1940s, the previous decade was widely viewed as a time of spiritual dryness, when healings and other miraculous phenomena were perceived as being less prevalent than in earlier decades of the movement. It was in this environment that the Latter Rain Movement, the most important controversy to affect Pentecostalism since World War II, began in North America and spread around the world in the late 1940s. Latter Rain leaders taught the restoration of the fivefold ministry led by apostles. These apostles were believed capable of imparting spiritual gifts through the laying on of hands. There were prominent participants of the early Pentecostal revivals, such as Stanley Frodsham and Lewi Pethrus, who endorsed the movement citing similarities to early Pentecostalism. However, Pentecostal denominations were critical of the movement and condemned many of its practices as unscriptural. One reason for the conflict with the denominations was the sectarianism of Latter Rain adherents. Many autonomous churches were birthed out of the revival.
A simultaneous development within Pentecostalism was the postwar Healing Revival. Led by healing evangelists William Branham, Oral Roberts, Gordon Lindsay, and T. L. Osborn, the Healing Revival developed a following among non-Pentecostals as well as Pentecostals. Many of these non-Pentecostals were baptized in the Holy Spirit through these ministries. The Latter Rain and the Healing Revival influenced many leaders of the charismatic movement of the 1960s and 1970s.
1960–present.
Before the 1960s, most non-Pentecostal Christians who experienced the Pentecostal baptism in the Holy Spirit typically kept their experience a private matter or joined a Pentecostal church afterward. The 1960s saw a new pattern develop where large numbers of Spirit baptized Christians from mainline churches in the United States, Europe, and other parts of the world chose to remain and work for spiritual renewal within their traditional churches. This initially became known as New or Neo-Pentecostalism (in contrast to the older classical Pentecostalism) but eventually became known as the Charismatic Movement. While cautiously supportive of the Charismatic Movement, the failure of Charismatics to embrace traditional Pentecostal taboos on dancing, drinking alcohol, smoking, and restrictions on dress and appearance initiated an identity crisis for classical Pentecostals, who were forced to reexamine long held assumptions about what it meant to be Spirit filled. The liberalizing influence of the Charismatic Movement on classical Pentecostalism can be seen in the disappearance of many of these taboos since the 1960s. Because of this, the cultural differences between classical Pentecostals and charismatics have lessened over time. The global renewal movements manifest many of these tensions as inherent characteristics of Pentecostalism and as representative of the character of global Christianity.

</doc>
<doc id="23558" url="http://en.wikipedia.org/wiki?curid=23558" title="Pangenesis">
Pangenesis

Pangenesis was Charles Darwin's hypothetical mechanism for heredity. He presented this 'provisional hypothesis' in his 1868 work "The Variation of Animals and Plants under Domestication" and felt that it brought 'together a multitude of facts which are at present left disconnected by any efficient cause'. The etymology of the word comes from the Greek words "pan" (a prefix meaning "whole", "encompassing") and "genesis" ("birth") or "genos" ("origin"). The hypothesis was eventually replaced by Mendel's laws of inheritance.
The pangenesis theory, similar to Hippocrates's views on the topic, imply that the whole of parental organisms participate in heredity (thus the prefix "pan") while adapting to cell theory. Much of Darwin's model was speculatively based on inheritance of tiny heredity particles he called gemmules that could be transmitted from parent to offspring. Darwin emphasized that only cells could regenerate new tissues or generate new organisms. He posited that atomic sized gemmules formed by cells would diffuse and aggregate in the reproductive organs.
Overview.
Darwin's pangenesis theory was complex as he tried to explain the process of sexual reproduction, passing of traits and complex developmental phenomena, such as cellular regeneration.
Some gemmules remained dormant for generations, whereas others were routinely carried by all offspring. He thought about these literally, "almost as if gemmules were letters in the postal system". Every child was built up from a mixture of the parents and grandparents' gemmules coming from either side. Darwin likened this to gardening: a flowerbed could be sprinkled with seeds "most of which soon germinate, some lie for a period dormant, whilst others perish.". He did not claim gemmules were in the blood, although his theory was often interpreted in this way. Responding to Fleming Jenkin's review of The Origin of Species, he argued that pangenesis would permit the preservation of some favourable variations in a population so that they wouldn't die out through blending.
Hugo de Vries characterized the theory in two propositions, of which he only accepted the first:
Darwin's pangenesis theory was criticised for its Lamarckian premise that parents could pass on traits acquired in their lifetime. Lamarckism fell from favour after August Weismann's research in the 1880s indicated that changes from use (such as lifting weights to increase muscle mass) and disuse (such as being lazy and becoming scrawny) were not heritable. Some Lamarckian principles, however, have not been entirely discounted and some of Darwin's pangenesis principles (in this regard) do relate to heritable aspects of phenotypic plasticity, while the status of gemmules has been firmly rejected. Darwin himself had noted that "the existence of free gemmules is a gratuitous assumption"; by some accounts in modern interpretation, gemmules may be considered a prescient mix of DNA, RNA, proteins, prions, and other mobile elements that are heritable in a non-Mendelian manner at the molecular level.
Later elaboration.
In his later work, "The Descent of Man," Darwin elaborated further on the model. In a section on the "Laws of inheritance," Darwin specified that two elements in particular were most important: the "transmission" and the "development" of inherited characteristics. Darwin's insights were that characteristics could be transmitted which were not at the time of transmission actually being manifest in the parent organism, and that certain traits would manifest themselves at the same point of development (say, old age) in both the parent and child organisms. In order to make sense of his theory of sexual selection, he also stipulated that certain traits could be passed through organisms but would only develop depending on the sex of the organism in question.
Galton's experiments on rabbits.
Darwin's half-cousin Francis Galton conducted wide-ranging inquiries into heredity which led him to refute Charles Darwin's hypothetical theory of pangenesis. In consultation with Darwin, he set out to see if gemmules were transported in the blood. In a long series of experiments in 1869 to 1871, he transfused the blood between dissimilar breeds of rabbits, and examined the features of their offspring . He found no evidence of characters transmitted in the transfused blood . Darwin challenged the validity of Galton's experiment, giving his reasons in an article published in 'Nature' where he wrote: "Now, in the chapter on Pangenesis in my "Variation of Animals and Plants under Domestication," I have not said one word about the blood, or about any fluid proper to any circulating system. It is, indeed, obvious that the presence of gemmules in the blood can form no necessary part of my hypothesis; for I refer in illustration of it to the lowest animals, such as the Protozoa, which do not possess blood or any vessels; and I refer to plants in which the fluid, when present in the vessels, cannot be considered as true blood." He goes on to admit: "Nevertheless, when I first heard of Mr. Galton's experiments, I did not sufficiently reflect on the subject, and saw not the difficulty of believing in the presence of gemmules in the blood."

</doc>
<doc id="23560" url="http://en.wikipedia.org/wiki?curid=23560" title="Proboscidea">
Proboscidea

The Proboscidea (from the Greek προβοσκίς and the Latin "proboscis") are a taxonomic order containing one living family, Elephantidae, and several extinct families. This order, first described by J. Illiger in 1811, encompasses the trunked mammals. Later proboscideans are distinguished by tusks and long, muscular trunks; these features are less developed or absent in early proboscideans.
The earliest known proboscidean is "Eritherium", followed by "Phosphatherium", a small animal about the size of a fox. These both date from late Paleocene deposits of Morocco.
Proboscideans diversified during the Eocene and early Oligocene. Several primitive families from these epochs have been described, including Numidotheriidae, Moeritheriidae, and Barytheriidae in Africa. (Anthracobunidae from the Indian subcontinent has also been included, but was excluded from Proboscidea by Shoshani & Tassy (2005) and has more recently been assigned to Perissodactyla.) These were followed by the earliest Deinotheriidae, or "hoe tuskers", which thrived during the Miocene and into the early Quaternary. Proboscideans from the Miocene also included "Stegolophodon", an early genus of the disputed family Stegodontidae; the diverse family of Gomphotheriidae, or "shovel tuskers", such as "Platybelodon" and "Amebelodon"; and the Mammutidae, or mastodons.
Most families of Proboscidea are now extinct, many since the end of the last glacial period. Recently extinct species include the last examples of gomphotheres in Central and South America, the American mastodon of family Mammutidae in North America, numerous stegodonts once found in Asia, the last of the mammoths, and several island species of dwarf elephants.
The classification of proboscideans is unstable and frequently revised, and some relationships within the order remain unclear. As of 2005, at least 177 species and subspecies of proboscideans, classified in 43 genera, are recognized; the order is summarized as:

</doc>
<doc id="23561" url="http://en.wikipedia.org/wiki?curid=23561" title="Paranthropus">
Paranthropus

Paranthropus (from Greek παρα, "para" "beside"; άνθρωπος, "ánthropos" "human"), is a genus of extinct hominins. Also known as robust australopithecines, they were bipedal hominids that probably descended from the gracile australopithecine hominids ("Australopithecus") 2.7 million years ago.
Members of this genus are characterised by robust craniodental anatomy, including gorilla-like sagittal cranial crests, which suggest strong muscles of mastication, and broad, grinding herbivorous teeth. However, "Paranthropus" skulls lack the transverse cranial crests that are also present in modern gorillas.
Discovery.
A partial cranium and mandible of "Paranthropus robustus" was discovered in 1938 by a schoolboy, Gert Terblanche, at Kromdraai B (70 km south west of Pretoria) in South Africa. It was described as a new genus and species by Robert Broom of the Transvaal Museum. The site has been excavated since 1993 by Francis Thackeray of the Transvaal Museum. A date of at least 1.95 million years has been obtained for Kromdraai B.
"Paranthropus boisei" was discovered by Mary Leakey on July 17, 1959, at the FLK Bed I site of Olduvai Gorge in Tanzania (specimen OH 5). Mary was working alone, as Louis Leakey was ill in camp. She rushed back to camp and, at the news, Louis made a remarkable recovery. They refrained from excavating until Des Bartlett had photographed the site.
In his notes Louis recorded a first name, "Titanohomo mirabilis", reflecting an initial impression of close human affinity. Louis and Mary began to call it "Dear Boy". Recovery was halted on August 7. Dear Boy was in context with Oldowan tools and animal bones.
The fossil was published in "Nature" dated August 15, 1959, but due to a strike of the printers the issue was not released until September. In it Louis placed the fossil in Broom's Australopithecinae family, creating a new genus for it, "Zinjanthropus", species "boisei". "Zinj" is an ancient Arabic word for the coast of East Africa and "boisei" referred to Charles Boise, an anthropological benefactor of the Leakeys. Louis based his classification on twenty differences from "Australopithecus".
Broom had died in 1951 but Dart was still living. He is said to have wept for joy on Louis' behalf on being personally shown Zinj, which Louis and Mary carried around in a tin (later a box). Louis had considered Broom's "Paranthropus" genus, but rejected it because he believed Zinj was in the "Homo" ancestral stock but "Paranthropus" was not. He relied heavily on the larger size of Zinj's canines.
At that time palaeoanthropology was in an overall mood to lump and was preaching against splitting. Consequently, the presentation of Zinj during the Fourth Pan-African Congress of Prehistorians in July in the then Belgian Congo, at which Louis was forced to read the delayed "Nature" article, nearly came to grief for Louis over the creation of a new genus. Dart rescued him with the now famous joke, "... what would have happened if Mrs. Ples had met Dear Boy one dark night."
The battle of the name raged on for many years and drove a wedge between Louis and Sir Wilfrid LeGros Clark, from 1955, who took the "Paranthropus" view. On the other hand it brought the Leakeys and Dr. Melville Bell Grosvenor of the National Geographic Society together. The Leakeys became international figures and had no trouble finding funds from then on. The Zinj question ultimately became part of the "Australopithecus"/"Paranthropus" question (which only applied to the robust Australopithecines).
Description.
All species of "Paranthropus" were bipedal, and many lived during a time when species of the genus "Homo" (which were possibly descended from "Australopithecus"), were prevalent. "Paranthropus" first appeared roughly 2.7 million years ago. Most species of "Paranthropus" had a brain about 40 percent of the size of a modern human. There was some size variation between the different species of "Paranthropus", but most stood roughly 1.3-1.4 m (4 ft 3 in to 4 ft 7 in) tall and were quite well muscled. "Paranthropus" is thought to have lived in wooded areas rather than the grasslands of "Australopithecus".
"Paranthropus" is thought to be bipedal based on its anatomical structure in its hips, legs, and feet that resemble both its ancestor, "Australopithecus afarensis", and modern humans. The pelvis is similar to "A. afarensis" but the hip joint, including the femoral head and acetabulum are smaller in "Paranthropus". The similar hip structure between "A. afarensis" and "Paranthropus" implies that they had a similar walking gait, and that "Paranthropus" moved like the "gracile australopiths". They show anatomical similarity to modern humans in the big toe of their foot and their well developed plantar aponeurosis. The hallux metatarsal shows increased base for more internal support, and more distal articular surface which causes more connection and support to the other bones in the foot. The extra support in the big toe and extensive plantar aponeurosis shows that "Paranthropus" had hyperextension of their toes for a "toe-off" gait cycle, characteristic of modern bipedalism in humans.
The behavior of "Paranthropus" was quite different from that of the genus "Homo", in that it was not as adaptable to its environment or as resourceful. Evidence of this exists in the form of its physiology which was specifically tailored to a diet of grubs and plants. This would have made it more reliant on favorable environmental conditions than members of the genus "Homo", such as "Homo habilis", which would eat a much wider variety of foods. Therefore, because it was a specialist species, it had more difficulty adapting to a changing climate, leading to its extinction.
Disputed taxonomy.
Evolutionary biologist Richard Dawkins notes "perhaps several different species" of robust hominids, and "as usual their affinities, and the exact number of species, are hotly disputed. Names that have been attached to various of these creatures...are "Australopithecus" (or "Paranthropus") "robustus", "Australopithecus" (or "Paranthropus" or "Zinjanthropus") "boisei", and "Australopithecus" (or "Paranthropus") "aethiopicus"."
Opinions differ whether the species "P. aethiopicus, P. boisei" and "P. robustus" should be included within the genus "Australopithecus". The emergence of the robusts could be either a display of divergent or convergent evolution. There is currently no consensus in the scientific community whether "P. aethiopicus, P. boisei" and "P. robustus" should be placed into a distinct genus, "Paranthropus", which is believed to have evolved from the ancestral "Australopithecus" line. Up until the last half-decade, the majority of the scientific community included all the species of both "Australopithecus" and "Paranthropus" in a single genus. Currently, both taxonomic systems are used and accepted in the scientific community. However, although "Australopithecus robustus" and "Paranthropus robustus" are used interchangeably for the same specimens, some researchers, beginning with Robert Broom, and continuing with people such as Bernard A. Wood, think that there is a difference between "Australopithecus" and "Paranthropus", and that there should be two genera.
Occurrence.
For the most part the "Australopithecus" species "A. afarensis", "A. africanus", and "A. anamensis" either disappeared from the fossil record before the appearance of early humans or seem to have been the ancestors of "Homo habilis", yet "P. boisei" and "P. aethiopicus" continued to evolve along a separate path distinct and unrelated to early humans. "Paranthropus" shared the earth with some early examples of the "Homo" genus, such as "H. habilis", "H. ergaster", and possibly even "H. erectus".
"Australopithecus afarensis" and "A. anamensis" had, for the most part, disappeared by this time. There were also significant morphological differences between "Australopithecus" and "Paranthropus", although the differences were found on the cranial remains. The postcranial remains were still very similar. "Paranthropus" was more massively built craniodentally and tended to sport gorilla-like sagittal crests on the cranium which anchored massive temporalis muscles of mastication.
Intelligence.
Species of "Paranthropus" had smaller braincases than "Homo", yet they had significantly larger braincases than "Australopithecus". "Paranthropus" is associated with stone tools both in southern and eastern Africa, although there is considerable debate whether they were made and utilized by these robust australopithecines or contemporaneous "Homo". Most believe that early "Homo" was the tool maker, but hand fossils from Swartkrans, South Africa, indicate that the hand of "Paranthropus robustus" was also adapted for precision grasping and tool use. 
Most "Paranthropus" species seem almost certainly not to have used language nor to have controlled fire, although they are directly associated with the latter at Swartkrans.
Diet.
In 2011 Thure E. Cerling of the University of Utah and colleagues, published a study in the "Proceedings of the National Academy of Sciences" of their work with the carbon isotopes in the enamel of 24 teeth from 22 "Paranthropus" individuals who lived in East Africa between 1.4 million and 1.9 million years ago. Their results suggest that "Paranthropus boisei" dined more heavily on C4 plants than any other human ancestor or human relative studied to date.

</doc>
<doc id="23562" url="http://en.wikipedia.org/wiki?curid=23562" title="Odd-toed ungulate">
Odd-toed ungulate

An odd-toed ungulate is a mammal with rear hooves consisting of an odd number of toes. Odd-toed ungulates compose the order Perissodactyla (Greek: περισσός, "perissós", "uneven", and δάκτυλος, "dáktylos", "finger/toe"). The middle toe on each hind hoof is usually larger than its neighbours. Odd-toed ungulates are relatively large grazers and, unlike the ruminant even-toed ungulates (artiodactyls), they have relatively simple stomachs because they are hindgut fermenters, digesting plant cellulose in their intestines rather than in one or more stomach chambers. Odd-toed ungulates include the horse, tapirs, and rhinoceroses.
Evolution.
Although no unequivocal fossils are known prior to the early Eocene, the odd-toed ungulates probably arose in what is now Asia, during the Paleocene-Eocene boundary (55 million years ago).
By the start of the Eocene, 55 million years ago (Mya), they had diversified and spread out to occupy several continents. Horses and tapirs both evolved in North America; rhinoceroses appear to have developed in Asia from tapir-like animals and then colonised the Americas during the middle Eocene (about 45 Mya). Of the approximately 15 families, only three survive. These families were very diverse in form and size; they included the enormous brontotheres and the bizarre chalicotheres. The largest perissodactyl, an Asian rhinoceros called "Paraceratherium", reached 15 tonne, more than twice the weight of an elephant.
Perissodactyls were the dominant group of large terrestrial browsers right through the Oligocene. However, the rise of grasses in the Miocene (about 20 Mya) saw a major change: the even-toed ungulates, which had more complex stomachs, were better able to adapt to coarse, low-nutrition diets, and soon rose to prominence. Nevertheless, many odd-toed species survived and prospered until the late Pleistocene (about 10,000 years ago), when they faced the pressure of human hunting and habitat change.
Taxonomy.
The members of the order fall into two suborders:
The three surviving families of odd-toed ungulate are classified as follows:
Based on morphology, odd-toed ungulates were long thought to form a clade with even-toed ungulates. However, recent phylogenetic studies have lacked full confidence in this conclusion; some studies link Perissodactyla with Ferae into a proposed clade Zooamata, while the Pegasoferae proposal goes further, suggesting Chiroptera (bats) are more closely related to odd-toed ungulates than even-toed ones. The most recent study, by Zhou et al. (2011), finds better (but not full) support for the traditional view, uniting Perissodactyla with Cetartiodactyla into a clade of "true ungulates", Euungulata.
Evolutionary relationships of fossil families.
Below is a simplified taxonomy showing the relationships of extant and extinct families after Lucas & Schoch, 1989, Mader, 1989, Prothero & Schoch, 1989, McKenna & Bell, 1997, Gunnell & Yarborough, 2000, Holroyd & Ciochon, 2000, Zonneveld, Gunnell & Bartels, 2000, Thewissen, Williams & Hussain, 2001, and Mihlbachler, Lucas, Emry & Bayshashov, 2004 and Hooker & Dashzeveg 2004, and Mihlbachler, 2005 
Characteristics.
The living perissodactyls are a diverse group. At one extreme, are the lithe and graceful horses; at the other, the huge, tank-like rhinoceroses; and in the middle, the vaguely pig-like tapirs. All extant perissodactyls are large, from the 110 kg "Tapirus kabomani" to the 2,300 kg white rhinoceros.
Extinct perissodactyls possessed a far more diverse range of forms, including the tiny, vaguely tapir-like paleotheres, the monstrous brontotheres, the knuckle-walking chalicotheres, and the gigantic rhinoceros "Indricotherium", which dwarfed even elephants.
However, all perissodactyls, extinct and extant, have a mesaxonic foot structure: the symmetry of the foot passes through the third digit, which holds the animal's weight. In equines, the mesaxonic foot has been modified so that the non-weight bearing digits have atrophied away, while the third toe has enlarged, so modern equines only have one toe. Unusually, in tapirs, the forefeet have four toes, lacking only the first digit, although the foot is at least partially mesaxonic, because the fifth digit is reduced and bears relatively little of the animal's weight.
Also, all perissodactyls are hindgut fermenters. Hindgut fermenters, in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum, where the food is digested by bacteria. No gallbladder is present.
Social structures.
Today, the equines are the only extant social perissodactyls. Horses organize themselves into small bands with a dominant mare at the top of the pecking order, as well as a resident stallion. Several bands will share a common territory, with some members of one band joining another band every so often. These bands, in turn, form a herd.
Huge fossil beds made of the bones of hundreds or thousands of individuals suggest that many of the larger brontothere species were social animals at least some of the time. Some prehistoric rhinoceroses, such as "Diceratherium", were also social animals that organized themselves into herds. However, modern-day rhinoceroses are solitary animals that maintain territories, often attacking members of their own species when their space has been invaded. Tapirs, too, are solitary animals, though they are shy, retiring creatures that do not defend or maintain territories.
Mating and reproduction.
As with the males of many other animal groups, male perissodactyls often spar with each other for the privilege to mate with receptive females. A male that has found a female will attempt to taste her urine to see if she is in estrus. The female may also signal that she is in estrus, such as the whistling used by cow Indian rhinoceroses and tapirs.
Perissodactyls tend to have one foal or calf at a time. Very rarely, the female may have twins. Gestation is very long, from about 11 months in horses to 16 months for rhinoceroses. The calf or foal is capable of standing within moments of birth, but is very dependent on its mother. The young stays with its mother even after being weaned, usually until it is chased off by the mother on the birth of a new foal or calf. At this time, in horses, the foal will enter into the herd proper; later, young stallions are often chased off and join bachelor herds. With rhinos and tapirs, the newly weaned calf wanders away to search for new feeding grounds.
Humans and conservation.
Domestication.
Humans have a historically long interaction with perissodactyls. The wild ass was the first equid to be domesticated, around 5000 BC in Egypt. Horses were domesticated a 1,000 years later. The zebroid, that is a zebra hybrid, began appearing in zoos and menageries during the 19th century. During the 16th century, the Spaniards brought horses with them to the Americas and inadvertently reintroduced them into North America. While no rhinoceros has been domesticated, they have been captured for zoos and menageries since ancient times.
Conservation.
The odd-toed ungulates have been among the most important herbivorous mammals; at times, they have been the dominant herbivores in many ecosystems. However, over the course of millions of years, many species became extinct due to climatic change, newer, coarser-leaved plants, predators, disease, and competition from other herbivores, particularly the artiodactyls. The Chalicotheriidae were the most recent family of perissodactyls to become entirely extinct. The perissodactyls' decline continues even today. Most are listed as threatened species; although no species are confirmed to be extinct, some subspecies have become extinct. The quagga was hunted for its meat, the tarpan was hunted for sport, and a subspecies of black rhinoceros was hunted for its horn (as with all other African rhinoceros species).
Perissodactyls tend to do well in captivity, and many breeding programs are in place to help replenish wild populations. Przewalski's horse has recently been released back to the wild. The captive-breeding programs for some equids are unusual in that breeders have been carefully selecting specimens to recreate various recently extinct equids, such as the tarpan and quagga. Most wild rhinoceroses are monitored, and some have their horns trimmed off to discourage horn-poachers.
References.
</dl>

</doc>
<doc id="23565" url="http://en.wikipedia.org/wiki?curid=23565" title="Pai gow">
Pai gow

Pai gow (; pinyin: pái jiǔ; jyutping: paai4 gau2) is a Chinese gambling game, played with a set of 32 Chinese dominoes. It is played openly in major casinos in China (including Macau); the United States (including Las Vegas, Nevada; Reno, Nevada; Connecticut; Atlantic City, New Jersey; Pennsylvania; and cardrooms in California); Canada (including Edmonton, Alberta and Calgary, Alberta); Australia; and, New Zealand. It dates back to at least the Song Dynasty.
The name "pai gow" is sometimes used to refer to a card game called pai gow poker (or “double-hand poker”), which is loosely based on pai gow. Pai gow loosely translates to "make nine" and was the basis for Baccarat (card game).
There are 35,960 four-tile combinations possible.
Rules.
Starting.
Tiles are randomized on the table and are stacked into eight stacks of four tiles each in an assembly known as the "woodpile". Various ritualistic "shuffles" are made, rearranging the tiles in the woodpile in standard ways that result in a new woodpile. Bets are then made.
Next, each player (including the dealer) are given four tiles with which to make two hands of two tiles each. The hand with the lower value is called the "front hand", and the hand with the higher value is called the "rear hand". If a player's front hand beats the dealer's front hand, and the player's rear hand beats the dealer's rear hand, then that player wins the bet. If a player's front and rear hands both lose to the dealer's respective hands, the player loses the bet. If one hand wins and the other loses, the player is said to "push", and gets back only the money he or she bet. Generally seven players will play, and each player's hands are compared only against the dealer's hands.
Basic scoring.
The name "pai gow" is loosely translated as "make nine" or "card nine". This reflects the fact that, with a few high-scoring exceptions, the best a hand can score is nine. To find the value of a hand, simply add the total number of pips on the two tiles, and drop the tens place.
So for instance, a 1-3 tile (a tile with one pip on one end and three pips on the other, for a total of four pips) used with a 2-3 tile (with five total pips) will score nine, since four plus five is nine. A 2-3 tile with a 5-6 tile will score six, and not sixteen, as the "1" in "16" is dropped. And a 5-5 tile with a 4-6 tile will score zero, since ten plus ten is twenty, and twenty reduces to zero when the tens place is dropped.
Gongs and Wongs.
There are special ways in which a hand can score more than nine points. The double-one tiles and double-six tiles are known as the "Day" and "Teen" tiles, respectively. If a Day or Teen tile is used with an eight, the pair is worth ten instead of the usual zero. (This is called a "Gong".) If a Day or Teen tile is used with a nine, the hand is worth eleven instead of one. (This is called a "Wong".) But a Day or Teen tile used with a ten is only worth two, not twelve; this is because only eights and nines can be combined with Days or Teens for higher values. (In other words, when Day or Teen tiles are combined with tiles other than an eight or nine, follow the normal scoring rules.)
Gee Joon tiles.
The 1-2 and the 2-4 tiles are called "Gee Joon" tiles (or sometimes called wildcards). Either tile can count as 3 or 6, whichever scores more. So a 1-2 tile can be used with a 5-6 tile to make a hand worth seven points, rather than four.
Pairs.
The 32 tiles in a Chinese dominoes set can be arranged into 16 pairs, as shown in the picture at the top of this article. Eleven of these pairs have identical tiles, and five of these pairs are made up of two tiles that score the same, but look different. (The latter group includes the Gee Joon tiles, which can score the same, whether as three or six.) If a hand is made up of a pair, it always scores higher than a non-pair, no matter what the value of the pips are. (Pairs are often thought of as being worth 12 points each.)
When two pairs are compared, the higher-valued pair wins. This is not determined by the sum of their pips, but by aesthetics. It must be memorized which pairs score more than other pairs. The highest pairs are the Gee Joon tiles, the Teens, the Days, and the red eights. The lowest scoring pairs are the mismatched nines, eights, sevens, and fives. But even the lowest-scoring pair will beat any non-pair.
Ties.
When one of a player's hands is compared to one of the banker's hands, it sometimes happens that both will have the same score. For instance, a player may have a front hand worth one point, consisting of a 3-4 tile and a 2-2 tile, and the banker may have a front hand also worth one point, made up of a 5-6 tile and a 5-5 tile. In these cases, determine which tile in each hand has a higher value, as determined by the pair rankings mentioned above. In this case, the 2-2 tile is in a higher-ranking pair than the 3-4 tile, and the 5-5 tile is in a higher-ranking pair than the 5-6 tile. (Again, the rankings of the pairs follows no obvious pattern and must be memorized.) Since the 5-5 pair outranks the 2-2 pair, the banker would win this front hand.
In the event of a tied point-value where the highest tile in both the player and the banker's hand is identical, the hand is called a copy, and the win goes to the banker. For example, a player may have a front hand worth 8 points, with a 4-4 tile and a 5-5 tile, and the banker may also have a front hand worth 8 points, with a 4-4 tile and a 6-4 tile. Since only the best tile in the hand is used to evaluate ties, the red eight 4-4 tile is used, and the hand is a copy. Ties are never broken by comparing the second-best tile. Similarly, the banker would win a tied hand where the banker has a 3-6 tile with a 3-4 tile, for a total of 6, against the player's 5-4 tile with a 5-2 tile, also totaling 6. Here all four tiles in both hands are identical, and the copy goes to the banker. 
There are two exceptions to the method described above. First, although the Gee Joon tiles form the highest-ranking pair, they are considered to have no value when evaluating ties. Second, any zero-zero tie is won by the Banker, regardless of the tiles in the hand.
Strategy.
The key element of pai gow strategy is to present the optimal front hand and rear hand given four tiles dealt to the player. There are three ways to arrange four tiles into two hands, though practically some combinations may be the same.
For instance, consider the four tiles at right. If tile A were made into a hand with tile B both resulting hands would score zero. However if tile A were paired with tile C, both hands would score 5. Or if tile A were paired with tile D, the front hand would score 3 and the rear hand would score 7. The player must decide which front hand-back hand combination is most likely to beat both of the dealers' hands, or at least to break a tie in the player's favor. In some cases a player with weaker tiles may deliberately attempt to attain a push so as to avoid losing the bet outright. Many players rely on superstition or tradition to choose tile pairings.

</doc>
<doc id="23572" url="http://en.wikipedia.org/wiki?curid=23572" title="Partially ordered set">
Partially ordered set

In mathematics, especially order theory, a partially ordered set (or poset) formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a set. A poset consists of a set together with a binary relation that indicates that, for certain pairs of elements in the set, one of the elements precedes the other. Such a relation is called a "partial order" to reflect the fact that not every pair of elements need be related: for some pairs, it may be that neither element precedes the other in the poset.
Thus, partial orders generalize the more familiar total orders, in which every pair is related. A finite poset can be visualized through its Hasse diagram, which depicts the ordering relation.
A familiar real-life example of a partially ordered set is a collection of people ordered by genealogical descendancy. Some pairs of people bear the descendant-ancestor relationship, but other pairs bear no such relationship.
Formal definition.
A (non-strict) partial order is a binary relation "≤" over a set "P" which is reflexive, antisymmetric, and transitive, i.e., which satisfies for all "a", "b", and "c" in "P":
In other words, a partial order is an antisymmetric preorder.
A set with a partial order is called a partially ordered set (also called a poset). The term "ordered set" is sometimes also used, as long as it is clear from the context that no other kind of order is meant. In particular, totally ordered sets can also be referred to as "ordered sets", especially in areas where these structures are more common than posets.
For "a, b", elements of a partially ordered set "P", if "a ≤ b" or "b ≤ a", then "a" and "b" are comparable. Otherwise they are incomparable. In the figure on top-right, e.g. {x} and {x,y,z} are comparable, while {x} and {y} are not. A partial order under which every pair of elements is comparable is called a total order or linear order; a totally ordered set is also called a chain (e.g., the natural numbers with their standard order). A subset of a poset in which no two distinct elements are comparable is called an antichain (e.g. the set of singletons in the top-right figure). An element "a" is said to be covered by another element "b", written "a"<:"b", if "a" is strictly less than "b" and no third element "c" fits between them; formally: if both "a"≤"b" and "a"≠"b" are true, and "a"≤"c"≤"b" is false for each "c" with "a"≠"c"≠"b". A more concise definition will be given below using the strict order corresponding to "≤". For example, {x} is covered by {x,z} in the top-right figure, but not by {x,y,z}.
Examples.
Standard examples of posets arising in mathematics include:
Extrema.
There are several notions of "greatest" and "least" element in a poset "P", notably:
For example, consider the positive integers, ordered by divisibility: 1 is a least element, as it divides all other elements; on the other hand this poset does not have a greatest element (although if one would include 0 in the poset, which is a multiple of any integer, that would be a greatest element; see figure). This partially ordered set does not even have any maximal elements, since any "g" divides for instance 2"g", which is distinct from it, so "g" is not maximal. If the number 1 is excluded, while keeping divisibility as ordering on the elements greater than 1, then the resulting poset does not have a least element, but any prime number is a minimal element for it. In this poset, 60 is an upper bound (though not a least upper bound) of the subset {2,3,5,10}, which does not have any lower bound (since 1 is not in the poset); on the other hand 2 is a lower bound of the subset of powers of 2, which does not have any upper bound.
Orders on the Cartesian product of partially ordered sets.
In order of increasing strength, i.e., decreasing sets of pairs, three of the possible partial orders on the Cartesian product of two partially ordered sets are (see figures):
All three can similarly be defined for the Cartesian product of more than two sets.
Applied to ordered vector spaces over the same field, the result is in each case also an ordered vector space.
See also orders on the Cartesian product of totally ordered sets.
Sums of partially ordered sets.
Another way to combine two posets is the ordinal sum<Ref></ref> (or linear sum), "Z" = "X" ⊕ "Y", defined on the union of the underlying sets "X" and "Y" by the order "a" ≤"Z" "b" if and only if:
If two posets are well-ordered, then so is their ordinal sum.
Strict and non-strict partial orders.
In some contexts, the partial order defined above is called a non-strict (or reflexive, or weak) partial order. In these contexts a strict (or irreflexive) partial order "<" is a binary relation that is irreflexive, transitive and asymmetric, i.e. which satisfies for all "a", "b", and "c" in "P":
There is a 1-to-1 correspondence between all non-strict and strict partial orders.
If "≤" is a non-strict partial order, then the corresponding strict partial order "<" is the irreflexive kernel given by:
Conversely, if "<" is a strict partial order, then the corresponding non-strict partial order "≤" is the reflexive closure given by:
This is the reason for using the notation "≤".
Using the strict order "<", the relation ""a" is covered by "b" can be equivalently rephrased as "a"<"b", but not "a"<"c"<"b" for any "c"".
Strict partial orders are useful because they correspond more directly to directed acyclic graphs (dags): every strict partial order is a dag, and the transitive closure of a dag is both a strict partial order and also a dag itself.
Inverse and order dual.
The inverse or converse ≥ of a partial order relation ≤ satisfies "x"≥"y" if and only if "y"≤"x". The inverse of a partial order relation is reflexive, transitive, and antisymmetric, and hence itself a partial order relation. The order dual of a partially ordered set is the same set with the partial order relation replaced by its inverse. The irreflexive relation > is to ≥ as < is to ≤.
Any one of the four relations ≤, <, ≥, and > on a given set uniquely determines the other three.
In general two elements "x" and "y" of a partial order may stand in any of four mutually exclusive relationships to each other: either "x" < "y", or "x" = "y", or "x" > "y", or "x" and "y" are "incomparable" (none of the other three). A totally ordered set is one that rules out this fourth possibility: all pairs of elements are comparable and we then say that trichotomy holds. The natural numbers, the integers, the rationals, and the reals are all totally ordered by their algebraic (signed) magnitude whereas the complex numbers are not. This is not to say that the complex numbers cannot be totally ordered; we could for example order them lexicographically via "x"+iy" < "u"+iv" if and only if "x" < "u" or ("x" = "u" and "y" < "v"), but this is not ordering by magnitude in any reasonable sense as it makes 1 greater than 100i. Ordering them by absolute magnitude yields a preorder in which all pairs are comparable, but this is not a partial order since 1 and i have the same absolute magnitude but are not equal, violating antisymmetry.
Mappings between partially ordered sets.
Given two partially ordered sets ("S",≤) and ("T",≤), a function "f": "S" → "T" is called order-preserving, or monotone, or isotone, if for all "x" and "y" in "S", "x"≤"y" implies "f"("x") ≤ "f"("y").
If ("U",≤) is also a partially ordered set, and both "f": "S" → "T" and "g": "T" → "U" are order-preserving, their composition ("g"∘"f"): "S" → "U" is order-preserving, too.
A function "f": "S" → "T" is called order-reflecting if for all "x" and "y" in "S", "f"("x") ≤ "f"("y") implies "x"≤"y".
If "f" is both order-preserving and order-reflecting, then it is called an order-embedding of ("S",≤) into ("T",≤).
In the latter case, "f" is necessarily injective, since "f"("x") = "f"("y") implies "x" ≤ "y" and "y" ≤ "x". If an order-embedding between two posets "S" and "T" exists, one says that "S" can be embedded into "T". If an order-embedding "f": "S" → "T" is bijective, it is called an order isomorphism, and the partial orders ("S",≤) and ("T",≤) are said to be isomorphic. Isomorphic orders have structurally similar Hasse diagrams (cf. right picture). It can be shown that if order-preserving maps "f": "S" → "T" and "g": "T" → "S" exist such that "g"∘"f" and "f"∘"g" yields the identity function on "S" and "T", respectively, then "S" and "T" are order-isomorphic.
For example, a mapping "f": ℕ → ℙ(ℕ) from the set of natural numbers (ordered by divisibility) to the power set of natural numbers (ordered by set inclusion) can be defined by taking each number to the set of its prime divisors. It is order-preserving: if "x" divides "y", then each prime divisor of "x" is also a prime divisor of "y". However, it is neither injective (since it maps both 12 and 6 to {2,3}) nor order-reflecting (since besides 12 doesn't divide 6). Taking instead each number to the set of its prime power divisors defines a map "g": ℕ → ℙ(ℕ) that is order-preserving, order-reflecting, and hence an order-embedding. It is not an order-isomorphism (since it e.g. doesn't map any number to the set {4}), but it can be made one by restricting its codomain to "g"(ℕ). The right picture shows a subset of ℕ and its isomorphic image under "g". The construction of such an order-isomorphism into a power set can be generalized to a wide class of partial orders, called distributive lattices, see "Birkhoff's representation theorem".
Number of partial orders.
Sequence [ A001035] in OEIS gives the number of partial orders on a set of "n" labeled elements:
The number of strict partial orders is the same as that of partial orders.
If we count only up to isomorphism, we get 1, 1, 2, 5, 16, 63, 318, … (sequence in OEIS).
Linear extension.
A partial order ≤* on a set "X" is an extension of another partial order ≤ on "X" provided that for all elements "x" and "y" of "X", whenever formula_1, it is also the case that "x" ≤* "y". A linear extension is an extension that is also a linear (i.e., total) order. Every partial order can be extended to a total order (order-extension principle).
In computer science, algorithms for finding linear extensions of partial orders (represented as the reachability orders of directed acyclic graphs) are called topological sorting.
In category theory.
Every poset (and every preorder) may be considered as a category in which every hom-set has at most one element. More explicitly, let hom("x", "y") = {("x", "y")} if "x" ≤ "y" (and otherwise the empty set) and ("y", "z")∘("x", "y") = ("x", "z"). Posets are equivalent to one another if and only if they are isomorphic. In a poset, the smallest element, if it exists, is an initial object, and the largest element, if it exists, is a terminal object. Also, every preordered set is equivalent to a poset. Finally, every subcategory of a poset is isomorphism-closed.
Partial orders in topological spaces.
If "P" is a partially ordered set that has also been given the structure of a topological space, then it is customary to assume that {("a", "b") : "a" ≤ "b"} is a closed subset of the topological product space formula_2. Under this assumption partial order relations are well behaved at limits in the sense that if formula_3, formula_4 and "a""i" ≤ "b""i" for all "i", then "a" ≤ "b".
Interval.
For "a" ≤ "b", the closed interval ["a","b"] is the set of elements "x" satisfying "a" ≤ "x" ≤ "b" (i.e. "a" ≤ "x" and "x" ≤ "b"). It contains at least the elements "a" and "b".
Using the corresponding strict relation "<", the open interval ("a","b") is the set of elements "x" satisfying "a" < "x" < "b" (i.e. "a" < "x" and "x" < "b"). An open interval may be empty even if "a" < "b". For example, the open interval (1,2) on the integers is empty since there are no integers "i" such that 1 < "i" < 2.
Sometimes the definitions are extended to allow "a" > "b", in which case the interval is empty.
The "half-open intervals" ["a","b") and ("a","b"] are defined similarly.
A poset is locally finite if every interval is finite. For example, the integers are locally finite under their natural ordering. The lexicographical order on the cartesian product ℕ×ℕ is not locally finite, since e.g. (1,2)≤(1,3)≤(1,4)≤(1,5)≤...≤(2,1).
Using the interval notation, the property ""a" is covered by "b"" can be rephrased equivalently as ["a","b"] = {"a","b"}.
This concept of an interval in a partial order should not be confused with the particular class of partial orders known as the interval orders.

</doc>
<doc id="23574" url="http://en.wikipedia.org/wiki?curid=23574" title="Psyche">
Psyche

Psyche (Psyché in French) is the Greek term for "soul" (ψυχή).
It may also refer to:

</doc>
<doc id="23575" url="http://en.wikipedia.org/wiki?curid=23575" title="Parmenides">
Parmenides

Parmenides of Elea (; Greek: Παρμενίδης ὁ Ἐλεάτης; fl. late sixth or early fifth century BCE) was a pre-Socratic Greek philosopher from Elea in Magna Graecia (Greater Greece, included Southern Italy). He was the founder of the Eleatic school of philosophy. The single known work of Parmenides is a poem, "On Nature", which has survived only in fragmentary form. In this poem, Parmenides describes two views of reality. In "the way of truth" (a part of the poem), he explains how reality (coined as "what-is") is one, change is impossible, and existence is timeless, uniform, necessary, and unchanging. In "the way of opinion," he explains the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. These ideas had a strong effect on Plato, and in turn, influenced the whole of Western philosophy.
Early life.
Parmenides was born in the Greek colony of Elea (now Ascea), which, according to Herodotus, had been founded shortly before 535 BCE. He was descended from a wealthy and illustrious family. 
His dates are uncertain; according to Diogenes Laërtius, he flourished just before 500 BCE, which would put his year of birth near 540 BCE, but Plato has him visiting Athens at the age of 65, when Socrates was a young man, c. 450 BCE, which, if true, suggests a year of birth of c. 515 BCE. He was said to have been a pupil of Xenophanes, and regardless of whether they actually knew each other, Xenophanes' philosophy is the most obvious influence on Parmenides. Diogenes Laërtius also describes Parmenides as a disciple of "Ameinias, son of Diochaites, the Pythagorean"; but there are no obvious Pythagorean elements in his thought. 
Career.
The first hero cult of a philosopher we know of was Parmenides' dedication of a heroon to his teacher Ameinias in Elea. Parmenides was the founder of the School of Elea, which also included Zeno of Elea and Melissus of Samos. Of his life in Elea, it was said that he had written the laws of the city. His most important pupil was Zeno, who according to Plato was 25 years his junior, and was regarded as his "eromenos". Parmenides had a large influence on Plato, who not only named a dialogue, "Parmenides", after him, but always spoke of him with veneration.
"On Nature".
Parmenides is one of the most significant of the pre-Socratic philosophers. His only known work, conventionally titled "On Nature", is a poem which has only survived in fragmentary form. Approximately 160 lines of the poem remain today; reportedly the original text had 3,000 lines. It is known, however, that the work was originally divided into three parts:
The proem is a narrative sequence in which the narrator travels "beyond the beaten paths of mortal men" to receive a revelation from an unnamed goddess (generally thought to be Persephone or Dike) on the nature of reality. "Aletheia", an estimated 90% of which has survived, and "doxa", most of which no longer exists, are then presented as the spoken revelation of the goddess without any accompanying narrative.
Parmenides attempted to distinguish between the unity of nature and its variety, insisting in the "Way of Truth" upon the reality of its unity, which is therefore the object of knowledge, and upon the unreality of its variety, which is therefore the object, not of knowledge, but of opinion. In the "Way of Opinion" he propounded a theory of the world of seeming and its development, pointing out, however, that, in accordance with the principles already laid down, these cosmological speculations do not pretend to anything more than mere appearance.
Proem.
In the proem, Parmenides describes the journey of a young man from light to the "halls of Night" ("the daughters of the Sun made haste to escort me, having left the halls of Night for the light"). Carried in a whirling chariot, and attended by the daughters of Helios the Sun, the man reaches a temple sacred to an unnamed goddess (variously identified by the commentators as Nature, Wisdom, or Themis), by whom the rest of the poem is spoken. He must learn all things, she tells him – both truth, which is certain, and human opinions, which are uncertain – for though one cannot rely on human opinions, they represent an aspect of the whole truth.
The Way of Truth.
The section known as "the way of truth" discusses that which is real and contrasts with the argument in the section called "the way of opinion," which discusses that which is illusory. Under the "way of truth," Parmenides stated that there are two ways of inquiry: that it "is", on the one side, and that it "is not". on the other side. He said that the latter argument is never feasible because there is no thing that can "not be":
There are extremely delicate issues here. In the original Greek the two ways are simply named "that Is" (ὅπως ἐστίν) and "that Not-Is" (ὡς οὐκ ἐστίν) (B 2.3 and 2.5) without the "it" inserted in our English translation. In ancient Greek, which, like many languages in the world, does not always require the presence of a subject for a verb, "is" functions as a grammatically complete sentence. Much debate has been focused on where and what the subject is. The simplest explanation as to why there is no subject here is that Parmenides wishes to express the simple, bare fact of existence in his mystical experience without the ordinary distinctions, just as the Latin "pluit" and the Greek "huei" (ὕει "rains") mean "it rains"; there is no subject for these impersonal verbs because they express the simple fact of raining without specifying what is doing the raining. This is, for instance, Hermann Fränkel's thesis. Many scholars still reject this explanation and have produced more complex metaphysical explanations. Since existence is an immediately intuited fact, non-existence is the wrong path because a thing cannot disappear, just as something cannot originate from nothing. In such mystical experience ("unio mystica"), however, the distinction between subject and object disappears along with the distinctions between objects, in addition to the fact that if nothing cannot be, it cannot be the object of thought either:
Thus, he concluded that "Is" could not have "come into being" because "nothing comes from nothing". Existence is necessarily eternal. That which truly is [x], has always been [x], and was never becoming [x]; that which is becoming [x] was never nothing (Not-[x]), but will never actually be. Parmenides was not struggling to formulate the laws of conservation of mass and conservation of energy; he was struggling with the metaphysics of change, which is still a relevant philosophical topic today.
Moreover he argued that movement was impossible because it requires moving into "the void", and Parmenides identified "the void" with nothing, and therefore (by definition) it does not exist. That which does exist is "The Parmenidean One", which is timeless, uniform, and unchanging:
Perception vs. Logos.
Parmenides claimed that there is no truth in the opinions of the mortals. Genesis-and-destruction, as Parmenides emphasizes, is a false opinion, because to be means to be completely, once and for all. What exists can in no way not exist. 
The Way of Opinion ("doxa").
After the exposition of the "arche" (ἀρχή), i.e. the origin, the necessary part of reality that is understood through reason or logos ("that [it] Is"), in the next section, "the Way of Appearance/Opinion/Seeming", Parmenides proceeds to explain the structure of the becoming cosmos (which is an illusion, of course) that comes from this origin.
The structure of the cosmos is a fundamental binary principle that governs the manifestations of all the particulars: "the aether fire of flame" (B 8.56), which is gentle, mild, soft, thin and clear, and self-identical, and the other is "ignorant night", body thick and heavy.
The structure of the cosmos then generated is recollected by Aetius (II, 7, 1):
For Parmenides says that there are circular bands wound round one upon the other, one made of the rare, the other of the dense; and others between these mixed of light and darkness. What surrounds them all is solid like a wall. Beneath it is a fiery band, and what is in the very middle of them all is solid, around which again is a fiery band. The most central of the mixed bands is for them all the origin and cause of motion and becoming, which he also calls steering goddess and keyholder and Justice and Necessity. The air has been separated off from the earth, vapourized by its more violent condensation, and the sun and the circle of the Milky Way are exhalations of fire. The moon is a mixture of both earth and fire. The "aether" lies around above all else, and beneath it is ranged that fiery part which we call heaven, beneath which are the regions around the earth.
Interpretations of Parmenides.
The traditional interpretation of Parmenides' work is that he argued that the every-day perception of reality of the physical world (as described in "doxa") is mistaken, and that the reality of the world is 'One Being' (as described in "aletheia"): an unchanging, ungenerated, indestructible whole. Under the "Way of Opinion", Parmenides set out a contrasting but more conventional view of the world, thereby becoming an early exponent of the duality of appearance and reality. For him and his pupils, the phenomena of movement and change are simply appearances of a changeless, eternal reality. This interpretation could settle because of various wrong translations of the fragments. For example, it is not at all clear that Parmenides refuted that which we call perception. The verb "noein", used frequently by Parmenides, could better be translated as 'to be aware of' than as 'to think'. Furthermore, it is hard to believe that 'being' is only within our heads, according to Parmenides.
Parmenides' philosophy is presented in the form of poetry. The philosophy he argued was, he says, given to him by a goddess, though the "mythological" details in Parmenides' poem do not bear any close correspondence to anything known from traditional Greek mythology:
It is with respect to this religious/mystical context that recent generations of scholars such as Alexander P. Mourelatos, Charles H. Kahn, and the controversial Peter Kingsley have begun to call parts of the traditional, rational logical/philosophical interpretation of Parmenides into question (Kingsley in particular stating that Parmenides practiced iatromancy). It has been claimed that previous scholars placed too little emphasis on the apocalyptic context in which Parmenides frames his revelation. As a result, traditional interpretations have put Parmenidean philosophy into a more modern, metaphysical context to which it is not necessarily well suited, which has led to misunderstanding of the true meaning and intention of Parmenides' message. The obscurity and fragmentary state of the text, however, renders almost every claim that can be made about Parmenides extremely contentious, and the traditional interpretation has by no means been abandoned.
Parmenides' considerable influence on the thinking of Plato is undeniable, and in this respect Parmenides has influenced the whole history of Western philosophy, and is often seen as its grandfather. Even Plato himself, in the "Sophist", refers to the work of "our Father Parmenides" as something to be taken very seriously and treated with respect. In the "Parmenides", the Eleatic philosopher, which may well be Parmenides himself, and Socrates argue about dialectic. In the "Theaetetus", Socrates says that Parmenides alone among the wise (Protagoras, Heraclitus, Empedocles, Epicharmus, and Homer) denied that everything is change and motion.
Parmenides is credited with a great deal of influence as the author of an "Eleatic challenge" that determined the course of subsequent philosophers' enquiries. For example, the ideas of Empedocles, Anaxagoras, Leucippus, and Democritus have been seen as in response to Parmenides' arguments and conclusions.
Parmenides' influence on philosophy reaches up till present times. The Italian philosopher Emanuele Severino has founded his extended philosophical investigations on the words of Parmenides. His philosophy is sometimes called Neo Parmenideism, and can be understood as an attempt to build a bridge between the poem on truth and the poem on opinion.
Influence on the development of science.
Parmenides made the ontological argument against nothingness, "essentially denying the possible existence of a void". According to Aristotle, this led Democritus and Leucippus, and many other physicists, to propose the atomic theory, which supposes that everything in the universe is either atoms or voids, specifically to contradict Parmenides' argument. Aristotle himself reasoned, in opposition to atomism, that in a complete vacuum, motion would encounter no resistance, and "no one could say why a thing once set in motion should stop anywhere; for why should it stop here rather than here? So that a thing will either be at rest or must be moved ad infinitum, unless something more powerful get in its way." See also "horror vacui."
Erwin Schrödinger identified Parmenides' monad of the "Way of Truth" as being the conscious self in "Nature and the Greeks". The scientific implications of this view have been discussed by scientist Anthony Hyman.
A shadow of Parmenides' ideas can be seen in the physical concept of Block time, which considers existence to consist of past, present, and future, and the flow of time to be illusory. In his critique of this idea, Karl Popper called Einstein "Parmenides". However, Popper did write:
So what was really new in Parmenides was his axiomatic-deductive method, which Leucippus and Democritus turned into a hypothetical-deductive method, and thus made part of scientific methodology.
References and further reading.
Extensive bibliography (up to 2004) by ; and annotated bibliography by 

</doc>
<doc id="23576" url="http://en.wikipedia.org/wiki?curid=23576" title="Tetraodontidae">
Tetraodontidae

Tetraodontidae is a family of primarily marine and estuarine fish of the order Tetraodontiformes. The family includes many familiar species, which are variously called pufferfish, puffers, balloonfish, blowfish, bubblefish, globefish, swellfish, toadfish, toadies, honey toads, sugar toads, and sea squab. They are morphologically similar to the closely related porcupinefish, which have large external spines (unlike the thinner, hidden spines of Tetraodontidae, which are only visible when the fish has puffed up). The scientific name refers to the four large teeth, fused into an upper and lower plate, which are used for crushing the shells of crustaceans and mollusks, their natural prey.
Pufferfish are generally believed to be the second-most poisonous vertebrates in the world, after the golden poison frog. Certain internal organs, such as liver, and sometimes the skin, contain tetrodotoxin and are highly toxic to most animals when eaten; nevertheless, the meat of some species is considered a delicacy in Japan (as 河豚, pronounced as "fugu"), Korea (as 복 "bok" or 복어 " bogeo "), and China (as 河豚 "hétún") when prepared by specially trained chefs who know which part is safe to eat and in what quantity. 
Genera.
The Tetraodontidae contain at least 120 species of puffers in 19 genera. 
Description.
They are typically small to medium in size, although a few species can reach lengths of greater than 100 cm.
Distribution.
They are most diverse in the tropics, relatively uncommon in the temperate zone, and completely absent from cold waters. 
Ecology and life history.
Although most species live in inshore and estuarine waters, 29 species spend their entire lifecycles in fresh water. These species are found in disjunct tropical regions of South America (one species), Central Africa (three species) and Southeast Asia (25 species).
Natural defenses.
The puffer's unique and distinctive natural defenses help compensate for its slow locomotion. It moves by combining pectoral, dorsal, anal, and caudal fins. This makes it highly maneuverable, but very slow, and therefore a comparatively easy predation target. Its tail fin is mainly used as a rudder, but it can be used for a sudden evasive burst of speed that shows none of the care and precision of its usual movements. The puffer's excellent eyesight, combined with this speed burst, is the first and most important defense against predators.
Its backup defense mechanism, used if successfully pursued, is to fill its extremely elastic stomach with water (or air when outside the water) until it is much larger and almost spherical in shape. Even if they are not visible when the puffer is not inflated, all puffers have pointed spines, so a hungry predator may suddenly find itself facing an unpalatable, pointy ball rather than a slow, tasty fish. Predators which do not heed this warning (or which are "lucky" enough to catch the puffer suddenly, before or during inflation) may die from choking, and predators that do manage to swallow the puffer may find their stomachs full of tetrodotoxin, making puffers an unpleasant, possibly lethal, choice of prey. This neurotoxin is found primarily in the ovaries and liver, although smaller amounts exist in the intestines and skin, as well as trace amounts in muscle. It does not always have a lethal effect on large predators, such as sharks, but it can kill humans.
Not all puffers are necessarily poisonous; "Takifugu oblongus", for example, is a "fugu" puffer that is not poisonous, and toxin level varies wildly even in fish that are. A puffer's neurotoxin is not necessarily as toxic to other animals as it is to humans, and puffers are eaten routinely by some species of fish, such as lizardfish and tiger sharks. Also, Japanese fish farmers have grown nonpoisonous puffers by controlling their diets.
Puffers are able to move their eyes independently, and many species can change the color or intensity of their patterns in response to environmental changes. In these respects, they are somewhat similar to the terrestrial chameleon. Although most puffers are drab, many have bright colors and distinctive markings, and make no attempt to hide from predators. This is likely an example of aposematism.
Reproduction.
Many marine puffers have a pelagic, or open-ocean, life stage. Spawning occurs after males slowly push females to the water surface or join females already present. The eggs are spherical and buoyant. Hatching occurs after roughly four days. The fry are tiny, but under magnification have a shape usually reminiscent of a pufferfish. They have a functional mouth and eyes, and must eat within a few days. Brackish-water puffers may breed in bays in a similar manner to marine species, or may breed more similarly to the freshwater species, in cases where they have moved far enough upriver.
Reproduction in freshwater species varies quite a bit. The dwarf puffers court with males following females, possibly displaying the crests and keels unique to this subgroup of species. After the female accepts his advances, she will lead the male into plants or another form of cover, where she can release eggs for fertilization. The male may help her by rubbing against her side. This has been observed in captivity, and they are the only commonly captive-spawned puffer species.
Target-group puffers have also been spawned in aquariums, and follow a similar courting behavior, minus the crest/keel display. However, eggs are laid on a flat piece of slate or other smooth, hard material, to which they adhere. The male will guard them until they hatch, carefully blowing water over them regularly to keep the eggs healthy. His parenting is finished when the young hatch, and the fry are on their own.
Information on breeding of specific species is very limited. "T. nigroviridis", the green-spotted puffer, has recently been artificially spawned under captive conditions. It is believed to spawn in bays in a similar manner to saltwater species, as their sperm was found to be motile only at full marine salinities, but actual wild breeding has never been observed.
In 2012, male pufferfish were documented carving large geometric, circular structures in the seabed sand in Amami Ōshima, Japan. The structures apparently serve to attract females and provide a safe place for them to lay their eggs.
Evolution.
The tetraodontids have been estimated to diverge from diodontids between 89 and 138 million years ago. The four major clades diverged during the Cretaceous between 80 and 101 million years ago. The oldest known pufferfish genus is "Eotetraodon", from the Lutetian epoch of Middle Eocene Europe, with fossils found in Monte Bolca and the Caucasus Mountains. The Monte Bolca species, "E. pygmaeus", coexisted with several other tetraodontiforms, including an extinct species of diodontid, primitive boxfish ("Proaracana" and "Eolactoria"), and other, totally extinct forms, such as "Zignoichthys" and the spinacanthids.
Human interaction.
Poisoning.
Pufferfish can be lethal if not served properly. Puffer poisoning usually results from consumption of incorrectly prepared puffer soup, "fugu chiri", or occasionally from raw puffer meat, "sashimi fugu". While "chiri" is much more likely to cause death, "sashimi fugu" often causes intoxication, light-headedness, and numbness of the lips, and is often eaten for this reason. Pufferfish tetrodotoxin deadens the tongue and lips, and induces dizziness and vomiting, followed by numbness and prickling over the body, rapid heart rate, decreased blood pressure, and muscle paralysis. The toxin paralyzes diaphragm muscles and stops the person who has ingested it from breathing. People who live longer than 24 hours typically survive, although possibly after a coma lasting several days. In Korea blowfish is made into a soup called Bokeo.
The source of tetrodotoxin in puffers has been a matter of debate, but it is increasingly accepted that bacteria in the fish's intestinal tract are the source.
Saxitoxin, the cause of paralytic shellfish poisoning and red tide, can also be found in certain puffers.
Thailand.
Pufferfish, called "pakpao" in Thailand, are usually consumed by mistake. They are often cheaper than other fish, and because they contain inconsistent levels of toxins between fish and season, there is little awareness or monitoring of the danger. Consumers are regularly hospitalized and some even die from the poisoning.
United States.
Cases of neurological symptoms, including numbness and tingling of the lips and mouth, have been reported to rise after the consumption of puffers caught in the area of Titusville, Florida, U.S. The symptoms generally resolve within hours to days, although one affected individual required intubation for 72 hours. As a result, Florida banned the harvesting of puffers from certain bodies of water.
The Philippines.
The Bureau of Fisheries and Aquatic Resources issued a warning not to eat puffer fish, locally known as "butete", after local fishermen died upon consuming puffer fish for dinner. The warning indicated that puffer fish toxin is 100 times more potent than cyanide.
Poisoning treatment.
Treatment consists of intestinal decontamination with gastric lavage and activated charcoal. Case reports suggest anticholinesterases such as edrophonium may be effective.

</doc>
<doc id="23577" url="http://en.wikipedia.org/wiki?curid=23577" title="Partial function">
Partial function

In mathematics, a partial function from "X" to "Y" (written as "f": "X" ↛ "Y") is a function "f": "X"′ → "Y", for some subset "X"′ of "X". It generalizes the concept of a function "f": "X" → "Y" by not forcing "f" to map "every" element of "X" to an element of "Y" (only some subset "X"′ of "X"). If "X"′ = "X", then "f" is called a total function and is equivalent to a function. Partial functions are often used when the exact domain, "X"′, is not known (e.g. many functions in computability theory).
Specifically, we will say that for any "x" ∈ "X", either:
For example we can consider the square root function restricted to the integers
Thus "g"("n") is only defined for "n" that are perfect squares (i.e., 0, 1, 4, 9, 16, ...). So, "g"(25) = 5, but "g"(26) is undefined.
Basic concepts.
There are two distinct meanings in current mathematical usage for the notion of the domain of a partial function. Most mathematicians, including recursion theorists, use the term "domain of "f"" for the set of all values "x" such that "f"("x") is defined ("X'" above). But some, particularly category theorists, consider the domain of a partial function "f":"X" → "Y" to be "X", and refer to "X'" as the domain of definition. Similarly, the term range can refer to either the codomain or the "image" of a function.
Occasionally, a partial function with domain "X" and codomain "Y" is written as "f": "X" ⇸ "Y", using an arrow with vertical stroke.
A partial function is said to be injective or surjective when the total function given by the restriction of the partial function to its domain of definition is. A partial function may be both injective and surjective.
Because a function is trivially surjective when restricted to its image, the term partial bijection denotes a partial function which is injective.
An injective partial function may be inverted to an injective partial function, and a partial function which is both injective and surjective has an injective function as inverse. Furthermore, a total function which is injective may be inverted to an injective partial function.
The notion of transformation can be generalized to partial functions as well. A partial transformation is a function "f": "A" → "B", where both "A" and "B" are subsets of some set "X".
Total function.
Total function is a synonym for function. The use of the prefix "total" is to suggest that it is a special case over a larger set X of a partial function over a subset of X. For example, when considering the operation of morphism composition in Concrete Categories, the composition operation formula_3 is a total function if and only if formula_4 has one element. The reason for this is that two morphisms formula_5 and formula_6 can only be composed as formula_7 if formula_8, that is, the codomain of formula_9 must equal the domain of formula_10.
Discussion and examples.
The first diagram above represents a partial function that is not a total function since the element 1 in the left-hand set is not associated with anything in the right-hand set. Whereas, the second diagram represents a total function since every element on the left-hand set is associated with exactly one element in the right hand set.
Natural logarithm.
Consider the natural logarithm function mapping the real numbers to themselves. The logarithm of a non-positive real is not a real number, so the natural logarithm function doesn't associate any real number in the codomain with any non-positive real number in the domain. Therefore, the natural logarithm function is not a total function when viewed as a function from the reals to themselves, but it is a partial function. If the domain is restricted to only include the positive reals (that is, if the natural logarithm function is viewed as a function from the positive reals to the reals), then the natural logarithm is a total function.
Subtraction of natural numbers.
Subtraction of natural numbers (non-negative integers) can be viewed as a partial function:
It is defined only when formula_13.
Bottom element.
In denotational semantics a partial function is considered as returning the bottom element when it is undefined.
In computer science a partial function corresponds to a subroutine that raises an exception or loops forever. The IEEE floating point standard defines a not-a-number value which is returned when a floating point operation is undefined and exceptions are suppressed, e.g. when the square root of a negative number is requested.
In a programming language where function parameters are statically typed, a function may be defined as a partial function because the language's type system cannot express the exact domain of the function, so the programmer instead gives it the smallest domain which is expressible as a type and contains the true domain.
In category theory.
The category of sets and partial functions is equivalent to but not isomorphic with the category of pointed sets and point-preserving maps. One textbook notes that "This formal completion of sets and partial maps by adding “improper,” “infinite” elements was reinvented many times, in particular, in topology (one-point compactification) and in theoretical computer science."
The category of sets and partial bijections is equivalent to its dual. It is the prototypical inverse category.
In abstract algebra.
Partial algebra generalizes the notion of universal algebra to partial operations. An example would be a field, in which the multiplicative inversion is the only proper partial operation (because division by zero is not defined).
The set of all partial functions (partial transformations) on a given base "X" set forms a regular semigroup called the semigroup of all partial transformations (or the partial transformation semigroup on "X"), typically denoted by formula_14. The set of all partial bijections on "X" forms the symmetric inverse semigroup.

</doc>
<doc id="23579" url="http://en.wikipedia.org/wiki?curid=23579" title="Photoelectric effect">
Photoelectric effect

The photoelectric effect is the observation that many metals emit electrons when light shines upon them. Electrons emitted in this manner can be called "photoelectrons". The phenomenon is commonly studied in electronic physics, as well as in fields of chemistry, such as quantum chemistry or electrochemistry.
According to classical electromagnetic theory, this effect can be attributed to the transfer of energy from the light to an electron in the metal. From this perspective, an alteration in either the amplitude or wavelength of light would induce changes in the rate of emission of electrons from the metal. Furthermore, according to this theory, a sufficiently dim light would be expected to show a lag time between the initial shining of its light and the subsequent emission of an electron. However, the experimental results did not correlate with either of the two predictions made by this theory.
Instead, as it turns out, electrons are only dislodged by the photoelectric effect if light reaches or exceeds a threshold frequency, below which no electrons can be emitted from the metal regardless of the amplitude and temporal length of exposure of light. To make sense of the fact that light can eject electrons even if its intensity is low, Albert Einstein proposed that a beam of light is not a wave propagating through space, but rather a collection of discrete wave packets (photons), each with energy "hf". This shed light on Max Planck's previous discovery of the Planck relation ("E" = "hf") linking energy ("E") and frequency ("f") as arising from quantization of energy. The factor "h" is known as the Planck constant.
In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905 Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets. This discovery led to the quantum revolution. In 1914, Robert Millikan's experiment confirmed Einstein's law on photoelectric effect. Einstein was awarded the Nobel Prize in 1921 for "his discovery of the law of the photoelectric effect", and Millikan was awarded the Nobel Prize in 1923 for "his work on the elementary charge of electricity and on the photoelectric effect".
The photoelectric effect requires photons with energies from a few electronvolts to over 1 MeV in elements with a high atomic number. Study of the photoelectric effect led to important steps in understanding the quantum nature of light and electrons and influenced the formation of the concept of wave–particle duality. Other phenomena where light affects the movement of electric charges include the photoconductive effect (also known as photoconductivity or photoresistivity), the photovoltaic effect, and the photoelectrochemical effect.
Emission mechanism.
The photons of a light beam have a characteristic energy proportional to the frequency of the light. In the photoemission process, if an electron within some material absorbs the energy of one photon and acquires more energy than the work function (the electron binding energy) of the material, it is ejected. If the photon energy is too low, the electron is unable to escape the material. Since an increase in the intensity of low-frequency light will only increase the number of low-energy photons sent over a given interval of time, this change in intensity will not create any single photon with enough energy to dislodge an electron. Thus, the energy of the emitted electrons does not depend on the intensity of the incoming light, but only on the energy (equivalently frequency) of the individual photons. It is an interaction between the incident photon and the outermost electrons.
Electrons can absorb energy from photons when irradiated, but they usually follow an "all or nothing" principle. All of the energy from one photon must be absorbed and used to liberate one electron from atomic binding, or else the energy is re-emitted. If the photon energy is absorbed, some of the energy liberates the electron from the atom, and the rest contributes to the electron's kinetic energy as a free particle.
Experimental observations of photoelectric emission.
The theory of the photoelectric effect must explain the experimental observations of the emission of electrons from an illuminated metal surface.
For a given metal, there exists a certain minimum frequency of incident radiation below which no photoelectrons are emitted. This frequency is called the "threshold frequency". Increasing the frequency of the incident beam, keeping the number of incident photons fixed (this would result in a proportionate increase in energy) increases the maximum kinetic energy of the photoelectrons emitted. Thus the stopping voltage increases. The number of electrons also changes because the probability that each photon results in an emitted electron is a function of photon energy. If the intensity of the incident radiation of a given frequency is increased, there is no effect on the kinetic energy of each photoelectron.
Above the threshold frequency, the maximum kinetic energy of the emitted photoelectron depends on the frequency of the incident light, but is independent of the intensity of the incident light so long as the latter is not too high.
For a given metal and frequency of incident radiation, the rate at which photoelectrons are ejected is directly proportional to the intensity of the incident light. An increase in the intensity of the incident beam (keeping the frequency fixed) increases the magnitude of the photoelectric current, although the stopping voltage remains the same.
The time lag between the incidence of radiation and the emission of a photoelectron is very small, less than 10−9 second.
The direction of distribution of emitted electrons peaks in the direction of polarization (the direction of the electric field) of the incident light, if it is linearly polarized.
Mathematical description.
The maximum kinetic energy formula_1 of an ejected electron is given by
formula_2
where formula_3 is the Planck constant and formula_4 is the frequency of the incident photon. The term formula_5 is the work function (sometimes denoted formula_6, or formula_7), which gives the minimum energy required to remove a delocalised electron from the surface of the metal. The work function satisfies
formula_8
where formula_9 is the threshold frequency for the metal. The maximum kinetic energy of an ejected electron is then
formula_10
Kinetic energy is positive, so we must have formula_11 for the photoelectric effect to occur.
Stopping potential.
The relation between current and applied voltage illustrates the nature of the photoelectric effect. For discussion, a light source illuminates a plate P, and another plate electrode Q collects any emitted electrons. We vary the potential between P and Q and measure the current flowing in the external circuit between the two plates.
If the frequency and the intensity of the incident radiation are fixed, the photoelectric current increases gradually with an increase in the positive potential on the collector electrode until all the photoelectrons emitted are collected. The photoelectric current attains a saturation value and does not increase further for any increase in the positive potential. The saturation current increases with the increase of the light intensity. It also increases with greater frequencies due to a greater probability of electron emission when collisions happen with higher energy photons.
If we apply a negative potential to the collector plate Q with respect to the plate P and gradually increase it, the photoelectric current decreases, becoming zero at a certain negative potential. The negative potential on the collector at which the photoelectric current becomes zero is called the "stopping potential" or "cut off" potential
i. For a given frequency of incident radiation, the stopping potential is independent of its intensity.
ii. For a given frequency of incident radiation, the stopping potential is determined by the maximum kinetic energy formula_1of the photoelectrons that are emitted. If "qe" is the charge on the electron and formula_13 is the stopping potential, then the work done by the retarding potential in stopping the electron is formula_14, so we have
formula_15
Recalling
formula_16
we see that the stopping voltage varies linearly with frequency of light, but depends on the type of material. For any particular material, there is a threshold frequency that must be exceeded, independent of light intensity, to observe any electron emission.
Three-step model.
In the X-ray regime, the photoelectric effect in crystalline material is often decomposed into three steps::50–51
In the three-step model, an electron can take multiple paths through these three steps. All paths can interfere in the sense of the path integral formulation.
For surface states and molecules the three-step model does still make some sense as even most atoms have multiple electrons which can scatter the one electron leaving.
History.
When a surface is exposed to electromagnetic radiation above a certain threshold frequency (typically visible light for alkali metals, near ultraviolet for other metals, and extreme ultraviolet for non-metals), the radiation is absorbed and electrons are emitted.
Light, and especially ultra-violet light, discharges negatively electrified bodies with the production of rays of the same nature as cathode rays. Under certain circumstances it can directly ionize gases. The first of these phenomena was discovered by Hertz and Hallwachs in 1887. The second was announced first by Philipp Lenard in 1900.
The ultra-violet light to produce these effects may be obtained from an arc lamp, or by burning magnesium, or by sparking with an induction coil between zinc or cadmium terminals, the light from which is very rich in ultra-violet rays. Sunlight is not rich in ultra-violet rays, as these have been absorbed by the atmosphere, and it does not produce nearly so large an effect as the arc-light. Many substances besides metals discharge negative electricity under the action of ultraviolet light: lists of these substances will be found in papers by G. C. Schmidt and O. Knoblauch.
19th century.
In 1839, Alexandre Edmond Becquerel discovered the photovoltaic effect while studying the effect of light on electrolytic cells. Though not equivalent to the photoelectric effect, his work on photovoltaics was instrumental in showing a strong relationship between light and electronic properties of materials. In 1873, Willoughby Smith discovered photoconductivity in selenium while testing the metal for its high resistance properties in conjunction with his work involving submarine telegraph cables.
Johann Elster (1854–1920) and Hans Geitel (1855–1923), students in Heidelberg, developed the first practical photoelectric cells that could be used to measure the intensity of light.:458 Elster and Geitel had investigated with great success the effects produced by light on electrified bodies.
In 1887, Heinrich Hertz observed the photoelectric effect and the production and reception of electromagnetic waves. He published these observations in the journal Annalen der Physik. His receiver consisted of a coil with a spark gap, where a spark would be seen upon detection of electromagnetic waves. He placed the apparatus in a darkened box to see the spark better. However, he noticed that the maximum spark length was reduced when in the box. A glass panel placed between the source of electromagnetic waves and the receiver absorbed ultraviolet radiation that assisted the electrons in jumping across the gap. When removed, the spark length would increase. He observed no decrease in spark length when he replaced glass with quartz, as quartz does not absorb UV radiation. Hertz concluded his months of investigation and reported the results obtained. He did not further pursue investigation of this effect.
The discovery by Hertz in 1887 that the incidence of ultra-violet light on a spark gap facilitated the passage of the spark, led immediately to a series of investigations by Hallwachs, Hoor, Righi and Stoletow. on the effect of light, and especially of ultra-violet light, on charged bodies. It was proved by these investigations that a newly cleaned surface of zinc, if charged with negative electricity, rapidly loses this charge however small it may be when ultra-violet light falls upon the surface; while if the surface is uncharged to begin with, it acquires a positive charge when exposed to the light, the negative electrification going out into the gas by which the metal is surrounded; this positive electrification can be much increased by directing a strong airblast against the surface. If however the zinc surface is positively electrified it suffers no loss of charge when exposed to the light: this result has been questioned, but a very careful examination of the phenomenon by Elster and Geitel has shown that the loss observed under certain circumstances is due to the discharge by the light reflected from the zinc surface of negative electrification on neighbouring conductors induced by the positive charge, the negative electricity under the influence of the electric field moving up to the positively electrified surface.
With regard to the "Hertz effect", the researches from the start showed a great complexity of the phenomenon of photoelectric fatigue — that is, the progressive diminution of the effect observed upon fresh metallic surfaces. According to an important research by Wilhelm Hallwachs, ozone played an important part in the phenomenon. However, other elements enter such as oxidation, the humidity, the mode of polish of the surface, etc. It was at the time not even sure that the fatigue is absent in a vacuum. 
In the period from February 1888 and until 1891, a detailed analysis of photoeffect was performed by Aleksandr Stoletov with results published in 6 works; four of them in "Comptes Rendus", one review in "Physikalische Revue" (translated from Russian), and the last work in "Journal de Physique". First, in these works Stoletov invented a new experimental setup which was more suitable for a quantitative analysis of photoeffect. Using this setup, he discovered the direct proportionality between the intensity of light and the induced photo electric current (the first law of photoeffect or Stoletov's law). One of his other findings resulted from measurements of the dependence of the intensity of the electric photo current on the gas pressure, where he found the existence of an optimal gas pressure Pm corresponding to a maximum photocurrent; this property was used for a creation of solar cells.
In 1899, J. J. Thomson investigated ultraviolet light in Crookes tubes. Thomson deduced that the ejected particles were the same as those previously found in the cathode ray, later called electrons, which he called "corpuscles". In the research, Thomson enclosed a metal plate (a cathode) in a vacuum tube, and exposed it to high frequency radiation. It was thought that the oscillating electromagnetic fields caused the atoms' field to resonate and, after reaching a certain amplitude, caused a subatomic "corpuscle" to be emitted, and current to be detected. The amount of this current varied with the intensity and colour of the radiation. Larger radiation intensity or frequency would produce more current.
20th century.
The discovery of the ionization of gases by ultra-violet light was made by Philipp Lenard in 1900. As the effect was produced across several centimeters of air and made very great positive and small negative ions, it was natural to interpret the phenomenon, as did J. J. Thomson, as a "Hertz effect" upon the solid or liquid particles present in the gas.
In 1902, Lenard observed that the energy of individual emitted electrons increased with the frequency (which is related to the color) of the light.
This appeared to be at odds with Maxwell's wave theory of light, which predicted that the electron energy would be proportional to the intensity of the radiation.
Lenard observed the variation in electron energy with light frequency using a powerful electric arc lamp which enabled him to investigate large changes in intensity, and that had sufficient power to enable him to investigate the variation of potential with light frequency. His experiment directly measured potentials, not electron kinetic energy: he found the electron energy by relating it to the maximum stopping potential (voltage) in a phototube. He found that the calculated maximum electron kinetic energy is determined by the frequency of the light. For example, an increase in frequency results in an increase in the maximum kinetic energy calculated for an electron upon liberation – ultraviolet radiation would require a higher applied stopping potential to stop current in a phototube than blue light. However Lenard's results were qualitative rather than quantitative because of the difficulty in performing the experiments: the experiments needed to be done on freshly cut metal so that the pure metal was observed, but it oxidised in a matter of minutes even in the partial vacuums he used. The current emitted by the surface was determined by the light's intensity, or brightness: doubling the intensity of the light doubled the number of electrons emitted from the surface.
The researches of Langevin and those of Eugene Bloch have shown that the greater part of the Lenard effect is certainly due to this 'Hertz effect'. The Lenard effect upon the gas itself nevertheless does exist. Refound by J. J. Thomson and then more decisively by Frederic Palmer, Jr., it was studied and showed very different characteristics than those at first attributed to it by Lenard.
In 1905, Albert Einstein solved this apparent paradox by describing light as composed of discrete quanta, now called photons, rather than continuous waves. Based upon Max Planck's theory of black-body radiation, Einstein theorized that the energy in each quantum of light was equal to the frequency multiplied by a constant, later called Planck's constant. A photon above a threshold frequency has the required energy to eject a single electron, creating the observed effect. This discovery led to the quantum revolution in physics and earned Einstein the Nobel Prize in Physics in 1921. By wave-particle duality the effect can be analyzed purely in terms of waves though not as conveniently.
Albert Einstein's mathematical description of how the photoelectric effect was caused by absorption of quanta of light was in one of his 1905 papers, named "On a Heuristic Viewpoint Concerning the Production and Transformation of Light". This paper proposed the simple description of "light quanta", or photons, and showed how they explained such phenomena as the photoelectric effect. His simple explanation in terms of absorption of discrete quanta of light explained the features of the phenomenon and the characteristic frequency.
The idea of light quanta began with Max Planck's published law of black-body radiation ("On the Law of Distribution of Energy in the Normal Spectrum") by assuming that Hertzian oscillators could only exist at energies "E" proportional to the frequency "f" of the oscillator by "E" = "hf", where "h" is Planck's constant. By assuming that light actually consisted of discrete energy packets, Einstein wrote an equation for the photoelectric effect that agreed with experimental results. It explained why the energy of photoelectrons was dependent only on the "frequency" of the incident light and not on its "intensity": a low-intensity, high-frequency source could supply a few high energy photons, whereas a high-intensity, low-frequency source would supply no photons of sufficient individual energy to dislodge any electrons. This was an enormous theoretical leap, but the concept was strongly resisted at first because it contradicted the wave theory of light that followed naturally from James Clerk Maxwell's equations for electromagnetic behavior, and more generally, the assumption of infinite divisibility of energy in physical systems. Even after experiments showed that Einstein's equations for the photoelectric effect were accurate, resistance to the idea of photons continued, since it appeared to contradict Maxwell's equations, which were well-understood and verified.
Einstein's work predicted that the energy of individual ejected electrons increases linearly with the frequency of the light. Perhaps surprisingly, the precise relationship had not at that time been tested. By 1905 it was known that the energy of photoelectrons increases with increasing "frequency" of incident light and is independent of the "intensity" of the light. However, the manner of the increase was not experimentally determined until 1914 when Robert Andrews Millikan showed that Einstein's prediction was correct.
The photoelectric effect helped to propel the then-emerging concept of wave–particle duality in the nature of light. Light simultaneously possesses the characteristics of both waves and particles, each being manifested according to the circumstances. The effect was impossible to understand in terms of the classical wave description of light, as the energy of the emitted electrons did not depend on the intensity of the incident radiation. Classical theory predicted that the electrons would 'gather up' energy over a period of time, and then be emitted.
Uses and effects.
Photomultipliers.
These are extremely light-sensitive vacuum tubes with a photocathode coated onto part (an end or side) of the inside of the envelope. The photocathode contains combinations of materials such as caesium, rubidium and antimony specially selected to provide a low work function, so when illuminated even by very low levels of light, the photocathode readily releases electrons. By means of a series of electrodes (dynodes) at ever-higher potentials, these electrons are accelerated and substantially increased in number through secondary emission to provide a readily detectable output current. Photomultipliers are still commonly used wherever low levels of light must be detected.
Image sensors.
Video camera tubes in the early days of television used the photoelectric effect, for example, Philo Farnsworth's "Image dissector" used a screen charged by the photoelectric effect to transform an optical image into a scanned electronic signal.
Gold-leaf electroscope.
Gold-leaf electroscopes are designed to detect static electricity. Charge placed on the metal cap spreads to the stem and the gold leaf of the electroscope. Because they then have the same charge, the stem and leaf repel each other. This will cause the leaf to bend away from the stem.
The electroscope is an important tool in illustrating the photoelectric effect. For example, if the electroscope is negatively charged throughout, there is an excess of electrons and the leaf is separated from the stem. If high-frequency light shines on the cap, the electroscope discharges and the leaf will fall limp. This is because the frequency of the light shining on the cap is above the cap's threshold frequency. The photons in the light have enough energy to liberate electrons from the cap, reducing its negative charge. This will discharge a negatively charged electroscope and further charge a positive electroscope. However, if the electromagnetic radiation hitting the metal cap does not have a high enough frequency (its frequency is below the threshold value for the cap), then the leaf will never discharge, no matter how long one shines the low-frequency light at the cap.:389–390
Photoelectron spectroscopy.
Since the energy of the photoelectrons emitted is exactly the energy of the incident photon minus the material's work function or binding energy, the work function of a sample can be determined by bombarding it with a monochromatic X-ray source or UV source, and measuring the kinetic energy distribution of the electrons emitted.:14–20
Photoelectron spectroscopy is usually done in a high-vacuum environment, since the electrons would be scattered by gas molecules if they were present. However, some companies are now selling products that allow photoemission in air. The light source can be a laser, a discharge tube, or a synchrotron radiation source.
The concentric hemispherical analyser (CHA) is a typical electron energy analyzer, and uses an electric field to change the directions of incident electrons, depending on their kinetic energies. For every element and core (atomic orbital) there will be a different binding energy. The many electrons created from each of these combinations will show up as spikes in the analyzer output, and these can be used to determine the elemental composition of the sample.
Spacecraft.
The photoelectric effect will cause spacecraft exposed to sunlight to develop a positive charge. This can be a major problem, as other parts of the spacecraft in shadow develop a negative charge from nearby plasma, and the imbalance can discharge through delicate electrical components. The static charge created by the photoelectric effect is self-limiting, though, because a more highly charged object gives up its electrons less easily.
Moon dust.
Light from the sun hitting lunar dust causes it to become charged through the photoelectric effect. The charged dust then repels itself and lifts off the surface of the Moon by electrostatic levitation. This manifests itself almost like an "atmosphere of dust", visible as a thin haze and blurring of distant features, and visible as a dim glow after the sun has set. This was first photographed by the Surveyor program probes in the 1960s. It is thought that the smallest particles are repelled up to kilometers high, and that the particles move in "fountains" as they charge and discharge.
Night vision devices.
Photons hitting a thin film of alkali metal or semiconductor material such as gallium arsenide in an image intensifier tube cause the ejection of photoelectrons due to the photoelectric effect. These are accelerated by an electrostatic field where they strike a phosphor coated screen, converting the electrons back into photons. Intensification of the signal is achieved either through acceleration of the electrons or by increasing the number of electrons through secondary emissions, such as with a micro-channel plate. Sometimes a combination of both methods is used. Additional kinetic energy is required to move an electron out of the conduction band and into the vacuum level. This is known as the electron affinity of the photocathode and is another barrier to photoemission other than the forbidden band, explained by the band gap model. Some materials such as Gallium Arsenide have an effective electron affinity that is below the level of the conduction band. In these materials, electrons that move to the conduction band are all of sufficient energy to be emitted from the material and as such, the film that absorbs photons can be quite thick. These materials are known as negative electron affinity materials.
Cross section.
The photoelectric effect is one interaction mechanism between photons and atoms. It is one of 12 theoretically possible interactions.
At the high photon energies comparable to the electron rest energy of , Compton scattering, another process, may take place. Above twice this () pair production may take place. Compton scattering and pair production are examples of two other competing mechanisms.
Indeed, even if the photoelectric effect is the favoured reaction for a particular single-photon bound-electron interaction, the result is also subject to statistical processes and is not guaranteed, albeit the photon has certainly disappeared and a bound electron has been excited (usually K or L shell electrons at gamma ray energies). The probability of the photoelectric effect occurring is measured by the cross section of interaction, σ. This has been found to be a function of the atomic number of the target atom and photon energy. A crude approximation, for photon energies above the highest atomic binding energy, is given by:
Here "Z" is atomic number and "n" is a number which varies between 4 and 5. (At lower photon energies a characteristic structure with edges appears, K edge, L edges, M edges, etc.) The obvious interpretation follows that the photoelectric effect rapidly decreases in significance, in the gamma ray region of the spectrum, with increasing photon energy, and that photoelectric effect increases steeply with atomic number. The corollary is that high-"Z" materials make good gamma-ray shields, which is the principal reason that lead ("Z" = 82) is a preferred and ubiquitous gamma radiation shield.
See also.
"Electronics:"
"Physics:"
"Chemistry:"
"Lists":
External links.
"Applets"

</doc>
<doc id="23580" url="http://en.wikipedia.org/wiki?curid=23580" title="Paleogene">
Paleogene

The Paleogene ( or ; also spelled Palaeogene or Palæogene; informally Lower Tertiary) is a geologic period and system that began and ended million years ago and comprises the first part of the Cenozoic Era. Lasting 43 million years, the Paleogene is most notable as being the time in which mammals evolved from relatively small, simple forms into a large group of diverse animals in the wake of the Cretaceous–Paleogene extinction event that ended the preceding Cretaceous Period. 
This period consists of the Paleocene, Eocene, and Oligocene Epochs. The end of the Paleocene (55.5/54.8 Mya) was marked by one of the most significant periods of global change during the Cenozoic, the Paleocene–Eocene Thermal Maximum, which upset oceanic and atmospheric circulation and led to the extinction of numerous deep-sea benthic foraminifera and on land, a major turnover in mammals. The Paleogene follows the Cretaceous Period and is followed by the Miocene Epoch of the Neogene Period. The terms 'Paleogene System' (formal) and 'lower Tertiary System' (informal) are applied to the rocks deposited during the 'Paleogene Period'. The somewhat confusing terminology seems to be due to attempts to deal with the comparatively fine subdivisions of time possible in the relatively recent geologic past, when more information is preserved. By dividing the Tertiary Period into two periods instead of directly into five epochs, the periods are more closely comparable to the duration of 'periods' in the Mesozoic and Paleozoic Eras.
Climate and geography.
The global climate during the Paleogene departed from the hot and humid conditions of the late Mesozoic era and began a cooling and drying trend which, although having been periodically disrupted by warm periods such as the Paleocene–Eocene Thermal Maximum, persists today. The trend was partly caused by the formation of the Antarctic Circumpolar Current, which significantly cooled oceanic water temperatures.
The continents during the Paleogene continued to drift closer to their current positions. India was in the process of colliding with Asia, subsequently forming the Himalayas. The Atlantic Ocean continued to widen by a few centimeters each year. Africa was moving north to meet with Europe and form the Mediterranean, while South America was moving closer to North America (they would later connect via the Isthmus of Panama). Inland seas retreated from North America early in the period. Australia had also separated from Antarctica and was drifting towards Southeast Asia.
Flora and fauna.
Mammals began a rapid diversification during this period. After the Cretaceous–Paleogene extinction event, which saw the demise of the non-avian dinosaurs, they transformed from a few small and generalized forms and began to evolve into most of the modern varieties we see today. Some of these mammals would evolve into large forms that would dominate the land, while others would become capable of living in marine, specialized terrestrial, and airborne environments. Some mammals took to the oceans and became modern cetaceans, while others took to the trees and became primates, the group to which humans belong. Birds, which were already well established by the end of the Cretaceous, also experienced an adaptive radiation as they took over the skies left empty by the now extinct Pterosaurs. Most other branches of life remained relatively unchanged in comparison to birds and mammals during this period.
As the Earth began to cool, tropical plants were less numerous and were now restricted to equatorial regions. Deciduous plants became more common, which could survive through the seasonal climate the world was now experiencing. One of the most notable floral developments during this period was the evolution of the first grass species. This new plant type expanded and formed new ecological environments we know today as savannas and prairies. These grasslands also began to replace many forests because they could survive better in the drier climate typical in many regions of the world during this period.
Geology.
Key events in the Paleogene</div Scale><div id=ScaleBar style="width:1px; float:left; height:36em; padding:0; background-color:#242020" />em;
 height:1em;
 margin-left:0em;
 width:0.63em;
">em; 
">Mesozoic
em;
 height:34em;
 margin-left:0em;
 width:0.63em;
">em; 
">Cenozoic
em;
 height:1.0em;
 margin-left:0.7em;
 width:6.3em;
">em; 
">Cretaceous
em;
 height:32.9131914894em;
 margin-left:0.7em;
 width:0.84em;
">em; 
">P<br>a<br>l<br>e<br>o<br>g<br>e<br>n<br>e
em;
 height:1.55489361702em;
 margin-left:0.7em;
 width:6.3em;
">em; 
">Neogene
em;
 height:7em;
 margin-left:1.54em;
 width:0.84em;
">em; 
">Paleo<br>c<br>e<br>n<br>e
em;
 height:16.8276595745em;
 margin-left:1.54em;
 width:0.84em;
">em; 
">Eocene
em;
 height:8.22595744681em;
 margin-left:1.54em;
 width:0.84em;
">em; 
">Olig<br>o<br>c<br>e<br>n<br>e
em;
 height:3.27021276596em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Danian
em;
 height:1.83829787234em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Selandian
em;
 height:2.35106382979em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Thanetian
em;
 height:6.18085106383em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Ypresian
em;
 height:4.87872340426em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Lutetian
em;
 height:2.42765957447em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Bartonian
em;
 height:3.04042553191em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Priabonian
em;
 height:4.34255319149em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Rupelian
em;
 height:3.78340425532em;
 margin-left:2.38em;
 width:4.62em;
">em; 
">Chattian
em;
 height:36em;
 margin-left:6.93em;
 width:0.07em;
">em; 
"> em;
 height:36em;
 margin-left:0.63em;
 width:0.07em;
">em; 
"> em;
 height:32.9131914894em;
 margin-left:1.54em;
 width:0.035em;
">em; 
"> em;
 height:32.9131914894em;
 margin-left:1.54em;
 width:0.035em;
">em; 
"> em;
 height:32.9131914894em;
 margin-left:2.38em;
 width:0.035em;
">em; 
"> </div Timeline>em;
">←PETM
em;
">←N. Amer. prairie expands
em;
">←First Antarctic permanent ice-sheets
em;
">←K-Pg mass<br>extinction
em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
"></div containsTSN></div Legend>An approximate timescale of key Paleogene events.<br>Axis scale: millions of years ago.<br></div caption></div Container>
Oil industry relevance.
The Paleogene is notable in the context of offshore oil drilling, and especially in Gulf of Mexico oil exploration, where it is usually referred to as the "Lower Tertiary". These rock formations represent the current cutting edge of deep-water oil discovery.
Lower Tertiary rock formations encountered in the Gulf of Mexico oil industry usually tend to be comparatively high temperature and high pressure reservoirs, often with high sand content (70%+) or under very thick evaporite sediment layers.
Lower Tertiary explorations to date include (partial list):

</doc>
<doc id="23582" url="http://en.wikipedia.org/wiki?curid=23582" title="Preorder">
Preorder

In mathematics, especially in order theory, a preorder or quasiorder is a binary relation that is reflexive and transitive. All partial orders and equivalence relations are preorders, but preorders are more general.
The name 'preorder' comes from the idea that preorders (that are not partial orders) are 'almost' (partial) orders, but not quite; they're neither necessarily anti-symmetric nor symmetric. Because a preorder is a binary relation, the symbol ≤ can be used as the notational device for the relation. However, because they are not necessarily anti-symmetric, some of the ordinary intuition associated to the symbol ≤ may not apply. On the other hand, a preorder can be used, in a straightforward fashion, to define a partial order and an equivalence relation. Doing so, however, is not always useful or worthwhile, depending on the problem domain being studied.
In words, when "a" ≤ "b", one may say that "b" "covers" "a" or that "a" "precedes" "b", or that "b" "reduces" to "a". Occasionally, the notation ← or formula_1 is used instead of ≤.
To every preorder, there corresponds a directed graph, with elements of the set corresponding to vertices, and the order relation between pairs of elements corresponding to the directed edges between vertices. The converse is not true: most directed graphs are neither reflexive nor transitive. Note that, in general, the corresponding graphs may be cyclic graphs: preorders may have cycles in them. A preorder that is antisymmetric no longer has cycles; it is a partial order, and corresponds to a directed acyclic graph. A preorder that is symmetric is an equivalence relation; it can be thought of as having lost the direction markers on the edges of the graph. In general, a preorder may have many disconnected components.
Formal definition.
Consider some set "P" and a binary relation ≤ on "P". Then ≤ is a preorder, or quasiorder, if it is reflexive and transitive, i.e., for all "a", "b" and "c" in "P", we have that:
A set that is equipped with a preorder is called a preordered set (or proset).
If a preorder is also antisymmetric, that is, "a" ≤ "b" and "b" ≤ "a" implies "a" = "b", then it is a partial order.
On the other hand, if it is symmetric, that is, if "a" ≤ "b" implies "b" ≤ "a", then it is an equivalence relation.
A preorder which is preserved in all contexts (i.e. respected by all functions on "P") is called a precongruence.
A precongruence which is also symmetric (i.e. is an equivalence relation) is a congruence relation.
Equivalently, a preordered set "P" can be defined as a category with objects the elements of "P", and each hom-set having at most one element (one for objects which are related, zero otherwise).
Alternately, a preordered set can be understood as an enriched category, enriched over the category 2 = (0→1).
A preordered class is a class equipped with a preorder. Every set is a class and so every preordered set is a preordered class. Preordered classes can be defined as thin categories, i.e. as categories with at most one morphism from an object to another.
Examples.
In computer science, one can find examples of the following preorders.
Example of a total preorder:
Uses.
Preorders play a pivotal role in several situations:
Constructions.
Every binary relation R on a set S can be extended to a preorder on S by taking the transitive closure and reflexive closure, R+=. The transitive closure indicates path connection in R: "x" R+ "y" if and only if there is an R-path from "x" to y.
Given a preorder formula_7 on S one may define an equivalence relation ~ on S such that "a" ~ "b" if and only if "a" formula_7 "b" and "b" formula_7 "a". (The resulting relation is reflexive since a preorder is reflexive, transitive by applying transitivity of the preorder twice, and symmetric by definition.)
Using this relation, it is possible to construct a partial order on the quotient set of the equivalence, S / ~, the set of all equivalence classes of ~. Note that if the preorder is R+=, S / ~ is the set of R-cycle equivalence classes: "x" ∈ ["y"] if and only if "x" = "y" or "x" is in an R-cycle with y. In any case, on S / ~ we can define ["x"] ≤ ["y"] if and only if "x" formula_7 "y". By the construction of ~, this definition is independent of the chosen representatives and the corresponding relation is indeed well-defined. It is readily verified that this yields a partially ordered set.
Conversely, from a partial order on a partition of a set S one can construct a preorder on S. There is a 1-to-1 correspondence between preorders and pairs (partition, partial order).
For a preorder "formula_7", a relation "<" can be defined as "a" < "b" if and only if ("a" formula_7 "b" and not "b" formula_7 "a"), or equivalently, using the equivalence relation introduced above, ("a" formula_7 "b" and not "a" ~ "b"). It is a strict partial order; every strict partial order can be the result of such a construction. If the preorder is anti-symmetric, hence a partial order "≤", the equivalence is equality, so the relation "<" can also be defined as "a" < "b" if and only if ("a" ≤ "b" and "a" ≠ "b").
Conversely we have "a" formula_7 "b" if and only if "a" < "b" or "a" ~ "b". This is the reason for using the notation "formula_7"; "≤" can be confusing for a preorder that is not anti-symmetric, it may suggest that "a" ≤ "b" implies that "a" < "b" or "a" = "b".
Note that with this construction multiple preorders "formula_7" can give the same relation "<", so without more information, such as the equivalence relation, "formula_7" cannot be reconstructed from "<". Possible preorders include the following:
Number of preorders.
As explained above, there is a 1-to-1 correspondence between preorders and pairs (partition, partial order). Thus the number of preorders is the sum of the number of partial orders on every partition. For example:
Interval.
For "a" formula_7 "b", the interval ["a","b"] is the set of points "x" satisfying "a" formula_7 "x" and "x" formula_7 "b", also written "a" formula_7 "x" formula_7 "b". It contains at least the points "a" and "b". One may choose to extend the definition to all pairs ("a","b"). The extra intervals are all empty.
Using the corresponding strict relation "<", one can also define the interval ("a","b") as the set of points "x" satisfying "a" < "x" and "x" < "b", also written "a" < "x" < "b". An open interval may be empty even if "a" < "b".
Also ["a","b") and ("a","b"] can be defined similarly.
References.
</dl>

</doc>
<doc id="23585" url="http://en.wikipedia.org/wiki?curid=23585" title="Psychoanalysis">
Psychoanalysis

Psychoanalysis is a set of psychological and psychotherapeutic theories and associated techniques, created by Austrian physician Sigmund Freud and stemming partly from the clinical work of Josef Breuer and others. Since then, psychoanalysis has been revised and developed in different directions. Some of Freud's colleagues and students, such as Alfred Adler and Carl Gustav Jung, went on to develop their own ideas independently. Freud insisted on retaining the term "psychoanalysis" for his school of thought, and Adler and Jung accepted this. The Neo-Freudians included Erich Fromm, Karen Horney, Harry Stack Sullivan.
The basic tenets of psychoanalysis include:
Under the broad umbrella of psychoanalysis there are at least 22 theoretical orientations regarding human mental development. The various approaches in treatment called "psychoanalysis" vary as much as the theories do. The term also refers to a method of analysing child development.
Freudian psychoanalysis refers to a specific type of treatment in which the "analysand" (analytic patient) verbally expresses his or her thoughts, including free associations, fantasies, and dreams, from which the analyst infers the unconscious conflicts causing the patient's symptoms and character problems, and interprets them for the patient to create insight for resolution of the problems. The analyst confronts and clarifies the patient's pathological defenses, wishes and guilt. Through the analysis of conflicts, including those contributing to resistance and those involving transference onto the analyst of distorted reactions, psychoanalytic treatment can hypothesize how patients unconsciously are their own worst enemies: how unconscious, symbolic reactions that have been stimulated by experience are causing symptoms. Freudian psychoanalysis relies on the concept that it is only after having a cathartic (e.g. healing) experience can a person be "cured" and aided.
Psychoanalysis has received criticism from a wide variety of sources. It is regarded by some critics as a pseudoscience. Nonetheless, it remains a strong influence within the realm of psychiatry, and more so in some quarters than others.
History.
1890s.
The idea of psychoanalysis first started to receive serious attention under Sigmund Freud. Sigmund Freud formulated his own theory of psychoanalysis in Vienna in the 1890s. Freud was a neurologist trying to find an effective treatment for patients with neurotic or hysterical symptoms. Freud realised that there were mental processes that were not conscious, whilst he was employed as a neurological consultant at the Children's Hospital, where he noticed that many aphasic children had no apparent organic cause for their symptoms. He then wrote a monograph about this subject. In 1885, Freud obtained a grant to study with Jean-Martin Charcot, a famed neurologist, at the Salpêtrière in Paris, where Freud followed the clinical presentations of Charcot, particularly in the areas of hysteria, paralyses and the anaesthesias. Charcot had introduced hypnotism as an experimental research tool and developed the photographic representation of clinical symptoms.
Freud's first theory to explain hysterical symptoms was presented in "Studies on Hysteria" (1895), co-authored with his mentor the distinguished physician Josef Breuer, which was generally seen as the birth of psychoanalysis. The work was based on Breuer's treatment of "Anna O.," which the patient herself had dubbed the "talking cure." Breuer wrote that many factors that could result in such symptoms, including various types of emotional trauma, and he also credited work by others such as Pierre Janet; while Freud contended that at the root of hysterical symptoms were repressed memories of distressing occurrences, almost always having direct or indirect sexual associations.
Around the same time Freud attempted to develop a neuro-physiological theory of unconscious mental mechanisms, which he soon gave up. It remained unpublished in his lifetime.
In 1896 Freud published his so-called seduction theory which proposed that the preconditions for hysterical symptoms are sexual excitations in infancy, and he claimed to have uncovered repressed memories of incidents of sexual abuse for all his current patients. However by 1898 he had privately acknowledged to his friend and colleague Wilhelm Fliess that he no longer believed in his theory, though he did not state this publicly until 1906. Though in 1896 he had reported that his patients "had no feeling of remembering the [infantile sexual] scenes," and assured him "emphatically of their unbelief," in later accounts he claimed that they had told him that they had been sexually abused in infancy. This became the received historical account until challenged by several Freud scholars in the latter part of the 20th century who argued that he had imposed his preconceived notions on his patients. However, building on his claims that the patients reported infantile sexual abuse experiences, Freud subsequently contended that his clinical findings in the mid-1890s provided evidence of the occurrence of unconscious fantasies, supposedly to cover up memories of infantile masturbation. Only much later did he claim the same findings as evidence for Oedipal desires.
1900–1940s.
By 1900, Freud had theorised that dreams had symbolic significance, and generally were specific to the dreamer. Freud formulated his second psychological theory— which hypothesises that the unconscious has or is a "primary process" consisting of symbolic and condensed thoughts, and a "secondary process" of logical, conscious thoughts. This theory was published in his 1900 book, "The Interpretation of Dreams". Chapter VII was a re-working of the earlier "Project" and Freud outlined his "Topographic Theory." In this theory, which was mostly later supplanted by the Structural Theory, unacceptable sexual wishes were repressed into the "System Unconscious," unconscious due to society's condemnation of premarital sexual activity, and this repression created anxiety.
This "topographic theory" is still popular in much of Europe, although it has fallen out of favour in much of North America. In 1905, Freud published "Three Essays on the Theory of Sexuality" in which he laid out his discovery of so-called psychosexual phases: oral (ages 0–2), anal (2–4), phallic-oedipal (today called 1st genital ) (3–6), latency (6-puberty), and mature genital (puberty-onward). His early formulation included the idea that because of societal restrictions, sexual wishes were repressed into an unconscious state, and that the energy of these unconscious wishes could be turned into anxiety or physical symptoms. Therefore the early treatment techniques, including hypnotism and abreaction, were designed to make the unconscious conscious in order to relieve the pressure and the apparently resulting symptoms.
In "On Narcissism" (1915) Freud turned his attention to the subject of narcissism. Still using an energic system, Freud characterized the difference between energy directed at the self versus energy directed at others, called cathexis. By 1917, in "Mourning and Melancholia," he suggested that certain depressions were caused by turning guilt-ridden anger on the self. In 1919 in "A Child is Being Beaten" he began to address the problems of self-destructive behavior (moral masochism) and frank sexual masochism. Based on his experience with depressed and self-destructive patients, and pondering the carnage of World War I, Freud became dissatisfied with considering only oral and sexual motivations for behavior. By 1920, Freud addressed the power of identification (with the leader and with other members) in groups as a motivation for behavior ("Group Psychology and the Analysis of the Ego"). In that same year (1920) Freud suggested his "dual drive" theory of sexuality and aggression in "Beyond the Pleasure Principle", to try to begin to explain human destructiveness. Also, it was the first appearance of his "structural theory" consisting three new concepts id, ego, and superego.
Three years later, he summarised the ideas of id, ego, and superego in a book entitled, "The Ego and the Id". In the book, he revised the whole theory of mental functioning, now considering that repression was only one of many defense mechanisms, and that it occurred to reduce anxiety. Hence, Freud characterised repression as both a cause and a result of anxiety. In 1926, in "Inhibitions, Symptoms and Anxiety," Freud characterised how intrapsychic conflict among drive and superego (wishes and guilt) caused anxiety, and how that anxiety could lead to an inhibition of mental functions, such as intellect and speech. "Inhibitions, Symptoms and Anxiety" was written in response to Otto Rank, who, in 1924, published "Das Trauma der Geburt" (translated into English in 1929 as "The Trauma of Birth"), analysing how art, myth, religion, philosophy and therapy were illuminated by separation anxiety in the "phase before the development of the Oedipus complex" (p. 216) . Freud's theories, however, characterized no such phase. According to Freud, the Oedipus complex, was at the centre of neurosis, and was the foundational source of all art, myth, religion, philosophy, therapy—indeed of all human culture and civilization. It was the first time that anyone in the inner circle had characterised something other than the Oedipus complex as contributing to intrapsychic development, a notion that was rejected by Freud and his followers at the time.
By 1936, the "Principle of Multiple Function" was clarified by Robert Waelder. He widened the formulation that psychological symptoms were caused by and relieved conflict simultaneously. Moreover, symptoms (such as phobias and compulsions) each represented elements of some drive wish (sexual and/or aggressive), superego, anxiety, reality, and defenses. Also in 1936, Anna Freud, Sigmund's famous daughter, published her seminal book, "The Ego and the Mechanisms of Defense", outlining numerous ways the mind could shut upsetting things out of consciousness.
1940s–present.
When Hitler's power grew, the Freud family and many of their colleagues flew to London. Within a year Sigmund Freud died. In the United States, also following the death of Freud, a new group of psychoanalysts began to explore the function of the ego. Led by Heinz Hartmann, Kris, Rappaport and Lowenstein, the group built upon understandings of the synthetic function of the ego as a mediator in psychic functioning . Hartmann in particular distinguished between autonomous ego functions (such as memory and intellect which could be secondarily affected by conflict) and synthetic functions which were a result of compromise formation . These "Ego Psychologists" of the 1950s paved a way to focus analytic work by attending to the defenses (mediated by the ego) before exploring the deeper roots to the unconscious conflicts. In addition there was burgeoning interest in child psychoanalysis. Although criticized since its inception, psychoanalysis has been used as a research tool into childhood development, and is still used to treat certain mental disturbances. In the 1960s, Freud's early thoughts on the childhood development of female sexuality were challenged; this challenge led to the development of a variety of understandings of female sexual development , many of which modified the timing and normality of several of Freud's theories (which had been gleaned from the treatment of women with mental disturbances). Several researchers followed Karen Horney's studies of societal pressures that influence the development of women. Most contemporary North American psychoanalysts employ theories that, while based on those of Sigmund Freud, include many modifications of theory and practice developed since his death in 1939 .
In the first decade of the 21st century there are approximately 35 training institutes for psychoanalysis in the United States accredited by the American Psychoanalytic Association (APsaA), which is a component organization of the International Psychoanalytical Association (IPA), and there are over 3000 graduated psychoanalysts practicing in the United States. The IPA accredits psychoanalytic training centers through such "component organisations" throughout the rest of the world, including countries such as Serbia, France, Germany, Austria, Italy, Switzerland, and many others, as well as about six institutes directly in the U.S.
Theories.
The predominant psychoanalytic theories can be organised into several theoretical schools. Although these theoretical schools differ, most of them emphasize the influence of unconscious elements on the conscious. There has also been considerable work done on consolidating elements of conflicting theories (cf. the work of Theodore Dorpat, B. Killingmo, and S. Akhtar). As in all fields of medicine, there are some persistent conflicts regarding specific causes of certain syndromes, and disputes regarding the ideal treatment techniques. In the 21st century, psychoanalytic ideas are embedded in Western culture, especially in fields such as childcare, education, literary criticism, cultural studies, and mental health, particularly psychotherapy. Though there is a mainstream of evolved analytic ideas, there are groups who follow the precepts of one or more of the later theoreticians. Psychoanalytic ideas also play roles in some types of literary analysis such as Archetypal literary criticism.
Topographic theory.
Topographic theory was named and first described by Sigmund Freud in "The Interpretation of Dreams" (1900). The theory hypothesizes that the mental apparatus can be divided into the systems Conscious, Preconcious, and Unconscious. These systems are not anatomical structures of the brain but, rather, mental processes. Although Freud retained this theory throughout his life he largely replaced it with the Structural theory. The Topographic theory remains as one of the meta-psychological points of view for describing how the mind functions in classical psychoanalytic theory.
Structural theory.
Structural theory divides the psyche into the id, the ego, and the super-ego. The id is present at birth as the repository of basic instincts, which Freud called "Triebe" ("drives"): unorganized and unconscious, it operates merely on the 'pleasure principle', without realism or foresight. The ego develops slowly and gradually, being concerned with mediating between the urging of the id and the realities of the external world; it thus operates on the 'reality principle'. The super-ego is held to be the part of the ego in which self-observation, self-criticism and other reflective and judgmental faculties develop. The ego and the super-ego are both partly conscious and partly unconscious.
Ego psychology.
Ego psychology was initially suggested by Freud in "Inhibitions, Symptoms and Anxiety" (1926). The theory was refined by Hartmann, Loewenstein, and Kris in a series of papers and books from 1939 through the late 1960s. Leo Bellak was a later contributor. This series of constructs, paralleling some of the later developments of cognitive theory, includes the notions of autonomous ego functions: mental functions not dependent, at least in origin, on intrapsychic conflict. Such functions include: sensory perception, motor control, symbolic thought, logical thought, speech, abstraction, integration (synthesis), orientation, concentration, judgment about danger, reality testing, adaptive ability, executive decision-making, hygiene, and self-preservation. Freud noted that inhibition is one method that the mind may utilize to interfere with any of these functions in order to avoid painful emotions. Hartmann (1950s) pointed out that there may be delays or deficits in such functions.
Frosch (1964) described differences in those people who demonstrated damage to their relationship to reality, but who seemed able to test it. Deficits in the capacity to organize thought are sometimes referred to as blocking or loose associations (Bleuler), and are characteristic of the schizophrenia . Deficits in abstraction ability and self-preservation also suggest psychosis in adults . Deficits in orientation and sensorium are often indicative of a medical illness affecting the brain (and therefore, autonomous ego functions) . Deficits in certain ego functions are routinely found in severely sexually or physically abused children, where powerful effects generated throughout childhood seem to have eroded some functional development .
According to ego psychology, ego strengths, later described by Kernberg (1975), include the capacities to control oral, sexual, and destructive impulses; to tolerate painful effects without falling apart; and to prevent the eruption into consciousness of bizarre symbolic fantasy. Synthetic functions, in contrast to autonomous functions, arise from the development of the ego and serve the purpose of managing conflict processes. Defenses are synthetic functions that protect the conscious mind from awareness of forbidden impulses and thoughts. One purpose of ego psychology has been to emphasize that some mental functions can be considered to be basic, rather than derivatives of wishes, affects, or defenses. However, autonomous ego functions can be secondarily affected because of unconscious conflict. For example, a patient may have an hysterical amnesia (memory being an autonomous function) because of intrapsychic conflict (wishing not to remember because it is too painful).
Taken together, the above theories present a group of metapsychological assumptions. Therefore, the inclusive group of the different classical theories provides a cross-sectional view of human mentation. There are six "points of view", five described by Freud and a sixth added by Hartmann. Unconscious processes can therefore be evaluated from each of these six points of view. The "points of view" are: 1. Topographic 2. Dynamic (the theory of conflict) 3. Economic (the theory of energy flow) 4. Structural 5. Genetic (propositions concerning origin and development of psychological functions) and 6. Adaptational (psychological phenomena as it relates to the external world).
Modern conflict theory.
Modern conflict theory, a variation of ego psychology, is a revised version of structural theory, most notably different by altering concepts related to where repressed thoughts were stored(Freud, 1923, 1926). Modern conflict theory centres around how emotional symptoms and character traits are complex solutions to mental conflict. It dispenses with the concepts of a fixed id, ego and superego, and instead posits conscious and unconscious conflict among wishes (dependent, controlling, sexual, and aggressive), guilt and shame, emotions (especially anxiety and depressive affect), and defensive operations that shut off from consciousness some aspect of the others. Moreover, healthy functioning (adaptive) is also determined, to a great extent, by resolutions of conflict.
A major objective of modern conflict-theory psychoanalysis is to change the balance of conflict in a patient by making aspects of the less adaptive solutions (also called "compromise formations") conscious so that they can be rethought, and more adaptive solutions found. Current theoreticians following Brenner's many suggestions (see especially Brenner's 1982 book, "The Mind in Conflict") include Sandor Abend, MD (Abend, Porder, & Willick, (1983), "Borderline Patients: Clinical Perspectives"), Jacob Arlow (Arlow and Brenner (1964), "Psychoanalytic Concepts and the Structural Theory"), and Jerome Blackman (2003), "101 Defenses: How the Mind Shields Itself".
Object relations theory.
Object relations theory attempts to explain the ups and downs of human relationships through a study of how internal representations of the self and others are organized. The clinical symptoms that suggest object relations problems (typically developmental delays throughout life) include disturbances in an individual's capacity to feel warmth, empathy, trust, sense of security, identity stability, consistent emotional closeness, and stability in relationships with significant others. (It is not suggested that one should trust everyone, for example.) Concepts regarding internal representations (also sometimes termed, "introspects," "self and object representations," or "internalization of self and other") although often attributed to Melanie Klein, were actually first mentioned by Sigmund Freud in his early concepts of drive theory ("Three Essays on the Theory of Sexuality", 1905). Freud's 1917 paper "Mourning and Melancholia", for example, hypothesized that unresolved grief was caused by the survivor's internalized image of the deceased becoming fused with that of the survivor, and then the survivor shifting unacceptable anger toward the deceased onto the now complex self-image.
Vamik Volkan, in "Linking Objects and Linking Phenomena", expanded on Freud's thoughts on this, describing the syndromes of "Established pathological mourning" vs. "reactive depression" based on similar dynamics. Melanie Klein's hypotheses regarding internalization during the first year of life, leading to paranoid and depressive positions, were later challenged by René Spitz (e.g., "The First Year of Life", 1965), who divided the first year of life into a coenesthetic phase of the first six months, and then a diacritic phase for the second six months. Margaret Mahler (Mahler, Fine, and Bergman, "The Psychological Birth of the Human Infant", 1975) and her group, first in New York, then in Philadelphia, described distinct phases and subphases of child development leading to "separation-individuation" during the first three years of life, stressing the importance of constancy of parental figures, in the face of the child's destructive aggression, to the child's internalizations, stability of affect management, and ability to develop healthy autonomy.
John Frosch, Otto Kernberg, Salman Akhtar and Sheldon Bach have developed the theory of self and object constancy as it affects adult psychiatric problems such as psychosis and borderline states. Peter Blos described (in a book called "On Adolescence", 1960) how similar separation-individuation struggles occur during adolescence, of course with a different outcome from the first three years of life: the teen usually, eventually, leaves the parents' house (this varies with the culture). During adolescence, Erik Erikson (1950–1960s) described the "identity crisis," that involves identity-diffusion anxiety. In order for an adult to be able to experience "Warm-ETHICS" (warmth, empathy, trust, holding environment (Winnicott), identity, closeness, and stability) in relationships (see Blackman, "101 Defenses: How the Mind Shields Itself", 2001), the teenager must resolve the problems with identity and redevelop self and object constancy.
Self psychology.
Self psychology emphasizes the development of a stable and integrated sense of self through empathic contacts with other humans, primary significant others conceived of as "selfobjects." Selfobjects meet the developing self's needs for mirroring, idealization, and twinship, and thereby strengthen the developing self. The process of treatment proceeds through "transmuting internalizations" in which the patient gradually internalizes the selfobject functions provided by the therapist.
Self psychology was proposed originally by Heinz Kohut, and has been further developed by Arnold Goldberg, Frank Lachmann, Paul and Anna Ornstein, Marian Tolpin, and others.
Jacques Lacan and Lacanian psychoanalysis.
Lacanian psychoanalysis, which integrates psychoanalysis with structural linguistics and Hegelian philosophy, is especially popular in France and parts of Latin America. Lacanian psychoanalysis is a departure from the traditional British and American psychoanalysis, which is predominantly Ego psychology. Jacques Lacan frequently used the phrase "retourner à Freud" ("return to Freud") in his seminars and writings, as he claimed that his theories were an extension of Freud's own, contrary to those of Anna Freud, the Ego Psychology, object relations and "self" theories and also claims the necessity of reading Freud's complete works, not only a part of them. Lacan's concepts concern the "mirror stage", the "Real", the "Imaginary", and the "Symbolic", and the claim that "the unconscious is structured as a language."
Though a major influence on psychoanalysis in France and parts of Latin America, Lacan and his ideas have taken longer to be translated into English and he has thus had a lesser impact on psychoanalysis and psychotherapy in the English-speaking world. In the UK and the US, his ideas are most widely used to analyze texts in literary theory. Due to his increasingly critical stance towards the deviation from Freud's thought, often singling out particular texts and readings from his colleagues, Lacan was excluded from acting as a training analyst in the IPA, thus leading him to create his own school in order to maintain an institutional structure for the many candidates who desired to continue their analysis with him.
Interpersonal psychoanalysis.
Interpersonal psychoanalysis accents the nuances of interpersonal interactions, particularly how individuals protect themselves from anxiety by establishing collusive interactions with others, and the relevance of actual experiences with other persons developmentally (e.g. family and peers) as well as in the present. This is contrasted with the primacy of intrapsychic forces, as in classical psychoanalysis . Interpersonal theory was first introduced by Harry Stack Sullivan, MD, and developed further by Frieda Fromm-Reichmann, Clara Thompson, Erich Fromm, and others who contributed to the founding of the William Alanson White Institute and Interpersonal Psychoanalysis in general.
Culturalist psychoanalysis.
Some psychoanalysts have been labeled "culturalist", because of the prominence they attributed culture in the genesis of behavior. Among others, Erich Fromm, Karen Horney, Harry Stack Sullivan, have been called culturalist psychoanalysts. They were famously in conflict with orthodox psychoanalysts.
Feminist psychoanalysis.
Feminist theories of psychoanalysis emerged towards the second half of the 20th century, in an effort to articulate the feminine, the maternal and sexual difference and development from the point of view of female subjects. For Freud, male is subject and female is object. For Freud, Winnicott and the object relations theories, the mother is structured as the object of the infant's rejection (Freud) and destruction (Winnicott). For Lacan, the “woman” can either accept the phallic symbolic as an object or incarnate a lack in the symbolic dimension that informs the structure of the human subject. Feminist psychoanalysis is mainly post-Freudian and post-Lacanian with theorists like Toril Moi, Joan Copjec, Juliet Mitchell, Teresa Brennan and Griselda Pollock that rethinks Art and Mythology following French feminist psychoanalysis, the gaze and sexual difference in, of and from the feminine. French theorists like Luce Irigaray challenges the phallogocentrism. Bracha Ettinger offers a "matrixial" subject's dimension that brings into account the prenatal stage (matrixial connectivity) and suggests a feminine-maternal Eros, matrixial gaze and Primal mother-phantasies. Jessica Benjamin addresses the question of the feminine and love. Feminist psychoanalysis informs and includes gender, queer and post-feminist theories.
Adaptive paradigm of psychoanalysis and psychotherapy.
The “adaptive paradigm of psychotherapy” develops out of the work of Robert Langs. The adaptive paradigm interprets psychic conflict primarily in terms of conscious and unconscious adaptation to reality. Langs’ recent work in some measure returns to the earlier Freud, in that Langs prefers a modified version of the topographic model of the mind (conscious, preconscious, and unconscious) over the structural model (id, ego, and super-ego), including the former’s emphasis on trauma (though Langs looks to death-related traumas rather than sexual traumas). At the same time, Langs’ model of the mind differs from Freud’s in that it understands the mind in terms of evolutionary biological principles.
Relational psychoanalysis.
Relational psychoanalysis combines interpersonal psychoanalysis with object-relations theory and with inter-subjective theory as critical for mental health, was introduced by Stephen Mitchell. Relational psychoanalysis stresses how the individual's personality is shaped by both real and imagined relationships with others, and how these relationship patterns are re-enacted in the interactions between analyst and patient. In New York, key proponents of relational psychoanalysis include Lew Aron, Jessica Benjamin, and Adrienne Harris. Fonagy and Target, in London, have propounded their view of the necessity of helping certain detached, isolated patients, develop the capacity for "mentalization" associated with thinking about relationships and themselves. Arietta Slade, Susan Coates, and Daniel Schechter in New York have additionally contributed to the application of relational psychoanalysis to treatment of the adult patient-as-parent, the clinical study of mentalization in parent-infant relationships, and the intergenerational transmission of attachment and trauma.
Interpersonal-relational psychoanalysis.
The term interpersonal-relational psychoanalysis is often used as a professional identification. Psychoanalysts under this broader umbrella debate about what precisely are the differences between the two schools, without any current clear consensus.
Intersubjective psychoanalysis.
The term "intersubjectivity" was introduced in psychoanalysis by George E. Atwood and Robert Stolorow (1984). Intersubjective approaches emphasize how both personality development and the therapeutic process are influenced by the interrelationship between the patient's subjective perspective and that of others. The authors of the interpersonal-relational and intersubjective approaches: Otto Rank, Heinz Kohut, Stephen A. Mitchell, Jessica Benjamin, Bernard Brandchaft, J. Fosshage, Donna M.Orange, Arnold "Arnie" Mindell, Thomas Ogden, Owen Renik, Irwin Z. Hoffman, Harold Searles, Colwyn Trewarthen, Edgar A. Levenson, Jay R. Greenberg, Edward R. Ritvo, Beatrice Beebe, Frank M. Lachmann, Herbert Rosenfeld and Daniel Stern.
Modern psychoanalysis.
"Modern psychoanalysis" is a term coined by Hyman Spotnitz and his colleagues to describe a body of theoretical and clinical approaches that aim to extend Freud's theories so as to make them applicable to the full spectrum of emotional disorders and broaden the potential for treatment to pathologies thought to be untreatable by classical methods . Interventions based on this approach are primarily intended to provide an emotional-maturational communication to the patient, rather than to promote intellectual insight. These interventions, beyond insight directed aims, are used to resolve resistances that are presented in the clinical setting. This school of psychoanalysis has fostered training opportunities for students in the United States and from countries worldwide. Its journal Modern Psychoanalysis has been published since 1976.
Psychopathology (mental disturbances).
Adult patients.
The various psychoses involve deficits in the autonomous ego functions (see above) of integration (organization) of thought, in abstraction ability, in relationship to reality and in reality testing. In depressions with psychotic features, the self-preservation function may also be damaged (sometimes by overwhelming depressive affect). Because of the integrative deficits (often causing what general psychiatrists call "loose associations," "blocking," "flight of ideas," "verbigeration," and "thought withdrawal"), the development of self and object representations is also impaired. Clinically, therefore, psychotic individuals manifest limitations in warmth, empathy, trust, identity, closeness and/or stability in relationships (due to problems with self-object fusion anxiety) as well.
In patients whose autonomous ego functions are more intact, but who still show problems with object relations, the diagnosis often falls into the category known as "borderline." Borderline patients also show deficits, often in controlling impulses, affects, or fantasies – but their ability to test reality remains more or less intact. Adults who do not experience guilt and shame, and who indulge in criminal behavior, are usually diagnosed as psychopaths, or, using DSM-IV-TR, antisocial personality disorder.
Panic, phobias, conversions, obsessions, compulsions and depressions (analysts call these "neurotic symptoms") are not usually caused by deficits in functions. Instead, they are caused by intrapsychic conflicts. The conflicts are generally among sexual and hostile-aggressive wishes, guilt and shame, and reality factors. The conflicts may be conscious or unconscious, but create anxiety, depressive affect, and anger. Finally, the various elements are managed by defensive operations – essentially shut-off brain mechanisms that make people unaware of that element of conflict. "Repression" is the term given to the mechanism that shuts thoughts out of consciousness. "Isolation of affect" is the term used for the mechanism that shuts sensations out of consciousness. Neurotic symptoms may occur with or without deficits in ego functions, object relations, and ego strengths. Therefore, it is not uncommon to encounter obsessive-compulsive schizophrenics, panic patients who also suffer with borderline personality disorder, etc.
This section above is partial to ego psychoanalytic theory "autonomous ego functions." As the "autonomous ego functions" theory is only a theory, it may yet be proven incorrect.
Childhood origins.
Freudian theories believe that adult problems can be traced to unresolved conflicts from certain phases of childhood and adolescence, caused by fantasy, stemming from their own drives. Freud, based on the data gathered from his patients early in his career, suspected that neurotic disturbances occurred when children were sexually abused in childhood (the so-called "seduction theory"). Later, Freud came to believe that, although child abuse occurs, neurotic symptoms were not associated with this. He believed that neurotic people often had unconscious conflicts that involved incestuous fantasies deriving from different stages of development. He found the stage from about three to six years of age (preschool years, today called the "first genital stage") to be filled with fantasies of having romantic relationships with both parents. Arguments were quickly generated in early 20th-century Vienna about whether adult seduction of children, i.e. child sexual abuse, was the basis of neurotic illness. There still is no complete agreement, although nowadays professionals recognize the negative effects of child sexual abuse on mental health.
Many psychoanalysts who work with children have studied the actual effects of child abuse, which include ego and object relations deficits and severe neurotic conflicts. Much research has been done on these types of trauma in childhood, and the adult sequelae of those. In studying the childhood factors that start neurotic symptom development, Freud found a constellation of factors that, for literary reasons, he termed the Oedipus complex (based on the play by Sophocles, "Oedipus Rex", where the protagonist unwittingly kills his father Laius and marries his mother Jocasta). The validity of the Oedipus complex is now widely disputed and rejected. The shorthand term, "oedipal" — later explicated by Joseph J. Sandler in "On the Concept Superego" (1960) and modified by Charles Brenner in "The Mind in Conflict" (1982) — refers to the powerful attachments that children make to their parents in the preschool years. These attachments involve fantasies of sexual relationships with either (or both) parent, and, therefore, competitive fantasies toward either (or both) parents. Humberto Nagera (1975) has been particularly helpful in clarifying many of the complexities of the child through these years.
"Positive" and "negative" oedipal conflicts have been attached to the heterosexual and homosexual aspects, respectively. Both seem to occur in development of most children. Eventually, the developing child's concessions to reality (that they will neither marry one parent nor eliminate the other) lead to identifications with parental values. These identifications generally create a new set of mental operations regarding values and guilt, subsumed under the term "superego." Besides superego development, children "resolve" their preschool oedipal conflicts through channeling wishes into something their parents approve of ("sublimation") and the development, during the school-age years ("latency") of age-appropriate obsessive-compulsive defensive maneuvers (rules, repetitive games).
Treatment.
Using the various analytic and psychological techniques to assess mental problems, some believe that there are particular constellations of problems that are especially suited for analytic treatment (see below) whereas other problems might respond better to medicines and other interpersonal interventions. To be treated with psychoanalysis, whatever the presenting problem, the person requesting help must demonstrate a desire to start an analysis. The person wishing to start an analysis must have some capacity for speech and communication. As well, they need to be able to have or develop trust and insight within the psychoanalytic session. Potential patients must undergo a preliminary stage of treatment to assess their amenability to psychoanalysis at that time, and also to enable the analyst to form a working psychological model, which the analyst will use to direct the treatment. Psychoanalysts mainly work with neurosis and hysteria in particular; however, adapted forms of psychoanalysis are used in working with schizophrenia and other forms of psychosis or mental disorder. Finally, if a prospective patient is severely suicidal a longer preliminary stage may be employed, sometimes with sessions which have a twenty minute break in the middle. There are numerous modifications in technique under the heading of psychoanalysis due to the individualistic nature of personality in both analyst and patient.
The most common problems treatable with psychoanalysis include: phobias, conversions, compulsions, obsessions, anxiety attacks, depressions, sexual dysfunctions, a wide variety of relationship problems (such as dating and marital strife), and a wide variety of character problems (for example, painful shyness, meanness, obnoxiousness, workaholism, hyperseductiveness, hyperemotionality, hyperfastidiousness). The fact that many of such patients also demonstrate deficits above makes diagnosis and treatment selection difficult.
Analytical organizations such as the IPA, APsaA and the European Federation for Psychoanalytic Psychotherapy have established procedures and models for the indication and practice of psychoanalytical therapy for trainees in analysis. The match between the analyst and the patient can be viewed as another contributing factor for the indication and contraindication for psychoanalytic treatment. The analyst decides whether the patient is suitable for psychoanalysis. This decision made by the analyst, besides made on the usual indications and pathology, is also based to a certain degree by the "fit" between analyst and patient. A person's suitability for analysis at any particular time is based on their desire to know something about where their illness has come from. Someone who is not suitable for analysis expresses no desire to know more about the root causes of their illness.
An evaluation may include one or more other analysts' independent opinions and will include discussion of the patient's financial situation and insurances.
Techniques.
The basic method of psychoanalysis is interpretation of the patient's unconscious conflicts that are interfering with current-day functioning – conflicts that are causing painful symptoms such as phobias, anxiety, depression, and compulsions. Strachey (1936) stressed that figuring out ways the patient distorted perceptions about the analyst led to understanding what may have been forgotten (also see Freud's paper "Repeating, Remembering, and Working Through"). In particular, unconscious hostile feelings toward the analyst could be found in symbolic, negative reactions to what Robert Langs later called the "frame" of the therapy – the setup that included times of the sessions, payment of fees, and necessity of talking. In patients who made mistakes, forgot, or showed other peculiarities regarding time, fees, and talking, the analyst can usually find various unconscious "resistances" to the flow of thoughts (sometimes called free association).
When the patient reclines on a couch with the analyst out of view, the patient tends to remember more, experiences more resistance and transference, and is able to reorganize thoughts after the development of insight – through the interpretive work of the analyst. Although fantasy life can be understood through the examination of dreams, masturbation fantasies (cf. Marcus, I. and Francis, J. (1975), "Masturbation from Infancy to Senescence") are also important. The analyst is interested in how the patient reacts to and avoids such fantasies (cf. Paul Gray (1994), "The Ego and the Analysis of Defense"). Various memories of early life are generally distorted – Freud called them "screen memories" – and in any case, very early experiences (before age two) – cannot be remembered (See the child studies of Eleanor Galenson on "evocative memory").
Variations in technique.
There is what is known among psychoanalysts as "classical technique," although Freud throughout his writings deviated from this considerably, depending on the problems of any given patient. Classical technique was summarized by Allan Compton, MD, as comprising instructions (telling the patient to try to say what's on their mind, including interferences); exploration (asking questions); and clarification (rephrasing and summarizing what the patient has been describing). As well, the analyst can also use confrontation to bringing an aspect of functioning, usually a defense, to the patient's attention. The analyst then uses a variety of interpretation methods, such as dynamic interpretation (explaining how being too nice guards against guilt, e.g. – defense vs. affect); genetic interpretation (explaining how a past event is influencing the present); resistance interpretation (showing the patient how they are avoiding their problems); transference interpretation (showing the patient ways old conflicts arise in current relationships, including that with the analyst); or dream interpretation (obtaining the patient's thoughts about their dreams and connecting this with their current problems). Analysts can also use reconstruction to estimate what may have happened in the past that created some current issue.
These techniques are primarily based on conflict theory (see above). As object relations theory evolved, supplemented by the work of Bowlby, Ainsworth, and Beebe, techniques with patients who had more severe problems with basic trust (Erikson, 1950) and a history of maternal deprivation (see the works of Augusta Alpert) led to new techniques with adults. These have sometimes been called interpersonal, intersubjective (cf. Stolorow), relational, or corrective object relations techniques. These techniques include expressing an empathic attunement to the patient or warmth; exposing a bit of the analyst's personal life or attitudes to the patient; allowing the patient autonomy in the form of disagreement with the analyst (cf. I.H. Paul, "Letters to Simon".); and explaining the motivations of others which the patient misperceives. Ego psychological concepts of deficit in functioning led to refinements in supportive therapy. These techniques are particularly applicable to psychotic and near-psychotic (cf., Eric Marcus, "Psychosis and Near-psychosis") patients. These supportive therapy techniques include discussions of reality; encouragement to stay alive (including hospitalization); psychotropic medicines to relieve overwhelming depressive affect or overwhelming fantasies (hallucinations and delusions); and advice about the meanings of things (to counter abstraction failures).
The notion of the "silent analyst" has been criticized. Actually, the analyst listens using Arlow's approach as set out in "The Genesis of Interpretation"), using active intervention to interpret resistances, defenses creating pathology, and fantasies. Silence is not a technique of psychoanalysis (also see the studies and opinion papers of Owen Renik, MD). "Analytic neutrality" is a concept that does not mean the analyst is silent. It refers to the analyst's position of not taking sides in the internal struggles of the patient. For example, if a patient feels guilty, the analyst might explore what the patient has been doing or thinking that causes the guilt, but not reassure the patient not to feel guilty. The analyst might also explore the identifications with parents and others that led to the guilt.
Interpersonal-Relational psychoanalysts emphasize the notion that it is impossible to be neutral. Sullivan introduced the term "participant-observer" to indicate the analyst inevitably interacts with the analysand, and suggested the detailed inquiry as an alternative to interpretation. The detailed inquiry involves noting where the analysand is leaving out important elements of an account and noting when the story is obfuscated, and asking careful questions to open up the dialogue.
Group therapy and play therapy.
Although single-client sessions remain the norm, psychoanalytic theory has been used to develop other types of psychological treatment. Psychoanalytic group therapy was pioneered by Trigant Burrow, Joseph Pratt, Paul F. Schilder, Samuel R. Slavson, Harry Stack Sullivan, and Wolfe. Child-centered counseling for parents was instituted early in analytic history by Freud, and was later further developed by Irwin Marcus, Edith Schulhofer, and Gilbert Kliman. Psychoanalytically based couples therapy has been promulgated and explicated by Fred Sander, MD. Techniques and tools developed in the first decade of the 21st century have made psychoanalysis available to patients who were not treatable by earlier techniques. This meant that the analytic situation was modified so that it would be more suitable and more likely to be helpful for these patients. M.N. Eagle (2007) believes that psychoanalysis cannot be a self-contained discipline but instead must be open to influence from and integration with findings and theory from other disciplines.
Psychoanalytic constructs have been adapted for use with children with treatments such as play therapy, art therapy, and storytelling. Throughout her career, from the 1920s through the 1970s, Anna Freud adapted psychoanalysis for children through play. This is still used today for children, especially those who are preadolescent (see Leon Hoffman, New York Psychoanalytic Institute Center for Children). Using toys and games, children are able to demonstrate, symbolically, their fears, fantasies, and defenses; although not identical, this technique, in children, is analogous to the aim of free association in adults. Psychoanalytic play therapy allows the child and analyst to understand children's conflicts, particularly defenses such as disobedience and withdrawal, that have been guarding against various unpleasant feelings and hostile wishes. In art therapy, the counselor may have a child draw a portrait and then tell a story about the portrait. The counselor watches for recurring themes—regardless of whether it is with art or toys.
Cultural variations.
Psychoanalysis can be adapted to different cultures, as long as the therapist or counselor understands the client's culture. For example, Tori and Blimes found that defense mechanisms were valid in a normative sample of 2,624 Thais. The use of certain defense mechanisms was related to cultural values. For example Thais value calmness and collectiveness (because of Buddhist beliefs), so they were low on regressive emotionality. Psychoanalysis also applies because Freud used techniques that allowed him to get the subjective perceptions of his patients. He takes an objective approach by not facing his clients during his talk therapy sessions. He met with his patients wherever they were, such as when he used free association — where clients would say whatever came to mind without self-censorship. His treatments had little to no structure for most cultures, especially Asian cultures. Therefore, it is more likely that Freudian constructs will be used in structured therapy (Thompson, et al., 2004). In addition, Corey postulates that it will be necessary for a therapist to help clients develop a cultural identity as well as an ego identity.
Cost and length of treatment.
The cost to the patient of psychoanalytic treatment ranges widely from place to place and between practitioners. Low-fee analysis is often available in a psychoanalytic training clinic and graduate schools. Otherwise, the fee set by each analyst varies with the analyst's training and experience. Since, in most locations in the United States, unlike in Ontario and Germany, classical analysis (which usually requires sessions three to five times per week) is not covered by health insurance, many analysts may negotiate their fees with patients whom they feel they can help, but who have financial difficulties. The modifications of analysis, which include dynamic therapy, brief therapies, and certain types of group therapy (cf. Slavson, S. R., "A Textbook in Analytic Group Therapy"), are carried out on a less frequent basis – usually once, twice, or three times a week – and usually the patient sits facing the therapist. As a result of the defense mechanisms and the lack of access to the unfathomable elements of the unconscious, psychoanalysis can be an expansive process that involves 2 to 5 sessions per week for several years. This type of therapy relies on the belief that reducing the symptoms will not actually help with the root causes or irrational drives. The analyst typically is a 'blank screen', disclosing very little about themselves in order that the client can use the space in the relationship to work on their unconscious without interference from outside.
The psychoanalyst uses various techniques as encouragement for the client to develop insights into their behavior and the meanings of symptoms, including ink blots, parapraxes, free association, interpretation (including dream analysis), resistance analysis and transference analysis.
Many studies have also been done on briefer "dynamic" treatments; these are more expedient to measure, and shed light on the therapeutic process to some extent. Brief Relational Therapy (BRT), Brief Psychodynamic Therapy (BPT), and Time-Limited Dynamic Therapy (TLDP) limit treatment to 20–30 sessions. On average, classical analysis may last 5.7 years, but for phobias and depressions uncomplicated by ego deficits or object relations deficits, analysis may run for a shorter period of time. Longer analyses are indicated for those with more serious disturbances in object relations, more symptoms, and more ingrained character pathology.
Training and research.
Psychoanalytic training in the United States, in most locations, involves personal analytic treatment for the trainee, conducted confidentially, with no report to the Education Committee of the Analytic Training Institute; approximately 600 hours of class instruction, with a standard curriculum, over a four-year period. Classes are often a few hours per week, or for a full day or two every other weekend during the academic year; this varies with the institute; and supervision once per week, with a senior analyst, on each analytic treatment case the trainee has. The minimum number of cases varies between institutes, often two to four cases. Male and female cases are required. Supervision must go on for at least a few years on one or more cases. Supervision is done in the supervisor's office, where the trainee presents material from the analytic work that week, examines the unconscious conflicts with the supervisor, and learns, discusses, and is advised about technique.
Many psychoanalytic training centers in the United States have been accredited by special committees of the APsaA or the IPA. Because of theoretical differences, other independent institutes arose, usually founded by psychologists, who until 1987 were not permitted access to psychoanalytic training institutes of the APsaA. Currently there are between 75 and 100 independent institutes in the United States. As well, other institutes are affiliated to other organizations such as the American Academy of Psychoanalysis and Dynamic Psychiatry, and the National Association for the Advancement of Psychoanalysis. At most psychoanalytic institutes in the United States, qualifications for entry include a terminal degree in a mental health field, such as Ph.D., Psy.D., M.S.W., or M.D. A few institutes restrict applicants to those already holding an M.D. or Ph.D., and most institutes in Southern California confer a Ph.D. or Psy.D. in psychoanalysis upon graduation, which involves completion of the necessary requirements for the state boards that confer that doctoral degree.The first training institute in America to educate non-medical psychoanalysts was The National Psychological Association for Psychoanalysis., (1978) in New York City. It was founded by the analyst Theodor Reik. The Contemporary Freudian (originally the New York Freudian Society) an offshoot of the National Psychological Association has a branch in Washington, DC. It is a component society/institute or the IPA.
Some psychoanalytic training has been set up as a post-doctoral fellowship in university settings, such as at Duke University, Yale University, New York University, Adelphi University, and Columbia University. Other psychoanalytic institutes may not be directly associated with universities, but the faculty at those institutes usually hold contemporaneous faculty positions with psychology Ph.D. programs and/or with medical school psychiatry residency programs.
The IPA is the world's primary accrediting and regulatory body for psychoanalysis. Their mission is to assure the continued vigor and development of psychoanalysis for the benefit of psychoanalytic patients. It works in partnership with its 70 constituent organizations in 33 countries to support 11,500 members. In the US, there are 77 psychoanalytical organizations, institutes associations in the United States, which are spread across the states of America. APSaA has 38 affiliated societies which have 10 or more active members who practice in a given geographical area. The aims of APSaA and other psychoanalytical organizations are: provide ongoing educational opportunities for its members, stimulate the development and research of psychoanalysis, provide training and organize conferences. There are eight affiliated study groups in the USA (two of them are in Latin America). A study group is the first level of integration of a psychoanalytical body within the IPA, followed by a provisional society and finally a member society.
The Division of Psychoanalysis (39) of the American Psychological Association (APA) was established in the early 1980s by several psychologists. Until the establishment of the Division of Psychoanalysis, psychologists who had trained in independent institutes had no national organization. The Division of Psychoanalysis now has approximately 4,000 members and approximately 30 local chapters in the United States. The Division of Psychoanalysis holds two annual meetings or conferences and offers continuing education in theory, research and clinical technique, as do their affiliated local chapters. The European Psychoanalytical Federation (EPF) is the organization which consolidates all European psychoanalytic societies. This organization is affiliated with the IPA. In 2002 there were approximately 3,900 individual members in 22 countries, speaking 18 different languages. There are also 25 psychoanalytic societies.
The American Association of Psychoanalysis in Clinical Social Work (AAPCSW) was established by Crayton Rowe in 1980 as a division of the Federation of Clinical Societies of Social Work and became an independent entity in 1990. Until 2007 it was known as the National Membership Committee on Psychoanalysis. The organization was originally founded because although social workers represented the larger number of people who were training to be psychoanalysts, they were underrepresented as supervisors and teachers at the institutes they attended. AAPCSW now has over 1000 members and has over 20 chapters. It holds a bi-annual national conference and numerous annual local conferences.
Experiences of psychoanalysts and psychoanalytic psychotherapists and research into infant and child development have led to new insights. Theories have been further developed and the results of empirical research are now more integrated in the psychoanalytic theory.
Psychoanalysis in Britain.
The London Psychoanalytical Society was founded by Ernest Jones on 30 October 1913. With the expansion of psychoanalysis in the United Kingdom the Society was renamed the British Psychoanalytical Society in 1919. Soon after, the Institute of Psychoanalysis was established to administer the Society’s activities. These include: the training of psychoanalysts, the development of the theory and practice of psychoanalysis, the provision of treatment through The London Clinic of Psychoanalysis, the publication of books in The New Library of Psychoanalysis and Psychoanalytic Ideas. The Institute of Psychoanalysis also publishes "The International Journal of Psychoanalysis", maintains a library, furthers research, and holds public lectures. The society has a Code of Ethics and an Ethical Committee. The society, the institute and the clinic are all located at Byron House.
The society is a component of the IPA, a body with members on all five continents that safeguards professional and ethical practice. The society is a member of the British Psychoanalytic Council (BPC); the BPC publishes a register of British psychoanalysts and psychoanalytical psychotherapists. All members of the British Psychoanalytical Society are required to undertake continuing professional development.
Members of the Society have included Michael Balint, Wilfred Bion, John Bowlby, Anna Freud, Melanie Klein, Joseph J. Sandler, and Donald Winnicott.
The Institute of Psychoanalysis is the foremost publisher of psychoanalytic literature. The 24-volume "Standard Edition of the Complete Psychological Works of Sigmund Freud" was conceived, translated, and produced under the direction of the British Psychoanalytical Society. The Society, in conjunction with Random House, will soon publish a new, revised and expanded Standard Edition. With the New Library of Psychoanalysis the Institute continues to publish the books of leading theorists and practitioners. "The International Journal of Psychoanalysis" is published by the Institute of Psychoanalysis. Now in its 84th year, it has one of the largest circulations of any psychoanalytic journal.
Research.
Over a hundred years of case reports and studies in the journal "Modern Psychoanalysis", the "Psychoanalytic Quarterly", the "International Journal of Psychoanalysis" and the "Journal of the American Psychoanalytic Association" have analyzed the efficacy of analysis in cases of neurosis and character or problems. Psychoanalysis modified by object relations techniques has been shown to be effective in many cases of ingrained problems of intimacy and relationship (cf. the many books of Otto Kernberg). As a therapeutic treatment, psychoanalytic techniques may be useful in a one-session consultation. Psychoanalytic treatment, in other situations, may run from about a year to many years, depending on the severity and complexity of the pathology.
Psychoanalytic theory has, from its inception, been the subject of criticism and controversy. Freud remarked on this early in his career, when other physicians in Vienna ostracized him for his findings that hysterical conversion symptoms were not limited to women. Challenges to analytic theory began with Otto Rank and Alfred Adler (turn of the 20th century), continued with behaviorists (e.g. Wolpe) into the 1940s and '50s, and have persisted (e.g. Miller). Criticisms come from those who object to the notion that there are mechanisms, thoughts or feelings in the mind that could be unconscious. Criticisms also have been leveled against the discovery of "infantile sexuality" (the recognition that children between ages two and six imagine things about procreation). Criticisms of theory have led to variations in analytic theories, such as the work of Ronald Fairbairn, Michael Balint, and John Bowlby. In the past 30 years or so, the criticisms have centered on the issue of empirical verification, in spite of many empirical, prospective research studies that have been empirically validated (e.g., See the studies of Barbara Milrod, at Cornell University Medical School, et al.). In the scientific literature there are some research supporting some of Freud's ideas, e.g. unconsciousness, repression etc.
Psychoanalysis has been used as a research tool into childhood development (cf. the journal "The Psychoanalytic Study of the Child"), and has developed into a flexible, effective treatment for certain mental disturbances. In the 1960s, Freud's early (1905) thoughts on the childhood development of female sexuality were challenged; this challenge led to major research in the 1970s and 80s, and then to a reformulation of female sexual development that corrected some of Freud's concepts. Also see the various works of Eleanor Galenson, Nancy Chodorow, Karen Horney, Françoise Dolto, Melanie Klein, Selma Fraiberg, and others. Most recently, psychoanalytic researchers who have integrated attachment theory into their work, including Alicia Lieberman, Susan Coates, and Daniel Schechter have explored the role of parental traumatization in the development of young children's mental representations of self and others.
There are different forms of psychoanalysis and psychotherapies in which psychoanalytic thinking is practiced. Besides classical psychoanalysis there is for example psychoanalytic psychotherapy, a therapeutic approach which widens "the accessibility of psychoanalytic theory and clinical practices that had evolved over 100 plus years to a larger number of individuals." Other examples of well known therapies which also use insights of psychoanalysis are mentalization-based treatment (MBT), and transference focused psychotherapy (TFP). There is also a continuing influence of psychoanalytic thinking in different settings in the mental health care. To give an example: in the psycho-therapeutic training in the Netherlands, psychoanalytic and system therapeutic theories, drafts, and techniques are combined and integrated. Other psychoanalytic schools include the Kleinian, Lacanian, and Winnicottian schools.
Evaluation of effectiveness.
The effectiveness of strict psychoanalysis is difficult to gauge; therapy as Freud intended it relies too much on the interpretation of the therapist which cannot be proven. The effectiveness of more modern, developed techniques can be gauged. Meta-analyses in 2012 and 2013 come to the conclusion that there is support or evidence for the efficacy of psychoanalytic therapy, thus further research is needed. Other meta-analyses published in the recent years showed psychoanalysis and psychodynamic therapy to be effective, with outcomes comparable or greater than other kinds of psychotherapy or antidepressant drugs, but these arguments have also been subjected to various criticisms.
In 2011, the American Psychological Association made 103 comparisons between psychodynamic treatment and a non-dynamic competitor and found that 6 were superior, 5 were inferior, 28 had no difference and 63 were adequate. The study found that this could be used as a basis "to make psychodynamic psychotherapy an "empirically validated" treatment."
Meta-analyses of Short Term Psychodynamic Psychotherapy (STPP) have found effect sizes ranging from .34-.71 compared to no treatment and was found to be slightly better than other therapies in follow up. Other reviews have found an effect size of .78-.91 for somatic disorders compared to no treatment and .69 for treating depression. A 2012 meta-analysis by the "Harvard Review of Psychiatry" of Intensive Short-Term Dynamic Psychotherapy (I-STPP) found effect sizes ranging from .84 for interpersonal problems to 1.51 for depression. Overall I-STPP had an effect size of 1.18 compared to no treatment.
A system review of Long Term Psychodynamic Psychotherapy in 2009 found an overall effect size of .33. Others have found effect sizes of .44-.68.
According to a 2004 French review conducted by INSERM, psychoanalysis was presumed or proven effective at treating panic disorder, post-traumatic stress and personality disorders.
The world's largest randomized controlled trial on therapy with anorexia outpatients, the ANTOP-Study, published 2013 in "The Lancet", proved modified psychodynamic therapy to be more effective than cognitive behavioral therapy in the long term.
A 2001 systematic review of the medical literature by the Cochrane Collaboration concluded that no data exist demonstrating that psychodynamic psychotherapy is effective in treating schizophrenia and severe mental illness, and cautioned that medication should always be used alongside any type of talk therapy in schizophrenia cases. A French review from 2004 found the same. The Schizophrenia Patient Outcomes Research Team advises against the use of psychodynamic therapy in cases of schizophrenia, arguing that more trials are necessary to verify its effectiveness.
Criticism.
As a field of science.
The strongest reason for considering Freud a pseudo-scientist is that he claimed to have tested – and thus to have provided the most cogent grounds for accepting – theories which are either untestable or even if testable had not been tested. It is spurious claims to have tested an untestable or untested theory which are the most pertinent grounds for deeming Freud and his followers pseudoscientists...
—Frank Cioffi
Psychoanalysis continues to be practiced by psychiatrists, social workers, and other mental health professionals, however, its practice is less common today than in years past. The theoretical foundations of psychoanalysis lie in the same philosophical currents that lead to interpretive phenomenology rather than in those that lead to scientific positivism, making the theory largely incompatible with scientific approaches to the study of the mind.
Early critics of psychoanalysis believed that its theories were based too little on quantitative and experimental research, and too much on the clinical case study method. Some have accused Freud of fabrication, most famously in the case of Anna O. Others have speculated that patients suffered from now easily identifiable conditions unrelated to psychoanalysis; for instance, Anna O. is thought to have suffered from an organic impairment such as tuberculous meningitis or temporal lobe epilepsy and not hysteria (see modern interpretations).
E. Fuller Torrey, writing in "Witchdoctors and Psychiatrists" (1986), stated that psychoanalytic theories have no more scientific basis than the theories of traditional native healers, "witchdoctors" or modern "cult" alternatives such as est. Frank Cioffi, author of "Freud and the Question of Pseudoscience", cites false claims of a sound scientific verification of the theory and its elements as the strongest basis for classifying the work of Freud and his school as pseudoscience.
Psychologist Alice Miller charged psychoanalysis with being similar to the poisonous pedagogies, which she described in her book "For Your Own Good". She scrutinized and rejected the validity of Freud's drive theory, including the Oedipus complex, which, according to her and Jeffrey Masson, blames the child for the abusive sexual behavior of adults.
Psychologist Joel Kupfersmid investigated the validity of the Oedipus complex, examining its nature and origins. He concluded that there is little evidence to support the existence of the Oedipus complex.
Cognitive scientists have also criticized psychoanalysis. Linguist Noam Chomsky has criticized psychoanalysis for lacking a scientific basis. Steven Pinker considered Freudian theory unscientific for understanding the mind. Evolutionary biologist Steven Jay Gould considered psychoanalysis as influenced by pseudoscientific theories such as recapitulation theory. Psychologists Hans Eysenck and John F. Kihlstrom have also criticized the field as pseudoscience.
A French 2004 report from INSERM said that psychoanalytic therapy is far less effective than other psychotherapies (including cognitive behavioral therapy). It used a meta-analysis of numerous other studies to find whether the treatment was "proven" or "presumed" to be effective on different diseases. Numerous studies have shown that its efficacy is related to the quality of the therapist, rather than the psychoanalytic school or technique or training.
Both Freud and psychoanalysis have been criticized in very extreme terms. Exchanges between critics and defenders of psychoanalysis have often been so heated that they have come to be characterized as the "Freud Wars". The classic dismissal still belongs to Richard Feynman, who wrote off psychoanalysts as "witch doctors":
"If you look at all of the complicated ideas that they have developed in an infinitesimal amount of time, if you compare to any other of the sciences how long it takes to get one idea after the other, if you consider all the structures and inventions and complicated things, the ids and the egos, the tensions and the forces, and the pushes and the pulls, I tell you they can't all be there. It's too much for one brain or a few brains to have cooked up in such a short time."
Karl Popper argued that psychoanalysis is a pseudoscience because its claims are not testable and cannot be refuted; that is, they are not falsifiable. Karl Kraus, an Austrian satirist, was the subject of a book written by noted libertarian author Thomas Szasz. The book "Anti-Freud: Karl Kraus's Criticism of Psychoanalysis and Psychiatry", originally published under the name "Karl Kraus and the Soul Doctors", portrayed Kraus as a harsh critic of Sigmund Freud and of psychoanalysis in general. Other commentators, such as Edward Timms, author of "Karl Kraus – Apocalyptic Satirist", have argued that Kraus respected Freud, though with reservations about the application of some of his theories, and that his views were far less black-and-white than Szasz suggests. Adolf Grünbaum argues that psychoanalytic based theories are falsifiable, but that the causal claims of psychoanalysis are unsupported by the available clinical evidence. A prominent academic in positive psychology wrote that 'Thirty years ago, the cognitive revolution in psychology overthrew both Freud and the behaviorists, at least in academia. ... [T]hinking ... is not just a [result] of emotion or behavior. ... [E]motion is always generated by cognition, not the other way around.'
Michel Foucault and Gilles Deleuze claimed that the institution of psychoanalysis has become a center of power and that its confessional techniques resemble the Christian tradition. Jacques Lacan criticized the emphasis of some American and British psychoanalytical traditions on what he has viewed as the suggestion of imaginary "causes" for symptoms, and recommended the return to Freud. Together with Deleuze, Félix Guattari criticised the Oedipal structure. Luce Irigaray criticised psychoanalysis, employing Jacques Derrida's concept of phallogocentrism to describe the exclusion of the woman from Freudian and Lacanian psychoanalytical theories. Deleuze and Guattari, in their 1972 work "Anti-Œdipus", take the cases of Gérard Mendel, Bela Grunberger and Janine Chasseguet-Smirgel, prominent members of the most respected associations (IPa), to suggest that, traditionally, psychoanalysis enthusiastically embraces a police state.
Additionally, this type of therapy is a very time-consuming and is unlikely to provide efficient and timely answers to the underlying problems of the patients. As a result, patients need to be very self-motivated, have the time and monetary resources to commit to this type of therapy. During the process, the patient may experience painful and unpleasant memories that may have been repressed, which may actually result in further distress. Finally, as the patient is the one sharing and becoming vulnerable to his or her therapist, while the therapist remains mostly quiet and does not disclose much information about him or herself, there are ethical issues to this power dynamic.
Freudian theory.
Many aspects of Freudian theory are indeed out of date, and they should be: Freud died in 1939, and he has been slow to undertake further revisions. His critics, however, are equally behind the times, attacking Freudian views of the 1920s as if they continue to have some currency in their original form. Psychodynamic theory and therapy have evolved considerably since 1939 when Freud's bearded countenance was last sighted in earnest. Contemporary psychoanalysts and psychodynamic therapists no longer write much about ids and egos, nor do they conceive of treatment for psychological disorders as an archaeological expedition in search of lost memories.
—Drew Westen
An increasing amount of empirical research from academic psychologists and psychiatrists has begun to address this criticism. A survey of scientific research suggested that while personality traits corresponding to Freud's oral, anal, Oedipal, and genital phases can be observed, they do not necessarily manifest as stages in the development of children. These studies also have not confirmed that such traits in adults result from childhood experiences (Fisher & Greenberg, 1977, p. 399). However, these stages should not be viewed as crucial to modern psychoanalysis. What is crucial to modern psychoanalytic theory and practice is the power of the unconscious and the transference phenomenon.
The idea of "unconscious" is contested because human behavior can be observed while human mental activity has to be inferred. However, the unconscious is now a popular topic of study in the fields of experimental and social psychology (e.g., implicit attitude measures, fMRI, and PET scans, and other indirect tests). The idea of unconscious, and the transference phenomenon, have been widely researched and, it is claimed, validated in the fields of cognitive psychology and social psychology (Westen & Gabbard 2002), though a Freudian interpretation of unconscious mental activity is not held by the majority of cognitive psychologists. Recent developments in neuroscience have resulted in one side arguing that it has provided a biological basis for unconscious emotional processing in line with psychoanalytic theory i.e., neuropsychoanalysis (Westen & Gabbard 2002), while the other side argues that such findings make psychoanalytic theory obsolete and irrelevant.
Shlomo Kalo explains that Materialism that flourished in the 19th Century severely harmed religion and rejected whatever called spiritual. The institution of the confession priest in particular was badly damaged. The empty void that this institution left behind was swiftly occupied by the newborn psychoanalysis.
In his writings Kalo claims that psychoanalysis basic approach is erroneous. It represents the mainline wrong assumptions that happiness is unreachable and that the natural desire of a human being is to exploit his fellow men for his own pleasure and benefit.
Freud's psychoanalysis was criticized by his wife, Martha. René Laforgue reported Martha Freud saying, "I must admit that if I did not realize how seriously my husband takes his treatments, I should think that psychoanalysis is a form of pornography." To Martha there was something vulgar about psychoanalysis, and she dissociated herself from it. According to Marie Bonaparte, Martha was upset with her husband's work and his treatment of sexuality.
Jacques Derrida incorporated aspects of psychoanalytic theory into his theory of deconstruction in order to question what he called the 'metaphysics of presence'. Derrida also turns some of these ideas against Freud, to reveal tensions and contradictions in his work. For example, although Freud defines religion and metaphysics as displacements of the identification with the father in the resolution of the Oedipal complex, Derrida insists in "The Postcard: From Socrates to Freud and Beyond" that the prominence of the father in Freud's own analysis is itself indebted to the prominence given to the father in Western metaphysics and theology since Plato.
Literature.
</dl>
Analyses, discussions and critiques.
</dl>
Responses to critiques.
</dl>

</doc>
<doc id="23587" url="http://en.wikipedia.org/wiki?curid=23587" title="Peking (disambiguation)">
Peking (disambiguation)

Peking may refer to:

</doc>
<doc id="23588" url="http://en.wikipedia.org/wiki?curid=23588" title="Pinyin">
Pinyin

Pinyin, or Hanyu Pinyin, is the official phonetic system for transcribing the Mandarin pronunciations of Chinese characters into the Latin alphabet in the People's Republic of China, Taiwan (Republic of China), and Singapore. It is often used to teach Standard Chinese and a pinyin without diacritic markers is often used in foreign publications to spell Chinese names familiar to non-Chinese and may be used as an input method to enter Chinese characters into computers.
The Hanyu Pinyin system was developed in the 1950s based on earlier forms of romanization. It was published by the Chinese government in 1958 and revised several times. The International Organization for Standardization (ISO) adopted pinyin as an international standard in 1982. The system was adopted as the official standard in Taiwan in 2009, where it is used for romanization alone rather than for educational and computer input purposes.
The word "Hànyǔ" () means the spoken language of the Han people and "pīnyīn" () literally means "spelled-out sounds".
History before 1949.
In 1605, the Jesuit missionary Matteo Ricci published "Xizi Qiji" () in Beijing. This was the first book to use the Roman alphabet to write the Chinese language. Twenty years later, another Jesuit in China, Nicolas Trigault, issued his "Xi Ru Ermu Zi" () at Hangzhou. Neither book had much immediate impact on the way in which Chinese thought about their writing system, and the romanizations they described were intended more for Westerners than for the Chinese.
One of the earliest Chinese thinkers to relate Western alphabets to Chinese was late Ming to early Qing Dynasty scholar-official, Fang Yizhi (; 1611–1671).
The first late Qing reformer to propose that China adopt a system of spelling was Song Shu (1862–1910). A student of the great scholars Yu Yue and Zhang Taiyan, Song had been to Japan and observed the stunning effect of the "kana" syllabaries and Western learning there. This galvanized him into activity on a number of fronts, one of the most important being reform of the script. While Song did not himself actually create a system for spelling Sinitic languages, his discussion proved fertile and led to a proliferation of schemes for phonetic scripts.
In the early 1930s, Communist Party of China leaders trained in Moscow introduced a phonetic alphabet using Roman letters which had been developed in the Soviet Oriental Institute of Leningrad and originally intended to improve literacy in the Russian Far East. This Sin Wenz or "New Writing" was much more linguistically sophisticated than earlier alphabets, with the major exception that it did not indicate tones.
In 1940, several thousand members attended a Border Region Sin Wenz Society convention. Mao Zedong and Zhu De, head of the army, both contributed their calligraphy (in characters) for the masthead of the Sin Wenz Society's new journal. Outside the CCP, other prominent supporters included Sun Yat-sen's son, Sun Fo; Cai Yuanpei, the country's most prestigious educator; Tao Xingzhi, a leading educational reformer; and Lu Xun. Over thirty journals soon appeared written in Sin Wenz, plus large numbers of translations, biographies (including Lincoln, Franklin, Edison, Ford, and Charlie Chaplin), some contemporary Chinese literature, and a spectrum of textbooks. In 1940, the movement reached an apex when Mao's Border Region Government declared that the Sin Wenz had the same legal status as traditional characters in government and public documents. Many educators and political leaders looked forward to the day when they would be universally accepted and completely replace characters. Opposition arose, however, because the system was less well adapted to writing regional languages, and therefore would require learning Mandarin. Sin Wenzi fell into relative disuse during the following years.
History after 1949.
Pinyin was developed as part of a Chinese government project in the 1950s. One of the prominent figures was Zhou Youguang, who is often called "the father of pinyin", as he led a government committee in developing the romanization system. Zhou was working in a New York bank when he decided to return to China to help rebuild the country after the establishment of the People's Republic of China in 1949. Zhou became an economics professor in Shanghai, and in 1954, when China's Ministry of Education created a Committee for the Reform of the Chinese Written Language, Zhou was assigned the task of helping to develop a new romanization system.
"Hanyu Pinyin" was based on several preexisting systems: "(Gwoyeu Romatzyh" of 1928, "Latinxua Sin Wenz" of 1931, and the diacritic markings from "zhuyin)". "I’m not the father of pinyin," Zhou said years later, "I’m the son of pinyin. It’s [the result of] a long tradition from the later years of the Qing dynasty down to today. But we restudied the problem and revisited it and made it more perfect."
A first draft was published on February 12, 1956. The first edition of "Hanyu Pinyin" was approved and adopted at the Fifth Session of the 1st National People's Congress on February 11, 1958. It was then introduced to primary schools as a way to teach Standard Chinese pronunciation and used to improve the literacy rate among adults.
Beginning in the early 1980s, Western publications addressing Mainland China began using the Hanyu Pinyin romanization system instead of earlier romanization systems; this change followed the normalization of diplomatic relations between the United States and the PRC in 1979. In 2001, the PRC Government issued the "National Common Language Law", providing a legal basis for applying pinyin. The current specification of the orthographic rules is laid down in the National Standard GB/T 16159-2012.
Usage.
Pinyin superseded older romanization systems such as Wade-Giles (1859; modified 1892) and Chinese postal map romanization, and replaced zhuyin as the method of Chinese phonetic instruction in mainland China. The ISO adopted pinyin as the standard romanization for modern Chinese in 1982 (ISO 7098:1982, superseded by ISO 7098:1991); the United Nations followed suit in 1986. It has also been accepted by the government of Singapore, the United States' Library of Congress, the American Library Association, and many other international institutions.
The spelling of Chinese geographical or personal names in pinyin has become the most common way to transcribe them in English. Pinyin has also become the dominant method for entering Chinese text into computers in Mainland China, in contrast to Taiwan where Bopomofo is most commonly used.
Families outside of Taiwan who speak Mandarin as a mother tongue use pinyin to help children associate characters with spoken words which they already know. Chinese families outside of Taiwan who speak some other language as their mother tongue use the system to teach children Mandarin pronunciation when they learn vocabulary in elementary school.
Since 1958, pinyin has been actively used in adult education as well, making it easier for formerly illiterate people to continue with self-study after a short period of pinyin literacy instruction.
Pinyin has become a tool for many foreigners to learn the Mandarin pronunciation, and is used to explain both the grammar and spoken Mandarin coupled with Chinese characters ().
Books containing both Chinese characters and pinyin are often used by foreign learners of Chinese; pinyin's role in teaching pronunciation to foreigners and children is similar in some respects to furigana-based books (with hiragana letters written above or next to kanji, directly analogous to zhuyin) in Japanese or fully vocalised texts in Arabic ("vocalised Arabic").
The tone-marking diacritics are commonly omitted in popular news stories and even in scholarly works. This results in some degree of ambiguity as to which words are being represented.
Overview.
When a foreign writing system with one set of sounds and coding/decoding system is taken to write a language, certain compromises may have to be made. The result is that the decoding systems used in some foreign languages will enable non-native speakers to produce sounds more closely resembling the target language than will the coding/decoding system used by other foreign languages. Native speakers of English will decode pinyin spellings to fairly close approximations of Mandarin except in the case of certain speech sounds that are not ordinarily produced by most native speakers of English: "j, q, x, z, c, s, zh, ch, sh," and "r" exhibiting the greatest discrepancies. (When Chinese speakers call out these letters, they read them as: "ji, qi, xi, zi, ci, si, zhi, chi, shi," and "ri." The "i" in the last four sounds more like "r" and the use of "i" is purely a matter of convention.) 
In this system, the correspondence between the Roman letter and the sound is sometimes idiosyncratic, though not necessarily more so than the way the Latin script is employed in other languages. For example, the aspiration distinction between "b, d, g" and "p, t, k" is similar to that of English (in which the two sets are however also differentiated by voicing), but not to that of French. "Z" and "c" also have that distinction, pronounced as [ts] and [tsʰ]. From "s, z, c" come the digraphs "sh, zh, ch" by analogy with English "sh, ch." Although this introduces the novel combination "zh," it is internally consistent in how the two series are related, and reminds the trained reader that many Chinese pronounce "sh, zh, ch" as "s, z, c" (and English-speakers use "zh" to represent /ʒ/ in foreign languages such as Russian anyway). In the "x, j, q" series, the pinyin use of "x" is similar to its use in Portuguese, Galician, Catalan, Basque and Maltese; and the pinyin "q" is akin to its value in Albanian; both pinyin and Albanian pronunciations may sound similar to the "ch" to the untrained ear. Pinyin vowels are pronounced in a similar way to vowels in Romance languages.
The pronunciation and spelling of Chinese words are generally given in terms of initials and finals, which represent the "segmental phonemic" portion of the language, rather than letter by letter. Initials are initial consonants, while finals are all possible combinations of medials (semivowels coming before the vowel), the nucleus vowel, and coda (final vowel or consonant).
Initials and finals.
Unlike European languages, clusters of letters – initials () and finals () – and not consonant and vowel letters, form the fundamental elements in pinyin (and most other phonetic systems used to describe the Han language). Every Mandarin syllable can be spelled with exactly one initial followed by one final, except for the special syllable "er" or when a trailing "-r" is considered part of a syllable (see below). The latter case, though a common practice in some sub-dialects, is rarely used in official publications. One exception is the city Harbin (), whose name comes from the Manchu language.
Even though most initials contain a consonant, finals are not always simple vowels, especially in compound finals (), i.e., when a "medial" is placed in front of the final. For example, the medials [i] and [u] are pronounced with such tight openings at the beginning of a final that some native Chinese speakers (especially when singing) pronounce "yī" (, clothes, officially pronounced /í/) as /jí/ and "wéi" (, to enclose, officially pronounced /uěi/) as /wěi/ or /wuěi/. Often these medials are treated as separate from the finals rather than as part of them; this convention is followed in the chart of finals below.
Initials.
In each cell below, the bold letters indicate pinyin, and the brackets enclose the symbol in the International Phonetic Alphabet.
1 "r" may phonetically be [ʐ] (a voiced retroflex fricative) or [ɻ] (a retroflex approximant). This pronunciation varies among different speakers and is not two different phonemes.<br>
2 "y" is pronounced [ɥ] (a labial-palatal approximant) before "u".<br>
3 the letters "w" and "y" are not included in the table of initials in the official pinyin system. They are an orthographic convention for the medials "i, u" and "ü" when no initial is present. When "i, u" or "ü" are finals and no initial is present, they are spelled "yi", "wu", and "yu", respectively.
The conventional order (excluding "w" and "y"), derived from the zhuyin system, is:
Finals.
Standard Chinese vowels (with IPA and Pinyin)
The following chart gives the combinations of medials and finals based on an analysis that assumes just two vowel nuclei, /a/ and /ə/; various allophones result depending on phonetic context.
In each cell below, the first line indicates IPA, the second indicates pinyin for a standalone (no-initial) form, and the third indicates pinyin for a combination with an initial. Other than finals modified by an "-r", which are omitted, the following is an exhaustive table of all possible finals.1
The only syllable-final consonants in Standard Chinese are "-n" and "-ng", and "-r", which is attached as a grammatical suffix. A Chinese syllable ending with any other consonant either is from a non-Mandarin language (a southern Chinese language such as Cantonese, or a minority language of China), or indicates the use of a non-pinyin romanization system (where final consonants may be used to indicate tones).
1 [ɑɻ] is written "er". For other finals formed by the suffix "-r," pinyin does not use special orthography; one simply appends "r" to the final that it is added to, without regard for any sound changes that may take place along the way. For information on sound changes related to final "r", please see Erhua#Rules.
2 "ü" is written as "u" after "j, q, x," or "y."
3 "uo" is written as "o" after "b, p, m," or "f."
4 "weng" is pronounced [ʊŋ] (written as "ong") when it follows an initial.
Technically, "i, u, ü" without a following vowel are finals, not medials, and therefore take the tone marks, but they are more concisely displayed as above. In addition, "ê" [ɛ] () and syllabic nasals "m" (, ), "n" (, ), "ng" (, ) are used as interjections.
Rules given in terms of English pronunciation.
All rules given here in terms of English pronunciation are approximations, as several of these sounds do not correspond directly to sounds in English.
Pronunciation of initials.
"Y" and "w" are equivalent to the semivowel medials "i, u," and "ü" (see below). They are spelled differently when there is no initial consonant in order to mark a new syllable: "fanguan" is "fan-guan," while "fangwan" is "fang-wan" (and equivalent to "*fang-uan)." With this convention, an apostrophe only needs to be used to mark an initial "a, e," or "o: Xi'an" (two syllables: [ɕi.an]) vs. "xian" (one syllable: [ɕi̯ɛn]). In addition, "y" and "w" are added to fully vocalic "i, u," and "ü" when these occur without an initial consonant, so that they are written "yi, wu," and "yu." Some Mandarin speakers do pronounce a [j] or [w] sound at the beginning of such words—that is, "yi" [i] or [ji], "wu" [u] or [wu], "yu" [y] or [ɥy],—so this is an intuitive convention. See below for a few finals which are abbreviated after a consonant plus "w/u" or "y/i" medial: "wen" → C+"un," "wei" → C+"ui," "weng" → C+"ong," and "you" → C+"iu."
The apostrophe (') is used before a syllable starting with a vowel ("a", "o", or "e") in a multiple-syllable word when the syllable does not start the word (which is most commonly realized as [ɰ]), unless the syllable immediately follows a hyphen or other dash. This is done to remove ambiguity that could arise, as in "Xi'an", which consists of the two syllables "xi" ("") "an" (""), compared to such words as "xian" (""). (This ambiguity does not occur when tone marks are used: The two tone marks in "Xīān" unambiguously show that the word consists of two syllables. However, even with tone marks, the city is usually spelled with an apostrophe as "Xī'ān".)
Pronunciation of finals.
The following is a list of finals in Standard Chinese, excepting most of those ending with "r".
To find a given final:
Orthography.
Letters.
Pinyin differs from other romanizations in several aspects, such as the following:
Most of the above are used to avoid ambiguity when writing words of more than one syllable in pinyin. For example "uenian" is written as "wenyan" because it is not clear which syllables make up "uenian"; "uen-ian", "uen-i-an" and "u-en-i-an" are all possible combinations whereas "wenyan" is unambiguous because "we", "nya", etc. do not exist in pinyin. See the pinyin table article for a summary of possible pinyin syllables (not including tones).
Word formation, capitalization, initialisms and punctuation.
Although Chinese characters represent single syllables, Mandarin Chinese is a polysyllabic language. Spacing in pinyin is based on whole words, not single syllables. However, there are often ambiguities in partitioning a word. "The Basic Rules of the Chinese Phonetic Alphabet Orthography" () were put into effect in 1988 by the National Educational Commission () and the National Language Commission (). These rules became a Guobiao standard in 1996 and were updated in 2012.
Tones.
The pinyin system also uses diacritics to mark the four tones of Mandarin. The diacritic is placed over the letter that represents the syllable nucleus, unless that letter is missing (see below). Many books printed in China use a mix of fonts, with vowels and tone marks rendered in a different font from the surrounding text, tending to give such pinyin texts a typographically ungainly appearance. This style, most likely rooted in early technical limitations, has led many to believe that pinyin's rules call for this practice and also for the use of a Latin alpha ("ɑ") rather than the standard style of the letter ("a") found in most fonts. The official rules of "Hanyu Pinyin," however, specify no such practice.
These tone marks normally are only used in Mandarin textbooks or in foreign learning texts, but they are essential for correct pronunciation of Mandarin syllables, as exemplified by the following classic example of five characters whose pronunciations differ only in their tones:
The words are "mother", "hemp", "horse", "scold" and a question particle, respectively.
Numerals in place of tone marks.
Before the advent of computers, many typewriter fonts did not contain vowels with macron or caron diacritics. Tones were thus represented by placing a tone number at the end of individual syllables. For example, "tóng" is written "tong2."
The number used for each tone is as the order listed above, except the neutral tone, which is either not numbered, or given the number 0 or 5, e.g. "ma5" for 吗/嗎, an interrogative marker.
Rules for placing the tone mark.
Briefly, the tone mark should always be placed by the order—"a, o, e, i, u, ü", with the only exception being "iu," where the tone mark is placed on the "u" instead. Pinyin tone marks appear primarily above the nucleus of the syllable, for example as in "kuài," where "k" is the initial, "u" the medial, "a" the nucleus, and "i" the coda. The exception is syllabic nasals like /m/, where the nucleus of the syllable is a consonant, the diacritic will be carried by a written dummy vowel.
When the nucleus is /ə/ (written "e" or "o"), and there is both a medial and a coda, the nucleus may be dropped from writing. In this case, when the coda is a consonant "n" or "ng," the only vowel left is the medial "i, u," or "ü," and so this takes the diacritic. However, when the coda is a vowel, it is the coda rather than the medial which takes the diacritic in the absence of a written nucleus. This occurs with syllables ending in "-ui" (from "wei": (wèi → -uì) and in "-iu" (from "you: yòu → -iù.") That is, in the absence of a written nucleus the finals have priority for receiving the tone marker, as long as they are vowels: if not, the medial takes the diacritic.
An algorithm to find the correct vowel letter (when there is more than one) is as follows:
Worded differently,
If the tone is written over an "i", the tittle above the "i" is omitted, as in "yī."
Phonological intuition.
The placement of the tone marker, when more than one of the written letters "a, e, i, o," and "u" appears, can also be inferred from the nature of the vowel sound in the medial and final. The rule is that the tone marker goes on the spelled vowel that is not a (near-)semi-vowel. The exception is that, for triphthongs that are spelled with only two vowel letters, both of which are the semi-vowels, the tone marker goes on the second spelled vowel.
Specifically, if the spelling of a diphthong begins with "i" (as in "ia") or "u" (as in "ua"), which here serves as a near-semi-vowel, this letter does not take the tone marker. Likewise, if the spelling of a diphthong ends with "o" or "u" representing a near-semi-vowel (as in "ao" or "ou"), this letter does not receive a tone marker. In a triphthong spelled with three of "a, e, i, o," and "u" (with "i" or "u" replaced by "y" or "w" at the start of a syllable), the first and third letters coincide with near-semi-vowels and hence do not receive the tone marker (as in "iao" or "uai" or "iou"). But if no letter is written to represent a triphthong's middle (non-semi-vowel) sound (as in "ui" or "iu"), then the tone marker goes on the final (second) vowel letter.
Using tone colors.
In addition to tone number and mark, tone color has been suggested as a visual aid for learning. Although there are no formal standards, there are a number of different color schemes in use. 
Third tone exceptions.
In spoken Chinese, the third tone is often pronounced as a "half third tone," in which the pitch does not rise. Additionally, when two third tones appear consecutively, such as in 你好 (nǐhǎo, hello), the first syllable is pronounced with the second tone. In pinyin, words like "hello" are still written with two third tones (nǐhǎo).
The "ü" sound.
An umlaut is placed over the letter "u" when it occurs after the initials "l" and "n" in order to represent the sound [y]. This is necessary in order to distinguish the front high rounded vowel in "lü" (e.g. ) from the back high rounded vowel in "lu" (e.g. ). Tonal markers are added on top of the umlaut, as in "lǘ".
However, the "ü" is "not" used in the other contexts where it could represent a front high rounded vowel, namely after the letters "j", "q", "x" and "y". For example, the sound of the word 鱼/魚 (fish) is transcribed in pinyin simply as "yú", not as "yǘ". This practice is opposed to Wade-Giles, which always uses "ü", and "Tongyong Pinyin," which always uses "yu". Whereas Wade-Giles needs to use the umlaut to distinguish between "chü" (pinyin "ju") and "chu" (pinyin "zhu"), this ambiguity cannot arise with pinyin, so the more convenient form "ju" is used instead of "jü". Genuine ambiguities only happen with "nu"/"nü" and "lu"/"lü", which are then distinguished by an umlaut.
Many fonts or output methods do not support an umlaut for "ü" or cannot place tone marks on top of "ü". Likewise, using "ü" in input methods is difficult because it is not present as a simple key on many keyboard layouts. For these reasons "v" is sometimes used instead by convention. For example, it is common for cellphones to use "v" instead of "ü". Additionally, some stores in China use "v" instead of "ü" in the transliteration of their names. The drawback is that there are no tone marks for the letter "v".
This also presents a problem in transcribing names for use on passports, affecting people with names that consist of the sound "lü" or "nü", particularly people with the surname 吕 ("Lǚ"), a fairly common surname, particularly compared to the surname 陆 (Lù), 鲁 (Lǔ), 卢 (Lú) and 路 (Lù). Previously, the practice varied among different passport issuing offices, with some transcribing as "LV" and "NV" while others used "LU" and "NU". On 10 July 2012, the Ministry of Public Security standardized the practice to use "LYU" and "NYU" in passports.
Although "nüe" written as "nue", and "lüe" written as "lue" are not ambiguous, "nue" or "lue" are not correct according to the rules; "nüe" and "lüe" should be used instead. However, some Chinese input methods (e.g. Microsoft Pinyin IME) support both "nve"/"lve" (typing "v" for "ü") and "nue"/"lue".
Pinyin in Taiwan.
Taiwan (Republic of China) adopted "Tongyong Pinyin," a modification of "Hanyu Pinyin," as the official romanization system on the national level between October 2002 and January 2009, when it switched to "Hanyu Pinyin". "Tongyong Pinyin" ("official phonetic"), a variant of pinyin developed in Taiwan, was designed to romanize languages and dialects spoken on the island in addition to Mandarin Chinese. The Kuomintang (Chinese Nationalist Party) resisted its adoption, preferring the "Hanyu Pinyin" system used in Mainland China and in general use internationally. Romanization preferences quickly became associated with issues of national identity. Preferences split along party lines: the Kuomintang and its affiliated parties in the pan-blue coalition supported the use of Hanyu Pinyin while the Democratic Progressive Party and its affiliated parties in the pan-green coalition favored the use of Tongyong Pinyin.
"Tongyong Pinyin" was made the official system in an administrative order that allowed its adoption by local governments to be voluntary. A few localities with governments controlled by the Kuomintang (KMT), most notably Taipei, Hsinchu, and Kinmen County, overrode the order and converted to "Hanyu Pinyin" before the January 1, 2009 national-level switch, though with a slightly different capitalization convention than mainland China. Most areas of Taiwan adopted Tongyong Pinyin, consistent with the national policy. Many street signs in Taiwan today still display "Tongyong Pinyin" but some, especially in northern Taiwan, display "Hanyu Pinyin." It is still not unusual to see spellings on street signs and buildings derived from the older Wade-Giles, MPS2 and other systems.
The adoption of "Hanyu Pinyin" as the official romanization system in Taiwan does not preclude the official retention of earlier spellings. International familiarity has led to the retention of the spelling "Taipei" ("Taibei" in pinyin systems) and even to its continuation in the name of New Taipei, a municipality created in 2010. Personal names on Taiwanese passports honor the choices of Taiwanese citizens, who often prefer the Wade-Giles romanization of their personal names. Transition to Hanyu Pinyin in official use is also necessarily gradual. Universities and other government entities retain earlier spellings in long-established names, budget restraints preclude widespread replacement of signage and stationery in every area, and questions remain about the ability of the national government to enforce the standard island-wide. Primary education in Taiwan continues to teach pronunciation using "zhuyin" (MPS or Mandarin Phonetic Symbols).
Comparison with other orthographies.
Pinyin is now used by foreign students learning Chinese as a second language.
Pinyin assigns some Latin letters sound values which are quite different from that of most languages. This has drawn some criticism as it may lead to confusion when uninformed speakers apply either native or English assumed pronunciations to words. However this is not a specific problem of pinyin, since many languages that use the Latin alphabet natively assign different values to the same letters. A recent study on Chinese writing and literacy concluded, "By and large, pinyin represents the Chinese sounds better than the Wade-Giles system, and does so with fewer extra marks."
Because Pinyin is purely a representation of the sounds of Mandarin, it completely lacks the semantic cues and contexts inherent in Chinese characters. Pinyin is also unsuitable for transcribing some Chinese spoken languages other than Mandarin, languages which by contrast have traditionally been written with Han characters allowing for written communication which, by its unified semanto-phonetic orthography, could theoretically be readable in any of the various vernaculars of Chinese where a phonetic script would have only localized utility.
Computer input systems.
Simple computer systems, able to display only 7-bit ASCII text (essentially the 26 Latin letters, 10 digits and punctuation marks), long provided a convincing argument in favor of pinyin over Chinese characters. Today, however, most computer systems are able to display characters from Chinese and many other writing systems as well, and have them entered with a Latin keyboard using an input method editor. Alternatively, some PDAs, tablet computers and digitizing tablets allow users to input characters directly by writing with a stylus.
Other languages.
Pinyin-like systems have been devised for other variants of Chinese. Guangdong Romanization is a set of romanizations devised by the government of Guangdong province for Cantonese, Teochew, Hakka (Moiyen dialect), and Hainanese. All of these are designed to use Latin letters in a similar way to pinyin.
In addition, in accordance to the "Regulation of Phonetic Transcription in Hanyu Pinyin Letters of Place Names in Minority Nationality Languages" () promulgated in 1976, place names in non-Han languages like Mongolian, Uyghur, and Tibetan are also officially transcribed using pinyin in a system adopted by the State Administration of Surveying and Mapping and Geographical Names Committee known as SASM/GNC romanization. The pinyin letters (26 Roman letters, ü, ê) are used to approximate the non-Han language in question as closely as possible. This results in spellings that are different from both the customary spelling of the place name, and the pinyin spelling of the name in Chinese:
"Tongyong Pinyin" was developed in Taiwan for use in rendering not only Mandarin Chinese, but other languages and dialects spoken on the island such as Taiwanese, Hakka and aboriginal languages.

</doc>
<doc id="23589" url="http://en.wikipedia.org/wiki?curid=23589" title="Parable of the Pearl">
Parable of the Pearl

The Parable of the Pearl (also called the Pearl of Great Price) is a parable of Jesus of Nazareth. It appears in only one of the Canonical gospels of the New Testament. According to the parable illustrates the great value of the Kingdom of Heaven.
It immediately follows the Parable of the Hidden Treasure, which has a similar theme and the parable has been depicted by artists such as Domenico Fetti. A version of this parable also appears in the non canonical Gospel of Thomas 76.
Narrative.
The brief Parable of the Pearl is as follows:
 Again, the kingdom of heaven is like unto a merchant man, seeking goodly pearls: Who, when he had found one pearl of great price, went and sold all that he had, and bought it.
 — Matthew 13:45-46, King James Version
Interpretation.
This parable is generally interpreted as illustrating the great value of the Kingdom of Heaven (pearls at that time had a greater value than they do today), and thus has a similar theme to the Parable of the Hidden Treasure. John Nolland comments that it shares the notions of "good fortune and demanding action in attaining the kingdom of heaven" with that parable, but adds the notion of "diligent seeking."
The valuable pearl is the "deal of a lifetime" for the merchant in the story. However, those who do not believe in the kingdom of heaven enough to stake their whole future on it are unworthy of the kingdom.
This interpretation of the parable is the inspiration for a number of hymns, including the Swedish hymn "Den Kostliga Pärlan" ("O That Pearl of Great Price!"), which begins:
<poem>
O that Pearl of great price! have you found it?
Is the Savior supreme in your love?
O consider it well, ere you answer,
As you hope for a welcome above.
Have you given up all for this Treasure?
Have you counted past gains as but loss?
Has your trust in yourself and your merits
Come to naught before Christ and His cross?
</poem>
A less common interpretation of the parable is that the merchant represents Jesus, and the pearl represents the Christian Church. This interpretation would give the parable a similar theme to that of the Parable of the Lost Sheep, the Lost Coin, and the Prodigal Son.
The phrase "Pearl of Great Price" has also been interpreted more widely to apply to things of great value in a number of religious contexts. For example, it is the title of a selection of Mormon writings. Pope Pius XII used the phrase to describe virginity.
The pearl itself is a beautiful, single entity, formed through suffering in the heart of the oyster (in the same way that believers endure lack of wealth or comfort) and like the Church, will be put on display in a coming day. Unlike precious stones which must be cut and polished to reveal their clarity and beauty, the pearl is perfect as it comes from the oyster.
In the Gospel of Thomas.
A version of the parable also appears in the Gnostic Gospel of Thomas (Saying 76):
 Jesus said, "The Father's kingdom is like a merchant who had a supply of merchandise and found a pearl. That merchant was prudent; he sold the merchandise and bought the single pearl for himself.
So also with you, seek his treasure that is unfailing, that is enduring, where no moth comes to eat and no worm destroys."
 — Gospel of Thomas 76, Patterson/Meyer translation
This work's version of the parable of the Hidden Treasure appears later (Saying 109), rather than immediately preceding, as in Matthew. However, the mention of a treasure in Saying 76 may reflect a source for the Gospel of Thomas in which the parables were adjacent, so that the original pair of parables has been "broken apart, placed in separate contexts, and expanded in a manner characteristic of folklore." In Gnostic thought the pearl may represent Christ or the true self. In the Gnostic Acts of Peter and the Twelve, found with the Gospel of Thomas in the Nag Hammadi library, the travelling pearl merchant Lithargoel is eventually revealed to be Jesus.
Depictions.
There have been several depictions of the New Testament parable in art, including works by Domenico Fetti, John Everett Millais, and Jan Luyken.

</doc>
<doc id="23590" url="http://en.wikipedia.org/wiki?curid=23590" title="Pantheism">
Pantheism

Pantheism is the belief that the Universe (or nature as the totality of everything) is identical with divinity, or that everything composes an all-encompassing, immanent God. Pantheists thus do not believe in a distinct personal or anthropomorphic god. Some Asian religions are considered to be pantheistically inclined.
Pantheism was popularised in the West as both a theology and philosophy based on the work of the 17th-century philosopher Baruch Spinoza,:p.7 whose book "Ethics" was an answer to Descartes' famous dualist theory that the body and spirit are separate. Spinoza held the monist view that the two are the same, and monism is a fundamental part of his philosophy. He was described as a "God-intoxicated man," and used the word God to describe the unity of all substance. Although the term pantheism was not coined until after his death, Spinoza is regarded as its most celebrated advocate.
Definitions.
Pantheism is derived from the Greek πᾶν "pan" (meaning "all") and θεός "theos" (meaning "God"). There are a variety of definitions of pantheism. Some consider it a theological and philosophical position concerning God.:p.8
As a religious position, some describe pantheism as the polar opposite of atheism. From this standpoint, pantheism is the view that everything is part of an all-encompassing, immanent God. All forms of reality may then be considered either modes of that Being, or identical with it. Some hold that pantheism is a non-religious philosophical position. To them, pantheism is the view that the Universe (in the sense of the totality of all existence) and God are identical (implying a denial of the personality and transcendence of God).
History.
The first known use of the term pantheism was in Latin, by the English mathematician Joseph Raphson in his work "De spatio reali", published in 1697. In "De spatio reali", Raphson begins with a distinction between atheistic ‘panhylists’ (from the Greek roots "pan", "all", and "hyle", "matter"), who believe everything is matter, and ‘pantheists’ who believe in “a certain universal substance, material as well as intelligent, that fashions all things that exist out of its own essence.”
 Raphson found the universe to be immeasurable in respect to a human's capacity of understanding, and believed that humans would never be able to comprehend it.
The Roman Catholic Church regarded (and still regards) pantheistic ideas as heresy. Giordano Bruno, an Italian monk who evangelized about an immanent and infinite God, was burned at the stake in 1600 by the Roman Inquisition. He has since become known as a celebrated pantheist and martyr of science. Bruno influenced many later thinkers including Baruch Spinoza, whose "Ethics", finished in 1675, was the major source from which pantheism spread.
The term was first used in the English language by the Irish writer John Toland in his work of 1705 "Socinianism Truly Stated, by a pantheist". Toland was influenced by both Spinoza and Bruno, and used the terms 'pantheist' and 'Spinozist' interchangeably. In 1720 he wrote the "Pantheisticon: or The Form of Celebrating the Socratic-Society" in Latin, envisioning a pantheist society which believed, "all things in the world are one, and one is all in all things ... what is all in all things is God, eternal and immense, neither born nor ever to perish." He clarified his idea of pantheism in a letter to Gottfried Leibniz in 1710 when he referred to "the pantheistic opinion of those who believe in no other eternal being but the universe".
Although the term "pantheism" did not exist before the 17th century, various pre-Christian religions and philosophies can be regarded as pantheistic. Pantheism is similar to the ancient Hindu philosophy of Advaita (non-dualism) to the extent that the 19th-century German Sanskritist Theodore Goldstücker remarked that Spinoza's thought was "... a western system of philosophy which occupies a foremost rank amongst the philosophies of all nations and ages, and which is so exact a representation of the ideas of the Vedanta, that we might have suspected its founder to have borrowed the fundamental principles of his system from the Hindus."
Others include some of the Presocratics, such as Heraclitus and Anaximander. The Stoics were pantheists, beginning with Zeno of Citium and culminating in the emperor-philosopher Marcus Aurelius. During the pre-Christian Roman Empire, Stoicism was one of the three dominant schools of philosophy, along with Epicureanism and Neoplatonism. The early Taoism of Lao Zi and Zhuangzi is also sometimes considered pantheistic.
In 1785, a major controversy about Spinoza's philosophy between Friedrich Jacobi, a critic, and Moses Mendelssohn, a defender, known in German as the "Pantheismus-Streit", helped to spread pantheism to many German thinkers in the late 18th and 19th centuries.
During the 19th century, pantheism was the theological viewpoint of many leading writers and philosophers, attracting figures such as William Wordsworth and Samuel Coleridge in Britain; Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling and Georg Wilhelm Friedrich Hegel in Germany; and Walt Whitman, Ralph Waldo Emerson and Henry David Thoreau in the United States. Seen as a growing threat by the Vatican, in 1864 it was formally condemned by Pope Pius IX in the "Syllabus of Errors".
In the mid-eighteenth century, the English theologian Daniel Waterland defined pantheism this way: "It supposes God and nature, or God and the whole universe, to be one and the same substance—one universal being; insomuch that men's souls are only modifications of the divine substance." In the early nineteenth century, the German theologian Julius Wegscheider defined pantheism as the belief that God and the world established by God are one and the same.
In the late 20th century, pantheism was often declared to be the underlying theology of Neopaganism, and Pantheists began forming organizations devoted specifically to Pantheism and treating it as a separate religion.
Recent developments.
In 2007, Dorion Sagan, the son of famous scientist and science communicator, Carl Sagan, published a book entitled "Dazzle Gradually: Reflections on the Nature of Nature" co-written by Sagan's ex-wife, Lynn Margulis. In a chapter entitled, "Truth of My Father", he declares: "My father believed in the God of Spinoza and Einstein, God not behind nature, but as nature, equivalent to it."
In 2008, one of Albert Einstein's letters, written in 1954 in German, in which he dismissed belief in a personal God, was sold at auction for more than US$330,000. Einstein wrote, "We followers of Spinoza see our God in the wonderful order and lawfulness of all that exists and in its soul ["Beseeltheit"] as it reveals itself in man and animal," in a letter to Eduard Büsching (25 October 1929) after Büsching sent Einstein a copy of his book "Es gibt keinen Gott". Einstein responded that the book only dealt with the concept of a personal God and not the impersonal God of pantheism. "I do not believe in a personal God and I have never denied this but have expressed it clearly," he wrote in another letter in 1954.
Pantheism is mentioned in a Papal encyclical in 2009 and a statement on New Year's Day in 2010, criticizing pantheism for denying the superiority of humans over nature and "seeing the source of man‍‍ '​‍s salvation in nature". In a review of the 2009 film "Avatar", Ross Douthat, an author, described pantheism as "Hollywood’s religion of choice for a generation now".
In 2011, a letter written in 1886 by William Herndon, Abraham Lincoln's law partner, was sold at auction for US$30,000. In it, Herndon writes of the U.S. President's evolving religious views, which included pantheism.
"Mr. Lincoln’s religion is too well known to me to allow of even a shadow of a doubt; he is or was a Theist and a Rationalist, denying all extraordinary – supernatural inspiration or revelation. At one time in his life, to say the least, he was an elevated Pantheist, doubting the immortality of the soul as the Christian world understands that term. He believed that the soul lost its identity and was immortal as a force. Subsequent to this he rose to the belief of a God, and this is all the change he ever underwent."
The subject is understandably controversial, but the contents of the letter is consistent with Lincoln's fairly lukewarm approach to organized religion.
Categorizations.
There are multiple varieties of pantheism:3 which have been placed along various spectra or in discrete categories.
Degree of determinism.
The American philosopher Charles Hartshorne used the term Classical Pantheism to describe the deterministic philosophies of Baruch Spinoza, the Stoics, and other like-minded figures. Pantheism (All-is-God) is often associated with monism (All-is-One) and some have suggested that it logically implies determinism (All-is-Now). Albert Einstein explained theological determinism by stating, "the past, present, and future are an 'illusion'". This form of pantheism has been referred to as "extreme monism", in which – in the words of one commentator – "God decides or determines everything, including our supposed decisions." Other examples of determinism-inclined pantheisms include those of Ralph Waldo Emerson, and Georg Wilhelm Friedrich Hegel.
However, some have argued against treating every meaning of "unity" as an aspect of pantheism, and there exist versions of pantheism that regard determinism as an inaccurate or incomplete view of nature. Examples include the beliefs of Friedrich Wilhelm Joseph Schelling and William James.
Degree of belief.
It may also be possible to distinguish two types of pantheism, one being more religious and the other being more philosophical. The Columbia Encyclopedia writes of the distinction:
Religious inclined pantheisms include some forms of Hinduism while philosophical inclined pantheisms include Stoicism.
Other.
In 1896, J. H. Worman, a theologian, identified seven categories of pantheism: Mechanical or materialistic (God the mechanical unity of existence); Ontological (fundamental unity, Spinoza); Dynamic; Psychical (God is the soul of the world); Ethical (God is the universal moral order, Johann Gottlieb Fichte); Logical (Hegel); and Pure (absorption of God into nature, which Worman equates with atheism).
More recently, Paul D. Feinberg, professor of biblical and systematic theology at Trinity Evangelical Divinity School, also identified seven categories of pantheism: Hylozoistic; Immanentistic; Absolutistic monistic; Relativistic monistic; Acosmic; Identity of opposites; and Neoplatonic or emanationistic.
Pantheism in religion.
Philosopher Michael Levine has said that there may be more pantheists than theists worldwide.:p.14 There are elements of pantheism in some forms of Christianity, Islam (Sufism), Buddhism, Judaism, Gnosticism, Neopaganism, and Theosophy as well as in several tendencies in many theistic religions. The Islamic religious tradition, in particular Sufism and Alevism, has a strong belief in the unitary nature (see also Sufi metaphysics) of the universe and the concept that everything in it is an aspect of God itself, although their perspective, like many traditional perspectives, may lean closer to panentheism. Many other traditional and folk religions including African traditional religions and Native American religions:p.67 can be seen as pantheistic, or a mixture of pantheism and other doctrines such as polytheism and animism. A variety of modern paganists also hold pantheistic views.
Hinduism.
Hindu religious texts are the oldest known literature containing pantheistic concepts. The Advaita Vedanta school of Hinduism teaches that the Atman (true self; human soul) is indistinct from Brahman (the unknown reality of everything).
The branches of Hinduism teaching forms of pantheism are known as non-dualist schools. All Mahāvākyas (Great Sayings) of the Upanishads, in one way or another, seem to indicate the unity of the world with the Brahman. It further says, "This whole universe is Brahman, from Brahman to a clod of earth."
Taoism.
In the tradition of its leading thinkers Lao Tzu and Zhuangzi, Taoism is comparable with pantheism, as the Tao is always spoken of with profound religious reverence and respect, similar to the way that pantheism discusses the "God" that is everything. The Tao te Ching never speaks of a transcendent God, but of a mysterious and numinous ground of being underlying all things. Zhuangzi emphasized the pantheistic content of Taoism even more clearly: "Heaven and I were created together, and all things and I are one." When Tung Kuo Tzu asked Zhuangzi where the Tao was, he replied that it was in the ant, the grass, the clay tile, even in excrement: "There is nowhere where it is not… There is not a single thing without Tao."
Organizations.
Two organizations that specify the word pantheism in their title formed in the last quarter of the 20th century. The Universal Pantheist Society, open to all varieties of pantheists and supportive of environmental causes, was founded in 1975. The World Pantheist Movement is headed by Paul Harrison, an environmentalist, writer and a former vice president of the Universal Pantheist Society, from which he resigned in 1996. The World Pantheist Movement was incorporated in 1999 to focus exclusively on promoting a strict metaphysical naturalistic version of pantheism, considered by some a form of religious naturalism. It has been described as an example of "dark green religion" with a focus on environmental ethics.
Related concepts.
Nature worship or nature mysticism is often conflated and confused with pantheism. It is pointed out by at least one expert in pantheist philosophy that Spinoza’s identification of God with nature is very different from a recent idea of a self identifying pantheist with environmental ethical concerns, Harold Wood, founder of the Universal Pantheist Society. His use of the word nature to describe his worldview is suggested to be vastly different from the "nature" of modern sciences. He and other nature mystics who also identify as pantheists use "nature" to refer to the limited natural environment (as opposed to man-made built environment). This use of "nature" is different from the broader use from Spinoza and other pantheists describing natural laws and the overall phenomena of the physical world. Nature mysticism may be compatible with pantheism but it may also be compatible with theism and other views.
Panentheism (from Greek πᾶν (pân) "all"; ἐν (en) "in"; and θεός (theós) "God"; "all-in-God") was formally coined in Germany in the 19th century in an attempt to offer a philosophical synthesis between traditional theism and pantheism, stating that God is substantially omnipresent in the physical universe but also exists "apart from" or "beyond" it as its Creator and Sustainer.:p.27 Thus panentheism separates itself from pantheism, positing the extra claim that God exists above and beyond the world as we know it.:p.11 The line between pantheism and panentheism can be blurred depending on varying definitions of God, so there have been disagreements when assigning particular notable figures to pantheism or panentheism.:pp. 71–72, 87–88, 105
Pandeism is another word derived from pantheism and is characterized as a combination of reconcilable elements of pantheism and deism. It assumes a Creator-deity which is at some point distinct from the universe and then merges with it, resulting in a universe similar to the pantheistic one in present essence, but differing in origin.
Panpsychism is the philosophical view held by many pantheists that consciousness, mind, or soul is a universal feature of all things. Some pantheists also subscribe to the distinct philosophical views hylozoism (or panvitalism), the view that everything is alive, and its close neighbor animism, the view that everything has a soul or spirit.

</doc>
<doc id="23591" url="http://en.wikipedia.org/wiki?curid=23591" title="Panentheism">
Panentheism

Panentheism (meaning "all-in-God", from the Ancient Greek πᾶν "pân" ("all"), ἐν "en" ("in") and Θεός "Theós" ("God")) is a belief system which posits that the divine – whether as a single God, number of gods, or other form of "cosmic animating force" – interpenetrates every part of the universe and extends, timelessly (and, presumably, spacelessly) beyond it. Unlike pantheism, which holds that the divine and the universe are identical, panentheism maintains a distinction between the divine and non-divine and the significance of both.
In pantheism, the universe in the first formulation is practically the whole itself, while in panentheism, the universe and the divine are not ontologically equivalent. God is viewed as the eternal #redirect maintaining the universe. Some versions suggest that the universe is nothing more than the manifest part of God. In some forms of panentheism, the cosmos exists within God, who in turn "transcends", "pervades" or is "in" the cosmos. While pantheism asserts that 'All is God', panentheism goes further to claim that God is greater than the universe. In addition, some forms indicate that the universe is contained within God, like in the concept of Tzimtzum. Much Hindu thought is highly characterized by panentheism and pantheism. Hasidic Judaism merges the elite ideal of nullification to paradoxical transcendent Divine Panentheism, through intellectual articulation of inner dimensions of Kabbalah, with the populist emphasis on the panentheistic Divine immanence in everything and deeds of kindness.
Ancient panentheism.
In the Americas (Pre-European).
Many North American Native Peoples (such as the Cree, Iroquois, Huron, Navajo, and others) were and still are largely panentheistic, conceiving of God as both confined in God's existence in Creation but also transcendent from it. (North American Native writers have also translated the word for God as the Great Mystery or as the Sacred Other) This concept is referred to by many as the Great Spirit. One exception can be modern Cherokee who are predominantly monotheistic but apparently not panentheistic (as the two are not mutually exclusive); yet in older Cherokee traditions many observe both aspects of pantheism and panentheism, and are often not beholden to exclusivity, encompassing other spiritual traditions without contradiction, a common trait among some tribes in the Americas. Most South American Native peoples were largely panentheistic as well (as were ancient South East Asian and African cultures). The Central American empires of the Mayas, Aztecs as well as the South American Incans (Tahuatinsuyu) were actually polytheistic and had very strong male deities.
According to Charles C. Mann's, "1491", only the lower classes of Aztec society were polytheistic. Writings from Aztec priests reveal them to be strong panentheists who considered the common mythology to be a symbolic oversimplification meant to be easier for the commoners to understand.
In Europe.
Neoplatonism is polytheistic and panentheistic. Plotinus taught that there was an ineffable transcendent "God" (The One) of which subsequent realities were emanations. From the One emanates the Divine Mind (Nous) and the Cosmic Soul (Psyche). In Neoplatonism the world itself is God [Timaeus 37]. This concept of divinity is associated with that of the "Logos", which had originated centuries earlier with Heraclitus (ca. 535–475 BC). The "Logos" pervades the cosmos, whereby all thoughts and all things originate, or as Heraclitus said: "He who hears not me but the Logos will say: All is one." Neoplatonists such as Iamblichus attempted to reconcile this perspective by adding another hypostasis above the original monad of force or Dunamis. This new all-pervasive monad encompassed all creation and its original uncreated emanations.
Modern philosophy.
Baruch Spinoza later claimed that "Whatsoever is, is in God, and without God nothing can be, or be conceived." "Individual things are nothing but modifications of the attributes of God, or modes by which the attributes of God are expressed in a fixed and definite manner." Though Spinoza has been called the "prophet" and "prince" of pantheism, in a letter to Henry Oldenburg Spinoza states that: "as to the view of certain people that I identify god with nature (taken as a kind of mass or corporeal matter), they are quite mistaken" For Spinoza, our universe (cosmos) is a mode under two attributes of Thought and Extension. God has infinitely many other attributes which are not present in our world. According to German philosopher Karl Jaspers, when Spinoza wrote "Deus sive Natura" (God or Nature) Spinoza did not mean to say that God and Nature are interchangeable terms, but rather that God's transcendence was attested by his infinitely many attributes, and that two attributes known by humans, namely Thought and Extension, signified God's immanence. Furthermore, Martial Guéroult suggested the term "Panentheism", rather than "Pantheism" to describe Spinoza’s view of the relation between God and the world. The world is not God, but it is, in a strong sense, "in" God. Yet, American philosopher and self-described Panentheist Charles Hartshorne referred to Spinoza's philosophy as "Classical Pantheism" and distinguished Spinoza's philosophy with panentheism.
The German philosopher Karl Christian Friedrich Krause (1781–1832) seeking to reconcile monotheism and pantheism, coined the term "panentheism" ("all in God") in 1828. This conception of God influenced New England transcendentalists such as Ralph Waldo Emerson. The term was popularized by Charles Hartshorne in his development of process theology and has also been adopted by proponents of various New Thought beliefs. The formalization of this term in the West in the 18th century was of course not new; philosophical treatises had been written on it in the context of Hinduism for millennia.
Philosophers who embraced panentheism have included Thomas Hill Green (1839–1882), James Ward (1843–1925), Andrew Seth Pringle-Pattison (1856–1931) and Samuel Alexander (1859–1938). Beginning in the 1940s, Hartshorne examined numerous conceptions of God. He reviewed and discarded pantheism, deism, and pandeism in favor of panentheism, finding that such a "doctrine contains all of deism and pandeism except their arbitrary negations." Hartshorne formulated God as a being who could become "more perfect": He has absolute perfection in categories for which absolute perfection is possible, and relative perfection (i.e., is superior to all others) in categories for which perfection cannot be precisely determined.
In religion.
Bahá'í Faith.
In the Bahá'í Faith, God is described as a single, imperishable God, the creator of all things, including all the creatures and forces in the universe. The connection between God and the world is that of the creator to his creation. God is understood to be independent of his creation, and that creation is dependent and contingent on God. God, however, is not seen to be part of creation as he cannot be divided and does not descend to the condition of his creatures. Instead, in the Bahá'í understanding, the world of creation emanates from God, in that all things have been realized by him and have attained to existence. Creation is seen as the expression of God's will in the contingent world, and every created thing is seen as a sign of God's sovereignty, and leading to knowledge of him; the signs of God are most particularly revealed in human beings.
Christianity.
Panentheism is also a feature of some Christian philosophical theologies and resonates strongly within the Orthodox Tradition. It also appears in Roman Catholic mysticism and process theology.
Process theological thinkers are generally regarded in the West as unorthodox, but process philosophical thought paved the way for open theism.
Eastern Christianity.
In Eastern Christianity (including the Eastern and Oriental Orthodox Churches, as well as the Church of the East), creation is not considered a literal "part of" God, and divinity is essentially distinct from creation. There is, in other words, an irradicable difference between the uncreated (i.e., God) and the created (i.e., everything else). This does not mean, however, that the creation is wholly separated from God, because the creation exists in and from the divine energies. These operations are the natural activity of God and are in some sense identifiable with God, but the creation is wholly distinct from the divine essence. God creates the universe by His will and from His energies. It is not an imprint or emanation of God's own essence ("ousia"), the essence He shares pre-eternally with His Word and Holy Spirit. Neither is it a directly literal outworking or effulgence of the divine, nor any other process which implies that creation is essentially God or a necessary part of God. The generally accepted use of "panentheism" to describe the God concept in Orthodox Christian theology is problematic for those who would insist that panentheism requires creation to be "part of" God.
God is not merely Creator of the universe, as His dynamic presence is necessary to sustain the existence of every created thing, small and great, visible and invisible. That is, God's energies (operations) maintain the existence of the created order and all created beings, even if those agencies have explicitly rejected him. His love for creation is such that He will not withdraw His presence, which would be the ultimate form of annihilation, not merely imposing death, but ending existence altogether. By this token, the entirety of creation is fundamentally "good" in its very being, and is not innately evil either in whole or in part. This does not deny the existence of spiritual or moral evil in a fallen universe, only the claim that it is an intrinsic property of creation. Sin results from the essential freedom of creatures to operate outside the divine order, not as a "necessary" consequence of having inherited human nature. (see problem of evil)
Other Christian panentheists.
Panentheistic conceptions of God occur amongst some modern theologians. Process theology and Creation Spirituality, two recent developments in Christian theology, contain panentheistic ideas.
Some argue that panentheism should also include the notion that God has always been related to some world or another, which denies the idea of creation out of nothing ("creatio ex nihilo"). Nazarene Methodist theologian Thomas Jay Oord advocates panentheism, but he uses the word "theocosmocentrism" to highlight the notion that God and some world or another are the primary conceptual starting blocks for eminently fruitful theology. This form of panentheism helps in overcoming the problem of evil and in proposing that God's love for the world is essential to who God is.
Panentheism was a major force in the Unitarian church for a long time, based on Ralph Waldo Emerson's concept of the Oversoul. This survives today as the panentheistic religion, Oversoul.
 Charles Hartshorne, who conjoined process theology with panentheism, maintained a lifelong membership in the Methodist church but was also a unitarian. In later years he joined the Austin, Texas, Unitarian Universalist congregation and was an active participant in that church.
Many Christians who believe in universalism hold panentheistic views of God in conjunction with their belief in "apocatastasis", also called universal reconciliation. Panentheistic Christian Universalists often believe that all creation's subsistence in God renders untenable the notion of final and permanent alienation from Him, citing Scriptural passages such as Ephesians 4:6 ("[God] is over all and through all and in all") and Romans 11:36 ("from [God] and through him and to him are all things") to justify both panentheism and universalism.
Hinduism.
Earliest reference to panentheistic thought in Hindu philosophy is in a creation myth contained in the later section of Rig Veda called the Purusha Sukta, which was compiled before 1100 BCE. The Purusha Sukta gives a description of the spiritual unity of the cosmos. It presents the nature of Purusha or the cosmic being as both immanent in the manifested world and yet transcendent to it. From this being the sukta holds, the original creative will proceeds, by which this vast universe is projected in space and time.
The most influential and dominant school of Indian philosophy, Advaita Vedanta, rejects theism and dualism by insisting that “Brahman [ultimate reality] is without parts or attributes…one without a second.” Since, Brahman has no properties, contains no internal diversity and is identical with the whole reality it cannot be understood as God. The relationship between Brahman and the creation is often thought to be panentheistic.
Panentheism is also expressed in the Bhagavad Gita. In verse IX.4, Krishna states: By Me all this universe is pervaded through My unmanifested form. All beings abide in Me but I do not abide in them.
Many schools of Hindu thought espouse monistic theism, which is thought to be similar to a panentheistic viewpoint. Nimbarka's school of differential monism (Dvaitadvaita), Ramanuja's school of qualified monism (Vishistadvaita) and Saiva Siddhanta and Kashmir Shaivism are all considered to be panentheistic. Caitanya's Gaudiya Vaishnavism, which elucidates the doctrine of Acintya Bheda Abheda (inconceivable oneness and difference), is also thought to be panentheistic. In Kashmir Shaivism, all things are believed to be a manifestation of Universal Consciousness (Cit or Brahman). So from the point of view of this school, the phenomenal world ("Śakti") is real, and it exists and has its being in Consciousness ("Cit"). Thus, Kashmir Shaivism is also propounding of theistic monism or panentheism.
Shaktism, or Tantra, is regarded as an Indian prototype of Panentheism. Shakti is considered to be the cosmos itself – she is the embodiment of energy and dynamism, and the motivating force behind all action and existence in the material universe. Shiva is her transcendent masculine aspect, providing the divine ground of all being. "There is no Shiva without Shakti, or Shakti without Shiva. The two [...] in themselves are One." Thus, it is She who becomes the time and space, the cosmos, it is She who becomes the five elements, and thus all animate life and inanimate forms. She is the primordial energy that holds all creation and destruction, all cycles of birth and death, all laws of cause and effect within Herself, and yet is greater than the sum total of all these. She is transcendent, but becomes immanent as the cosmos (Mula Prakriti). She, the Primordial Energy, directly becomes Matter.
Sikhism.
The Sikh gurus have described God in numerous ways in their hymns included in the Guru Granth Sahib, the holy scripture of Sikhism, but the oneness of the deity is consistently emphasized throughout. God is described in the Mool Mantar, the first passage in the Guru Granth Sahib, and the basic formula of the faith is:
— ੴ ਸਤਿ ਨਾਮੁ ਕਰਤਾ ਪੁਰਖੁ ਨਿਰਭਉ ਨਿਰਵੈਰੁ ਅਕਾਲ ਮੂਰਤਿ ਅਜੂਨੀ ਸੈਭੰ ਗੁਰ ਪ੍ਰਸਾਦਿ ॥
"Ik oankar sat naam kartaa purakh nirbhau nirvair akaal moorat ajooni saibhan gur prasad"
One Universal Creator God, Thy Name Is Truth, Creative Being Personified, No Fear, No Hatred, Image Of The Timeless One, Beyond Birth, Self Existent, By Guru's Grace.
Guru Arjan, the fifth guru of Sikhs, says, "God is beyond colour and form, yet His/Her presence is clearly visible" (Sri Guru Granth Sahib,Ang 74), and "Nanak's Lord transcends the world as well as the scriptures of the east and the west, and yet He/She is clearly manifest" (Sri Guru Granth Sahib,Ang 397).
Knowledge of the ultimate Reality is not a matter for reason; it comes by revelation of the ultimate reality through nadar (grace) and by anubhava (mystical experience). Says Guru Nanak; "budhi pathi na paiai bahu chaturaiai bhai milai mani bhane." This translates to "He/She is not accessible through intellect, or through mere scholarship or cleverness at argument; He/She is met, when He/She pleases, through devotion" (GG, 436).
Guru Nanak prefixed the numeral one (ik) to it, making it Ik Oankar or Ekankar to stress God's oneness. God is named and known only through his Own immanent nature. The only name which can be said to truly fit God's transcendent state is Sat (Sanskrit Satnam, Truth), the changeless and timeless Reality. God is transcendent and all-pervasive at the same time. Transcendence and immanence are two aspects of the same single Supreme Reality. The Reality is immanent in the entire creation, but the creation as a whole fails to contain God fully. As says Guru Tegh Bahadur, Nanak IX, "He has himself spread out His/Her Own “maya” (worldly illusion) which He oversees; many different forms He assumes in many colours, yet He stays independent of all" (GG, 537).
Islam.
Several Sufi saints and thinkers, primarily Ibn Arabi, held beliefs that were somewhat panentheistic. These notions later took shape in the theory of wahdat ul-wujud (the Unity of All Things). Some Sufi Orders, notably the Bektashis and the Universal Sufi movement, continue to espouse panentheistic beliefs. Nizari Ismaili follow panentheism according to Ismaili doctrine.
Judaism.
While mainstream Rabbinic Judaism is classically monotheistic, and follows in the footsteps of the Aristotelian theologian Maimonides, the panentheistic conception of God can be found among certain mystical Jewish traditions. A leading scholar of Kabbalah, Moshe Idel ascribes this doctrine to the kabbalistic system of Rabbi Moses Cordovero (1522–1570) and in the eighteenth century to the Baal Shem Tov, founder of the Hasidic movement, as well as his contemporaries, Rabbi Dov Ber, the Maggid of Mezeritch, and Menahem Mendel, the Maggid of Bar. This may be said of many, if not most, subsequent Hasidic masters. There is some debate as to whether Lurianic Kabbalah, with its doctrine of Tzimtzum, can be regarded as panentheistic. According to Hasidism, the infinite Ein Sof is incorporeal and exists in a state that is both transcendent and immanent. This appears to be the view of non-Hasidic Rabbi Chaim of Volozhin, as well. Aspects of panentheism are also evident in the theology of Reconstructionist Judaism as presented in the writings of Mordecai Kaplan.
Gnosticism.
Some branches of Gnosticism teach a panentheistic view of reality, and hold to the belief that God exists in the visible world only as sparks of spiritual "light". The goal of human existence is to know the sparks within oneself in order to return to God, who is in the Fullness (or Pleroma).
Gnosticism is panentheistic, believing that the true God is simultaneously both separate from the physical universe and present within it. As Jesus states in the Gospel of Thomas, "I am the light that is over all things. I am all... Split a piece of wood; I am there. Lift up the stone, and you will find me there." This seemingly contradictory interpretation of Gnostic theology is not without controversy, since one interpretation of dualistic theology holds that a perfect God of pure spirit would not manifest himself through the fallen world of matter. As Mani, the founder of Manichaeism, stated, "The true God has nothing to do with the material world or cosmos", and, "It is the Prince of Darkness who spoke with Moses, the Jews and their priests. Thus the Christians, the Jews, and the Pagans are involved in the same error when they worship this God. For he leads them astray in the lusts he taught them.
Valentinian Gnosticism teaches that matter came about through emanations of the supreme being, and to some this event is held to be more accidental than intentional. To other Gnostics, these emanations are akin to the Sephirot of the Kabbalists; they are deliberate manifestations of a transcendent God through a complex system of intermediaries.
Buddhism.
The Reverend Zen Master Soyen Shaku was the first Zen Buddhist Abbot to tour the United States in 1905-6. He wrote a series of essays collected into the book "Zen For Americans". In the essay titled "The God Conception of Buddhism" he attempts to explain how a Buddhist looks at the ultimate without an anthropomorphic God figure while still being able to relate to the term God in a Buddhist sense:
At the outset, let me state that Buddhism is not atheistic as the term is ordinarily understood. It has certainly a God, the highest reality and truth, through which and in which this universe exists. However, the followers of Buddhism usually avoid the term God, for it savors so much of Christianity, whose spirit is not always exactly in accord with the Buddhist interpretation of religious experience. Again, Buddhism is not pantheistic in the sense that it identifies the universe with God. On the other hand, the Buddhist God is absolute and transcendent; this world, being merely its manifestation, is necessarily fragmental and imperfect. To define more exactly the Buddhist notion of the highest being, it may be convenient to borrow the term very happily coined by a modern German scholar, "panentheism," according to which God is πᾶν καὶ ἕν (all and one) and more than the totality of existence. 
The essay then goes on to explain first utilizing the term "God" for the American audience to get an initial understanding of what he means by "panentheism," and then discusses the terms that Buddhism uses in place of "God" such as Dharmakaya, Buddha or AdiBuddha, and Tathagata.
See also.
</dl>

</doc>
<doc id="23592" url="http://en.wikipedia.org/wiki?curid=23592" title="Paraphilia">
Paraphilia

Paraphilia (also known as sexual perversion and sexual deviation) is the experience of intense sexual arousal to atypical objects, situations, or individuals. No consensus has been found for any precise border between unusual sexual interests and paraphilic ones. There is debate over which, if any, of the paraphilias should be listed in diagnostic manuals, such as the "Diagnostic and Statistical Manual of Mental Disorders" (DSM) or the International Classification of Diseases.
The number and taxonomy of paraphilias is under debate; one source lists as many as 549 types of paraphilias. The DSM-5 has specific listings for eight paraphilic disorders. Several sub-classifications of the paraphilias have been proposed, and some argue that a fully dimensional, spectrum or complaint-oriented approach would better reflect the evidence.
Terminology and stigma.
General.
Many terms have been used to describe atypical sexual interests, and there remains debate regarding technical accuracy and perceptions of stigma. Sexologist John Money popularized the term "paraphilia" as a non-pejorative designation for unusual sexual interests. Money described paraphilia as "a sexuoerotic embellishment of, or alternative to the official, ideological norm." Psychiatrist Glen Gabbard writes that despite efforts by Stekel and Money, "the term "paraphilia" remains pejorative in most circumstances."
Coinage of the term "paraphilia" ("paraphilie") has been credited to Friedrich Salomon Krauss in 1903, and it entered the English language in 1913, in reference to Krauss by urologist William J. Robinson. It was used with some regularity by Wilhelm Stekel in the 1920s. The term comes from the Greek παρά ("para") "beside" and φιλία ("-philia") "friendship, love".
In the late 19th century, psychologists and psychiatrists started to categorize various paraphilias as they wanted a more descriptive system than the legal and religious constructs of sodomy and perversion. Before the introduction of the term "paraphilia" in the DSM-III (1980), the term "sexual deviation" was used to refer to paraphilias in the first two editions of the manual. In 1981, an article published in "American Journal of Psychiatry" described paraphilia as "recurrent, intense sexually arousing fantasies, sexual urges, or behaviors generally involving:
Homosexuality and non-heterosexuality.
Homosexuality was at one time discussed as a sexual deviation. Sigmund Freud and subsequent psychoanalytic thinkers considered homosexuality and paraphilias to result from psychosexual non-normative relations to the Oedipal complex. As such, the term "sexual perversion" or the epithet "pervert" have historically referred to gay men, as well as other non-heterosexuals (people who fall out of the perceived norms of sexual orientation).
By the mid-20th century, mental health practitioners began formalizing "deviant sexuality" classifications into categories. Originally coded as 000-x63, homosexuality was the top of the classification list (Code 302.0) until the American Psychiatric Association removed homosexuality from the DSM in 1974. Martin Kafka writes, "Sexual disorders once considered paraphilias (e.g., homosexuality) are now regarded as variants of normal sexuality."
A 2012 literature study by James Cantor, clinical psychologist, comparing homosexuality with paraphilias found that homosexuality was sufficiently dissimilar from the paraphilias as to be considered an unrelated construct.
Political history.
Sexual deviance and paraphilia has been defined politically in many different ways. In the 20th century in western culture, the three main sources of political unrest in the arena of sexual deviance were from interracial relationships and marriages, same-sex relationships and same-sex marriage, and children born out of wedlock. Political control of sexuality through laws against sexual deviance has created a power-differential. People of color, women, and LGBT people's sex lives and sexuality are seen as a political problem and issue, taking away these people’s agency and ownership of their own sexuality.
Border between typical and atypical sexual interests.
Albert Eulenburg (1914) noted a commonality across the paraphilias, using the terminology of his time, "All the forms of sexual perversion...have one thing in common: their roots reach down into the matrix of natural and normal sex life; there they are somehow closely connected with the feelings and expressions of our physiological erotism. They are...hyperbolic intensifications, distortions, monstrous fruits of certain partial and secondary expressions of this erotism which is considered 'normal' or at least within the limits of healthy sex feeling."
The clinical literature contains reports of many paraphilias, only some of which receive their own entries in the diagnostic taxonomies of the American Psychiatric Association or the World Health Organization. There is disagreement regarding which sexual interests should be deemed paraphilic disorders versus normal variants of sexual interest. For example, as of May 2000, per DSM-IV-TR, "Because some cases of Sexual Sadism may not involve harm to a victim (e.g., inflicting humiliation on a consenting partner), the wording for sexual sadism involves a hybrid of the DSM-III-R and DSM-IV wording (i.e., "the person has acted on these urges with a non-consenting person, or the urges, sexual fantasies, or behaviors cause marked distress or interpersonal difficulty").
The DSM-IV-TR also acknowledges that the diagnosis and classification of paraphilias across cultures or religions "is complicated by the fact that what is considered deviant in one cultural setting may be more acceptable in another setting”. Some argue that cultural relativism is important to consider when discussing paraphilias, because there is wide variance concerning what is sexually acceptable across cultures.
Consensual adult activities and adult entertainment involving sexual roleplay, novel, superficial, or trivial aspects of sexual fetishism, or incorporating the use of sex toys are not necessarily paraphilic. Paraphilial psychopathology is not the same as psychologically normative adult human sexual behaviors, sexual fantasy, and sex play.
Intensity and specificity.
Clinicians distinguish between optional, preferred and exclusive paraphilias, though the terminology is not completely standardized. An "optional" paraphilia is an alternative route to sexual arousal. For example, a man with otherwise unremarkable sexual interests might in some cases seek or enhance sexual arousal by wearing women's underwear. In preferred paraphilias, a person prefers the paraphilia to conventional sexual activities, but also engages in conventional sexual activities.
The literature includes single-case studies of exceedingly rare and idiosyncratic paraphilias. These include an adolescent male who had a strong fetishistic interest in the exhaust pipes of cars, a young man with a similar interest in a specific type of car, and a man who had a paraphilic interest in sneezing (both his own and the sneezing of others).
Sex differences and research limitations.
Research has shown that paraphilias are rarely observed in women. However, there have been some studies on females with paraphilias. Sexual masochism has been found to be the most commonly observed paraphilia in women, with approximately 1 in 20 cases of sexual masochism being female.
Many acknowledge the scarcity of research on female paraphilias. The majority of paraphilia studies are conducted on people who have been convicted of sex crimes. Since the number of male convicted sex offenders far exceeds the number of female convicted sex offenders, research on paraphilic behavior in women is consequently lacking. Some researchers argue that an underrepresentation exists concerning pedophilia in females. Due to the low number of women in studies on pedophilia, most studies are based from "exclusively male samples". This likely underrepresentation may also be attributable to a "societal tendency to dismiss the negative impact of sexual relationships between young boys and adult women". Michele Elliott has done extensive research on child sexual abuse committed by females, publishing the book "Female Sexual Abuse of Children: The Last Taboo" in an attempt to challenge the gender-biased discourse surrounding sex crimes. John Hunsley states that physiological limitations in the study of female sexuality must also be acknowledged when considering research on paraphilias. He states that while a man's sexual arousal can be directly measured from his erection (see penile plethysmograph), a woman's sexual arousal cannot be measured as clearly (see vaginal photoplethysmograph), and therefore research concerning female sexuality is rarely as conclusive as research on men.
Classification as mental illness.
There is scientific and political controversy regarding the continued inclusion of sex-related diagnoses such as the paraphilias in the DSM, due to the stigma of being classified as a mental illness.
Some groups seeking greater understanding and acceptance of sexual diversity have lobbied for changes to the legal and medical status of unusual sexual interests and practices. Charles Allen Moser, a physician and advocate for sexual minorities, has argued that the diagnoses should be eliminated from diagnostic manuals.
DSM-I and DSM-II.
In American psychiatry, prior to the publication of the DSM-I, paraphilias were classified as cases of "psychopathic personality with pathologic sexuality". The DSM-I (1952) included sexual deviation as a personality disorder of sociopathic subtype. The only diagnostic guidance was that sexual deviation should have been "reserved for deviant sexuality which [was] not symptomatic of more extensive syndromes, such as schizophrenic or obsessional reactions". The specifics of the disorder were to be provided by the clinician as a "supplementary term" to the sexual deviation diagnosis; examples of this supplementary term provided in the DSM-I included homosexuality, transvestism, pedophilia, fetishism, and sexual sadism, including rape. There were no restrictions in the DSM-I on what this supplementary term could be. Researcher Anil Aggrawal writes that the 1952 first edition of the "Diagnostic and Statistical Manual of Mental Disorders" (DSM) specified "the type of the pathological behavior, such as homosexuality, transvestism, pedophilia, fetishism, and sexual sadism, including rape, sexual assault, mutilation."
The DSM-II (1968) continued to use the term "sexual deviations", but no longer ascribed them under personality disorders, but rather them alongside them in a broad category titled "personality disorders and certain other nonpsychotic mental disorders". The types of sexual deviations listed in the DSM-II were: sexual orientation disturbance (homosexuality), fetishism, pedophilia, transvestitism (sic), exhibitionism, voyeurism, sadism, masochism, and "other sexual deviation". No definition or examples were provided for "other sexual deviation", but the general category of sexual deviation was meant to describe the sexual preference of individuals that was "directed primarily toward objects other than people of opposite sex, toward sexual acts not usually associated with coitus, or toward coitus performed under bizarre circumstances, as in necrophilia, pedophilia, sexual sadism, and fetishism." Except for the removal of homosexuality from the DSM-III onwards, this definition provided a general standard that has guided specific definitions of paraphilias in subsequent DSM editions, up to DSM-IV-TR.
DSM-III through DSM-IV.
The term "paraphilia" was introduced in the DSM-III (1980) as a subset of the new category of "psychosexual disorders."
The DSM-III-R (1987) renamed the broad category to sexual disorders, renamed atypical paraphilia to paraphilia NOS (not otherwise specified), renamed transvestism as transvestic fetishism, added frotteurism, and moved zoophilia to the NOS category. It also provided seven nonexhaustive examples of NOS paraphilias, which besides zoophilia included telephone scatologia, necrophilia, partialism, coprophilia, klismaphilia, and urophilia.
The DSM-IV (1994) retained the sexual disorders classification for paraphilias, but added an even broader category, "sexual and gender identity disorders," which includes them. The DSM-IV retained the same types of paraphilias listed in DSM-III-R, including the NOS examples, but introduced some changes to the definitions of some specific types.
DSM-IV-TR.
The DSM-IV-TR describes paraphilias as "recurrent, intense sexually arousing fantasies, sexual urges or behaviors generally involving (1) nonhuman objects, (2) the suffering or humiliation of oneself or one's partner, or (3) children or other nonconsenting persons that occur over a period of 6 months" (Criterion A), which "cause clinically significant distress or impairment in social, occupational, or other important areas of functioning" (Criterion B). DSM-IV-TR names eight specific paraphilic disorders (Exhibitionism, Fetishism, Frotteurism, Pedophilia, Sexual masochism, Sexual sadism, Voyeurism, and Transvestic fetishism, plus a residual category, Paraphilia—Not Otherwise Specified). Criterion B differs for exhibitionism, frotteurism, and pedophilia to include acting on these urges, and for sadism, acting on these urges with a nonconsenting person. Sexual arousal in association with objects that were designed for sexual purposes is not diagnosable.
Some paraphilias may interfere with the capacity for sexual activity with consenting adult partners.
In the current version of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR), a paraphilia is not diagnosable as a psychiatric disorder unless it causes distress to the individual or harm to others.
DSM-5.
The DSM-5 adds a distinction between "paraphilias" and "paraphilic disorders", stating that paraphilias do not require or justify psychiatric treatment in themselves, and defining "paraphilic disorder" as "a paraphilia that is currently causing distress or impairment to the individual or a paraphilia whose satisfaction has entailed personal harm, or risk of harm, to others".
The DSM-5 Paraphilias Subworkgroup reached a "consensus that paraphilias are not "ipso facto" psychiatric disorders", and proposed "that the DSM-V make a distinction between "paraphilias" and paraphilic "disorders". [...] One would "ascertain" a paraphilia (according to the nature of the urges, fantasies, or behaviors) but "diagnose" a paraphilic disorder (on the basis of distress and impairment). In this conception, having a paraphilia would be a necessary but not a sufficient condition for having a paraphilic disorder." The 'Rationale' page of any paraphilia in the electronic DSM-5 draft continues: "This approach leaves intact the distinction between normative and non-normative sexual behavior, which could be important to researchers, but without automatically labeling non-normative sexual behavior as psychopathological. It also eliminates certain logical absurdities in the DSM-IV-TR. In that version, for example, a man cannot be classified as a transvestite—however much he cross-dresses and however sexually exciting that is to him—unless he is unhappy about this activity or impaired by it. This change in viewpoint would be reflected in the diagnostic criteria sets by the addition of the word "Disorder" to all the paraphilias. Thus, Sexual Sadism would become Sexual Sadism Disorder; Sexual Masochism would become Sexual Masochism Disorder, and so on."
Bioethics professor Alice Dreger interpreted these changes as "a subtle way of saying sexual kinks are basically okay – so okay, the sub-work group doesn’t actually bother to define paraphilia. But a paraphilic disorder is defined: that’s when an atypical sexual interest causes distress or impairment to the individual or harm to others." Interviewed by Dreger, Ray Blanchard, the Chair of the Paraphilias Sub-Work Group, explained: "We tried to go as far as we could in depathologizing mild and harmless paraphilias, while recognizing that severe paraphilias that distress or impair people or cause them to do harm to others are validly regarded as disorders."
Charles Allen Moser pointed out that this change is not really substantive as DSM-IV already acknowledged a difference between paraphilias and non-pathological but unusual sexual interests, a distinction that is virtually identical to what is being proposed for DSM-5, and it is a distinction that, in practice, has often been ignored. Linguist Andrew Clinton Hinderliter argued that "Including some sexual interests—but not others—in the DSM creates a fundamental asymmetry and communicates a negative value judgment against the sexual interests included," and leaves the paraphilias in a situation similar to ego-dystonic homosexuality, which was removed from the DSM because it did not meet the DSM's definition of mental disorder.
The DSM-5 acknowledges that many dozens of paraphilias exist, but only has specific listings for eight that are forensically important and relatively common. These are voyeuristic disorder, exhibitionistic disorder, frotteuristic disorder, sexual masochism disorder, sexual sadism disorder, pedophilic disorder, fetishistic disorder, and transvestic disorder. Other paraphilias can be diagnosed under the Other Specified Paraphilic Disorder or Unspecified Paraphilic Disorder listings, if accompanied by distress or impairment.
Causes.
The causes of paraphilic sexual preferences in people are unclear, although a growing body of research points to a possible prenatal neurodevelopmental correlation. A 2008 study analyzing the sexual fantasies of 200 heterosexual men by using the Wilson Sex Fantasy Questionnaire exam, determined that males with a pronounced degree of fetish interest had a greater number of older brothers, a high 2D:4D digit ratio (which would indicate excessive prenatal estrogen exposure), and an elevated probability of being left-handed, suggesting that disturbed hemispheric brain lateralization may play a role in deviant attractions.
Behavioral explanations propose that paraphilias are conditioned early in life, during an experience that pairs the paraphilic stimulus with intense sexual arousal. Susan Nolen-Hoeksema suggests that, once established, masturbatory fantasies about the stimulus reinforce and broaden the paraphilic arousal.
Treatments.
The treatment of paraphilias and related disorders has been challenging for patients and clinicians. In the past, surgical castration was advocated as a therapy for men with pedophilia, but has been abandoned for the time being because most governments consider it a cruel punishment where the express willingness and consent of the patient is not objectively indicated. Psychotherapy, self-help groups, and pharmacotherapy (including anti-androgen hormone therapy sometimes referred to as "chemical castration") have all been used. Other drug treatments for these disorders do exist, however.
Antiandrogens such as cyproterone acetate and medroxyprogesterone acetate have been widely used as therapy in these men to reduce sex drive. However, their efficacy is limited and they have many side effects, including breast growth, headaches, weight gain, and reduction in bone density. Even if compliance is good, only 60 to 80 percent of men benefit from this type of drug. Long-acting gonadotropin-releasing hormones, such as Triptorelin (Trelstar) which reduces the release of gonadotropin hormones, are also used. This drug is a synthetic hormone which may also lead to reduced sex drive.
Psychostimulants have been used recently to augment the effects of serotonergic drugs in paraphiliacs. In theory, the prescription of a psychostimulant without pretreatment with an SSRI might further disinhibit sexual behavior, but when taken together, the psychostimulant may actually reduce impulsive tendencies. Methylphenidate (Ritalin) is a substituted phenethylamine stimulant used primarily to manage the symptoms of attention deficit hyperactivity disorder (ADHD). Recent studies imply that methylphenidate may also act on serotonergic systems. Amphetamine is also used medically as an adjunct to antidepressants in refractory cases of depression.
Legal issues.
In the United States, following a series of landmark cases in the Supreme Court of the United States, persons diagnosed with paraphilias and a history of anti-social behavior, particularly pedophilia ("Kansas v. Hendricks", 1997) and exhibitionism ("Kansas v. Crane", 2002), can be held indefinitely in civil confinement under various state legislation generically known as sexually violent predator laws and the federal Adam Walsh Act ("United States v. Comstock", 2010).

</doc>
<doc id="23593" url="http://en.wikipedia.org/wiki?curid=23593" title="Pediatrics">
Pediatrics

Pediatrics (also spelled paediatrics or pædiatrics) is the branch of medicine that deals with the medical care of infants, children, and adolescents, and the age limit usually ranges from birth up to 18 years of age (in some places until completion of secondary education, and until age 21 in the United States). A medical practitioner who specializes in this area is known as a pediatrician, or paediatrician. The word "paediatrics" and its cognates mean "healer of children"; they derive from two Greek words: παῖς ("pais" "child") and ἰατρός ("iatros" "doctor, healer"). Pediatricians work both in hospitals, particularly those working in its specialized subfields such as neonatology, and as primary care physicians who specialize in children.
History.
Pediatrics is a relatively new medical specialty. Hippocrates, Aristotle, Celsus, Soranus, and Galen, understood the differences in growing and maturing organisms that necessitated different treatment: "Ex toto non sic pueri ut viri curari debent" ( "In general, boys should not be treated in the same way as men."Celsus).
Some of the oldest traces of pediatrics can be discovered in Ancient India where children's doctors were called as "kumara bhrtya". "Sushruta Samhita" an ayurvedic text, composed during the sixth century BC contains the text about pediatrics. Another ayurvedic text from this period is "Kashyapa Samhita".
A second century AD manuscript by the Greek physician and gynecologist Soranus of Ephesus dealt with neonatal pediatrics. Byzantine physicians Oribasius, Aëtius of Amida, Alexander Trallianus, and Paulus Aegineta contributed to the field. The Byzantines also built "brephotrophia" (crêches). Islamic writers served as a bridge for Greco-Roman and Byzantine medicine and added ideas of their own, especially Haly Abbas, Serapion, Avicenna, and Averroes. The Persian scholar and doctor al-Razi (865–925) published a short treatise on diseases among children. The first book about pediatrics was "Libellus [Opusculum] de aegritudinibus et remediis infantium" 1472 ("Little Book on Children Diseases and Treatment"), by the Italian pediatrician Paolo Bagellardo. In sequence came Bartholomäus Metlinger's "Ein Regiment der Jungerkinder" 1473, Cornelius Roelans (1450-1525) no title Buchlein, or Latin compendium, 1483, and Heinrich von Louffenburg (1391-1460) "Versehung des Leibs" written in 1429 (published 1491), together form the "Pediatric Incunabula", four great medical treatises on children's physiology and pathology.
The Swedish physician Nils Rosén von Rosenstein (1706–1773) is considered to be the founder of modern pediatrics as a medical specialty, while his work "The diseases of children, and their remedies" (1764) is considered to be "the first modern textbook on the subject". Pediatrics as a specialized field of medicine continued to develope in the mid-19th century; Abraham Jacobi (1830–1919) is known as the father of pediatrics in the USA because of his many contributions to the field. He was born in Germany, where he received his medical training, but later practiced in New York City.
The first generally accepted pediatric hospital is the "Hôpital des Enfants Malades" (French: Hospital for Sick Children), which opened in Paris in June 1802 on the site of a previous orphanage. From its beginning, this famous hospital accepted patients up to the age of fifteen years, and it continues to this day as the pediatric division of the Necker-Enfants Malades Hospital, created in 1920 by merging with the physically contiguous "Necker Hospital", founded in 1778.
In other European countries, the Charité (a hospital founded in 1710) in Berlin established a separate Pediatric Pavilion in 1830, followed by similar institutions at Sankt Petersburg in 1834, and at Vienna and Breslau (now Wrocław), both in 1837. In 1852 Britain's first pediatric hospital, the Hospital for Sick Children, Great Ormond Streets. The first Children's hospital in Scotland opened in 1860 in Edinburgh. In the US, the first similar institutions were the Children's Hospital of Philadelphia, which opened in 1855, and then Boston Children's Hospital (1869).
Differences between adult and pediatric medicine.
The body size differences are paralleled by maturational changes. The smaller body of an infant or neonate is substantially different physiologically from that of an adult. Congenital defects, genetic variance, and developmental issues are of greater concern to pediatricians than they often are to adult physicians.
A major difference between pediatrics and adult medicine is that children are minors and, in most jurisdictions, cannot make decisions for themselves. The issues of guardianship, privacy, legal responsibility and informed consent must always be considered in every pediatric procedure. In a sense, pediatricians often have to treat the parents and sometimes, the family, rather than just the child. Adolescents are in their own legal class, having rights to their own health care decisions in certain circumstances. In basic terms, pediatricians take care of all of the children's needs from emotional support to medical support.
Training of pediatricians.
The training of pediatricians varies considerably across the world. Depending on jurisdiction and university, a medical degree course may be either undergraduate-entry or graduate-entry. The former commonly takes five or six years, and has been usual in the Commonwealth. Entrants to graduate-entry courses (as in the US), usually lasting four or five years, have previously completed a three- or four-year university degree, commonly but by no means always in sciences. Medical graduates hold a degree specific to the country and university in and from which they graduated. This degree qualifies that medical practitioner to become licensed or registered under the laws of that particular country, and sometimes of several countries, subject to requirements for "internship" or "conditional registration".
Pediatricians must undertake further training in their chosen field. This may take from four to eleven or more years, (depending on jurisdiction and the degree of specialization). The post-graduate training for a primary care physician, including primary care pediatricians, is generally not as lengthy as for a hospital-based medical specialist.
In most jurisdictions, entry-level degrees are common to all branches of the medical profession, but in some jurisdictions, specialization in pediatrics may begin before completion of this degree. In some jurisdictions, pediatric training is begun immediately following completion of entry-level training. In other jurisdictions, junior medical doctors must undertake generalist (unstreamed) training for a number of years before commencing pediatric (or any other) specialization. Specialist training is often largely under the control of pediatric organizations (see below) rather than universities,and depending on jurisdiction.
Subspecialties.
Subspecialties of pediatrics include:

</doc>
<doc id="23597" url="http://en.wikipedia.org/wiki?curid=23597" title="Physiology">
Physiology

Physiology (; from " "φύσις" (physis)", meaning "nature, origin", and " "-λογία" (-logia)", meaning "study of") is the scientific study of the normal function in living systems. A sub-discipline of biology, its focus is in how organisms, organ systems, organs, cells, and bio-molecules carry out the chemical or physical functions that exist in a living system. Given the size of the field it is divided into, among others, animal physiology (including that of human), plant physiology, cellular physiology, microbial physiology (see microbial metabolism), bacterial physiology, and viral physiology. Nobel Prize in Physiology or Medicine is awarded to those who make significant achievements in this discipline since 1901 by the Royal Swedish Academy of Sciences. In medicine, a physiologic state is one occurring from normal body function, rather than pathologically, which is centered on the abnormalities that occur in animal diseases, including humans. 
History.
Physiological studies date back to ancient civilizations of India, Egypt alongside anatomical studies but did not utilize dissections and vivisection.
The study of human physiology as a medical field dates back to at least 420 BC to the time of Hippocrates, also known as the "father of medicine." Hippocrates incorporated his belief system called the theory of humours, which consisted of four basic substance: earth, water, air and fire. Each substance is known for having a corresponding humour: black bile, phlegm, blood and yellow bile, respectively. Hippocrates also noted some emotional connections to the four humours, which Claudis Galenus would later expand on. The critical thinking of Aristotle and his emphasis on the relationship between structure and function marked the beginning of physiology in Ancient Greece. Like Hippocrates, Aristotle took to the humeral theory of disease, which also consisted of four primary qualities in life: hot,cold, wet and dry. Claudius Galenus (c. ~130–200 AD), known as Galen of Pergamum, was the first to use experiments to probe the functions of the body. Unlike Hippocrates though, Galen argued that humoral imbalances can be located in specific organs, including the entire body. His modification of this theory better equipped doctors to make more precise diagnoses. Galen also played off of Hippocrates idea that emotions were also tied to the humours, and added the notion of temperaments: sanguine corresponds with blood; phlegmatic is tied to phlegm; yellow bile is connected to choleric; and black bile corresponds with melancholy. Galen also saw the human body consisting of three connected systems: the brain and nerves, which are responsible for thoughts and sensations; the heart and arteries, which give life; and the liver and veins, which can be attributed to nutrition and growth. To top it off, Galen was also the founder of experimental physiology. And for the next 1,400 years, Galenic physiology was a powerful and influential tool in medicine. 
Jean Fernel (1497 - 1558), a French physician, introduced the term "physiology".
In the 19th century, physiological knowledge began to accumulate at a rapid rate, in particular with the 1838 appearance of the Cell theory of Matthias Schleiden and Theodor Schwann. It radically stated that organisms are made up of units called cells. Claude Bernard's (1813–1878) further discoveries ultimately led to his concept of "milieu interieur" (internal environment), which would later be taken up and championed as "homeostasis" by American physiologist Walter B. Cannon in 1929. By homeostasis, Cannon meant "the maintenance of steady states in the body and the physiological processes through which they are regulated." In other words, the body's ability to regulate its internal environment. It should be noted that, William Beaumont was the first American to utilize the practical application of physiology.
1858- Joseph Lister studied the cause of blood coagulation and inflammation that resulted after previous injuries and surgical wounds. He later discovered and implemented antiseptics in the operating room, and as a result decreases death rate from surgery by a substantial amount.
1891- Ivan Pavlov performed research on "conditional reflexes" that involved dogs' saliva production in response to a plethora of sounds and visual stimuli. 
In the 20th century, biologists also became interested in how organisms other than human beings function, eventually spawning the fields of comparative physiology and ecophysiology. Major figures in these fields include Knut Schmidt-Nielsen and George Bartholomew. Most recently, evolutionary physiology has become a distinct subdiscipline.
1910- August Krogh, in 1920 won the Nobel Prize for discovering how, in capillaries, blood flow is regulated.
1954- Andre Huxley and Hugh Huxley, alongside their research team, discovered the sliding filaments in skeletal muscle, known today as the sliding filament theory. 
Today, and times before, physiologists continuously trying to find answers to important questions concerning how populations interact, the environment on earth, and in single cell functions. 
Subdisciplines.
There are many ways to categorize the subdiscplines of physiology:
Human physiology.
Human physiology seeks to understand the mechanisms that work to keep the human body alive and functioning, through scientific enquiry into the nature of mechanical, physical, and biochemical functions of humans, their organs, and the cells of which they are composed. The principal level of focus of physiology is at the level of organs and systems within systems. The endocrine and nervous systems play major roles in the reception and transmission of signals that integrate function in animals. Homeostasis is a major aspect with regard to such interactions within plants as well as animals. The biological basis of the study of physiology, integration refers to the overlap of many functions of the systems of the human body, as well as its accompanied form. It is achieved through communication that occurs in a variety of ways, both electrical and chemical.
Much of the foundation of knowledge in human physiology was provided by animal experimentation. Physiology is the study of function and is closely related to anatomy which is the study of form. Due to the frequent connection between form and function, physiology and anatomy are intrinsically linked and are studied in tandem as part of a medical curriculum.
Gender Equality and Prominent Early Female Contributors.
Initially, women were largely excluded from official involvement in any physiological society.The American Physiological Society was founded in 1887 and included only men in its ranks. In 1902 the American Physiological Society elected Ida Hyde as the first female member of the society. Ida worked very hard to promote gender equality in every aspect of science and medicine. She was a representative of American Association of University Women and a global advocate for gender equality in education. 
Soon thereafter, in 1913, J.S. Haldane proposed that women be allowed to formally join The Society of Physiology, which had been founded in 1876. On July 3rd, 1915, six women were officially admitted into The Society. These six included, Florence Buchanan, Winifred Cullis, Ruth C. Skelton, Sarah C. M. Sowton, Constance Leetham Terry and Enid M. Tribe. Male members of The Society submitted each of these women for consideration and then voted on whether or not the women's accomplishments and potential merited membership in The Society. 
Though the acceptance of women into these prestigious physiological societies did not mark an end to inequality, it was a pivotal moment in determining the course of the next century.
Women in Modern Physiology.
Getty Cori (with husband Carl Cori) received a Nobel Prize in Physiology or Medicine in 1947 for their discovery the phosphate-containing from glucose (glycogen) and how it works within an animal metabolism for energy. They discovered the Cori Cycle (also known as the Lactic Acid Cycle) which is when the muscle converts glycogen to lactic acid, which is carried by the bloodstream to the liver where it is converted to glycogen then is broken down to glucose that will be carried back to the muscles and converted back to glycogen and used as an energy source. 
Gertrude Elion (with George Hitchings and Sir James Black) received the Nobel Prize for Physiology or Medicine in 1988 for their development of drugs to treat several major diseases. She first worked Hitchings assistant then colleague developing drugs for leukemia, autoimmune disorder, gout, malaria, and viral herpes through analyzing the biochemistry of normal cell and those that are cancer cells, bacteria, viruses, and other pathogens. 
Linda Buck (with Richard Axel) received a Nobel Prize in Physiology or Medicine in 2004 for their discoveries of odorant receptors and the organization of the olfactory system. 
Francoise Barre-Sinoussi (with Luc Montaginer) received a Nobel Prized in Physiology or Medicine in 2008 for their work in identifying the human immunodeficiency virus (HIV), the cause of acquired immunodeficiency syndrome (AIDS). Through dissection of an infected patients’ lymph node, they determined that AIDS was caused by a retrovirus, known as HIV. Their work led to the development of new antiviral drugs and diagnostic methods.
Elizabeth Blackburn (with Carol Greider and Jack Szostak) was awarded the 2009 Nobel Prize for Physiology or Medicine for the discoveries of the genetic composition and function of telomeres and the enzyme called telomerase. Her studies include the interactions of these cellular components and their roles in cancer and aging. 
Susan Wray Ph.D Cellular and Molecular Physiologist researched and published her paper (on March 15, 2015) about the inhibitory effect of visfatin and leptin on human and rat myometrial contractility. Results from her data showed that visfatin inhibits myometrial contractility more than leptin. Data suggests that the increased output of visfatin and leptin in obese pregnant women may impair uterine contractility resulting in an unplanned Caesarean delivery.

</doc>
<doc id="23601" url="http://en.wikipedia.org/wiki?curid=23601" title="Pi">
Pi

The number π is a mathematical constant, the ratio of a circle's circumference to its diameter, commonly approximated as 3.14159. It has been represented by the Greek letter "π" since the mid-18th century, though it is also sometimes spelled out as "pi" ().
Being an irrational number, π cannot be expressed exactly as a common fraction, although fractions such as 22/7 and other rational numbers are commonly used to approximate π. Consequently its decimal representation never ends and never settles into a permanent repeating pattern. The digits appear to be randomly distributed; however, to date, no proof of this has been discovered. Also, π is a transcendental number – a number that is not the root of any non-zero polynomial having rational coefficients. This transcendence of π implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge.
Although ancient civilizations needed the value of π to be computed accurately for practical reasons, it was not calculated to more than seven digits, using geometrical techniques, in Chinese mathematics and to about five in Indian mathematics in the 5th century CE. The historically first exact formula for π, based on infinite series, was not available until a millennium later, when in the 14th century the Madhava–Leibniz series was discovered in Indian mathematics. In the 20th and 21st centuries, mathematicians and computer scientists discovered new approaches that, when combined with increasing computational power, extended the decimal representation of π to, as of late 2013, over 13.3 trillion (1013) digits. Scientific applications generally require no more than 40 digits of π so the primary motivation for these computations is the human desire to break records. However, the extensive calculations involved have been used to test supercomputers and high-precision multiplication algorithms.
Because its definition relates to the circle, π is found in many formulae in trigonometry and geometry, especially those concerning circles, ellipses or spheres. It is also found in formulae used in other branches of science such as cosmology, number theory, statistics, fractals, thermodynamics, mechanics and electromagnetism. The ubiquity of π makes it one of the most widely known mathematical constants both inside and outside the scientific community: Several books devoted to it have been published, the number is celebrated on Pi Day and record-setting calculations of the digits of π often result in news headlines. Attempts to memorize the value of π with increasing precision have led to records of over 67,000 digits.
Fundamentals.
Name.
The symbol used by mathematicians to represent the ratio of a circle's circumference to its diameter is the lowercase Greek letter π, sometimes spelled out as "pi". In English, π is pronounced as "pie" ( , ). In mathematical use, the lowercase letter π (or π in sans-serif font) is distinguished from its capital counterpart Π, which denotes a product of a sequence.
The choice of the symbol π is discussed in the section "Adoption of the symbol π".
Definition.
π is commonly defined as the ratio of a circle's circumference "C" to its diameter "d":
The ratio "C"/"d" is constant, regardless of the circle's size. For example, if a circle has twice the diameter of another circle it will also have twice the circumference, preserving the ratio "C"/"d". This definition of π implicitly makes use of flat (Euclidean) geometry; although the notion of a circle can be extended to any curved (non-Euclidean) geometry, these new circles will no longer satisfy the formula π = "C"/"d". There are also other definitions of π that do not immediately involve circles at all. For example, π is twice the smallest positive "x" for which cos("x") equals 0.
Properties.
π is an irrational number, meaning that it cannot be written as the ratio of two integers (fractions such as 22/7 are commonly used to approximate π; no common fraction (ratio of whole numbers) can be its exact value). Since π is irrational, it has an infinite number of digits in its decimal representation, and it does not settle into an infinitely repeating pattern of digits. There are several proofs that π is irrational; they generally require calculus and rely on the reductio ad absurdum technique. The degree to which π can be approximated by rational numbers (called the irrationality measure) is not precisely known; estimates have established that the irrationality measure is larger than the measure of "e" or ln(2) but smaller than the measure of Liouville numbers.
More strongly, π is a transcendental number, which means that it is not the solution of any non-constant polynomial with rational coefficients, such as "x"5/120 − "x"3/6 + "x" = 0.
The transcendence of π has two important consequences: First, π cannot be expressed using any finite combination of rational numbers and square roots or "n"-th roots such as 3√31 or √10. Second, since no transcendental number can be constructed with compass and straightedge, it is not possible to "square the circle". In other words, it is impossible to construct, using compass and straightedge alone, a square whose area is equal to the area of a given circle. Squaring a circle was one of the important geometry problems of the classical antiquity. Amateur mathematicians in modern times have sometimes attempted to square the circle and sometimes claim success despite the fact that it is impossible.
The digits of π have no apparent pattern and have passed tests for statistical randomness, including tests for normality; a number of infinite length is called normal when all possible sequences of digits (of any given length) appear equally often. The conjecture that π is normal has not been proven or disproven. Since the advent of computers, a large number of digits of π have been available on which to perform statistical analysis. Yasumasa Kanada has performed detailed statistical analyses on the decimal digits of π and found them consistent with normality; for example, the frequency of the ten digits 0 to 9 were subjected to statistical significance tests, and no evidence of a pattern was found. Despite the fact that π's digits pass statistical tests for randomness, π contains some sequences of digits that may appear non-random to non-mathematicians, such as the Feynman point, which is a sequence of six consecutive 9s that begins at the 762nd decimal place of the decimal representation of π.
Continued fractions.
Like all irrational numbers, π cannot be represented as a common fraction (also known as a simple or vulgar fraction), by the very definition of "irrational". But every irrational number, including π, can be represented by an infinite series of nested fractions, called a continued fraction:
Truncating the continued fraction at any point generates a fraction that provides an approximation for π; two such fractions (22/7 and 355/113) have been used historically to approximate the constant. Each approximation generated in this way is a best rational approximation; that is, each is closer to π than any other fraction with the same or a smaller denominator. Because π is known to be transcendental, it is by definition not algebraic and so cannot be a quadratic irrational. Therefore π cannot have a periodic continued fraction. Although the simple continued fraction for π (shown above) also does not exhibit any other obvious pattern, mathematicians have discovered several generalized continued fractions that do, such as:
Approximate value.
Some approximations of "pi" include:
History.
Antiquity.
The Great Pyramid at Giza, constructed  2589–2566 BC, was built with a perimeter of about 1760 cubits and a height of about 280 cubits; the ratio 1760/280 ≈ 6.2857 is approximately equal to 2π ≈ 6.2832. Based on this ratio, some Egyptologists concluded that the pyramid builders had knowledge of π and deliberately designed the pyramid to incorporate the proportions of a circle. Others maintain that the suggested relationship to π is merely a coincidence, because there is no evidence that the pyramid builders had any knowledge of π, and because the dimensions of the pyramid are based on other factors.
The earliest written approximations of π are found in Egypt and Babylon, both within 1 percent of the true value. In Babylon, a clay tablet dated 1900–1600 BC has a geometrical statement that, by implication, treats π as 25/8 = 3.1250. In Egypt, the Rhind Papyrus, dated around 1650 BC, but copied from a document dated to 1850 BC has a formula for the area of a circle that treats π as (16/9)2 ≈ 3.1605.
In India around 600 BC, the "Shulba Sutras" (Sanskrit texts that are rich in mathematical contents) treat π as (9785/5568)2 ≈ 3.088. In 150 BC, or perhaps earlier, Indian sources treat π as √10 ≈ 3.1622.
Two verses in the Hebrew Bible (written between the 8th and 3rd centuries BC) describe a ceremonial pool in the Temple of Solomon with a diameter of ten cubits and a circumference of thirty cubits; the verses imply π is about three if the pool is circular. Rabbi Nehemiah explained the discrepancy as being due to the thickness of the vessel. His early work of geometry, "Mishnat ha-Middot", was written around 150 AD and takes the value of π to be three and one seventh. (See Approximations of π)
Polygon approximation era.
The first recorded algorithm for rigorously calculating the value of π was a geometrical approach using polygons, devised around 250 BC by the Greek mathematician Archimedes. This polygonal algorithm dominated for over 1,000 years, and as a result π is sometimes referred to as "Archimedes' constant". Archimedes computed upper and lower bounds of π by drawing a regular hexagon inside and outside a circle, and successively doubling the number of sides until he reached a 96-sided regular polygon. By calculating the perimeters of these polygons, he proved that 223/71 < π < 22/7 (that is 3.1408 < π < 3.1429). Archimedes' upper bound of 22/7 may have led to a widespread popular belief that π is equal to 22/7. Around 150 AD, Greek-Roman scientist Ptolemy, in his "Almagest", gave a value for π of 3.1416, which he may have obtained from Archimedes or from Apollonius of Perga. Mathematicians using polygonal algorithms reached 39 digits of π in 1630, a record only broken in 1699 when infinite series were used to reach 71 digits.
In ancient China, values for π included 3.1547 (around 1 AD), √10 (100 AD, approximately 3.1623), and 142/45 (3rd century, approximately 3.1556). Around 265 AD, the Wei Kingdom mathematician Liu Hui created a polygon-based iterative algorithm and used it with a 3,072-sided polygon to obtain a value of π of 3.1416. Liu later invented a faster method of calculating π and obtained a value of 3.14 with a 96-sided polygon, by taking advantage of the fact that the differences in area of successive polygons form a geometric series with a factor of 4. The Chinese mathematician Zu Chongzhi, around 480 AD, calculated that π ≈ 355/113 (a fraction that goes by the name "Milü" in Chinese), using Liu Hui's algorithm applied to a 12,288-sided polygon. With a correct value for its seven first decimal digits, this value of 3.141592920... remained the most accurate approximation of π available for the next 800 years.
The Indian astronomer Aryabhata used a value of 3.1416 in his "Āryabhaṭīya" (499 AD). Fibonacci in c. 1220 computed 3.1418 using a polygonal method, independent of Archimedes. Italian author Dante apparently employed the value 3+√2/10 ≈ 3.14142.
The Persian astronomer Jamshīd al-Kāshī produced 9 sexagesimal digits, roughly the equivalent of 16 decimal digits, in 1424 using a polygon with 3×228 sides, which stood as the world record for about 180 years. French mathematician François Viète in 1579 achieved 9 digits with a polygon of 3×217 sides. Flemish mathematician Adriaan van Roomen arrived at 15 decimal places in 1593. In 1596, Dutch mathematician Ludolph van Ceulen reached 20 digits, a record he later increased to 35 digits (as a result, π was called the "Ludolphian number" in Germany until the early 20th century). Dutch scientist Willebrord Snellius reached 34 digits in 1621, and Austrian astronomer Christoph Grienberger arrived at 38 digits in 1630 using 1040 sides, which remains the most accurate approximation manually achieved using polygonal algorithms.
Infinite series.
The calculation of π was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute π with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for π most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute π was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his "Tantrasamgraha", around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, "Yuktibhāṣā", from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite series are described, including series for sine, tangent, and cosine, which are now referred to as the Madhava series or Gregory–Leibniz series. Madhava used infinite series to estimate π to 11 digits around 1400, but that value was improved on around 1430 by the Persian mathematician Jamshīd al-Kāshī, using a polygonal algorithm.
The first infinite sequence discovered in Europe was an infinite product (rather than an infinite sum, which are more typically used in π calculations) found by French mathematician François Viète in 1593:
The second infinite sequence found in Europe, by John Wallis in 1655, was also an infinite product. The discovery of calculus, by English scientist Isaac Newton and German mathematician Gottfried Wilhelm Leibniz in the 1660s, led to the development of many infinite series for approximating π. Newton himself used an arcsin series to compute a 15 digit approximation of π in 1665 or 1666, later writing "I am ashamed to tell you to how many figures I carried these computations, having no other business at the time."
In Europe, Madhava's formula was rediscovered by Scottish mathematician James Gregory in 1671, and by Leibniz in 1674:
This formula, the Gregory–Leibniz series, equals π/4 when evaluated with "z" = 1. In 1699, English mathematician Abraham Sharp used the Gregory–Leibniz series to compute π to 71 digits, breaking the previous record of 39 digits, which was set with a polygonal algorithm. The Gregory–Leibniz series is simple, but converges very slowly (that is, approaches the answer gradually), so it is not used in modern π calculations.
In 1706 John Machin used the Gregory–Leibniz series to produce an algorithm that converged much faster:
Machin reached 100 digits of π with this formula. Other mathematicians created variants, now known as Machin-like formulae, that were used to set several successive records for calculating digits of π. Machin-like formulae remained the best-known method for calculating π well into the age of computers, and were used to set records for 250 years, culminating in a 620-digit approximation in 1946 by Daniel Ferguson – the best approximation achieved without the aid of a calculating device.
A record was set by the calculating prodigy Zacharias Dase, who in 1844 employed a Machin-like formula to calculate 200 decimals of π in his head at the behest of German mathematician Carl Friedrich Gauss. British mathematician William Shanks famously took 15 years to calculate π to 707 digits, but made a mistake in the 528th digit, rendering all subsequent digits incorrect.
Rate of convergence.
Some infinite series for π converge faster than others. Given the choice of two infinite series for π, mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate π to any given accuracy. A simple infinite series for π is the Gregory–Leibniz series:
As individual terms of this infinite series are added to the sum, the total gradually gets closer to π, and – with a sufficient number of terms – can get as close to π as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of π.
An infinite series for π (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is:
The following table compares the convergence rates of these two series:
After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of π, whereas the sum of Nilakantha's series is within 0.002 of the correct value of π. Nilakantha's series converges faster and is more useful for computing digits of π. Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term.
Irrationality and transcendence.
Not all mathematical advances relating to π were aimed at increasing the accuracy of approximations. When Euler solved the Basel problem in 1735, finding the exact value of the sum of the reciprocal squares, he established a connection between π and the prime numbers that later contributed to the development and study of the Riemann zeta function:
Swiss scientist Johann Heinrich Lambert in 1761 proved that π is irrational, meaning it is not equal to the quotient of any two whole numbers. Lambert's proof exploited a continued-fraction representation of the tangent function. French mathematician Adrien-Marie Legendre proved in 1794 that π2 is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that π is transcendental, confirming a conjecture made by both Legendre and Euler.
Adoption of the symbol π.
The earliest known use of the Greek letter π to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in his 1706 work "Synopsis Palmariorum Matheseos; or, a New Introduction to the Mathematics". The Greek letter first appears there in the phrase "1/2 Periphery (π)" in the discussion of a circle with radius one. Jones may have chosen π because it was the first letter in the Greek spelling of the word "periphery". However, he writes that his equations for π are from the "ready pen of the truly ingenious Mr. John Machin", leading to speculation that Machin may have employed the Greek letter before Jones. It had indeed been used earlier for geometric concepts. William Oughtred used π and δ, the Greek letter equivalents of p and d, to express ratios of periphery and diameter in the 1647 and later editions of "Clavis Mathematicae".
After Jones introduced the Greek letter in 1706, it was not adopted by other mathematicians until Euler started using it, beginning with his 1736 work "Mechanica". Before then, mathematicians sometimes used letters such as "c" or "p" instead. Because Euler corresponded heavily with other mathematicians in Europe, the use of the Greek letter spread rapidly. In 1748, Euler used π in his widely read work "Introductio in analysin infinitorum" (he wrote: "for the sake of brevity we will write this number as π; thus π is equal to half the circumference of a circle of radius 1") and the practice was universally adopted thereafter in the Western world.
Modern quest for more digits.
Computer era and iterative algorithms.
The Gauss–Legendre iterative algorithm:Initialize
Iterate
Then an estimate for π is given by
The development of computers in the mid-20th century again revolutionized the hunt for digits of π. American mathematicians John Wrench and Levi Smith reached 1,120 digits in 1949 using a desk calculator. Using an inverse tangent (arctan) infinite series, a team led by George Reitwiesner and John von Neumann that same year achieved 2,037 digits with a calculation that took 70 hours of computer time on the ENIAC computer. The record, always relying on an arctan series, was broken repeatedly (7,480 digits in 1957; 10,000 digits in 1958; 100,000 digits in 1961) until 1 million digits were reached in 1973.
Two additional developments around 1980 once again accelerated the ability to compute π. First, the discovery of new iterative algorithms for computing π, which were much faster than the infinite series; and second, the invention of fast multiplication algorithms that could multiply large numbers very rapidly. Such algorithms are particularly important in modern π computations, because most of the computer's time is devoted to multiplication. They include the Karatsuba algorithm, Toom–Cook multiplication, and Fourier transform-based methods.
The iterative algorithms were independently published in 1975–1976 by American physicist Eugene Salamin and Australian scientist Richard Brent. These avoid reliance on infinite series. An iterative algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges to the desired value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, in what is now termed the arithmetic–geometric mean method (AGM method) or Gauss–Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent–Salamin algorithm.
The iterative algorithms were widely used after 1980 because they are faster than infinite series algorithms: whereas infinite series typically increase the number of correct digits additively in successive terms, iterative algorithms generally "multiply" the number of correct digits at each step. For example, the Brent-Salamin algorithm doubles the number of digits in each iteration. In 1984, the Canadian brothers John and Peter Borwein produced an iterative algorithm that quadruples the number of digits in each step; and in 1987, one that increases the number of digits five times in each step. Iterative methods were used by Japanese mathematician Yasumasa Kanada to set several records for computing π between 1995 and 2002. This rapid convergence comes at a price: the iterative algorithms require significantly more memory than infinite series.
Motivations for computing π.
For most numerical calculations involving π, a handful of digits provide sufficient precision. According to Jörg Arndt and Christoph Haenel, thirty-nine digits are sufficient to perform most cosmological calculations, because that is the accuracy necessary to calculate the volume of the known universe with a precision of one atom. Despite this, people have worked strenuously to compute π to thousands and millions of digits. This effort may be partly ascribed to the human compulsion to break records, and such achievements with π often make headlines around the world. They also have practical benefits, such as testing supercomputers, testing numerical analysis algorithms (including high-precision multiplication algorithms); and within pure mathematics itself, providing data for evaluating the randomness of the digits of π.
Rapidly convergent series.
Modern π calculators do not use iterative algorithms exclusively. New infinite series were discovered in the 1980s and 1990s that are as fast as iterative algorithms, yet are simpler and less memory intensive. The fast iterative algorithms were anticipated in 1914, when the Indian mathematician Srinivasa Ramanujan published dozens of innovative new formulae for π, remarkable for their elegance, mathematical depth, and rapid convergence. One of his formulae, based on modular equations, is
This series converges much more rapidly than most arctan series, including Machin's formula. Bill Gosper was the first to use it for advances in the calculation of π, setting a record of 17 million digits in 1985. Ramanujan's formulae anticipated the modern algorithms developed by the Borwein brothers and the Chudnovsky brothers. The Chudnovsky formula developed in 1987 is
It produces about 14 digits of π per term, and has been used for several record-setting π calculations, including the first to surpass 1 billion (109) digits in 1989 by the Chudnovsky brothers, 2.7 trillion (2.7×1012) digits by Fabrice Bellard in 2009, and 10 trillion (1013) digits in 2011 by Alexander Yee and Shigeru Kondo. For similar formulas, see also the Ramanujan–Sato series.
In 2006, Canadian mathematician Simon Plouffe used the PSLQ integer relation algorithm to generate several new formulas for π, conforming to the following template:
where "q" is eπ (Gelfond's constant), "k" is an odd number, and "a", "b", "c" are certain rational numbers that Plouffe computed.
Spigot algorithms.
Two algorithms were discovered in 1995 that opened up new avenues of research into π. They are called spigot algorithms because, like water dripping from a spigot, they produce single digits of π that are not reused after they are calculated. This is in contrast to infinite series or iterative algorithms, which retain and use all intermediate digits until the final result is produced.
American mathematicians Stan Wagon and Stanley Rabinowitz produced a simple spigot algorithm in 1995. Its speed is comparable to arctan algorithms, but not as fast as iterative algorithms.
Another spigot algorithm, the BBP digit extraction algorithm, was discovered in 1995 by Simon Plouffe:
This formula, unlike others before it, can produce any individual hexadecimal digit of π without calculating all the preceding digits. Individual binary digits may be extracted from individual hexadecimal digits, and octal digits can be extracted from one or two hexadecimal digits. Variations of the algorithm have been discovered, but no digit extraction algorithm has yet been found that rapidly produces decimal digits. An important application of digit extraction algorithms is to validate new claims of record π computations: After a new record is claimed, the decimal result is converted to hexadecimal, and then a digit extraction algorithm is used to calculate several random hexadecimal digits near the end; if they match, this provides a measure of confidence that the entire computation is correct.
Between 1998 and 2000, the distributed computing project PiHex used Bellard's formula (a modification of the BBP algorithm) to compute the quadrillionth (1015th) bit of π, which turned out to be 0. In September 2010, a Yahoo! employee used the company's Hadoop application on one thousand computers over a 23-day period to compute 256 bits of π at the two-quadrillionth (2×1015th) bit, which also happens to be zero.
Use.
Because π is closely related to the circle, it is found in many formulae from the fields of geometry and trigonometry, particularly those concerning circles, spheres, or ellipses. Formulae from other branches of science also include π in some of their important formulae, including sciences such as statistics, fractals, thermodynamics, mechanics, cosmology, number theory, and electromagnetism.
Geometry and trigonometry.
π appears in formulae for areas and volumes of geometrical shapes based on circles, such as ellipses, spheres, cones, and tori. Below are some of the more common formulae that involve π.
The formulae above are special cases of the surface area "S""n"("r") and volume "V""n"("r") of an "n"-dimensional sphere.
formula_18
formula_19
π appears in definite integrals that describe circumference, area, or volume of shapes generated by circles. For example, an integral that specifies half the area of a circle of radius one is given by:
In that integral the function √1-"x"2 represents the top half of a circle (the square root is a consequence of the Pythagorean theorem), and the integral ∫ computes the area between that half of a circle and the "x" axis.
The trigonometric functions rely on angles, and mathematicians generally use radians as units of measurement. π plays an important role in angles measured in radians, which are defined so that a complete circle spans an angle of 2π radians. The angle measure of 180° is equal to π radians, and 1° = π/180 radians.
Common trigonometric functions have periods that are multiples of π; for example, sine and cosine have period 2π, so for any angle "θ" and any integer "k",
formula_21 and formula_22
Monte Carlo methods.
Monte Carlo methods, which evaluate the results of multiple random trials, can be used to create approximations of π. Buffon's needle is one such technique: If a needle of length "ℓ" is dropped "n" times on a surface on which parallel lines are drawn "t" units apart, and if "x" of those times it comes to rest crossing a line ("x" > 0), then one may approximate π based on the counts:
Another Monte Carlo method for computing π is to draw a circle inscribed in a square, and randomly place dots in the square. The ratio of dots inside the circle to the total number of dots will approximately equal π/4.
Monte Carlo methods for approximating π are very slow compared to other methods, and are never used to approximate π when speed or accuracy are desired.
Complex numbers and analysis.
Any complex number, say "z", can be expressed using a pair of real numbers. In the polar coordinate system, one number (radius or "r") is used to represent "z"'s distance from the origin of the complex plane and the other (angle or φ) to represent a counter-clockwise rotation from the positive real line as follows:
where "i" is the imaginary unit satisfying "i"2 = −1. The frequent appearance of π in complex analysis can be related to the behavior of the exponential function of a complex variable, described by Euler's formula:
where the constant "e" is the base of the natural logarithm. This formula establishes a correspondence between imaginary powers of "e" and points on the unit circle centered at the origin of the complex plane. Setting "φ" = π in Euler's formula results in Euler's identity, celebrated by mathematicians because it contains the five most important mathematical constants:
There are "n" different complex numbers "z" satisfying "z""n" = 1, and these are called the ""n"-th roots of unity". They are given by this formula:
Cauchy's integral formula governs complex analytic functions and establishes an important relationship between integration and differentiation, including the fact that the values of a complex function within a closed boundary are entirely determined by the values on the boundary:
An occurrence of π in the Mandelbrot set fractal was discovered by American David Boll in 1991. He examined the behavior of the Mandelbrot set near the "neck" at (−0.75, 0). If points with coordinates (−0.75, ε) are considered, as ε tends to zero, the number of iterations until divergence for the point multiplied by ε converges to π. The point (0.25, ε) at the cusp of the large "valley" on the right side of the Mandelbrot set behaves similarly: the number of iterations until divergence multiplied by the square root of ε tends to π.
The gamma function extends the concept of factorial (normally defined only for non-negative integers) to all complex numbers, except the negative real integers. When the gamma function is evaluated at half-integers, the result contains π; for example formula_29 and formula_30. The gamma function can be used to create a simple approximation to "n"! for large "n": formula_31 which is known as Stirling's approximation.
Number theory and Riemann zeta function.
The Riemann zeta function "ζ"("s") is used in many areas of mathematics. When evaluated at "s" = 2 it can be written as
Finding a simple solution for this infinite series was a famous problem in mathematics called the Basel problem. Leonhard Euler solved it in 1735 when he showed it was equal to π2/6. Euler's result leads to the number theory result that the probability of two random numbers being relatively prime (that is, having no shared factors) is equal to 6/π2. This probability is based on the observation that the probability that any number is divisible by a prime "p" is 1/"p" (for example, every 7th integer is divisible by 7.) Hence the probability that two numbers are both divisible by this prime is 1/"p"2, and the probability that at least one of them is not is 1-1/"p"2. For distinct primes, these divisibility events are mutually independent; so the probability that two numbers are relatively prime is given by a product over all primes:
This probability can be used in conjunction with a random number generator to approximate π using a Monte Carlo approach.
Probability and statistics.
The fields of probability and statistics frequently use the normal distribution as a simple model for complex phenomena; for example, scientists generally assume that the observational error in most experiments follows a normal distribution. π is found in the Gaussian function (which is the probability density function of the normal distribution) with mean μ and standard deviation σ:
The area under the graph of the normal distribution curve is given by the Gaussian integral:
while the related integral for the Cauchy distribution is
Outside mathematics.
Describing physical phenomena.
Although not a physical constant, π appears routinely in equations describing fundamental principles of the universe, often because of π's relationship to the circle and to spherical coordinate systems. A simple formula from the field of classical mechanics gives the approximate period "T" of a simple pendulum of length "L", swinging with a small amplitude ("g" is the earth's gravitational acceleration):
One of the key formulae of quantum mechanics is Heisenberg's uncertainty principle, which shows that the uncertainty in the measurement of a particle's position (Δ"x") and momentum (Δ"p") cannot both be arbitrarily small at the same time (where "h" is Planck's constant):
In the domain of cosmology, π appears in Einstein's field equation, a fundamental formula which forms the basis of the general theory of relativity and describes the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy:
where "R""ik" is the Ricci curvature tensor, "R" is the scalar curvature, "g""ik" is the metric tensor, Λ is the cosmological constant, "G" is Newton's gravitational constant, "c" is the speed of light in vacuum, and "T""ik" is the stress–energy tensor.
Coulomb's law, from the discipline of electromagnetism, describes the electric field between two electric charges ("q"1 and "q"2) separated by distance "r" (with "ε"0 representing the vacuum permittivity of free space):
The fact that π is approximately equal to 3 plays a role in the relatively long lifetime of orthopositronium. The inverse lifetime to lowest order in the fine structure constant "α" is
where "m" is the mass of the electron.
π is present in some structural engineering formulae, such as the buckling formula derived by Euler, which gives the maximum axial load "F" that a long, slender column of length "L", modulus of elasticity "E", and area moment of inertia "I" can carry without buckling:
The field of fluid dynamics contains π in Stokes' law, which approximates the frictional force"F" exerted on small, spherical objects of radius "R", moving with velocity "v" in a fluid with dynamic viscosity "η":
The Fourier transform, defined below, is a mathematical operation that expresses time as a function of frequency, known as its frequency spectrum. It has many applications in physics and engineering, particularly in signal processing.
Under ideal conditions (uniform gentle slope on an homogeneously erodible substrate), the sinuosity of a meandering river approaches π. The sinuosity is the ratio between the actual length and the straight-line distance from source to mouth. Faster currents along the outside edges of a river's bends cause more erosion than along the inside edges, thus pushing the bends even farther out, and increasing the overall loopiness of the river. However, that loopiness eventually causes the river to double back on itself in places and "short-circuit", creating an ox-bow lake in the process. The balance between these two opposing factors leads to an average ratio of π between the actual length and the direct distance between source and mouth.
Memorizing digits.
Many persons have memorized large numbers of digits of π, a practice called piphilology. One common technique is to memorize a story or poem in which the word lengths represent the digits of π: The first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. An early example of a memorization aid, originally devised by English scientist James Jeans, is "How I want a drink, alcoholic of course, after the heavy lectures involving quantum mechanics." When a poem is used, it is sometimes referred to as a "piem". Poems for memorizing π have been composed in several languages in addition to English.
The record for memorizing digits of π, certified by "Guinness World Records", is 67,890 digits, recited in China by Lu Chao in 24 hours and 4 minutes on 20 November 2005. In 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records. Record-setting π memorizers typically do not rely on poems, but instead use methods such as remembering number patterns and the method of loci.
A few authors have used the digits of π to establish a new form of constrained writing, where the word lengths are required to represent the digits of π. The "Cadaeic Cadenza" contains the first 3835 digits of π in this manner, and the full-length book "Not a Wake" contains 10,000 words, each representing one digit of π.
In popular culture.
Perhaps because of the simplicity of its definition and its ubiquitous presence in formulae, π has been represented in popular culture more than other mathematical constructs.
In the 2008 Open University and BBC documentary co-production, The Story of Maths, aired in October 2008 on BBC Four, British mathematician Marcus du Sautoy shows a visualization of the - historically first exact - formula for calculating the π when visiting India and exploring its contributions to trigonometry.
In the Palais de la Découverte (a science museum in Paris) there is a circular room known as the "pi room". On its wall are inscribed 707 digits of π. The digits are large wooden characters attached to the dome-like ceiling. The digits were based on an 1853 calculation by English mathematician William Shanks, which included an error beginning at the 528th digit. The error was detected in 1946 and corrected in 1949.
In Carl Sagan's novel "Contact" it is suggested that the creator of the universe buried a message deep within the digits of π. The digits of π have also been incorporated into the lyrics of the song "Pi" from the album "Aerial" by Kate Bush, and a song by Hard 'n Phirm.
Many schools in the United States observe Pi Day on 14 March (written 3/14 in the US style). π and its digital representation are often used by self-described "math geeks" for inside jokes among mathematically and technologically minded groups. Several college cheers at the Massachusetts Institute of Technology include "3.14159". Pi Day in 2015 was particularly significant because the date and time 3/14/15 9:26:53 reflected many more digits of pi.
During the 2011 auction for Nortel's portfolio of valuable technology patents, Google made a series of unusually specific bids based on mathematical and scientific constants, including π.
In 1958 Albert Eagle proposed replacing π by τ = π/2 to simplify formulas. However, no other authors are known to use tau in this way. Some people use a different value for tau, τ = 6.283185... = 2π, arguing that τ, as the number of radians in one turn or as the ratio of a circle's circumference to its radius rather than its diameter, is more natural than π and simplifies many formulas. Celebrations of this number, because it approximately equals 6.28, by making 28 June "Tau Day" and eating "twice the pie", have been reported in the media. However this use of τ has not made its way into mainstream mathematics.
In 1897, an amateur American mathematician attempted to persuade the Indiana legislature to pass the Indiana Pi Bill, which described a method to square the circle and contained text that implied various incorrect values for π, including 3.2. The bill is notorious as an attempt to establish a value of scientific constant by legislative fiat. The bill was passed by the Indiana House of Representatives, but rejected by the Senate.
Notes.
</dl>
Further reading.
</dl>

</doc>
<doc id="23603" url="http://en.wikipedia.org/wiki?curid=23603" title="Postmodernism">
Postmodernism

Postmodernism is a late-20th-century movement in the arts, architecture, and criticism that was a departure from modernism. Postmodernism includes skeptical interpretations of culture, literature, art, philosophy, history, economics, architecture, fiction, and literary criticism. It is often associated with deconstruction and post-structuralism because its usage as a term gained significant popularity at the same time as twentieth-century post-structural thought.
The term "postmodernism" has been applied to a host of movements, mainly in art, music, and literature, that reacted against tendencies in modernism, and are typically marked by revival of historical elements and techniques.
History.
The term "postmodern" was first used around the 1870s. John Watkins Chapman suggested "a Postmodern style of painting" as a way to depart from French Impressionism. J. M. Thompson, in his 1914 article in "The Hibbert Journal" (a quarterly philosophical review), used it to describe changes in attitudes and beliefs in the critique of religion: "The raison d'etre of Post-Modernism is to escape from the double-mindedness of Modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition."
In 1921 and 1925, postmodernism had been used to describe new forms of art and music. In 1942 H. R. Hays described it as a new literary form. However, as a general theory for a historical movement it was first used in 1939 by Arnold J. Toynbee: "Our own Post-Modern Age has been inaugurated by the general war of 1914–1918".
In 1949 the term was used to describe a dissatisfaction with modern architecture, and led to the postmodern architecture movement, perhaps also a response to the modernist architectural movement known as the International Style. Postmodernism in architecture is marked by a re-emergence of surface ornament, reference to surrounding buildings in urban architecture, historical reference in decorative forms (eclecticism), and non-orthogonal angles.
In 1971, in a lecture delivered at the Institute of Contemporary Art, London, Mel Bochner described "post-modernism" in art as having started with Jasper Johns, "who first rejected sense-data and the singular point-of-view as the basis for his art, and treated art as a critical investigation."
More recently, Walter Truett Anderson described postmodernism as belonging to one of four typological world views, which he identifies as either (a) Postmodern-ironist, which sees truth as socially constructed, (b) Scientific-rational, in which truth is found through methodical, disciplined inquiry, (c) Social-traditional, in which truth is found in the heritage of American and Western civilization, or (d) Neo-romantic, in which truth is found through attaining harmony with nature and/or spiritual exploration of the inner self.
Postmodernist ideas in philosophy and the analysis of culture and society expanded the importance of critical theory and has been the point of departure for works of literature, architecture, and design, as well as being visible in marketing/business and the interpretation of history, law and culture, starting in the late 20th century. These developments—re-evaluation of the entire Western value system (love, marriage, popular culture, shift from industrial to service economy) that took place since the 1950s and 1960s, with a peak in the Social Revolution of 1968—are described with the term "Postmodernity", as opposed to "Postmodernism", a term referring to an opinion or movement. Postmodernism has also been used interchangeably with the term post-structuralism out of which postmodernism grew, a proper understanding of postmodernism or doing justice to the postmodernist thought demands an understanding of the poststructuralist movement and the ideas of its advocates. Post-structuralism resulted similarly to postmodernism by following a time of structuralism. It is characterized by new ways of thinking through structuralism, contrary to the original form. "Postmodernist" describes part of a movement; "Postmodern" places it in the period of time since the 1950s, making it a part of contemporary history.
Influence on art.
Architecture.
The idea of Postmodernism in architecture began as a response to the perceived blandness and failed Utopianism of the Modern movement. Modern Architecture, as established and developed by Walter Gropius and Le Corbusier, was focused on the pursuit of a perceived ideal perfection, and attempted harmony of form and function, and dismissal of "frivolous ornament." Critics of modernism argued that the attributes of perfection and minimalism themselves were subjective, and pointed out anachronisms in modern thought and questioned the benefits of its philosophy. Definitive postmodern architecture such as the work of Michael Graves and Robert Venturi rejects the notion of a 'pure' form or 'perfect' architectonic detail, instead conspicuously drawing from all methods, materials, forms and colors available to architects.
Modernist Ludwig Mies van der Rohe is associated with the phrase "less is more"; in contrast Venturi famously said, "Less is a bore." Postmodernist architecture was one of the first aesthetic movements to openly challenge Modernism as antiquated and "totalitarian", favoring personal preferences and variety over objective, ultimate truths or principles.
It is this atmosphere of criticism, skepticism, and emphasis on difference over and against unity that distinguishes the postmodernism aesthetic. Among writers defining the terms of this discourse is Charles Jencks, described by Architectural Design Magazine as "the definer of Post-Modernism for thirty years" and the "internationally acclaimed critic..., whose name became synonymous with Post-modernism in the 80s".
Urban planning.
Postmodernism is a rejection of 'totality', of the notion that planning could be 'comprehensive', widely applied regardless of context, and rational. In this sense, Postmodernism is a rejection of its predecessor: Modernism. From the 1920s onwards, the Modern movement sought to design and plan cities which followed the logic of the new model of industrial mass production; reverting to large-scale solutions, aesthetic standardisation and prefabricated design solutions (Goodchild 1990). Postmodernism also brought a break from the notion that planning and architecture could result in social reform, which was an integral dimension of the plans of Modernism (Simonsen 1990). Furthermore, Modernism eroded urban living by its failure to recognise differences and aim towards homogenous landscapes (Simonsen 1990, 57). Within Modernism, urban planning represented a 20th-century move towards establishing something stable, structured, and rationalised within what had become a world of chaos, flux and change (Irving 1993, 475). The role of planners predating Postmodernism was one of the 'qualified professional' who believed they could find and implement one single 'right way' of planning new urban establishments (Irving 1993). In fact, after 1945, urban planning became one of the methods through which capitalism could be managed and the interests of developers and corporations could be administered (Irving 1993, 479).
Considering Modernism inclined urban planning to treat buildings and developments as isolated, unrelated parts of the overall urban ecosystems created fragmented, isolated, and homogeneous urban landscapes (Goodchild, 1990). One of the greater problems with Modernist-style of planning was the disregard of resident or public opinion, which resulted in planning being forced upon the majority by a minority consisting of affluent professionals with little to no knowledge of real 'urban' problems characteristic of post-Second World War urban environments: slums, overcrowding, deteriorated infrastructure, pollution and disease, among others (Irving 1993). These were precisely the 'urban ills' Modernism was meant to 'solve', but more often than not, the types of 'comprehensive', 'one size fits all' approaches to planning made things worse., and residents began to show interest in becoming involved in decisions which had once been solely entrusted to professionals of the built environment. Advocacy planning and participatory models of planning emerged in the 1960s to counter these traditional elitist and technocratic approaches to urban planning (Irving 1993; Hatuka & D'Hooghe 2007). Furthermore, an assessment of the 'ills' of Modernism among planners during the 1960s, fuelled development of a participatory model that aimed to expand the range of participants in urban interventions (Hatuka & D'Hooghe 2007, 21).
Jane Jacobs's 1961 book "The Death and Life of Great American Cities" was a sustained critique of urban planning as it had developed within Modernism and marked a transition from modernity to postmodernity in thinking about urban planning (Irving 1993, 479). However, the transition from Modernism to Postmodernism is often said to have happened at 3:32pm on 15 July in 1972, when Pruitt Igoe; a housing development for low-income people in St. Louis designed by architect Minoru Yamasaki, which had been a prize-winning version of Le Corbusier's 'machine for modern living' was deemed uninhabitable and was torn down (Irving 1993, 480). Since then, Postmodernism has involved theories that embrace and aim to create diversity, and it exalts uncertainty, flexibility and change (Hatuka & D'Hooghe 2007). Postmodern planning aims to accept pluralism and heighten awareness of social differences in order to accept and bring to light the claims of minority and disadvantaged groups (Goodchild 1990). It is important to note that urban planning discourse within Modernity and Postmodernity has developed in different contexts, even though they both grew within a capitalist culture. Modernity was shaped by a capitalist ethic of Fordist-Keynesian paradigm of mass, standardized production and consumption, while postmodernity was created out of a more flexible form of capital accumulation, labor markets and organisations (Irving 1993, 60). Also, there is a distinction between a postmodernism of 'reaction' and one of 'resistance'. A postmodernism of 'reaction' rejects Modernism and seeks to return to the lost traditions and history in order to create a new cultural synthesis, while Postmodernity of 'resistance' seeks to deconstruct Modernism and is a critique of the origins without necessarily returning to them (Irving 1993, 60). As a result of Postmodernism, planners are much less inclined to lay a firm or steady claim to there being one single 'right way' of engaging in urban planning and are more open to different styles and ideas of 'how to plan' (Irving 474).
Literature.
Literary postmodernism was officially inaugurated in the United States with the first issue of "boundary 2", subtitled "Journal of Postmodern Literature and Culture", which appeared in 1972. David Antin, Charles Olson, John Cage, and the Black Mountain College school of poetry and the arts were integral figures in the intellectual and artistic exposition of postmodernism at the time. "boundary 2" remains an influential journal in postmodernist circles today.
Jorge Luis Borges's (1939) short story "Pierre Menard, Author of the Quixote", is often considered as predicting postmodernism and conceiving the ideal of the ultimate parody. Samuel Beckett is sometimes seen as an important precursor and influence. Novelists who are commonly connected with postmodern literature include Vladimir Nabokov, William Gaddis, Umberto Eco, John Hawkes, William Burroughs, Giannina Braschi, Kurt Vonnegut, John Barth, Jean Rhys, Donald Barthelme, E.L. Doctorow, Richard Kalich, Jerzy Kosinski, Don DeLillo, Thomas Pynchon (Pynchon's work has also been described as "high modern"), Ishmael Reed, Kathy Acker, Ana Lydia Vega, Jachym Topol and Paul Auster.
In 1971, the Arab-American scholar Ihab Hassan published "The Dismemberment of Orpheus: Toward a Postmodern Literature," an early work of literary criticism from a postmodern perspective, in which the author traces the development of what he calls "literature of silence" through Marquis de Sade, Franz Kafka, Ernest Hemingway, Beckett, and many others, including developments such as the Theatre of the Absurd and the nouveau roman. In 'Postmodernist Fiction' (1987), Brian McHale details the shift from modernism to postmodernism, arguing that the former is characterized by an epistemological dominant, and that postmodern works have developed out of modernism and are primarily concerned with questions of ontology. In "Constructing Postmodernism" (1992), McHale's second book, he provides readings of postmodern fiction and of some of the contemporary writers who go under the label of cyberpunk. McHale's "What Was Postmodernism?" (2007), follows Raymond Federman's lead in now using the past tense when discussing postmodernism.
Music.
Postmodern music is either music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of the modernist. Because of this, Postmodern music is mostly defined in opposition to modernist music, and a work can either be modernist, or postmodern, but not both. Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude".
The postmodern impulse in classical music arose in the 1960s with the advent of musical minimalism. Composers such as Terry Riley, Henryk Górecki, Bradley Joseph, John Adams, George Crumb, Steve Reich, Philip Glass, Michael Nyman, and Lou Harrison reacted to the perceived elitism and dissonant sound of atonal academic modernism by producing music with simple textures and relatively consonant harmonies, whilst others, most notably John Cage challenged the prevailing narratives of beauty and objectivity common to Modernism. Some composers have been openly influenced by popular music and world ethnic musical traditions.
Postmodern Classical music as well is not a musical "style", but rather refers to music of the postmodern era. It bears the same relationship to postmodernist music that postmodernity bears to postmodernism. Postmodern music, on the other hand, shares characteristics with postmodernist art—that is, art that comes "after" and reacts "against" modernism (see Modernism in Music).
Though representing a general return to certain notions of music-making that are often considered to be classical or romantic, not all postmodern composers have eschewed the experimentalist or academic tenets of modernism. The works of Dutch composer Louis Andriessen, for example, exhibit experimentalist preoccupation that is decidedly anti-romantic. Eclecticism and freedom of expression, in reaction to the rigidity and aesthetic limitations of modernism, are the hallmarks of the postmodern influence in musical composition.
Graphic design.
Postmodern designers were in the beginning stages of what we now refer to as "graphic design". They created works beginning in the 1970s without any set adherence to rational order and formal organization. They also seemed to entirely pay no attention to traditional conventions such as legibility. Another characteristic of postmodern graphic design is that "retro, techno, punk, grunge, beach, parody, and pastiche were all conspicuous trends. Each had its own sites and venues, detractors and advocates". Yet, while postmodern design did not consist of one unified graphic style, the movement was an expressive and playful time for designers who searched for more and more ways to go against the system. Key influential postmodern graphic designers include Wolfgang Weingart, April Greiman, Tibor Kalman, and Jamie Reid.
Deconstruction.
One of the most well-known postmodernist concerns is "deconstruction," a theory for philosophy, literary criticism, and textual analysis developed by Jacques Derrida. The notion of a "deconstructive" approach implies an analysis that questions the already evident understanding of a text in terms of presuppositions, ideological underpinnings, hierarchical values, and frames of reference. A deconstructive approach further depends on the techniques of close reading without reference to cultural, ideological, moral opinions or information derived from an authority over the text such as the author. At the same time Derrida famously writes: "Il n'y a pas d'hors-texte ("there is no such thing as outside-of-the-text")." Derrida implies that the world follows the grammar of a text undergoing its own deconstruction. Derrida's method frequently involves recognizing and spelling out the different, yet similar interpretations of the meaning of a given text and the problematic implications of binary oppositions within the meaning of a text. Derrida's philosophy inspired a postmodern movement called deconstructivism among architects, characterized by the intentional fragmentation, distortion, and dislocation of architectural elements in designing a building. Derrida discontinued his involvement with the movement after the publication of his collaborative project with architect Peter Eisenmann in "Chora L Works: Jacques Derrida and Peter Eisenman".
Postmodernism and structuralism.
Structuralism was a philosophical movement developed by French academics in the 1950s, partly in response to French Existentialism. It has been seen variously as an expression of Modernism, High modernism, or postmodernism . "Post-structuralists" were thinkers who moved away from the strict interpretations and applications of structuralist ideas. Many American academics consider post-structuralism to be part of the broader, less well-defined postmodernist movement, even though many post-structuralists insisted it was not. Thinkers who have been called structuralists include the anthropologist Claude Lévi-Strauss, the linguist Ferdinand de Saussure, the Marxist philosopher Louis Althusser, and the semiotician Algirdas Greimas. The early writings of the psychoanalyst Jacques Lacan and the literary theorist Roland Barthes have also been called structuralist. Those who began as structuralists but became post-structuralists include Michel Foucault, Roland Barthes, Jean Baudrillard, Gilles Deleuze. Other post-structuralists include Jacques Derrida, Pierre Bourdieu, Jean-François Lyotard, Julia Kristeva, Hélène Cixous, and Luce Irigaray. The American cultural theorists, critics and intellectuals whom they influenced include Judith Butler, John Fiske, Rosalind Krauss, Avital Ronell, and Hayden White.
Post-structuralism is not defined by a set of shared axioms or methodologies, but by an emphasis on how various aspects of a particular culture, from its most ordinary, everyday material details to its most abstract theories and beliefs, determine one another. Post-structuralist thinkers reject Reductionism and Epiphenomenalism and the idea that cause-and-effect relationships are top-down or bottom-up. Like structuralists, they start from the assumption that people's identities, values and economic conditions determine each other rather than having "intrinsic" properties that can be understood in isolation. Thus the French structuralists considered themselves to be espousing Relativism and Constructionism. But they nevertheless tended to explore how the subjects of their study might be described, reductively, as a set of "essential" relationships, schematics, or mathematical symbols. (An example is Claude Lévi-Strauss's algebraic formulation of mythological transformation in "The Structural Study of Myth"). Post-structuralists thinkers went further, questioning the existence of any distinction between the nature of a thing and its relationship to other things.
Post-structuralism.
Post-structuralists generally reject the notion of formulations of "essential relations" in primitive cultures, languages, or descriptions of psychological phenomena being forms of Aristotelianism, rationalism, or idealism. Another common thread among thinkers associated with the post-structuralist movement is the criticism of the absolutist, quasi-scientific claims of structuralist theorists as more reflective of the mechanistic bias inspired by bureaucratization and industrialization than of the inner-workings of actual primitive cultures, languages or psyches. Generally, post-structuralists emphasize the inter-determination and contingency of social and historical phenomena with each other and with the cultural values and biases of perspective. Such realities were not to be dissected, in the manner of some structuralists, as a system of facts that could exist "independently" from values and paradigms (either those of the analysts or the subjects themselves), but to be understood as both causes and effects of each other. For this reason, most post-structuralists hold a more open-ended view of function within systems than did Structuralists and were sometimes accused of circularity and ambiguity. Post-structuralists countered that, when closely examined, all formalized claims describing phenomena, reality, or truth, rely on some form of circular reasoning and self-referential logic that is often paradoxical in nature. Thus, it was important to uncover the hidden patterns of circularity, self-reference and paradox within a given set of statements rather than feign objectivity, as such an investigation might allow new perspectives to have influence and new practices to be sanctioned or adopted. In this latter respect, post-structuralists were, as a group, continuing the philosophical project initiated by Martin Heidegger, and saw themselves as extending the implications of Friedrich Nietzsche's work.
Post-structuralist writing tends to connect observations and references from many, widely varying disciplines into a synthetic view of knowledge and its relationship to experience, the body, society and economy—a synthesis in which it sees itself as participating. Structuralists, while also somewhat inter-disciplinary, were more comfortable within departmental boundaries and often maintained the autonomy of their analytical methods over the objects they analyzed. Post-structuralists, unlike structuralists, did not privilege a system of (abstract) "relations" over the specifics to which such relations were applied, but tended to see the notion of "the relation" or of systemization itself as part-and-parcel of any stated conclusion rather than a reflection of reality as an independent, self-contained state or object. If anything, if a part of objective reality, theorization and systemization to Post-structuralists was an exponent of larger, more nebulous patterns of control in social orders—patterns that could not be encapsulated in theory without simultaneously conditioning it. For this reason, certain post-structural thinkers were also criticized by more realist, naturalist or essentialist thinkers of anti-intellectualism or anti-philosophy. Post-structuralists, in contrast to structuralists, tend to place a great deal of skepticism on the independence of theoretical premises from collective bias and the influence of power, and reject the notion of a "pure" or "scientific" methodology in social analysis, semiotics or philosophical speculation. No theory, they said—especially when concerning human society or psychology—was capable of reducing phenomena to elemental systems or abstract patterns, nor could abstract systems be dismissed as secondary derivatives of a fundamental nature: systemization, phenomena, and values were part of each other.
Post-postmodernism.
Recently metamodernism, post-postmodernism and the "death of postmodernism" have been widely debated: in 2007 Andrew Hoborek noted in his introduction to a special issue of the journal "Twentieth Century Literature" titled "After Postmodernism" that "declarations of postmodernism's demise have become a critical commonplace". A small group of critics has put forth a range of theories that aim to describe culture or society in the alleged aftermath of postmodernism, most notably Raoul Eshelman (performatism), Gilles Lipovetsky (hypermodernity), Nicolas Bourriaud (altermodern), and Alan Kirby (digimodernism, formerly called pseudo-modernism). None of these new theories and labels have so far gained very widespread acceptance. The exhibition "Postmodernism - Style and Subversion 1970–1990" at the Victoria and Albert Museum (London, 24 September 2011 – 15 January 2012) was billed as the first show to document postmodernism as a historical movement.
Criticisms.
Criticisms of postmodernism are intellectually diverse, including the assertions that postmodernism is meaningless and promotes obscurantism. For example, Noam Chomsky has argued that postmodernism is meaningless because it adds nothing to analytical or empirical knowledge. He asks why postmodernist intellectuals do not respond like people in other fields when asked, "what are the principles of their theories, on what evidence are they based, what do they explain that wasn't already obvious, etc.?...If [these requests] can't be met, then I'd suggest recourse to Hume's advice in similar circumstances: 'to the flames'." Christian philosopher William Lane Craig has noted "The idea that we live in a postmodern culture is a myth. In fact, a postmodern culture is an impossibility; it would be utterly unlivable. People are not relativistic when it comes to matters of science, engineering, and technology; rather, they are relativistic and pluralistic in matters of religion and ethics. But, of course, that's not postmodernism; that's modernism!"
Formal, academic critiques of postmodernism can also be found in works such as "Beyond the Hoax" and "Fashionable Nonsense".
However, as for continental philosophy, American academics have tended to label it "postmodernist", especially practitioners of "French Theory". Such a trend might derive from U.S. departments of Comparative Literature. It is interesting to note that Félix Guattari, often considered a "postmodernist", rejected its theoretical assumptions by arguing that the structuralist and postmodernist visions of the world were not flexible enough to seek explanations in psychological, social and environmental domains at the same time.
Philosopher Daniel Dennett declared, "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."
Further reading.
</dl>

</doc>
<doc id="23604" url="http://en.wikipedia.org/wiki?curid=23604" title="Photography">
Photography

Photography is the science, art and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.
Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically "developed" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.
Photography is employed in many fields of science, manufacturing (e.g. photolithography) and business, as well as its more direct uses for art, recreational purposes, and mass communication.
Etymology.
The word "photography" was created from the Greek roots φωτός ("phōtos"), genitive of φῶς ("phōs"), "light" and γραφή ("graphé") "representation by means of lines" or "drawing", together meaning "drawing with light".
Several people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, "photographie", in private notes which a Brazilian photography historian believes were written in 1834. Johann von Maedler, a Berlin astronomer, is credited in a 1932 German history of photography as having used it in an article published on 25 February 1839 in the German newspaper "Vossische Zeitung". Both of these claims are now widely reported but apparently neither has ever been independently confirmed as beyond reasonable doubt. Credit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.
History.
Precursor technologies.
Photography is the result of combining several technical discoveries. Long before the first photographs were made, Chinese philosopher Mo Di and Greek mathematicians Aristotle and Euclid described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments, Ibn al-Haytham (Alhazen) (965–1040) studied the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Techniques described in the Book of Optics are capable of producing primitive photographs using medieval materials.
Daniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book "Giphantie", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.
The discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural cameras obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to fix and retain the image produced by the camera obscura.
Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means "dark chamber" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.
The first success of reproducing images without a camera occurred when Thomas Wedgwood, from the famous family of potters, obtained copies of paintings on leather using silver salts. Since he had no way of permanently fixing those reproductions (stabilizing the image by washing out the non-exposed silver salts), they would turn completely black in the light and thus had to be kept in a dark room for viewing.
Plate photography.
Invented in the early decades of the 19th century, photography by means of the camera seemed able to capture more detail and information than traditional media, such as painting and sculpture. Photography as a usable process dates to the 1820s with the discovery of chemical photography. The first medium was photographic plate. The first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the "View from the Window at Le Gras", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).
Because Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. Working in partnership with Louis Daguerre, he discovered a somewhat more sensitive process that produced visually superior results, but it still required a few hours of exposure in the camera. Niépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process, the essential elements of which were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the approximately ten-minute-long exposure to be visible. Eventually, France agreed to pay Daguerre a pension for his process in exchange for the right to present his invention to the world as the gift of France, which occurred on 19 August 1839.
Meanwhile, in Brazil, Hercules Florence had already created his own process in 1832, naming it "Photographie", and an English inventor, William Fox Talbot, had created another method of making a reasonably light-fast silver process image but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, with exposures comparable to the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies, the basis of most chemical photography up to the present day. Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.
John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the "blueprint". He was the first to use the terms "photography", "negative" and "positive". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to "fix" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.
In the March 1851 issue of "The Chemist", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.
Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize for Physics in 1908.
Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography it has persisted into the 2010s.
Film photography.
Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.
The first flexible photographic roll film was marketed by George Eastman in 1885, but this original "film" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose ("celluloid"), now usually called "nitrate film".
Although cellulose acetate or "safety film" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.
Films remained dominant form of photography until early 21st century when advances in digital photography made them largely obsolete. Although 21st century is dominated by digital photography, film continues to be used by enthusiasts and format lovers.
Black-and-white.
All photography was originally monochrome, or "black-and-white". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its "classic" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray, but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, first used more than 150 years ago, produces brownish tones.
Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.
Color.
Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not "fix" the photograph to prevent the color from quickly fading when exposed to white light.
The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by physicist James Clerk Maxwell in 1855. Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.
Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color "fringes" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.
Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.
Autochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.
Kodachrome, the first modern "integral tripack" (or "monopack") color film, was introduced by Kodak in 1935. It captured the three color components in a multilayer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.
Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently available color films still employ a multilayer emulsion and the same principles, most closely resembling Agfa's product.
Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.
Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive "look".
Digital photography.
In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.
Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.
Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.
Synthesis photography.
Synthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.
Technical aspects.
The camera is the image-forming device, and photographic plate, photographic film or a silicon electronic image sensor is the sensing medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.
Photographers control the camera and lens to "expose" the light recording material to the required amount of light to form a "latent image" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.
The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).
As soon as photographic materials became "fast" (sensitive) enough for taking candid or surreptitious pictures, small "detective" cameras were made, some actually disguised as a book or handbag or pocket watch (the "Ticka" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.
The movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a "frame". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the "frame rate" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.
Camera controls.
In all but certain specialized cameras, the process of obtaining a usable exposure must involve the use, manually or automatically, of a few controls to ensure the photograph is clear, sharp and well illuminated. The controls usually include but are not limited to the following:
Many other elements of the imaging device itself may have a pronounced effect on the quality and/or aesthetic effect of a given photograph; among them are:
Exposure and rendering.
Camera controls are interrelated. The total amount of light reaching the film plane (the 'exposure') changes with the duration of exposure, aperture of the lens, and on the effective focal length of the lens (which in variable focal length lenses, can force a change in aperture as the lens is zoomed). Changing any of these controls can alter the exposure. Many cameras may be set to adjust most or all of these controls automatically. This automatic functionality is useful for occasional photographers in many situations.
The duration of an exposure is referred to as shutter speed, often even in cameras that do not have a physical shutter, and is typically measured in fractions of a second. It is quite possible to have exposures from one up to several seconds, usually for still-life subjects, and for night scenes exposure times can be several hours. However, for a subject that is in motion use a fast shutter speed. This will prevent the photograph from coming out blurry.
The effective aperture is expressed by an f-number or f-stop (derived from focal ratio), which is proportional to the ratio of the focal length to the diameter of the aperture. Longer lenses will pass less light even though the diameter of the aperture is the same due to the greater distance the light has to travel; shorter lenses (a shorter focal length) will be brighter with the same size of aperture.
The smaller the f/number, the larger the effective aperture. The present system of f/numbers to give the effective aperture of a lens was standardized by an international convention. There were earlier, different series of numbers in older cameras.
If the f-number is decreased by a factor of √2, the aperture diameter is increased by the same factor, and its area is increased by a factor of 2. The f-stops that might be found on a typical lens include 2.8, 4, 5.6, 8, 11, 16, 22, 32, where going up "one stop" (using lower f-stop numbers) doubles the amount of light reaching the film, and stopping down one stop halves the amount of light.
Image capture can be achieved through various combinations of shutter speed, aperture, and film or sensor speed. Different (but related) settings of aperture and shutter speed enable photographs to be taken under various conditions of film or sensor speed, lighting and motion of subjects and/or camera, and desired depth of field. A slower speed film will exhibit less "grain", and a slower speed setting on an electronic sensor will exhibit less "noise", while higher film and sensor speeds allow for a faster shutter speed, which reduces motion blur or allows the use of a smaller aperture to increase the depth of field.
For example, a wider aperture is used for lower light and a lower aperture for more light. If a subject is in motion, then a high shutter speed may be needed. A tripod can also be helpful in that it enables a slower shutter speed to be used.
For example, f/8 at 8 ms (1/125 of a second) and f/5.6 at 4 ms (1/250 of a second) yield the same amount of light. The chosen combination has an impact on the final result. The aperture and focal length of the lens determine the depth of field, which refers to the range of distances from the lens that will be in focus. A longer lens or a wider aperture will result in "shallow" depth of field (i.e. only a small plane of the image will be in sharp focus). This is often useful for isolating subjects from backgrounds as in individual portraits or macro photography.
Conversely, a shorter lens, or a smaller aperture, will result in more of the image being in focus. This is generally more desirable when photographing landscapes or groups of people. With very small apertures, such as pinholes, a wide range of distance can be brought into focus, but sharpness is severely degraded by diffraction with such small apertures. Generally, the highest degree of "sharpness" is achieved at an aperture near the middle of a lens's range (for example, f/8 for a lens with available apertures of f/2.8 to f/16). However, as lens technology improves, lenses are becoming capable of making increasingly sharp images at wider apertures.
Image capture is only part of the image forming process. Regardless of material, some process must be employed to render the latent image captured by the camera into a viewable image. With slide film, the developed film is just mounted for projection. Print film requires the developed film negative to be printed onto photographic paper or transparency. Prior to the advent of laser jet and inkjet printers, celluloid photographic negative images had to be mounted in an enlarger which projected the image onto a sheet of light-sensitive paper for a certain length of time (usually measured in seconds or fractions of a second). This sheet then was soaked in a chemical bath of developer (to bring out the image) followed immediately by a stop bath (to neutralize the progression of development and prevent the image from changing further once exposed to normal light). After this, the paper was hung until dry enough to safely handle. This post-production process allowed the photographer to further manipulate the final image beyond what had already been captured on the negative, adjusting the length of time the image was projected by the enlarger and the duration of both chemical baths to change the image's intensity, darkness, clarity, etc. This process is still employed by both amateur and professional photographers, but the advent of digital imagery means that the vast majority of modern photographic work is captured digitally and rendered via printing processes that are no longer dependent on chemical reactions to light. Such digital images may be uploaded to an image server (e.g., a photo-sharing web site), viewed on a television, or transferred to a computer or digital photo frame. Every type can then be produced as a hard copy on regular paper or photographic paper via a printer.
Prior to the rendering of a viewable image, modifications can be made using several controls. Many of these controls are similar to controls during image capture, while some are exclusive to the rendering process. Most printing controls have equivalent digital concepts, but some create different effects. For example, dodging and burning controls are different between digital and film processes. Other printing modifications include:
Other photographic techniques.
Stereoscopic.
Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as "3-D" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film, and more recently in digital electronic methods (including cellphone cameras).
Full-spectrum, ultraviolet and infrared.
Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.
Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.
Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).
Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.
Light field photography.
Digital methods of image capture and display processing have enabled the new technology of "light field photography" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected "after" the photograph has been captured. As explained by Michael Faraday in 1846, the "light field" is understood as 5-dimensional, with each point in 3-d space having attributes of two more angles that define the direction of each ray passing through that point.
These additional vector attributes can be captured optically through the use of microlenses at each pixel-point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.
Other imaging techniques.
Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.
Modes of production.
Amateur.
An amateur photographer is one who practices photography as a hobby and not for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera.
Commercial.
Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:
The market for photographic services demonstrates the aphorism "A picture is worth a thousand words", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.
Many people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.
Art.
During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.
At first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.
The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images "written with light"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.
Clive Bell in his classic essay "Art" states that only "significant form" can distinguish art from what is not art.
There must be some one quality without which a work of art cannot exist; possessing which, in the least degree, no work is altogether worthless. What is this quality? What quality is shared by all objects that provoke our aesthetic emotions? What quality is common to Sta. Sophia and the windows at Chartres, Mexican sculpture, a Persian bowl, Chinese carpets, Giotto's frescoes at Padua, and the masterpieces of Poussin, Piero della Francesca, and Cezanne? Only one answer seems possible — significant form. In each, lines and colors combined in a particular way, certain forms and relations of forms, stir our aesthetic emotions.
On 14 February 2004, Sotheby's London sold the 2001 photograph "99 Cent II Diptychon" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.
Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.
Science and forensics.
The camera has a long and distinguished history as a means of recording phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close up.
By 1853, Charles Brooke had invented a technology for the automatic registration of instruments by photography. These instruments included barometers, thermometers, psychrometers, and magnetometers, which recorded their readings by means of an automated photographic process.
Science uses image technology that has derived from the design of the Pin Hole camera. X-Ray machine are similar in design to Pin Hole cameras with high grade filters and laser radiation.
Photography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.
Social and cultural implications.
There are many ongoing questions about different aspects of photography. In her writing "On Photography" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, "To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power." Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines it can be argued that photography is a subjective form of representation.
Modern photography has raised a number of concerns on its impact on society. In Alfred Hitchcock's "Rear Window" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'. Michal Powell's "Peeping Tom" (1960) portrays the camera as both sexual and sadistically violent technology that literally kills in this picture and at the same time captures images of the pain and anguish evident on the faces of the female victims.
The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.
Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures, or are forbidden from combining elements of multiple photos to make "photomontages", passing them as "real" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allows digital fingerprinting of photos to detect tampering for purposes of forensic photography.
Photography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that "to photograph is to turn people into objects that can be symbolically possessed." Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.
One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a "tourist gaze"
in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a "reverse gaze" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.
Additionally, photography has been the topic of many songs in popular culture.
Law.
Photography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.

</doc>
<doc id="23607" url="http://en.wikipedia.org/wiki?curid=23607" title="Pentateuch (disambiguation)">
Pentateuch (disambiguation)

The Pentateuch is the first part of the Bible, consisting of Genesis, Exodus, Leviticus, Numbers, and Deuteronomy.
It can also refer to:

</doc>
<doc id="23612" url="http://en.wikipedia.org/wiki?curid=23612" title="Postmodern philosophy">
Postmodern philosophy

Postmodern philosophy is a philosophical direction which is critical of the foundational assumptions and universalizing tendency of Western philosophy. It emphasizes the importance of power relationships, personalization and discourse in the "construction" of truth and world views.
Postmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, and presence from absence.
Postmodern philosophy has strong relations with the substantial literature of critical theory.
Definitional issues.
Philosopher John Deely has argued for the contentious claim that the label "postmodern" for thinkers such as Derrida "et al." is "premature". Insofar as the "so-called" postmoderns follow the thoroughly "modern" trend of idealism, it is more an "ultra"modernism than anything else. A postmodernism that lives up to its name, therefore, must no longer confine itself to the premodern preoccupation with "things" nor with the modern confinement to "ideas," but must come to terms with the way of signs embodied in the semiotic doctrines of such thinkers as the Portuguese philosopher John Poinsot and the American philosopher Charles Sanders Peirce. Writes Deely,
The epoch of Greek and Latin philosophy was based on "being" in a quite precise sense: the existence exercised by things independently of human apprehension and attitude. The much briefer epoch of modern philosophy based itself rather on the instruments of human knowing, but in a way that unnecessarily compromised being. As the 20th century ends, there is reason to believe that a new philosophical epoch is dawning along with the new century, promising to be the richest epoch yet for human understanding. The postmodern era is positioned to synthesize at a higher level—the level of experience, where the being of things and the activity of the finite knower compenetrate one another and provide the materials whence can be derived knowledge of nature and knowledge of culture in their full symbiosis—the achievements of the ancients and the moderns in a way that gives full credit to the preoccupations of both. The postmodern era has for its distinctive task in philosophy the exploration of a new path, no longer the ancient way of things nor the modern way of ideas, but the way of signs, whereby the peaks and valleys of ancient and modern thought alike can be surveyed and cultivated by a generation which has yet further peaks to climb and valleys to find.
History.
Precursors.
While the idea of postmodernity had been around since the 1940s, postmodern philosophy originated primarily in France during the mid-20th century. However, several philosophical antecedents inform many of postmodern philosophy's concerns.
It was greatly influenced by the writings of Søren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including phenomenologists Edmund Husserl and Martin Heidegger, psychoanalyst Jacques Lacan, structuralist Roland Barthes, and the analytic philosopher, Ludwig Wittgenstein. Postmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage and artists who practiced collage, and the architecture of Las Vegas and the Pompidou Centre.
Early postmodern philosophers.
The most influential early postmodern philosophers were Jean Baudrillard, Jean-François Lyotard, and Jacques Derrida. Michel Foucault is also often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of "power", and changes fundamentally in different historical periods.
The writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a "postindustrial" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or "metanarratives") about knowledge and the world—comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new "language game" -- one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).
Derrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and "logos", as opposed to absence and markings or writings.
In America, the most famous pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the "Myth of the Given" allowed for an abandonment of the view of the thought or language as a mirror of a reality or external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.

</doc>
<doc id="23613" url="http://en.wikipedia.org/wiki?curid=23613" title="Postmodern music">
Postmodern music

Postmodern music is either simply music of the postmodern era, or music that follows aesthetical and philosophical trends of postmodernism. As the name suggests, the postmodernist movement formed partly in reaction to modernism. Even so, postmodern music still does not primarily define itself in opposition to modernist music; this label is applied instead by critics and theorists.
Postmodern music is not a distinct musical style, but rather refers to music of the postmodern era. The terms "postmodern", "postmodernism", "postmodernist", and "postmodernity" are exasperating terms . Indeed, postmodernists question the tight definitions and categories of academic disciplines, which they regard simply as the remnants of modernity .
The postmodernist musical attitude.
Postmodernism in music is not a distinct musical style, but rather refers to music of the postmodern era. Postmodernist music, on the other hand, shares characteristics with postmodernist art—that is, art that comes after and reacts against modernism (see Modernism in Music).
Fredric Jameson, a major figure in the thinking on postmodernism and culture, calls postmodernism "the cultural dominant of the logic of late capitalism" , meaning that, through globalization, postmodern culture is tied inextricably with capitalism (Mark Fisher, writing 20 years later, goes further, essentially calling it the sole cultural possibility ). Drawing from Jameson and other theorists, David Beard and Kenneth Gloag argue that, in music, postmodernism is not just an attitude but also an inevitability in the current cultural climate of fragmentation . As early as 1938, Theodor Adorno had already identified a trend toward the dissolution of "a culturally dominant set of values" , citing the commodification of all genres as beginning of the end of genre or value distinctions in music .
In some respects, Postmodern music could be categorized as simply the music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism, but with Jameson in mind, it is clear these definitions are inadequate. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of modernism, but in fact postmodern music is more to do with functionality and the effect of globalization than it is with a specific reaction, movement, or attitude . In the face of capitalism, Jameson says, "It is safest to grasp the concept of the postmodern as an attempt to think the present historically in an age that has forgotten how to think historically in the first place" .
Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude". Kramer enumerates 16 (arguably subjective) "characteristics of postmodern music, by which I mean music that is understood in a postmodern manner, or that calls forth postmodern listening strategies, or that provides postmodern listening experiences, or that exhibits postmodern compositional practices." According to , postmodern music:
Daniel Albright summarizes the main tendencies of musical postmodernism as :
Timescale.
One author has suggested that the emergence of postmodern music in popular music occurred in the late 1960s, influenced in part by psychedelic rock and one or more of the later Beatles albums . Beard and Gloag support this position, citing Jameson's theory that "the radical changes of musical styles and languages throughout the 1960s [are] now seen as a reflection of postmodernism" (; see also ). Others have placed the beginnings of postmodernism in the arts, with particular reference to music, at around 1930 (; ).

</doc>
<doc id="23615" url="http://en.wikipedia.org/wiki?curid=23615" title="Protocol">
Protocol

Protocol may refer to:

</doc>
<doc id="23617" url="http://en.wikipedia.org/wiki?curid=23617" title="Pump">
Pump

A pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action. Pumps can be classified into three major groups according to the method they use to move the fluid: "direct lift", "displacement", and "gravity" pumps.
Pumps operate by some mechanism (typically reciprocating or rotary), and consume energy to perform mechanical work by moving the fluid. Pumps operate via many energy sources, including manual operation, electricity, engines, or wind power, come in many sizes, from microscopic for use in medical applications to large industrial pumps.
Mechanical pumps serve in a wide range of applications such as pumping water from wells, aquarium filtering, pond filtering and aeration, in the car industry for water-cooling and fuel injection, in the energy industry for pumping oil and natural gas or for operating cooling towers. In the medical industry, pumps are used for biochemical processes in developing and manufacturing medicine, and as artificial replacements for body parts, in particular the artificial heart and penile prosthesis.
In biology, many different types of chemical and bio-mechanical pumps have evolved, and biomimicry is sometimes used in developing new types of mechanical pumps.
Types.
Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.
Pumps can be classified by their method of displacement into positive displacement pumps, impulse pumps, velocity pumps, gravity pumps, steam pumps and valveless pumps.
Positive displacement pump.
A positive displacement pump makes a fluid move by trapping a fixed amount and forcing (displacing) that trapped volume into the discharge pipe.
Some positive displacement pumps use an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pump as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant through each cycle of operation.
Positive displacement pump behavior and safety.
Positive displacement pumps, unlike centrifugal or roto-dynamic pumps, theoretically can produce the same flow at a given speed (RPM) no matter what the discharge pressure. Thus, positive displacement pumps are "constant flow machines". However, a slight increase in internal leakage as the pressure increases prevents a truly constant flow rate.
A positive displacement pump must not operate against a closed valve on the discharge side of the pump, because it has no shutoff head like centrifugal pumps. A positive displacement pump operating against a closed discharge valve continues to produce flow and the pressure in the discharge line increases until the line bursts, the pump is severely damaged, or both.
A relief or safety valve on the discharge side of the positive displacement pump is therefore necessary. The relief valve can be internal or external. The pump manufacturer normally has the option to supply internal relief or safety valves. The internal valve is usually only used as a safety precaution. An external relief valve in the discharge line, with a return line back to the suction line or supply tank provides increased safety.
Positive displacement types.
A positive displacement pump can be further classified according to the mechanism used to move the fluid:
Rotary positive displacement pumps.
Positive displacement rotary pumps move fluid using a rotating mechanism that creates a vacuum that captures and draws in the liquid.
"Advantages:" Rotary pumps are very efficient because they naturally remove air from the lines, eliminating the need to bleed the air from the lines manually.
"Drawbacks:" The nature of the pump demands very close clearances between the rotating pump and the outer edge, making it rotate at a slow, steady speed. If rotary pumps are operated at high speeds, the fluids cause erosion, which eventually causes enlarged clearances that liquid can pass through, which reduces efficiency.
Rotary positive displacement pumps fall into three main types: 
Reciprocating positive displacement pumps.
Reciprocating pumps move the fluid using one or more oscillating pistons, plungers, or membranes (diaphragms), while valves restrict fluid motion to the desired direction.
Pumps in this category range from "simplex", with one cylinder, to in some cases "quad" (four) cylinders, or more. Many reciprocating-type pumps are "duplex" (two) or "triplex" (three) cylinder. They can be either "single-acting" with suction during one direction of piston motion and discharge on the other, or "double-acting" with suction and discharge in both directions. The pumps can be powered manually, by air or steam, or by a belt driven by an engine. This type of pump was used extensively in the 19th century—in the early days of steam propulsion—as boiler feed water pumps. Now reciprocating pumps typically pump highly viscous fluids like concrete and heavy oils, and serve in special applications that demand low flow rates against high resistance. Reciprocating hand pumps were widely used to pump water from wells. Common bicycle pumps and foot pumps for inflation use reciprocating action.
These positive displacement pumps have an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pumps as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant given each cycle of operation.
Typical reciprocating pumps are:
Various positive displacement pumps.
The positive displacement principle applies in these pumps:
Gear pump.
This is the simplest of rotary positive displacement pumps. It consists of two meshed gears that rotate in a closely fitted casing. The tooth spaces trap fluid and force it around the outer periphery. The fluid does not travel back on the meshed part, because the teeth mesh closely in the centre. Gear pumps see wide use in car engine oil pumps and in various hydraulic power packs.
Screw pump.
A Screw pump is a more complicated type of rotary pump that uses two or three screws with opposing thread—e.g., one screw turns clockwise and the other counterclockwise. The screws are mounted on parallel shafts that have gears that mesh so the shafts turn together and everything stays in place. The screws turn on the shafts and drive fluid through the pump. As with other forms of rotary pumps, the clearance between moving parts and the pump's casing is minimal.
Progressing cavity pump.
Widely used for pumping difficult materials, such as sewage sludge contaminated with large particles, this pump consists of a helical rotor, about ten times as long as its width. This can be visualized as a central core of diameter "x" with, typically, a curved spiral wound around of thickness half "x", though in reality it is manufactured in single casting. This shaft fits inside a heavy duty rubber sleeve, of wall thickness also typically "x". As the shaft rotates, the rotor gradually forces fluid up the rubber sleeve. Such pumps can develop very high pressure at low volumes.
Roots-type pumps.
Named after the Roots brothers who invented it, this lobe pump displaces the liquid trapped between two long helical rotors, each fitted into the other when perpendicular at 90°, rotating inside a triangular shaped sealing line configuration, both at the point of suction and at the point of discharge. This design produces a continuous flow with equal volume and no vortex. It can work at low pulsation rates, and offers gentle performance that some applications require.
Applications include:
Peristaltic pump.
A "peristaltic pump" is a type of positive displacement pump. It contains fluid within a flexible tube fitted inside a circular pump casing (though linear peristaltic pumps have been made). A number of "rollers", "shoes", or "wipers" attached to a rotor compresses the flexible tube. As the rotor turns, the part of the tube under compression closes (or "occludes"), forcing the fluid through the tube. Additionally, when the tube opens to its natural state after the passing of the cam it draws ("restitution") fluid into the pump. This process is called peristalsis and is used in many biological systems such as the gastrointestinal tract.
Plunger pumps.
"Plunger pumps" are reciprocating positive displacement pumps.
These consist of a cylinder with a reciprocating plunger. The suction and discharge valves are mounted in the head of the cylinder. In the suction stroke the plunger retracts and the suction valves open causing suction of fluid into the cylinder. In the forward stroke the plunger pushes the liquid out of the discharge valve.
Efficiency and common problems: With only one cylinder in plunger pumps, the fluid flow varies between maximum flow when the plunger moves through the middle positions, and zero flow when the plunger is at the end positions. A lot of energy is wasted when the fluid is accelerated in the piping system. Vibration and "water hammer" may be a serious problem. In general the problems are compensated for by using two or more cylinders not working in phase with each other.
Triplex-style plunger pumps.
Triplex plunger pumps use three plungers, which reduces the pulsation of single reciprocating plunger pumps. Adding a pulsation dampener on the pump outlet can further smooth the "pump ripple", or ripple graph of a pump transducer. The dynamic relationship of the high-pressure fluid and plunger generally requires high-quality plunger seals. Plunger pumps with a larger number of plungers have the benefit of increased flow, or smoother flow without a pulsation dampener. The increase in moving parts and crankshaft load is one drawback.
Car washes often use these triplex-style plunger pumps (perhaps without pulsation dampeners). In 1968, William Bruggeman significantly reduced the size of the triplex pump and increased the lifespan so that car washes could use equipment with smaller footprints. Durable high pressure seals, low pressure seals and oil seals, hardened crankshafts, hardened connecting rods, thick ceramic plungers and heavier duty ball and roller bearings improve reliability in triplex pumps. Triplex pumps now are in a myriad of markets across the world.
Triplex pumps with shorter lifetimes are commonplace to the home user. A person who uses a home pressure washer for 10 hours a year may be satisfied with a pump that lasts 100 hours between rebuilds. Industrial-grade or continuous duty triplex pumps on the other end of the quality spectrum may run for as much as 2,080 hours a year.
The oil and gas drilling industry uses massive semi trailer-transported triplex pumps called mud pumps to pump drilling mud, which cools the drill bit and carries the cuttings back to the surface.
Drillers use triplex or even quintuplex pumps to inject water and solvents deep into shale in the extraction process called "fracking".
Compressed-air-powered double-diaphragm pumps.
One modern application of positive displacement diaphragm pumps is compressed-air-powered double-diaphragm pumps. Run on compressed air these pumps are intrinsically safe by design, although all manufacturers offer ATEX certified models to comply with industry regulation. These pumps are relatively inexpensive and can perform a wide variety of duties, from pumping water out of bunds, to pumping hydrochloric acid from secure storage (dependent on how the pump is manufactured – elastomers / body construction). Lift is normally limited to roughly 6m although heads can reach almost 200 psi.
Rope pumps.
Devised in China as chain pumps over 1000 years ago, these pumps can be made from very simple materials: A rope, a wheel and a PVC pipe are sufficient to make a simple rope pump. For this reason they have become extremely popular around the world since the 1980s. Rope pump efficiency has been studied by grass roots organizations and the techniques for making and running them have been continuously improved.
Impulse pumps.
Impulse pumps use pressure created by gas (usually air). In some impulse pumps the gas trapped in the liquid (usually water), is released and accumulated somewhere in the pump, creating a pressure that can push part of the liquid upwards.
Impulse pumps include:
Hydraulic ram pumps.
A hydraulic ram is a water pump powered by hydropower.
It takes in water at relatively low pressure and high flow-rate and outputs water at a higher hydraulic-head and lower flow-rate. The device uses the water hammer effect to develop pressure that lifts a portion of the input water that powers the pump to a point higher than where the water started.
The hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower, and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.
Velocity pumps.
Rotodynamic pumps (or dynamic pumps) are a type of velocity pump in which kinetic energy is added to the fluid by increasing the flow velocity. This increase in energy is converted to a gain in potential energy (pressure) when the velocity is reduced prior to or as the flow exits the pump into the discharge pipe. This conversion of kinetic energy to pressure is explained by the "First law of thermodynamics", or more specifically by "Bernoulli's principle".
Dynamic pumps can be further subdivided according to the means in which the velocity gain is achieved.
These types of pumps have a number of characteristics:
A practical difference between dynamic and positive displacement pumps is how they operate under closed valve conditions. Positive displacement pumps physically displace fluid, so closing a valve downstream of a positive displacement pump produces a continual pressure build up that can cause mechanical failure of pipeline or pump. Dynamic pumps differ in that they can be safely operated under closed valve conditions (for short periods of time).
Radial-flow pumps.
 These simply referred to as centripetal design pumps. The fluid enters along the axis or center, is accelerated by the impeller and exits at right angles to the shaft(radially). Radial-flow pumps operate at higher pressures and lower flow rates than axial and mixed-flow pumps.
Axial-flow pumps.
These are referred to as centrifugal design pumps. The fluid is pushed outward from the center or axis. Axial-flow pumps / Centrifugal design pumps operate at much lower pressures and higher flow rates than radial-flow pumps / cepumps.
Mixed-flow pumps.
Mixed-flow pumps function as a compromise between radial and axial-flow pumps. The fluid experiences both radial acceleration and lift and exits the impeller somewhere between 0 and 90 degrees from the axial direction. As a consequence mixed-flow pumps operate at higher pressures than axial-flow pumps while delivering higher discharges than radial-flow pumps. The exit angle of the flow dictates the pressure head-discharge characteristic in relation to radial and mixed-flow.
Eductor-jet pump.
This uses a jet, often of steam, to create a low pressure. This low pressure sucks in fluid and propels it into a higher pressure region.
Gravity pumps.
Gravity pumps include the "syphon" and "Heron's fountain". The "hydraulic ram" is also sometimes called a gravity pump; in a gravity pump the water is lifted by gravitational force.
Steam pumps.
Steam pumps have been for a long time mainly of historical interest. They include any type of pump powered by a steam engine and also pistonless pumps such as Thomas Savery's or the Pulsometer steam pump.
Recently there has been a resurgence of interest in low power solar steam pumps for use in smallholder irrigation in developing countries. Previously small steam engines have not been viable because of escalating inefficiencies as vapour engines decrease in size. However the use of modern engineering materials coupled with alternative engine configurations has meant that these types of system are now a cost effective opportunity.
Valveless pumps.
Valveless pumping assists in fluid transport in various biomedical and engineering systems. In a valveless pumping system, no valves (or physical occlusions) are present to regulate the flow direction. The fluid pumping efficiency of a valveless system, however, is not necessarily lower than that having valves. In fact, many fluid-dynamical systems in nature and engineering more or less rely upon valveless pumping to transport the working fluids therein. For instance, blood circulation in the cardiovascular system is maintained to some extent even when the heart’s valves fail. Meanwhile, the embryonic vertebrate heart begins pumping blood long before the development of discernible chambers and valves. In microfluidics, valveless impedance pumps have been fabricated, and are expected to be particularly suitable for handling sensitive biofluids. Ink jet printers operating on the Piezoelectric transducer principle also use valveless pumping. The pump chamber is emptied through the printing jet due to reduced flow impedance in that direction and refilled by capillary action.
Pump repairs.
Examining pump repair records and mean time between failures (MTBF) is of great importance to responsible and conscientious pump users. In view of that fact, the preface to the 2006 Pump User’s Handbook alludes to "pump failure" statistics. For the sake of convenience, these failure statistics often are translated into MTBF (in this case, installed life before failure).
In early 2005, Gordon Buck, John Crane Inc.’s chief engineer for Field Operations in Baton Rouge, LA, examined the repair records for a number of refinery and chemical plants to obtain meaningful reliability data for centrifugal pumps. A total of 15 operating plants having nearly 15,000 pumps were included in the survey. The smallest of these plants had about 100 pumps; several plants had over 2000. All facilities were located in the United States. In addition, considered as "new", others as "renewed" and still others as "established". Many of these plants—but not all—had an alliance arrangement with John Crane. In some cases, the alliance contract included having a John Crane Inc. technician or engineer on-site to coordinate various aspects of the program.
Not all plants are refineries, however, and different results occur elsewhere. In chemical plants, pumps have traditionally been "throw-away" items as chemical attack limits life. Things have improved in recent years, but the somewhat restricted space available in "old" DIN and ASME-standardized stuffing boxes places limits on the type of seal that fits. Unless the pump user upgrades the seal chamber, the pump only accommodates more compact and simple versions. Without this upgrading, lifetimes in chemical installations are generally around 50 to 60 percent of the refinery values.
Unscheduled maintenance is often one of the most significant costs of ownership, and failures of mechanical seals and bearings are among the major causes. Keep in mind the potential value of selecting pumps that cost more initially, but last much longer between repairs. The MTBF of a better pump may be one to four years longer than that of its non-upgraded counterpart. Consider that published average values of avoided pump failures range from US$2600 to US$12,000. This does not include lost opportunity costs. One pump fire occurs per 1000 failures. Having fewer pump failures means having fewer destructive pump fires.
As has been noted, a typical pump failure based on actual year 2002 reports, costs US$5,000 on average. This includes costs for material, parts, labor and overhead. Extending a pump's MTBF from 12 to 18 months would save US$1,667 per year — which might be greater than the cost to upgrade the centrifugal pump's reliability.
Applications.
Pumps are used throughout society for a variety of purposes. Early applications includes the use of the windmill or watermill to pump water. Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.
Because of the wide variety of applications, pumps have a plethora of shapes and sizes: from very large to very small, from handling gas to handling liquid, from high pressure to low pressure, and from high volume to low volume.
Priming a pump.
Typically, a liquid pump can't simply draw air. The feed line of the pump must first be filled with the liquid that requires pumping. An operator must introduce liquid into the system to initiate the pumping. This is called "priming" the pump. Loss of prime is usually due to ingestion of air into the pump. The clearances and displacement ratios in pumps for liquids, whether thin or more viscous, usually cannot displace air due to its compressibility.
Pumps as public water supplies.
One sort of pump once common worldwide was a hand-powered water pump, or 'pitcher pump'. It was commonly installed over community water wells in the days before piped water supplies.
In parts of the British Isles, it was often called "the parish pump". Though such community pumps are no longer common, people still used the expression "parish pump" to describe a place or forum where matters of local interest are discussed.
Because water from pitcher pumps is drawn directly from the soil, it is more prone to contamination. If such water is not filtered and purified, consumption of it might lead to gastrointestinal or other water-borne diseases. A notorious case is the 1854 Broad Street cholera outbreak. At the time it was not known how cholera was transmitted, but physician John Snow suspected contaminated water and had the handle of the public pump he suspected removed; the outbreak then subsided.
Modern hand-operated community pumps are considered the most sustainable low-cost option for safe water supply in resource-poor settings, often in rural areas in developing countries. A hand pump opens access to deeper groundwater that is often not polluted and also improves the safety of a well by protecting the water source from contaminated buckets. Pumps such as the Afridev pump are designed to be cheap to build and install, and easy to maintain with simple parts. However, scarcity of spare parts for these type of pumps in some regions of Africa has diminished their utility for these areas.
Sealing multiphase pumping applications.
Multiphase pumping applications, also referred to as tri-phase, have grown due to increased oil drilling activity. In addition, the economics of multiphase production is attractive to upstream operations as it leads to simpler, smaller in-field installations, reduced equipment costs and improved production rates. In essence, the multiphase pump can accommodate all fluid stream properties with one piece of equipment, which has a smaller footprint. Often, two smaller multiphase pumps are installed in series rather than having just one massive pump.
For midstream and upstream operations, multiphase pumps can be located onshore or offshore and can be connected to single or multiple wellheads. Basically, multiphase pumps are used to transport the untreated flow stream produced from oil wells to downstream processes or gathering facilities. This means that the pump may handle a flow stream (well stream) from 100 percent gas to 100 percent liquid and every imaginable combination in between. The flow stream can also contain abrasives such as sand and dirt. Multiphase pumps are designed to operate under changing/fluctuating process conditions. Multiphase pumping also helps eliminate emissions of greenhouse gases as operators strive to minimize the flaring of gas and the venting of tanks where possible.
Types and features of multiphase pumps.
"Helico-Axial Pumps (Centrifugal)" 
A rotodynamic pump with one single shaft that requires two mechanical seals, this pump uses an open-type axial impeller. It's often called a "Poseidon pump", and can be described as a cross between an axial compressor and a centrifugal pump.
"Twin Screw (Positive Displacement)"
The twin screw pump is constructed of two inter-meshing screws that move the pumped fluid. Twin screw pumps are often used when pumping conditions contain high gas volume fractions and fluctuating inlet conditions. Four mechanical seals are required to seal the two shafts.
"Progressive Cavity Pumps (Positive Displacement)"
Progressive cavity pumps are single-screw types typically used in shallow wells or at the surface. This pump is mainly used on surface applications where the pumped fluid may contain a considerable amount of solids such as sand and dirt.
"Electric Submersible Pumps (Centrifugal)" 
These pumps are basically multistage centrifugal pumps and are widely used in oil well applications as a method for artificial lift. These pumps are usually specified when the pumped fluid is mainly liquid.
"Buffer Tank"
A buffer tank is often installed upstream of the pump suction nozzle in case of a slug flow. The buffer tank breaks the energy of the liquid slug, smoothes any fluctuations in the incoming flow and acts as a sand trap.
As the name indicates, multiphase pumps and their mechanical seals can encounter a large variation in service conditions such as changing process fluid composition, temperature variations, high and low operating pressures and exposure to abrasive/erosive media. The challenge is selecting the appropriate mechanical seal arrangement and support system to ensure maximized seal life and its overall effectiveness.
Specifications.
Pumps are commonly rated by horsepower, flow rate, outlet pressure in metres (or feet) of head, inlet suction in suction feet (or metres) of head.
The head can be simplified as the number of feet or metres the pump can raise or lower a column of water at atmospheric pressure.
From an initial design point of view, engineers often use a quantity termed the specific speed to identify the most suitable pump type for a particular combination of flow rate and head.
Pumping power.
The power imparted into a fluid increases the energy of the fluid per unit volume. Thus the power relationship is between the conversion of the mechanical energy of the pump mechanism and the fluid elements within the pump. In general, this is governed by a series of simultaneous differential equations, known as the Navier–Stokes equations. However a more simple equation relating only the different energies in the fluid, known as Bernoulli's equation can be used. Hence the power, P, required by the pump:
where Δp is the change in total pressure between the inlet and outlet (in Pa), and Q, the volume flow-rate of the fluid is given in m3/s. The total pressure may have gravitational, static pressure and kinetic energy components; i.e. energy is distributed between change in the fluid's gravitational potential energy (going up or down hill), change in velocity, or change in static pressure. η is the pump efficiency, and may be given by the manufacturer's information, such as in the form of a pump curve, and is typically derived from either fluid dynamics simulation (i.e. solutions to the Navier–Stokes for the particular pump geometry), or by testing. The efficiency of the pump depends upon the pump's configuration and operating conditions (such as rotational speed, fluid density and viscosity etc.)
For a typical "pumping" configuration, the work is imparted on the fluid, and is thus positive. For the fluid imparting the work on the pump (i.e. a turbine), the work is negative. Power required to drive the pump is determined by dividing the output power by the pump efficiency. Furthermore, this definition encompasses pumps with no moving parts, such as a siphon.
Pump efficiency.
Pump efficiency is defined as the ratio of the power imparted on the fluid by the pump in relation to the power supplied to drive the pump. Its value is not fixed for a given pump, efficiency is a function of the discharge and therefore also operating head. For centrifugal pumps, the efficiency tends to increase with flow rate up to a point midway through the operating range (peak efficiency) and then declines as flow rates rise further. Pump performance data such as this is usually supplied by the manufacturer before pump selection. Pump efficiencies tend to decline over time due to wear(e.g. increasing clearances as impellers reduce in size).
When a system design includes a centrifugal pump, an important issue it its design is matching the "head loss-flow characteristic" with the pump so that it operates at or close to the point of its maximum efficiency.
Pump efficiency is an important aspect and pumps should be regularly tested. Thermodynamic pump testing is one method.

</doc>
<doc id="23618" url="http://en.wikipedia.org/wiki?curid=23618" title="Progressive">
Progressive

Progressive is an adjectival form of progress and may refer to:

</doc>
<doc id="23619" url="http://en.wikipedia.org/wiki?curid=23619" title="Pressure">
Pressure

Pressure (symbol: "p" or "P") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled "gage" pressure) is the pressure relative to the local atmospheric or ambient pressure.
Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre; similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure and the torr is defined as 1⁄760 of this. Manometric units such as the centimetre of water, millimetre of mercury and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.
Definition.
Pressure is the amount of force acting per unit area. The symbol of pressure is "p" or "P".
Formula.
Mathematically:
where:
Pressure is a scalar quantity. It relates the vector surface element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates the two normal vectors:
The minus sign comes from the fact that the force is considered towards the surface element, while the normal vector points outward.
It is incorrect (although rather usual) to say "the pressure is directed in such or such direction". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.
Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume.
Units.
The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m2 or kg·m−1·s−2). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.
Other units of pressure, such as pounds per square inch and bar, are also in common use. The CGS unit of pressure is the barye (ba), equal to 1 dyn·cm−2 or 0.1 Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (g/cm2 or kg/cm2) and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is expressly forbidden in SI. The technical atmosphere (symbol: at) is 1 kgf/cm2 (98.0665 kPa or 14.223 psi).
Since a system under pressure has potential to perform work on its surroundings, pressure is a measure of potential energy stored per unit volume. It is therefore related to energy density and may be measured in units such as joules per cubic metre.
Some meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, where the hecto- prefix is rarely used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.
The standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at earth mean sea level and is defined as .
Because pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height "h" and density "ρ" is given by the hydrostatic pressure equation "p" = "ρgh". Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. When millimetres of mercury or inches of mercury are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These "manometric units" are still encountered in many fields. Blood pressure is measured in millimetres of mercury in most of the world, and lung pressures in centimetres of water are still common.
Underwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the standard units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1 bar, and is not the same as a linear metre of depth, and 33.066 fsw = 1 atm. Note that the pressure conversion from msw to fsw is different from the length conversion: 10 msw = 32.6336 fsw, while 10 m = 32.8083 ft
Gauge pressure is often given in units with 'g' appended, e.g. 'kPag', 'barg' or 'psig', and units for measurements of absolute pressure are sometimes given a suffix of 'a', to avoid confusion, for example 'kPaa', 'psia'. However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure For example, ""p"g = 100 psi" rather than ""p" = 100 psig".
Differential pressure is expressed in units with 'd' appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.
Presently or formerly popular pressure units include the following:
Examples.
As an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.
Another example is of a common knife. If we try to cut a fruit with the flat side it obviously will not cut. But if we take the thin side, it will cut smoothly. The reason is that the flat side has a greater surface area (less pressure) and so it does not cut the fruit. When we take the thin side, the surface area is reduced and so it cuts the fruit easily and quickly. This is one example of a practical application of pressure.
For gases, pressure is sometimes measured not as an "absolute pressure", but relative to atmospheric pressure; such measurements are called "gauge pressure". An example of this is the air pressure in an automobile tire, which might be said to be "220 kPa (32 psi)", but is actually 220 kPa (32 psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100 kPa (14.7 psi), the absolute pressure in the tire is therefore about 320 kPa (46.7 psi). In technical work, this is written "a gauge pressure of 220 kPa (32 psi)". Where space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as "kPa (gauge)" or "kPa (absolute)", is permitted. In non-SI technical work, a gauge pressure of 32 psi is sometimes written as "32 psig" and an absolute pressure as "32 psia", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.
Gauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is 100 kPa, a gas (such as helium) at 200 kPa (gauge) (300 kPa [absolute]) is 50% denser than the same gas at 100 kPa (gauge) (200 kPa [absolute]). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.
Scalar nature.
In a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because we are dealing with an extremely large number of molecules and because the motion of the individual molecules is random in every direction, we do not detect any motion. If we enclose the gas within a container, we detect a pressure in the gas from the molecules colliding with the walls of our container. We can put the walls of our container anywhere inside the gas, and the force per unit area (the pressure) is the same. We can shrink the size of our "container" down to a very small point (becoming less true as we approach the atomic scale), and the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.
A closely related quantity is the stress tensor "σ", which relates the vector force formula_6 to the vector area formula_7 via
This tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term "pressure" will refer only to the scalar pressure.
According to the theory of general relativity, pressure increases the strength of a gravitational field (see stress–energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.
Types.
Fluid pressure.
Fluid pressure is the pressure at some point within a fluid, such as water or air (for more information specifically about liquid pressure, see section below).
Fluid pressure occurs in one of two situations:
Pressure in open conditions usually can be approximated as the pressure in "static" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the hydrostatic pressure. 
Closed bodies of fluid are either "static", when the fluid is not moving, or "dynamic", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.
The concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid, zero viscosity. The equation for all points of a system filled with a constant-density fluid is
where:
Explosion or deflagration pressures.
Explosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.
Negative pressures.
While pressures are, in general, positive, there are several situations in which negative pressures may be encountered:
Stagnation pressure.
Stagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:
where 
The pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.
Surface pressure and surface tension.
There is a two-dimensional analog of pressure – the lateral force per unit length applied on a line perpendicular to the force.
Surface pressure is denoted by π and shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, , at constant temperature.
Surface tension is another example of surface pressure, but with a reversed sign, because "tension" is the opposite to "pressure".
Pressure of an ideal gas.
In an ideal gas, molecules have no volume and do not interact. Pressure varies linearly with temperature, volume, and quantity according to the ideal gas law,
where:
Real gases exhibit a more complex dependence on the variables of state.
Vapor pressure.
Vapor pressure is the pressure of a vapor in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.
The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapor bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.
The vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.
Liquid pressure.
When a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.
Liquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. The pressure due to a liquid in liquid columns of constant density or at a depth within a substance is represented by the following formula:
where:
Another way of saying this same formula is the following:
The pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible – that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.
Atmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the "total" pressure acting on a liquid. The total pressure of a liquid, then, is "ρgh" plus the pressure of the atmosphere. When this distinction is important, the term "total pressure" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.
It is important to recognize that the pressure does not depend on the "amount" of liquid present. Volume is not the important factor – depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of 3 m exerts only half the average pressure that a small 6 m deep pond does. A person will feel the same pressure whether his/her head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake. If four vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference what vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the narrower vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.
Restating this as energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface, gravitational potential energy is large but liquid pressure energy is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure energy. The sum of pressure energy and gravitational potential energy per unit volume is constant throughout the volume of the fluid and the two energy components change linearly with the depth. Mathematically, it is described by Bernoulli's equation where velocity head is zero and comparisons per unit volume in the vessel are:
Terms have the same meaning as in section Fluid pressure.
Direction of liquid pressure.
An experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts his/her head, the person will feel the same amount of water pressure on his/her ears. Because a liquid can flow, this pressure isn't only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a boat is pushed upward by water pressure (buoyancy).
When a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure doesn't have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why water spurting from a hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth – that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_21, where "h" is the depth below the free surface. Interestingly, this is the same speed the water (or anything else) would have if freely falling the same vertical distance "h".
Kinematic pressure.
is the kinematic pressure, where formula_2 is the pressure and formula_24 constant mass density. The SI unit of "P" is m2/s2. Kinematic pressure is used in the same manner as kinematic viscosity formula_25 in order to compute Navier–Stokes equation without explicitly showing the density formula_24.
References.
</ref>" style="-moz-column-count: 1

</doc>
<doc id="23621" url="http://en.wikipedia.org/wiki?curid=23621" title="Polygon">
Polygon

In geometry, a polygon is traditionally a plane figure that is bounded by a finite chain of straight line segments closing in a loop to form a closed chain or "circuit". These segments are called its "edges" or "sides", and the points where two edges meet are the polygon's "vertices" (singular: vertex) or "corners". The interior of the polygon is sometimes called its "body". An "n"-gon is a polygon with "n" sides. A polygon is a 2-dimensional example of the more general polytope in any number of dimensions.
The word "polygon" derives from the Greek πολύς ("polús") "much", "many" and γωνία ("gōnía") "corner", "angle", or γόνυ ("gónu") "knee".
The basic geometrical notion has been adapted in various ways to suit particular purposes. Mathematicians are often concerned only with the bounding closed polygonal chain and with simple polygons which do not self-intersect, and they often define a polygon accordingly. A polygonal boundary may be allowed to intersect itself, creating star polygons. Geometrically two edges meeting at a corner are required to form an angle that is not straight (180°); otherwise, the line segments may be considered parts of a single edge; however mathematically, such corners may sometimes be allowed. These and other generalizations of polygons are described below.
Classification.
Number of sides.
Polygons are primarily classified by the number of sides. See table below.
Convexity and non-convexity.
Polygons may be characterized by their convexity or type of non-convexity:
Properties.
Euclidean geometry is assumed throughout.
Angles.
Any polygon has as many corners as it has sides. Each corner has several angles. The two most important ones are:
Area and centroid.
Simple polygons.
For a non-self-intersecting (simple) polygon with "n" vertices "xi, yi" ( "i" = 1 to "n"), the area and centroid are given by:
To close the polygon, the first and last vertices are the same, i.e., "xn", "yn" = "x"0, "y"0. The vertices must be ordered according to positive or negative orientation (counterclockwise or clockwise, respectively); if they are ordered negatively, the value given by the area formula will be negative but correct in absolute value, but when calculating formula_9 and formula_10, the signed value of formula_11 (which in this case is negative) should be used. This is commonly called the Shoelace formula or Surveyor's formula.
The area "A" of a simple polygon can also be computed if the lengths of the sides, "a"1, "a"2, ..., "an" and the exterior angles, "θ"1, "θ"2, ..., "θn" are known, from:
The formula was described by Lopshits in 1963.
If the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points.
In every polygon with perimeter "p" and area "A ", the isoperimetric inequality formula_13 holds.
If any two simple polygons of equal area are given, then the first can be cut into polygonal pieces which can be reassembled to form the second polygon. This is the Bolyai-Gerwien theorem.
The area of a regular polygon is also given in terms of the radius "r" of its inscribed circle and its perimeter "p" by
This radius is also termed its apothem and is often represented as "a".
The area of a regular "n"-gon with side "s" inscribed in a unit circle is
The area of a regular "n"-gon in terms of the radius "r" of its circumscribed circle and its perimeter "p" is given by
The area of a regular "n"-gon, inscribed in a unit-radius circle, with side "s" and interior angle "θ" can also be expressed trigonometrically as
The lengths of the sides of a polygon do not in general determine the area. However, if the polygon is cyclic the sides "do" determine the area. Of all "n"-gons with given sides, the one with the largest area is cyclic. Of all "n"-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).
Self-intersecting polygons.
The area of a self-intersecting polygon can be defined in two different ways, each of which gives a different answer:
Generalizations of polygons.
The idea of a polygon has been generalized in various ways. Some of the more important include:
Naming polygons.
The word "polygon" comes from Late Latin "polygōnum" (a noun), from Greek πολύγωνον ("polygōnon/polugōnon"), noun use of neuter of πολύγωνος ("polygōnos/polugōnos", the masculine adjective), meaning "many-angled". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix "-gon", e.g. "pentagon", "dodecagon". The triangle, quadrilateral and nonagon are exceptions.
Beyond decagons (10-sided) and dodecagons (12-sided), mathematicians generally use numerical notation, for example 17-gon and 257-gon.
Exceptions exist for side counts that are more easily expressed in verbal form, or are used by non-mathematicians. Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.
Constructing higher names.
To construct the name of a polygon with more than 20 and less than 100 edges, combine the prefixes as follows. The "kai" term applies to 13-gons and higher was used by Kepler, and advocated by John H. Conway for clarity to concatenated prefix numbers in the naming of quasiregular polyhedra.
History.
Polygons have been known since ancient times. The regular polygons were known to the ancient Greeks, with the pentagram, a non-convex regular polygon (star polygon), appearing as early as the 7th century B.C. on a krater by Aristonothos, found at Caere and now in the Capitoline Museum.
The first known systematic study of non-convex polygons in general was made by Thomas Bradwardine in the 14th century.
In 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.
Polygons in nature.
Polygons appear in rock formations, most commonly as the flat facets of crystals, where the angles between the sides depend on the type of mineral from which the crystal is made.
Regular hexagons can occur when the cooling of lava forms areas of tightly packed columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.
In biology, the surface of the wax honeycomb made by bees is an array of hexagons, and the sides and base of each cell are also polygons.
Polygons in computer graphics.
A polygon in a computer graphics (image generation) system is a two-dimensional shape that is modelled and stored within its database. A polygon can be colored, shaded and textured, and its position in the database is defined by the coordinates of its vertices (corners).
Naming conventions differ from those of mathematicians:
Any surface is modelled as a tessellation called meshed polygons. If a square mesh has "n" + 1 points (vertices) per side, there are "n" squared squares in the mesh, or 2"n" squared triangles since there are two triangles in a square. There are ("n" + 1)2 / 2("n"2) vertices per triangle. Where "n" is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).
The imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.
In computer graphics and computational geometry, it is often necessary to determine whether a given point "P" = ("x"0,"y"0) lies inside a simple polygon given by a sequence of line segments. This is called the Point in polygon test.

</doc>
<doc id="23622" url="http://en.wikipedia.org/wiki?curid=23622" title="Player character">
Player character

A player character or playable character (PC) is a character in a role-playing or video game whose actions are directly controlled by a player of the game and is typically a protagonist of the game's story. The characters that are not controlled by a player are called non-player characters (NPCs). The actions of non-player characters are typically handled by the game itself in video games, or by a gamemaster for tabletop role-playing games. The player character functions as a persona of the player controlling it. Player characters are often also metonymically called players.
Video games typically have one player character for each person playing the game. Some games offer a group of player characters for the player to choose from, allowing the player to control one of them at a time. Where more than one player character is available, the characters may have different abilities, strengths, and weaknesses to make the game play style different.
Overview.
Avatars.
A player character may sometimes be based on a real person, especially in sports games that use the names and likenesses of real sports people. Historical people and leaders may sometimes appear as characters too, particularly in strategy or empire building games such as in Sid Meier's "Civilization" series. Curiously, in the case of Civilization, a player's chosen historical character is the same throughout the course of the game despite the fact that a campaign can last several hundred years before and after the lifetime of the real historical persona. Such a player character is more properly an avatar as the player character's name and image typically have little bearing on the game itself. Avatars are also commonly seen in casino game simulations.
Role-playing games.
In role playing games such as "Dungeons and Dragons" or "Final Fantasy," a player typically creates or takes on the identity of a character that may have nothing in common with the player. The character is usually of a certain (often fictional) race and class (such as zombie, berserker, rifleman, elf, or cleric), each with strengths and weaknesses. The attributes of the characters (such as magic and fighting ability) are given as numerical values which can be increased as the gamer progresses and gains rank and experience points through accomplishing goals or fighting enemies.
Blank characters.
In many video games, and especially first-person shooters, the player character is a "blank slate" without any notable characteristics or even backstory. "Crono", "Link" and "Chell" are examples of such characters. These characters are generally silent protagonists.
Some games will go even further, never showing or naming the player-character at all. This is somewhat common in first-person videogames, such as in "Myst", but is more often done in strategy video games such as "Dune 2000" and "". In such games, the only real indication that the player has a character (instead of an omnipresent status), is from the cutscenes during which the character is being given a mission briefing or debriefing; the player is usually addressed as "general", "commander", or another military rank. Typically, this is done so that the player may imagine themself in the adventure without being required to play a character who is of a different age, race, gender, or background.
In gaming culture, such a character was called Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person, abbreviated as AFGNCAAP; a term that originated in "" where it is used satirically to refer to the player. 
Fighting games.
Fighting games typically have a larger amount of player characters to choose from, with some basic moves available to all or most characters and some unique moves only available to one or a few characters. Having many different characters to play as and against, all possessing different moves and abilities, is necessary to create a larger gameplay variety in such games.
Secret characters.
A secret or unlockable character may be a playable character in a video game available after completing the game or meeting other requirements. In some video games, characters that are not secret but appear only as non-player characters like bosses or enemies become playable characters after completing certain requirements, or sometimes cheating.

</doc>
<doc id="23623" url="http://en.wikipedia.org/wiki?curid=23623" title="Parish">
Parish

A parish is a church territorial unit constituting a division within a diocese. A parish is under the pastoral care and clerical jurisdiction of a parish priest, who might be assisted by one or more curates, and who operates from a parish church. Historically, a parish often covered the same geographical area as a manor (its association with the parish church remaining paramount).
By extension the term "parish" refers not only to the territorial unit but to the people of its community or congregation as well as to church property within it. In England this church property was technically in ownership of the parish priest "ex-officio", vested in him on his institution to that parish.
Etymology and use.
First attested in English in the late 13th century, the word "parish" comes from the Old French "paroisse", in turn from Latin: "paroecia", the latinisation of the Ancient Greek: παροικία "paroikia", "sojourning in a foreign land", itself from πάροικος ("paroikos"), "dwelling beside, stranger, sojourner", which is a compound of παρά ("pará"), "beside, by, near" and οἶκος ("oîkos"), "house".
As an ancient concept, the term "parish" occurs in the long-established Christian denominations: Roman Catholic, Anglican Communion, the Eastern Orthodox Church, and Lutheran churches, and in some Methodist and Presbyterian administrations.
The eighth Archbishop of Canterbury Theodore of Tarsus (c. 602–690) applied to the Anglo-Saxon township unit, where it existed, the ecclesiastical term "parish".
Church territorial structure.
Broadly speaking, the parish is the standard unit in episcopal polity of church administration, although parts of a parish may be subdivided as a "chapelry", with a chapel of ease or filial church serving as the local place of worship in cases of difficulty to access the main parish church.
In the wider picture of ecclesiastical polity, a "parish" comprises a division of a diocese or see. Parishes within a diocese may be grouped into a deanery or "vicariate forane" (or simply "vicariate"), overseen by a dean or "vicar forane", or in some cases by an archpriest. Some churches of the Anglican Communion have deaneries as units of an archdeaconry.
Roman Catholic Church.
In the Roman Catholic Church, each parish normally has its own parish priest (in some countries called pastor), who has responsibility and canonical authority over the parish.
What in most English-speaking countries is termed the "parish priest" is referred to as the "pastor" in the United States, where the term "parish priest" is used of any priest assigned to a parish even in a subordinate capacity. These are called "assistant priests", "parochial vicars", "curates", or, in the United States, "associate pastors" and "assistant pastors".
Each diocese (administrative region) is divided into parishes, each with their own central church called the parish church, where religious services take place. Some larger parishes or parishes that have been combined under one pastor may have two or more such churches, or the parish may be responsible for chapels (or chapels of ease) located at some distance from the mother church for the convenience of distant parishioners.
Normally, a parish comprises all Catholics living in its area, but parishes can also be established within a defined geographical region on a personal basis for Catholics of a particular rite, language, nationality or the like. An example is that of personal parishes established in accordance with the 7 July 2007 motu proprio "Summorum Pontificum" for those attached to the older form of the Roman Rite.
Most Catholic parishes are part of Latin Rite dioceses, which together cover the whole territory of a country. There can also be overlapping parishes of eparchies of Eastern Catholic Churches, personal ordinariates or military ordinariates.
Church of England.
The Church of England geographical structure uses the local parish church as its basic unit. The parish system survived the Reformation with the Anglican Church's secession from Rome remaining largely untouched, thus it shares its roots with the Church of Rome's system described above. Parishes may extend into different counties or hundreds and historically many parishes comprised extra outlying portions in addition to its principal district, usually being described as 'detached' and intermixed with the lands of other parishes. Church of England parishes nowadays all lie within one of 44 dioceses divided between the provinces of Canterbury, 30 and York, 14.
Each parish normally has its own parish priest (either a vicar or rector, owing to the vagaries of the feudal tithe system: rectories usually having had greater income) and perhaps supported by one or more curates or deacons - although as a result of ecclesiastical pluralism some parish priests might have held more than one parish living, placing a curate in charge of those where they do not reside. Now, however, it is common for a number of neighbouring parishes to be placed under one benefice in the charge of a priest who conducts services by rotation, with additional services being provided by lay readers or other non-ordained members of the church community.
In England civil parishes and their governing parish councils evolved in the 19th century as ecclesiastical parishes began to be relieved of what became considered to be civic responsibilities. Thus their boundaries began to diverge. The word "parish" acquired a secular usage. Since 1895, a parish council elected by public vote or a (civil) parish meeting administers a civil parish and is formally recognised as the level of local government below a district council.
The traditional structure of the Church of England with the parish as the basic unit has been exported to other countries and churches throughout the Anglican Communion and Commonwealth but does not necessarily continue to be administered in the same way.
Church of Scotland.
The parish is also the basic level of church administration in the Church of Scotland. Spiritual oversight of each parish church in Scotland is responsibility of the congregation's Kirk Session. Patronage was regulated in 1711 (Patronage Act) and abolished in 1874, with the result that ministers must be elected by members of the congregation. Many parish churches in Scotland today are "linked" with neighbouring parish churches served by a single minister. Since the abolition of parishes as a unit of civil government in Scotland in 1929, Scottish parishes have purely ecclesiastical significance and the boundaries may be adjusted by the local Presbytery.
Methodist Church.
Although they are more often simply called congregations and have no geographic boundaries, in the United Methodist Church congregations are called parishes. A prominent example of this usage comes in The Book of Discipline of The United Methodist Church, in which the committee of every local congregation that handles staff support is referred to as the committee on Pastor-Parish Relations. This committee gives recommendations to the bishop on behalf of the parish/congregation since it is the United Methodist Bishop of the Episcopal Area that appoints a pastor to each congregation. The same is true in the African Methodist Episcopal Church and the Christian Methodist Episcopal Church.
In New Zealand, a local grouping of Methodist churches that share one or more ministers (which in the United Kingdom would be called a circuit) is referred to as a parish.
References.
</dl>

</doc>
<doc id="23624" url="http://en.wikipedia.org/wiki?curid=23624" title="Procopius">
Procopius

Procopius of Caesarea (Greek: Προκόπιος ὁ Καισαρεύς, Latin: "Procopius Caesarensis"; c. AD 500 – c. AD 560) was a prominent late antique scholar from Palaestina Prima. Accompanying the Roman general Belisarius in the wars of the Emperor Justinian, he became the principal historian of the 6th century, writing the "Wars of Justinian", the "Buildings of Justinian" and the celebrated "Secret History". He is commonly held to be the last major historian of the ancient Western world.
Life.
Apart from his own writings, the main source for Procopius' life is an entry in the "Suda", a 10th-century Byzantine encyclopedia that tells nothing about his early life. He was a native of Caesarea in Palaestina Prima. He would have received a conventional élite education in the Greek classics and then rhetoric, perhaps at the famous School of Gaza, may have attended law school, possibly at Berytus (modern Beirut) or Constantinople, and became a "rhetor" (barrister or advocate). He evidently knew Latin, as was natural for a man with legal training. In 527, the first year of Eastern Roman Emperor Justinian I's reign, he became the "adsessor" (legal adviser) for Belisarius, Justinian's chief military commander who was then beginning a brilliant career.
Procopius was with Belisarius on the eastern front until the latter was defeated at the Battle of Callinicum in AD 531 and recalled to Constantinople. Procopius witnessed the Nika riots of January, 532, which Belisarius and his fellow general Mundo repressed with a massacre in the Hippodrome. In 533, he accompanied Belisarius on his victorious expedition against the Vandal kingdom in North Africa, took part in the capture of Carthage, and remained in Africa with Belisarius' successor Solomon the Eunuch when Belisarius returned to Constantinople. Procopius recorded a few of the extreme weather events of 535–536, although these were presented as a backdrop to Byzantine military activities, such as a mutiny, in and near Carthage. He rejoined Belisarius for his campaign against the Ostrogothic kingdom in Italy and experienced the Gothic siege of Rome that lasted a year and nine days, ending in mid-March 538. He witnessed Belisarius' entry into the Gothic capital, Ravenna, in 540. Book Eight of "The Wars of Justinian", and the "Secret History", suggest that his relationship with Belisarius seems to have cooled thereafter. When Belisarius was sent back to Italy in 544 to cope with a renewal of the war with the Goths, now led by the able king Totila, Procopius appears to have no longer been on Belisarius' staff.
It is not known when Procopius himself died, and many historians (James Howard-Johnson, Averil Cameron, Geoffrey Greatrex) date his death to 554, but in 562 there was an urban prefect of Constantinople who happened to be called Procopius. In that year, Belisarius was implicated in a conspiracy and was brought before this urban prefect.
Writings.
The writings of Procopius are the primary source of information for the rule of the Roman emperor Justinian. Procopius was the author of a history in eight books of the wars fought by Justinian I, a panegyric on Justinian's public works throughout the empire, and a book known as the "Secret History" that claims to report the scandals that Procopius could not include in his published history.
"The Wars of Justinian".
Procopius' "Wars of Justinian" (Greek: Ὑπὲρ τῶν πολέμων λόγοι "Hypèr tōn polémon lógoi", Latin: "De Bellis", "About the Wars") is clearly his most important work, although it is not as well known as the "Secret History". The first seven books, which may have been published as a unit, seem to have been largely completed by 545, but were updated to mid-century before publication, for the latest event mentioned belongs to early 551. The first two books (often known as the "Persian War", Latin "De Bello Persico") deal with the conflict between the Romans and Sassanid Persia in Mesopotamia, Syria, Armenia, Lazica and Caucasian Iberia (roughly modern-day Georgia). It details the campaigns of the Sasanian Shah Kavadh I, the 'Nika' revolt in Constantinople in 532, the war by Kavadh's successor, Khosrau I, in 540 and his destruction of Antioch and the transportation of its inhabitants to Mesopotamia, and the great plague that devastated Constantinople in 542. They also cover the early career of the Roman general Belisarius, Procopius' patron, in some detail. The next two books, the "Vandal War" (Latin "De Bello Vandalico"), cover Belisarius' successful campaign against the Vandal kingdom in Roman Africa. The remaining books cover the "Gothic War" (Latin "De Bello Gothico"), the campaigns by Belisarius and others to recapture Italy, then under the rule of the Ostrogoths. This includes accounts of the sieges of Naples and Rome.
Later, Procopius added an eighth book ("Wars VIII" or "Gothic War IV"), which brings the history to 552/553, when a Roman army led by the eunuch Narses finally destroyed the Ostrogothic kingdom. This eighth book covers campaigns both in Italy and on the Eastern frontier.
The "Wars of Justinian" was influential on later Byzantine history-writing. A continuation of Procopius' work was written after Procopius' death by the poet and historian Agathias of Myrina.
"Secret History".
The famous "Secret History" (Greek: Ἀπόκρυφη Ἱστορία "Apókryphe Istoría", Latin: "Historia Arcana") was discovered centuries later in the Vatican Library and published by Niccolò Alamanni in 1623 at Lyons. Its existence was already known from the "Suda", which referred to it as the "Anekdota" (Ancient Greek: Ἀνέκδοτα, Latin "Anecdota", "unpublished writings"). The "Secret History" covers roughly the same years as the first seven books of the "History of Justinian's Wars" and appears to have been written after they were published. Current consensus generally dates it to 550 or 558, or maybe even as late as 562.
The "Secret History" reveals an author who had become deeply disillusioned with the emperor Justinian and his wife, Empress Theodora, as well as Belisarius, his former commander and patron, and Antonina, Belisarius' wife. The anecdotes claim to expose the secret springs of their public actions, as well as the private lives of the emperor, his wife and their entourage. Justinian is portrayed as cruel, venal, prodigal and incompetent; as for Theodora, the reader is treated to the most detailed and titillating portrayals of vulgarity and insatiable lust combined with shrewish and calculating mean-spiritedness.
Among the more titillating (and doubtful) revelations in the "Secret History" is Procopius' account of Theodora's thespian accomplishments:
Her husband Justinian, meanwhile, was a monster whose head could suddenly vanish—at least according to this passage:
"The Buildings of Justinian".
Procopius "Buildings of Justinian" (Greek: Περὶ Κτισμάτων "Perì Ktismáton", Latin: "De Aedificiis", "On Buildings") is a panegyric on Justinian's building activity in the empire. The first book may date to before the collapse of the first dome of Hagia Sophia in 557, but some scholars think that it is possible that the work postdates the building of the bridge over the Sangarius in the late 550s. The "Peri ktismaton" (or "De Aedificiis") tells us nothing further about Belisarius, but it takes a sharply different attitude towards Justinian. He is presented as an idealised Christian emperor who built churches for the glory of God and defenses for the safety of his subjects and who showed particular concern for the water supply. He built new aqueducts as well as restoring those that had fallen into disuse.
Historians consider Buildings to be an incomplete work, due to evidence of the surviving version being a draft with two possible redactions.
Theodora, who was dead when this panegyric was written, is mentioned only briefly, but Procopius' praise of her beauty is fulsome. The panegyric was likely written at Justinian's behest, however, and it is doubtful that the sentiments expressed are sincere.
Due to the panegyrical nature of the "The Buildings", historians have discovered in several occasions discrepancies between claims made by Procopius and other primary sources. A primary example is in Procopius starting the reign of Justinian in 518, which was actually the start of the reign of Justin I, Justinian's predecessor and uncle. This discrepancy can be seen as part of Procopius' panegyric method, as it allowed him to credit buildings constructed under the rule Justin I as Justinian's accomplishments. In this context can be mentioned the renovations to the walls of Edessa after a flood in 525, along with several churches in the region, all of which were completed under Justinian's uncle. Similarly, Procopius falsely credits Justinian for the extensive re-fortifications made in the cities of Tomis and Histria in Scythia Minor, along the Danubian frontier, actual accomplishments of Anastasius I, predecessor of Justin I.
Context.
Procopius belongs to the school of late antique secular historians who continued the traditions of the Second Sophistic; they wrote in Attic Greek, their models were Herodotus and especially Thucydides, and their subject matter was secular history. They avoided vocabulary unknown to Attic Greek and inserted an explanation when they had to use contemporary words. Thus Procopius explains to his readers that "ekklesia", meaning a Christian church, is the equivalent of a temple or shrine and that monks are "the most temperate of Christians ... whom men are accustomed to call monks". ("Wars" 2.9.14; 1.7.22) In classical Athens, monks had been unknown and an "ekklesia" was the assembly of Athenian citizens that passed the laws.
The secular historians eschewed the history of the Christian church, which they left to ecclesiastical history—a genre that was founded by Eusebius of Caesarea. However, Averil Cameron has argued convincingly that Procopius' works reflect the tensions between the classical and Christian models of history in 6th century Byzantium. This is supported by Mary Whitby's analysis of Procopius' Book I depiction of Constantinople and the Church of Hagia Sophia in comparison to contemporary pagan panegyrics. Procopius can be seen as depicting Justinian as essentially God's Vicegerent, making the case for buildings being a primarily religious panegyric.
Procopius indicated ("Secret History" 26.18) that he planned to write an ecclesiastical history himself and, if he had, he would probably have followed the rules of that genre. But, as far as it is known, the ecclesiastical history remained unwritten.
A number of historical novels based on Procopius' works (along with other sources) have been written, one of which, "Count Belisarius", was written by poet and novelist Robert Graves in 1938. Procopius himself appears as a minor character in Felix Dahn's "A Struggle for Rome" and in L. Sprague de Camp's alternate history novel "Lest Darkness Fall". The novel's main character, archaeologist Martin Padway, derives most of his knowledge of historical events from the "Secret History".

</doc>
<doc id="23626" url="http://en.wikipedia.org/wiki?curid=23626" title="Property">
Property

In the abstract, property is that which belongs to or with something, whether as an attribute or as a component of said thing. In the context of this article, property is one or more components (rather than attributes), whether physical or incorporeal, of a person's estate; or so belonging to, as in being owned by, a person or jointly a group of people or a legal entity like a corporation or even a society. (Given such meaning, the word property is uncountable, and as such, is not described with an indefinite article or as plural.) Depending on the nature of the property, an owner of property has the right to consume, alter, share, redefine, rent, mortgage, pawn, sell, exchange, transfer, give away or destroy it, or to exclude others from doing these things, as well as perhaps to abandon it; whereas regardless of the nature of the property, the owner thereof has the right to properly use it (as a durable, mean or factor, or whatever), or at the very least exclusively keep it.
Property that jointly belongs to more than one party may be possessed or controlled thereby in very similar or very distinct ways, whether simply or complexly, whether equally or unequally. However, there is an expectation that each party's will (rather discretion) with regard to the property be clearly defined and unconditional, so as to distinguish ownership and easement from rent. The parties might expect their wills to be unanimous, or alternately every given one of them, when no opportunity for or possibility of dispute with any other of them exists, may expect his, her, its or their own will to be sufficient and absolute.
The Restatement (First) of Property defines Property as anything, tangible or intangible whereby a legal relationship between persons and the State enforces a possessory interest or legal title in that thing. This mediating relationship between individual, property and state is called as property regimes.
In sociology and Anthropology, property is often defined as a relationship between two or more individuals and an object, in which at least one of these individuals holds a bundle of rights over the object. The distinction between "collective property" and "private property" is regarded as a confusion since different individuals often hold differing rights over a single object.
Important widely recognized types of property include real property (the combination of land and any improvements to or on the land), personal property (physical possessions belonging to a person), private property (property owned by legal persons, business entities or individual natural persons), public property (state owned or publicly owned and available possessions) and intellectual property (exclusive rights over artistic creations, inventions, etc.), although the latter is not always as widely recognized or enforced. An article of property may have physical and incorporeal parts. A title, or a right of ownership, establishes the relation between the property and other persons, assuring the owner the right to dispose of the property as the owner sees fit.
Overview.
Often property is defined by the code of the local sovereignty, and protected wholly or more usually partially by such entity, the owner being responsible for any remainder of protection. The standards of proof concerning proofs of ownerships are also addressed by the code of the local sovereignty, and such entity plays a role accordingly, typically somewhat managerial. Some philosophers assert that property rights arise from social convention, while others find justifications for them in morality or in natural law.
Property, in the first instance, is a thing-in-itself. When a person finds a thing and takes that thing into that person's possession and control, then that thing becomes a thing-for-you for that person. Once the person has that thing in that person's possession, that thing becomes that person's property by reason of discovery and conquest, and that person has the individual right to defend that property (property interest) against all others by reason of self-help. Typically, persons join together to form a political state which may develop a formal legal system which enforces and protects property rights so that the individual can go to court to get protection and enforcement of that person's property rights, rather than having to use self-help. It is possible that when a person has constructive possession of personal property, but another person has actual possession, then the person having constructive possession has bare legal title, while the other person has actual possession. Generally, the ground and any buildings which are permanently attached are considered real property, while movable goods and intangibles such as a copyright are considered personal property. Also, property cannot be considered a reified concept, because in the first instance, property is very concrete as a physical thing-in-itself.
Various scholarly disciplines (such as law, economics, anthropology or sociology) may treat the concept more systematically, but definitions vary, most particularly when involving contracts. Positive law defines such rights, and the judiciary can adjudicate and enforce property rights.
According to Adam Smith, the expectation of profit from "improving one's stock of capital" rests on private property rights. Capitalism has as a central assumption that property rights encourage their holders to develop the property, generate wealth, and efficiently allocate resources based on the operation of markets. From this has evolved the modern conception of property as a right enforced by positive law, in the expectation that this will produce more wealth and better standards of living. However, Smith also expressed a very critical view on the effects of property laws on inequality:
In his text "The Common Law", Oliver Wendell Holmes describes property as having two fundamental aspects. The first, possession, can be defined as control over a resource based on the practical inability of another to contradict the ends of the possessor. The second, title, is the expectation that others will recognize rights to control resource, even when it is not in possession. He elaborates the differences between these two concepts, and proposes a history of how they came to be attached to persons, as opposed to families or to entities such as the church.
Both communism and some kinds of socialism have also upheld the notion that private ownership of capital is inherently illegitimate. This argument centers mainly on the idea that private ownership of capital always benefits one class over another, giving rise to domination through the use of this privately owned capital. Communists do not oppose personal property that is "hard-won, self-acquired, self-earned" (as the Communist Manifesto puts it) by members of the proletariat. Both socialism and communism distinguish carefully between private ownership of capital (land, factories, resources, etc.) and private property (homes, material objects, and so forth).
Types of property.
Most legal systems distinguish between different types of property, especially between land (immovable property, estate in land, real estate, real property) and all other forms of property—goods and chattels, movable property or personal property, including the value of legal tender if not the legal tender itself, as the manufacturer rather than the possessor might be the owner. They often distinguish tangible and intangible property. One categorization scheme specifies three species of property: land, improvements (immovable man-made things), and personal property (movable man-made things).
Dead Investment property, it is a property from which you cannot derive any profits. Scenario the property might be not in a suitable condition to use or cannot be used to earn profits. Investors also say that the properties where we stay are Dead Investments.
In common law, real property (immovable property) is the combination of interests in land and improvements thereto, and personal property is interest in movable property. Real property rights are rights relating to the land. These rights include ownership and usage. Owners can grant rights to persons and entities in the form of leases, licenses and easements.
Throughout the last centuries of the second millennium, with the development of more complex theories of property, the concept of personal property had become divided into tangible property (such as cars and clothing) and intangible property (such as financial instruments—including stocks and bonds—intellectual property—including patents, copyrights and trademarks—digital files, communication channels, and certain forms of identifier—including Internet domain names, some forms of network address, some forms of handle and again trademarks).
Treatment of intangible property is such that an article of property is, by law or otherwise by traditional conceptualization, subject to expiration even when inheritable, which is a key distinction from tangible property. Upon expiration, the property, if of the intellectual category, becomes a part of public domain, to be used by but not owned by anybody, and possibly used by more than one party simultaneously due the inapplicability of scarcity to intellectual property. Whereas things such as communications channels and pairs of electromagnetic spectrum band and signal transmission power can only be used by a single party at a time, or a single party in a divisible context, if owned or used at all. Thus far or usually those are not considered property, or at least not private property, even though the party bearing right of exclusive use may transfer that right to another.
Related concepts.
Of the following, only sale and at-will sharing involve no encumbrance.
Issues in property theory.
What can be property?
The two major justifications given for original property, or the homestead principle, are effort and scarcity. John Locke emphasized effort, "mixing your labor" with an object, or clearing and cultivating virgin land. Benjamin Tucker preferred to look at the telos of property, i.e. What is the purpose of property? His answer: to solve the scarcity problem. Only when items are relatively scarce with respect to people's desires do they become property. For example, hunter-gatherers did not consider land to be property, since there was no shortage of land. Agrarian societies later made arable land property, as it was scarce. For something to be economically scarce it must necessarily have the "exclusivity property"—that use by one person excludes others from using it. These two justifications lead to different conclusions on what can be property. Intellectual property—incorporeal things like ideas, plans, orderings and arrangements (musical compositions, novels, computer programs)—are generally considered valid property to those who support an effort justification, but invalid to those who support a scarcity justification, since the things don't have the exclusivity property (however, those who support a scarcity justification may still support other "intellectual property" laws such as Copyright, as long as these are a subject of contract instead of government arbitration). Thus even ardent propertarians may disagree about IP. By either standard, one's body is one's property.
From some anarchist points of view, the validity of property depends on whether the "property right" requires enforcement by the state. Different forms of "property" require different amounts of enforcement: intellectual property requires a great deal of state intervention to enforce, ownership of distant physical property requires quite a lot, ownership of carried objects requires very little, while ownership of one's own body requires absolutely no state intervention. Some anarchists don't believe in property at all.
Many things have existed that did not have an owner, sometimes called the commons. The term "commons," however, is also often used to mean something quite different: "general collective ownership"—i.e. common ownership. Also, the same term is sometimes used by statists to mean government-owned property that the general public is allowed to access (public property). Law in all societies has tended to develop towards reducing the number of things not having clear owners. Supporters of property rights argue that this enables better protection of scarce resources, due to the tragedy of the commons, while critics argue that it leads to the 'exploitation' of those resources for personal gain and that it hinders taking advantage of potential network effects. These arguments have differing validity for different types of "property"—things that are not scarce are, for instance, not subject to the tragedy of the commons. Some apparent critics advocate general collective ownership rather than ownerlessness.
Things that do not have owners include: ideas (except for intellectual property), seawater (which is, however, protected by anti-pollution laws), parts of the seafloor (see the United Nations Convention on the Law of the Sea for restrictions), gases in Earth's atmosphere, animals in the wild (although in most nations, animals are tied to the land. In the United States and Canada wildlife are generally defined in statute as property of the state. This public ownership of wildlife is referred to as the North American Model of Wildlife Conservation and is based on The Public Trust Doctrine.), celestial bodies and outer space, and land in Antarctica.
The nature of children under the age of majority is another contested issue here. In ancient societies children were generally considered the property of their parents. Children in most modern societies theoretically own their own bodies but are not considered competent to exercise their rights, and their parents or guardians are given most of the actual rights of control over them.
Questions regarding the nature of ownership of the body also come up in the issue of abortion, drugs and euthanasia.
In many ancient legal systems (e.g. early Roman law), religious sites (e.g. temples) were considered property of the God or gods they were devoted to. However, religious pluralism makes it more convenient to have religious sites owned by the religious body that runs them.
Intellectual property and air (airspace, no-fly zone, pollution laws, which can include tradable emissions rights) can be property in some senses of the word.
Ownership of land can be held separately from the ownership of rights over that land, including sporting rights, mineral rights, development rights, air rights, and such other rights as may be worth segregating from simple land ownership.
Who can be an owner?
Ownership laws may vary widely among countries depending on the nature of the property of interest (e.g. firearms, real property, personal property, animals). Persons can own property directly. In most societies legal entities, such as corporations, trusts and nations (or governments) own property.
In many countries women have limited access to property following restrictive inheritance and family laws, under which only men have actual or formal rights to own property.
In the Inca empire, the dead emperors, who were considered gods, still controlled property after death.
Whether and to what extent the state may interfere with property.
Under United States law the principal limitations on whether and the extent to which the State may interfere with property rights are set by the Constitution. The "Takings" clause requires that the government (whether state or federal—for the 14th Amendment's due process clause imposes the 5th Amendment's takings clause on state governments) may take private property only for a public purpose, after exercising due process of law, and upon making "just compensation." If an interest is not deemed a "property" right or the conduct is merely an intentional tort, these limitations do not apply and the doctrine of sovereign immunity precludes relief. Moreover, if the interference does not almost completely make the property valueless, the interference will not be deemed a taking but instead a mere regulation of use. On the other hand, some governmental regulations of property use have been deemed so severe that they have been considered "regulatory takings." Moreover, conduct sometimes deemed only a nuisance or other tort has been held a taking of property where the conduct was sufficiently persistent and severe.
Theories of property.
There exist many theories of property. One is the relatively rare first possession theory of property, where ownership of something is seen as justified simply by someone seizing something before someone else does. Perhaps one of the most popular is the natural rights definition of property rights as advanced by John Locke. Locke advanced the theory that God granted dominion over nature to man through Adam in the book of Genesis. Therefore, he theorized that when one mixes one’s labor with nature, one gains a relationship with that part of nature with which the labor is mixed, subject to the limitation that there should be "enough, and as good, left in common for others." (see Lockean proviso)
From the RERUM NOVARUM, Pope Leo XIII wrote "It is surely undeniable that, when a man engages in remunerative labor, the impelling reason and motive of his work is to obtain property, and thereafter to hold it as his very own."
Anthropology studies the diverse systems of ownership, rights of use and transfer, and possession under the term "theories of property." Western legal theory is based, as mentioned, on the owner of property being a legal person. However, not all property systems are founded on this basis.
In every culture studied ownership and possession are the subject of custom and regulation, and "law" where the term can meaningfully be applied. Many tribal cultures balance individual ownership with the laws of collective groups: tribes, families, associations and nations. For example the 1839 Cherokee Constitution frames the issue in these terms:
Sec. 2. The lands of the Cherokee Nation shall remain common property; but the improvements made thereon, and in the possession of the citizens respectively who made, or may rightfully be in possession of them: Provided, that the citizens of the Nation possessing exclusive and indefeasible right to their improvements, as expressed in this article, shall possess no right or power to dispose of their improvements, in any manner whatever, to the United States, individual States, or to individual citizens thereof; and that, whenever any citizen shall remove with his effects out of the limits of this Nation, and become a citizen of any other government, all his rights and privileges as a citizen of this Nation shall cease: Provided, nevertheless, That the National Council shall have power to re-admit, by law, to all the rights of citizenship, any such person or persons who may, at any time, desire to return to the Nation, on memorializing the National Council for such readmission.
Communal property systems describe ownership as belonging to the entire social and political unit. Such arrangements can under certain conditions erode open access resources. This development has been critiqued by the tragedy of the commons.
Corporate systems describe ownership as being attached to an identifiable group with an identifiable responsible individual. The Roman property law was based on such a corporate system.
Different societies may have different theories of property for differing types of ownership. Pauline Peters argued that property systems are not isolable from the social fabric, and notions of property may not be stated as such, but instead may be framed in negative terms: for example the taboo system among Polynesian peoples.
Property in philosophy.
In medieval and Renaissance Europe the term "property" essentially referred to land. After much rethinking, land has come to be regarded as only a special case of the property genus. This rethinking was inspired by at least three broad features of early modern Europe: the surge of commerce, the breakdown of efforts to prohibit interest (then called "usury"), and the development of centralized national monarchies.
Ancient philosophy.
Urukagina, the king of the Sumerian city-state Lagash, established the first laws that forbade compelling the sale of property.
The Ten Commandments shown in Exodus 20:2-17 and Deuteronomy 5:6-21 stated that the Israelites were not to steal, but the connection between Bronze Age concepts of theft and modern concepts of property is suspect.
Aristotle, in "Politics," advocates "private property." He argues that self-interest leads to neglect of the commons. "[T]hat which is common to the greatest number has the least care bestowed upon it. Every one thinks chiefly of his own, hardly at all of the common interest; and only when he is himself concerned as an individual."
In addition he says that when property is common, there are natural problems that arise due to differences in labor: "If they do not share equally enjoyments and toils, those who labor much and get little will necessarily complain of those who labor little and receive or consume much. But indeed there is always a difficulty in men living together and having all human relations in common, but especially in their having common property." ()
Cicero held that there is no private property under natural law but only under human law. Seneca viewed property as only becoming necessary when men become avarice. St. Ambrose latter adopted this view and St. Augustine even derided heretics for complaining the Emperor could not confiscate property they had labored for.
Medieval philosophy.
Thomas Aquinas (13th century).
The canon law "Decretum Gratiani" maintained that mere human law creates property, repeating the phrases used by St. Augustine. St. Thomas Aquinas agreed with regard to the private consumption of property but modified patristic theory in finding that the private possession of property is necessary. Thomas Aquinas concludes that, given certain detailed provisions,
Modern philosophy.
Thomas Hobbes (17th century).
The principal writings of Thomas Hobbes appeared between 1640 and 1651—during and immediately following the war between forces loyal to King Charles I and those loyal to Parliament. In his own words, Hobbes' reflection began with the idea of "giving to every man his own," a phrase he drew from the writings of Cicero. But he wondered: How can anybody call anything his own? He concluded: My own can only truly be mine if there is one unambiguously strongest power in the realm, and that power treats it as mine, protecting its status as such.
James Harrington (17th century).
A contemporary of Hobbes, James Harrington, reacted to the same tumult in a different way: he considered property natural but not inevitable. The author of "Oceana", he may have been the first political theorist to postulate that political power is a consequence, not the cause, of the distribution of property. He said that the worst possible situation is one in which the commoners have half a nation's property, with crown and nobility holding the other half—a circumstance fraught with instability and violence. A much better situation (a stable republic) will exist once the commoners own most property, he suggested.
In later years, the ranks of Harrington's admirers included American revolutionary and founder John Adams.
Robert Filmer (17th century).
Another member of the Hobbes/Harrington generation, Sir Robert Filmer, reached conclusions much like Hobbes', but through Biblical exegesis. Filmer said that the institution of kingship is analogous to that of fatherhood, that subjects are but children, whether obedient or unruly, and that property rights are akin to the household goods that a father may dole out among his children—his to take back and dispose of according to his pleasure.
John Locke (17th century).
In the following generation, John Locke sought to answer Filmer, creating a rationale for a balanced constitution in which the monarch had a part to play, but not an overwhelming part. Since Filmer's views essentially require that the Stuart family be uniquely descended from the patriarchs of the Bible, and since even in the late 17th century that was a difficult view to uphold, Locke attacked Filmer's views in his First Treatise on Government, freeing him to set out his own views in the Second Treatise on Civil Government. Therein, Locke imagined a pre-social world, each of the unhappy residents of which are willing to create a social contract because otherwise "the enjoyment of the property he has in this state is very unsafe, very unsecure," and therefore the "great and chief end, therefore, of men's uniting into commonwealths, and putting themselves under government, is the preservation of their property." They would, he allowed, create a monarchy, but its task would be to execute the will of an elected legislature. "To this end" (to achieve the previously specified goal), he wrote, "it is that men give up all their natural power to the society they enter into, and the community put the legislative power into such hands as they think fit, with this trust, that they shall be governed by declared laws, or else their peace, quiet, and property will still be at the same uncertainty as it was in the state of nature."
Even when it keeps to proper legislative form, though, Locke held that there are limits to what a government established by such a contract might rightly do.
"It cannot be supposed that [the hypothetical contractors] they should intend, had they a power so to do, to give any one or more an absolute arbitrary power over their persons and estates, and put a force into the magistrate's hand to execute his unlimited will arbitrarily upon them; this were to put themselves into a worse condition than the state of nature, wherein they had a liberty to defend their right against the injuries of others, and were upon equal terms of force to maintain it, whether invaded by a single man or many in combination. Whereas by supposing they have given up themselves to the absolute arbitrary power and will of a legislator, they have disarmed themselves, and armed him to make a prey of them when he pleases..."
Note that both "persons "and" estates" are to be protected from the arbitrary power of any magistrate, inclusive of the "power and will of a legislator." In Lockean terms, depredations against an estate are just as plausible a justification for resistance and revolution as are those against persons. In neither case are subjects required to allow themselves to become prey.
To explain the ownership of property Locke advanced a labor theory of property.
David Hume (18th century).
In contrast to the figures discussed in this section thus far David Hume lived a relatively quiet life that had settled down to a relatively stable social and political structure. He lived the life of a solitary writer until 1763 when, at 52 years of age, he went off to Paris to work at the British embassy.
In contrast, one might think, to his polemical works on religion and his empiricism-driven skeptical epistemology, Hume's views on law and property were quite conservative.
He did not believe in hypothetical contracts, or in the love of mankind in general, and sought to ground politics upon actual human beings as one knows them. "In general," he wrote, "it may be affirmed that there is no such passion in human mind, as the love of mankind, merely as such, independent of personal qualities, or services, or of relation to ourselves." Existing customs should not lightly be disregarded, because they have come to be what they are as a result of human nature. With this endorsement of custom comes an endorsement of existing governments, because he conceived of the two as complementary: "A regard for liberty, though a laudable passion, ought commonly to be subordinate to a reverence for established government."
Therefore, Hume's view was that there are property rights because of and to the extent that the existing law, supported by social customs, secure them. He offered some practical home-spun advice on the general subject, though, as when he referred to avarice as "the spur of industry," and expressed concern about excessive levels of taxation, which "destroy industry, by engendering despair."
Adam Smith.
"Civil government, so far as it is instituted for the security of property, is, in reality, instituted for the defense of the rich against the poor, or of those who have property against those who have none at all."— Adam Smith, "The Wealth of Nations", 1776
"The property which every man has in his own labour, as it is the original foundation of all other property, so it is the most sacred and inviolable. The patrimony of a poor man lies in the strength and dexterity of his hands; and to hinder him from employing this strength and dexterity in what manner he thinks proper without injury to his neighbour, is a plain violation of this most sacred property. It is a manifest encroachment upon the just liberty both of the workman, and of those who might be disposed to employ him. As it hinders the one from working at what he thinks proper, so it hinders the others from employing whom they think proper. To judge whether he is fit to be employed, may surely be trusted to the discretion of the employers whose interest it so much concerns. The affected anxiety of the law-giver lest they should employ an improper person, is evidently as impertinent as it is oppressive." 
- (Source: Adam Smith, "The Wealth of Nations", 1776, Book I, Chapter X, Part II.)
By the mid 19th century, the industrial revolution had transformed England and the United States, and had begun in France. The established conception of what constitutes property expanded beyond land to encompass scarce goods in general. In France, the revolution of the 1790s had led to large-scale confiscation of land formerly owned by church and king. The restoration of the monarchy led to claims by those dispossessed to have their former lands returned.
Karl Marx.
Section VIII, "Primitive Accumulation" of Capital involves a critique of Liberal Theories of property rights. Marx notes that under Feudal Law, peasants were legally as entitled to their land as the aristocracy was to its manors. Marx cites several historical events in which large numbers of the peasantry were removed from their lands, which were then seized by the aristocracy. This seized land was then used for commercial ventures (sheep heading). Marx sees this "Primitive Accumulation as integral to the creation of English Capitalism. This event created a large unlanded class which had to work for wages in order to survive. Marx asserts that Liberal theories of property are "idyllic" fairy tales that hide a violent historical process.
Charles Comte – legitimate origin of property.
Charles Comte, in "Traité de la propriété" (1834), attempted to justify the legitimacy of private property in response to the Bourbon Restoration. According to David Hart, Comte had three main points: "firstly, that interference by the state over the centuries in property ownership has had dire consequences for justice as well as for economic productivity; secondly, that property is legitimate when it emerges in such a way as not to harm anyone; and thirdly, that historically some, but by no means all, property which has evolved has done so legitimately, with the implication that the present distribution of property is a complex mixture of legitimately and illegitimately held titles."
Comte, as Proudhon later did, rejected Roman legal tradition with its toleration of slavery. He posited a communal "national" property consisting of non-scarce goods, such as land in ancient hunter-gatherer societies. Since agriculture was so much more efficient than hunting and gathering, private property appropriated by someone for farming left remaining hunter-gatherers with more land per person, and hence did not harm them. Thus this type of land appropriation did not violate the Lockean proviso – there was "still enough, and as good left." Comte's analysis would be used by later theorists in response to the socialist critique on property.
Pierre Proudhon – property is theft.
In his 1849 treatise "What is Property?", Pierre Proudhon answers with "Property is theft!" In natural resources, he sees two types of property, "de jure" property (legal title) and "de facto" property (physical possession), and argues that the former is illegitimate. Proudhon's conclusion is that "property, to be just and possible, must necessarily have equality for its condition."
His analysis of the product of labor upon natural resources as property (usufruct) is more nuanced. He asserts that land itself cannot be property, yet it should be held by individual possessors as stewards of mankind with the product of labor being the property of the producer. Proudhon reasoned that any wealth gained without labor was stolen from those who labored to create that wealth. Even a voluntary contract to surrender the product of labor to an employer was theft, according to Proudhon, since the controller of natural resources had no moral right to charge others for the use of that which he did not labor to create and therefore did not own.
Proudhon's theory of property greatly influenced the budding socialist movement, inspiring anarchist theorists such as Mikhail Bakunin who modified Proudhon's ideas, as well as antagonizing theorists like Karl Marx.
Frédéric Bastiat – property is value.
Frédéric Bastiat's main treatise on property can be found in chapter 8 of his book "Economic Harmonies" (1850). In a radical departure from traditional property theory, he defines property not as a physical object, but rather as a relationship between people with respect to an object. Thus, saying one owns a glass of water is merely verbal shorthand for "I may justly gift or trade this water to another person". In essence, what one owns is not the object but the value of the object. By "value," Bastiat apparently means "market value"; he emphasizes that this is quite different from utility. "In our relations with one another, we are not owners of the utility of things, but of their value, and value is the appraisal made of reciprocal services."
Bastiat theorized that, as a result of technological progress and the division of labor, the stock of communal wealth increases over time; that the hours of work an unskilled laborer expends to buy e.g. 100 liters of wheat decreases over time, thus amounting to "gratis" satisfaction. Thus, private property continually destroys itself, becoming transformed into communal wealth. The increasing proportion of communal wealth to private property results in a tendency toward equality of mankind. "Since the human race started from the point of greatest poverty, that is, from the point where there were the most obstacles to be overcome, it is clear that all that has been gained from one era to the next has been due to the spirit of property."
This transformation of private property into the communal domain, Bastiat points out, does not imply that private property will ever totally disappear. This is because man, as he progresses, continually invents new and more sophisticated needs and desires.
Andrew J. Galambos – A Precise Definition of Property.
Andrew J. Galambos (1924–1997) was an astrophysicist and philosopher who innovated a social structure that seeks to maximize human peace and freedom. Galambos’ concept of property was basic to his philosophy. He defined property as a man’s life and all non-procreative derivatives of his life. (Because the English language is deficient in omitting the feminine from “man” when referring to humankind, it is implicit and obligatory that the feminine is included in the term “man”.)
Galambos taught that property is essential to a non-coercive social structure. That is why he defined freedom as follows: “Freedom is the societal condition that exists when every individual has full (100%) control over his own property.” Galambos defines property as having the following elements:
Property includes all non-procreative derivatives of an individual’s life; this means children are not the property of their parents. and "primary property" (a person's own ideas).
Galambos emphasized repeatedly that true government exists to protect property and that the state attacks property.
For example, the state requires payment for its services in the form of taxes whether or not people desire such services. Since an individual’s money is his property, the confiscation of money in the form of taxes is an attack on property. Military conscription is likewise an attack on a person’s primordial property.
Contemporary views.
Contemporary political thinkers who believe that natural persons enjoy rights to own property and to enter into contracts espouse two views about John Locke. On the one hand, some admire Locke, such as W.H. Hutt (1956), who praised Locke for laying down the "quintessence of individualism". On the other hand, those such as Richard Pipes regard Locke's arguments are weak, and think that undue reliance thereon has weakened the cause of individualism in recent times. Pipes has written that Locke's work "marked a regression because it rested on the concept of Natural Law" rather than upon Harrington's sociological framework.
Hernando de Soto has argued that an important characteristic of capitalist market economy is the functioning state protection of property rights in a formal property system which clearly records ownership and transactions. These property rights and the whole formal system of property make possible:
All of the above, according to de Soto, enhance economic growth.

</doc>
<doc id="23627" url="http://en.wikipedia.org/wiki?curid=23627" title="Police">
Police

A police force is a constituted body of persons empowered by the state to enforce the law, protect property, and limit civil disorder. Their powers include the legitimized use of force. The term is most commonly associated with police services of a state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing.
Law enforcement, however, constitutes only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Many police forces suffer from police corruption to a greater or lesser degree. The police force is usually a public sector service, meaning they are paid through taxes.
Alternative names for police force include constabulary, gendarmerie, police department, police service, crime prevention, protective services, law enforcement agency, civil guard or civic guard. Members may be referred to as police officers, troopers, sheriffs, constables, rangers, peace officers or civic/civil guards.
As police are often interacting with individuals, slang terms are numerous. Many slang terms for police officers are decades or centuries old with lost etymology.
Etymology.
First attested in English in the early 15th century, initially in a range of senses ecompassing '(public) policy; state; public order', the word "police" comes from Middle French "police" ('public order, administration, government'), in turn from Latin "politia", which is the Latinisation of the Greek πολιτεία ("politeia"), "citizenship, administration, civil polity". This is derived from πόλις ("polis"), "city".
History.
Ancient policing.
Law enforcement in Ancient China was carried out by "prefects" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their "prefecture", or jurisdiction. Under each prefect were "subprefects" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the "prefecture system" spread to other cultures such as Korea and Japan.
In Ancient Greece, publicly owned slaves were used by magistrates as police. In Athens, a group of 300 Scythian slaves (the ῥαβδοῦχοι, "rod-bearers") was used to guard public meetings to keep order and for crowd control, and also assisted with dealing with criminals, handling prisoners, and making arrests. Other duties associated with modern policing, such as investigating crimes, were left to the citizens themselves.
In the Roman Empire, the Army, rather than a dedicated police organization, provided security. Local watchmen were hired by cities to provide some extra security. Magistrates such as "procurators fiscal" and "quaestors" investigated crimes. There was no concept of public prosecution, so victims of crime or their families had to organize and manage the prosecution themselves.
Under the reign of Augustus, when the capital had grown to almost one million inhabitants, 14 wards were created; the wards were protected by seven squads of 1,000 men called "vigiles", who acted as firemen and nightwatchmen. Their duties included apprehending thieves and robbers and capturing runaway slaves. The vigiles were supported by the Urban Cohorts who acted as a heavy-duty anti-riot force and even the Praetorian Guard if necessary.
Medieval policing.
In Medieval Spain, "hermandades", or "brotherhoods", peacekeeping associations of armed individuals, were a characteristic of municipal life, especially in Castile. As medieval Spanish kings often could not offer adequate protection, protective municipal leagues began to emerge in the 12th century against bandits and other rural criminals, and against the lawless nobility or to support one or another claimant to a crown.
These organizations were intended to be temporary, but became a long-standing fixture of Spain. The first recorded case of the formation of an "hermandad" occurred when the towns and the peasantry of the north united to police the pilgrim road to Santiago de Compostela in Galicia, and protect the pilgrims against robber knights.
Throughout the Middle Ages such alliances were frequently formed by combinations of towns to protect the roads connecting them, and were occasionally extended to political purposes. Among the most powerful was the league of North Castilian and Basque ports, the Hermandad de las marismas: Toledo, Talavera, and Villarreal.
As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand and Isabella established the centrally organized and efficient "Holy Brotherhood" ("Santa Hermandad") as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835.
The Fehmic courts of Germany provided some policing in the absence of strong state institutions.
In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' 'War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under King Francis I (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France.
The English system of maintaining public order since the Norman conquest was a private system of tithings, led by a constable, which was based on a social obligation for the good conduct of the others; more common was that local lords and nobles were responsible for maintaining order in their lands, and often appointed a constable, sometimes unpaid, to enforce the law. There was also a system investigative "juries".
The Assize of Arms of 1252, which required the appointment of constables to summon men to arms, quell breaches of the peace, and to deliver offenders to the sheriffs or reeves, is cited as one of the earliest creation of the English police. The Statute of Winchester of 1285 is also cited as the primary legislation regulating the policing of the country between the Norman Conquest and the Metropolitan Police Act 1829.
From about 1500, private watchmen were funded by private individuals and organisations to carry out police functions. They were later nicknamed 'Charlies', probably after the reigning monarch King Charles II. Thief-takers were also rewarded for catching thieves and returning the stolen property.
The first use of the word police ("Polles") in English comes from the book "The Second Part of the Institutes of the Lawes of England" published in 1642.
Early Modern policing.
The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the "Parlement" of Paris on March 15, 1667 created the office of "lieutenant général de police" ("lieutenant general of police"), who was to be the head of the new Paris police force, and defined the task of the police as "ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties".
This office was first held by Gabriel Nicolas de la Reynie, who had 44 "commissaires de police" (police commissioners) under his authority. In 1709, these commissioners were assisted by "inspecteurs de police" (police inspectors). The city of Paris was divided into 16 districts policed by the "commissaires", each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns.
After the French Revolution, Napoléon I reorganized the police in Paris and other cities with more than 5,000 inhabitants on February 17, 1800 as the Prefecture of Police. On March 12, 1829, a government decree created the first uniformed police in France, known as "sergents de ville" ("city sergeants"), which the Paris Prefecture of Police's website claims were the first uniformed policemen in the world.
In 1737, George II began paying some London and Middlesex watchmen with tax monies, beginning the shift to government control. In 1749 Henry Fielding began organizing a force of quasi-professional constables known as the Bow Street Runners. The Macdaniel affair added further impetus for a publicly salaried police force that did not depend on rewards. Nonetheless, In 1828, there were privately financed police units in no fewer than 45 parishes within a 10-mile radius of London.
The word "police" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were "disliked as a symbol of foreign oppression" (according to "Britannica 1911"). Before the 19th century, the first use of the word "police" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798.
Policing in London.
In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was "perfectly congenial to the principle of the British constitution." Moreover, he went so far as to praise the French system, which had reached "the greatest degree of perfection" in his estimation. 
With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and "on the game." The force was a success after its first year, and his men had "established their worth by saving £122,000 worth of cargo and by the rescuing of several lives." Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, "The Commerce and Policing of the River Thames". It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney.
Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's "new" police three decades later.
Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men.
Metropolitan police force.
London was fast reaching a size unprecedented in world history, due to the onset of the Industrial Revolution. It became clear that the locally maintained system of volunteer constables and "watchmen" was ineffective, both in detecting and preventing crime. A parliamentary committee was appointed to investigate the system of policing in London. Upon Sir Robert Peel being appointed as Home Secretary in 1822, he established a second and more effective committee, and acted upon its findings.
Royal Assent to the Metropolitan Police Act was given, and the Metropolitan Police Service was established on September 29, 1829 in London as the first modern and professional police force in the world.
Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralized, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public.
Due to public fears concerning the deployment of the military in domestic matters, Peel organised the force along civilian lines, rather than paramilitary. To appear neutral, the uniform was deliberately manufactured in blue, rather than red which was then a military colour, along with the officers being armed only with a wooden truncheon and a rattle to signal the need for assistance. Along with this, police ranks did not include military titles, with the exception of Sergeant.
To distance the new police force from the initial public view of it as a new tool of government repression, Peel publicised the so-called 'Peelian Principles', which set down basic guidelines for ethical policing:
The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and it's powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the 'Continental model' of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state.
In 1863, the Metropolitan Police were issued with the distinctive Custodian helmet, and in 1884 they switched to the use of whistles that could be heard from much further away. The Metropolitan Police became a model for the police forces in most countries, such as the United States, and most of the British Empire. Bobbies can still be found in many parts of the Commonwealth of Nations.
Other countries.
Australia.
In Australia the first police force having centralised command as well as jurisdiction over an entire colony was the South Australia Police, formed in 1838 under Henry Inman.
However, whilst the New South Wales Police Force was established in 1862, it was made up from a large number of policing and military units operating within the then Colony of New South Wales and traces its links back to the Royal Marines. The passing of the Police Regulation Act of 1862 essentially tightly regulated and centralised all of the police forces operating throughout the Colony of New South Wales.
The New South Wales Police Force remains the largest police force in Australia in terms of personnel and physical resources. It is also the only police force that requires its recruits to undertake university studies at the recruit level and has the recruit pay for their own education.
Brazil.
In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the "Intendência Geral de Polícia" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local "military police", with order maintenance tasks. The Federal Railroad Police was created in 1852.
Canada.
In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873.
The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police.
Lebanon.
In Lebanon, modern police were established in 1861, with creation of the Gendarmerie.
India.
In India, the police is under the control of respective States and union territories and is known to be under State Police Services (SPS). The candidates selected for the SPS are usually posted as Deputy Superintendent of Police or Assistant Commissioner of Police once their probationary period ends. On prescribed satisfactory service in the SPS, the officers are nominated to the Indian Police Service. The service color is usually dark blue and red, while the uniform color is "Khaki".
United States.
In British North America, policing was initially provided by local elected officials. For instance, the New York Sheriff's Office was founded in 1626, and the Albany County Sheriff's Department in the 1660s. In the colonial period, policing was provided by elected sheriffs and local militias.
In 1789 the U.S. Marshals Service was established, followed by other federal services such as the U.S. Parks Police (1791) and U.S. Mint Police (1792). The first city police services were established in Philadelphia in 1751, Richmond, Virginia in 1807, Boston in 1838, and New York in 1845. The U.S. Secret Service was founded in 1865 and was for some time the main investigative body for the federal government.
In the American Old West, policing was often of very poor quality. The Army often provided some policing alongside poorly resourced sheriffs and temporarily organised posses. Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.
In recent years, in addition to federal, state, and local forces, some special districts have been formed to provide extra police protection in designated areas. These districts may be known as neighborhood improvement districts, crime prevention districts, or security districts.
In 2005, the Supreme Court of the United States ruled that police do not have a constitutional duty to protect a person from harm.
Development of theory.
Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's "" ("Treatise on the Police"), first published in 1705. The German "Polizeiwissenschaft" (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of "Bibliographie der Kameralwissenschaften" (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850.
As conceptualized by the "Polizeiwissenschaft", the police had an administrative,economic and social duty ("procuring abundance"). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices.
The concept of preventive policing, or policing to deter crime from taking place, gained influence in the late 18th century. Police Magistrate John Fielding, head of the Bow Street Runners, argued that "...it is much better to prevent even one man from being a rogue than apprehending and bringing forty to justice."
The Utilitarian philosopher, Jeremy Bentham, promoted the views of Italian Marquis Cesare Beccaria, and disseminated a translated version of "Essay on Crime in Punishment". Bentham espoused the guiding principle of "the greatest good for the greatest number:
Patrick Colquhoun's influential work, "A Treatise on the Police of the Metropolis" (1797) was heavily influenced by Benthamite thought. Colquhoun's Thames River Police was founded on these principles, and in contrast to the Bow Street Runners, acted as a deterrent by their continual presence on the riverfront, in addition to being able to intervene if they spotted a crime in progress.
Edwin Chadwick's 1829 article, "Preventive police" in the "London Review", argued that prevention ought to be the "primary" concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that "A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation." In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - "crime doesn't pay". In the second draft of his 1829 Police Act, the "object" of the new Metropolitan Police, was changed by Robert Peel to the "principal object," which was the "prevention of crime." Later historians would attribute the perception of England's "appearance of orderliness and love of public order" to the preventive principle entrenched in Peel's police system.
Development of modern police forces around the world was contemporary to the formation of the state, later defined by sociologist Max Weber as achieving a "monopoly on the legitimate use of physical force" and which was primarily exercised by the police and the military. Marxist theory situates the development of the modern state as part of the rise of capitalism, in which the police are one component of the bourgeoisie's repressive apparatus for subjugating the working class.
Personnel and organization.
Police forces include both preventive (uniformed) police and detectives. Terminology varies from country to country. Police functions include protecting life and property, enforcing criminal law, criminal investigations, regulating traffic, crowd control, and other public safety duties.
Uniformed police.
Preventive Police, also called Uniform Branch, Uniformed Police, Uniform Division, Administrative Police, Order Police, or Patrol, designates the police that patrol and respond to emergencies and other incidents, as opposed to detective services. As the name "uniformed" suggests, they wear uniforms and perform functions that require an immediate recognition of an officer's legal authority, such as traffic control, stopping and detaining motorists, and more active crime response and prevention.
Preventive police almost always make up the bulk of a police service's personnel. In Australia and Britain, patrol personnel are also known as "general duties" officers. Atypically, Brazil's preventive police are known as Military Police.
Detectives.
Police detectives are responsible for investigations and detective work. Detectives may be called Investigations Police, Judiciary/Judicial Police, and Criminal Police. In the UK, they are often referred to by the name of their department, the Criminal Investigation Department (CID). Detectives typically make up roughly 15%-25% of a police service's personnel.
Detectives, in contrast to uniformed police, typically wear 'business attire' in bureaucratic and investigative functions where a uniformed presence would be either a distraction or intimidating, but a need to establish police authority still exists. "Plainclothes" officers dress in attire consistent with that worn by the general public for purposes of blending in.
In some cases, police are assigned to work "undercover", where they conceal their police identity to investigate crimes, such as organized crime or narcotics crime, that are unsolvable by other means. In some cases this type of policing shares aspects with espionage.
Despite popular conceptions promoted by movies and television, many US police departments prefer not to maintain officers in non-patrol bureaus and divisions beyond a certain period of time, such as in the detective bureau, and instead maintain policies that limit service in such divisions to a specified period of time, after which officers must transfer out or return to patrol duties. This is done in part based upon the perception that the most important and essential police work is accomplished on patrol in which officers become acquainted with their beats, prevent crime by their presence, respond to crimes in progress, manage crises, and practice their skills.
Detectives, by contrast, usually investigate crimes after they have occurred and after patrol officers have responded first to a situation. Investigations often take weeks or months to complete, during which time detectives spend much of their time away from the streets, in interviews and courtrooms, for example. Rotating officers also promotes cross-training in a wider variety of skills, and serves to prevent "cliques" that can contribute to corruption or other unethical behavior.
Auxiliary.
Police may also take on auxiliary administrative duties, such as issuing firearms licenses. The extent that police have these functions varies among countries, with police in France, Germany, and other continental European countries handling such tasks to a greater extent than British counterparts.
Specialized units.
Specialized preventive and detective groups, or Specialist Investigation Departments exist within many law enforcement organizations either for dealing with particular types of crime, such as traffic law enforcement and crash investigation, homicide, or fraud; or for situations requiring specialized skills, such as underwater search, aviation, explosive device disposal ("bomb squad"), and computer crime.
Most larger jurisdictions also employ specially selected and trained quasi-military units armed with military-grade weapons for the purposes of dealing with particularly violent situations beyond the capability of a patrol officer response, including high-risk warrant service and barricaded suspects. In the United States these units go by a variety of names, but are commonly known as SWAT (Special Weapons And Tactics) teams.
In counterinsurgency-type campaigns, select and specially trained units of police armed and equipped as light infantry have been designated as police field forces who perform paramilitary-type patrols and ambushes whilst retaining their police powers in areas that were highly dangerous.
Because their situational mandate typically focuses on removing innocent bystanders from dangerous people and dangerous situations, not violent resolution, they are often equipped with non-lethal tactical tools like chemical agents, "flashbang" and concussion grenades, and rubber bullets. The London Metropolitan police's Specialist Firearms Command (CO19) is a group of armed police used in dangerous situations including hostage taking, armed robbery/assault and terrorism.
Military police.
Military police may refer to:
Religious police.
Some Islamic societies have religious police, who enforce the application of Islamic Sharia law. Their authority may include the power to arrest unrelated men and women caught socializing, anyone engaged in homosexual behavior or prostitution; to enforce Islamic dress codes, and store closures during Islamic prayer time.
They enforce Muslim dietary laws, prohibit the consumption or sale of alcoholic beverages and pork, and seize banned consumer products and media regarded as un-Islamic, such as CDs/DVDs of various Western musical groups, television shows and film. In Saudi Arabia, the Mutaween actively prevent the practice or proselytizing of non-Islamic religions within Saudi Arabia, where they are banned.
Varying jurisdictions.
Police forces are usually organized and funded by some level of government. The level of government responsible for policing varies from place to place, and may be at the national, regional or local level. In some places there may be multiple police forces operating in the same area, with different ones having jurisdiction according to the type of crime or other circumstances.
For example in the UK, policing is primarily the responsibility of a regional police force; however specialist units exist at the national level. In the US, there is typically a state police force, but crimes are usually handled by local police forces that usually only cover a few municipalities. National agencies, such as the FBI, only have jurisdiction over federal crimes or those with an interstate component.
In addition to conventional urban or regional police forces, there are other police forces with specialized functions or jurisdiction. In the United States, the federal government has a number of police forces with their own specialized jurisdictions.
Some examples are the Federal Protective Service, which patrols and protects government buildings; the postal police, which protect postal buildings, vehicles and items; the Park Police, which protect national parks, or Amtrak Police which patrol Amtrak stations and trains.
There are also some government agencies that perform police functions in addition to other duties. The U.S. Coast Guard carries out many police functions for boaters.
In major cities, there may be a separate police agency for public transit systems, such as the New York City Port Authority Police or the MTA police, or for major government functions, such as sanitation, or environmental functions.
International policing.
The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention.
Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000).
Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996).
Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to "compose the panic scenes of the security-control society". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity.
Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009).
Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement.
Equipment.
Weapons.
In many jurisdictions, police officers carry firearms, primarily handguns, in the normal course of their duties. In the United Kingdom (except Northern Ireland), Iceland, Ireland, Norway, New Zealand, and Malta, with the exception of specialist units, officers do not carry firearms as a matter of course.
Police often have specialist units for handling armed offenders, and similar dangerous situations, and can (depending on local laws), in some extreme circumstances, call on the military (since Military Aid to the Civil Power is a role of many armed forces). Perhaps the most high-profile example of this was, in 1980 the Metropolitan Police handing control of the Iranian Embassy Siege to the Special Air Service.
They can also be armed with non-lethal (more accurately known as "less than lethal" or "less-lethal") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A "shoot-to-kill" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries.
Communications.
Modern police forces make extensive use of radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed computers have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and "ticket books" or citations.
Vehicles.
Police vehicles are used for detaining, patrolling and transporting. The average police patrol vehicle is a specially modified, four door sedan (saloon in British English). Police vehicles are usually marked with appropriate logos and are equipped with sirens and flashing light bars to aid in making others aware of police presence.
Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers.
Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot.
Police forces use an array of specialty vehicles such as helicopters, airplanes, watercraft, mobile command posts, vans, trucks, all-terrain vehicles, motorcycles, and SWAT armored vehicles.
Other safety equipment.
Police cars may also contain fire extinguishers or defibrillators.
Strategies.
The advent of the police car, two-way radio, and telephone in the early 20th century transformed policing into a reactive strategy that focused on responding to calls for service. With this transformation, police command and control became more centralized.
In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime, rather than broader focus on crime prevention.
The Kansas City Preventive Patrol study in the 1970s found this approach to policing to be ineffective. Patrol officers in cars were disconnected from the community, and had insufficient contact and interaction with the community. In the 1980s and 1990s, many law enforcement agencies began to adopt community policing strategies, and others adopted problem-oriented policing.
Broken windows policing was another, related approach introduced in the 1980s by James Q. Wilson and George L. Kelling, who suggested that police should pay greater attention to minor "quality of life" offenses and disorderly conduct. This method was first introduced and made popular by New York City Mayor, Rudy Giuliani, in the early 1990s.
The concept behind this method is simple: broken windows, graffiti, and other physical destruction or degradation of property, greatly increases the chances of more criminal activities and destruction of property. When criminals see the abandoned vehicles, trash, and deplorable property, they assume that authorities do not care and do not take active approaches to correct problems in these areas. Therefore, correcting the small problems prevents more serious criminal activity.
Building upon these earlier models, intelligence-led policing has emerged as the dominant philosophy guiding police strategy. Intelligence-led policing and problem-oriented policing are complementary strategies, both which involve systematic use of information. Although it still lacks a universally accepted definition, the crux of intelligence-led policing is an emphasis on the collection and analysis of information to guide police operations, rather than the reverse.
Power restrictions.
In many nations, criminal procedure law has been developed to regulate officers' discretion, so that they do not arbitrarily or unjustly exercise their powers of arrest, search and seizure, and use of force. In the United States, "Miranda v. Arizona" led to the widespread use of Miranda warnings or constitutional warnings.
In "Miranda" the court created safeguards against self-incriminating statements made after an arrest. The court held that "The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"
Police in the United States are also prohibited from holding criminal suspects for more than a reasonable amount of time (usually 24–48 hours) before arraignment, using torture, abuse or physical threats to extract confessions, using excessive force to effect an arrest, and searching suspects' bodies or their homes without a warrant obtained upon a showing of probable cause. The four exceptions to the constitutional requirement of a search warrant are:
In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search " [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only.
Using deception for confessions is permitted, but not coercion. There are exceptions or exigent circumstances such as an articulated need to disarm a suspect or searching a suspect who has already been arrested (Search Incident to an Arrest). The Posse Comitatus Act severely restricts the use of the military for police activity, giving added importance to police SWAT units.
British police officers are governed by similar rules, such as those introduced to England and Wales under the Police and Criminal Evidence Act 1984 (PACE), but generally have greater powers. They may, for example, legally search any suspect who has been arrested, or their vehicles, home or business premises, without a warrant, and may seize anything they find in a search as evidence.
All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent.
Conduct, accountability and public confidence.
Police services commonly include units for investigating crimes committed by the police themselves. These units are typically called Inspectorate-General, or in the US, "internal affairs". In some countries separate organizations outside the police exist for such purposes, such as the British Independent Police Complaints Commission.
Likewise, some state and local jurisdictions, for example, Springfield, Illinois have similar outside review organizations. The Police Service of Northern Ireland is investigated by the Police Ombudsman for Northern Ireland, an external agency set up as a result of the Patten report into policing the province. In the Republic of Ireland the Garda Síochána is investigated by the Garda Síochána Ombudsman Commission, an independent commission that replaced the Garda Complaints Board in May 2007.
The Special Investigations Unit of Ontario, Canada, is one of only a few civilian agencies around the world responsible for investigating circumstances involving police and civilians that have resulted in a death, serious injury, or allegations of sexual assault. The agency has made allegations of insufficient cooperation from various police services hindering their investigations.
In Hong Kong, any allegations of corruption within the police will be investigated by the Independent Commission Against Corruption and the Independent Police Complaints Council, two agencies which are independent of the police force.
Due to a long-term decline in public confidence for law enforcement in the United States, body cameras worn by police officers are under consideration.
Use of force.
Police forces also find themselves under criticism for their use of force, particularly deadly force. Specifically, tension increases when a police officer of one ethnic group harms or kills a suspect of another one. In the United States, such events occasionally spark protests and accusations of racism against police and allegations that police departments practice racial profiling.
In the United States since the 1960s, concern over such issues has increasingly weighed upon law enforcement agencies, courts and legislatures at every level of government. Incidents such as the 1965 Watts Riots, the videotaped 1991 beating by Los Angeles Police officers of Rodney King, and the riot following their acquittal have been suggested by some people to be evidence that U.S. police are dangerously lacking in appropriate controls.
The fact that this trend has occurred contemporaneously with the rise of the US civil rights movement, the "War on Drugs", and a precipitous rise in violent crime from the 1960s to the 1990s has made questions surrounding the role, administration and scope of police authority increasingly complicated. 
Police departments and the local governments that oversee them in some jurisdictions have attempted to mitigate some of these issues through community outreach programs and community policing to make the police more accessible to the concerns of local communities, by working to increase hiring diversity, by updating training of police in their responsibilities to the community and under the law, and by increased oversight within the department or by civilian commissions.
In cases in which such measures have been lacking or absent, civil lawsuits have been brought by the United States Department of Justice against local law enforcement agencies, authorized under the 1994 Violent Crime Control and Law Enforcement Act. This has compelled local departments to make organizational changes, enter into consent decree settlements to adopt such measures, and submit to oversight by the Justice Department.
Protection of individuals.
Since 1855, the Supreme Court of the United States has consistently ruled that law enforcement officers have no duty to protect any individual, despite the motto "protect and serve". Their duty is to enforce the law in general. The first such case was in 1855 (" (Supreme Court of the United States 1855). ") and the most recent in 2005 ("Town of Castle Rock v. Gonzales").
In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table.
In addition, there are Federal Law Enforcement agencies in the United States whose mission includes providing protection for executives such as the President and accompanying family members, visiting foreign dignitaries, and other high-ranking individuals. Such agencies include The United States Secret Service and the United States Park Police.
International forces.
In many countries, particularly those with a federal system of government, there may be several police or police like organizations, each serving different levels of government and enforcing different subsets of the applicable law. The United States has a highly decentralized and fragmented system of law enforcement, with over 17,000 state and local law enforcement agencies.
Some countries, such as Chile, Israel, the Philippines, France, Austria, New Zealand and South Africa, use a centralized system of policing. Other countries have multiple police forces, but for the most part their jurisdictions do not overlap. In the United States however, several different law enforcement agencies may have authority in a particular jurisdiction at the same time, each with their own command.
Other countries where jurisdiction of multiple police agencies overlap, include Guardia Civil and the Policía Nacional in Spain, the Polizia di Stato and Carabinieri in Italy and the Police Nationale and National Gendarmerie in France.
Most countries are members of the International Criminal Police Organization (Interpol), established to detect and fight transnational crime and provide for international co-operation and co-ordination of other police activities, such as notifying relatives of the death of foreign nationals. Interpol does not conduct investigations or arrests by itself, but only serves as a central point for information on crime, suspects and criminals. Political crimes are excluded from its competencies.

</doc>
<doc id="23628" url="http://en.wikipedia.org/wiki?curid=23628" title="PDP-10">
PDP-10

The PDP-10 is a discontinued mainframe computer family manufactured by Digital Equipment Corporation (DEC) from 1966 into the 1980s.
The PDP-10 architecture was an almost identical version of the earlier PDP-6 architecture, sharing the same 36-bit word length and slightly extending the instruction set (but with improved hardware implementation). Some aspects of the instruction set are unusual, most notably the "byte" instructions, which operated on bit fields of any size from 1 to 36 bits inclusive according to the general definition of a byte as "a contiguous sequence of a fixed number of bits".
The PDP-10 was the machine that made time-sharing common, and this and other features made it a common fixture in many university computing facilities and research labs during the 1970s, the most notable being MIT's AI Lab and Project MAC, Stanford's SAIL, Computer Center Corporation (CCC), and Carnegie Mellon University. Its main operating systems, TOPS-10 and TENEX, were used to build out the early ARPANET. For these reasons the PDP-10 looms large in early hacker folklore.
Projects to extend the PDP-10 line were eclipsed by the success of the unrelated VAX superminicomputer, and the cancellation of the PDP-10 line was announced in 1983.
Models and technical evolution.
The original PDP-10 processor was the KA10, introduced in 1968. It used discrete transistors packaged in DEC's Flip-Chip technology, with backplanes wire wrapped via a semi-automated manufacturing process. Its cycle time was 1 μs and its add time 2.1 μs. In 1973, the KA10 was replaced by the KI10, which used TTL SSI. This was joined in 1975 by the higher-performance KL10 (later faster variants), which was built from ECL, was microprogrammed, and had cache memory. A smaller, less expensive model, the KS10, was introduced in 1978, using TTL and Am2901 bit-slice components and including the PDP-11 Unibus to connect peripherals.
KA10.
The KA10 had a maximum main memory capacity (both virtual and physical) of 256 kilowords (equivalent to 1152 kilobytes). As supplied by DEC, it did not include paging hardware; memory management consisted of two sets of protection and relocation registers, called "base and bounds" registers. This allowed each half of a user's address space to be limited to a set section of main memory, designated by the base physical address and size. This allowed the model (later used by Unix) of separate read-only shareable code segment (normally the high segment) and read-write data/stack segment (normally the low segment). Some KA10 machines, first at MIT, and later at Bolt, Beranek and Newman (BBN), were modified to add virtual memory and support for demand paging, as well as more physical memory.
KI10 and KL10.
The KI10 and later processors offered paged memory management, and also supported a larger physical address space of 4 megawords. KI10 models included 1060, 1070 and 1077, the latter incorporating two CPUs.
The original KL10 TOPS-10 (also marketed as DECsystem-10) models (1080, 1088, etc.) used the original PDP-10 memory bus, with external memory modules. Module in this context meant a cabinet, dimensions roughly (WxHxD) 30 x 75 x 30 in. with a capacity of 32 to 256 kWords of magnetic core memory (the picture on the right hand side of the introduction shows six of these cabinets). The processors used in the DECSYSTEM-20 (2040, 2050, 2060, 2065), commonly but incorrectly called "KL20", used internal memory, mounted in the same cabinet as the CPU. The 10xx models also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. The differences between the 10xx and 20xx models were more cosmetic than real; some 10xx systems had "20-style" internal memory and I/O, and some 20xx systems had "10-style" external memory and an I/O bus. In particular, all ARPAnet TOPS-20 systems had an I/O bus because the AN20 IMP interface was an I/O bus device. Both could run either TOPS-10 or TOPS-20 microcode and thus the corresponding operating system.
MASSbus.
The I/O architecture of the 20xx series KL machines was based on a new DEC bus design called the MASSbus. While many attributed the success of the PDP-11 to DEC's decision to make the PDP-11 Unibus an open architecture, DEC reverted to prior philosophy with the KL, making MASSbus both unique and proprietary. Consequently, there were no aftermarket peripheral manufacturers who made devices for the MASSbus, and DEC chose to price their own MASSbus devices, notably the RP06 disk drive, at a substantial premium above comparable IBM-compatible devices. CompuServe for one, designed its own alternative disk controller that could operate on the MASSbus, but connect to IBM style 3330 disk subsystems.
Model B.
Later, the "Model B" version of the 2060 processors removed the 256 kiloword limitation on the virtual address space, by allowing the use of up to 32 "sections" of up to 256 kilowords each, along with substantial changes to the instruction set. "Model A" and "Model B" KL10 processors can be thought of as being different CPUs. The first operating system that took advantage of the Model B's capabilities was TOPS-20 release 3, and user mode extended addressing was offered in TOPS-20 release 4. TOPS-20 versions after release 4.1 would only run on a Model B.
TOPS-10 versions 7.02 and 7.03 also used extended addressing when run on a 1090 (or 1091) Model B processor running TOPS-20 microcode.
MCA25.
The final upgrade to the KL10 was the MCA25 upgrade of a 2060 to 2065 (or a 1091 to 1095), which gave some performance increases for programs which run in multiple sections.
KS10.
The KS10 design was crippled to be a Model A even though most of the necessary data paths needed to support the Model B architecture were present. This was no doubt intended to segment the market, but it greatly shortened the KS10's product life.
Frontend systems.
The KL class machines could not be started without the assist of a PDP-11/40 frontend computer installed in every system. The PDP-11 was booted from a dual-ported RP06 disk drive (or alternatively from an 8" floppy disk drive or DECtape), and then commands could be given to the PDP-11 to start the main processor, which was typically booted from the same RP06 disk drive as the PDP-11. The PDP-11 would perform watchdog functions once the main processor was running.
The KS system used a similar boot procedure. An 8080 CPU loaded the microcode from an RM03, RM80, or RP06 disk or magnetic tape and then started the main processor. The 8080 switched modes after the operating system booted and controlled the console and remote diagnostic serial ports.
Instruction set architecture.
From the first PDP-6's to the Model A KL-10s, the user-mode instruction set architecture was largely the same. This section covers that architecture. (Multi-section extended addressing is covered in the "DECsystem-10/DECSYSTEM-20 Processor Reference Manual".)
Addressing.
The PDP-10 has 36-bit words and 18-bit word addresses. In supervisor mode, instruction addresses correspond directly to physical memory. In user mode, addresses are translated to physical memory. Earlier models gave a user process a "high" and a "low" memory: addresses with a 0 top bit used one base register, and higher addresses used another. Each segment was contiguous. Later architectures had paged memory access, allowing non-contiguous address spaces. The CPU's general-purpose registers can also be addressed as memory locations 0-15.
Registers.
There are 16 general-purpose, 36-bit registers. The right half of these registers (other than register 0) may be used for indexing. A few instructions operate on pairs of registers. The "PC Word" consists of a 13-bit condition register (plus 5 always zero bits) in the left half and an 18-bit Program Counter in the right half. The condition register, which records extra bits from the results of arithmetic operations ("e.g." overflow), can be accessed by only a few instructions.
Supervisor mode.
There are two operational modes, supervisor and user mode. Besides the difference in memory referencing described above, supervisor-mode programs can execute input/output operations.
Communication from user-mode to supervisor-mode is done through Unimplemented User Operations (UUOs): instructions which are not defined by the hardware are trapped by the supervisor. This mechanism is also used to emulate operations which may not have hardware implementations in cheaper models.
Data types.
The major datatypes which are directly supported by the architecture are two's complement 36-bit integer arithmetic (including bitwise operations), 36-bit floating-point, and halfwords. Extended, 72-bit, floating point is supported through special instructions designed to be used in multi-instruction sequences. Byte pointers are supported by special instructions. A word structured as a "count" half and a "pointer" half facilitates the use of bounded regions of memory, notably stacks.
Instructions.
The instruction set is very symmetric. Every instruction consists of a 9-bit opcode, a 4-bit register code, and a 23-bit effective address field, which consists in turn of a 1-bit indirect bit, a 4-bit register code, and an 18-bit offset. Instruction execution begins by calculating the effective address. It adds the contents of the given register (if non-zero) to the offset; then, if the indirect bit is 1, fetches the word at the calculated address and repeats the effective address calculation until an effective address with a zero indirect bit is reached. The resulting effective address can be used by the instruction either to fetch memory contents, or simply as a constant. Thus, for example, MOVEI A,3(C) adds 3 to the 18 lower bits of register C and puts the result in register A, without touching memory.
There are three main classes of instruction: arithmetic, logical, and move; conditional jump; conditional skip (which may have side effects). There are also several smaller classes.
The arithmetic, logical, and move operations include variants which operate immediate-to-register, memory-to-register, register-to-memory, register-and-memory-to-both or memory-to-memory. Since registers may be addressed as part of memory, register-to-register operations are also defined. (Not all variants are useful, though they are well-defined.) For example, the ADD operation has as variants ADDI (add an 18-bit "I"mmediate constant to a register), ADDM (add register contents to a "M"emory location), ADDB (add to "B"oth, that is, add register contents to memory and also put the result in the register). A more elaborate example is HLROM ("H"alf "L"eft to "R"ight, "O"nes to "M"emory), which takes the Left half of the register contents, places them in the Right half of the memory location, and replaces the left half of the memory location with Ones.
The conditional jump operations examine register contents and jump to a given location depending on the result of the comparison. The mnemonics for these instructions all start with JUMP, JUMPA meaning "jump always" and JUMP meaning "jump never" - as a consequence of the symmetrical design of the instruction set, it contains several no-ops such as JUMP. For example, JUMPN A,LOC jumps to the address LOC if the contents of register A is non-zero. There are also conditional jumps based on the processor's condition register using the JRST instruction. On the KA10 and KI10, JRST was faster than JUMPA, so the standard unconditional jump was JRST.
The conditional skip operations compare register and memory contents and skip the next instruction (which is often an unconditional jump) depending on the result of the comparison. A simple example is CAMN A,LOC which compares the contents of register A with the contents of location LOC and skips the next instruction if they are not equal. A more elaborate example is TLCE A,LOC (read "Test Left Complement, skip if Equal"), which using the contents of LOC as a mask, selects the corresponding bits in the left half of register A. If all those bits are "E"qual to zero, skip the next instruction; and in any case, replace those bits by their boolean complement.
Some smaller instruction classes include the shift/rotate instructions and the procedure call instructions. Particularly notable are the stack instructions PUSH and POP, and the corresponding stack call instructions PUSHJ and POPJ. The byte instructions use a special format of indirect word to extract and store arbitrary-sized bit fields, possibly advancing a pointer to the next unit.
Software.
The original PDP-10 operating system was simply called "Monitor", but was later renamed TOPS-10. Eventually the PDP-10 system itself was renamed the DECsystem-10. Early versions of Monitor and TOPS-10 formed the basis of Stanford's WAITS operating system and the Compuserve time-sharing system.
Over time, some PDP-10 operators began running operating systems assembled from major components developed outside DEC. For example, the main Scheduler might come from one university, the Disk Service from another, and so on. The commercial timesharing services such as CompuServe, On-Line Systems (OLS), and Rapidata maintained sophisticated inhouse systems programming groups so that they could modify the operating system as needed for their own businesses without being dependent on DEC or others. There were also strong user communities such as DECUS through which users could share software that they had developed.
BBN developed their own alternative operating system, TENEX, which fairly quickly became the de facto standard in the research community. DEC later ported Tenex to the KL10, enhanced it considerably, and named it TOPS-20, forming the DECSYSTEM-20 line. MIT also had developed their own influential system, the Incompatible Timesharing System (named in parody of the Compatible Time-Sharing System, developed at MIT for a modified IBM 7094).
Tymshare developed TYMCOM-X, derived from TOPS-10 but using a page-based file system like TOPS-20.
Clones.
In 1971 to 1972 researchers at Xerox PARC were frustrated by top company management's refusal to let them purchase a PDP-10. Xerox had just bought Scientific Data Systems in 1969, and wanted PARC to use an SDS machine.
Instead, a group led by Charles P. Thacker designed and constructed two PDP-10 clone systems named "MAXC" (pronounced "Max", in honour of Max Palevsky, who had sold SDS to Xerox) for their own use. MAXC was also a backronym for Multiple Access Xerox Computer.
MAXC ran a modified version of TENEX.
Third-party attempts to sell PDP-10 clones were relatively unsuccessful; see Foonly, Systems Concepts, and XKL.
Use by CompuServe.
One of the largest collections of DECsystem-10 architecture systems ever assembled was at CompuServe, which at its peak operated over 200 loosely-coupled systems in three data centers in Columbus, Ohio. CompuServe used these systems as 'hosts', providing access to commercial applications as well as the CompuServe Information Service. While the first such systems were purchased from DEC, when DEC abandoned the PDP-10 architecture in favor of the VAX, CompuServe and other PDP-10 customers began purchasing plug compatible computers from Systems Concepts. As of January 2007, CompuServe continues to operate a small number of PDP-10 architecture machines to perform some billing and routing functions.
The main power supplies used in the KL-series machines were so inefficient that CompuServe engineers designed a replacement power supply that consumed about half the energy. CompuServe offered to license the design for its KL power supply to DEC for free if DEC would promise that any new KL purchased by CompuServe would have the more efficient power supply installed. DEC declined the offer.
Another modification made to the PDP-10 by CompuServe engineers was the replacement of the hundreds of incandescent indicator lamps on the KI10 processor cabinet with LED lamp modules. The cost of the conversion was easily offset by the cost savings in electric consumption, the reduction of heat, and the manpower required to replace burned-out lamps. Digital followed this step all over the world. The picture on the right hand side shows the light panel of the MF10 memory which is contemporaneous with the KI10 CPU. This item is part of a computer museum, and was populated with LEDs in 2008 for demonstration purposes only. There were no similar banks of indicator lamps on KL and KS processors.
Cancellation and influence.
The PDP-10 was eventually eclipsed by the VAX superminicomputer machines (descendants of the PDP-11) when DEC recognized that the PDP-10 and VAX product lines were competing with each other and decided to concentrate its software development effort on the more profitable VAX. The PDP-10 product line cancellation was announced in 1983, including cancelling the ongoing Jupiter project to produce a new high-end PDP-10 processor (despite that project being in good shape at the time of the cancellation) and the Minnow project to produce a desktop PDP-10, which may then have been at the prototyping stage.
This event spelled the doom of ITS and the technical cultures that had spawned the original jargon file, but by the 1990s it had become something of a badge of honor among old-time hackers to have cut one's teeth on a PDP-10.
The PDP-10 assembly language instructions LDB and DPB (load/deposit byte) live on as functions in the programming language Common Lisp. See the "References" section on the LISP article — the 36-bit word size of the PDP-6 and PDP-10 was influenced by the programming convenience of having 2 LISP pointers, each 18 bits, in one word.
Will Crowther created "Adventure", the prototypical computer adventure game, for a PDP-10. Don Daglow created the first computer baseball game (1971) and "Dungeon" (1975), the first role-playing video game on a PDP-10. Walter Bright originally created "Empire" for the PDP-10. Roy Trubshaw and Richard Bartle created the first MUD on a PDP-10. In addition, "Zork" was written on the PDP-10, and Infocom used several PDP-10s for game development and testing.
Bill Gates and Paul Allen originally wrote Altair BASIC using an Intel 8080 emulator running on a PDP-10 at Harvard University. They founded Microsoft shortly after.
Emulation or simulation.
The software for simulation of historical computers SIMH contains a module to emulate the KS10 CPU on a Windows or Unix-based machine. Copies of DEC's original distribution tapes are available as downloads from the Internet so that a running TOPS-10 or TOPS-20 system may be established. ITS is also available for SIMH.
Ken Harrenstien's KLH10 software for Unix-like systems emulates a KL10B processor with extended addressing and 4 MW of memory or a KS10 processor with 512 KW of memory. The KL10 emulation supports v.442 of the KL10 microcode, which enables it to run the final versions of both TOPS-10 and TOPS-20. The KS10 emulation supports both ITS v.262 microcode for the final version of KS10 ITS and DEC v.130 microcode for the final versions of KS TOPS-10 and TOPS-20.
"This article is based in part on the Jargon File, which is in the public domain."

</doc>
<doc id="23629" url="http://en.wikipedia.org/wiki?curid=23629" title="DECSYSTEM-20">
DECSYSTEM-20

The DECSYSTEM-20 was a 36-bit Digital Equipment Corporation PDP-10 mainframe computer running the TOPS-20 operating system (products introduced in 1977).
PDP-10 computers running the TOPS-10 operating system were labeled "DECsystem-10" as a way of differentiating them from the PDP-11. Later on, those systems running TOPS-20 (on the KL10 PDP-10 processors) were labeled "DECSYSTEM-20" (the block capitals being the result of a lawsuit brought against DEC by Singer, which once made a computer called "system-10"). The DECSYSTEM-20 was sometimes called PDP-20, although this designation was never used by DEC. 
The following models were produced:
The only significant difference the user could see between a DECsystem-10 and a DECSYSTEM-20 was the operating system and the color of the paint. Most (but not all) machines sold to run TOPS-10 were painted "Blasi Blue", whereas most TOPS-20 machines were painted "Terracotta" (often mistakenly called "Chinese Red" or orange; the actual name of the color on the paint cans was Terracotta).
There were some significant internal differences between the earlier KL10 Model A processors, used in the earlier DECsystem-10s running on KL10 processors, and the later KL10 Model Bs, used for the DECSYSTEM-20s. Model As used the original PDP-10 memory bus, with external memory modules. The later Model B processors used in the DECSYSTEM-20 used internal memory, mounted in the same cabinet as the CPU. The Model As also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. 
The last released implementation of DEC's 36-bit architecture was the single cabinet DECSYSTEM-2020, using a KS10 processor. 
The DECSYSTEM-20 was primarily designed and used as a small mainframe for timesharing. That is, multiple users would concurrently log on to individual user accounts and share use of the main processor to compile and run applications. Separate disk allocations were maintained for all users by the operating system, and various levels of protection could be maintained by for System, Owner, Group, and World users. A model 2060, for example, could typically host up to 40 to 60 simultaneous users before exhibiting noticeably reduced response time.
Remaining machines.
The Living Computer Museum of Seattle, Washington maintains a 2065 running TOPS-10, which is available to interested parties via telnet upon registration (at no cost) at their website.

</doc>
<doc id="23630" url="http://en.wikipedia.org/wiki?curid=23630" title="Programmed Data Processor">
Programmed Data Processor

Programmed Data Processor ("PDP") was a series of minicomputers made and marketed by the Digital Equipment Corporation from 1957 to 1990. The name 'PDP' intentionally avoided the use of the term 'computer' because, at the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines, and the venture capitalists behind Digital (especially Georges Doriot) would not support Digital's attempting to build a "computer"; the word "minicomputer" had not yet been coined. So instead, Digital used their existing line of logic modules to build a "Programmed Data Processor" and aimed it at a market which could not afford the larger computers. 
The various PDP machines can generally be grouped into families based on word length.
PDP series.
Members of the PDP series include:
External links.
Various sites list documents by Charles Lasner, the creator of the alt.sys.pdp8 discussion group, and related documents by various members of the alt.sys.pdp8 readership with even more authoritative information about the various models, especially detailed focus upon the various members of the PDP-8 "family" of computers both made and not made by DEC.

</doc>
<doc id="23631" url="http://en.wikipedia.org/wiki?curid=23631" title="Primary mirror">
Primary mirror

A primary mirror (or primary) is the principal light-gathering surface (the objective) of a reflecting telescope.
Description.
The primary mirror of a reflecting telescope is a spherical or parabolic shaped disks of polished reflective metal (speculum metal up to the mid 19th century), or in later telescopes, glass or other material coated with a reflective layer. One of the first known reflecting telescopes, Newton's reflector of 1668, used a 3.3 cm polished metal primary mirror. The next major change was to use silver on glass rather than metal, in the 19th century such was with the Crossley reflector. This was changed to vacuum deposited aluminum on glass, used on the 200-inch Hale telescope.
Solid primary mirrors have to sustain their own weight and not deform under gravity, which limits the maximum size for a single piece primary mirror.
Segmented mirror configurations are used to get around the size limitation on single primary mirrors. For example, the Giant Magellan Telescope will have seven 8.4 meter primary mirrors, with the resolving power equivalent to a 24.5 m optical aperture.
Superlative primary mirrors.
The largest optical telescope in the world as of 2009 to use a non-segmented single-mirror as its primary mirror is the 8.2 m (8.7 yards) Subaru telescope of the National Astronomical Observatory of Japan, located in Mauna Kea Observatory on Hawaii since 1997; however, this is not the largest diameter single mirror in a telescope, the U.S./German/Italian Large Binocular Telescope has two 8.4 m (9.2 yards) mirrors (which can be used together for interferometric mode). Both of these are smaller than the 10 m segmented primary mirrors on the two Keck telescope. The Hubble Space Telescope has a 2.4 m (7 ft 10 in) primary mirror.
Radio and submillimeter telescopes use much larger dishes or antennae, which do not have to be made as precisely as the mirrors used in optical telescopes. The Arecibo Observatory uses a 305 m dish, which is the world largest single-dish radio telescope fixed to the ground. The Green Bank Telescope has the world's largest steerable single radio dish with 100 m in diameter. There are largest radio arrays, composed of multiple dishes which have better image resolution but less sensitivity.

</doc>
<doc id="23632" url="http://en.wikipedia.org/wiki?curid=23632" title="Platonic idealism">
Platonic idealism

Platonic idealism usually refers to Plato's theory of forms or doctrine of ideas.
Some commentators hold Plato argued that truth is an abstraction. In other words, we are urged to believe that Plato's theory of ideas is an abstraction, divorced from the so-called external world, of modern European philosophy, despite the fact Plato taught that ideas are ultimately real, and different from non-ideal things—indeed, he argued for a distinction between the ideal and non-ideal realm.
These commentators speak thus: For example, a particular tree, with a branch or two missing, possibly alive, possibly dead, and with the initials of two lovers carved into its bark, is distinct from the abstract form of Tree-ness. A Tree is the ideal that each of us holds that allows us to identify the imperfect reflections of trees all around us.
Plato gives the divided line as an outline of this theory. At the top of the line, the Form of the Good
is found, directing everything underneath.
Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular).
Platonism is an ancient school of philosophy, founded by Plato; at the beginning, this school had a physical existence at a site just outside the walls of Athens called the Academy, as well as the intellectual unity of a shared approach to philosophizing.
Platonism is usually divided into three periods:
Plato's students used the hypomnemata as the foundation to his philosophic approach to knowledge. The hypomnemata constituted a material memory of things read, heard, or thought, thus offering these as an accumulated treasure for rereading and later meditation. For the Neoplatonist they also formed a raw material for the writing of more systematic treatises in which were given arguments and means by which to struggle against some defect (such as anger, envy, gossip, flattery) or to overcome some difficult circumstance (such as a mourning, an exile, downfall, disgrace).
Platonism is considered to be, in mathematics departments the world over, the predominant philosophy of mathematics, especially regarding the foundations of mathematics.
One statement of this philosophy is the thesis that mathematics is not created but discovered.
A lucid statement of this is found in an essay written by the British mathematician G. H. Hardy in defense of pure mathematics.
The absence in this thesis of clear distinction between mathematical and nonmathematical "creation" leaves open the inference that it applies to allegedly creative endeavors in art, music, and literature.
It is unknown if Plato's ideas of idealism have some earlier origin, but Plato held Pythagoras in high regard, and Pythagoras as well as his followers in the movement known as Pythagoreanism claimed the world was literally built up from numbers, an abstract, absolute form.

</doc>
<doc id="23633" url="http://en.wikipedia.org/wiki?curid=23633" title="List of physicists">
List of physicists

Following is a list of physicists who are for their achievements.

</doc>
<doc id="23634" url="http://en.wikipedia.org/wiki?curid=23634" title="Protein">
Protein

Proteins ( or ) are large biological molecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within living organisms, including catalyzing metabolic reactions, replicating DNA, responding to stimuli, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in folding of the protein into a specific three-dimensional structure that determines its activity.
A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than about 20-30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; however, in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by posttranslational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Sometimes proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.
Once formed, proteins only exist for a certain period of time and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal and or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.
Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyze biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. Proteins are also necessary in animals' diets, since animals cannot synthesize all the amino acids they need and must obtain essential amino acids from food. Through the process of digestion, animals break down ingested protein into free amino acids that are then used in metabolism.
Proteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.
Biochemistry.
Most proteins consist of linear polymers built from series of up to 20 different -α-amino acids. All proteinogenic amino acids possess common structural features, including an α-carbon to which an amino group, a carboxyl group, and a variable side chain are bonded. Only proline differs from this basic structure as it contains an unusual ring to the N-end amine group, which forces the CO–NH amide moiety into a fixed conformation. The side chains of the standard amino acids, detailed in the list of standard amino acids, have a great variety of chemical structures and properties; it is the combined effect of all of the amino acid side chains in a protein that ultimately determines its three-dimensional structure and its chemical reactivity.
The amino acids in a polypeptide chain are linked by peptide bonds. Once linked in the protein chain, an individual amino acid is called a "residue," and the linked series of carbon, nitrogen, and oxygen atoms are known as the "main chain" or "protein backbone."
The peptide bond has two resonance forms that contribute some double-bond character and inhibit rotation around its axis, so that the alpha carbons are roughly coplanar. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. The end of the protein with a free carboxyl group is known as the C-terminus or carboxy terminus, whereas the end with a free amino group is known as the N-terminus or amino terminus.
The words "protein", "polypeptide," and "peptide" are a little ambiguous and can overlap in meaning. "Protein" is generally used to refer to the complete biological molecule in a stable conformation, whereas "peptide" is generally reserved for a short amino acid oligomers often lacking a stable three-dimensional structure. However, the boundary between the two is not well defined and usually lies near 20–30 residues. "Polypeptide" can refer to any single linear chain of amino acids, usually regardless of length, but often implies an absence of a defined conformation.
Synthesis.
Biosynthesis.
Proteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine-uracil-guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (also known as a "primary transcript") using various forms of Post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.
The process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase "charges" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the "nascent chain". Proteins are always biosynthesized from N-terminus to C-terminus.
The size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported in units of "daltons" (synonymous with atomic mass units), or the derivative unit kilodalton (kDa). Yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost 3,000 kDa and a total length of almost 27,000 amino acids.
Chemical synthesis.
Short proteins can also be synthesized chemically by a family of methods known as peptide synthesis, which rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.
Structure.
Most proteins fold into unique 3-dimensional structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:
Proteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as "conformations", and transitions between them are called "conformational changes." Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution proteins also undergo variation in structure through thermal vibration and the collision with other molecules.
Proteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.
A special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.
Structure determination.
Discovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function. Common experimental methods of structure determination include X-ray crystallography and NMR spectroscopy, both of which can produce information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal beta sheet/ helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can also produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.
Many more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.
Cellular functions.
Proteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an "Escherichia coli" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.
The chief characteristic of proteins that also allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or "pocket" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (<10−15 M) but does not bind at all to its amphibian homolog onconase (>1 M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.
Proteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein–protein interactions also regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can also bind to, or even be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.
Importantly, as interactions between proteins are reversible, and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.
Enzymes.
The best-known role of proteins in the cell is as enzymes, which catalyze chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalyzed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous—as much as 1017-fold increase in rate over the uncatalyzed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).
The molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction—three to four residues on average—that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.
Dirigent proteins are members of a class of proteins which dictate the stereochemistry of a compound synthesized by other enzymes.
Cell signaling and ligand binding.
Many proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.
Antibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.
Many ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, but must also release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.
Transmembrane proteins can also serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.
Structural proteins.
Structural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can also play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.
Other proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They also generate the forces exerted by contracting muscles and play essential roles in intracellular transport.
Methods of study.
The activities and structures of proteins may be examined "in vitro," "in vivo, and in silico". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.
Protein purification.
To perform "in vitro" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according their charge using electrofocusing.
For natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a "tag" consisting of a specific amino acid sequence, often a series of histidine residues (a "His-tag"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of different tags have been developed to help researchers purify specific proteins from complex mixtures.
Cellular localization.
The study of proteins "in vivo" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a "reporter" such as green fluorescent protein (GFP). The fused protein's position within the cell can be cleanly and efficiently visualized using microscopy, as shown in the figure opposite.
Other methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.
Other possibilities exist, as well. For example, immunohistochemistry usually utilizes an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it does increase the likelihood, and is more amenable to large-scale studies.
Finally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique also uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.
Through another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.
Proteomics.
The total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of a large number of proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of a large number of proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein–protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.
Bioinformatics.
A vast array of computational methods have been developed to analyze the structure, function, and evolution of proteins.
The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.
Structure prediction and simulation.
Complementary to the field of structural genomics, protein structure prediction seeks to develop efficient ways to provide plausible models for proteins whose structures have not yet been determined experimentally. The most successful type of structure prediction, known as homology modeling, relies on the existence of a "template" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a "perfect" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein–protein interaction prediction.
The processes of protein folding and binding can be simulated using such technique as molecular mechanics, in particular, molecular dynamics and Monte Carlo, which increasingly take advantage of parallel and distributed computing (Folding@home project; molecular modeling on GPU). The folding of small alpha-helical protein domains such as the villin headpiece and the HIV accessory protein have been successfully simulated "in silico", and hybrid methods that combine standard molecular dynamics with quantum mechanics calculations have allowed exploration of the electronic states of rhodopsins.
Nutrition.
Most microorganisms and plants can biosynthesize all 20 standard amino acids, while animals (including humans) must obtain some of the amino acids from the diet. The amino acids that an organism cannot synthesize on its own are referred to as essential amino acids. Key enzymes that synthesize certain amino acids are not present in animals — such as aspartokinase, which catalyzes the first step in the synthesis of lysine, methionine, and threonine from aspartate. If amino acids are present in the environment, microorganisms can conserve energy by taking up the amino acids from their surroundings and downregulating their biosynthetic pathways.
In animals, amino acids are obtained through the consumption of foods containing protein. Ingested proteins are then broken down into amino acids through digestion, which typically involves denaturation of the protein through exposure to acid and hydrolysis by enzymes called proteases. Some ingested amino acids are used for protein biosynthesis, while others are converted to glucose through gluconeogenesis, or fed into the citric acid cycle. This use of protein as a fuel is particularly important under starvation conditions as it allows the body's own proteins to be used to support life, particularly those found in muscle. Amino acids are also an important dietary source of nitrogen.
History and etymology.
Proteins were recognized as a distinct class of biological molecules in the eighteenth century by Antoine Fourcroy and others, distinguished by the molecules' ability to coagulate or flocculate under treatments with heat or acid. Noted examples at the time included albumin from egg whites, blood serum albumin, fibrin, and wheat gluten.
Proteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, C400H620N100O120P1S1. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term "protein" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος ("proteios"), meaning "primary", "in the lead", or "standing in front". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da.
Early nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that "flesh makes flesh." Karl Heinrich Ritthausen extended known protein forms with the identification of glutamic acid. At the Connecticut Agricultural Experiment Station a detailed review of the vegetable proteins was compiled by Thomas Burr Osborne. Working with Lafayette Mendel and applying Liebig's law of the minimum in feeding laboratory rats, the nutritionally essential amino acids were established. The work was continued and communicated by William Cumming Rose. The understanding of proteins as polypeptides came through the work of Franz Hofmeister and Hermann Emil Fischer. The central role of proteins as enzymes in living organisms was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.
The difficulty in purifying proteins in large quantities made them very difficult for early protein biochemists to study. Hence, early studies focused on proteins that could be purified in large quantities, e.g., those of blood, egg white, various toxins, and digestive/metabolic enzymes obtained from slaughterhouses. In the 1950s, the Armour Hot Dog Co. purified 1 kg of pure bovine pancreatic ribonuclease A and made it freely available to scientists; this gesture helped ribonuclease A become a major target for biochemical study for the following decades.
Linus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.
The first protein to be sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958.
The first protein structures to be solved were hemoglobin and myoglobin, by Max Perutz and Sir John Cowdery Kendrew, respectively, in 1958. s of 2014[ [update]], the Protein Data Bank has over 90,000 atomic-resolution structures of proteins. In more recent times, cryo-electron microscopy of large macromolecular assemblies and computational protein structure prediction of small protein domains are two methods approaching atomic resolution.
Textbooks.
</dl>

</doc>
<doc id="23635" url="http://en.wikipedia.org/wiki?curid=23635" title="Physical chemistry">
Physical chemistry

Physical chemistry is the study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of laws and concepts of physics. It applies the principles, practices and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics and dynamics, equilibrium.
Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which physical chemistry was founded are concepts related to the bulk rather than on molecular/atomic structure alone. For example, chemical equilibrium, and colloids. 
Some of the relationships that physical chemistry strives to resolve include the effects of:
Key concepts.
The key concepts of physical chemistry are the ways in which pure physics is applied to chemical problems.
One of the key concepts in classical chemistry is that all chemical compounds can be described as groups of atoms bonded together and chemical reactions can be described as the making and breaking of those bonds. Predicting the properties of chemical compounds from a description of atoms and how they bond is one of the major goals of physical chemistry. To describe the atoms and bonds precisely, it is necessary to know both where the nuclei of the atoms are, and how electrons are distributed around them.<br>
Quantum chemistry, a subfield of physical chemistry especially concerned with the application of quantum mechanics to chemical problems, provides tools to determine how strong and what shape bonds are, how nuclei move, and how light can be absorbed or emitted by a chemical compound. Spectroscopy is the related sub-discipline of physical chemistry which is specifically concerned with the interaction of electromagnetic radiation with matter.
Another set of important questions in chemistry concerns what kind of reactions can happen spontaneously and which properties are possible for a given chemical mixture. This is studied in chemical thermodynamics, which sets limits on quantities like how far a reaction can proceed, or how much energy can be converted into work in an internal combustion engine, and which provides links between properties like the thermal expansion coefficient and rate of change of entropy with pressure for a gas or a liquid. It can frequently be used to assess whether a reactor or engine design is feasible, or to check the validity of experimental data. To a limited extent, quasi-equilibrium and non-equilibrium thermodynamics can describe irreversible changes. However, classical thermodynamics is mostly concerned with systems in equilibrium and reversible changes and not what actually does happen, or how fast, away from equilibrium.
Which reactions do occur and how fast is the subject of chemical kinetics, another branch of physical chemistry. A key idea in chemical kinetics is that for reactants to react and form products, most chemical species must go through transition states which are higher in energy than either the reactants or the products and serve as a barrier to reaction. In general, the higher the barrier, the slower the reaction. A second is that most chemical reactions occur as a sequence of elementary reactions, each with its own transition state. Key questions in kinetics include how the rate of reaction depends on temperature and on the concentrations of reactants and catalysts in the reaction mixture, as well as how catalysts and reaction conditions can be engineered to optimize the reaction rate.
The fact that how fast reactions occur can often be specified with just a few concentrations and a temperature, instead of needing to know all the positions and speeds of every molecule in a mixture, is a special case of another key concept in physical chemistry, which is that to the extent an engineer needs to know, everything going on in a mixture of very large numbers (perhaps of the order of the Avogadro constant, 6 x 1023) of particles can often be described by just a few variables like pressure, temperature, and concentration. The precise reasons for this are described in statistical mechanics, a specialty within physical chemistry which is also shared with physics. Statistical mechanics also provides ways to predict the properties we see in everyday life from molecular properties without relying on empirical correlations based on chemical similarities.
History.
The term "physical chemistry" was coined by Mikhail Lomonosov in 1752, when he presented a lecture course entitled "A Course in True Physical Chemistry" (Russian: «Курс истинной физической химии») before the students of Petersburg University. In the preamble to these lectures he gives definition: "Physical chemistry is the science that must explain under provisions of physical experiments the reason for what is happening in complex bodies through chemical operations".
Modern physical chemistry originated in the 1860s to 1880s with work on chemical thermodynamics, electrolytes in solutions, chemical kinetics and other subjects. One milestone was the publication in 1876 by Josiah Willard Gibbs of his paper, "On the Equilibrium of Heterogeneous Substances". This paper introduced several of the cornerstones of physical chemistry, such as Gibbs energy, chemical potentials, Gibbs phase rule. Other milestones include the subsequent naming and accreditation of enthalpy to Heike Kamerlingh Onnes and to macromolecular processes. 
The first scientific journal specifically in the field of physical chemistry was the German journal, "Zeitschrift für Physikalische Chemie", founded in 1887 by Wilhelm Ostwald and Jacobus Henricus van 't Hoff. Together with Svante August Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. All three were awarded with the Nobel Prize in Chemistry between 1901-1909.
Developments in the following decades include the application of statistical mechanics to chemical systems and work on colloids and surface chemistry, where Irving Langmuir made many contributions. Another important step was the development of quantum mechanics into quantum chemistry from the 1930s, where Linus Pauling was one of the leading names. Theoretical developments have gone hand in hand with developments in experimental methods, where the use of different forms of spectroscopy, such as infrared spectroscopy, microwave spectroscopy, EPR spectroscopy and NMR spectroscopy, is probably the most important 20th century development.
Further development in physical chemistry may be attributed to discoveries in nuclear chemistry, especially in isotope separation (before and during World War II), more recent discoveries in astrochemistry, as well as the development of calculation algorithms in the field of "additive physicochemical properties" (practically all physicochemical properties, such as boiling point, critical point, surface tension, vapor pressure, etc. - more than 20 in all - can be precisely calculated from chemical structure alone, even if the chemical molecule remains unsynthesized), and in this area is concentrated practical importance of contemporary physical chemistry.
See Group contribution method, Lydersen method, Joback method, Benson group increment theory, QSPR, QSAR
Journals.
Some journals that deal with physical chemistry include:
Historical journals that covered both chemistry and physics include Annales de chimie et de physique (started in 1789, published under the name given here from 1815–1914).

</doc>
<doc id="23636" url="http://en.wikipedia.org/wiki?curid=23636" title="Perimeter">
Perimeter

A perimeter is a path that surrounds a two-dimensional shape. The word comes from the Greek "peri" (around) and "meter" (measure). The term may be used either for the path or its length - it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.
Calculating the perimeter has considerable practical applications. The perimeter can be used to calculate the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.
Formulas.
The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated as any path with formula_1 where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced with other algebraic forms in order to be solved: an advanced notion of perimeter, which includes hypersurfaces bounding volumes in formula_4-dimensional Euclidean spaces can be found in the theory of Caccioppoli sets.
Polygons.
Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.
The perimeter of a polygon equals the sum of the lengths of its edges. In particular, the perimeter of a rectangle which width is formula_5 and length formula_6 is equal to formula_7. 
An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.
A regular polygon may be defined by the number of its sides and by its "radius", that is to say, the constant distance between its centre and each of its vertices. One can calculate the length of its sides using trigonometry. If "R" is a regular polygon's radius and "n" is the number of its sides, then its perimeter is 
A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. A cleaver is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths.
Circumference of a circle.
The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, π (the Greek "p" for perimeter), such that if "P" is the circle's perimeter and "D" its diameter then:
In terms of the radius "r" of the circle, this formula becomes:
To calculate a circle's perimeter, knowledge of its radius or diameter and of the number π is sufficient. The problem is that π is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of π is important for the calculation. The search for the digits of π is relevant to many fields, such as mathematical analysis, algorithmics and computer science.
Perception of perimeter.
The perimeter and the area are the main two measures of geometric figures. Confusing them is frequent, as well as believing that the greater one of them is, the greater is the other. Indeed, an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is 2 times the area of the shape on the map.
Nevertheless there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.
Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. But a field's production is proportional to its area, not to its perimeter: many naive peasants may have got fields with long perimeters but low areas (thus, low crops).
If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, some people may confuse perimeter with convex hull. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. On the animated picture on the left, all the figures have the same convex hull: the big, first hexagon.
Isoperimetry.
The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive: it is the circle. In particular, that is why drops of fat on a broth surface are circular.
This problem may seem simple, but its mathematical proof needs sophisticated theorems. The isoperimetric problem is sometimes simplified: to find the quadrilateral, or the triangle or another particular figure, with the largest area amongst those having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with "n" sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is an irregular polygon.

</doc>
<doc id="23637" url="http://en.wikipedia.org/wiki?curid=23637" title="Phase (matter)">
Phase (matter)

In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform.:86:3 Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air over the water is a third phase. The glass of the jar is another separate phase. (See State of Matter#Glass)
The term "phase" is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term "phase" is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of "phase" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.
Types of phases.
Distinct phases may be described as different states of matter such as gas, liquid, solid, plasma or Bose–Einstein condensate. Useful mesophases between solid and liquid form other states of matter.
Distinct phases may also exist within a given state of matter. As shown in the diagram for iron alloys, several phases exist for both the solid and liquid states. Phases may also be differentiated based on solubility as in polar (hydrophilic) or non-polar (hydrophobic). A mixture of water (a polar liquid) and oil (a non-polar liquid) will spontaneously separate into two phases. Water has a very low solubility (is insoluble) in oil, and oil has a low solubility in water. Solubility is the maximum amount of a solute that can dissolve in a solvent before the solute ceases to dissolve and remains in a separate phase. A mixture can separate into more than two liquid phases and the concept of phase separation extends to solids, i.e., solids can form solid solutions or crystallize into distinct crystal phases. Metal pairs that are mutually soluble can form alloys, whereas metal pairs that are mutually insoluble cannot.
As many as eight immiscible liquid phases have been observed. Mutually immiscible liquid phases are formed from water (aqueous phase), hydrophobic organic solvents, perfluorocarbons (fluorous phase), silicones, several different metals, and also from molten phosphorus. Not all organic solvents are completely miscible, e.g. a mixture of ethylene glycol and toluene may separate into two distinct organic phases.
Phases do not need to macroscopically separate spontaneously. Emulsions and colloids are examples of immiscible phase pair combinations that do not physically separate.
Phase equilibrium.
Left to equilibration, many compositions will form a uniform single phase, but depending on the temperature and pressure even a single substance may separate into two or more distinct phases. Within each phase, the properties are uniform but between the two phases properties differ.
Water in a closed jar with an air space over it forms a two phase system. Most of the water is in the liquid phase, where it is held by the mutual attraction of water molecules. Even at equilibrium molecules are constantly in motion and, once in a while, a molecule in the liquid phase gains enough kinetic energy to break away from the liquid phase and enter the gas phase. Likewise, every once in a while a vapor molecule collides with the liquid surface and condenses into the liquid. At equilibrium, evaporation and condensation processes exactly balance and there is no net change in the volume of either phase.
At room temperature and pressure, the water jar reaches equilibrium when the air over the water has a humidity of about 3%. This percentage increases as the temperature goes up. At 100 °C and atmospheric pressure, equilibrium is not reached until the air is 100% water. If the liquid is heated a little over 100 °C, the transition from liquid to gas will occur not only at the surface, but throughout the liquid volume: the water boils.
Number of phases.
For a given composition, only certain phases are possible at a given temperature and pressure. The number and type of phases that will form is hard to predict and is usually determined by experiment. The results of such experiments can be plotted in phase diagrams.
The phase diagram shown here is for a single component system. In this simple system, which phases that are possible depends only on pressure and temperature. The markings show points where two or more phases can co-exist in equilibrium. At temperatures and pressures away from the markings, there will be only one phase at equilibrium.
In the diagram, the blue line marking the boundary between liquid and gas does not continue indefinitely, but terminates at a point called the critical point. As the temperature and pressure approach the critical point, the properties of the liquid and gas become progressively more similar. At the critical point, the liquid and gas become indistinguishable. Above the critical point, there are no longer separate liquid and gas phases: there is only a generic fluid phase referred to as a supercritical fluid. In water, the critical point occurs at around 647 K (374 °C or 705 °F) and 22.064 MPa.
An unusual feature of the water phase diagram is that the solid–liquid phase line (illustrated by the dotted green line) has a negative slope. For most substances, the slope is positive as exemplified by the dark green line. This unusual feature of water is related to ice having a lower density than liquid water. Increasing the pressure drives the water into the higher density phase, which causes melting.
Another interesting though not unusual feature of the phase diagram is the point where the solid–liquid phase line meets the liquid–gas phase line. The intersection is referred to as the triple point. At the triple point, all three phases can coexist.
Experimentally, the phase lines are relatively easy to map due to the interdependence of temperature and pressure that develops when multiple phases forms. See Gibbs' phase rule. Consider a test apparatus consisting of a closed and well insulated cylinder equipped with a piston. By charging the right amount of water and applying heat, the system can be brought to any point in the gas region of the phase diagram. If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed.
Interfacial phenomena.
Between two phases in equilibrium there is a narrow region where the properties are not that of either phase. Although this region may be very thin, it can have significant and easily observable effects, such as causing a liquid to exhibit surface tension. In mixtures, some components may preferentially move toward the interface. In terms of modeling, describing, or understanding the behavior of a particular system, it may be efficacious to treat the interfacial region as a separate phase.
Crystal phases.
A single material may have several distinct solid states capable of forming separate phases. Water is a well-known example of such a material. For example, water ice is ordinarily found in the hexagonal form ice Ih, but can also exist as the cubic ice Ic, the rhombohedral ice II, and many other forms. Polymorphism is the ability of a solid to exist in more than one crystal form. For pure chemical elements, polymorphism is known as allotropy. For example, diamond, graphite, and fullerenes are different allotropes of carbon.
Phase transitions.
When a substance undergoes a phase transition (changes from one state of matter to another) it usually either takes up or releases energy. For example, when water evaporates, the kinetic energy expended as the evaporating molecules escape the attractive forces of the liquid is reflected in a decrease in temperature. The amount of energy required to induce the transition is more than the amount required to heat the water from room temperature to just short of boiling temperature, which is why evaporation is useful for cooling. See Enthalpy of vaporization. The reverse process, condensation, releases heat. The heat energy, or enthalpy, associated with a solid to liquid transition is the enthalpy of fusion and that associated with a solid to gas transition is the enthalpy of sublimation.

</doc>
<doc id="23638" url="http://en.wikipedia.org/wiki?curid=23638" title="Outline of physical science">
Outline of physical science

The following outline is provided as an overview of and topical guide to physical science.
Physical science is the branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).
What type of thing is physical science?
Physical science can be described as all of the following:
History of physical science.
History of physical science – history of the branch of natural science that studies non-living systems, in contrast to the biological sciences. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).
General principles of the physical sciences.
Basic principles of physics.
Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:
Basic principles of astronomy.
Astronomy – science of celestial bodies and their interactions in space. Its studies includes the following:
Basic principles of chemistry.
Chemistry – branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.
Basic principles of earth science.
Earth science – the science of the planet Earth, as of 2014[ [update]] the only identified life-bearing planet. Its studies include the following:

</doc>
<doc id="23639" url="http://en.wikipedia.org/wiki?curid=23639" title="Gasoline">
Gasoline

Gasoline , also known as petrol outside of North America, is a transparent, petroleum-derived liquid that is used primarily as a fuel in internal combustion engines. It consists mostly of organic compounds obtained by the fractional distillation of petroleum, enhanced with a variety of additives; a 42-gallon barrel of crude oil yields about 19 gallons of gasoline, when processed in an oil refinery.
The characteristic of a particular gasoline blend to resist igniting too early (which causes knocking and reduces efficiency in reciprocating engines) is measured by its octane rating. Gasoline is produced in several grades of octane rating. Tetraethyllead and other lead compounds are no longer used in most areas to regulate and increase octane-rating, but many other additives are put into gasoline to improve its chemical stability, control corrosiveness and provide fuel system 'cleaning,' and determine performance characteristics under intended use. Sometimes, gasoline also contains ethanol as an alternative fuel, for economic or environmental reasons.
Gasoline, as used worldwide in the vast number of internal combustion engines used in transport and industry, has a significant impact on the environment, both in local effects (e.g., smog) and in global effects (e.g., effect on the climate). Gasoline may also enter the environment uncombusted, as liquid and as vapors, from leakage and handling during production, transport and delivery, from storage tanks, from spills, etc. As an example of efforts to control such leakage, many (underground) storage tanks are required to have extensive measures in place to detect and prevent such leaks.
The material safety data sheet for unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts. Benzene and many anti-knocking additives are carcinogenic.
Octane rating.
Spark ignition engines are designed to burn gasoline in a controlled process called deflagration. In some cases, however, the unburned mixture can autoignite by detonating from pressure and heat alone, rather than ignite from the spark plug at exactly the right time, which causes rapid pressure rise which can damage the engine. This phenomenon is often referred to as engine knocking or end-gas knock. One way to reduce knock in spark ignition engines is to increase the gasoline's resistance to autoignition, which is expressed by its octane rating.
Octane rating is measured relative to a mixture of 2,2,4-trimethylpentane (an isomer of octane) and n-heptane. There are different conventions for expressing octane ratings, so the same physical fuel may have several different octane ratings based on the measure used. One of the best known is the research octane number (RON).
The octane rating of typical commercially-available gasoline varies by country. In Finland, Sweden, and Norway, 95 RON is the standard for regular unleaded gasoline and 98 RON is also available as a more expensive option. In the UK, ordinary regular unleaded gasoline is 95 RON (commonly available), premium unleaded gasoline is always 97 RON, and super unleaded is usually 97-98 RON. However, both Shell and BP produce fuel at 102 RON for cars with high-performance engines and in 2006 the supermarket chain Tesco began to sell super unleaded gasoline rated at 99 RON. In the US, octane ratings in unleaded fuels can vary between 85 and 87 AKI (91-92 RON) for regular, through 89-90 AKI (94-95 RON) for mid-grade (equivalent to European regular), up to 90-94 AKI (95-99 RON) for premium (European premium).
South Africa's largest city, Johannesburg, is located on the Highveld at 1753 m above sea level. So the South African AA recommends 95 octane gasoline (petrol) at low altitude and 93 octane for use in Johannesburg because "The higher the altitude the lower the air pressure, and the lower the need for a high octane fuel as there is no real performance gain".
The octane rating became important as the military sought higher output for aircraft engines in the late 1930s and the 1940s. A higher octane rating allows a higher compression ratio or supercharger boost, and thus higher temperatures and pressures, which translate to higher power output. Some scientists even predicted that a nation with a good supply of high octane gasoline would have the advantage in air power. In 1943, the Rolls Royce Merlin aero engine produced 1320 horsepower (984 kW) using 100 RON fuel from a modest 27 liter displacement. Towards the end of the second world war, experiments were conducted using 150 RON fuel (100/150 avgas), obtained by adding 2.5% aniline to 100 octane avgas.
Stability.
Quality gasoline should be stable for six months if stored properly but gasoline will break down slowly over time due to the separation of the components. Gasoline stored for a year will most likely be able to be burned in an internal combustion engine without too much trouble but the effects of long term storage will become more noticeable with each passing month until a time comes when the gasoline should be diluted with ever increasing amounts of freshly made fuel so that the older gasoline may be used up. If left undiluted, improper operation will occur and this may include engine damage from misfiring and/or the lack of proper action of the fuel within a fuel injection system and from an onboard computer attempting to compensate (if applicable to the vehicle). Storage should be in an airtight container (to prevent oxidation or water vapors mixing in with the gas) that can withstand the vapor pressure of the gasoline without venting (to prevent the loss of the more volatile fractions) at a stable cool temperature (to reduce the excess pressure from liquid expansion, and to reduce the rate of any decomposition reactions). When gasoline is not stored correctly, gums and solids may be created, which can corrode system components and accumulate on wetted surfaces, resulting in a condition called "stale fuel". Gasoline containing ethanol is especially subject to absorbing atmospheric moisture, then forming gums, solids, or two phases (a hydrocarbon phase floating on top of a water-alcohol phase).
The presence of these degradation products in the fuel tank, fuel lines plus a carburetor or fuel injection components makes it harder to start the engine or causes reduced engine performance. On resumption of regular engine use, the buildup may or may not eventually cleaned out by the flow of fresh gasoline. The addition of a fuel stabilizer to gasoline can extend the life of fuel that is not or cannot be stored properly though removal of all fuel from a fuel system is the only real solution to the problem of long term storage of an engine or a machine or vehicle. Some typical fuel stabilizers are proprietary mixtures containing mineral spirits, isopropyl alcohol, 1,2,4-trimethylbenzene, or other additives. Fuel stabilizer is commonly used for small engines, such as lawnmower and tractor engines, especially when their use is seasonal (low to no use for one or more seasons of the year). Users have been advised to keep gasoline containers more than half full and properly capped to reduce air exposure, to avoid storage at high temperatures, to run an engine for ten minutes to circulate the stabilizer through all components prior to storage, and to run the engine at intervals to purge stale fuel from the carburetor.
Gasoline stability requirements are set in standard ASTM D4814-14b. The standard describes the various characteristics and requirements of automotive fuels for use over a wide range of operating conditions in ground vehicles equipped with spark-ignition engines.
Energy content.
Energy is obtained from the combustion of gasoline by the conversion of a hydrocarbon to carbon dioxide and water. The combustion of octane follows this reaction:
Gasoline contains about 42.4 MJ/kg (120 MJ/US gal, 33.3 kWh/US gal) quoting the lower heating value . Gasoline blends differ, and therefore actual energy content varies according to the season and producer by up to 4% more or less than the average, according to the US EPA. On average, about 74 L of gasoline (19.5 US gal, 16.3 imp gal) are available from a barrel of crude oil (about 46% by volume), varying due to quality of crude and grade of gasoline. The remainder are products ranging from tar to naphtha.
A high-octane-rated fuel, such as liquefied petroleum gas (LPG) has an overall lower power output at the typical 10:1 compression ratio of a gasoline engine. However, with an engine tuned to the use of LPG (i.e. via higher compression ratios, such as 12:1 instead of 10:1), the power output can be improved. This is because higher-octane fuels allow for a higher compression ratio without knocking, resulting in a higher cylinder temperature, which improves efficiency. Also, increased mechanical efficiency is created by a higher compression ratio through the concomitant higher expansion ratio on the power stroke, which is by far the greater effect. The higher expansion ratio extracts more work from the high-pressure gas created by the combustion process. An Atkinson cycle engine uses the timing of the valve events to produce the benefits of a high expansion ratio without the disadvantages, chiefly detonation, of a high compression ratio. A high expansion ratio is also one of the two key reasons for the efficiency of diesel engines, along with the elimination of pumping losses due to throttling of the intake air flow.
The lower energy content (per liter) of LPG in comparison to gasoline is due mainly to its lower density. Energy content per kilogram is higher than for gasoline (higher hydrogen to carbon ratio, for an example see ).
Molecular weights of the above reagents are C8H18 114, O2 32, CO2 44, H2O 18; therefore 1 kg of fuel reacts with 3.51 kg of oxygen to produce 3.09 kg of carbon dioxide and 1.42 kg of water.
Density.
The density of gasoline ranges from 0.71–0.77 kg/L (719.7 kg/m3 ; 0.026 lb/in3; 6.073 lb/US gal; 7.29 lb/imp gal), higher densities having a greater volume of aromatics. Since gasoline floats on water, water cannot generally be used to extinguish a gasoline fire unless used in a fine mist.
Finished marketable gasoline is traded with a standard reference of 0.755 kg/L, and its price is escalated/de-escalated according to its actual density.
Chemical analysis and production.
Gasoline is produced in oil refineries. Roughly 19 US gallons of gasoline is derived from a 42-gallon barrel of crude oil. Material separated from crude oil via distillation, called virgin or straight-run gasoline, does not meet specifications for modern engines (particularly the octane rating, see below), but can be pooled to the gasoline blend.
The bulk of a typical gasoline consists of hydrocarbons with between 4 and 12 carbon atoms per molecule (commonly referred to as C4-C12). It is a mixture of paraffins (alkanes), cycloalkanes (naphthenes), and olefins (alkenes), where the usage of the terms paraffin and olefin is particular to the oil industry. The actual ratio depends on:
The various refinery streams blended to make gasoline have different characteristics. Some important streams are:
The terms above are the jargon used in the oil industry and terminology varies.
Currently, many countries set limits on gasoline aromatics in general, benzene in particular, and olefin (alkene) content. Such regulations led to increasing preference for high octane pure paraffin (alkane) components, such as alkylate, and is forcing refineries to add processing units to reduce benzene content. In the EU the benzene limit is set at 1% volume for all grade of automotive gasoline.
Gasoline can also contain other organic compounds, such as organic ethers (deliberately added), plus small levels of contaminants, in particular organosulfur compounds, but these are usually removed at the refinery.
Additives.
Inhaled (huffed) gasoline vapor is a common intoxicant that has become epidemic in some poorer communities and indigenous groups in Australia, Canada, New Zealand, and some Pacific Islands. In response, Opal fuel has been developed by the BP Kwinana Refinery in Australia, and contains only 5% aromatics, which weakens the effects of inhalation.
Antiknock additives.
Almost all countries in the world have phased out automotive leaded fuel. In 2011 six countries in the world were still using leaded gasoline: Afghanistan, Myanmar, North Korea, Algeria, Iraq and Yemen. It was expected that by the end of 2013 those countries would ban leaded gasoline, but actually it will take longer. Algeria will replace leaded with unleaded automotive fuel only in 2015. 
Different additives have replaced the lead compounds. The most popular additives include aromatic hydrocarbons, ethers and alcohol (usually ethanol or methanol).
For technical reasons the use of leaded additives is still permitted world-wide for the formulation of some grades of aviation gasoline such as 100LL, because the required octane rating would be technically infeasible to reach without the use of leaded additives.
Tetraethyllead.
Gasoline, when used in high-compression internal combustion engines, tends to autoignite ("detonate") causing damaging "engine knocking" (also called "pinging" or "pinking") noise. To address this problem, tetraethyllead (TEL) was widely adopted as an additive for gasoline in the 1920s. With the discovery of the extent of environmental and health damage caused by the lead, however, and the incompatibility of lead with catalytic converters, leaded gasoline was phased out beginning in 1973. By 1995, leaded fuel accounted for only 0.6% of total gasoline sales and less than 2000 short tons (1814 t) of lead per year in the USA. From 1 January 1996, the U.S. Clean Air Act banned the sale of leaded fuel for use in on-road vehicles in the USA. The use of TEL also necessitated other additives, such as dibromoethane.
First European countries started replacing lead by the end of the 1980s and by the end of the 1990s leaded gasoline was banned within the entire European Union.
MMT.
Methylcyclopentadienyl manganese tricarbonyl (MMT) is used in Canada and in Australia to boost octane. It also helps old cars designed for leaded fuel run on unleaded fuel without need for additives to prevent valve problems. Its use in the US has been restricted by regulations.
Fuel stabilizers (antioxidants and metal deactivators).
Gummy, sticky resin deposits result from oxidative degradation of gasoline upon long term storage. These harmful deposits arise from the oxidation of alkenes and other minor components in gasoline (see drying oils). Improvements in refinery techniques have generally reduced the susceptibility of gasolines to these problems. Previously, catalytically or thermally cracked gasolines are most susceptible to oxidation. The formation of these gums is accelerated by copper salts, which can be neutralized by additives called metal deactivators.
This degradation can be prevented through the addition of 5–100 ppm of antioxidants, such as phenylenediamines and other amines. Hydrocarbons with a bromine number of 10 or above can be protected with the combination of unhindered or partially hindered phenols and oil soluble strong amine bases, such as hindered phenols. "Stale" gasoline can be detected by a colorimetric enzymatic test for organic peroxides produced by oxidation of the gasoline.
Gasolines are also treated with metal deactivators, which are compounds that sequester (deactivate) metal salts that otherwise accelerate the formation of gummy residues. The metal impurities might arise from the engine itself or as contaminants in the fuel.
Detergents.
Gasoline, as delivered at the pump, also contains additives to reduce internal engine carbon buildups, improve combustion, and to allow easier starting in cold climates. High levels of detergent can be found in Top Tier Detergent Gasolines. The specification for Top Tier Detergent gasolines was developed by four automakers: GM, Honda, Toyota and BMW. According to the bulletin, the minimal EPA requirement is not sufficient to keep engines clean. Typical detergents include alkylamines and alkyl phosphates at the level of 50-100 ppm.
Ethanol.
European Union.
In the EU, 5% ethanol can be added within the common gasoline spec (EN 228). Discussions are ongoing to allow 10% blending of ethanol (available in Finnish, French and German gas stations). In Finland most gasoline stations sell 95E10, which is 10% of ethanol; and 98E5, which is 5% ethanol. Most gasoline sold in Sweden has 5-15% ethanol added.
Brazil.
In Brazil, the Brazilian National Agency of Petroleum, Natural Gas and Biofuels (ANP) requires gasoline for automobile use to have from 18 to 25% of ethanol added to its composition. However, grades with higher ethanol content are available.
Australia.
Legislation requires retailers to label fuels containing ethanol on the dispenser, and limits ethanol use to 10% of gasoline in Australia. Such gasoline is commonly called E10 by major brands, and it is cheaper than regular unleaded gasoline.
United States of America.
The federal Renewable Fuel Standard (RFS) effectively requires refiners and blenders to blend renewable biofuels (mostly ethanol) with gasoline, sufficient to meet a growing annual target of total gallons blended. Although the mandate does not require a specific percentage of ethanol, annual increases in the target combined with declining gasoline consumption has caused the typical ethanol content in gasoline to approach 10%. Most fuel pumps display a sticker that states that the fuel may contain up to 10% ethanol, an intentional disparity that reflects the varying actual percentage. Until late 2010, fuels retailers were only authorized to sell fuel containing up to 10 percent ethanol (E10), and most vehicle warranties (except for flexible fuel vehicles) authorize fuels that contain no more than 10 percent ethanol. In parts of the United States, ethanol is sometimes added to gasoline without an indication that it is a component.
India.
The Government of India in October 2007 decided to make 5% ethanol blending (with gasoline) mandatory. Currently, 10% Ethanol blended product (E10) is being sold in various parts of the country.
Dye.
In Australia, the lowest grade of gasoline (RON 91) is dyed a light shade of red/orange and the medium grade (RON 95) is dyed yellow.
In the United States, aviation gasoline (avgas) is dyed to identify its octane rating and to distinguish it from kerosene-based jet fuel, which is clear.
In Canada the gasoline for marine and farm use is dyed red and is not subject to road tax .
Oxygenate blending.
Oxygenate blending adds oxygen-bearing compounds such as MTBE, ETBE and ethanol. The presence of these oxygenates reduces the amount of carbon monoxide and unburned fuel in the exhaust gas. In many areas throughout the US, oxygenate blending is mandated by EPA regulations to reduce smog and other airborne pollutants. For example, in Southern California, fuel must contain 2% oxygen by weight, resulting in a mixture of 5.6% ethanol in gasoline. The resulting fuel is often known as reformulated gasoline (RFG) or oxygenated gasoline, or in the case of California, California reformulated gasoline. The federal requirement that RFG contain oxygen was dropped on 6 May 2006 because the industry had developed VOC-controlled RFG that did not need additional oxygen.
MTBE was phased out in the US due to ground water contamination and the resulting regulations and lawsuits. Ethanol and, to a lesser extent, the ethanol-derived ETBE are common replacements. A common ethanol-gasoline mix of 10% ethanol mixed with gasoline is called gasohol or E10, and an ethanol-gasoline mix of 85% ethanol mixed with gasoline is called E85. The most extensive use of ethanol takes place in Brazil, where the ethanol is derived from sugarcane. In 2004, over 3.4 billion US gallons (2.8 billion imp gal/13 million m³) of ethanol was produced in the United States for fuel use, mostly from corn, and E85 is slowly becoming available in much of the United States, though many of the relatively few stations vending E85 are not open to the general public. The use of bioethanol, either directly or indirectly by conversion of such ethanol to bio-ETBE, is encouraged by the European Union Directive on the Promotion of the use of biofuels and other renewable fuels for transport. Since producing bioethanol from fermented sugars and starches involves distillation, though, ordinary people in much of Europe cannot legally ferment and distill their own bioethanol at present (unlike in the US, where getting a BATF distillation permit has been easy since the 1973 oil crisis).
Safety.
Environmental considerations.
Combustion of 1 USgal of gasoline produces 8788 g of carbon dioxide (2.3 kg/l), a greenhouse gas.
The main concern with gasoline on the environment, aside from the complications of its extraction and refining, is the potential effect on the climate. Unburnt gasoline and evaporation from the tank, when in the atmosphere, react in sunlight to produce photochemical smog. Vapor pressure initially rises with some addition of ethanol to gasoline, but the increase is greatest at 10% by volume. At higher concentrations of ethanol above 10%, the vapor pressure of the blend starts to decrease. At a 10% ethanol by volume, the rise in vapor pressure may potentially increase the problem of photochemical smog. This rise in vapor pressure could be mitigated by increasing the percentage of ethanol in the gasoline mixture.
The chief risks of such leaks come not from vehicles, but from gasoline delivery truck accidents and leaks from storage tanks. Because of this risk, most (underground) storage tanks now have extensive measures in place to detect and prevent any such leaks, such as monitoring systems (Veeder-Root, Franklin Fueling).
Toxicity.
The material safety data sheet for unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts, including benzene (up to 5% by volume), toluene (up to 35% by volume), naphthalene (up to 1% by volume), trimethylbenzene (up to 7% by volume), methyl "tert"-butyl ether (MTBE) (up to 18% by volume, in some states) and about ten others. Hydrocarbons in gasoline generally exhibit low acute toxicities, with LD50 of 700 – 2700 mg/kg for simple aromatic compounds. Benzene and many antiknocking additives are carcinogenic.
Inhalation.
Huffed gasoline is a common intoxicant that has become epidemic in some poorer communities and indigenous groups in Australia, New Zealand, and some Pacific Islands. In response, Opal fuel has been developed by the BP Kwinana Refinery in Australia, and contains only 5% aromatics (unlike the usual 25%), which weakens the effects of inhalation.
Flammability.
Like other hydrocarbons, gasoline burns in a limited range of its vapor phase and, coupled with its volatility, this makes leaks highly dangerous when sources of ignition are present. Gasoline has a lower explosion limit of 1.4% by volume and an upper explosion limit of 7.6%. If the concentration is below 1.4%, the air-gasoline mixture is too lean and does not ignite. If the concentration is above 7.6%, the mixture is too rich and also does not ignite. However, gasoline vapor rapidly mixes and spreads with air, making unconstrained gasoline quickly flammable.
Use and pricing.
The United States accounts for about 44% of the world’s gasoline consumption. In 2003 The US consumed 476 GL, which equates to 1.3 GL of gasoline each day. The US used about 510 GL of gasoline in 2006, of which 5.6% was mid-grade and 9.5% was premium grade.
Europe.
Unlike the US, countries in Europe impose substantial taxes on fuels such as gasoline. The price of gasoline in Europe is typically more than twice that in the US. 
In Italy, due to the amendments imposed by Monti's Government in December 2011, the price of gasoline has passed, in the period of two weeks, from 1.50 €/l (7.48 US$/gal) to 1.75 €/l (8.72 US$/gal); on March 17, a gasoline station located near Ancona has reached the psychological threshold of 2 €/l: the price was €2.001/l (9.97 US$/gal). This chart must be compared to the USA national average price of gasoline of 0.71 €/l.
United States.
From 1998 to 2004, the price of gasoline fluctuated between $1 and $2 USD per U.S. gallon. After 2004, the price increased until the average gas price reached a high of $4.11 per U.S. gallon in mid-2008, but receded to approximately $2.60 per U.S. gallon by September 2009. More recently, the U.S. experienced an upswing in gas prices through 2011, and by 1 March 2012, the national average was $3.74 per gallon.
In the United States, most consumer goods bear pre-tax prices, but gasoline prices are posted with taxes included. Taxes are added by federal, state, and local governments. As of 2009, the federal tax is 18.4¢ per gallon for gasoline and 24.4¢ per gallon for diesel (excluding red diesel). Among states, the highest gasoline tax rates, including the federal taxes as of 2005, are New York (62.9¢/gal), Hawaii (60.1¢/gal), and California (60¢/gal). However, many states' taxes are a percentage and thus vary in amount depending on the cost of the gasoline.
About 9% of all gasoline sold in the US in May 2009 was premium grade, according to the Energy Information Administration. "Consumer Reports" magazine says, “If [your owner’s manual] says to use regular fuel, do so—there’s no advantage to a higher grade.” The Associated Press said premium gas—which is a higher octane and costs more per gallon than regular unleaded—should be used only if the manufacturer says it is “required”. Cars with turbocharged engines and high compression ratios often specify premium gas because higher octane fuels reduce the incidence of "knock", or fuel pre-detonation.
History.
The first automotive combustion engines, so-called Otto engines, were developed in the last quarter of the 19th century in Germany. The fuel was a relatively volatile hydrocarbon obtained from coal gas. With a boiling point near 85 °C (octanes boil about 40 °C higher), it was well suited for early carburetors (evaporators). The development of a "spray nozzle" carburetor enabled the use of less volatile fuels. Further improvements in engine efficiency were attempted at higher compression ratios, but early attempts were blocked by knocking (premature explosion of fuel). In the 1920s, antiknock compounds were introduced by Migley and Boyd, specifically tetraethyllead (TEL). This innovation started a cycle of improvements in fuel efficiency that coincided with the large-scale development of oil refining to provide more products in the boiling range of gasoline. In the 1950s oil refineries started to focus on high octane fuels, and then detergents were added to gasoline to clean the jets and carburetors. The 1970s witnessed greater attention to the environmental consequences of burning gasoline. These considerations led to the phasing out of TEL and its replacement by other antiknock compounds. Subsequently, low-sulfur gasoline was introduced, in part to preserve the catalysts in modern exhaust systems.
Etymology and terminology.
"Gasoline" is cited (under the spelling "gasolene") from 1863 in the "Oxford English Dictionary". It was never a trademark, although it may have been derived from older trademarks such as "Cazeline" and "Gazeline".
Variant spellings of "gasoline" have been used to refer to raw petroleum since the 16th century. "Petrol" was first used as the name of a refined petroleum product around 1870 by British wholesaler Carless, Capel & Leonard, who marketed it as a solvent. When the product later found a new use as a motor fuel, Frederick Simms, an associate of Gottlieb Daimler, suggested to Carless that they register the trade mark "petrol", but by this time the word was already in general use, possibly inspired by the French "pétrole", and the registration was not allowed. Carless registered a number of alternative names for the product, while their competitors used the term "motor spirit" until the 1930s.
In many countries, gasoline has a colloquial name derived from that of the chemical benzene ("e.g.", German "Benzin", Dutch "benzine", Italian "benzina", Polish "benzyna", Chilean Spanish "bencina", Thai เบนซิน "bayn sin ", Greek βενζίνη "venzini", Romanian "benzină", Swedish "bensin", Arabic بنزين "binzīn"). Argentina, Uruguay, Paraguay and Italy use the colloquial name "nafta" derived from that of the chemical naphtha.
The terms "mogas", short for motor gasoline, or "autogas", short for automobile gasoline, are used to distinguish automobile fuel from aviation fuel, or "avgas".
Comparison with other fuels.
Volumetric and mass energy density of some fuels compared with gasoline (in the rows with gross and net, they are from):
References.
Bibliography.
</dl>
External links.
Images

</doc>
<doc id="23640" url="http://en.wikipedia.org/wiki?curid=23640" title="Pentose">
Pentose

A pentose is a monosaccharide with five carbon atoms. Pentoses are organized into two groups. Aldopentoses have an aldehyde functional group at position 1. Ketopentoses have a ketone functional group in position 2 or 3.
Aldopentoses.
The aldopentoses have three chiral centers and therefore eight different (2^3) stereoisomers are possible.
Ketopentoses.
The 2-ketopentoses have two chiral centers, and therefore four different stereoisomers are possible (2^2). The 3-ketopentoses are rare.
Properties.
The aldehyde and ketone functional groups in these carbohydrates react with neighbouring hydroxyl functional groups to form intramolecular hemiacetals and hemiketals, respectively. The resulting ring structure is related to furan, and is termed a furanose. The ring spontaneously opens and closes, allowing rotation to occur about the bond between the carbonyl group and the neighbouring carbon atom — yielding two distinct configurations (α and β). This process is termed mutarotation.
Ribose is a constituent of RNA, and the related deoxyribose of DNA.
A polymer composed of pentose sugars is called a pentosan.
Tollens’ test for pentoses.
The Tollens’ test for pentoses relies on reaction of the furfural with phloroglucinol to produce a colored compound with high molar absorptivity.

</doc>
