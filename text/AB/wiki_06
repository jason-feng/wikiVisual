<doc id="14142" url="http://en.wikipedia.org/wiki?curid=14142" title="Harley-Davidson">
Harley-Davidson

Harley-Davidson Inc (NYSE: [ HOG], formerly HDI), often abbreviated H-D or Harley, is an American motorcycle manufacturer. Founded in Milwaukee, Wisconsin during the first decade of the 20th century, it was one of two major American motorcycle manufacturers (Indian being the other) to survive the Great Depression. Harley-Davidson also survived a period of poor quality control and competition from Japanese manufacturers.
From 1977 to 2014 the only motorcycles sold to the public under the Harley-Davidson brand have been heavyweight motorcycles designed for highway cruising, with engine displacements greater than 700 cc. Harley-Davidson motorcycles, or "Harleys", are noted for the tradition of heavy customization that gave rise to the chopper style of motorcycle. Except for the modern VRSC and Street model families, current Harley-Davidson motorcycles reflect the styles of classic Harley designs. While Harley-Davidson's attempts to establish itself in the light motorcycle market have met little success and have largely been abandoned since the 1978 sale of its Italian Aermacchi subsidiary, the company re-entered the middleweight market in 2015 with its Street series of motorcycles.
Harley-Davidson sustains a large brand community which keeps active through clubs, events and a museum. Licensing of the Harley-Davidson brand and logo accounted for $40 million (0.8%) of the company's net revenue in 2010.
History.
Beginning.
In 1901, 20 year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cuin and four-inch (102 mm) flywheels. The engine was designed for use in a regular pedal-bicycle frame. Over the next two years, Harley and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend, Henry Melk. It was finished in 1903 with the help of Arthur's brother, Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance. They quickly wrote off their first motor-bicycle as a valuable learning experiment.
Work immediately began on a new and improved second-generation machine. This first "real" Harley-Davidson motorcycle had a bigger engine of 24.74 cuin with 9.75 in flywheels weighing 28 lb. The machine's advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle (designed by Joseph Merkel, later of Flying Merkel fame). The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. The boys also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.
The prototype of the new loop-frame Harley-Davidson was assembled in a 10 x shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was then toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. It was ridden by Edward Hildebrand and placed fourth. This is the first documented appearance of a Harley-Davidson motorcycle in the historical record.
In January 1905, small advertisements were placed in the "Automobile and Cycle Trade Journal" offering bare Harley-Davidson engines to the do-it-yourself trade. By April, complete motorcycles were in production on a very limited basis. That year, the first Harley-Davidson dealer, Carl H. Lang of Chicago, sold three bikes from the five built in the Davidson backyard shed. Years later the original shed was taken to the Juneau Avenue factory where it would stand for many decades as a tribute to the Motor Company's humble origins until it was accidentally destroyed by contractors cleaning the factory yard in the early 1970s.
In 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a 40 x single-story wooden structure. The company produced about 50 motorcycles that year.
In 1907, William S. Harley graduated from the University of Wisconsin–Madison with a degree in mechanical engineering. That year additional factory expansion came with a second floor and later with facings and additions of Milwaukee pale yellow ("cream") brick. With the new facilities production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since.
Production in 1905 and 1906 were all single-cylinder models with 26.84 cuin engines. In February 1907 a prototype model with a 45-degree V-Twin engine was displayed at the Chicago Automobile Show. Although shown and advertised, very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cuin and produced about 7 hp. This gave about double the power of the first singles. Top speed was about 60 mph. Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.
By 1911, some 150 makes of motorcycles had already been built in the United States – although just a handful would survive the 1910s.
In 1911, an improved V-Twin model was introduced. The new engine had mechanically operated intake valves, as opposed to the "automatic" intake valves used on earlier V-Twins that opened by engine vacuum. With a displacement of 49.48 cuin, the 1911 V-Twin was smaller than earlier twins, but gave better performance. After 1913 the majority of bikes produced by Harley-Davidson would be V-Twin models.
In 1912, Harley-Davidson introduced their patented "Ful-Floteing Seat", which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight. More than 3 in of travel was available. Harley-Davidson would use seats of this type until 1958.
By 1913, the yellow brick factory had been demolished and on the site a new 5-story structure had been built. Begun in 1910, the factory with its many additions would take up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and would dominate motorcycle racing after 1914. Production that year swelled to 16,284 machines.
World War I.
In 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time the motorcycle had been adopted for combat service. The U.S. military purchased over 15,000 motorcycles from Harley Davidson during World War I.
Bicycles.
Harley-Davidson launched a line of bicycles in 1917 in hopes of recruiting customers for its motorcycles. Besides the traditional diamond frame men's bicycle, models included a step-through frame 3-18 "Ladies Standard" and a 5-17 "Boy Scout" for youth. The effort was discontinued in 1923 because of disappointing sales.
The bicycles were built for Harley-Davidson in Dayton, Ohio, by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.
1920s.
By 1920, Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced, and dealers in 67 countries.
In 1921, a Harley-Davidson, ridden by Otto Walker, was the first motorcycle ever to win a race at an average speed greater than 100 mph.
During the 1920s, several improvements were put in place, such as a new 74 cubic inch (1,212.6  cc) V-Twin, introduced in 1922, and the "Teardrop" gas tank in 1925. A front brake was added in 1928 although notably only on the J/JD models.
In the late summer of 1929, Harley-Davidson introduced its 45 cuin flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the "D" model, produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to this model as the "three cylinder Harley" because the generator was upright and parallel to the front cylinder. The 2.745 in bore and 3.8125 in stroke would continue in most versions of the 750 engine; exceptions include the XA and the XR-750.
Great Depression.
The Great Depression began a few months after the introduction of their 45 cubic inch model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.
In order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.
In the mid-1930s, Alfred Rich Child opened a production line in Japan with the 74 cuin VL. The Japanese license-holder, Sankyo Seiyako Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.
An 80 cuin flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.
In 1936, the 61E and 61EL models with the "Knucklehead" OHV engines was introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.
By 1937, all Harley-Davidson's flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the "Knucklehead" OHV engine. The revised 74 cuin V and VL models were renamed U and UL, the 80 cuin VH and VLH to be renamed UH and ULH, and the 45 cuin R to be renamed W.
In 1941, the 74 cuin "Knucklehead" was introduced as the F and the FL. The 80 cuin flathead UH and ULH models were discontinued after 1941, while the 74" U & UL flathead models were produced up to 1948.
World War II.
One of only two American cycle manufacturers to survive the Great Depression, Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.
Harley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its 45 cuin WL line, called the WLA. The A in this case stood for "Army". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. More than 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy ‘E’ Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.
Shipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1950 to 1952 for use in the Korean War.
The U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley largely copied the BMW engine and drive train and produced the shaft-driven 750 cc 1942 Harley-Davidson XA. This shared no dimensions, no parts and no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100 °F (56 °C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general purpose vehicle, and the WLA—already in production—was sufficient for its limited police, escort, and courier roles. Only 1,000 were made and the XA never went into full production. It remains the only shaft-driven Harley-Davidson ever made.
Small Harleys: Hummers and Aermacchis.
As part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as "Hummers" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.
In 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250 cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350 cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.
After the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.
Harley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125 cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250 cc two-stroke SS-250 replaced the four-stroke 350 cc Sprint in 1974.
Harley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva.
Overseas.
Prior to WWII, Harley-Davidson's were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) starting in 1929 under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.
Tarnished reputation.
In 1952, following their application to the US Tariff Commission for a 40% tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.
In 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and a lower quality of bikes. The bikes were expensive and inferior in performance, handling, and quality to Japanese motorcycles. Sales and quality declined, and the company almost went bankrupt. The "Harley-Davidson" name was mocked as "Hardly Ableson", "Hardly Driveable," and "Hogly Ferguson",
and the nickname "Hog" became pejorative.
In 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley with Confederate-specific paint and details.
Restructuring and revival.
In 1981, AMF sold the company to a group of thirteen investors led by Vaughn Beals and Willie G. Davidson for $80 million. Inventory was strictly controlled using the just-in-time system.
In the early eighties, Harley-Davidson claimed that Japanese manufacturers were importing motorcycles into the US in such volume as to harm or threaten to harm domestic producers. After an investigation by the US International Trade Commission, President Reagan imposed in 1983 a 45% tariff on imported bikes with engine capacities greater than 700 cc. Harley-Davidson subsequently rejected offers of assistance from Japanese motorcycle makers.<ref name="7/83 US IMPOSES 45% TARIFF ON IMPORTED MOTORCYCLES"> – 7/83 US Imposes 45% Tariff on Imported Motorcycles</ref> However, the company did offer to drop the request for the tariff in exchange for loan guarantees from the Japanese.
Rather than trying to match the Japanese, the new management deliberately exploited the "retro" appeal of the machines, building motorcycles that deliberately adopted the look and feel of their earlier machines and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.
Harley-Davidson bought the "Sub Shock" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.
In response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.
The "Sturgis" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.
By 1990, with the introduction of the "Fat Boy", Harley once again became the sales leader in the heavyweight (over 750 cc) market. At the time of the Fat Boy model introduction, a story rapidly spread that its silver paint job and other features were inspired by the B-29; and Fat Boy was a combination of the names of the atomic bombs Fat Man and Little Boy. However, the Urban Legend Reference Pages lists this story as an urban legend.
1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1995. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR2, FXR3 & FXR4).
In 2000, Ford Motor Company added a Harley-Davidson trim level to the F-150, which was produced until 2003. In 2004, Ford introduced a Harley-Davidson trim level to the Super Duty, which was produced until the trim was discontinued for the 2011 model year. Production of the F-150 Harley-Davidson resumed in 2006 and continued until 2012, when Ford discontinued the F-150 Harley-Davidson edition for the 2013 model year due to low sales, accounting for only 1-2% of the F-150's total sales.
Construction started on the $75 million, 130,000 square-foot (12,000 m2) Harley-Davidson Museum in the Menomonee Valley on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, café and meeting space.
Buell Motorcycle Company.
Harley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought forty-nine percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.
In an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production.
On October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately. The stated reason was to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.
First overseas factory in Brazil.
In 1998 the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.
Claims of stock price manipulation.
During its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42 million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.
Problems with Police Touring models.
Starting around 2000, several police departments started reporting problems with high speed instability on the Harley-Davidson Touring motorcycles. A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006. The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.
2007 strike.
On February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.
The day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.
Harley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.
MV Agusta Group.
On July 11, 2008 Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for $109M USD (€70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.
On October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta. Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni, ending the transaction in the first week of August 2010. Castiglioni is the company's former owner and had been MV Agusta's chairman since Harley-Davidson bought it in 2008.
Operations in India.
In August 2009, Harley-Davidson announced plans to enter the market in India, and started selling motorcycles there in 2010. The company established a subsidiary, Harley-Davidson India, in Gurgaon, near Delhi, in 2011, and created an Indian dealer network.
Financial crisis.
According to Interbrand, the value of the Harley-Davidson brand fell by 43% to $4.34 billion in 2009. The fall in value is believed to be connected to the 66% drop in the company profits in two quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54 million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25% of its total workforce (around 3,500 employees). The company announced on September 14, 2010 that it would remain in Wisconsin.
Motorcycle engines.
The classic Harley-Davidson engines are V-twin engines, with a 45° angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.
This 45° angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy "potato-potato" sound so strongly linked to the Harley-Davidson brand.
To simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively "wasting a spark". The exhaust note is basically a throaty growling sound with some popping.
The 45° design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315° later, then there is a 405° gap until the first cylinder fires again, giving the engine its unique sound.
Harley-Davidson has used various ignition systems throughout its history – be it the early points and condenser system, (Big Twin up to 1978 and Sportsters up to 1978), magneto ignition system used on some 1958 to 1969 Sportsters, early electronic with centrifugal mechanical advance weights, (all models 1978 and a half to 1979), or the late electronic with transistorized ignition control module, more familiarly known as the black box or the brain, (all models 1980 to present).
Starting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.
In 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the "Harley Sound." This research resulted in the bikes that were introduced in compliance with EU standards for 1998.
On February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: "The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to federally register its trademark.
Revolution engine.
The Revolution engine is based on the VR-1000 Superbike race program, co-developed by Harley-Davidson's Powertrain Engineering team and Porsche Engineering in Stuttgart, Germany. It is a liquid cooled, dual overhead cam, internally counterbalanced 60 degree V-twin engine with a displacement of 69 cubic inch (1,130 cc), producing 115 hp at 8,250 rpm at the crank, with a redline of 9,000 rpm.
It was introduced for the new V-Rod line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model.
A 1,250 cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250 cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims 123 hp at the crank for the 2008 VRSCAW model. The VRXSE "Destroyer" is equipped with a stroker (75 mm crank) Screamin' Eagle 79 cubic inch (1,300 cc) Revolution Engine, producing more than 165 hp.
750cc and 500cc versions of the Revolution engine will be used in Harley Davidson's new Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.
Single-cylinder engines.
The first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.
Single-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.
Model families.
Modern Harley-branded motorcycles fall into one of six model families: Touring, Softail, Dyna, Sportster, Vrod and Street. These model families are distinguished by the frame, engine, suspension, and other characteristics.
Touring.
Touring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, "e.g.", FLHR (Road King) and FLTR (Road Glide).
The touring family, also known as "dressers" or "baggers", includes Road King, Road Glide, Street Glide and Electra Glide models offered in various trims. The Road Kings have a "retro cruiser" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the "Batwing" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the "Sharknose". The Sharknose includes a unique, dual front headlight.
Touring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.
The frame was modified for the 1994 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the 96 cuin Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.
In 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.
In 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the 6 usgal fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.
For the 2009 model year, Harley-Davidson has redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, 17 in front wheels for all but the FLHRC Road King Classic, and a 2–1–2 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.
Also released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103 cuin engine exclusive to the trike.
In 2014, Harley-Davidsdon released a redesign for specific touring bikes and called it "Project Rushmore". Changes include a new High Output 103CI engine, one handed easy open saddlebags and compartments, a new Boom Box Infotainment system with either 4.3" or 6.5" screens featuring touchscreen functionality [6.5" models only], Bluetooth (media and phone), available GPS and SiriusXM, Text-to-Speech functionality and USB connectivity with charging. Other features include ABS with Reflex™ linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.
Softail.
These big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the "hardtail" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with "Springer" front ends and "Heritage" styling that incorporate design cues from throughout their history.
Softail models utilize the big-twin engine (F) and the Softail chassis (ST).
Dyna.
Dyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.
Prior to 2008, Dyna models typically featured a narrow, XL-style front fork and front wheel, which the manufacturer included the letter "X" in the model designation to indicate. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which maintained the thickness of the XL forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2–1–2 exhaust, twin headlamps, a 180 mm rear tire, and, for the first time in the Dyna lineup, a 130 mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation: with detachable painted hard saddlebags, touring windshield, larger fork tubes, headlight nacelle and a wide front tire with full fender. The new front end resembled the big-twin FL models from 1968-1971.
The Dyna family used the 88 cuin twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cuin as the factory increased the stroke to 4.375 in. For the 2012 model year, the manufacturer began to offer Dyna models with the 103 cuin upgrade. All Dyna models use a rubber-mounted engine to isolate engine vibration.
Dyna models utilize the big-twin engine (F), small-diameter telescopic forks similar to those used on the Sportster (X) until 2012 when one model used the large diameter forks similar in appearance to those used on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to the present, all Dyna models have designations that begin with FXD, "e.g.", FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).
Sportster.
Introduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883 cc or 1,200 cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.
Up until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.
In the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.
In the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce 91 bhp, four-piston dual front disc brakes, and an aluminum swing arm. "Motorcyclist" featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their "First Ride" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.
One possible reason for the delayed availability in the United States was the fact that Harley-Davidson had to obtain the "XR1200" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.
Except for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100 cc to 1,200 cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883 cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.
VRSC.
Introduced in 2001, the VRSC family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming power cruiser segment, the "V-Rod" makes use of an engine developed jointly with Porsche that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.
In 2008, Harley added the anti-lock braking system as a factory installed option on all VRSC models. Harley also increased the displacement of the stock engine from 1130 to, which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.
VRSC models include:
VRSCA: V-Rod (2002–2006), VRSCAW: V-Rod (2007–2010), VRSCB: V-Rod (2004–2005), VRSCD: Night Rod (2006–2008), VRSCDX: Night Rod Special (2007–2014), VRSCSE: Screamin' Eagle CVO V-Rod (2005), VRSCSE2: Screamin' Eagle CVO V-Rod (2006), VRSCR: Street Rod (2006–2007), VRSCX: Screamin' Eagle Tribute V-Rod (2007), VRSCF: V-Rod Muscle (2009–2014).
VRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240 mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII(CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, "e.g.", VRSCDX denotes the Night Rod Special.
VRXSE.
The VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300 cc "stroked" incarnation, featuring a 75 mm crankshaft, 105 mm Pistons, and 58 mm throttle bodies.
The V-Rod Destroyer is not a street legal motorcycle. As such, it uses "X" instead of "SC" to denote a non-street bike. "SE" denotes a CVO Special Edition.
Street.
The Street, Harley Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on 5 February 2014. The Street 750 weighs 218 kg and has a ground clearance of 144 mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.
The Street 750 uses an all-new, liquid-cooled, 60° V-twin engine called the Revolution X. In the Street 750, the engine displaces 749 cc and produces 65 Nm at 4,000 rpm. A six speed transmission is used.
The Street 750 and the smaller-displacement Street 500 will be available in late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.
Custom Vehicle Operations.
Custom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.
Environmental record.
The Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an "environmental warranty." The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the . This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.
Paul Gotthold, Director of Operations for the EPA, congratulated the motor company:
Harley-Davidson has taken their environmental responsibilities very seriously and has already made substantial progress in the investigation and cleanup of past contamination. Proof of Harley's efforts can be found in the recent EPA determination that designates the Harley property as 'under control' for cleanup purposes. This determination means that there are no serious contamination problems at the facility. Under the new One Cleanup Program, Harley, EPA, and PADEP will expedite the completion of the property investigation and reach a final solution that will permanently protect human health and the environment.
Harley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth "protection to the purchaser (Harley-Davidson) against environmental risks."
Brand culture.
According to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. Now, only 15% of Harley buyers are under 35, and as of 2005, the median age had risen to 46.7.
The income of the average Harley-Davidson rider has risen, as well. In 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.
Harley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5% of the company's net revenue ($41 million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.
Harley-Davidson motorcycles has long been associated with the sub-cultures of the biker, motorcycle clubs, and Outlaw motorcycle clubs.
Origin of "Hog" nickname.
Beginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the "hog boys," consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product taking advantage of the long-standing nickname by turning "hog" into the acronym HOG., for Harley Owners Group. Harley-Davidson attempted to trademark "hog", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, NY, in 1999 when the appellate panel ruled that "hog" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.
On August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.
Bobbers.
Harley-Davidson FL "big twins" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called "bobbers" or sometimes "choppers" because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of outlaw bike gangs like the Hells Angels, referred to stock FLs as "garbage wagons".
Harley Owners Group.
Harley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,
and other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.
HOG members typically spend 30% more than other Harley owners, on such items as clothing and Harley-Davidson-sponsored events.
In 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.
Today, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.
HOG benefits include organized group rides, exclusive products and product discounts, insurance discounts, and the Hog Tales newsletter. A one year full membership is included with the purchase of a new, unregistered Harley-Davidson.
In 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.
3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters 
Factory tours and museum.
Harley-Davidson Museum in Milwaukee
Harley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.
Due to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin was closed in 2009.
Anniversary celebrations.
Beginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the "Ride Home". This new tradition has continued every 5 years, and is referred to unofficially as "Harleyfest," in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28–31, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th anniversary celebration was held on August 29–31, 2013.
Labor Hall of Fame.
William S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson, Sr. were inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.
References.
</dl>

</doc>
<doc id="14144" url="http://en.wikipedia.org/wiki?curid=14144" title="Hiberno-English">
Hiberno-English

Hiberno‐English or Irish English is the set of English dialects natively written and spoken within the Republic of Ireland as well as Northern Ireland. It comprises a number of sub-varieties, such as Mid-Ulster English, Dublin English, and Cork English.
English was brought to Ireland as a result of the Norman invasion of the late 12th century. Initially, it was mainly spoken in an area known as the Pale around Dublin, with mostly Irish spoken throughout the rest of the country. By the Tudor period, Irish culture and language had regained most of the territory lost to the colonists: even in the Pale, "all the common folk… for the most part are of Irish birth, Irish habit, and of Irish language". However, the English conquest and colonisation of Ireland in the 16th century marked a revival in the use of English. By the mid-19th century, English was the majority language spoken in the country. It has retained this status to the present day, with even those whose first language is Irish being fluent in English as well.
Modern Hiberno-English has some features influenced by the Irish language and it also retains some archaic English elements. Most of these are more used in the spoken language than in formal written language, which is much closer to Standard British English, with a few differences in vocabulary. Unlike the United States and Canada, Ireland does not have its own spelling rules and uses British English spelling.
Vocabulary.
Loan words from Irish.
A number of Irish-language loan words are used in Hiberno-English, particularly in an official state capacity. For example, the head of government is the Taoiseach, the deputy head is the Tánaiste, the parliament is the Oireachtas and its lower house is Dáil Éireann. Less formally, people also use loan words in day-to-day speech, although this has been on the wane in recent decades and among the young.
Derived words from Irish.
Another group of Hiberno-English words are those "derived" from the Irish language. Some are words in English that have entered into general use, while others are unique to Ireland. These words and phrases are often Anglicised versions of words in Irish or direct translations into English. In the latter case, they often give a meaning to a word or phrase that is generally not found in wider English use.
Derived words from Old and Middle English.
Another class of vocabulary found in Hiberno-English are words and phrases common in Old and Middle English, but which have since become obscure or obsolete in the modern English language generally. Hiberno-English has also developed particular meanings for words that are still in common use in English generally.
Other words.
In addition to the three groups above, there are also additional words and phrases whose origin is disputed or unknown. While this group may not be unique to Ireland, their usage is not widespread, and could be seen as characteristic of the language in Ireland.
Grammar and syntax.
The syntax of the Irish language is quite different from that of English. Various aspects of Irish syntax have influenced Hiberno-English, though many of these idiosyncrasies are disappearing in suburban areas and among the younger population.
The other major influence on Hiberno-English that sets it apart from modern English in general is the retention of words and phrases from Old- and Middle-English.
From Irish.
Reduplication.
Reduplication is an alleged trait of Hiberno-English strongly associated with stage-Irish and Hollywood films.
Yes and no.
Irish lacks words that directly translate as "yes" or "no", and instead repeats the verb used in the question, negated if necessary, to answer. Hiberno-English uses "yes" and "no" less frequently than other English dialects as speakers can repeat the verb, positively or negatively, instead of (or in redundant addition to) using "yes" or "no".
Recent past construction.
Irish indicates recency of an action by adding "after" to the present continuous (a verb ending in "-ing"), a construction known as the "hot news perfect" or "after perfect". The idiom for "I had done X when I did Y" is "I was after doing X when I did Y", modelled on the Irish usage of the compound prepositions "i ndiaidh", "tar éis", and "in éis": "bhí mé tar éis/i ndiaidh/in éis X a dhéanamh, nuair a rinne mé Y".
A similar construction is seen where exclamation is used in describing a recent event:
When describing less astonishing or significant events, a structure resembling the German perfect can be seen:
This correlates with an analysis of "H1 Irish" proposed by Adger & Mitrovic, in a deliberate parallel to the status of German as a V2 language.
Reflection for emphasis.
The reflexive version of pronouns is often used for emphasis or to refer indirectly to a particular person, etc., according to context. "Herself", for example, might refer to the speaker's boss or to the woman of the house. Use of "herself" or "himself" in this way often indicates that the speaker attributes some degree of arrogance or selfishness to the person in question. Note also the indirectness of this construction relative to, for example, "She's coming now"
This is not limited only to the verb "to be": it is also used with "to have" when used as an auxiliary; and, with other verbs, the verb "to do" is used. This is most commonly used for intensification, especially in Ulster English.
Prepositional pronouns.
There are some language forms that stem from the fact that there is no verb "to have" in Irish. Instead, possession is indicated in Irish by using the preposition "at", (in Irish, "ag."). To be more precise, Irish uses a prepositional pronoun that combines "ag" "at" and "mé" "me" to create "agam".
In English, the verb "to have" is used, along with a "with me" or "on me" that derives from "Tá … agam." This gives rise to the frequent
Somebody who can speak a language "has" a language, in which Hiberno-English has borrowed the grammatical form used in Irish.
When describing something, many Hiberno-English speakers use the term "in it" where "there" would usually be used. This is due to the Irish word "ann" (pronounced "oun" or "on") fulfilling both meanings.
Another idiom is this thing or that thing described as "this man here" or "that man there", which also features in Newfoundland English in Canada.
Conditionals have a greater presence in Hiberno-English due to the tendency to replace the simple present tense with the conditional (would) and the simple past tense with the conditional perfect (would have).
Bring and take: Irish use of these words differs from that of British English because it follows the Gaelic grammar for "beir" and "tóg". English usage is determined by direction; person determines Irish usage. So, in English, one takes ""from" here "to" there", and brings it ""to" here "from" there". In Irish, a person takes only when accepting a transfer of possession of the object from someone else – and a person brings at all other times, irrespective of direction (to or from).
To be.
The Irish equivalent of the verb "to be" has two present tenses, one (the present tense proper or "aimsir láithreach") for cases which are generally true or are true at the time of speaking and the other (the habitual present or "aimsir ghnáthláithreach") for repeated actions. Thus, "you are [now, or generally]" is "tá tú", but "you are [repeatedly]" is "bíonn tú". Both forms are used with the verbal noun (equivalent to the English present participle) to create compound tenses.
The corresponding usage in English is frequently found in rural areas, especially Mayo/Sligo in the west of Ireland and Wexford in the south-east, along with border areas of the North and Republic. In this form, the verb "to be" in English is similar to its use in Irish, with a "does be/do be" (or "bees", although less frequently) construction to indicate the continuous, or habitual, present:
From Old and Middle English.
In old-fashioned usage, "it is" can be freely abbreviated "’tis", even as a standalone sentence. This also allows the double contraction "’tisn’t", for "it is not".
Irish has separate forms for the second person singular ("tú") and the second person plural ("sibh").
Mirroring Irish, and almost every other Indo European language, the plural "you" is also distinguished from the singular in Hiberno-English, normally by use of the otherwise archaic English word "ye" [jiː]; the word "yous" (sometimes written as "youse") also occurs, but primarily only in Dublin and across Ulster. In addition, in some areas in Leinster, north Connacht and parts of Ulster, the hybrid word "ye-s", pronounced "yis", may be used. The pronunciation differs with that of the northwestern being [jiːz] and the Leinster pronunciation being [jɪz].
The word "ye", "yis" or "yous", otherwise archaic, is still used in place of "you" for the second-person plural. "Ye'r", "Yisser" or "Yousser" are the possessive forms, e.g. "Where are yous going?"
The verb "mitch" is very common in Ireland, indicating being truant from school. This word appears in Shakespeare, (though he wrote in Early Modern English rather than Middle English,) but is seldom heard these days in British English, although pockets of usage persist in some areas (notably South Wales, Devon, and Cornwall). In parts of Connacht and Ulster the "mitch" is often replaced by the verb "scheme", while Dublin it is replaced by "on the hop/bounce".
Another usage familiar from Shakespeare is the inclusion of the second person pronoun after the imperative form of a verb, as in "Wife, go you to her ere you go to bed" (Romeo and Juliet, Act III, Scene IV). This is still common in Ulster: "Get youse your homework done or you're no goin' out!" In Munster, you will still hear children being told, "Up to bed, let ye" [lɛˈtʃi]
For influence from Scotland see Ulster Scots and Ulster English.
Other grammatical influences.
Now is often used at the end of sentences or phrases as a semantically empty word, completing an utterance without contributing any apparent meaning. Examples include "Bye now" (= "Goodbye"), "There you go now" (when giving someone something), "Ah now!" (expressing dismay), "Hold on now" (= "wait a minute"), "Now then" as a mild attention-getter, etc. This usage is universal among English dialects, but occurs more frequently in Hiberno-English. It is also used in the manner of the Italian 'prego' or German 'bitte', for example a barman might say "Now, Sir." when delivering drinks.
So is often used for emphasis ("I can speak Irish, so I can"), or it may be tacked onto the end of a sentence to indicate agreement, where "then" would often be used in Standard English ("Bye so", "Let's go so", "That's fine so", "We'll do that so"). The word is also used to contradict a negative statement ("You're not pushing hard enough" – "I am so!"). (This contradiction of a negative is also seen in American English, though not as often as "I am too", or "Yes, I am".) The practice of indicating emphasis with "so" and including reduplicating the sentence's subject pronoun and auxiliary verb (is, are, have, has, can, etc.) such as in the initial example, is particularly prevalent in more northern dialects such as those of Sligo, Mayo and the counties of Ulster.
Sure is often used as a tag word, emphasising the obviousness of the statement, roughly translating as but/and/well. Can be used as "to be sure", the famous Irish stereotype phrase. (But note that the other stereotype of "Sure and …" is not actually used in Ireland.) Or "Sure, I can just go on Wednesday", "I will not, to be sure." The word is also used at the end of sentences (primarily in Munster), for instance "I was only here five minutes ago, sure!" and can express emphasis or indignation.
To is often omitted from sentences where it would exist in British English. For example, "I'm not allowed go out tonight", instead of "I'm not allowed "to" go out tonight".
Will is often used where British English would use "shall" or American English "should" (as in "Will I make us a cup of tea?"). The distinction between "shall" (for first-person simple future, and second- and third-person emphatic future) and "will" (second- and third-person simple future, first-person emphatic future), maintained by many in England, does not exist in Hiberno-English, with "will" generally used in all cases.
Once is sometimes used in a different way from how it is used in other dialects; in this usage, it indicates a combination of logical and causal conditionality: "I have no problem laughing at myself once the joke is funny." Other dialects of English would probably use "if" in this situation.
Major dialects and accents.
Modern phonologists often divide Irish English into five major varieties:
Ulster English.
Ulster English (or northern Irish English) here refers collectively to the varieties of the Ulster province, including Northern Ireland and neighbouring counties outside of Northern Ireland, which has been greatly influenced by Ulster Irish as well as the Scots language, brought over by Scottish settlers during the Plantation of Ulster. Its main subdivisions are mid Ulster English as well as Ulster Scots English, more directly influenced by the Scots language.
Unique among the Irish English dialects, Ulster varieties pronounce:
Local Dublin English.
Local Dublin English (or popular Dublin English) here refers to a traditional, broad, working-class variety spoken in the Republic of Ireland's capital of Dublin. It is the only Irish English variety that in earlier history was non-rhotic; however, it is today weakly rhotic, and it uniquely pronounces:
The local Dublin accent is also known for a phenomenon called "vowel breaking", in which the vowel sounds , , , and in closed syllables are "broken" into two syllables, approximating ], ], ], and ], respectively.
Non-local Dublin English.
Non-local Dublin English here refers collectively to non-localised, non-working class, and more recent varieties of Dublin and the surrounding eastern region of Ireland. It includes mainstream Dublin English, a widely common, middle-class variety that preserves a few local Dublin features while setting the basis for an otherwise supraregional (excluding the north) Irish English accent, as well as new Dublin English (also, advanced Dublin English and, formerly, fashionable Dublin English), a youthful variety beginning in the 1990s among, originally, the "avant-garde" and now those aspiring to a non-local "urban sophistication". New Dublin English itself, first associated with affluent and middle-class inhabitants of southside Dublin, has replaced (yet was largely influenced by) the moribund Dublin 4 accent (popularly known as "DART speak" or, later, "Dortspeak"), which originated around the 1970s from Dubliners who rejected traditional notions of Irishness, regarding themselves as more trendy and sophisticated; however, particular aspects of the Dublin 4 accent became quickly noticed and ridiculed as sounding affected, causing these features to fall out of fashion by the 1990s.
West and South-West Irish English.
West and South-West Irish English here refers to rural, broad varieties of Ireland's West and South-West Regions. Both are known for:
South-West Irish English (commonly known, by specific county, as Cork English, Kerry English, or Limerick English) also features two major defining characteristics of its own: the raising of to [ɪ] when before /n/ or /m/ (as in "again" or "pen"), and the noticeable intonation pattern of a slight pitch rise followed by a significant pitch drop on stressed long-vowel syllables.
Supraregional southern Irish English.
Supraregional southern Irish English (sometimes, simply, supraregional Irish English) here refers to a variety crossing regional boundaries throughout all of the Republic of Ireland, except the north. As mentioned earlier, mainstream Dublin English of the early- to mid-1900s is the direct influence and catalyst for this variety. Most speakers born in the 1980s or later are showing fewer features of the twentieth-century mainstream supraregional form and more characteristics of an advanced supraregional variety that aligns clearly with the rapidly-spreading new Dublin accent (see more above, under "Non-local Dublin English").
Ireland's surparegional dialect, uniquely pronounces:
Pronunciation and phonology.
The following charts list the vowels typical of Irish English dialects as well as the several distinctive consonants of Irish English. Phonological characteristics of overall Hiberno-English as well as of the five aforementioned sub-divisions of Hiberno-English—northern Ireland (or Ulster); local Dublin; non-Local Dublin; West & South-West Ireland; and supraregional (southern) Ireland—are all listed in the charts below:
Pure vowels (Monophthongs) 
The defining pure vowels of Hiberno-English:
The following pure vowel sounds are defining characteristics of Hiberno-English:
All pure vowels of various Hiberno-English dialects:
Footnotes:
^1 
In southside Dublin's once-briefly fashionable "Dublin 4" (or "Dortspeak") accent, the "/ɑː/ and broad /æ/" set becomes rounded as [ɒː].
^2 In South-West Ireland, before /n/ or /m/ is raised to [ɪ].
Other notes:
Gliding vowels (Diphthongs) 
The defining diphthongs of Hiberno-English:
The following gliding vowel (diphthong) sounds are defining characteristics of Irish English:
All diphthongs of various Hiberno-English dialects:
R-coloured vowels 
The defining "r"-coloured vowels of Hiberno-English:
The following "r"-coloured vowel features are defining characteristics of Hiberno-English: 
All "r"-coloured vowels of various Hiberno-English dialects:
Footnotes:
^1 Every major accent of Irish English is rhotic (pronounces "r" after a vowel sound). The local Dublin accent is the only one that during an earlier time was non-rhotic, though it is lightly rhotic today. The rhotic consonant in most Irish accents is an approximant [ɹ], which, before a vowel sound in older and more traditional varieties, can also be a tapped [ɾ].
^2 In local Dublin and other very traditional Irish English varieties ranging from the south to the north, the phoneme /ɜr/ is split, either pronounced as [ɛː(ɹ)] or [ʊː(ɹ)], depending on spelling and preceding consonants. In the local Dublin accent, words spelled with "-ear", "-or", and "-ur" are always pronounced as [ʊː(ɹ)], while words written as either "-er" or "-ir" are pronounced as [ɛː(ɹ)], unless following a labial consonant (e.g. "bird" or "first"), which gives this sound the [ʊː(ɹ)] realisation. Rhotic examples: "bird" as [bʊːɹd], "circle" as [ˈsɛːɹkəl], "first" as [fʊːɹs] or [fʊːɹst], "heard" as [hʊːɹd], "herd" as [hɛːɹd], "turn" as [tʰʊːɹn], and "work" as [wʊːɹk]. (In mainstream Dublin, new Dublin, and supraregional Irish accents, this distinction is seldom preserved, with both /ɜr/ phonemes typically merged into the fully rhotic [ɚː].)
^3 In non-rhotic local Dublin varieties, /ər/ is either lowered to [ɐ] or backed and raised to [ɤ].
^4 The distinction between /ɔr/ and /ɔər/ is widely preserved in Ireland, so that, for example, "horse" and "hoarse" are not merged in most Irish English dialects; however, they are usually merged in Belfast and (fashionable) new Dublin.
Consonants 
The defining consonants of Hiberno-English:
The consonants of Hiberno-English mostly align to the typical English consonant sounds. However, a few Irish English consonants have distinctive, varying qualities. The following consonant features are defining characteristics of Hiberno-English: 
Unique consonants in various Hiberno-English dialects:
Footnotes:
^1 In traditional, conservative Ulster English, /k/ and /g/ is palatalised before a low front vowel.
^2 Local Dublin undergoes cluster simplification, so that stop consonant sounds occurring after fricatives or sonorants may be left unpronounced, resulting, for example, in "poun(d)" and "las(t)".
^3 Rhoticity: Every major accent of Irish English is strongly rhotic (pronounces "r" after a vowel sound), though to a weaker degree with the local Dublin accent. The accents of local Dublin and some smaller eastern towns like Drogheda were historically non-rhotic and now only very lightly rhotic or variably rhotic, with the rhotic consonant being an alveolar approximant, [ɹ]. In extremely traditional and conservative accents (exemplified, for instance, in the speech of older speakers throughout the coumtry, even in South-West Ireland, such as Mícheál Ó Muircheartaigh and Jackie Healy-Rae), the rhotic consonant, before a vowel sound, can also be an alveolar tap, [ɾ]. The rhotic consonant for the northern Ireland and new Dublin accents is a retroflex approximant, [ɻ]. Dublin's retroflex approximant has no precedent outside of northern Ireland and is a genuine innovation of the past two decades. A guttural/uvular [ʁ] is found in north-east Leinster. Otherwise, the rhotic consonant of virtually all other Irish accents is the postalveolar approximant, [ɹ].
^4 The symbol [θ̠] is used here to represent the voiceless alveolar non-sibilant fricative, sometimes known as a "slit fricative", whose articulation is described as being apico-alveolar.
^5 Overall, /hw/ and /w/ are being increasingly merged in supraregional Irish English, for example, making "wine" and "whine" homophones, as in most varieties of English around the world.
Other phonological characteristics of Irish English include that consonant clusters ending in /j/ are distinctive:
The naming of the letter "H" as "haitch" is standard, while the letter "R" is called "or", the letter "A" is often pronounced "ah", and the letter "Z" is often referred to as "e-zed" in working-class Dublin and Belfast accents or parodies of same.
Some words gain a syllable in Irish speech, like "film", which becomes "fillum".

</doc>
<doc id="14147" url="http://en.wikipedia.org/wiki?curid=14147" title="Harmonic analysis">
Harmonic analysis

 
Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as signal processing, quantum mechanics, and neuroscience.
The term "harmonics" originated as the ancient Greek word, "harmonikos," meaning "skilled in music." In physical eigenvalue problems it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning. 
The classical Fourier transform on R"n" is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution "f", we can attempt to translate these requirements in terms of the Fourier transform of "f". The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if "f" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic analysis setting. See also: Convergence of Fourier series.
Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.
Abstract harmonic analysis.
One of the most modern branches of harmonic analysis, having its roots in the mid-twentieth century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups. 
The theory for abelian locally compact groups is called Pontryagin duality.
Harmonic analysis studies the properties of that duality and Fourier transform, and attempts to extend those features to different settings, for instance to the case of non-abelian Lie groups. 
For general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.
If the group is neither abelian nor compact, no general satisfactory theory is currently known. ("Satisfactory" means at least as strong as the Plancherel theorem.) However, many specific cases have been analyzed, for example SL"n". In this case, representations in infinite dimensions play a crucial role.

</doc>
<doc id="14148" url="http://en.wikipedia.org/wiki?curid=14148" title="Home run">
Home run

In baseball, a home run (abbreviated HR, also "homer", "dinger", "bomb", "blast", or "four-bagger") is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home safely in one play without any errors being committed by the defensive team in the process. In modern baseball, the feat is typically achieved by hitting the ball over the outfield fence between the foul poles (or making contact with either foul pole) without first touching the ground, resulting in an automatic home run. There is also the "inside-the-park" home run, increasingly rare in modern baseball, where the batter reaches home safely while the baseball is in play on the field. When a home run is scored, the batter is also credited with a hit and a run scored, and an RBI for each runner that scores, including himself. Likewise, the pitcher is recorded as having given up a hit, and a run for each runner that scores including the batter.
Home runs are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams—hence the old saying, variously attributed to slugger Ralph Kiner, or to a teammate talking about Kiner, "Home run hitters drive Cadillacs, and singles hitters drive Fords."
Types of home runs.
Out of the park.
The most common type of home run involves hitting the ball over the outfield fence, or above a line on the outfield fence specifically designed to indicate a home run, in flight, in fair territory, without it being caught or deflected back by an outfielder into the playing field. This is sometimes called a home run "out of the ballpark", although that term is frequently used to indicate a blow that completely clears any outfield seating, as a home run is usually automatically assumed to have left the field of play unless otherwise indicated. A batted ball that hits the ground (in fair territory) and bounces out of play is "not" a home run, but an "automatic double" in most stadiums (colloquially called a ground rule double).
A batted ball is also considered a home run if the ball touches any of the following while in flight, even if the ball subsequently rebounds back onto the playing field:
A home run accomplished in any of the above manners is an automatic home run. The ball is considered dead, and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so won't cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).
Inside-the-park home run.
An inside-the-park home run occurs when a batter hits the ball into play and is able to circle the bases before the fielders can put him out. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.
In the early days of baseball, outfields were relatively much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.
With outfields much less spacious and more uniformly designed than in the game's early days, inside-the-park home runs are now a rarity. They are usually the result of a ball being hit by a very fast runner, coupled with an outfielder either misjudging the flight of the ball (e.g., diving and missing) or the ball taking an unexpected bounce. Either way, this sends the ball into open space in the outfield and thereby allows the batter-runner to circle the bases before the defensive team can put him out. The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.
If any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored; instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.
An example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball off the right-center field wall, which caromed in the opposite direction from where National League right fielder Ken Griffey, Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history, and led to Suzuki being named the game's Most Valuable Player.
Number of runs batted in.
Home runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is seldom called a "one-run homer", but rather a solo home run, solo homer, or "solo shot". With one runner on base, two runs are scored (the baserunner and the batter) and thus the home run is often called a two-run homer or two-run shot. Similarly, a home runs with two runners on base is a three-run homer or three-run shot.
The term "four-run homer" is seldom used; instead, it is nearly always called a "grand slam". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.
Grand slam.
A grand slam occurs when the bases are "loaded" (that is, there are base runners standing at first, second, and third base) and the batter hits a home run. According to "The Dickson Baseball Dictionary", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.
On July 25, 1956 Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9–8 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.
On April 23, 1999, Fernando Tatís made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tatís also set a Major-League record with 8 RBI in one inning.
On July 29, 2003 against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate. In fact, he hit three home runs in that game, and his two grand slams were in consecutive at-bats.
On August 25, 2011 the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually went on to win the game 22–9, after trailing 7–1.
Specific situation home runs.
These types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.
Walk-off home run.
A walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the players can "walk off" the field.
Two World Series have ended via the "walk-off" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a 9th inning solo home run in the 7th game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a 9th inning 3-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.
Such a home run can also be called a "sudden death" or "sudden victory" home run. That usage has lessened as "walk-off home run" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death homer would probably be the "Shot Heard 'Round the World" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants.
A walk-off home run over the fence is an exception to baseball's one run rule. Normally if the home team is tied or behind in the ninth or extra innings the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches 1st base or the moment the runner touches home plate—whichever happens last. However, this is superseded by the "ground rule", which provides automatic doubles (when a ball-in-play hits the ground first then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.
Lead-off home run.
A lead-off home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB, Rickey Henderson holds the record with 81 lead-off home runs. Craig Biggio holds the National League record with 53, second overall to Henderson.
In 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.
Back-to-back.
When two consecutive batters each hit a home run, this is described as back-to-back home runs. It is still considered back-to-back even if both batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back, the most recent occurrence on August 25, 2014 when Delmon Young, J.J. Hardy and Chris Davis of the Baltimore Orioles hit back-to-back-to-back home runs off Jake Odorizzi and Kirby Yates of the Tampa Bay Rays.
Four home runs in a row by consecutive batters has only occurred eight times in the history of Major League Baseball. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on August 11, 2010, when the Arizona Diamondbacks hit four in a row against the Milwaukee Brewers in Miller Park as Adam LaRoche, Miguel Montero, Mark Reynolds and Stephen Drew homered off pitcher Dave Bush. Bush became the third pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963 and Chase Wright on April 22, 2007.
On August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9-2. In this game, Jim Thome, Paul Konerko, Alexei Ramirez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez blasted their homers off of Joel Peralta, while Uribe did it off of Rob Tejeda. The next batter, veteran backstop Toby Hall, tried aimlessly to hit the ball as far as possible, but his effort resulted in a strike out.
On April 22, 2007 the Boston Red Sox were trailing the New York Yankees 3–0 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit back-to-back-to-back-to-back home runs to put them up 4–3. They eventually went on to win the game 7–6 after a three-run home run by Mike Lowell in the bottom of the 7th inning. On September 18, 2006 trailing 9–5 to the San Diego Padres in the 9th inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the 10th, the Dodgers won the game in the bottom of the 10th, on a walk-off two run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his homer was the second of the four.
On September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8–6. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians.
Twice in MLB history have two brothers hit back-to-back home runs. On April 23, 2013, brothers BJ Upton and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.
Simple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a homer, he might have his concentration broken and might alter his normal approach in an attempt to "make up for it" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved "Babe Ruth's called shot" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.
In Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back homers in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.
Another notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey, Sr. and Ken Griffey, Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.
On May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off of starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.
On June 19th 2012, Jose Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnacion for a lead change in each instance.
Consecutive home runs by one batter.
The occurrence of individuals hitting home runs in consecutive at-bats is not unusual, but three or more is rare. The record for consecutive home runs by a batter under any circumstances is 4. Of the sixteen players (through 2012) who have hit 4 in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.
Bases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, 4 in four games played, during September 17–22, 1957, for the Red Sox. Williams hit a pinch-hit homer on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had 4 walks interspersed among his 4 homers.
In World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at bats. He had also hit one in his last at bat of the previous game, giving him four home runs on four consecutive swings. (His home run in game 5 was also hit on the first pitch, although this did not add to any significant streak.) The four in a row set the record for consecutive homers across two Series games.
In Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series, also tying the record with Pujols, Jackson, and Ruth.
Nomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: 3 homers in 2 innings, on July 23, 2002, for the Boston Red Sox.
Home run cycle.
An offshoot of hitting for the cycle, a "home run cycle" is where a player hits a solo, 2-run, 3-run, and grand slam home run all in one game. This is an extremely rare feat, as it requires the batter to not only hit four home runs in a game (which itself has only occurred 16 times in the Major Leagues), but also to hit those home runs with the specific number of runners already on base. Although it is a rare accomplishment, it is largely dependent on circumstances outside the player's control, such as his preceding teammates' ability to get on base, as well as the order in which he comes to bat in any particular inning.
Another variant of the home run cycle would be the "natural home run cycle", which would require a batter to hit a solo, 2-run, 3-run, and grand slam home run in that order.
Though multiple home run cycles have been recorded in collegiate baseball, the only home run cycle in a professional baseball game belongs to Tyrone Horne, who stroked four long balls for the minor league, Double-A Arkansas Travelers in a game against the San Antonio Missions on July 27, 1998.
On May 20, 1998, the Triple-A Indianapolis Indians performed a feat possibly never before duplicated in professional baseball. In the fifth inning of a game against the Pawtucket Red Sox, Indianapolis players hit for a "Homer Cycle". Pete Rose, Jr. opened the inning with a solo home run, Jason Williams connected for a 3-run shot, Glenn Murray slugged a grand slam, and Guillermo Garcia finished the scoring with a 2-run blast. The Indians won the game 11–4.
A major league player has come close to hitting for the home run cycle many times. On April 2, 1997, Tino Martinez, first baseman of the New York Yankees, was a grand slam away from accomplishing this feat in a 16–2 victory over his former team, the Seattle Mariners. He hit a 3-run home run in the 1st inning, a 2-run home run in the 3rd and a solo shot in the 5th; all off starting pitcher Scott Sanders. He would get four more plate appearances that night, three of which came with the bases loaded. He grounded out with the bases loaded in the 6th and singled with a man on in the 8th. In the 9th inning, he would come to bat twice with the bases loaded where he first walked and later struck out to end the inning. On April 26, 2005 Alex Rodriguez of the New York Yankees hit 3 home runs off Los Angeles Angels pitcher Bartolo Colón. Rodriguez hit a 3-run home run, 2-run home run, and a grand slam in the first, third, and fourth innings, respectively. He later, in the bottom of the eighth inning, just missed a solo home run, lining out to Jeff DaVanon in deep center field. On May 16, 2008 Jayson Werth of the Philadelphia Phillies hit 3 home runs off Toronto Blue Jays pitchers David Purcey and Jesse Litsch. Werth hit a 3-run home run, a grand slam, and a solo home run in the second, third, and fifth innings, respectively. On June 26, 2009, Andre Ethier of Los Angeles Dodgers also came close to hitting for the home run cycle when he hit a three-run home run off Jason Vargas in the second, a two-run home run off Roy Corcoran in the sixth, and a solo home run off Miguel Batista in the eighth inning in a Dodger home game against Seattle Mariners.
On July 7, 2009, Chicago White Sox first baseman Paul Konerko came within a three-run home run of hitting the home run cycle. He hit a solo home run in the second inning off Cleveland Indians pitcher Jeremy Sowers, a grand slam in the sixth inning off reliever Chris Perez, and a two-run home run in the seventh inning off reliever Winston Abreu.
On August 1, 2009, Andrew McCutchen, center fielder of the Pittsburgh Pirates, hit a solo home run in the first inning, a two run home run in the fourth inning, and a three run home run in the sixth inning of a game against the Washington Nationals in Pittsburgh, Pennsylvania. He had one final at bat in the game. Had he hit a grand slam in his final at bat, McCutchen would have been the only major league player in history to hit for the natural home run cycle—a solo home run first, a two run home run second, a three run home run third and a grand slam fourth.
On September 17, 2010, Shin-Soo Choo, right fielder for the Cleveland Indians, hit three home runs in an away game against the Kansas City Royals. Choo hit a 2-run homer 420 ft to right in the top of the 4th inning. In Choo's next at bat, in the top of the 6th, he hit a towering grand slam to deep center. In the top of the 8th Choo hit a 405 ft solo homer over the right field wall. In the top of the 9th, the Indians had two men on at 1st and 2nd with Choo waiting on deck, when Asdrúbal Cabrera grounded out to first to end the inning. With the Indians leading 11–4, the Royals went out 1–2–3 to end the game.
On June 27, 2012, the New York Mets hit a home run cycle against the Chicago Cubs at Wrigley Field. Daniel Murphy hit a two run home run in the fourth inning off of Jeff Samardzija, Ike Davis hit a three run home run in the sixth inning off of Samardzija, Daniel Murphy hit a solo home run in the fifth, his second of the game, off of Casey Coleman, and Scott Hairston hit a grand slam in the sixth inning off of Coleman to cap a 17–1 win over the Cubs.
History of the home run.
In the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home "run" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century, because in baseball's early days a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called "manufacturing runs" or "small ball".
The home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the "inside game" to the "power game" in order to keep up.
Before 1931, a ball that bounced over an outfield fence during a major league game was considered a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became ground rule doubles in most parks. A carryover of the old rule is that if a player deflects a ball over the outfield fence without it touching the ground, it is a home run.
Also, until approximately that time, the ball had to not only go over the fence in fair territory, but to land in the bleachers in fair territory or to still be visibly fair when disappearing behind a wall. The rule stipulated "fair when last seen" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second "foul pole" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal "touch down" of the ball in the end zone but now only requires the "breaking of the [vertical] plane" of the goal line, in baseball the ball need only "break the plane" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).
Babe Ruth's 60th home run in 1927 was somewhat controversial, because it landed barely in fair territory in the stands down the right field line. Ruth lost a number of home runs in his career due to the when-last-seen rule. Bill Jenkinson, in "The Year Babe Ruth Hit 104 Home Runs", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.
Further, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to "force" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.
Major League Baseball keeps running totals of all-time home runs by team, including teams no longer active (prior to 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in MLB history with a grand slam on September 8, 2008. Sheffield had hit MLB's 249,999th home run against Gio Gonzalez in his previous at-bat.
The all-time, verified professional baseball record for career home runs for one player, excluding the U. S. Negro Leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.
In Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&T Park off pitcher Mike Bacsik. Only seven other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Willie Mays (660), Ken Griffey, Jr. (630), Sammy Sosa (609), Jim Thome (612) and Alex Rodriguez (active player) (660).
The single season record is 73, set by Barry Bonds in 2001. Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.
Negro League slugger Josh Gibson's Baseball Hall of Fame plaque says he hit "almost 800" home runs in his career. The "Guinness Book of World Records" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, Baseball, states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record keeping in the Negro Leagues. The 1993 edition of the MacMillan "Baseball Encyclopedia" attempted to compile a set of Negro League records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 "official" Negro League games they were able to account for in his 17-year career, about 1 homer every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or 1 homer every 3.5 games. The large gap in the numbers for Gibson reflect the fact that Negro League clubs played relatively far fewer league games and many more "barnstorming" or exhibition games during the course of a season, than did the major league clubs of that era.
Other legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit "the longest home run ever" at an estimated distance of 643 ft, although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season 3 times), Ken Griffey, Jr. and Eddie Mathews. The longest verifiable home run distance is about 575 ft, by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.
The location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The hallowed spot sits outside Miller Park, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th homer landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured homer in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.
Instant replay.
Replays "to get the call right" have been used extremely sporadically in the past, but the use of instant replay to determine "boundary calls"—home runs and foul balls—was not officially allowed until 2008.
In a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.
In November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:
On August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008 in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run, until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.
About two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Peña of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Peña a home run.
Aside from the two aforementioned reviews at Tampa Bay, replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.
On October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that appeared to hit a camera protruding over the wall and into the field of play in deep left field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.
See also.
Career achievements

</doc>
<doc id="14149" url="http://en.wikipedia.org/wiki?curid=14149" title="Harappa">
Harappa

Harappans (]; Punjabi: ਹਰਪਾ; Urdu: ہڑپّہا‎; Gujarati: હરપા; Kannada: ಹರಪ್ಪ; Tamil: ஹரப; Hindi: हरपा; is an archaeological site in Punjab, Pakistan, about 24 km west of Sahiwal. The site takes its name from a modern village located near the former course of the Ravi River. The current village of Harappa is 6 km from the ancient site. Although modern Harappa has a railway station left from the period of British Raj, it is today just a small crossroads town of population 15,000.
The site of the ancient city contains the ruins of a Bronze Age fortified city, which was part of the Cemetery H culture and the Indus Valley Civilization, centered in Sindh and the Punjab. The city is believed to have had as many as 23,500 residents and occupied about 150 ha at its greatest extent during the Mature Harappan phase (2600–1900 BC), which is considered large for its time. Per archaeological convention of naming a previously unknown civilization by its first excavated site, the Indus Valley Civilization is also called the Harappan Civilization.
The ancient city of Harappa was heavily damaged under the British rule, when bricks from the ruins were used as track ballast in the making of the Lahore-Multan Railroad. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artifacts during the early stages of construction work. A plea from the prominent Pakistani archaeologist Ahmad Hasan Dani to the Ministry of Culture resulted in a restoration of the site.
History.
The Indus Valley Civilization (also known as the Harappan culture) has its earliest roots in cultures such as that of Mehrgarh, approximately 6000 BCE. The two greatest cities, Mohenjo-daro and Harappa, emerged circa 2600 BCE along the Indus River valley in Punjab and Sindh. The civilization, with a writing system, urban centers, and diversified social and economic system, was rediscovered in the 1920s after excavations at Mohenjo-daro in Sindh near Larkana, and Harappa, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in east Punjab, India in the north, to Gujarat in the south and east, and to Balochistan in the west have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railroad (as part of the Sind and Punjab Railway), used brick from the Harappa ruins for track ballast, an abundance of artifacts has nevertheless been found. The bricks discovered were made of red sand, clay, stones and were baked at very high temperature.
Culture and economy.
Indus Valley civilization was mainly an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Sumer in southern Mesopotamia. Both Mohenjo-Daro and Harappa are generally characterized as having "differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers." Although such similarities have given rise to arguments for the existence of a standardized system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.
The chart weights and measures of the Indus Valley Civilization, on the other hand, were highly standardized, and conform to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. "Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, were domesticated," as well as "fowl for fighting". Wheel-made pottery—some of it adorned with animal and geometric motifs—has been found in profusion at all the major Indus sites. A centralized administration for each city, though not the whole civilization, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy.
What is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (a pit of skulls located south-east of the city walls). Furthermore, rates of cranio-facial trauma and infection increased through time, demonstrating that the civilization collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety, a basic feature of hierarchical societies world-wide.
Archaeology.
The excavators of the site have proposed the following chronology of Harappa's occupation:
By far the most exquisite and obscure artifacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world, and despite the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascription of Indus Valley Civilization iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence of such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area. This is especially evident in the radically varying interpretations of Harappan material culture as seen from both Pakistan- and India-based scholars.
Suggested earliest writing.
Clay and stone tablets unearthed at Harappa, which were carbon dated 3300–3200 BCE., contain trident-shaped and plant-like markings. They have suggested as possibly the earliest writings anywhere in the world, as opined by Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. This primitive writing is placed slightly earlier than primitive writings of the Sumerians of Mesopotamia, dated c.3100 BCE. These markings have similarities to what later became Indus Script. This discovery also suggests that the earliest writings by mankind developed independently in three places (Harappa, Mesopotamia and Ancient Egypt) between c.3500 BCE and 3100 BCE.

</doc>
<doc id="14153" url="http://en.wikipedia.org/wiki?curid=14153" title="Hendecasyllable">
Hendecasyllable

The hendecasyllable is a line of eleven syllables, used in Ancient Greek and Latin quantitative verse as well as in medieval and modern European poetry.
In quantitative verse.
The classical hendecasyllable is a quantitative meter used in Ancient Greece in Aeolic verse and in scolia, and later by the Roman poet Catullus. Each line has eleven syllables; hence the name, which comes from the Greek word for eleven. The heart of the line is the choriamb (- u u -). The pattern (also known as the Phalaecian) is as follows (using "-" for a long syllable, "u" for a short and "x" for an "anceps" or variable syllable):
 x x - u u - u - u - - 
 (where x x is either - u or - - or u -)
Another form of hendecasyllabic verse is the "Sapphic" (so named for its use in the Sapphic stanza), with the pattern:
 - x - x - u u - u - -
"The hendecasyllabic offers the opportunity to maintain the basic sapphic rhythm for a long period, building up momentum."
Of the polymetric poems of Catullus, forty-three are hendecasyllabic. The metre has been imitated in English, notably by Alfred Tennyson, Swinburne, and Robert Frost, cf. "For Once Then Something." Contemporary American poets Annie Finch ("Lucid Waking") and Patricia Smith ("The Reemergence of the Noose") have published recent examples. Poets wanting to capture the hendecasyllabic rhythm in English have simply transposed the pattern into its accentual-syllabic equivalent: /u|/u|/uu/u|/u|, or trochee/trochee/dactyl/trochee/trochee, so that the long/short pattern becomes a stress/unstress pattern. Tennyson, however, maintained the quantitative features of the metre:
For an example, see Catullus 1.
In Italian poetry.
The hendecasyllable (Italian: "endecasillabo") is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called "endecasillabo a minore", or lesser hendecasyllable, and has the first hemistich equivalent to a "quinario"; the second is called "endecasillabo a maiore", or greater hendecasyllable, and has a "settenario" as the first hemistich.
The most usual stress schemes for the Italian hendecasyllable are stresses on sixth and tenth syllables (for example, ""Nel mezzo del cammin di nostra vita"," Dante Alighieri, first line of "The Divine Comedy)," and on the fourth, seventh and tenth syllables (""Un incalzar di cavalli accorrenti"," Ugo Foscolo, "Dei sepolcri").
Most classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme system varies from terza rima to ottava, from sonnet to canzone. From the early 16th century, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. An early example is "Le Api" ("the bees") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525, which begins:
Like other early Italian-language tragedies, the "Sophonisba" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the "Canti" of Giacomo Leopardi, where hendecasyllables are alternated with "settenari". The effect of "endecasillabi sciolti" ("untied" hendecasyllables) may be considered similar to that of English blank verse.
It has a role in Italian poetry, and a formal structure, comparable to the iambic pentameter in English or the alexandrine in French.
In English poetry.
The term "hendecasyllable" is sometimes used in English poetry to describe a line of iambic pentameter with an extra short syllable at the end, as in the first line of John Keats's "Endymion:" "A thing of beauty is a joy for ever."

</doc>
<doc id="14155" url="http://en.wikipedia.org/wiki?curid=14155" title="Hebrides">
Hebrides

The Hebrides (; Scottish Gaelic: "Innse Gall"; Old Norse: "Suðreyjar") comprise a widespread and diverse archipelago off the west coast of mainland Scotland. There are two main groups: the Inner and Outer Hebrides. These islands have a long history of occupation dating back to the Mesolithic, and the culture of the residents has been affected by the successive influences of Celtic, Norse, and English-speaking peoples. This diversity is reflected in the names given to the islands, which are derived from the languages that have been spoken there in historic and perhaps prehistoric times.
Various artists have been inspired by their Hebridean experiences. Today the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. Although the Hebrides lack biodiversity in comparison to mainland Britain, these islands have much to offer the naturalist. Seals, for example, are present around the coasts in internationally important numbers.
Geology, geography and climate.
The Hebrides have a diverse geology ranging in age from Precambrian strata that are amongst the oldest rocks in Europe to Paleogene igneous intrusions.
The Hebrides can be divided into two main groups, separated from one another by The Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides are a chain of more than 100 islands and small skerries located about 70 km west of mainland Scotland. There are 15 inhabited islands in this archipelago. The main islands include Barra, Benbecula, Berneray, Harris, Lewis, North Uist, South Uist, and St Kilda. In total, the islands have an area of approximately 7200 km2 and a population of 44,759.
A complication is that there are various descriptions of the scope of the Hebrides. The "Collins Encyclopedia of Scotland" describes the Inner Hebrides as lying "east of The Minch", which would include any and all offshore islands. There are various islands that lie in the sea lochs such as Eilean Bàn and Eilean Donan that might not ordinarily be described as "Hebridean" but no formal definitions exist.
In the past the Outer Hebrides were often referred to as the Long Isle (Scottish Gaelic: "An t-Eilean Fada"). Today, they are also known as the Western Isles although this phrase can also be used to refer to the Hebrides in general.
The Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides the average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is 1100 mm and sunshine hours range from 1,100 - 1,200 per annum. The summer days are relatively long and May to August is the driest period.
History.
Prehistory.
The Hebrides were settled during the Mesolithic era around 6500 BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on Rùm is dated to 8590+/-95 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.
Celtic era.
In 55 BC, the Greek historian Diodorus Siculus wrote that there was an island called "Hyperborea" (which means "far to the north"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.
A traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before AD 83. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.
The first written records of native life begin in the 6th century AD, when the founding of the kingdom of Dál Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of Dál Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cenél Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.
North of Dál Riata, the Inner and Outer Hebrides were nominally under Pictish control although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.”
Norwegian control.
Viking raids began on Scottish shores towards the end of the 8th century and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of Hafrsfjord in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis "fire played high in the heaven" as "flame spouted from the houses" and that in the Uists "the king dyed his sword red in blood".
The Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Celtic kinsman of the Manx royal house.
Following the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.
Scottish control.
As the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.
The Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.
In 1598, King James VI authorised some "Gentleman Adventurers" from Fife to civilise the "most barbarous Isle of Lewis". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on Bearasaigh in Loch Ròg. The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail, (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.
Early British era.
With the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen "came out" in support of the Jacobite Earl of Mar in the "15" and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price. In the wake of the rebellion, the clan system was broken up and islands of the Hebrides became a series of landed estates.
The early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Telford's "Bridge across the Atlantic" improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic. The "Battle of the Braes" involved a demonstration against lack of access to land and the serving of eviction notices. This event was instrumental in the creation of the Napier Commission, which reported in 1884 on the situation in the Highlands and disturbances continued until the passing of the 1886 Crofters' Act.
Modern economy.
For those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.
There were however continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.
Media and the arts.
"The Hebrides", also known as "Fingal's Cave", is a famous overture composed by Felix Mendelssohn while residing on these islands, while Granville Bantock composed the "Hebridean Symphony". Contemporary musicians associated with the islands include Ian Anderson, Donovan and Runrig. The poet Sorley MacLean was born on Raasay, the setting for his best known poem, "Hallaig".
The novelist Compton Mackenzie lived on Barra and George Orwell wrote "1984" whilst living on Jura. J.M. Barrie's "Marie Rose" contains references to Harris inspired by a holiday visit to Amhuinnsuidhe Castle and he wrote a screenplay for the 1924 film adaptation of "Peter Pan" whilst on Eilean Shona. Enya's song "Ebudæ" from "Shepherd Moons" is named for the Hebrides (see below). The 1973 British horror film "The Wicker Man" is set on the fictional Hebridean island of Summerisle. The experimental first-person adventure video game "Dear Esther" takes place on an unnamed Hebridean island. The 2011 British romantic comedy "The Decoy Bride" is set on the fictional Hebrides island of Hegg.
Language.
The residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.
It is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived via Ireland due to the growing influence of the kingdom of Dál Riata from the 6th century onwards and became the dominant language of the southern Hebrides at that time. For a time, the military might of the "Gall-Ghàidhils" meant that Old Norse was prevalent in the Hebrides and, north of Ardnamurchan, the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was "Suðreyjar", which means "Southern Isles". It was given in contrast to the "Norðreyjar", or the "Northern Isles" of Orkney and Shetland.
South of Ardnamurchan Gaelic place names are the most common and, after the 13th century, Gaelic became the main language of the entire Hebridean archipelago. The use of Scots and English became prominent in recent times but the Hebrides still contain the largest concentration of Scottish Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where the majority of people speak the language. The Scottish Gaelic college, Sabhal Mòr Ostaig, is based on Skye and Islay.
Ironically, given the status of the Western Isles as the last Gàidhlig-speaking stronghold in Scotland, the Gaelic language name for the islands – "Innse Gall" – means "isles of the foreigners" which has roots in the time when they were under Norse colonisation.
Etymology.
The earliest written references that have survived relating to the islands were made by Pliny the Elder in his "Natural History", where he states that there are 30 "Hebudes", and makes a separate reference to "Dumna", which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of Agricola, writes that there are five "Ebudes" (possibly meaning the Inner Hebrides) and "Dumna". Later texts in classical Latin, by writers such as Solinus, use the forms "Hebudes" and "Hæbudes".
The name "Ebudes" recorded by Ptolemy may be pre-Celtic. Islay is Ptolemy's "Epidion", the use of the "p" hinting at a Brythonic or Pictish tribal name, Epidii, although the root is not Gaelic. Woolf (2012) has suggested that "Ebudes" may be "an Irish attempt to reproduce the word "Epidii" phonetically rather than by translating it" and that the tribe's name may come from the root "epos" meaning "horse". Watson (1926) also notes the possible relationship between "Ebudes" and the ancient Irish Ulaid tribal name "Ibdaig" and the personal name of a king Iubdán recorded in the "Silva Gadelica".
The names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic but the roots of several other Hebrides may have a pre-Celtic origin. Adomnán, the 7th century abbot of Iona, records Colonsay as "Colosus" and Tiree as "Ethica", both of which may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is "Ljoðhús" in Old Norse and although various suggestions have been made as to a Norse meaning (such as "song house") the name is not of Gaelic origin and the Norse credentials are questionable.
The earliest comprehensive written list of Hebridean island names was undertaken by Donald Monro in 1549, which in some cases also provides the earliest written form of the island name. The derivations of all of the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.
Outer Hebrides.
Lewis and Harris is the largest island in Scotland and the third largest in the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. Remarkably, the island does not have a common name in either English or Gaelic and is referred to as "Lewis and Harris", "Lewis with Harris", "Harris with Lewis" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, "Erimon" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the Ancient Greek "erimos" meaning "desert". The origin of Uist (Old Norse: "Ívist") is similarly unclear.
Inner Hebrides.
There are various examples of Inner Hebridean island names that were originally Gaelic but have become completely replaced. For example Adomnán records "Sainea", "Elena", "Ommon" and "Oideacha" in the Inner Hebrides, which names must have passed out of usage in the Norse era and whose locations are not clear. One of the complexities is that an island may have had a Celtic name, that was replaced by a similar sounding Norse name, but then reverted to an essentially Gaelic name with a Norse "øy" or "ey" ending. See for example Rona below.
Uninhabited islands.
The names of uninhabited islands follow the same general patterns as the inhabited islands. The following are the ten largest in the Hebrides and their outliers.
The etymology of St Kilda, a small archipelago west of the Outer Hebrides, and its main island Hirta is very complex. No saint is known by the name of Kilda and various theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name "St Kilda" first appears on a Dutch map dated 1666, and that it may have been derived from Norse "sunt kelda" ("sweet wellwater") or from a mistaken Dutch assumption that the spring "Tobar Childa" was dedicated to a saint. ("Tobar Childa" is a tautological placename, consisting of the Gaelic and Norse words for "well", i.e. "well well"). The origin of the Gaelic for "Hirta", "Hiort" or "Hirt", which long pre-dates the use of "St Kilda", is similarly open to interpretation. Watson (1926) offers the Old Irish "hirt", a word meaning "death", possibly relating to the dangerous seas. Maclean (1977), drawing on an Icelandic saga describing an early 13th-century voyage to Ireland that mentions a visit to the islands of "Hirtir", speculates that the shape of Hirta resembles a stag, "hirtir" being "stags" in Norse.
The etymology of small islands may be no less complex. In relation to Dubh Artach, R. L. Stevenson believed that "black and dismal" was a translation of the name, noting that "as usual, in Gaelic, it is not the only one."
Natural history.
In some respects the Hebrides generally lack biodiversity in comparison to mainland Britain, with for example only half the number of mammalian species the latter has. However these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The last named was re-introduced to Rùm in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.
Red deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland in internationally important numbers, with colonies of the former found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, Killer whales, basking sharks, porpoises and dolphins are among the sealife that can be seen. 
Heather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.
Loch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.
There has been considerable controversy over hedgehogs. The animals are not native to the Outer Hebrides having been introduced in the 1970s to reduce garden pests, but their spread has posed a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 with trapped animals then being relocated to the mainland.
References and footnotes.
</dl>

</doc>
<doc id="14158" url="http://en.wikipedia.org/wiki?curid=14158" title="HMS Dreadnought">
HMS Dreadnought

Several ships and one submarine of the Royal Navy have borne the name HMS "Dreadnought" in the expectation that they would "dread nought", i.e. "fear nothing, but God". The 1906 ship was one of the Royal Navy's most famous vessels; battleships built after her were referred to as 'dreadnoughts', and earlier battleships became known as pre-dreadnoughts.

</doc>
<doc id="14159" url="http://en.wikipedia.org/wiki?curid=14159" title="Hartmann Schedel">
Hartmann Schedel

Hartmann Schedel (13 February 1440 – 28 November 1514) was a German physician, humanist, historian, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. 
Schedel is best known for his writing the text for the "Nuremberg Chronicle", known as "Schedelsche Weltchronik" (English: "Schedel's World Chronicle"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446 – 1520) and Sebastian Kammermeister (1446 – 1503). Maps in the "Chronicle" were the first ever illustrations of many cities and countries.
With the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books were previously rare and very expensive.
Schedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.

</doc>
<doc id="14160" url="http://en.wikipedia.org/wiki?curid=14160" title="Hexameter">
Hexameter

Hexameter is a metrical line of verses consisting of six feet. It was the standard epic metre in classical Greek and Latin literature, such as in the "Iliad", "Odyssey" and "Aeneid". Its use in other genres of composition include Horace's satires, Ovid's "Metamorphoses," and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by the god Hermes.
Classical Hexameter.
In classical hexameter, the six feet follow these rules:
A short syllable (υ) is a syllable with a short vowel and one consonant at the end. A long syllable (–) is a syllable that either has a long vowel, two or more consonants at the end (or a long consonant), or both. However, spaces between words are not counted, so for instance "hat" is normally short, but it is long in "hat throw," due to the "th" in the next word.
(An example in English is Coleridge's self-describing line:
In Shelley's "Adonais", the last line of every stanza is a hexameter. In line 18, the meter can be shown as follows: "He had | adorned | and hid | the com | ing bulk | of death."—this line has six feet in the meter.
In Wordsworth's "", an example of a hexameter can be found in the last line (l. 18) of stanza 2: “That there | hath past | away | a glo | ry from | the earth”—the syllables can be split up into six feet.
Variations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.
Application.
Although the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) are a few minor languages spoken in Africa, Ancient Greek, Latin and Hungarian.
While the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's "Poly-Olbion" (1612) in couplets of iambic hexameter. An example from Drayton (marking the feet):
In the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.
Several attempts were made in the 19th century to naturalise the dactylic hexameter to English, by Henry Wadsworth Longfellow, Arthur Hugh Clough and others, none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.
In the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem ""Metai" (The Seasons)" is considered the most successful hexameter text in Lithuanian as yet.

</doc>
<doc id="14162" url="http://en.wikipedia.org/wiki?curid=14162" title="Timeline of Polish history">
Timeline of Polish history

This is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of Prime Ministers of Poland.
 Centuries: 5th · 6th · 7th · 8th · 9th · 10th · 11th · 12th · 13th · 14th · 15th · 16th · 17th · 18th · 19th · 20th · 21st · See also · Further reading

</doc>
<doc id="14168" url="http://en.wikipedia.org/wiki?curid=14168" title="Himalia">
Himalia

Himalia may refer to:

</doc>
<doc id="14169" url="http://en.wikipedia.org/wiki?curid=14169" title="Heracleidae">
Heracleidae

In Greek mythology, the Heracleidae (; Ancient Greek: Ἡρακλεῖδαι) or Heraclids were the numerous descendants of Heracles (Hercules), especially applied in a narrower sense to the descendants of Hyllus, the eldest of his four sons by Deianira (Hyllus was also sometimes thought of as Heracles' son by Melite.) Other Heracleidae included Macaria, Lamos, Manto, Bianor, Tlepolemus, and Telephus. These Heraclids were a group of Dorian kings who conquered the Peloponnesian kingdoms of Mycenae, Sparta and Argos; according to the literary tradition in Greek mythology, they claimed a right to rule through their ancestor. Since Karl Otfried Müller's "Die Dorier" (1830, English translation 1839), I. ch. 3, their rise to dominance has been associated with a "Dorian invasion". Though details of genealogy differ from one ancient author to another, the cultural significance of the mythic theme, that the descendants of Heracles, exiled after his death, "returned" after some generations in order to reclaim land that their ancestors had held in Mycenaean Greece, was to assert the primal legitimacy of a traditional ruling clan that traced its origin, thus its legitimacy, to Heracles.
Origin.
Heracles, whom Zeus had originally intended to be ruler of Argos, Lacedaemon and Messenian Pylos, had been supplanted by the cunning of Hera, and his intended possessions had fallen into the hands of Eurystheus, king of Mycenae. After the death of Heracles, his children, after many wanderings, found refuge from Eurystheus at Athens. Eurystheus, on his demand for their surrender being refused, attacked Athens, but was defeated and slain. Hyllus and his brothers then invaded Peloponnesus, but after a year's stay were forced by a pestilence to quit. They withdrew to Thessaly, where Aegimius, the mythical ancestor of the Dorians, whom Heracles had assisted in war against the Lapithae, adopted Hyllus and made over to him a third part of his territory.
After the death of Aegimius, his two sons, Pamphylus and Dymas, voluntarily submitted to Hyllus (who was, according to the Dorian tradition in Herodotus V. 72, really an Achaean), who thus became ruler of the Dorians, the three branches of that race being named after these three heroes. Desiring to reconquer his paternal inheritance, Hyllus consulted the Delphic oracle, which told him to wait for "the third fruit", (or "the third crop") and then enter Peloponnesus by "a narrow passage by sea". Accordingly, after three years, Hyllus marched across the isthmus of Corinth to attack Atreus, the successor of Eurystheus, but was slain in single combat by Echemus, king of Tegea. This second attempt was followed by a third under Cleodaeus and a fourth under Aristomachus, both unsuccessful.
Dorian invasion.
At last, Temenus, Cresphontes and Aristodemus, the sons of Aristomachus, complained to the oracle that its instructions had proved fatal to those who had followed them. They received the answer that by the "third fruit" the "third generation" was meant, and that the "narrow passage" was not the isthmus of Corinth, but the straits of Rhium. They accordingly built a fleet at Naupactus, but before they set sail, Aristodemus was struck by lightning (or shot by Apollo) and the fleet destroyed, because one of the Heracleidae had slain an Acarnanian soothsayer.
The oracle, being again consulted by Temenus, bade him offer an expiatory sacrifice and banish the murderer for ten years, and look out for a man with three eyes to act as guide. On his way back to Naupactus, Temenus fell in with Oxylus, an Aetolian, who had lost one eye, riding on a horse (thus making up the three eyes) and immediately pressed him into his service. According to another account, a mule on which Oxylus rode had lost an eye. The Heracleidae repaired their ships, sailed from Naupactus to Antirrhium, and thence to Rhium in Peloponnesus. A decisive battle was fought with Tisamenus, son of Orestes, the chief ruler in the peninsula, who was defeated and slain. This conquest was traditionally dated eighty years after the Trojan War. 
The Heracleidae, who thus became practically masters of Peloponnesus, proceeded to distribute its territory among themselves by lot. Argos fell to Temenus, Lacedaemon to Procles and Eurysthenes, the twin sons of Aristodemus; and Messenia to Cresphontes (tradition maintains that Cresphontes cheated in order to obtain Messenia, which had the best land of all.) The fertile district of Elis had been reserved by agreement for Oxylus. The Heracleidae ruled in Lacedaemon until 221 BCE, but disappeared much earlier in the other countries.
This conquest of Peloponnesus by the Dorians, commonly called the "Dorian invasion" or the "Return of the Heraclidae", is represented as the recovery by the descendants of Heracles of the rightful inheritance of their hero ancestor and his sons. The Dorians followed the custom of other Greek tribes in claiming as ancestor for their ruling families one of the legendary heroes, but the traditions must not on that account be regarded as entirely mythical. They represent a joint invasion of Peloponnesus by Aetolians and Dorians, the latter having been driven southward from their original northern home under pressure from the Thessalians. It is noticeable that there is no mention of these Heraclidae or their invasion in Homer or Hesiod. Herodotus (vi. 52) speaks of poets who had celebrated their deeds, but these were limited to events immediately succeeding the death of Heracles.
List of Heraclid kings.
At Sparta.
At Sparta, the Heraclids formed two dynasties ruling jointly: the Agiads and the Eurypontids.
At Corinth.
At Corinth the Heraclids ruled as the Bacchiadae dynasty before the aristocratic revolution, which brought a Bacchiad aristocracy into power. The kings were as follows:
In Euripides' tragedy.
The story was first amplified by the Greek tragedians, who probably drew their inspiration from local legends, which glorified the services rendered by Athens to the rulers of Peloponnesus.
The Heracleidae are the main subject of Euripides' play, "Heracleidae". J. A. Spranger found the political subtext of "Heracleidae", never far to seek, so particularly apt in Athens towards the end of the peace of Nicias, in 419 BCE, that he suggested the date as its first performance.
In the tragedy, Iolaus, Heracles' old comrade, and his children, Macaria and her brothers and sisters have hidden from Eurystheus in Athens, which was ruled by King Demophon; as the first scene makes clear, their expectation is that the blood relationship of the kings with Heracles and their father's past indebtedness to Theseus, will finally provide them sanctuary. As Eurysttheus prepared to attack, an oracle told Demophon that he would win if and only if a noble woman was sacrificed to Persephone. Macaria volunteered for the sacrifice and a spring was named the Macarian spring in her honor.

</doc>
<doc id="14170" url="http://en.wikipedia.org/wiki?curid=14170" title="HIV">
HIV

The human immunodeficiency virus (HIV) is a lentivirus (a subgroup of retrovirus) that causes the acquired immunodeficiency syndrome (AIDS), a condition in humans in which progressive failure of the immune system allows life-threatening opportunistic infections and cancers to thrive. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. Infection with HIV occurs by the transfer of blood, semen, vaginal fluid, pre-ejaculate, or breast milk. Within these bodily fluids, HIV is present as both free virus particles and virus within infected immune cells.
HIV infects vital cells in the human immune system such as helper T cells (specifically CD4+ T cells), macrophages, and dendritic cells. HIV infection leads to low levels of CD4+ T cells through a number of mechanisms, including apoptosis of uninfected bystander cells, direct viral killing of infected cells, and killing of infected CD4+ T cells by CD8 cytotoxic lymphocytes that recognize infected cells. When CD4+ T cell numbers decline below a critical level, cell-mediated immunity is lost, and the body becomes progressively more susceptible to opportunistic infections.
Virology.
Classification.
HIV is a member of the genus "Lentivirus", part of the family "Retroviridae". Lentiviruses have many morphologies and biological properties in common. Many species are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded reverse transcriptase that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded integrase and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system. Alternatively, the virus may be transcribed, producing new RNA genomes and viral proteins that are packaged and released from the cell as new virus particles that begin the replication cycle anew.
Two types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was initially discovered and termed both LAV and HTLV-III. It is more virulent, more infective, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2 compared to HIV-1 implies that fewer of those exposed to HIV-2 will be infected per exposure. Because of its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.
Structure and genome.
HIV is different in structure from other retroviruses. It is roughly spherical with a diameter of about 120 nm, around 60 times smaller than a red blood cell, yet large for a virus. It is composed of two copies of positive single-stranded RNA that codes for the virus's nine genes enclosed by a conical capsid composed of 2,000 copies of the viral protein p24. The single-stranded RNA is tightly bound to nucleocapsid proteins, p7, and enzymes needed for the development of the virion such as reverse transcriptase, proteases, ribonuclease and integrase. A matrix composed of the viral protein p17 surrounds the capsid ensuring the integrity of the virion particle.
This is, in turn, surrounded by the viral envelope that is composed of two layers of fatty molecules called phospholipids taken from the membrane of a human cell when a newly formed virus particle buds from the cell. Embedded in the viral envelope are proteins from the host cell and about 70 copies of a complex HIV protein that protrudes through the surface of the virus particle. This protein, known as Env, consists of a cap made of three molecules called glycoprotein (gp) 120, and a stem consisting of three gp41 molecules that anchor the structure into the viral envelope. This glycoprotein complex enables the virus to attach to and fuse with target cells to initiate the infectious cycle.
Both these surface proteins, especially gp120, have been considered as targets of future treatments or vaccines against HIV.
The RNA genome consists of at least seven structural landmarks (LTR, TAR, RRE, PE, SLIP, CRS, and INS), and nine genes ("gag", "pol", and "env", "tat", "rev", "nef", "vif", "vpr", "vpu", and sometimes a tenth "tev", which is a fusion of tat env and rev), encoding 19 proteins. Three of these genes, "gag", "pol", and "env", contain information needed to make the structural proteins for new virus particles. For example, "env" codes for a protein called gp160 that is broken down by a cellular protease to form gp120 and gp41. The six remaining genes, "tat", "rev", "nef", "vif", "vpr", and "vpu" (or "vpx" in the case of HIV-2), are regulatory genes for proteins that control the ability of HIV to infect cells, produce new copies of virus (replicate), or cause disease.
The two Tat proteins (p16 and p14) are transcriptional transactivators for the LTR promoter acting by binding the TAR RNA element. The TAR may also be processed into microRNAs that regulate the apoptosis genes ERCC1 and IER3. The Rev protein (p19) is involved in shuttling RNAs from the nucleus and the cytoplasm by binding to the RRE RNA element. The Vif protein (p23) prevents the action of APOBEC3G (a cellular protein that deaminates Cytidine to Uridine in the single stranded viral DNA and/or interferes with reverse transcription). The Vpr protein (p14) arrests cell division at G2/M. The Nef protein (p27) down-regulates CD4 (the major viral receptor), as well as the MHC class I and class II molecules.
Nef also interacts with SH3 domains. The Vpu protein (p16) influences the release of new virus particles from infected cells. The ends of each strand of HIV RNA contain an RNA sequence called the long terminal repeat (LTR). Regions in the LTR act as switches to control production of new viruses and can be triggered by proteins from either HIV or the host cell. The Psi element is involved in viral genome packaging and recognized by Gag and Rev proteins. The SLIP element (TTTTTT) is involved in the frameshift in the Gag-Pol reading frame required to make functional Pol.
Tropism.
The term viral tropism refers to the cell types a virus infects. HIV can infect a variety of immune cells such as CD4+ T cells, macrophages, and microglial cells. HIV-1 entry to macrophages and CD4+ T cells is mediated through interaction of the virion envelope glycoproteins (gp120) with the CD4 molecule on the target cells and also with chemokine coreceptors.
Macrophage (M-tropic) strains of HIV-1, or non-syncytia-inducing strains (NSI; now called R5 viruses ) use the "β"-chemokine receptor CCR5 for entry and are, thus, able to replicate in macrophages and CD4+ T cells. This CCR5 coreceptor is used by almost all primary HIV-1 isolates regardless of viral genetic subtype. Indeed, macrophages play a key role in several critical aspects of HIV infection. They appear to be the first cells infected by HIV and perhaps the source of HIV production when CD4+ cells become depleted in the patient. Macrophages and microglial cells are the cells infected by HIV in the central nervous system. In tonsils and adenoids of HIV-infected patients, macrophages fuse into multinucleated giant cells that produce huge amounts of virus.
T-tropic isolates, or syncytia-inducing (SI; now called X4 viruses ) strains replicate in primary CD4+ T cells as well as in macrophages and use the "α"-chemokine receptor, CXCR4, for entry. Dual-tropic HIV-1 strains are thought to be transitional strains of HIV-1 and thus are able to use both CCR5 and CXCR4 as co-receptors for viral entry.
The "α"-chemokine SDF-1, a ligand for CXCR4, suppresses replication of T-tropic HIV-1 isolates. It does this by down-regulating the expression of CXCR4 on the surface of these cells. HIV that use only the CCR5 receptor are termed R5; those that use only CXCR4 are termed X4, and those that use both, X4R5. However, the use of coreceptor alone does not explain viral tropism, as not all R5 viruses are able to use CCR5 on macrophages for a productive infection and HIV can also infect a subtype of myeloid dendritic cells, which probably constitute a reservoir that maintains infection when CD4+ T cell numbers have declined to extremely low levels.
Some people are resistant to certain strains of HIV. For example, people with the CCR5-Δ32 mutation are resistant to infection with R5 virus, as the mutation stops HIV from binding to this coreceptor, reducing its ability to infect target cells.
Sexual intercourse is the major mode of HIV transmission. Both X4 and R5 HIV are present in the seminal fluid, which is passed from a male to his sexual partner. The virions can then infect numerous cellular targets and disseminate into the whole organism. However, a selection process leads to a predominant transmission of the R5 virus through this pathway. How this selective process works is still under investigation, but one model is that spermatozoa may selectively carry R5 HIV as they possess both CCR3 and CCR5 but not CXCR4 on their surface and that genital epithelial cells preferentially sequester X4 virus. In patients infected with subtype B HIV-1, there is often a co-receptor switch in late-stage disease and T-tropic variants appear that can infect a variety of T cells through CXCR4. These variants then replicate more aggressively with heightened virulence that causes rapid T cell depletion, immune system collapse, and opportunistic infections that mark the advent of AIDS. Thus, during the course of infection, viral adaptation to the use of CXCR4 instead of CCR5 may be a key step in the progression to AIDS. A number of studies with subtype B-infected individuals have determined that between 40 and 50 percent of AIDS patients can harbour viruses of the SI and, it is presumed, the X4 phenotypes.
HIV-2 is much less pathogenic than HIV-1 and is restricted in its worldwide distribution. The adoption of "accessory genes" by HIV-2 and its more promiscuous pattern of coreceptor usage (including CD4-independence) may assist the virus in its adaptation to avoid innate restriction factors present in host cells. Adaptation to use normal cellular machinery to enable transmission and productive infection has also aided the establishment of HIV-2 replication in humans. A survival strategy for any infectious agent is not to kill its host but ultimately become a commensal organism. Having achieved a low pathogenicity, over time, variants more successful at transmission will be selected.
Replication cycle.
Entry to the cell.
The HIV virion enters macrophages and CD4+ T cells by the adsorption of glycoproteins on its surface to receptors on the target cell followed by fusion of the viral envelope with the cell membrane and the release of the HIV capsid into the cell.
Entry to the cell begins through interaction of the trimeric envelope complex (gp160 spike) and both CD4 and a chemokine receptor (generally either CCR5 or CXCR4, but others are known to interact) on the cell surface. gp120 binds to integrin α4β7 activating LFA-1 the central integrin involved in the establishment of virological synapses, which facilitate efficient cell-to-cell spreading of HIV-1. The gp160 spike contains binding domains for both CD4 and chemokine receptors.
The first step in fusion involves the high-affinity attachment of the CD4 binding domains of gp120 to CD4. Once gp120 is bound with the CD4 protein, the envelope complex undergoes a structural change, exposing the chemokine binding domains of gp120 and allowing them to interact with the target chemokine receptor. This allows for a more stable two-pronged attachment, which allows the N-terminal fusion peptide gp41 to penetrate the cell membrane. Repeat sequences in gp41, HR1, and HR2 then interact, causing the collapse of the extracellular portion of gp41 into a hairpin. This loop structure brings the virus and cell membranes close together, allowing fusion of the membranes and subsequent entry of the viral capsid.
After HIV has bound to the target cell, the HIV RNA and various enzymes, including reverse transcriptase, integrase, ribonuclease, and protease, are injected into the cell. During the microtubule-based transport to the nucleus, the viral single-strand RNA genome is transcribed into double-strand DNA, which is then integrated into a host chromosome.
HIV can infect dendritic cells (DCs) by this CD4-CCR5 route, but another route using mannose-specific C-type lectin receptors such as DC-SIGN can also be used. DCs are one of the first cells encountered by the virus during sexual transmission. They are currently thought to play an important role by transmitting HIV to T-cells when the virus is captured in the mucosa by DCs. The presence of FEZ-1, which occurs naturally in neurons, is believed to prevent the infection of cells by HIV.
HIV-1 entry, as well as entry of many other retroviruses, has long been believed to occur exclusively at the plasma membrane. More recently, however, productive infection by pH-independent, clathrin-dependent endocytosis of HIV-1 has also been reported and was recently suggested to constitute the only route of productive entry.
Replication and transcription.
Shortly after the viral capsid enters the cell, an enzyme called "reverse transcriptase" liberates the single-stranded (+)RNA genome from the attached viral proteins and copies it into a complementary DNA (cDNA) molecule. The process of reverse transcription is extremely error-prone, and the resulting mutations may cause drug resistance or allow the virus to evade the body's immune system. The reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that creates a sense DNA from the "antisense" cDNA. Together, the cDNA and its complement form a double-stranded viral DNA that is then transported into the cell nucleus. The integration of the viral DNA into the host cell's genome is carried out by another viral enzyme called "integrase".
This integrated viral DNA may then lie dormant, in the latent stage of HIV infection. To actively produce the virus, certain cellular transcription factors need to be present, the most important of which is NF-"κ"B (NF kappa B), which is upregulated when T-cells become activated. This means that those cells most likely to be killed by HIV are those currently fighting infection.
During viral replication, the integrated DNA provirus is transcribed into mRNA, which is then spliced into smaller pieces. These small pieces are exported from the nucleus into the cytoplasm, where they are translated into the regulatory proteins Tat (which encourages new virus production) and Rev. As the newly produced Rev protein accumulates in the nucleus, it binds to viral mRNAs and allows unspliced RNAs to leave the nucleus, where they are otherwise retained until spliced. At this stage, the structural proteins Gag and Env are produced from the full-length mRNA. The full-length RNA is actually the virus genome; it binds to the Gag protein and is packaged into new virus particles.
HIV-1 and HIV-2 appear to package their RNA differently. HIV-1 will bind to any appropriate RNA. HIV-2 will preferentially bind to the mRNA that was used to create the Gag protein itself. This may mean that HIV-1 is better able to mutate (HIV-1 infection progresses to AIDS faster than HIV-2 infection and is responsible for the majority of global infections).
Recombination.
Two RNA genomes are encapsidated in each HIV-1 particle (see Structure and genome of HIV). Upon infection and replication catalyzed by reverse transcriptase, recombination between the two genomes can occur. Recombination occurs as the single-strand (+)RNA genomes are reverse transcribed to form DNA. During reverse transcription the nascent DNA can switch multiple times between the two copies of the viral RNA. This form of recombination is known as copy-choice. Recombination events may occur throughout the genome. From 2 to 20 events per genome may occur at each replication cycle, and these events can rapidly shuffle the genetic information that is transmitted from parental to progeny genomes.
Viral recombination produces genetic variation that likely contributes to the evolution of resistance to anti-retroviral therapy. Recombination may also contribute, in principle, to overcoming the immune defenses of the host. Yet, for the adaptive advantages of genetic variation to be realized, the two viral genomes packaged in individual infecting virus particles need to have arisen from separate progenitor parental viruses of differing genetic constitution. It is unknown how often such mixed packaging occurs under natural conditions.
Bonhoeffer et al. suggested that template switching by the reverse transcriptase acts as a repair process to deal with breaks in the ssRNA genome. In addition, Hu and Temin suggested that recombination is an adaptation for repair of damage in the RNA genomes. Strand switching (copy-choice recombination) by reverse transcriptase could generate an undamaged copy of genomic DNA from two damaged ssRNA genome copies. This view of the adaptive benefit of recombination in HIV could explain why each HIV particle contains two complete genomes, rather than one. Furthermore, the view that recombination is a repair process implies that the benefit of repair can occur at each replication cycle, and that this benefit can be realized whether or not the two genomes differ genetically. On the view that that recombination in HIV is a repair process, the generation of recombinational variation would be a consequence, but not the cause of, the evolution of template switching.
HIV-1 infection causes chronic ongoing inflammation and production of reactive oxygen species. Thus, the HIV genome may be vulnerable to oxidative damages, including breaks in the single-stranded RNA. For HIV, as well as for viruses generally, successful infection depends on overcoming host defensive strategies that often include production of genome-damaging reactive oxygen. Thus, Michod et al. suggested that recombination by viruses is an adaptation for repair of genome damages, and that recombinational variation is a byproduct that may provide a separate benefit.
Assembly and release.
The final step of the viral cycle, assembly of new HIV-1 virions, begins at the plasma membrane of the host cell. The Env polyprotein (gp160) goes through the endoplasmic reticulum and is transported to the Golgi complex where it is cleaved by furin resulting in the two HIV envelope glycoproteins, gp41 and gp120. These are transported to the plasma membrane of the host cell where gp41 anchors gp120 to the membrane of the infected cell. The Gag (p55) and Gag-Pol (p160) polyproteins also associate with the inner surface of the plasma membrane along with the HIV genomic RNA as the forming virion begins to bud from the host cell. The budded virion is still immature as the gag polyproteins still need to be cleaved into the actual matrix, capsid and nucleocapsid proteins. This cleavage is mediated by the also packaged viral protease and can be inhibited by antiretroviral drugs of the protease inhibitor class. The various structural components then assemble to produce a mature HIV virion. Only mature virions are then able to infect another cell.
Spread in vivo.
HIV is now known to spread between CD4+ T cells by two parallel routes: cell-free spread and cell-to-cell spread, i.e. it employs hybrid spreading mechanisms. In the cell-free spread, virus particles bud from an infected T cell, enter the blood/extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread. Two pathways of cell-to-cell transmission have been reported. Firstly, an infected T cell can transmit virus directly to a target T cell via a virological synapse. Secondly, an antigen presenting cell (APC) can also transmit HIV to T cells by a process that either involves productive infection (in the case of macrophages) or capture and transfer of virions "in trans" (in the case of dendritic cells). Whichever pathway is used, infection by cell-to-cell transfer is reported to be much more efficient than cell-free virus spread. A number of factors contribute to this increased efficiency, including polarised virus budding towards the site of cell-to-cell contact, close apposition of cells which minimizes fluid-phase diffusion of virions, and clustering of HIV entry receptors on the target cell to the contact zone. Cell-to-cell spread is thought to be particularly important in lymphoid tissues where CD4+ T lymphocytes are densely packed and likely to frequently interact. Intravital imaging studies have supported the concept of the HIV virological synapse "in vivo". The hybrid spreading mechanisms of HIV contribute to the virus's ongoing replication against antiretroviral therapies.
Genetic variability.
HIV differs from many viruses in that it has very high genetic variability. This diversity is a result of its fast replication cycle, with the generation of about 1010 virions every day, coupled with a high mutation rate of approximately 3 x 10−5 per nucleotide base per cycle of replication and recombinogenic properties of reverse transcriptase.
This complex scenario leads to the generation of many variants of HIV in a single infected patient in the course of one day. This variability is compounded when a single cell is simultaneously infected by two or more different strains of HIV. When simultaneous infection occurs, the genome of progeny virions may be composed of RNA strands from two different strains. This hybrid virion then infects a new cell where it undergoes replication. As this happens, the reverse transcriptase, by jumping back and forth between the two different RNA templates, will generate a newly synthesized retroviral DNA sequence that is a recombinant between the two parental genomes. This recombination is most obvious when it occurs between subtypes.
The closely related simian immunodeficiency virus (SIV) has evolved into many strains, classified by the natural host species. SIV strains of the African green monkey (SIVagm) and sooty mangabey (SIVsmm) are thought to have a long evolutionary history with their hosts. These hosts have adapted to the presence of the virus, which is present at high levels in the host's blood but evokes only a mild immune response, does not cause the development of simian AIDS, and does not undergo the extensive mutation and recombination typical of HIV infection in humans.
In contrast, when these strains infect species that have not adapted to SIV ("heterologous" hosts such as rhesus or cynomologus macaques), the animals develop AIDS and the virus generates genetic diversity similar to what is seen in human HIV infection. Chimpanzee SIV (SIVcpz), the closest genetic relative of HIV-1, is associated with increased mortality and AIDS-like symptoms in its natural host. SIVcpz appears to have been transmitted relatively recently to chimpanzee and human populations, so their hosts have not yet adapted to the virus. This virus has also lost a function of the Nef gene that is present in most SIVs. For non-pathogenic SIV variants, Nef suppresses T-cell activation through the CD3 marker. Nef’s function in non-pathogenic forms of SIV is to downregulate expression of inflammatory cytokines, MHC-1, and signals that affect T cell trafficking. In HIV-1 and SIVcpz, Nef does not inhibit T-cell activation and it has lost this function. Without this function, T cell depletion is more likely, leading to immunodeficiency.
Three groups of HIV-1 have been identified on the basis of differences in the envelope ("env") region: M, N, and O. Group M is the most prevalent and is subdivided into eight subtypes (or clades), based on the whole genome, which are geographically distinct. The most prevalent are subtypes B (found mainly in North America and Europe), A and D (found mainly in Africa), and C (found mainly in Africa and Asia); these subtypes form branches in the phylogenetic tree representing the lineage of the M group of HIV-1. Coinfection with distinct subtypes gives rise to circulating recombinant forms (CRFs). In 2000, the last year in which an analysis of global subtype prevalence was made, 47.2% of infections worldwide were of subtype C, 26.7% were of subtype A/CRF02_AG, 12.3% were of subtype B, 5.3% were of subtype D, 3.2% were of CRF_AE, and the remaining 5.3% were composed of other subtypes and CRFs. Most HIV-1 research is focused on subtype B; few laboratories focus on the other subtypes. The existence of a fourth group, "P", has been hypothesised based on a virus isolated in 2009. The strain is apparently derived from gorilla SIV (SIVgor), first isolated from western lowland gorillas in 2006.
HIV-2’s closet relative is SIVsm, a strain of SIV found in Sooty Mangabees. Since HIV-1 is derived from SIVcpz, and HIV-2 from SIVsm, the genetic sequence of HIV-2 is only partially homologous to HIV-1 and more closely resembles that of SIVsm.
Diagnosis.
Many HIV-positive people are unaware that they are infected with the virus. For example, in 2001 less than 1% of the sexually active urban population in Africa had been tested, and this proportion is even lower in rural populations. Furthermore, in 2001 only 0.5% of pregnant women attending urban health facilities were counselled, tested or receive their test results. Again, this proportion is even lower in rural health facilities. Since donors may therefore be unaware of their infection, donor blood and blood products used in medicine and medical research are routinely screened for HIV.
HIV-1 testing is initially by an enzyme-linked immunosorbent assay (ELISA) to detect antibodies to HIV-1. Specimens with a nonreactive result from the initial ELISA are considered HIV-negative unless new exposure to an infected partner or partner of unknown HIV status has occurred. Specimens with a reactive ELISA result are retested in duplicate. If the result of either duplicate test is reactive, the specimen is reported as repeatedly reactive and undergoes confirmatory testing with a more specific supplemental test (e.g., Western blot or, less commonly, an immunofluorescence assay (IFA)). Only specimens that are repeatedly reactive by ELISA and positive by IFA or reactive by Western blot are considered HIV-positive and indicative of HIV infection. Specimens that are repeatedly ELISA-reactive occasionally provide an indeterminate Western blot result, which may be either an incomplete antibody response to HIV in an infected person or nonspecific reactions in an uninfected person.
Although IFA can be used to confirm infection in these ambiguous cases, this assay is not widely used. In general, a second specimen should be collected more than a month later and retested for persons with indeterminate Western blot results. Although much less commonly available, nucleic acid testing (e.g., viral RNA or proviral DNA amplification method) can also help diagnosis in certain situations. In addition, a few tested specimens might provide inconclusive results because of a low quantity specimen. In these situations, a second specimen is collected and tested for HIV infection.
Modern HIV testing is extremely accurate. A single screening test is correct more than 99% of the time. The chance of a false-positive result in standard two-step testing protocol is estimated to be about 1 in 250,000 in a low risk population. Testing post exposure is recommended initially and at six weeks, three months, and six months.
The latest recommendations of the CDC show that HIV testing must start with an immunoassay combination test for HIV-1 and HIV-2 antibodies and p24 antigen. A negative result rules out HIV exposure, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay to detect which is present. This gives rise to four possible scenarios:
Research.
HIV/AIDS research includes all medical research that attempts to prevent, treat, or cure HIV/AIDS, as well as fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.
Many governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions, such as research into sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and antiretroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, circumcision and HIV, and accelerated aging effects.
History.
Discovery.
AIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injection drug users and gay men with no known cause of impaired immunity who showed symptoms of "Pneumocystis carinii" pneumonia (PCP), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, additional gay men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PCP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak.
In the beginning, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used "Kaposi's Sarcoma and Opportunistic Infections", the name by which a task force had been set up in 1981. In the general press, the term "GRID", which stood for gay-related immune deficiency, had been coined. The CDC, in search of a name, and looking at the infected communities coined "the 4H disease," as it seemed to single out homosexuals, heroin users, hemophiliacs, and Haitians. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and "AIDS" was introduced at a meeting in July 1982. By September 1982 the CDC started using the name AIDS.
In 1983, two separate research groups led by Robert Gallo and Luc Montagnier independently declared that a novel retrovirus may have been infecting AIDS patients, and published their findings in the same issue of the journal "Science". Gallo claimed that a virus his group had isolated from an AIDS patient was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo's group called their newly isolated virus HTLV-III. At the same time, Montagnier's group isolated a virus from a patient presenting with swelling of the lymph nodes of the neck and physical weakness, two classic symptoms of AIDS. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986, LAV and HTLV-III were renamed HIV.
Origins.
Both HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa, and are believed to have transferred to humans (a process known as zoonosis) in the early 20th Century.
HIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIV(cpz) endemic in the chimpanzee subspecies "Pan troglodytes troglodytes"). The closest relative of HIV-2 is SIV (smm), a virus of the sooty mangabey ("Cercocebus atys atys"), an old world monkey living in litoral West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.
HIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.
There is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus, and it is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, it can only spread throughout the population in the presence of one or more of high-risk transmission channels, which are thought to have been absent in Africa prior to the 20th century.
Specific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including a higher degree of sexual promiscuity, the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are typically low, they are increased many fold if one of the partners suffers from a sexually transmitted infection resulting in genital ulcers. Early 1900s colonial cities were notable due to their high prevalence of prostitution and genital ulcers to the degree that as of 1928 as many as 45% of female residents of eastern Kinshasa were thought to have been prostitutes and as of 1933 around 15% of all residents of the same city were infected by one of the forms of syphilis.
An alternative view holds that unsafe medical practices in Africa during years following World War II, such as unsterile reuse of single use syringes during mass vaccination, antibiotic, and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.
The earliest well documented case of HIV in a human dates back to 1959 in the Congo. The virus may have been present in the United States as early as the mid-to-late 1950s, as a sixteen-year-old male presented with symptoms in 1966 died in 1969.
Further reading.
</dl>

</doc>
<doc id="14173" url="http://en.wikipedia.org/wiki?curid=14173" title="HOL">
HOL

Hol or HOL may refer to:

</doc>
<doc id="14174" url="http://en.wikipedia.org/wiki?curid=14174" title="Hostile witness">
Hostile witness

 <ns>10</ns>
 <id>2198199</id>
 <revision>
 <id>646505875</id>
 <parentid>645922985</parentid>
 <timestamp>2015-02-10T15:06:52Z</timestamp>
 <contributor>
 <username>Edokter</username>
 <id>1624037</id>
 </contributor>
 <minor />
 <comment>Reverted edits by () to last version by 24.131.80.54</comment>
 <model>wikitext</model>
 <format>text/x-wiki</format>
A hostile witness, otherwise known as an adverse witness or an unfavorable witness, is a witness at trial whose testimony on direct examination is either openly antagonistic or appears to be contrary to the legal position of the party who called the witness. 
Process.
During direct examination, if the examining attorney who called the witness finds that their testimony is antagonistic or contrary to the legal position of their client, the attorney may request that the judge declare the witness hostile. If the request is granted, the attorney may proceed to ask the witness leading questions. Leading questions either suggest the answer ("You saw my client sign the contract, correct?") or challenge (impeach) the witness' testimony. As a rule, leading questions are generally only allowed during cross-examination, but a hostile witness is an exception to this rule. 
In cross-examination conducted by the opposing party's attorney, a witness is presumed to be hostile and the examining attorney is not required to seek the judge's permission before asking leading questions. Attorneys can influence a hostile witness' responses by using Gestalt psychology to influence the way the witness perceives the situation, and utility theory to understand his likely responses. The attorney will integrate a hostile witness' expected responses into the larger case strategy through pretrial planning and through adapting as necessary during the course of the trial.

</doc>
<doc id="14179" url="http://en.wikipedia.org/wiki?curid=14179" title="Henry I of England">
Henry I of England

Henry I (c. 1068 – 1 December 1135), also known as Henry Beauclerc, was King of England from 1100 to 1135. Henry was the fourth son of William the Conqueror and was educated in Latin and the liberal arts. On William's death in 1087, Henry's older brothers, Robert Curthose and William Rufus, inherited Normandy and England respectively, but Henry was left landless. Henry purchased the County of Cotentin in western Normandy from Robert, but William and Robert deposed him in 1091. Henry gradually rebuilt his power base in the Cotentin and allied himself with William against Robert. Henry was present when William died in a hunting accident in 1100, and he seized the English throne, promising at his coronation to correct many of William's less popular policies. Henry married Matilda of Scotland but continued to have a large number of mistresses, by whom he had many illegitimate children.
Robert, who invaded in 1101, disputed Henry's control of England. This military campaign ended in a negotiated settlement that confirmed Henry as king. The peace was short-lived, and Henry invaded the Duchy of Normandy in 1105 and 1106, finally defeating Robert at the Battle of Tinchebray. Henry kept Robert imprisoned for the rest of his life. Henry's control of Normandy was challenged by Louis VI of France, Baldwin of Flanders and Fulk of Anjou, who promoted the rival claims of Robert's son, William Clito, and supported a major rebellion in the Duchy between 1116 and 1119. Following Henry's victory at the Battle of Brémule, a favourable peace settlement was agreed with Louis in 1120.
Considered by contemporaries to be a harsh but effective ruler, Henry skilfully manipulated the barons in England and Normandy. In England, he drew on the existing Anglo-Saxon system of justice, local government and taxation, but also strengthened it with additional institutions, including the royal exchequer and itinerant justices. Normandy was also governed through a growing system of justices and an exchequer. Many of the officials that ran Henry's system were "new men" of obscure backgrounds rather than from families of high status, who rose through the ranks as administrators. Henry encouraged ecclesiastical reform, but became embroiled in a serious dispute in 1101 with Archbishop Anselm of Canterbury, which was resolved through a compromise solution in 1105. He supported the Cluniac order and played a major role in the selection of the senior clergy in England and Normandy.
Henry's only legitimate son and heir, William Adelin, drowned in the "White Ship" disaster of 1120, throwing the royal succession into doubt. Henry took a second wife, Adeliza, in the hope of having another son, but their marriage was childless. In response to this, Henry declared his daughter, Matilda, as his heir and married her to Geoffrey of Anjou. Relationships between Henry and the couple became strained, and fighting broke out along the border with Anjou. Henry died on 1 December 1135 after a week of illness. Despite his plans for Matilda, the King was succeeded by his nephew, Stephen of Blois, resulting in a period of civil war known as the Anarchy.
Early life, 1068–1099.
Childhood and appearance, 1068–86.
Henry was probably born in England in 1068, in either the summer or the last weeks of the year, possibly in the town of Selby in Yorkshire. His father was William, who had originally been the Duke of Normandy and then, following the invasion of 1066, became the King of England, with lands stretching into Wales. The invasion had created an Anglo-Norman elite, many with estates spread across both sides of the English Channel. These Anglo-Norman barons typically had close links to the kingdom of France, which was then a loose collection of counties and smaller polities, under only the minimal control of the king. Henry's mother, Matilda of Flanders, was the granddaughter of Robert II of France, and she probably named Henry after her uncle, King Henry I of France.
Henry was the youngest of William and Matilda's four sons. Physically he resembled his older brothers Robert Curthose, Richard and William Rufus, being, as historian David Carpenter describes, "short, stocky and barrel-chested," with black hair. As a result of their age differences and Richard's early death, Henry would have probably seen relatively little of his older brothers. He probably knew his sister, Adela, well, as the two were close in age. There is little documentary evidence for his early years; historians Warren Hollister and Kathleen Thompson suggest he was brought up predominantly in England, while Judith Green argues he was initially brought up in the Duchy. He was probably educated by the Church, possibly by Bishop Osmund, the King's chancellor, at Salisbury Cathedral; it is uncertain if this indicated an intent by his parents for Henry to become a member of the clergy. It is also uncertain how far Henry's education extended, but he was probably able to read Latin and had some background in the liberal arts. He was given military training by an instructor called Robert Achard, and Henry was knighted by his father on 24 May 1086.
Inheritance, 1087–88.
In 1087 William was fatally injured during a campaign in the Vexin. Henry joined his dying father near Rouen in September, where the King partitioned his possessions between his sons. The rules of succession in western Europe at the time were uncertain; in some parts of France, primogeniture, in which the eldest son would inherit a title, was growing in popularity. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands – usually considered to be the most valuable – and younger sons given smaller, or more recently acquired, partitions or estates.
In dividing his lands, William appears to have followed the Norman tradition, distinguishing between Normandy, which he had inherited, and England, which he had acquired through war. William's second son, Richard, had died in a hunting accident, leaving Henry and his two brothers to inherit William's estate. Robert, the eldest, despite being in armed rebellion against his father at the time of his death, received Normandy. England was given to William Rufus, who was in favour with the dying king. Henry was given a large sum of money, usually reported as £5,000, with the expectation that he would also be given his mother's modest set of lands in Buckinghamshire and Gloucestershire. William's funeral at Caen was marred by angry complaints from a local man, and Henry may have been responsible for resolving the dispute by buying off the protester with silver.
Robert returned to Normandy, expecting to have been given both the Duchy and England, to find that William Rufus had crossed the Channel and been crowned king, as William II. The two brothers disagreed fundamentally over the inheritance, and Robert soon began to plan an invasion of England to seize the kingdom, helped by a rebellion by some of the leading nobles against William Rufus. Henry remained in Normandy and took up a role within Robert's court, possibly either because he was unwilling to openly side with William Rufus, or because Robert might have taken the opportunity to confiscate Henry's inherited money if he had tried to leave. William Rufus sequestered Henry's new estates in England, leaving Henry landless.
In 1088, Robert's plans for the invasion of England began to falter, and he turned to Henry, proposing that his brother lend him some of his inheritance, which Henry refused. Henry and Robert then came to an alternative arrangement, in which Robert would make Henry the count of western Normandy, in exchange for £3,000. Henry's lands were a new countship based around a delegation of the ducal authority in the Cotentin, but it extended across the Avranchin, with control over the bishoprics of both. This also gave Henry influence over two major Norman leaders, Hugh d'Avranches and Richard de Redvers, and the abbey of Mont Saint-Michel, whose lands spread out further across the Duchy. Robert's invasion force failed to leave Normandy, leaving William Rufus secure in England.
Count of the Cotentin, 1088–90.
Henry quickly established himself as count, building up a network of followers from western Normandy and eastern Brittany, whom historian John Le Patourel has characterised as "Henry's gang". His early supporters included Roger of Mandeville, Richard of Redvers, Richard d'Avranches and Robert Fitzhamon, along with the churchman Roger of Salisbury. Robert attempted to go back on his deal with Henry and re-appropriate the county, but Henry's grip was already sufficiently firm to prevent this. Robert's rule of the Duchy was chaotic, and parts of Henry's lands became almost independent of central control from Rouen.
During this period, neither William nor Robert seems to have trusted Henry. Waiting until the rebellion against William Rufus was safely over, Henry returned to England in July 1088. He met with the King but was unable to persuade him to grant him his mother's estates, and travelled back to Normandy in the autumn. While he had been away, however, Odo, the Bishop of Bayeux, who regarded Henry as a potential competitor, had convinced Robert that Henry was conspiring against the duke with William Rufus. On landing, Odo seized Henry and imprisoned him in Neuilly-la-Forêt, and Robert took back the county of the Cotentin. Henry was held there over the winter, but in the spring of 1089 the senior elements of the Normandy nobility prevailed upon Robert to release him.
Although no longer formally the Count of Cotentin, Henry continued to control the west of Normandy. The struggle between Henry's brothers continued. William Rufus continued to put down resistance to his rule in England, but began to build a number of alliances against Robert with barons in Normandy and neighbouring Ponthieu. Robert allied himself with Philip I of France. In late 1090 William Rufus encouraged Conan Pilatus, a powerful burgher in Rouen, to rebel against Robert; Conan was supported by most of Rouen and made appeals to the neighbouring ducal garrisons to switch allegiance as well.
Robert issued an appeal for help to his barons, and Henry was the first to arrive in Rouen in November. Violence broke out, leading to savage, confused street fighting as both sides attempted to take control of the city. Robert and Henry left the castle to join the battle, but Robert then retreated, leaving Henry to continue the fighting. The battle turned in favour of the ducal forces and Henry took Conan prisoner. Henry was angry that Conan had turned against his feudal lord. He had him taken to the top of Rouen Castle and then, despite Conan's offers to pay a huge ransom, threw him off the top of the castle to his death. Contemporaries considered Henry to have acted appropriately in making an example of Conan, and Henry became famous for his exploits in the battle.
Fall and rise, 1091–99.
In the aftermath, Robert forced Henry to leave Rouen, probably because Henry's role in the fighting had been more prominent than his own, and possibly because Henry had asked to be formally reinstated as the count of the Cotentin. In early 1091, William Rufus invaded Normandy with a sufficiently large army to bring Robert to the negotiating table. The two brothers signed a treaty at Rouen, granting William Rufus a range of lands and castles in Normandy. In return, William Rufus promised to support Robert's attempts to regain control of the neighbouring county of Maine, once under Norman control, and help in regaining control over the Duchy, including Henry's lands. They nominated each other as heirs to England and Normandy, excluding Henry from any succession while either one of them lived.
War now broke out between Henry and his brothers. Henry mobilised a mercenary army in the west of Normandy, but as William Rufus and Robert's forces advanced, his network of baronial support melted away. Henry focused his remaining forces at Mont Saint Michel, where he was besieged, probably in March 1091. The site was easy to defend, but lacked fresh water. The chronicler William of Malmesbury suggested that when Henry's water ran short, Robert allowed his brother fresh supplies, leading to remonstrations between Robert and William Rufus. The events of the final days of the siege are unclear: the besiegers had begun to argue about the future strategy for the campaign, but Henry then abandoned Mont-Saint Michel, probably as part of a negotiated surrender. He left for Brittany and crossed over into France.
Henry's next steps are not well documented; one chronicler, Orderic Vitalis, suggests that he travelled in the French Vexin, along the Normandy border, for over a year with a small band of followers. By the end of the year, Robert and William Rufus had fallen out once again, and the Treaty of Rouen had been abandoned. In 1092, Henry and his followers seized the Normandy town of Domfront. Domfront had previously been controlled by Robert of Bellême, but the inhabitants disliked his rule and invited Henry to take over the town, which he did in a bloodless coup. Over the next two years, Henry re-established his network of supporters across western Normandy, forming what Judith Green terms a "court in waiting". By 1094, he was allocating lands and castles to his followers as if he was the Duke of Normandy. William Rufus began to support Henry with money, encouraging his campaign against Robert, and Henry used some of this to construct a substantial castle at Domfront.
William Rufus crossed into Normandy to take the war to Robert in 1094, and when progress stalled, called upon Henry for assistance. Henry responded, but travelled to London instead of joining the main campaign further east in Normandy, possibly at the request of the King, who in any event abandoned the campaign and returned to England. Over the next few years, Henry appears to have strengthened his power base in western Normandy, visiting England occasionally to attend at William Rufus's court. In 1095 Pope Urban II called the First Crusade, encouraging knights from across Europe to join. Robert joined the Crusade, borrowing money from William Rufus to do so, and granting the King temporary custody of his part of the Duchy in exchange. The King appeared confident of regaining the remainder of Normandy from Robert, and Henry appeared ever closer to William Rufus, the pair campaigning together in the Norman Vexin between 1097 and 1098.
Early reign, 1100–06.
Taking the throne, 1100.
Henry became King of England following the death of William Rufus, who had been shot while hunting. On the afternoon of 2 August 1100, the King had gone hunting in the New Forest, accompanied by a team of huntsmen and a number of the Norman nobility, including Henry. An arrow was fired, possibly by the baron Walter Tirel, which hit and killed William Rufus. Numerous conspiracy theories have been put forward suggesting that the King was killed deliberately; most modern historians reject these, as hunting was a risky activity, and such accidents were common. Chaos broke out, and Tirel fled the scene for France, either because he had fired the fatal shot, or because he had been incorrectly accused and feared that he would be made a scapegoat for the King's death.
Henry rode to Winchester, where an argument ensued as to who now had the best claim to the throne. William of Breteuil championed the rights of Robert, who was still abroad, returning from the Crusade, and to whom Henry and the barons had given homage in previous years. Henry argued that, unlike Robert, he had been born to a reigning king and queen, thereby giving him a claim under the right of porphyrogeniture. Tempers flared, but Henry, supported by Henry de Beaumont and Robert of Meulan, held sway and persuaded the barons to follow him. He occupied Winchester Castle and seized the royal treasury.
Henry was hastily crowned king in Westminster Abbey on 5 August by Maurice, the Bishop of London, as Anselm, the Archbishop of Canterbury, had been exiled by William Rufus, and Thomas, the Archbishop of York, was in the north of England at Ripon. In accordance with English tradition and in a bid to legitimise his rule, Henry issued a coronation charter laying out various commitments. The new king presented himself as having restored order to a trouble-torn country. He announced that he would abandon William Rufus's policies towards the Church, which had been seen as oppressive by the clergy; he promised to prevent royal abuses of the barons' property rights, and assured a return to the gentler customs of Edward the Confessor; he asserted that he would "establish a firm peace" across England and ordered "that this peace shall henceforth be kept".
In addition to his existing circle of supporters, many of whom were richly rewarded with new lands, Henry quickly co-opted many of the existing administration into his new royal household. William Giffard, William Rufus's chancellor, was made the Bishop of Winchester, and the prominent sheriffs Urse d'Abetot, Haimo Dapifer and Robert Fitzhamon continued to play a senior role in government. By contrast, the unpopular Ranulf Flambard, the Bishop of Durham and a key member of the previous regime, was imprisoned in the Tower of London and charged with corruption. The late king had left many church positions unfilled, and Henry set about nominating candidates to these, in an effort to build further support for his new government. The appointments needed to be consecrated, and Henry wrote to Anselm, apologising for having been crowned while the Archbishop was still in France and asking him to return at once.
Marriage to Matilda, 1100.
On 11 November 1100 Henry married Matilda, the daughter of Malcolm III of Scotland. Henry was now around 31 years old, but late marriages for noblemen were not unusual in the 11th century. The pair had probably first met earlier the previous decade, possibly being introduced through Bishop Osmund of Salisbury. Historian Warren Hollister argues that Henry and Matilda were emotionally close, but their union was also certainly politically motivated. Matilda had originally been named Edith, an Anglo-Saxon name, and was a member of the West Saxon royal family, being the niece of Edgar the Ætheling, the great-granddaughter of Edmund Ironside and a descendent of Alfred the Great. For Henry, marrying Matilda gave his reign increased legitimacy, and for Matilda, an ambitious woman, it was an opportunity for high status and power in England.
Matilda had been educated in a sequence of convents, however, and may well have taken the vows to formally become a nun, which formed an obstacle to the marriage progressing. She did not wish to be a nun and appealed to Anselm for permission to marry Henry, and the Archbishop established a council at Lambeth Palace to judge the issue. Despite some dissenting voices, the council concluded that although Matilda had lived in a convent, she had not actually become a nun and was therefore free to marry, a judgement that Anselm then affirmed, allowing the marriage to proceed. Matilda proved an effective queen for Henry, acting as a regent in England on occasion, addressing and presiding over councils, and extensively supporting the arts. The couple soon had two children, Matilda, born in 1102, and William Adelin, born in 1103; it is possible that they also had a second son, Richard, who died young. Following the birth of these children, Matilda preferred to remain based in Westminster while Henry travelled across England and Normandy, either for religious reasons or because she enjoyed being involved in the machinery of royal governance.
Henry had a considerable sexual appetite and enjoyed a substantial number of sexual partners, resulting in a large number of illegitimate children, at least nine sons and 13 daughters, many of whom he appears to have recognised and supported. It was normal for unmarried Anglo-Norman noblemen to have sexual relations with prostitutes and local women, and kings were also expected to have mistresses. Some of these relationships occurred before Henry was married, but many others took place after his marriage to Matilda. Henry had a wide range of mistresses from a range of backgrounds, and the relationships appear to have been conducted relatively openly. He may have chosen some of his noble mistresses for political purposes, but the evidence to support this theory is limited.
Treaty of Alton, 1101–02.
By early 1101, Henry's new regime was established and functioning, but many of the Anglo-Norman elite still supported Robert, or would be prepared to switch sides if Henry's elder brother appeared likely to gain power in England. In February, Flambard escaped from the Tower of London and crossed the Channel to Normandy, where he injected fresh direction and energy to Robert's attempts to mobilise an invasion force. By July, Robert had formed an army and a fleet, ready to move against Henry in England. Raising the stakes in the conflict, Henry seized Flambard's lands and, with the support of Anselm, Flambard was removed from his position as bishop. Henry held court in April and June, where the nobility renewed their oaths of allegiance to him, but their support still appeared partial and shaky.
With the invasion imminent, Henry mobilised his forces and fleet outside Pevensey, close to Robert's anticipated landing site, training some of them personally in how to counter cavalry charges. Despite English levies and knights owing military service to the Church arriving in considerable numbers, many of his barons did not appear. Anselm intervened with some of the doubters, emphasising the religious importance of their loyalty to Henry. Robert unexpectedly landed further up the coast at Portsmouth on 20 July with a modest force of a few hundred men, but these were quickly joined by many of the barons in England. However, instead of marching into nearby Winchester and seizing Henry's treasury, Robert paused, giving Henry time to march west and intercept the invasion force.
The two armies met at Alton where peace negotiations began, possibly initiated by either Henry or Robert, and probably supported by Flambard. The brothers then agreed to the Treaty of Alton, under which Robert released Henry from his oath of homage and recognised him as king; Henry renounced his claims on western Normandy, except for Domfront, and agreed to pay Robert £2,000 a year for life; if either brother died without a male heir, the other would inherit his lands; the barons whose lands had been seized by either the King or the Duke for supporting his rival would have them returned, and Flambard would be reinstated as bishop; the two brothers would campaign together to defend their territories in Normandy. Robert remained in England for a few months more with Henry before returning to Normandy.
Despite the treaty, Henry set about inflicting severe penalties on the barons who had stood against him during the invasion. William de Warenne, the Earl of Surrey, was accused of fresh crimes, which were not covered by the Alton amnesty, and was banished from England. In 1102 Henry then turned against Robert of Bellême and his brothers, the most powerful of the barons, accusing him of 45 different offences. Robert escaped and took up arms against Henry. Henry besieged Robert's castles at Arundel, Tickhill and Shrewsbury, pushing down into the south-west to attack Bridgnorth. His power base in England broken, Robert accepted Henry's offer of banishment and left the country for Normandy.
Conquest of Normandy, 1103–06.
Henry's network of allies in Normandy became stronger during 1103. Henry married Juliana, one of his illegitimate daughters, to Eustace of Breteuil, and another illegitimate daughter, Matilda, to Rotrou, the Count of Perche, on the Normandy border. Henry attempted to win over other members of the Normandy nobility and gave other English estates and lucrative offers to key Norman lords. Duke Robert continued to fight Robert of Bellême, but the Duke's position worsened, until by 1104, he had to ally himself formally with Bellême to survive. Arguing that Duke Robert had broken the terms of their treaty, Henry crossed over the Channel to Domfront, where he met with senior barons from across Normandy, eager to ally themselves with the King. Henry confronted his brother and accused him of siding with his enemies, before returning to England.
Normandy continued to disintegrate into chaos. In 1105, Henry sent his friend Robert Fitzhamon and a force of knights into the Duchy, apparently to provoke a confrontation with Duke Robert. Fitzhamon was captured, and Henry used this as an excuse to invade, promising to restore peace and order. Henry had the support of most of the neighbouring counts around Normandy's borders, and King Philip of France was persuaded to remain neutral. Henry occupied western Normandy, and advanced east on Bayeux, where Fitzhamon was held. The city refused to surrender, and Henry besieged it, burning it to the ground. Terrified of meeting the same fate, the town of Caen switched sides and surrendered, allowing Henry to advance on Falaise, which he took with some casualties. Henry's campaign stalled, and the King instead began peace discussions with Robert. The negotiations were inconclusive and the fighting dragged on until Christmas, when Henry returned to England.
Henry invaded again in July 1106, hoping to provoke a decisive battle. After some initial tactical successes, he turned south-west towards the castle of Tinchebray. He besieged the castle and Duke Robert, supported by Robert of Bellême, advanced from Falaise to relieve it. After attempts at negotiation failed, the Battle of Tinchebray took place, probably on 28 September. The battle lasted around an hour, and began with a charge by Duke Robert's cavalry; the infantry and dismounted knights of both sides then joined the battle. Henry's reserves, led by Elias, the Count of Maine and Alan, the Duke of Brittany, attacked the enemy's flanks, routing first Bellême's troops and then the bulk of the ducal forces. Duke Robert was taken prisoner, but Bellême escaped.
Henry mopped up the remaining resistance in Normandy, and Robert ordered his last garrisons to surrender. Reaching Rouen, Henry reaffirmed the laws and customs of Normandy and took homage from the leading barons and citizens. The lesser prisoners taken at Tinchebray were released, but Robert and several other leading nobles were imprisoned indefinitely. Henry's nephew, Robert's son William Clito, was only three years old and was released to the care of Helias of Saint-Saens, a Norman baron. Henry reconciled himself with Robert of Bellême, who gave up the ducal lands he had seized and rejoined the royal court. Henry had no way of legally removing the Duchy from his brother Robert, and initially Henry avoiding using the title "duke" at all, emphasising that, as the King of England, he was only acting as the guardian of the troubled Duchy.
Government, family and household.
Government, law and court.
Henry inherited the kingdom of England from William Rufus, giving him a claim of suzerainty over Wales and Scotland, and acquired the Duchy of Normandy, a complex entity with troubled borders. The borders between England and Scotland were still uncertain during Henry's reign, with Anglo-Norman influence pushing northwards through Cumbria, but Henry's relationship with King David of Scotland was generally good, partially due to Henry's marriage to his daughter. In Wales, Henry used his power to coerce and charm the indigenous Welsh princes, while Norman Marcher Lords pushed across the valleys of South Wales. Normandy was controlled via various interlocking networks of ducal, ecclesiastical and family contacts, backed by a growing string of important ducal castles along the borders. Alliances and relationships with neighbouring counties along the Norman border were particularly important to maintaining the stability of the Duchy.
Henry ruled through the various barons and lords in England and Normandy, whom he manipulated skilfully for political effect. Political friendships, termed "amicitia" in Latin, were important during the 12th century, and Henry maintained a wide range of these, mediating between his friends in various factions across his realm when necessary, and rewarding those who were loyal to him. Henry also had a reputation for punishing those barons who stood against him, and he maintained an effective network of informers and spies who reported to him on events. Henry was a harsh, firm ruler, but not excessively so by the standards of the day. Over time, he increased the degree of his control over the barons, removing his enemies and bolstering his friends until the "reconstructed baronage", as historian Warren Hollister describes it, was predominantly loyal and dependent on the King.
Henry's itinerant royal court comprised various parts. At the heart was Henry's domestic household, called the "domus"; a wider grouping was termed the "familia regis", and formal gatherings of the court were termed "curia". The "domus" was divided into several parts. The chapel, headed by the chancellor, looked after the royal documents, the chamber dealt with financial affairs and the master-marshal was responsible for travel and accommodation. The "familia regis" included Henry's mounted household troops, up to several hundred strong, who came from a wider range of social backgrounds, and could be deployed across England and Normandy as required. Initially Henry continued his father's practice of regular crown-wearing ceremonies at his "curia", but they became less frequent as the years passed. Henry's court was grand and ostentatious, financing the construction of large new buildings and castles with a range of precious gifts on display, including the King's private menagerie of exotic animals, which he kept at Woodstock Palace. Despite being a lively community, Henry's court was more tightly controlled than those of previous kings. Strict rules controlled personal behaviour and prohibited members of the court from pillaging neighbouring villages, as had been the norm under William Rufus.
Henry was responsible for a substantial expansion of the royal justice system. In England, Henry drew on the existing Anglo-Saxon system of justice, local government and taxes, but strengthened it with additional central governmental institutions. Roger of Salisbury began to develop the royal exchequer after 1110, using it to collect and audit revenues from the King's sheriffs in the shires. Itinerant justices began to emerge under Henry, travelling around the country managing eyre courts, and many more laws were formally recorded. Henry gathered increasing revenue from the expansion of royal justice, both from fines and from fees. The first Pipe Roll that is known to have survived dates from 1130, recording royal expenditures. Henry reformed the coinage in 1107, 1108 and in 1125, inflicting harsh corporal punishments to English coiners who had been found guilty of debasing the currency. In Normandy, Henry restored law and order after 1106, operating through a body of Norman justices and an exchequer system similar to that in England. Norman institutions grew in scale and scope under Henry, although less quickly than in England. Many of the officials that ran Henry's system were termed "new men", relatively low-born individuals who rose through the ranks as administrators, managing justice or the royal revenues.
Relations with the church.
Church and the King.
Henry's ability to govern was intimately bound up with the Church, which formed the key to the administration of both England and Normandy, and this relationship changed considerably over the course of his reign. William the Conqueror had reformed the English Church with the support of his Archbishop of Canterbury, Lanfranc, who became a close colleague and advisor to the King. Under William Rufus this arrangement had collapsed, the King and Archbishop Anselm had become estranged and Anselm had gone into exile. Henry also believed in Church reform, but on taking power in England he became embroiled in the investiture controversy.
The argument concerned who should invest a new bishop with his staff and ring: traditionally, this had been carried out by the king in a symbolic demonstration of royal power, but Pope Urban II had condemned this practice in 1099, arguing that only the papacy could carry out this task, and declaring that the clergy should not give homage to their local temporal rulers. Anselm returned to England from exile in 1100 having heard Urban's pronouncement, and informed Henry that he would be complying with the Pope's wishes. Henry was in a difficult position. On one hand, the symbolism and homage was important to him; on the other hand, he needed Anselm's support in his struggle with his brother Duke Robert.
Anselm stuck firmly to the letter of the papal decree, despite Henry's attempts to persuade him to give way in return for a vague assurance of a future royal compromise. Matters escalated, with Anselm going back into exile and Henry confiscating the revenues of his estates. Anselm threatened excommunication, and in July 1105 the two men finally negotiated a solution. A distinction was drawn between the secular and ecclesiastical powers of the prelates, under which Henry gave up his right to invest his clergy, but retained the custom of requiring them to come and do homage for the temporalities, the landed properties they held in England. Despite this argument, the pair worked closely together, combining to deal with Duke Robert's invasion of 1101, for example, and holding major reforming councils in 1102 and 1108.
A long-running dispute between the Archbishops of Canterbury and York flared up under Anselm's successor, Ralph d'Escures. Canterbury, traditionally the senior of the two establishments, had long argued that the Archbishop of York should formally promise to obey their Archbishop, but York argued that the two episcopates were independent within the English Church and that no such promise was necessary. Henry supported the primacy of Canterbury, to ensure that England remained under a single ecclesiastical administration, but the Pope preferred the case of York. The matter was complicated by Henry's personal friendship with Thurstan, the Archbishop of York, and the King's desire that the case should not end up in a papal court, beyond royal control. Henry badly needed the support of the Papacy in his struggle with Louis of France, however, and therefore allowed Thurstan to attend the Council of Rheims in 1119, where Thurstan was then consecrated by the Pope with no mention of any duty towards Canterbury. Henry believed that this went against assurances Thurstan had previously made and exiled him from England until the King and Archbishop came to a negotiated solution the following year.
Even after the investiture dispute, the King continued to play a major role in the selection of new English and Norman bishops and archbishops. Henry appointed many of his officials to bishoprics and, as historian Martin Brett suggests, "some of his officers could look forward to a mitre with all but absolute confidence". Henry's chancellors, and those of his queens, became bishops of Durham, Hereford, London, Lincoln, Winchester and Salisbury. Henry increasingly drew on a wider range of these bishops as advisors – particularly Roger of Salisbury – breaking with the earlier tradition of relying primarily on the Archbishop of Canterbury. The result was a cohesive body of administrators through which Henry could exercise careful influence, holding general councils to discuss key matters of policy. This stability shifted slightly after 1125, when Henry began to inject a wider range of candidates into the senior positions of the Church, often with more reformist views, and the impact of this generation would be felt in the years after Henry's death.
Personal beliefs and piety.
Like other rulers of the period, Henry donated to the Church and patronised various religious communities, but contemporary chroniclers did not consider him an unusually pious king. His personal beliefs and piety may, however, have developed during the course of his life. Henry had always taken an interest in religion, but in his later years he may have become much more concerned about spiritual affairs. If so, the major shifts in his thinking would appear to have occurred after 1120, when his son William Adelin died, and 1129, when his daughter's marriage teetered on the verge of collapse.
As a proponent of religious reform, Henry gave extensively to reformist groups within the Church. He was a keen supporter of the Cluniac order, probably for intellectual reasons. He donated money to the abbey at Cluny itself, and after 1120 gave generously to Reading Abbey, a Cluniac establishment. Construction on Reading began in 1121, and Henry endowed it with rich lands and extensive privileges, making it a symbol of his dynastic lines. He also focused effort on promoting the conversion of communities of clerks into Augustinian canons, the foundation of leper hospitals, expanding the provision of nunneries, and the charismatic orders of the Savigniacs and Tironensians. He was a keen collector of relics, sending an embassy to Constantinople in 1118 to collect Byzantine items, some of which were donated to Reading Abbey.
Later reign, 1107–35.
Continental and Welsh politics, 1108–14.
Normandy faced an increased threat from France, Anjou and Flanders after 1108. Louis VI succeeded to the French throne in 1108 and began to reassert central royal power. Louis demanded Henry give homage to him and that two disputed castles along the Normandy border be placed into the control of neutral castellans. Henry refused, and Louis responded by mobilising an army. After some arguments, the two kings negotiated a truce and retreated without fighting, leaving the underlying issues unresolved. Fulk V assumed power in Anjou in 1109 and began to rebuild Angevin authority. Fulk also inherited the county of Maine, but refused to recognise Henry as his feudal lord and instead allied himself with Louis. Robert II of Flanders also briefly joined the alliance, before his death in 1111.
In 1108, Henry betrothed his eight-year-old daughter, Matilda, to Henry V, the future Holy Roman Emperor. For King Henry, this was a prestigious match; for Henry V, it was an opportunity to restore his financial situation and fund an expedition to Italy, as he received a dowry of £6,666 from England and Normandy. Raising this money proved challenging, and required the implementation of a special "aid", or tax, in England. Matilda was crowned Henry V's queen in 1110.
Henry responded to the French and Angevin threat by expanding his own network of supporters beyond the Norman borders. Some Norman barons deemed unreliable were arrested or dispossessed, and Henry used their forfeited estates to bribe his potential allies in the neighbouring territories, in particular Maine. Around 1110, Henry attempted to arrest the young William Clito, but William's mentors moved him to the safety of Flanders before he could be taken. At about this time, Henry probably began to style himself as the Duke of Normandy. Robert of Bellême turned against Henry once again, and when he appeared at Henry's court in 1112 in a new role as a French ambassador, he was arrested and imprisoned.
Rebellions broke out in France and Anjou between 1111 and 1113, and Henry crossed into Normandy to support his nephew, Count Theobald of Blois, who had sided against Louis in the uprising. In a bid to diplomatically isolate the French King, Henry betrothed his young son, William Adelin, to Fulk's daughter Matilda, and married his illegitimate daughter Matilda to Conan III, the Duke of Brittany, creating alliances with Anjou and Brittany respectively. Louis backed down and in March 1113 met with Henry near Gisors to agree a peace settlement, giving Henry the disputed fortresses and confirming Henry's overlordship of Maine, Bellême and Brittany.
Meanwhile, the situation in Wales was deteriorating. Henry had conducted a campaign in South Wales in 1108, pushing out royal power in the region and colonising the area around Pembroke with Flemings. By 1114, some of the resident Norman lords were under attack, while in Mid-Wales, Owain ap Cadwgan blinded one of the political hostages he was holding, and in North Wales Gruffudd ap Cynan threatened the power of the Earl of Chester. Henry sent three armies into Wales that year, with Gilbert Fitz Richard leading a force from the south, Alexander, King of Scotland, pressing from the north and Henry himself advancing into Mid-Wales. Owain and Gruffudd sued for peace, and Henry accepted a political compromise. Henry reinforced the Welsh Marches with his own appointees, strengthening the border territories.
Rebellion, 1115–20.
Concerned about the succession, Henry sought to persuade Louis VI to accept his son, William Adelin, as the legitimate future Duke of Normandy, in exchange for his son's homage. Henry crossed into Normandy in 1115 and assembled the Norman barons to swear loyalty; he also almost successfully negotiated a settlement with King Louis, affirming William's right to the Duchy in exchange for a large sum of money, but the deal fell through and Louis, backed by his ally Baldwin of Flanders, instead declared that he considered William Clito the legitimate heir to the Duchy.
War broke out after Henry returned to Normandy with an army to support Theobald of Blois, who was under attack from Louis. Henry and Louis raided each other's towns along the border, and a wider conflict then broke out, probably in 1116. Henry was pushed onto the defensive as French, Flemish and Angevin forces began to pillage the Normandy countryside. Amaury III of Montfort and many other barons rose up against Henry, and there was an assassination plot from within his own household. Henry's wife, Matilda, died in early 1118, but the situation in Normandy was sufficiently pressing that Henry was unable to return to England for her funeral.
Henry responded by mounting campaigns against the rebel barons and deepening his alliance with Theobald. Baldwin of Flanders was wounded in battle and died in September 1118, easing the pressure on Normandy from the north-east. Henry attempted to crush a revolt in the city of Alençon, but was defeated by Fulk and the Angevin army. Forced to retreat from Alençon, Henry's position deteriorated alarmingly, as his resources became overstretched and more barons abandoned his cause. Early in 1119, Eustace of Breteuil and Henry's daughter, Juliana, threatened to join the baronial revolt. Hostages were exchanged in a bid to avoid conflict, but relations broke down and both sides mutilated their captives. Henry attacked and took the town of Breteuil, despite Juliana's attempt to kill her father with a crossbow. In the aftermath, Henry dispossessed the couple of almost all of their lands in Normandy.
Henry's situation improved in May 1119 when he enticed Fulk to switch sides by finally agreeing to marry William Adelin to Fulk's daughter, Matilda, and paying Fulk a large sum of money. Fulk left for the Levant, leaving the County of Maine in Henry's care, and the King was free to focus on crushing his remaining enemies. During the summer Henry advanced into the Norman Vexin, where he encountered Louis's army, resulting in the Battle of Brémule. Henry appears to have deployed scouts and then organised his troops into several carefully formed lines of dismounted knights. Unlike Henry's forces, the French knights remained mounted; they hastily charged the Anglo-Norman positions, breaking through the first rank of the defences but then becoming entangled in Henry's second line of knights. Surrounded, the French army began to collapse. In the melee, Henry was hit by a sword blow, but his armour protected him. Louis and William Clito escaped from the battle, leaving Henry to return to Rouen in triumph.
The war slowly petered out after this battle, and Louis took the dispute over Normandy to Pope Callixtus II's council in Reims that October. Henry faced a number of French complaints concerning his acquisition and subsequent management of Normandy, and despite being defended by Geoffrey, the Archbishop of Rouen, Henry's case was shouted down by the pro-French elements of the council. Callixtus declined to support Louis, however, and merely advised the two rulers to seek peace. Amaury de Montfort came to terms with Henry, but Henry and William Clito failed to find a mutually satisfactory compromise. In June 1120, Henry and Louis formally made peace on terms advantageous to the English King: William Adelin gave homage to Louis, and in return Louis confirmed William's rights to the Duchy.
Succession crisis, 1120–23.
Henry's succession plans were thrown into chaos by the sinking of the "White Ship" on 25 November 1120. Henry had left the port of Barfleur for England in the early evening, leaving William Adelin and many of the younger members of the court to follow on that night in a separate vessel, the "White Ship". Both the crew and passengers were drunk and, just outside the harbour, the ship hit a submerged rock. The ship sank, killing as many as 300 people, with only one survivor, a butcher from Rouen. Henry's court was initially too scared to report William's death to the King. When he was finally told, he collapsed with grief.
The disaster left Henry with no legitimate son, his various nephews now the closest male heirs. Henry announced he would take a new wife, Adeliza of Louvain, opening up the prospect of a new royal son, and the two were married at Windsor Castle in January 1121. Henry appears to chosen her because she was attractive and came from a prestigious noble line. Adela seems to have been fond of Henry and joined him in his travels, probably to maximise the chances of her conceiving a child. The "White Ship" disaster initiated fresh conflict in Wales, where the drowning of Richard, Earl of Chester, encouraged a rebellion led by Maredudd ap Bleddyn. Henry intervened in North Wales that summer with an army and, although the King was hit by a Welsh arrow, the campaign reaffirmed royal power across the region.
With William dead, Henry's alliance with Anjou – which had been based on his son marrying Fulk's daughter – began to disintegrate. Fulk returned from the Levant and demanded that Henry return Matilda and her dowry, a range of estates and fortifications in Maine. Matilda left for Anjou, but Henry argued that the dowry had in fact originally belonged to him before it came into the possession of Fulk, and so declined to hand the estates back to Anjou. Fulk married his daughter Sibylla to William Clito, and granted them Maine. Once again, conflict broke out, as Amaury de Montfort allied himself with Fulk and led a revolt along the Norman-Anjou border in 1123. Amaury was joined by several other Norman barons, headed by Waleran de Beaumont, one of the sons of Henry's old ally, Robert of Meulan.
Henry despatched Robert of Gloucester and Ranulf le Meschin to Normandy and then intervened himself in late 1123. Henry began the process of besieging the rebel castles, before wintering in the Duchy. In the spring, campaigning began again. Ranulf received intelligence that the rebels were returning to one of their bases at Vatteville, allowing him to ambush them en route at Rougemontiers; Waleran charged the royal forces, but his knights were cut down by Ranulf's archers and the rebels were quickly overwhelmed. Waleran was captured, but Amaury escaped. Henry mopped up the remainder of the rebellion, blinding some of the rebel leaders – considered, at the time, a more merciful punishment than execution – and recovering the last rebel castles. Henry paid Pope Callixtus a large amount of money, in exchange for the Papacy annulling the marriage of William Clito and Sibylla on the grounds of consanguinity.
Planning the succession, 1124–34.
Henry and his new wife did not conceive any children, generating prurient speculation as to the possible explanation, and the future of the dynasty appeared at risk. Henry may have begun to look among his nephews for a possible heir. He may have considered Stephen of Blois as a possible option and, perhaps in preparation for this, he arranged a beneficial marriage for Stephen to a wealthy heiress, Matilda. Theobald of Blois, his close ally, may have also felt that he was in favour with Henry. William Clito, who was King Louis's preferred choice, remained opposed to Henry and was therefore unsuitable. Henry may have also considered his own illegitimate son, Robert of Gloucester, as a possible candidate, but English tradition and custom would have looked unfavourably on this.
Henry's plans shifted when the Empress Matilda's husband, the Emperor Henry, died in 1125. King Henry recalled his daughter to England the next year and declared that, should he die without a male heir, she was to be his rightful successor. The Anglo-Norman barons were gathered together at Westminster on Christmas 1126, where they swore to recognise Matilda and any future legitimate heir she might have. Putting forward a woman as a potential heir in this way was unusual: opposition to Matilda continued to exist within the English court, and Louis was vehemently opposed to her candidacy.
Fresh conflict broke out in 1127, when Charles, the childless Count of Flanders, was murdered, creating a local succession crisis. Backed by King Louis, William Clito was chosen by the Flemings to become their new ruler. This development potentially threatened Normandy, and Henry began to finance a proxy war in Flanders, promoting the claims of William's Flemish rivals. In an effort to disrupt the French alliance with William, Henry mounted an attack into France in 1128, forcing Louis to cut his aid to William. William died unexpectedly in July, removing the last major challenger to Henry's rule and bringing the war in Flanders to a halt. Without William, the baronial opposition in Normandy lacked a leader. A fresh peace was made with France, and the King was finally able to release the remaining prisoners from the revolt of 1123, including Waleran of Meulan, who was rehabilitated into the royal court.
Meanwhile, Henry rebuilt his alliance with Fulk of Anjou, this time by marrying Matilda to Fulk's eldest son, Geoffrey. The pair were betrothed in 1127 and married the following year. It is unknown whether Henry intended Geoffrey to have any future claim on England or Normandy, and he was probably keeping his son-in-law's status deliberately uncertain. Similarly, although Matilda was granted a number of Normandy castles as part of her dowry, it was not specified when the couple would actually take possession of them. Fulk left Anjou for Jerusalem in 1129, declaring Geoffrey the Count of Anjou and Maine. The marriage proved difficult, as the couple did not particularly like each other and the disputed castles proved a point of contention, resulting in Matilda returning to Normandy later that year. Henry appears to have blamed Geoffrey for the separation, but in 1131 the couple were reconciled. Much to the pleasure and relief of Henry, Matilda then gave birth to a sequence of two sons, Henry and Geoffrey, in 1133 and 1134.
Death and legacy.
Death, 1135.
Relations between Henry, Matilda, and Geoffrey became increasingly strained during the King's final years. Matilda and Geoffrey suspected that they lacked genuine support in England. In 1135 they urged Henry to hand over the royal castles in Normandy to Matilda whilst he was still alive, and insisted that the Norman nobility swear immediate allegiance to her, thereby giving the couple a more powerful position after Henry's death. Henry angrily declined to do so, probably out of concern that Geoffrey would try to seize power in Normandy. A fresh rebellion broke out amongst the barons in southern Normandy, led by William, the Count of Ponthieu, whereupon Geoffrey and Matilda intervened in support of the rebels.
Henry campaigned throughout the autumn, strengthening the southern frontier, and then travelled to Lyons-la-Forêt in November to enjoy some hunting, still apparently healthy. There Henry fell ill – according to the chronicler Henry of Huntingdon, he ate an excessive number of lampreys against his physician's advice – and his condition worsened over the course of a week. Once the condition appeared terminal, Henry gave confession and summoned Archbishop Hugh of Amiens, who was joined by Robert of Gloucester and other members of the court. In accordance with custom, preparations were made to settle Henry's outstanding debts and to revoke outstanding sentences of forfeiture. The King died on 1 December 1135, and his corpse was taken to Rouen accompanied by the barons, where it was embalmed; his entrails were buried locally at Port-du-Salut Abbey, and the preserved body was taken on to England, where it was interred at Reading Abbey.
Despite Henry's efforts, the succession was disputed. When news began to spread of the King's death, Geoffrey and Matilda were in Anjou supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. The Norman nobility discussed declaring Theobald of Blois king. Theobald's younger brother, Stephen of Blois, quickly crossed from Boulogne to England, however, accompanied by his military household. With the help of his brother, Henry of Blois, he seized power in England and was crowned king on 22 December. The Empress Matilda did not give up her claim to England and Normandy, leading to the prolonged civil war known as the Anarchy between 1135 and 1153.
Historiography.
Historians have drawn on a range of sources on Henry, including the accounts of chroniclers; other documentary evidence, including early pipe rolls; and surviving buildings and architecture. The three main chroniclers to describe the events of Henry's life were William of Malmesbury, Orderic Vitalis, and Henry of Huntingdon, but each incorporated extensive social and moral commentary into their accounts and borrowed a range of literary devices and stereotypical events from other popular works. Other chroniclers include Eadmer, Hugh the Chanter, Abbot Suger, and the authors of the Welsh "Brut". Not all royal documents from the period have survived, but there are a number of royal acts, charters, writs, and letters, along with some early financial records. Some of these have since been discovered to be forgeries, and others had been subsequently amended or tampered with.
Late medieval historians seized on the accounts of selected chroniclers regarding Henry's education and gave him the title of Henry "Beauclerc", a theme echoed in the analysis of Victorian and Edwardian historians such as Francis Palgrave and Henry Davis. The historian Charles David dismissed this argument in 1929, showing the more extreme claims for Henry's education to be without foundation. Modern histories of Henry commenced with Richard Southern's work in the early 1960s, followed by extensive research during the rest of the 20th century into a wide number of themes from his reign in England, and a much more limited number of studies of his rule in Normandy. Only two major, modern biographies of Henry have been produced, Warren Hollister's posthumous volume in 2001, and Judith Green's 2006 work.
Interpretation of Henry's personality by historians has altered over time. Earlier historians such as Austin Poole and Richard Southern considered Henry as a cruel, draconian ruler. More recent historians, such as Hollister and Green, view his implementation of justice much more sympathetically, particularly when set against the standards of the day, but even Green has noted that Henry was "in many respects highly unpleasant", and Alan Cooper has observed that many contemporary chroniclers were probably too scared of the King to voice much criticism. Historians have also debated the extent to which Henry's administrative reforms genuinely constituted an introduction of what Hollister and John Baldwin have termed systematic, "administrative kingship", or whether his outlook remained fundamentally traditional.
Henry's burial at Reading Abbey is marked by a local cross, but Reading Abbey was slowly demolished during the Dissolution of the Monasteries in the 15th century. The exact location is uncertain, but the most likely location of the tomb itself is now in a built-up area of central Reading, on the site of the former abbey choir. A plan to locate his remains was announced in March 2015, with support from English Heritage and Philippa Langley, who aided with the successful exhumation of Richard III.
Family and children.
Legitimate.
Henry and his first wife, Matilda, had at least two legitimate children:
Henry and his second wife, Adeliza, had no children.
Illegitimate.
Henry had a number of illegitimate children by various mistresses.
Bibliography.
</dl>

</doc>
<doc id="14183" url="http://en.wikipedia.org/wiki?curid=14183" title="Hentai">
Hentai

Hentai (変態 or へんたい) "  " is a word of Japanese origin which is short for (変態性欲, hentai seiyoku)
In Japanese, the term describes any type of perverse or bizarre sexual desire or act; it does not represent a genre of work. Internationally, hentai is a catch-all term to describe a genre of anime and manga pornography. English adopts and uses hentai as a genre of pornography by the commercial sale and marketing of explicit works under this label.
The word's narrow Japanese-language usage and broad international usage are often incompatible. "Weather Report Girl" is considered yuri hentai in English usage for its depiction of lesbian sex, but in Japan it is just yuri. The definition clash also appears with the Japanese definition of yuri as any lesbian relationship, as opposed to its sexually explicit definition in English usage.
Term.
Hentai (変態 or へんたい) "  " is a kanji compound of 変 ("hen"; "change", "weird", or "strange") and 態 ("tai"; "appearance" or "condition"). It also means "perversion" or "abnormality", especially when used as an adjective.:99 It is the shortened form of the phrase (変態性欲, hentai seiyoku)
 which means "sexual perversion". The character "hen" is catch-all for queerness as a peculiarity—it does not carry an explicit sexual reference.:99 While the term has expanded in use to cover a range of publications including homosexual publications,:107 it remains primarily a heterosexual term, as terms indicating homosexuality entered Japan as foreign words.:100 Japanese pornographic works are often simply tagged as 18-kin (18禁, "18-prohibited")
, meaning "prohibited to those not yet 18 years old", and seijin manga (成人漫画, "adult manga")
. Less official terms also in use include ero anime (エロアニメ), ero manga (エロ漫画), and the English acronym AV (for "adult video"). Usage of the term hentai does not define a genre in Japan.
Hentai is defined differently in English. The "Oxford Dictionary Online" defines hentai as "a subgenre of the Japanese genres of manga and anime, characterized by overtly sexualized characters and sexually explicit images and plots." The origin of the word in English is unknown, but AnimeNation's John Oppliger points to the early 1990s, when a "Dirty Pair" erotic "doujinshi" (self-published work) titled "H-Bomb" was released, and when many websites sold access to images culled from Japanese erotic visual novels and games. The earliest English use of the term traces back to the rec.arts.anime boards; with a 1990 post concerning Happosai of "Ranma ½" and the first discussion of the meaning in 1991. A 1995 Glossary on the rec.arts.anime boards contained reference to the Japanese usage and the evolving definition of hentai as "pervert" or "perverted sex". "The Anime Movie Guide", published in 1997, defines "ecchi" (エッチ, etchi) as the initial sound of hentai (i.e., the name of the letter "H", as pronounced in Japanese); it included that ecchi was "milder than hentai". A year later it was defined as a genre in "Good Vibrations Guide to Sex". At the beginning of 2000, "hentai" was listed as the 41st most popular search term of the internet, while "anime" ranked 99th. The attribution has been applied retroactively to works such as "Urotsukidōji", "La Blue Girl", and "Cool Devices". "Urotsukidōji" had previously been described with terms such as "Japornimation", and "erotic grotesque", prior to being identified as hentai.
Etymology.
The history of word "hentai" has its origins in science and psychology. By the middle of the Meiji era, the term appeared in publications to describe unusual or abnormal traits, including paranormal abilities and psychological disorders. A translation of German sexologist Richard von Krafft-Ebing's text "Psychopathia Sexualis" originated the concept of "hentai seiyoku", as a "perverse or abnormal sexual desire". Though it was popularized outside psychology, as in the case of Mori Ōgai's 1909 novel "Vita Sexualis". Continued interest in "hentai seiyoku", resulted in numerous journals and publications on sexual advice which circulated in the public, served to establish the sexual connotation of 'hentai' as perverse. Any perverse or abnormal act could be hentai, such as committing "shinjū" (love suicide). It was Nakamura Kokyo's journal "Abnormal Psychology" which started the popular sexology boom in Japan which would see the rise of other popular journals like "Sexuality and Human Nature", "Sex Research" and "Sex". Originally, Tanaka Kogai wrote articles for "Abnormal Psychology", but it would be Tanaka's own journal "Modern Sexuality" which would become one of the most popular sources of scholarly information about erotic and neurotic expression. "Modern Sexuality" was created to promote fetishism, S&M, and necrophilia as a facet of modern life. The ero-guro movement and depiction of perverse, abnormal and often erotic undertones were a response to interest in "hentai seiyoku".
Following the end of World War II, Japan took a new interest in sexualization and public sexuality. Mark McLelland puts forth the observation that the term "hentai" found itself shortened to "H" and that the English pronunciation was "etchi", referring to lewdness and which did not carry the stronger connotation of abnormality or perversion. By the 1950s, the "hentai seiyoku" publications became their own genre and included fetish and homosexual topics. By the 1960s, the homosexual content was dropped in favor of subjects like sadomasochism and stories of lesbianism targeted to male readers. The late 1960s brought a sexual revolution which expanded and solidified the normalizing the terms identity in Japan that continues to exist today through publications such as Bessatsu Takarajima's "Hentai-san ga iku" series.
History.
With the usage of hentai as any erotic depiction, the history of these depictions are split into its media. Japanese artwork and comics serve as the first example of hentai material, coming to represent the iconic style after the publication of Azuma Hideo's "Cybele" in 1979. Japanese animation (anime) had its first hentai, in both definitions, with the 1984 release of Wonderkid's "Lolita Anime", overlooking the erotic and sexual depictions in 1969's "One Thousand and One Arabian Nights" and the bare breasted Cleopatra in 1970's "Cleopatra" film. Erotic games, another area of contention, has the iconic art style first depicted in sexual acts in 1985's "Tenshitachi no Gogo". The history of each medium itself, complicated based on the broad definition and usage.
Origin of erotic manga.
Depictions of sex and abnormal sex can be traced back through the ages, predating the term "hentai". Shunga (春画), a Japanese term for erotic art, is thought to have and existed in some form since Heian period. From the 16th to the 19th century, Shunga works were suppressed by shoguns. A well-known example is "The Dream of the Fisherman's Wife" which depicts a woman being pleasured by two octopi. Shunga production fell with the rise of pornographic photographs in the late 19th century.
To define erotic manga, a definition for manga is needed. While the Hokusai Manga uses the term "manga" in its title, it does not depict the story-telling aspect common to modern manga, as the images are unrelated. Osamu Tezuka, sometimes referred to as the "God of Manga", helped define the look and form of manga itself. His debut work "New Treasure Island" was released in 1947 as a comic book through Ikuei Publishing and sold 400,000 copies, though it was the popularity of Tezuka's "Astro Boy", "Metropolis", and "Jungle Emperor" manga that would come to define the media. This story-driven manga style is distinctly unique from comic strips like "Sazae-san", and story-driven works are now dominating shoujo and shonen magazines.
Mature themes in manga have existed since the 1940s, but these depictions were more realistic than the cartoon-cute characters popularized by Tezuka. Early well-known "ero-gekiga" releases were "Ero Mangatropa" (1973), "Erogenica" (1975), and "Alice" (1977).:135 The distinct shift in the style of Japanese pornographic comics from realistic to cartoon-cute characters is accredited to Azuma Hideo, "The Father of Lolicon". In 1979, he penned "Cybele" which offered the first commentary on unrealistic depictions of sexual acts between Tezuka-style characters. This would start a pornographic manga movement. The lolicon boom of the 1980s saw the rise of magazines such as the "Lemon People" and "Petit Apple Pie" anthologies.
The publication of erotic materials in America can be traced back to at least 1990, when IANVS Publications printed its first "Anime Shower Special". In March 1994, Antarctic Press released "Bondage Fairies", an English translation of "Insect Hunter".
Origin of erotic anime.
Because there are fewer animation productions, most erotic works are retroactively tagged as hentai since the coining of the term in English. Hentai is typically defined as consisting of excessive nudity, graphic sexual intercourse whether or not it is perverse. The term ecchi, is typically related to fanservice, with no sexual intercourse being depicted.
Two early works escape being defined as hentai, but contain erotic themes. This is likely do to the obscurity and unfamiliarity of the works, arriving in America and fading from public focus a full twenty years before importation and surging interests coined the Americanized term of hentai. The first is the 1969 film "One Thousand and One Arabian Nights" which faithfully includes erotic elements of the original story.:27 In 1970, "", was the first animated film to carry an X rating, but it was mislabeled as erotica in America.:104
The term typically identifies the "Lolita Anime" series as the first erotic anime and original video animation (OVA), it was released in 1984 by Wonder Kids. Containing 8 episodes, the series focused on underage sex and rape and including one episode containing BDSM bondage.:376 Several sub-series were released in response, including a second "Lolita Anime" series was released by Nikkatsu.:376 It has not been officially licensed or distributed outside of its original release.
The "Cream Lemon" franchise of works, which ran from 1984 to 2005, with a number of them entering the American market in various forms. The "The Brothers Grime" series released by Excalibur Films contained "Cream Lemon" works as early as 1986. However, they were not billed as anime and were introduced during the same time that the first underground distribution of erotic works began.
The American release of licensed erotic anime was first attempted in 1991 by Central Park Media, with "I Give My All", but it never occurred. In December 1992, Devil Hunter Yohko was the first risque (ecchi) title was released by A.D. Vision. While it contains no sexual intercourse it pushes the limits of the ecchi category with sexual dialogue, nudity and one scene in which the heroine is about to be raped.
It was Central Park Media's 1993 release of "Urotsukidoji" which brought the first "hentai" film to the American viewers. Often cited for creating the hentai and tentacle rape genres, it contains extreme depictions of violence and monster sex. It is notable for being the first depiction of tentacle sex on screen. When the movie premiered in America it was described as being "drenched in graphic scenes of perverse sex and ultra-violence".
Following this release, a wealth of pornographic content began to arrive in America. With companies such as A.D. Vision, Central Park Media and Media Blasters releasing licensed titles under various labels. A.D. Vision's label Soft Cel Pictures would release 19 titles in 1995 alone. Another label, Critical Mass was created in 1996 to release an unedited edition of "Violence Jack". When A.D. Vision's hentai label Soft Cel Pictures shut down in 2005, most of its titles were acquired by Critical Mass. Following the bankruptcy of Central Park Media in 2009, the licenses for all Anime 18-related products and movies were transferred to Critical Mass.
Origin of erotic games.
The term eroge (erotic game) literally defines any erotic game, but has become synonymous with video games depicting the artistic styles of anime and manga. The origins of eroge began in the early 1980s, while the computer industry in Japan was struggling to define a computer standard with makers like NEC, Sharp, Fujitsu competing against one another. The PC98 series, despite lacking in processing power, CD drives and limited graphics came to dominate the market, with the popularity of eroge games contributing to their success.
Due to the vague definitions of any erotic game, depending on its classification, the first erotic game is a subjective one. If the definition applies to adult themes, the first game was "Softporn Adventure". Released in America in 1981 for the Apple II, "Softporn Adventure" was a text-based comedic game from On-Line Systems. If the definition of eroge is defined as the first graphical depictions and/or Japanese adult themes, it would be Koei's 1982 release of "Night Life". Sexual intercourse is depicted through simple graphic outlines. Notably, "Night Life" was not intended to be erotic so much as an instructional guide "to support married life". A series of "undressing" games appeared as early as 1983, such as "Strip Mahjong". The first anime-styled erotic game was Tenshitachi no Gogo, released in 1985 by JAST. In 1988, ASCII released the first erotic role-playing game "Chaos Angel". In 1989, AliceSoft released the turn-based RPG "Rance" and ELF released "Dragon Knight".
In the late 1980s, eroge began to stagnate under high prices and the majority of games containing uninteresting plots and mindless sex. ELF's 1992 release of "Dokyusei" came as customer frustration with eroge was mounting and spawned a new genre of games called dating sims. "Dokyusei" was unique because it had no defined plot and required the player to build a relationship with different girls in order to advance the story. Each girl had their own story, but the prospect of consummating a relationship required the girl coming to love the player, there was no easy sex.
The term visual novel is vague, with Japanese and English definitions classifying the genre as a type of interactive fiction game driven by narration and limited player interaction. While the term is often retroactively applied to many games, it was Leaf that coined the term with their "Leaf Visual Novel Series" (LVNS) with the 1996 release of "Shizuku" and "Kizuato". The success of these two dark eroge games would be followed by the third and final installment of the LVNS, 1997 romantic eroge "To Heart". Eroge visual novels took a new emotional turn with Tactics' 1998 release "". Key's 1999 release of "Kanon" proved to be a major success and would go on to have numerous console ports, two manga series and two anime series.
Censorship.
Japanese laws have impacted depictions of works since the Meiji Restoration, but these predate common definition of hentai material. Since becoming law in 1907, Article 175 of the Criminal Code of Japan forbids the publication of obscene materials. Specifically, depictions of male-female sexual intercourse and pubic hair are considered obscene, but bare genitalia is not. As censorship is required for published works, the most common representations are the blurring dots on pornographic videos and "bars" or "lights" on still images. In 1986, Toshio Maeda sought to get past censorship on depictions of sexual intercourse, by creating tentacle sex. This lead to the large number of works containing sexual intercourse with monsters, demons, robots, and aliens, whose genitals look different from men. While western views attribute hentai to any explicit work, it was the product of this censorship which became not only the first titles legally imported to America and Europe, but the first successful ones. While uncut for American release, the United Kingdom's release of "Urotsukidoji" removed many scenes of the violence and tentacle rape scenes.
It was also because of this law that the artists began to depict the characters with a minimum of anatomical details and without pubic hair, by law, prior to 1991. Part of the ban was lifted when Nagisa Oshima prevailed over the obscenity charges at his trial for his film "In the Realm of the Senses". Though not enforced, it did not apply to anime and manga as they were not deemed artistic exceptions. Though alterations of material or censorship and even banning of works are common. The U.S. release of the "La Blue Girl" altered the age of the heroine from 16 to 18 and removed sex scenes with a dwarf ninja named Nin-nin, and removed the Japanese censoring blurring dots. "La Blue Girl" was outright rejected by UK censors who refused to classify it and prohibited its distribution.
Demographics.
As a medium, the most popular consumer are men. Eroge games in particular combine three favored media, cartoons, pornography and gaming into an experience. The hentai genre engages a wide audience that expands yearly, with that audience desiring better quality and storylines, or works which push the creative envelope. The unusual and extreme depictions in hentai is not about perversion so much as it is an example of the profit-oriented industry. Anime depicting normal sexual situations enjoy less market success than those that break social norms, such as sex at schools or bondage.
According to Dr. Megha Hazuria Gorem, a clinical psychologist, "Because toons are a kind of final fantasy, you can make the person look the way you want him or her to look. Every fetish can be fulfilled." Dr. Narayan Reddy, a sexologist, commented on the eroge games, "Animators make new games because there is a demand for them, and because they depict things that the gamers do not have the courage to do in real life, or that might just be illegal, these games are an outlet for suppressed desire."
Classification.
The hentai genre can be divided into numerous subgenres, the broadest of which encompasses heterosexual and homosexual acts. Hentai that features mainly heterosexual interactions occur in both male-targeted ("ero") and female-targeted ("ladies' comics") form. Those that feature mainly homosexual interactions are known as yaoi (male-male) and yuri (female-female). Both yaoi and, to a lesser extent, yuri are generally aimed at members of the opposite sex from the persons depicted. While yaoi and yuri are not always explicit, the pornographic history and association remains. Yaoi's pornographic usage has remained strong in textual form through fanfiction. The definition of yuri has begun to be replaced by the broader definitions of "lesbian-themed animation or comics".
Hentai is perceived as "dwelling" on sexual fetishes. These include dozens of fetish and paraphilia related subgenres, which can be further classified with additional terms, such as heterosexual or homosexual types.
Many works are focused on depicting the mundane and the impossible across every conceivable act and situation no matter how fantastical. The largest subgenre of hentai is "futanari" (hermaphroditism), which most often features a female with a penis or penis-like appendage in place of, or in addition to normal female genitals. Futanari characters are primarily depicted as having sex with other women and will almost always be submissive with a male; exceptions include Yonekura Kengo's work, which features female empowerment and domination over males.

</doc>
<doc id="14186" url="http://en.wikipedia.org/wiki?curid=14186" title="Henry VII of England">
Henry VII of England

Henry VII (Welsh: "Harri Tudur"; 28 January 1457 – 21 April 1509) was King of England and Lord of Ireland from his seizing the crown on 22 August 1485 until his death on 21 April 1509, as the first monarch of the House of Tudor.
Henry won the throne when his forces defeated Richard III at the Battle of Bosworth Field. He was the last king of England to win his throne on the field of battle. Henry cemented his claim by marrying Elizabeth of York, daughter of Edward IV and niece of Richard III. Henry was successful in restoring the power and stability of the English monarchy after the political upheavals of the civil wars known as the Wars of the Roses. He founded the Tudor dynasty and, after a reign of nearly 24 years, was peacefully succeeded by his son, Henry VIII.
Although Henry can be credited with the restoration of political stability in England, and a number of commendable administrative, economic and diplomatic initiatives, the latter part of his reign was characterised by a financial greed which stretched the bounds of legality. The capriciousness and lack of due process which indebted many in England were soon ended upon Henry VII's death after a commission revealed widespread abuses. According to the contemporary historian Polydore Vergil, simple "greed" in large part underscored the means by which royal control was over-asserted in Henry's final years.
Ancestry and early life.
Henry VII was born at Pembroke Castle on 28 January 1457 to the 13-year-old Margaret Beaufort, Countess of Richmond. His father, Edmund Tudor, 1st Earl of Richmond, died three months before his birth.
Henry's paternal grandfather, Owen Tudor, originally from the Tudors of Penmynydd, Isle of Anglesey in Wales, had been a page in the court of Henry V. He rose to become one of the "Squires to the Body to the King" after military service at Agincourt. Owen is said to have secretly married the widow of Henry V, Catherine of Valois. One of their sons was Edmund Tudor, father of Henry VII. Edmund was created Earl of Richmond in 1452, and "formally declared legitimate by Parliament".
Henry's main claim to the English throne derived from his mother through the House of Beaufort. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, Duke of Lancaster, fourth son of Edward III, and his third wife Katherine Swynford. Katherine was Gaunt's mistress for about 25 years; when they married in 1396, they already had four children, including Henry's great-grandfather John Beaufort. Thus Henry's claim was somewhat tenuous: it was from a woman, and by illegitimate descent. In theory, the Portuguese and Spanish royal families had a better claim (as far as "legitimacy" is concerned) as descendants of Catherine of Lancaster, the daughter of John of Gaunt and his second wife Constance of Castile.
Gaunt's nephew Richard II legitimised Gaunt's children by Katherine Swynford by Letters Patent in 1397. In 1407, Henry IV, who was Gaunt's son by his first wife, issued new Letters Patent confirming the legitimacy of his half-siblings, but also declaring them ineligible for the throne. Henry IV's action was of doubtful legality, as the Beauforts were previously legitimised by an Act of Parliament, but it further weakened Henry's claim.
Nonetheless, by 1483 Henry was the senior male Lancastrian claimant remaining, after the deaths in battle or by murder or execution of Henry VI, his son Edward of Westminster, Prince of Wales, and the other Beaufort line of descent through Lady Margaret's uncle, the 2nd Duke of Somerset.
Henry also made some political capital out of his Welsh ancestry, for example in attracting military support and safeguarding his army's passage through Wales on its way to the Battle of Bosworth. He came from an old-established Anglesey family which claimed descent from Cadwaladr (in legend, the last ancient British king) and on occasion, Henry displayed the red dragon of Cadwaladr. He took it, as well as the standard of St George, on his procession through London after victory at Bosworth. A contemporary writer and Henry's biographer, Bernard André, also made much of Henry's Welsh descent.
In reality, however, his hereditary connections to Welsh aristocracy were not strong. He was descended by the paternal line, through several generations, from Ednyfed Fychan, the seneschal (steward) of Gwynedd and through this seneschal's wife from Rhys ap Tewdwr, the King of Deheubarth in South Wales.
His more immediate ancestor Tudur ap Goronwy had aristocratic land rights, but his sons, who were first cousins to Owain Glyndŵr, sided with Owain in his revolt. One son was executed and the family land was forfeited. Another son, Henry's great-grandfather, became a butler to the Bishop of Bangor. Owen Tudor, the son of the butler, like the children of other rebels, was provided for by Henry V, a circumstance which precipitated his access to Queen Catherine of Valois.
Notwithstanding this lineage, to the bards of Wales, Henry was a candidate for Y Mab Darogan – "The Son of Prophecy" who would free the Welsh from oppression.
In 1456, Henry's father Edmund Tudor was captured while fighting for Henry VI in South Wales against the Yorkists. He died in Carmarthen Castle, three months before Henry was born. Henry's uncle Jasper Tudor, the Earl of Pembroke and Edmund's younger brother, undertook to protect the young widow, who was 13 years old when she gave birth to Henry. When Edward IV became King in 1461, Jasper Tudor went into exile abroad. Pembroke Castle, and later the Earldom of Pembroke, were granted to the Yorkist William Herbert, who also assumed the guardianship of Margaret Beaufort and the young Henry.
Henry lived in the Herbert household until 1469, when Richard Neville, Earl of Warwick (the "Kingmaker"), went over to the Lancastrians. Herbert was captured fighting for the Yorkists and executed by Warwick. When Warwick restored Henry VI in 1470, Jasper Tudor returned from exile and brought Henry to court. When the Yorkist Edward IV regained the throne in 1471, Henry fled with other Lancastrians to Brittany, where he spent most of the next 14 years.
Rise to the throne.
By 1483, his mother, despite being married to a Yorkist (Lord Stanley), was actively promoting Henry as an alternative to Richard III.
At Rennes Cathedral on Christmas Day 1483, Henry pledged to marry Edward IV's eldest daughter, Elizabeth of York, who was also Edward's heir since the presumed death of her brothers, the Princes in the Tower (King Edward V and his brother Richard of Shrewsbury, Duke of York). Henry then received the homage of his supporters.
With money and supplies borrowed from his host Francis II, Duke of Brittany, Henry tried to land in England, but his conspiracy unravelled, resulting in the execution of his primary co-conspirator, the Duke of Buckingham. Now supported by Francis II's prime-minister Pierre Landais, Richard III attempted to extradite Henry from Brittany, but Henry escaped to France. He was welcomed by the French, who readily supplied him with troops and equipment for a second invasion.
Having gained the support of the Woodvilles, in-laws of the late Edward IV, he sailed with a small French and Scottish force. Henry landed in Mill Bay, Pembrokeshire, close to his birthplace. He marched towards England accompanied by his uncle Jasper and the Earl of Oxford. Wales was traditionally a Lancastrian stronghold, and Henry owed the support he gathered to his Welsh birth and ancestry, being directly descended, through his father, from Rhys ap Gruffydd. He amassed an army of around 5,000 soldiers.
Henry was aware that his best chance to seize the throne was to engage Richard quickly and defeat him immediately, as Richard had reinforcements in Nottingham and Leicester. Richard only needed to avoid being killed to keep his throne. Though outnumbered, Henry's Lancastrian forces decisively defeated Richard's Yorkist army at the Battle of Bosworth Field on 22 August 1485. Several of Richard's key allies, such as the Earl of Northumberland and William and Thomas Stanley, crucially switched sides or left the battlefield. Richard III's death at Bosworth Field effectively ended the Wars of the Roses, although it was not the last battle Henry had to fight.
Reign.
The first concern Henry had was to secure his hold on the throne.
He honoured his pledge of December 1483 to marry Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. The marriage took place on 18 January 1486 at Westminster. The marriage unified the warring houses and gave his children a strong claim to the throne. The unification of the houses of York and Lancaster by this marriage is symbolised by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster. It also ended future discussion as to whether the descendants of the fourth son of Edward III, Edmund, Duke of York, through marriage to Philippa, heiress of the second son, Lionel, Duke of Clarence, had a superior or inferior claim to those of the third son John of Gaunt, who had held the throne for three generations.
In addition, Henry had Parliament repeal "Titulus Regius", the statute that declared Edward IV's marriage invalid and his children illegitimate, thus legitimising his wife. Amateur historians Bertram Fields and Sir Clements Markham have claimed that he may have been involved in the murder of the Princes in the Tower, as the repeal of "Titulus Regius" gave the Princes a stronger claim to the throne than his own. Alison Weir, however, points out that the Rennes ceremony, two years earlier, was possible only if Henry and his supporters were certain that the Princes were already dead.
Henry's second action was to declare himself king retroactively from 21 August 1485, the day before Bosworth Field. This meant that anyone who had fought for Richard against him would be guilty of treason. Thus, Henry could legally confiscate the lands and property of Richard III while restoring his own. However, he spared Richard's nephew and designated heir, the Earl of Lincoln. He also created Margaret Plantagenet, a Yorkist heiress, Countess of Salisbury sui juris. He took great care not to address the baronage, or summon Parliament, until after his coronation, which took place in Westminster Abbey on 30 October 1485. Almost immediately afterwards, he issued an edict that any gentleman who swore fealty to him would, notwithstanding any previous attainder, be secure in his property and person.
Henry secured his crown principally by dividing and undermining the power of the nobility, especially through the aggressive use of bonds and recognisances to secure loyalty. He also enacted laws against livery and maintenance, the great lords' practice of having large numbers of "retainers" who wore their lord's badge or uniform and formed a potential private army.
While he was still in Leicester after the battle of Bosworth Field Henry was already taking precautions to avoid any rebellions against his reign. Before leaving Leicester to go to London, Henry dispatched Robert Willoughby to Sheriff Hutton in Yorkshire, to have the ten-year-old Edward, Earl of Warwick, arrested and taken to the Tower of London. Edward was the son of George, duke of Clarence, and as such he presented a threat as a potential rival to the new King Henry VII for the throne of England. However, Henry was threatened by several active rebellions over the next few years. The first was the Rebellion of the Stafford brothers and Viscount Lovell of 1486, which collapsed without fighting.
In 1487, Yorkists led by Lincoln rebelled in support of Lambert Simnel, a boy who was claimed to be the Earl of Warwick, son of Edward IV's brother Clarence (who had last been seen as a prisoner in the Tower). The rebellion began in Ireland, where the traditionally Yorkist nobility, headed by the powerful Gerald FitzGerald, 8th Earl of Kildare, proclaimed Simnel King and provided troops for his invasion of England. The rebellion was defeated and Lincoln killed at the Battle of Stoke. Henry showed remarkable clemency to the surviving rebels: he pardoned Kildare and the other Irish nobles, and he made the boy, Simnel, a servant in the royal kitchen.
In 1490, a young Fleming, Perkin Warbeck, appeared and claimed to be Richard, the younger of the "Princes in the Tower". Warbeck won the support of Edward IV's sister Margaret of Burgundy. He led attempted invasions of Ireland in 1491 and England in 1495, and persuaded James IV of Scotland to invade England in 1496. In 1497 Warbeck landed in Cornwall with a few thousand troops, but was soon captured and executed.
In 1499, Henry had the Earl of Warwick executed. However, he spared Warwick's elder sister Margaret. She survived until 1541, when she was executed by Henry VIII.
Henry married Elizabeth of York with the hope of uniting the Yorkist and Lancastrian sides of the Plantagenet dynastic disputes. In this, he was largely successful. However, such a level of paranoia persisted that anyone (John de la Pole, Earl of Richmond, is an example) with blood ties to the Plantagenets was suspected of coveting the throne.
Economics.
Unlike his predecessors, Henry VII came to the throne without personal experience in estate management or financial administration. Yet during his reign Henry VII became a fiscally prudent monarch who restored the fortunes of an effectively bankrupt exchequer. Henry VII introduced stability to the financial administration of England by keeping the same financial advisors throughout his reign. For instance, excepting only the first few months of the reign, Lord Dynham and Thomas Howard, earl of Surrey were the only two office holders in the position of Lord High Treasurer of England throughout the reign of Henry VII.
Henry VII improved tax collection within the realm even by introducing ruthlessly efficient mechanisms of taxation. In this he was supported by his chancellor, Archbishop John Morton, whose "Morton's Fork" was a catch-22 method of ensuring that nobles paid increased taxes. Morton's Fork may actually have been invented by another of Henry's supporters—Richard Foxe. However, whether it's called "Fox's Fork" or "Morton's Fork" the result was the same. Those nobles who spent little must have saved much and, thus, they could afford the increased taxes. On the other hand, those nobles who spent much, obviously had the means to pay the increased taxes. Royal government was also reformed with the introduction of the King's Council that kept the nobility in check.
Foreign policy.
Henry VII's policy was both to maintain peace and to create economic prosperity. Up to a point, he succeeded. He was not a military man and had no interest in trying to regain French territories lost during the reigns of his predecessors; he was therefore ready to conclude a treaty with France at Etaples that brought money into the coffers of England, and ensured the French would not support pretenders to the English throne, such as Perkin Warbeck. However, this treaty came at a slight price, as Henry mounted a minor invasion of Brittany in November 1492. Henry decided to keep Brittany out of French hands, signed an alliance with Spain to that end, and sent seven thousand troops to France. The confused, fractious nature of Breton politics undermined his efforts, which finally failed after three sizeable expeditions, at a cost of £24,000. However, as France was becoming more concerned with the Italian Wars, the French were happy to agree to the Treaty of Etaples.
Henry had been under the financial and physical protection of the French throne or its vassals for most of his life, prior to his ascending the throne of England. To strengthen his position, however, he subsidised shipbuilding, so strengthening the navy (he commissioned Europe's first ever – and the world's oldest surviving – dry dock at Portsmouth in 1495) and improving trading opportunities.
By the time of his death, he had amassed a personal fortune of £1.25 million (equivalent to £ in 2015).
Henry VII was one of the first European monarchs to recognise the importance of the newly united Spanish kingdom and concluded the Treaty of Medina del Campo, by which his son, Arthur Tudor, was married to Catherine of Aragon. He also concluded the Treaty of Perpetual Peace with Scotland (the first treaty between England and Scotland for almost two centuries), which betrothed his daughter Margaret to King James IV of Scotland. By means of this marriage, Henry VII hoped to break the Auld Alliance between Scotland and France. Though this was not achieved during his reign, the marriage eventually led to the union of the English and Scottish crowns under Margaret's great-grandson, James VI and I following the death of Henry's granddaughter Elizabeth I.
He also formed an alliance with Holy Roman Emperor Maximilian I (1493–1519) and persuaded Pope Innocent VIII to issue a Papal bull of Excommunication against all pretenders to Henry's throne.
Trade agreements.
Henry's most successful diplomatic achievement as regards the economy was the "Magnus Intercursus" ("great agreement") of 1496. In 1494, Henry embargoed trade (mainly in wool) with the Netherlands as retaliation for Margaret of Burgundy's support of Perkin Warbeck. The Merchant Adventurers, the company which enjoyed the monopoly of the Flemish wool trade, relocated from Antwerp to Calais. At the same time, Flemish merchants were ejected from England. The stand-off eventually paid off for Henry. Both parties realised they were mutually disadvantaged by the reduction in commerce. Its restoration by the "Magnus Intercursus" was very much to England's benefit in removing taxation for English merchants and significantly increasing England's wealth. In turn, Antwerp became an extremely important trade entrepot, through which, for example, goods from the Baltic, spices from the east and Italian silks were exchanged for English cloth.
In 1506, Henry extorted the Treaty of Windsor from Philip the Handsome of Burgundy. Philip had been shipwrecked on the English coast, and while Henry's guest, was bullied into an agreement so favourable to England at the expense of the Netherlands that it was dubbed the "Malus Intercursus" ("evil agreement"). France, Burgundy, the Holy Roman Empire, Spain and the Hanseatic League all rejected the treaty, which was never in force. Philip died shortly after the negotiations.
Henry VII was also enriched by trading alum which was used in the wool and cloth trades for dyeing fabric. Since Europe had only one area where it was mined (Tolfa, Italy), it was a scarce commodity and therefore valuable. Starting in 1486, Henry VII became involved in the alum trade. With the assistance of the Italian merchant-banker, Lodovico della Fava and the Italian banker, Girolamo Frescobaldi, Henry VII became deeply involved in the alum trade by licensing ships, obtaining alum from the Ottoman Empire, and selling it to the Low Countries and England. This trade made an expensive commodity cheaper which raised opposition with Pope Julius II since the Tolfa, Italy alum mine was a part of papal territory thereby giving the Pope monopoly control over alum.
Law enforcement and Justices of Peace.
Henry's principal problem was to restore royal authority in a realm recovering from the Wars of the Roses. There were too many powerful noblemen and, as a consequence of the system of so-called bastard feudalism, each had what amounted to private armies of indentured retainers (mercenaries masquerading as servants).
He was content to allow the nobles their regional influence if they were loyal to him. For instance, the Stanley family had control of Lancashire and Cheshire, upholding the peace on the condition that they stayed within the law. In other cases, he brought his over-powerful subjects to heel by decree. He passed laws against "livery" (the upper classes' flaunting of their adherents by giving them badges and emblems) and "maintenance" (the keeping of too many male "servants"). These laws were used shrewdly in levying fines upon those that he perceived as threats.
However, his principal weapon was the Court of Star Chamber. This revived an earlier practice of using a small (and trusted) group of the Privy Council as a personal or Prerogative Court, able to cut through the cumbersome legal system and act swiftly. Serious disputes involving the use of personal power, or threats to royal authority, were thus dealt with.
Henry VII used Justices of the Peace on a large, nationwide scale. They were appointed for every shire and served for a year at a time. Their chief task was to see that the laws of the country were obeyed in their area. Their powers and numbers steadily increased during the time of the Tudors, never more so than under Henry's reign. Despite this, Henry was keen to constrain their power and influence, applying the same principles to the Justices of the Peace as he did to the nobility: a similar system of bonds and recognisances to that which applied to both the gentry and the nobles who tried to exert their elevated influence over these local officials.
All Acts of Parliament were overseen by the Justices of the Peace. For example, Justices of the Peace could replace suspect jurors in accordance with the 1495 act preventing the corruption of juries. They were also in charge of various administrative duties, such as the checking of weights and measures.
By 1509, Justices of the Peace were key enforcers of law and order for Henry VII. They were unpaid, which, in comparison with modern standards, meant a lesser tax bill to pay for a police force. Local gentry saw the office as one of local influence and prestige and were therefore willing to serve. Overall, this was a successful area of policy for Henry, both in terms of efficiency and as a method of reducing the corruption endemic within the nobility of the Middle Ages.
Later years and death.
In 1502, Henry VII's first son and heir-apparent, Arthur Tudor, died suddenly at Ludlow Castle, very likely from a viral respiratory illness known, at the time, as the "English sweating sickness". This made Henry, Duke of York (Henry VIII) heir-apparent to the throne. The King, normally a reserved man, surprised his courtiers by his intense grief at his son's death, while his concern for the Queen is evidence that the marriage was a happy one.
Henry VII wanted to maintain the Spanish alliance. He therefore arranged a papal dispensation from Pope Julius II for Prince Henry to marry his brother's widow Catherine, a relationship that would have otherwise precluded marriage in the Roman Catholic Church. In 1503, Queen Elizabeth died in childbirth, so King Henry had the dispensation also permit him to marry Catherine himself. After obtaining the dispensation, Henry had second thoughts about the marriage of his son and Catherine. Catherine's mother Isabella I of Castile had died and Catherine's sister Joanna had succeeded her; Catherine was therefore daughter of only one reigning monarch and so less desirable as a spouse for Henry VII's heir-apparent. The marriage did not take place during his lifetime. Otherwise, at the time of his father's arranging of the marriage to Catherine of Aragon, the future Henry VIII was too young to contract the marriage according to Canon Law, and would be ineligible until age fourteen.
Henry made half-hearted plans to remarry and beget more heirs, but these never came to anything. In 1505 he was sufficiently interested in a potential marriage to Joan, the recently widowed Queen of Naples, that he sent ambassadors to Naples to report on the 27-year-old's physical suitability. The wedding never took place, and curiously the physical description Henry sent with his ambassadors describing what he desired in a new wife matched the description of Elizabeth. After 1503, records show the Tower of London was never again used as a royal residence by Henry Tudor, and all royal births under Henry VIII took place in palaces. Henry VII was shattered by the loss of Elizabeth, and her death broke his heart. Immediately after Elizabeth's death, Henry became very sick and nearly died himself, and only allowed Margaret Beaufort, his mother, near him: "privily departed to a solitary place, and would that no man should resort unto him."
Henry VII died at Richmond Palace on 21 April 1509 of tuberculosis and was buried at Westminster Abbey, next to his wife, Elizabeth, in the chapel he commissioned. He was succeeded by his second son, Henry VIII (reign 1509–47). His mother survived him, dying two months later on 29 June 1509.
Appearance and character.
At twenty seven, Henry was tall, slender, with small blue eyes and noticeably bad teeth in a long, sallow face beneath very fair hair. Amiable and high-spirited, Henry Tudor was friendly if dignified in manner, while it was clear to everyone that he was extremely intelligent. His biographer, Professor Chrimes, credits him - even before he had become King - with possessing "a high degree of personal magnetism, ability to inspire confidence, and a growing reputation for shrewd decisiveness". On the debit side, he may have looked a little delicate as he suffered from poor health.
Legacy and memory.
Historians have always compared Henry VII with his continental contemporaries, especially Louis XI of France and Ferdinand II of Aragon. By 1600 historians emphasised Henry's wisdom in drawing lessons in statecraft from other monarchs. By 1900 the "New Monarchy" interpretation stressed the common factors that in each country led to the revival of monarchical power. This approach raised puzzling questions about similarities and differences in the development of national states. In the late 20th century a model of European state formation was prominent in which Henry less resembles Louis and Ferdinand.
Henry's titles.
Henry's full style as king was: "Henry, by the Grace of God, King of England, France and Lord of Ireland"
Arms.
Upon his succession as king, Henry became entitled to bear the arms of his kingdom. After his marriage, he used the red-and-white rose as his emblem – this continued to be his dynasty's emblem, known as the Tudor rose.
Issue.
Henry and Elizabeth's children are listed below.
An illegitimate son has also been attributed to Henry by "a Breton Lady":
Further descendants.
Henry VII's elder surviving daughter Margaret was married first to James IV of Scotland (reigned 1488–1513). Their son became James V of Scotland (reigned 1513–42), whose daughter became Mary, Queen of Scots (reigned 1542–67). Margaret Tudor's second marriage was to Archibald Douglas; their grandson, Henry Stuart, Lord Darnley married Mary, Queen of Scots. Their son, James VI of Scotland (reigned 1567–1625), inherited the throne of England as James I (reigned 1603–25) after the death of Henry's granddaughter, Elizabeth I (reigned 1558–1603). After divorcing Douglas, her third and final marriage was to Henry Stewart, with whom she had another daughter, Dorothea Stewart.
Henry VII's other surviving daughter, Mary first married King Louis XII of France (reigned 1498–1515), who died after only about three months of marriage. She then married the Duke of Suffolk without the permission of her brother, now King Henry VIII. Their daughter Frances married Henry Grey, and her children included Lady Jane Grey, in whose name her parents and in-laws tried to seize the throne after Edward VI of England (reigned 1547–53) died.
The current monarch of the United Kingdom, Elizabeth II, is a direct descendant of Henry VII. The daughter of Henry's double-great-great grandson James I/VI, Elizabeth Stuart, was the mother of Sophia of Hanover whose descendants were the monarchs of the House of Hanover and the succeeding House of Saxe-Coburg-Gotha/Windsor.

</doc>
<doc id="14187" url="http://en.wikipedia.org/wiki?curid=14187" title="Henry VIII of England">
Henry VIII of England

Henry VIII (28 June 1491 – 28 January 1547) was King of England from 21 April 1509 until his death. He was Lord, and later assumed the Kingship, of Ireland, and continued the nominal claim by English monarchs to the Kingdom of France. Henry was the second monarch of the Tudor dynasty, succeeding his father, Henry VII.
Besides his six marriages, Henry VIII is known for his role in the separation of the Church of England from the Roman Catholic Church. His disagreements with the Pope led to his separation of the Church of England from papal authority, with himself, as king, as the Supreme Head of the Church of England and to the Dissolution of the Monasteries. Because his principal dispute was with papal authority, rather than with doctrinal matters, he remained a believer in core Catholic theological teachings despite his excommunication from the Roman Catholic Church. Henry oversaw the legal union of England and Wales with the Laws in Wales Acts 1535 and 1542. He is also well known for a long personal rivalry with both Francis I of France and the Holy Roman Emperor Charles V, his contemporaries with whom he frequently warred.
Domestically, Henry is known for his radical changes to the English Constitution, ushering in the theory of the divine right of kings to England. Besides asserting the sovereign's supremacy over the Church of England, thus initiating the English Reformation, he greatly expanded royal power. Charges of treason and heresy were commonly used to quash dissent, and those accused were often executed without a formal trial, by means of bills of attainder. He achieved many of his political aims through the work of his chief ministers, some of whom were banished or executed when they fell out of his favour. Figures such as Thomas Wolsey, Thomas More, Thomas Cromwell, Richard Rich, and Thomas Cranmer figured prominently in Henry's administration. An extravagant spender, he used the proceeds from the Dissolution of the Monasteries and acts of the Reformation Parliament to convert to royal revenue money formerly paid to Rome. Despite the influx of money from these sources, Henry was continually on the verge of financial ruin, due to his personal extravagance, as well as his numerous costly continental wars.
His contemporaries considered Henry in his prime to be an attractive, educated and accomplished king, and he has been described as "one of the most charismatic rulers to sit on the English throne". Besides ruling with considerable power, he was also an author and composer. His desire to provide England with a male heir – which stemmed partly from personal vanity and partly from his belief that a daughter would be unable to consolidate Tudor power and maintain the fragile peace that existed following the Wars of the Roses – led to the two things for which Henry is most remembered: his six marriages and his break with the Pope (who would not allow an annulment of Henry's first marriage) and the Roman Catholic Church, leading to the English Reformation. Henry became severely obese and his health suffered, contributing to his death in 1547. He is frequently characterised in his later life as a lustful, egotistical, harsh, and insecure king. He was succeeded by his son Edward VI.
Life.
Early years.
Born 28 June 1491 at Greenwich Palace, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York. Of the young Henry's six siblings, only three – Arthur, Prince of Wales; Margaret; and Mary – survived infancy. He was baptised by Richard Fox, the Bishop of Exeter, at a church of the Observant Franciscans close to the palace. In 1493, at the age of two, Henry was appointed Constable of Dover Castle and Lord Warden of the Cinque Ports. He was subsequently appointed Earl Marshal of England and Lord Lieutenant of Ireland at age three, and was inducted into the Order of the Bath soon after. The day after the ceremony he was created Duke of York and a month or so later made Warden of the Scottish Marches. In May 1495, he was appointed to the Order of the Garter. Henry was given a first-rate education from leading tutors, becoming fluent in Latin and French, and learning at least some Italian. Not much is known about his early life – save for his appointments – because he was not expected to become king. In November 1501, Henry also played a considerable part in the ceremonies surrounding the marriage of his brother, Prince Arthur, to Catherine of Aragon, the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. As Duke of York, Henry used the arms of his father as king, differenced by a "label of three points ermine".
In 1502, Arthur died at the age of 15, after 20 weeks of marriage to Catherine. Arthur's death thrust all his duties upon his younger brother, the 10-year-old Henry. After a little debate, Henry became the new Duke of Cornwall in October 1502, and the new Prince of Wales and Earl of Chester in February 1503. Henry VII gave the boy few tasks. Young Henry was strictly supervised and did not appear in public. As a result, the young Henry would later ascend the throne "untrained in the exacting art of kingship."
Henry VII renewed his efforts to seal a marital alliance between England and Spain, by offering his second son in marriage to Arthur's widow Catherine. Both Isabella and Henry VII were keen on the idea, which had arisen very shortly after Arthur's death. On 23 June 1503, a treaty was signed for their marriage, and they were betrothed two days later. A papal dispensation was only needed for the "impediment of public honesty" if the marriage had not been consummated as Catherine and her duenna claimed, but Henry VII and the Spanish ambassador set out instead to obtain a dispensation for "affinity", which took account of the possibility of consummation. The young Henry's age, only eleven, prevented cohabitation. Isabella's death in 1504, and the ensuing problems of succession in Castile, complicated matters. Her father preferred her to stay in England, but Henry VII's relations with Ferdinand had deteriorated. Catherine was therefore left in limbo for some time, culminating in Prince Henry's rejection of the marriage as soon he was able, at the age of 14. Ferdinand's solution was to make his daughter ambassador, allowing her to stay in England indefinitely. Devout, she began to believe that it was God's will that she marry the prince despite his opposition.
Early reign.
Henry VII died on 21 April 1509, and the young Henry succeeded him as king. Soon after his father's burial on 10 May, Henry suddenly declared that he would indeed marry Catherine, leaving unresolved issues concerning the papal dispensation and a missing part of the marriage portion. The new king maintained that it had been his father's dying wish that he marry Catherine. Whether or not this was true, it was certainly convenient. Holy Roman Emperor Maximilian I had been attempting to marry his granddaughter (and Catherine's niece) Eleanor to Henry; she had now been jilted. Henry's wedding to Catherine was kept low-key and was held at the friar's church in Greenwich on 11 June 1509. On 23 June 1509, Henry led Catherine from the Tower of London to Westminster Abbey for their coronation, which took place the following day. It was a grand affair: the king's passage was lined with tapestries and laid with fine cloth. Following the ceremony, there was a grand banquet in Westminster Hall. As Catherine wrote to her father, "our time is spent in continuous festival".
Two days after Henry's coronation, he arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510. Historian Ian Crofton has maintained that such executions would become Henry's primary tactic for dealing with those who stood in his way; the two executions were certainly not the last. Henry also returned to the public some of the money supposedly extorted by the two ministers. By contrast, Henry's view of the House of York – potential rival claimants for the throne – was more moderate than his father's had been. Several who had been imprisoned by his father, including the Marquess of Dorset, were pardoned. Others (most notably Edmund de la Pole) went unreconciled; de la Pole was eventually beheaded in 1513, an execution prompted by his brother Richard siding against the king.
Soon after, Catherine conceived, but the child, a girl, was stillborn on 31 January 1510. About four months later, Catherine again became pregnant. On New Year's Day 1511, the child – Henry – was born. After the grief of losing their first child, the couple were pleased to have a boy and there were festivities to celebrate, including a jousting tournament. However, the child died seven weeks later. Catherine miscarried again in 1514, but gave birth in February 1516 to a girl, Mary. Relations between Henry and Catherine had been strained, but they eased slightly after Mary's birth, and there is little to suggest the marriage was anything but "unusually good" in the period.
During this period, Henry had mistresses. It was revealed in 1510 that Henry had been conducting an affair with one of the sisters of Edward Stafford, 3rd Duke of Buckingham, either Elizabeth or Anne Hastings, Countess of Huntingdon. The most significant mistress for about three years, starting in 1516, was Elizabeth Blount. Blount is one of only two completely undisputed mistresses, few for a virile young king. Exactly how many Henry had is disputed: David Loades believes Henry had mistresses "only to a very limited extent", whilst Alison Weir believes there were numerous other affairs. Catherine did not protest, and in 1518 fell pregnant again with another girl, who was also stillborn. Blount gave birth in June 1519 to Henry's illegitimate son, Henry FitzRoy. The young boy was made Duke of Richmond in June 1525 in what some thought was one step on the path to his eventual legitimisation. In 1533, FitzRoy married Mary Howard, but died childless three years later. At the time of Richmond's death in June 1536, Parliament was enacting the Second Succession Act, which could have allowed him to become king.
France and the Habsburgs.
In 1510, France, with a fragile alliance with the Holy Roman Empire in the League of Cambrai, was winning a war against Venice. Henry renewed his father's friendship with Louis XII of France, an issue which divided his council. Certainly war with the combined might of the two powers would have been exceedingly difficult. Shortly after, Henry also signed a contradictory pact with Ferdinand against France. The problem was resolved with the creation of the anti-French Holy League by Pope Julius II in October 1511, which brought Louis into conflict with Ferdinand. Henry brought England into the Holy League shortly after, with an initial joint Anglo-Spanish attack on Aquitaine planned for the spring to recover it for England. It appeared to be the start of making Henry's dreams of ruling France a reality. The attack, following a formal declaration of war in April, was not led by Henry personally. It was a considerable failure – Ferdinand used it simply to further his own ends – and it strained the Anglo-Spanish alliance. Nevertheless, the French were pushed out of Italy soon after, and the alliance survived, with both parties keen to win further victories over the French. Henry then pulled off a diplomatic coup by convincing the Emperor to join the Holy League. Remarkably, Henry had also secured the promised title of "Most Christian King of France", and possibly coronation by the Pope himself in Paris, if only Louis could be defeated.
On 30 June 1513, Henry invaded France, and his troops defeated a French army at the Battle of the Spurs – a minor result, but one which was seized on by the English for propaganda purposes. Soon after, the English took Thérouanne and handed it over to Maximillian; Tournai, a more significant settlement, followed. Henry had led the army personally, complete with large entourage. His absence from the country prompted his brother-in-law, James IV of Scotland, to invade England at the behest of Louis. The English army, overseen by Queen Catherine, decisively defeated the Scots at the Battle of Flodden on 9 September 1513. Among the dead was the Scottish king, ending Scotland's brief involvement in the war. These campaigns had given Henry a taste of the military success he so desired. However, despite initial indications that he would pursue a 1514 campaign, Henry decided against such a move. He had been supporting Ferdinand and Maximilian financially during the campaign but had got back little; England's own coffers were now empty. With the replacement of Julius by Pope Leo X, who was inclined to negotiate for peace with France, Henry signed his own treaty with Louis: his sister Mary would become Louis' wife, having previously been pledged to the younger Charles, and peace secured for eight years, a remarkably long time.
Following the deaths of his grandfathers, Ferdinand and Maximilian, in 1516 and 1519 respectively, Charles of Austria ascended the thrones of Spain and Holy Roman Empire; Francis I became king of France on Louis' death. Cardinal Thomas Wolsey's careful diplomacy had resulted in the Treaty of London in 1518, aimed at uniting the kingdoms of western Europe in the wake of a new Ottoman threat, and it seemed that peace might be secured. Henry met Francis I on 7 June 1520 at the Field of the Cloth of Gold near Calais for a fortnight of lavish entertainment. Both hoped for friendly relations in place of the wars of the previous decade. The strong air of competition laid to rest any hopes of a renewal of the Treaty of London, however, and conflict was inevitable. Henry had more in common with Charles, whom he met once before and once after Francis. Charles brought the Empire into war with France in 1521; Henry offered to mediate, but little was achieved and by the end of the year Henry had aligned England with Charles. He still clung to his previous aim of restoring English lands in France, but also to securing an alliance with Burgundy and the continuing support of Charles. A small English attack in the north of France made up little ground. Charles defeated and captured Francis at Pavia, and could dictate peace; he believed he owed Henry nothing. Henry decided to take England out of the war before his ally, signing the Treaty of the More on 30 August 1525.
Annulment from Catherine.
Around this time, Henry conducted an affair with Mary Boleyn, Catherine's lady-in-waiting. There has been speculation that Mary's two children, Catherine and Henry Carey, were fathered by Henry, but this has never been proved and the King never acknowledged them as he did Henry FitzRoy. In 1525, as Henry grew more impatient with Catherine's inability to produce the male heir he desired, he became enamoured of Mary's sister, Anne Boleyn, then a charismatic young woman in the Queen's entourage. Anne, however, resisted his attempts to seduce her, and refused to become his mistress as her sister Mary Boleyn had. It was in this context that Henry considered his three options for finding a dynastic successor and hence resolving what came to be described at court as the King's "great matter". These options were legitimising Henry FitzRoy, which would take the intervention of the pope and would be open to challenge; marrying off Mary as soon as possible and hoping for a grandson to inherit directly, but Mary was an undersized child and was unlikely to conceive before Henry's death; or somehow rejecting Catherine and marrying someone else of child-bearing age. Probably seeing the possibility of marrying Anne, the third was ultimately the most attractive possibility to Henry, and it soon became the King's absorbing desire to annul his marriage to Catherine. It was a decision that would see Henry reject papal authority and initiate the English Reformation.
Henry's precise motivations and intentions over the coming years are not widely agreed on. Henry himself, at least in the early part of his reign, was a devout and well-informed Catholic to the extent that his 1521 publication "Assertio Septem Sacramentorum" ("Defence of the Seven Sacraments") earned him the title of "Fidei Defensor" (Defender of the Faith) from Pope Leo X. The work represented a staunch defence of papal supremacy, albeit one couched in somewhat contingent terms. It is not clear exactly when Henry changed his mind on the issue as he grew more intent on a second marriage. Certainly, by 1527 he had convinced himself that in marrying Catherine, his brother's wife, he had acted contrary to Leviticus 20:21, an impediment the Pope had never had (he now believed) the authority to dispense with. It was this argument Henry took to Pope Clement VII in 1527 in the hope of having his marriage to Catherine annulled, forgoing at least one less openly defiant line of attack. In going public, all hope of tempting Catherine to retire to a nunnery or otherwise stay quiet were lost. Henry sent his secretary, William Knight, to appeal directly to the Holy See by way of a deceptively worded draft papal bull. Knight was unsuccessful; the Pope could not be misled so easily.
Other missions concentrated on arranging an ecclesiastical court to meet in England, with a representative from Clement VII. Though Clement agreed to the creation of such a court, he never had any intention of empowering his legate, Lorenzo Campeggio, to decide in Henry's favour. This bias was perhaps the result of pressure from Charles V, Catherine's nephew, though it is not clear how far this influenced either Campeggio or the Pope. After less than two months of hearing evidence, Clement called the case back to Rome in July 1529, from which it was clear that it would never re-emerge. With the chance for an annulment lost and England's place in Europe forfeit, Wolsey bore the blame; charged with "praemunire" in October 1529, his fall from grace was "sudden and total". Briefly reconciled with Henry (and officially pardoned) in the first half of 1530, he was charged once more in November 1530, this time for treason, but died while awaiting trial. After a short period in which Henry took government upon his own shoulders, Sir Thomas More took on the role of Lord Chancellor and chief minister to Henry. Intelligent and able, but also a devout Catholic and opponent of the annulment, More initially cooperated with the king's new policy, denouncing Wolsey in Parliament.
A year later, Catherine was banished from court, and her rooms were given to Anne. Anne was an unusually educated and intellectual woman for her time, and was keenly absorbed and engaged with the ideas of the Protestant Reformers, though the extent to which she herself was a committed Protestant is much debated. When Archbishop of Canterbury William Warham died, Anne's influence and the need to find a trustworthy supporter of the annulment had Thomas Cranmer appointed to the vacant position. This was approved by the Pope, unaware of the King's nascent plans for the Church.
Marriage to Anne Boleyn.
In the winter of 1532, Henry met with Francis I at Calais and enlisted the support of the French king for his new marriage. Immediately upon returning to Dover in England, Henry and Anne went through a secret wedding service. She soon became pregnant, and there was a second wedding service in London on 25 January 1533. On 23 May 1533, Cranmer, sitting in judgment at a special court convened at Dunstable Priory to rule on the validity of the king's marriage to Catherine of Aragon, declared the marriage of Henry and Catherine null and void. Five days later, on 28 May 1533, Cranmer declared the marriage of Henry and Anne to be valid. Catherine was formally stripped of her title as queen, becoming instead "princess dowager" as the widow of Arthur. In her place, Anne was crowned queen consort on 1 June 1533. The queen gave birth to a daughter slightly prematurely on 7 September 1533. The child was christened Elizabeth, in honour of Henry's mother, Elizabeth of York.
Following the marriage, there was a period of consolidation taking the form of a series of statutes of the Reformation Parliament aimed at finding solutions to a series of particular problems, whilst protecting the new reforms from challenge, convincing the public of their legitimacy, and exposing and dealing with opponents. Although the canon law was dealt with at length by Cranmer and others, these acts were advanced by Thomas Cromwell, Thomas Audley and the Duke of Norfolk as well as a significant role for Henry himself. Following these acts, Thomas More resigned as Lord Chancellor, leaving Cromwell as Henry's chief minister. With the Act of Succession 1533, Catherine's daughter, Mary, was declared illegitimate; his marriage to Anne legitimate; and Anne's issue next in the line of succession. With the Acts of Supremacy in 1534, Parliament also recognised the King's status as head of the church in England and with the Act in Restraint of Appeals in 1532, abolished the right of appeal to Rome. It was only then that Pope Clement took the step of excommunicating Henry and Thomas Cranmer, although it was not made official until some time later.
The king and queen were not pleased with married life. The royal couple enjoyed periods of calm and affection, but Anne refused to play the submissive role expected of her. The vivacity and opinionated intellect that had made her so attractive as an illicit lover made her too independent for the largely ceremonial role of a royal wife, given that Henry expected absolute obedience from those who interacted with him in an official capacity at court. It made her many enemies. For his part, Henry disliked Anne's constant irritability and violent temper. After a false pregnancy or miscarriage in 1534, he saw her failure to give him a son as a betrayal. As early as Christmas 1534, Henry was discussing with Cranmer and Cromwell the chances of leaving Anne without having to return to Catherine. Henry is traditionally believed to have had an affair with Margaret ("Madge") Shelton in 1535, although historian Antonia Fraser argues that Henry in fact had an affair with her sister Mary Shelton.
Opposition to Henry's religious policies was quickly suppressed in England. A number of dissenting monks, including the first Carthusian Martyrs, were executed and many more pilloried. The most prominent resisters included John Fisher, Bishop of Rochester, and Sir Thomas More, both of whom refused to take the oath to the King. Neither Henry nor Cromwell sought to have the men executed; rather, they hoped that the two might change their minds and save themselves. Fisher openly rejected Henry as supreme head of the Church, but – unlike Fisher – More was careful to avoid openly breaking the Treason Act, which (unlike later acts) did not forbid mere silence. Both men were subsequently convicted of high treason, however – More on the evidence of a single conversation with Richard Rich, the Solicitor General. Both were duly executed in the summer of 1535.
These suppressions, as well as the Dissolution of the Lesser Monasteries Act of 1536, in turn contributed to more general resistance to Henry's reforms, most notably in the Pilgrimage of Grace, a large uprising in northern England in October 1536. Some 20,000 to 40,000 rebels were led by Robert Aske, together with parts of the northern nobility. Henry VIII promised the rebels he would pardon them and thanked them for raising the issues to his attention. Aske told the rebels they had been successful and they could disperse and go home. Henry saw the rebels as traitors and did not feel obliged to keep his promises with them, so when further violence occurred after Henry's offer of a pardon he was quick to break his promise of clemency. The leaders, including Aske, were arrested and executed for treason. About 200 rebels were executed, and the disturbances ended.
Execution of Anne Boleyn.
On 8 January 1536 news reached the king and the queen that Catherine of Aragon had died. Henry called for public displays of joy regarding Catherine's death. The queen was pregnant again, and she was aware of the consequences if she failed to give birth to a son. Later that month, the King was unhorsed in a tournament and was badly injured and it seemed for a time that the king's life was in danger. When news of this accident reached the queen, she was sent into shock and miscarried a male child that was about 15 weeks old, on the day of Catherine's funeral, 29 January 1536. For most observers, this personal loss was the beginning of the end of the royal marriage. Given the king's desperate desire for a son, the sequence of Anne's pregnancies has attracted much interest. Author Mike Ashley speculated that Anne had two stillborn children after Elizabeth's birth and before the birth of the male child she miscarried in 1536. Most sources attest only to the birth of Elizabeth in September 1533, a possible miscarriage in the summer of 1534, and the miscarriage of a male child, of almost four months gestation, in January 1536.
Although the Boleyn family still held important positions on the Privy Council, Anne had many enemies, including the Duke of Suffolk. Even her own uncle, the Duke of Norfolk, had come to resent her attitude to her power. The Boleyns preferred France over the Emperor as a potential ally, and the King's favour had swung towards the latter (partly because of Cromwell), damaging the family's influence. Also opposed to Anne were supporters of reconciliation with Princess Mary (among them the former supporters of Catherine), who had now reached maturity. A second annulment was now a real possibility, although it is commonly believed that it was Cromwell's anti-Boleyn influence that led opponents to look for a way of having her executed.
Anne's downfall came shortly after she had recovered from her final miscarriage. Whether it was primarily the result of allegations of conspiracy, adultery or witchcraft remains a matter of debate among historians. Early signs of a fall from grace included the King's new mistress, Jane Seymour, being moved into new quarters, and Anne's brother, George Boleyn, being refused the Order of the Garter, which was instead given to Nicholas Carew. Between 30 April and 2 May, five men, including Anne's brother, were arrested on charges of treasonable adultery, accused of having sexual relationships with the queen. Anne was also arrested, accused of treasonous adultery and incest. Although the evidence against them was unconvincing, the accused were found guilty and condemned to death. George Boleyn and the other accused men were executed on 17 May 1536. At 8 am on 19 May 1536, Anne was executed on Tower Green.
Marriage to Jane Seymour; domestic and foreign affairs.
The day after Anne's execution in 1536 Henry became engaged to Seymour, who had been one of the Queen's ladies-in-waiting. They were married ten days later. On 12 October 1537, Jane gave birth to a son, Prince Edward, the future Edward VI. The birth was difficult, and the queen died on 24 October 1537 from an infection and was buried in Windsor. The euphoria that had accompanied Edward's birth became sorrow, but it was only over time that Henry came to long for his wife. At the time, Henry recovered quickly from the shock. Measures were immediately put in place to find another wife for Henry, which, at the insistence of Cromwell and the court, were focused on the European continent.
With Charles V distracted by the internal politics of his many kingdoms and external threats, and Henry and Francis on relatively good terms, domestic and not foreign policy issues had been Henry's priority in the first half of the 1530s. In 1536, for example, Henry granted his assent to the Laws in Wales Act 1535, which legally annexed Wales, uniting England and Wales into a single nation. This was followed by the Second Succession Act (the Act of Succession 1536), which declared Henry's children by Jane to be next in the line of succession and declared both Mary and Elizabeth illegitimate, thus excluding them from the throne. The king was also granted the power to further determine the line of succession in his will, should he have no further issue. However, when Charles and Francis made peace in January 1539, Henry became increasingly apprehensive. Cromwell as spymaster supplied Henry with a constant list of threats to the kingdom (real or imaginary, minor or serious), and Henry became increasingly paranoid. Enriched by the dissolution of the monasteries, Henry used some of his financial reserves to build a series of coastal defences and set some aside for use in the event of a Franco-German invasion.
Marriage to Anne of Cleves.
At this time, Henry wished to marry once again to ensure the succession. Cromwell, now Earl of Essex, suggested Anne, the sister of the Duke of Cleves, who was seen as an important ally in case of a Roman Catholic attack on England, for the duke fell between Lutheranism and Catholicism. Hans Holbein the Younger was dispatched to Cleves to paint a portrait of Anne for the king. Despite speculation that Holbein painted her in an overly flattering light, it is more likely that the portrait was accurate; Holbein remained in favour at court. After regarding Holbein's portrayal, and urged by the complimentary description of Anne given by his courtiers, the king agreed to wed Anne. However, it was not long before Henry wished to annul the marriage so he could marry another. Anne did not argue, and confirmed that the marriage had never been consummated. Anne's previous betrothal to the Duke of Lorraine's son provided further grounds for the annulment. The marriage was subsequently dissolved, and Anne received the title of "The King's Sister", two houses and a generous allowance. It was already clear that Henry had fallen for Catherine Howard, the Duke of Norfolk's niece, the politics of which worried Cromwell, for Norfolk was a political opponent.
Shortly after, the religious reformers (and protégés of Cromwell) Robert Barnes, William Jerome and Thomas Garret were burned as heretics. Cromwell, meanwhile, fell out of favour although it is unclear exactly why, for there is little evidence of differences of domestic or foreign policy; despite his role, he was not officially accused of being responsible for Henry's failed marriage. Cromwell was now amongst enemies at court, with Norfolk also able to draw on his niece's position. Cromwell was charged with treason, selling export licences, granting passports, and drawing up commissions without permission, and may also have been blamed for the Cleves failure and the failure of the foreign policy it accompanied. He was subsequently attainted and beheaded. Cromwell was not replaced as Vicegerent in Spirituals, a position which had been created for him.
Marriage to Catherine Howard.
On 28 July 1540 (the same day Cromwell was executed), Henry married the young Catherine Howard, a first cousin and lady-in-waiting of Anne Boleyn. He was absolutely delighted with his new queen, and awarded her the lands of Cromwell and a vast array of jewellery. Soon after her marriage, however, Queen Catherine had an affair with the courtier Thomas Culpeper. She employed Francis Dereham, who was previously informally engaged to her and had an affair with her prior to her marriage, as her secretary. The court was informed of her affair with Dereham whilst Henry was away; they dispatched Thomas Cranmer to investigate, who brought evidence of Queen Catherine's previous affair with Dereham to the king's notice. Though Henry originally refused to believe the allegations, Dereham confessed. It took another meeting of the council, however, before Henry believed and went into a rage, blaming the council before consoling himself in hunting. When questioned, the queen could have admitted a prior contract to marry Dereham, which would have made her subsequent marriage to Henry invalid, but she instead claimed that Dereham had forced her to enter into an adulterous relationship. Dereham, meanwhile, exposed Queen Catherine's relationship with Thomas Culpeper. Culpeper and Dereham were executed, and Catherine too was beheaded on 13 February 1542.
In 1540, Henry sanctioned the destruction of shrines to saints. In 1542, England's remaining monasteries were all dissolved, and their property transferred to the Crown. Abbots and priors lost their seats in the House of Lords; only archbishops and bishops came to comprise the ecclesiastical element of the body. The Lords Spiritual, as members of the clergy with seats in the House of Lords were known, were for the first time outnumbered by the Lords Temporal.
Second invasion of France and the "Rough Wooing".
The 1539 alliance between Francis and Charles had soured, eventually degenerating into renewed war. With Catherine of Aragon and Anne Boleyn dead, relations between Charles and Henry improved considerably, and Henry concluded a secret alliance with the Emperor. He decided to enter the Italian War in favour of his new ally. An invasion of France was planned for 1543. In preparation for it, Henry moved to eliminate the potential threat of Scotland under the youthful James V. This would continue the Reformation in Scotland, which was still Catholic, and Henry hoped to unite the crowns of England and Scotland by marriage of James' daughter, the future Mary, Queen of Scots to his son Edward. Henry made war on Scotland for several years in pursuit of this goal, a campaign dubbed "the Rough Wooing".
The Scots were defeated at Battle of Solway Moss on 24 November 1542, and James died on 15 December. The Scottish Regent Arran agreed to the marriage in the Treaty of Greenwich on 1 July 1543.
Despite the success with Scotland, Henry hesitated to invade France, annoying Charles. Henry finally went to France in June 1544 with a two-pronged attack. One force under Norfolk ineffectively besieged Montreuil. The other, under Suffolk, laid siege to Boulogne (19 July 1544). Henry later took personal command, and Boulogne fell on 18 September 1544. However, Henry had refused Charles' request to march against Paris. Charles' own campaign fizzled, and he made peace with France that same day. Henry was left alone against France, unable to make peace. Francis tried to invade England in the summer of 1545, but it was a fiasco. WIth both kingdoms out of money, they signed the Treaty of Camp on 7 June 1546. Henry secured Boulogne for eight years, then to be returned to France for 2 million crowns (£750,000).
Henry needed the money; the 1544 campaign had cost £650,000, and England was once again bankrupt.
Meanwhile, though Henry still clung to the Treaty of Greenwich, the Scots repudiated it in December 1543. Henry launched another war on Scotland, sending an army to burn Edinburgh and lay waste to the country. The Scots would not submit, though. Defeat at Ancrum Moor prompted a second invasion force. This war was nominally ended by the Treaty of Camp. Unrest continued in Scotland, including French and English interventions, up to Henry's death.
Marriage to Catherine Parr.
Henry married his last wife, the wealthy widow Catherine Parr, in July 1543. A reformer at heart, she argued with Henry over religion. Ultimately, Henry remained committed to an idiosyncratic mixture of Catholicism and Protestantism; the reactionary mood which had gained ground following the fall of Cromwell had neither eliminated his Protestant streak nor been overcome by it. Parr helped reconcile Henry with his daughters Mary and Elizabeth. In 1543, an Act of Parliament put the daughters back in the line of succession after Edward, Prince of Wales. The same act allowed Henry to determine further succession to the throne in his will.
Physical decline.
Late in life, Henry became obese, with a waist measurement of 54 in, and had to be moved about with the help of mechanical inventions. He was covered with painful, pus-filled boils and possibly suffered from gout. His obesity and other medical problems can be traced from the jousting accident in 1536, in which he suffered a leg wound. The accident re-opened and aggravated a previous leg wound he had sustained years earlier, to the extent that his doctors found it difficult to treat. The wound festered for the remainder of his life and became ulcerated, thus preventing him from maintaining the level of physical activity he had previously enjoyed. The jousting accident is believed to have caused Henry's mood swings, which may have had a dramatic effect on his personality and temperament.
The theory that Henry suffered from syphilis has been dismissed by most historians. A more recent theory suggests that Henry's medical symptoms are characteristic of untreated type 2 diabetes. Alternatively, his wives' pattern of pregnancies and his mental deterioration have led some to suggest that the king may have been Kell positive and suffered from McLeod syndrome. According to another study, Henry VIII's history and body morphology was probably the result of traumatic brain injury after his 1536 jousting accident, which in turn led to a neuroendocrine cause of his obesity. This analysis identifies growth hormone deficiency (GHD) as the source for his increased adiposity but also significant behavioural changes noted in his later years, including his multiple marriages.
Death and burial.
Henry's obesity hastened his death at the age of 55, which occurred on 28 January 1547 in the Palace of Whitehall, on what would have been his father's 90th birthday. He allegedly uttered his last words: "Monks! Monks! Monks!" perhaps in reference to the monks he caused to be evicted during the Dissolution of the Monasteries. 
On 14 February 1547 Henry's coffin lay overnight at Syon Monastery, "en route" for burial in St George's Chapel, Windsor. Twelve years before in 1535 a Franciscan friar named William Peyto (or Peto, Petow) (d.1558 or 1559), had preached before the King at Greenwich Palace "that God's judgements were ready to fall upon his head and that dogs would lick his blood, as they had done to Ahab", whose infamy rests upon 1 Kings 16:33: "And Ahab did more to provoke the Lord God of Israel to anger than all the kings of Israel that were before him". The prophecy was said to have been fulfilled during this night at Syon, when some "corrupted matter of a bloody colour" fell from the coffin to the floor. 
Henry VIII was interred in St George's Chapel in Windsor Castle, next to Jane Seymour.
Over a hundred years later, King Charles I (1625–1649) was buried in the same vault.
Succession.
After his death, his only legitimate son, Edward, his son by Jane Seymour, inherited the Crown, becoming Edward VI (1547–1553). Since Edward was then only nine years old, he could not exercise actual power. Henry's will designated 16 executors to serve on a council of regency until Edward reached the age of 18. The executors chose Edward Seymour, 1st Earl of Hertford, Jane Seymour's elder brother, to be Lord Protector of the Realm. In default of heirs to Edward, the throne was to pass to Mary, Henry VIII's daughter by Catherine of Aragon, and her heirs. If Mary's issue failed, the crown was to go to Elizabeth, Henry's daughter by Anne Boleyn, and her heirs. Finally, if Elizabeth's line became extinct, the crown was to be inherited by the descendants of Henry VIII's deceased younger sister, Queen Mary of France, the Greys. The descendants of Henry's sister Margaret – the Stuarts, rulers of Scotland – were however excluded from succession. This final provision failed when James VI of Scotland became James I of England upon Elizabeth's death.
Public image.
Henry cultivated the image of a Renaissance man, and his court was a centre of scholarly and artistic innovation and glamorous excess, epitomised by the Field of the Cloth of Gold. He scouted the country for choirboys, taking some directly from Wolsey's choir, and introduced Renaissance music into court. Musicians included Benedict de Opitiis, Richard Sampson, Ambrose Lupo, and Venetian organist Dionisio Memo. Henry himself kept a considerable collection of instruments; he was skilled on the lute, could play the organ, and was a talented player of the virginals. He could also sight read music and sing well. He was an accomplished musician, author, and poet; his best known piece of music is "Pastime with Good Company" ("The Kynges Ballade"). He is often reputed to have written "Greensleeves" but probably did not. He was an avid gambler and dice player, and excelled at sports, especially jousting, hunting, and real tennis. He was known for his strong defence of conventional Christian piety. The King was involved in the original construction and improvement of several significant buildings, including Nonsuch Palace, King's College Chapel, Cambridge and Westminster Abbey in London. Many of the existing buildings Henry improved were properties confiscated from Wolsey, such as Christ Church, Oxford; Hampton Court Palace; the Palace of Whitehall; and Trinity College, Cambridge.
Henry was an intellectual. The first English king with a modern humanist education, he read and wrote English, French and Latin, and was thoroughly at home in his well-stocked library. He personally annotated many books and wrote and published one of his own. He is said to have written the song "Helas madam". He founded Christ Church Cathedral School, Oxford, in 1546. To promote the public support for the reformation of the church, Henry had numerous pamphlets and lectures prepared. For example, Richard Sampson's "Oratio" (1534) was an argument for absolute obedience to the monarchy and claimed that the English church had always been independent from Rome. At the popular level, theatre and minstrel troupes funded by the crown travelled around the land to promote the new religious practices: the pope and Catholic priests and monks were mocked as foreign devils, while the glorious king was hailed as a brave and heroic defender of the true faith. Henry worked hard to present an image of unchallengeable authority and irresistible power.
A large well-built athlete (over six feet tall and strong and broad in proportion), he excelled at jousting and hunting. More than pastimes, they were political devices that served multiple goals, from enhancing his athletic royal image to impressing foreign emissaries and rulers, to conveying Henry's ability to suppress any rebellion. Thus he arranged a jousting tournament at Greenwich in 1517, where he wore gilded armour, gilded horse trappings, and outfits of velvet, satin and cloth of gold dripping with pearls and jewels. It suitably impressed foreign ambassadors, one of whom wrote home that, "The wealth and civilisation of the world are here, and those who call the English barbarians appear to me to render themselves such." Henry finally retired from jousting in 1536 after a heavy fall from his horse left him unconscious for two hours, but he continued to sponsor two lavish tournaments a year. He then started adding weight and lost the trim, athletic figure that had made him so handsome; Henry's courtiers began dressing in heavily padded clothes to emulate – and flatter – their increasingly stout monarch. Towards the end of his reign his health rapidly declined due to unhealthy eating.
Government.
The power of Tudor monarchs, including Henry, was 'whole' and 'entire', ruling, as they claimed, by the grace of God alone. The crown could also rely on the exclusive use of those functions that constituted the royal prerogative. These included acts of diplomacy (including royal marriages), declarations of war, management of the coinage, the issue of royal pardons and the power to summon and dissolve parliament as and when required. Nevertheless, as evident during Henry's break with Rome, the monarch worked within established limits, whether legal or financial, that forced him to work closely with both the nobility and parliament (representing the gentry). In practice, Tudor monarchs used patronage to maintain a royal court that included formal institutions such as the Privy Council as well more informal advisers and confidants. Both the rise and fall of court nobles could be swift: although the often-quoted figure of 72,000 executions during his reign is inflated, Henry did undoubtedly execute at will, burning or beheading two of his wives, twenty peers, four leading public servants, six close attendants and friends, one cardinal (John Fisher) and numerous abbots. Among those who were in favour at any given point in Henry's reign, one could usually be identified as a chief minister, though one of the enduring debates in the historiography of the period has been the extent to which those chief ministers controlled Henry rather than vice versa. In particular, historian G. R. Elton has argued that one such minister, Thomas Cromwell, led a "Tudor revolution in government" quite independently from the king, whom Elton presented as an opportunistic, essentially lazy participant in the nitty-gritty of politics who relied on others both for ideas and to do most of the work. Where Henry did intervene personally in the running of the country, Elton argued, he mostly did so to its detriment. The prominence and influence of faction in Henry's court is similarly discussed in the context of at least five episodes of Henry's reign, including the downfall of Anne Boleyn.
From 1514 to 1529, however, it was Thomas Wolsey (1473–1530), a cardinal of the established Church, who oversaw domestic and foreign policy for the young king from his position as Lord Chancellor. Wolsey centralised the national government and extended the jurisdiction of the conciliar courts, particularly the Star Chamber. The Star Chamber's overall structure remained unchanged, but Wolsey used it to provide for much-needed reform of the criminal law. The power of the court itself did not outlive Wolsey, however, since no serious administrative reform was undertaken and its role eventually devolved to the localities. Wolsey helped fill the gap left by Henry's declining participation in government (particularly in comparison to his father) but did so mostly by imposing himself in the King's place. His use of these courts to pursue personal grievances, and particularly to treat delinquents as if mere examples of a whole class worthy of punishment, angered the rich, who were annoyed as well by his enormous wealth and ostentatious living. Wolsey had greatly disappointed the king when he failed to secure an annulment from Queen Catherine. The treasury was empty after years of extravagance; the peers and people were dissatisfied and Henry needed an entirely new approach; Wolsey had to be replaced. After 16 years at the top, he lost power in 1529 and in 1530 was arrested on false charges of treason and died in custody. Wolsey's fall was a warning to the Pope and to the clergy of England of what might be expected for failure to comply with the king's wishes. Henry then took full control of his government, although at court numerous complex factions continued to try to ruin and destroy each other.
Thomas Cromwell (c. 1485–1540) also came to define Henry's government. Returning to England from the continent in 1514 or 1515, he soon entered Wolsey's service. He turned to law, also picking up a good knowledge of the Bible, and was admitted to Gray's Inn in 1524. He became Wolsey's "man of all work". Cromwell, driven in part by his religious beliefs, attempted to reform the body politic of the English government through discussion and consent, and through the vehicle of continuity and not outward change. He was seen by many people as the man they wanted to bring about their shared aims, including Thomas Audley. By 1531, Cromwell and those associated with him were already responsible for the drafting of much legislation. Cromwell's first office was that of the master of the King's jewels in 1532, from which Cromwell began to invigorate the government finances. By this point, Cromwell's power as an efficient administrator in a Council full of politicians exceeded what Wolsey had achieved. Cromwell did much work through his many offices to remove the tasks of government from the Royal Household (and ideologically from the personal body of the King) and into a public state. He did so, however, in a haphazard fashion that left several remnants, because he needed to retain Henry's support, his own power, and the possibility of actually achieving the plan he set out. Cromwell made the various income streams put in place by Henry VII more formal and assigned largely autonomous bodies for their administration. The role of the King's Council was transferred to a reformed Privy Council, much smaller and more efficient than its predecessor. A difference emerged between the financial health of the king, and that of the country, although Cromwell's fall undermined much of his bureaucracy, which required his hand to keep order among the many new bodies and prevent profligate spending which strained relations as well as finances. Cromwell's reforms ground to a halt in 1539, the initiative lost, and he failed to secure the passage of an enabling act, the Proclamation by the Crown Act 1539. Cromwell's association with the Cleves marriage, whilst not fatal in itself, weakened Cromwell as an anti-Cromwell faction was on the rise. Henry then pursued the hand of Catherine Howard, the Duke of Norfolk's niece, and it was Norfolk who eventually brought Cromwell down. He was executed on 28 July 1540.
Finances.
Financially, the reign of Henry was a near-disaster. Although he inherited a prosperous economy (and further augmented his royal treasury by seizures of church lands), Henry's heavy spending and long periods of mismanagement damaged the economy. Henry hung 2,000 tapestries in his palaces – by comparison, James V of Scotland hung just 200 tapestries. He took pride in showing off his collection of weapons, which included exotic archery equipment, 2,250 pieces of land ordnance and 6,500 handguns.
Henry inherited a vast fortune from his father Henry VII who had, in contrast to his son, been frugal and careful with money. This fortune was estimated to £1,250,000 (£375 million by today's standards). Much of this wealth was spent by Henry on maintaining his court and household, including many of the building works he undertook on royal palaces. Tudor monarchs had to fund all the expenses of government out of their own income. This income came from the Crown lands that Henry owned as well as from customs duties like tonnage and poundage, granted by parliament to the king for life. During Henry's reign the revenues of the Crown remained constant (around £100,000), but were eroded by inflation and rising prices brought about by war. Indeed it was war and Henry's dynastic ambitions in Europe that meant that the surplus he had inherited from his father was exhausted by the mid-1520s. Whereas Henry VII had not involved Parliament in his affairs very much, Henry VIII had to turn to Parliament during his reign for money, in particular for grants of subsidies to fund his wars. The Dissolution of the Monasteries provided a means to replenish the treasury and as a result the Crown took possession of monastic lands worth £120,000 (£36 million) a year. The Crown had profited a small amount in 1526 when Wolsey had put England onto a gold, rather than silver, standard, and had debased the currency slightly. Cromwell debased the currency more significantly, starting in Ireland in 1540. The English pound halved in value against the Flemish pound between 1540 and 1551 as a result. The nominal profit made was significant, helping to bring income and expenditure together, but it had a catastrophic effect on the overall economy of the country. In part, it helped to bring about a period of very high inflation from 1544 onwards.
Reformation.
Henry is generally credited with initiating the English Reformation – the process of transforming England from a Catholic country to a Protestant one – though his progress at the elite and mass levels is disputed, and the precise narrative not widely agreed. Certainly, in 1527, Henry, until then an observant and well-informed Catholic, appealed to the Pope for an annulment of his marriage to Catherine. No annulment was immediately forthcoming, the result in part of Charles V's control of the Papacy. The traditional narrative gives this refusal as the trigger for Henry's rejection of papal supremacy (which he had previously defended), though as historian A. F. Pollard has argued, even if Henry had not needed a divorce, Henry may have come to reject papal control over the governance of England purely for political reasons.
In any case, between 1532 and 1537, Henry instituted a number of statutes that dealt with the relationship between king and pope and hence the structure of the nascent Church of England. These included the Statute in Restraint of Appeals (passed 1533), which extended the charge of "praemunire" against all who introduced papal bulls into England, potentially exposing them to the death penalty if found guilty. Other acts included the Supplication against the Ordinaries and the Submission of the Clergy, which recognised Royal Supremacy over the church. The Ecclesiastical Appointments Act 1534 required the clergy to elect bishops nominated by the Sovereign. The Act of Supremacy in 1534 declared that the King was "the only Supreme Head in Earth of the Church of England" and the Treasons Act 1534 made it high treason, punishable by death, to refuse the Oath of Supremacy acknowledging the King as such. Similarly, following the passage of the Act of Succession 1533, all adults in the Kingdom were required to acknowledge the Act's provisions (declaring Henry's marriage to Anne legitimate and his marriage to Catherine illegitimate) by oath; those who refused were subject to imprisonment for life, and any publisher or printer of any literature alleging that the marriage to Anne was invalid subject to the death penalty. Finally, in response to the excommunication of Henry, the Peter's Pence Act was passed, and it reiterated that England had "no superior under God, but only your Grace" and that Henry's "imperial crown" had been diminished by "the unreasonable and uncharitable usurpations and exactions" of the Pope. The King had much support from the Church under Cranmer.
Henry, to Cromwell's annoyance, insisted on parliamentary time to discuss questions of faith, which he achieved through the Duke of Norfolk. This led to the passing of the Act of Six Articles, whereby six major questions were all answered by asserting the religious orthodoxy, thus restraining the reform movement in England. It was followed by the beginnings of a reformed liturgy and of the Book of Common Prayer, which would take until 1549 to complete. The victory won by religious conservatives did not convert into much change in personnel, however, and Cranmer remained in his position. Overall, the rest of Henry's reign saw a subtle movement away from religious orthodoxy, helped in part by the deaths of prominent figures from before the break with Rome, especially the executions of Thomas More and John Fisher in 1535 for refusing to renounce papal authority. Henry established a new political theology of obedience to the crown that was continued for the next decade. It reflected Martin Luther's new interpretation of the fourth commandment ("Honour thy father and mother"), brought to England by William Tyndale. The founding of royal authority on the Ten Commandments was another important shift: reformers within the Church utilised the Commandments' emphasis on faith and the word of God, while conservatives emphasised the need for dedication to God and doing good. The reformers' efforts lay behind the publication of the "Great Bible" in 1539 in English. Protestant Reformers still faced persecution, particularly over objections to Henry's annulment. Many fled abroad where they met further difficulties, including the influential Tyndale, who was eventually executed and his body burned at Henry's behest.
When taxes once payable to Rome were transferred to the Crown, Cromwell saw the need to assess the taxable value of the Church's extensive holdings as they stood in 1535. The result was an extensive compendium, the "Valor Ecclesiasticus". In September of the same year, Cromwell commissioned a more general visitation of religious institutions, to be undertaken by four appointee visitors. The visitation focussed almost exclusively on the country's religious houses, with largely negative conclusions. In addition to reporting back to Cromwell, the visitors made the lives of the monks more difficult by enforcing strict behavioural standards. The result was to encourage self-dissolution. In any case, the evidence gathered by Cromwell led swiftly to the beginning of the state-enforced dissolution of the monasteries with all religious houses worth less than £200 vested by statute in the crown in January 1536. After a short pause, surviving religious houses were transferred one by one to the Crown and onto new owners, and the dissolution confirmed by a further statute in 1539. By January 1540 no such houses remained: some 800 had been dissolved. The process had been efficient, with minimal resistance, and brought the crown some £90,000 a year. The extent to which the dissolution of all houses was planned from the start is debated by historians; there is some evidence that major houses were originally intended only to be reformed. Cromwell's actions transferred a fifth of England's landed wealth to new hands. The programme was designed primarily to create a landed gentry beholden to the crown, which would use the lands much more efficiently. Although little opposition to the supremacy could be found in England's religious houses, they had links to the international church and were an obstacle to further religious reform.
Response to the reforms was mixed. The religious houses had been the only support of the impoverished, and the reforms alienated much of the population outside London, helping to provoke the great northern rising of 1536–1537, known as the Pilgrimage of Grace. Elsewhere the changes were accepted and welcomed, and those who clung to Catholic rites kept quiet or moved in secrecy. They would re-emerge in the reign of Henry's daughter Mary (1553–1558).
Military.
Apart from permanent garrisons at Berwick, Calais, and Carlisle, England's standing army numbered only a few hundred men. This was increased only slightly by Henry. Henry's invasion force of 1513, some 30,000 men, was composed of billmen and longbowmen, at a time when the other European nations were moving to hand guns and pikemen. The difference in capability was at this stage not significant, however, and Henry's forces had new armour and weaponry. They were also supported by battlefield artillery, a relatively new invention, and several large and expensive siege guns. The invasion force of 1544 was similarly well-equipped and organised, although command on the battlefield was laid with the dukes of Suffolk and Norfolk, which in the case of the latter produced disastrous results at Montreuil.
Henry is traditionally cited as one of the founders of the Royal Navy. Technologically, Henry invested in large cannon for his warships, an idea that had taken hold in other countries, to replace the smaller serpentines in use. He also flirted with designing ships personally – although his contribution to larger vessels, if any, is not known, it is believed that he influenced the design of rowbarges and similar galleys. Henry was also responsible for the creation of a permanent navy, with the supporting anchorages and dockyards. Tactically, Henry's reign saw the Navy move away from boarding tactics to employ gunnery instead. The Navy was enlarged up to fifty ships (the "Mary Rose" was one of them), and Henry was responsible for the establishment of the "council for marine causes" to specifically oversee all the maintenance and operation of the Navy, becoming the basis for the later Admiralty.
Henry's break with Rome incurred the threat of a large-scale French or Spanish invasion. To guard against this, in 1538, he began to build a chain of expensive, state-of-the-art defences, along Britain's southern and eastern coasts from Kent to Cornwall, largely built of material gained from the demolition of the monasteries. These were known as Henry VIII's Device Forts. He also strengthened existing coastal defence fortresses such as Dover Castle and, at Dover, Moat Bulwark and Archcliffe Fort, which he personally visited for a few months to supervise. Wolsey had many years before conducted the censuses required for an overhaul of the system of militia, but no reform came of it. Under Cromwell, in 1538–9, the shire musters were overhauled, but Cromwell's work served most to demonstrate quite how inadequate they were in organisation. The building works, including that at Berwick, along with the reform of the militias and musters, were eventually finished under Queen Mary.
Ireland.
At the beginning of Henry's reign, Ireland was effectively divided into three zones: the Pale, where English rule was unchallenged; Leinster and Munster, the so-called "obedient land" of Anglo-Irish peers; and the Gaelic Connaught and Ulster, with merely nominal English rule. Until 1513, Henry continued the policy of his father, to allow Irish lords to rule in the king's name and accept steep divisions between the communities. However, upon the death of the 8th Earl of Kildare, governor of Ireland, fractional Irish politics combined with a more ambitious Henry to cause trouble. When Thomas Butler, 7th Earl of Ormond died, Henry recognised one successor for Ormond's English, Welsh and Scottish lands, whilst in Ireland another took control. Kildare's successor, the 9th Earl, was replaced as Lord Lieutenant of Ireland by Thomas Howard, Earl of Surrey in 1520. Surrey's ambitious aims were costly, but ineffective; English rule became trapped between winning the Irish lords over with diplomacy, as favoured by Henry and Wolsey, and a sweeping military occupation as proposed by Surrey. Surrey was recalled in 1521, with Piers Butler – one of claimants to the Earldom of Ormond – appointed in his place. Butler proved unable to control opposition, including that of Kildare. Kildare was appointed chief governor in 1524, resuming his dispute with Butler, which had before been in a lull. Meanwhile, the Earl of Desmond, an Anglo-Irish peer, had turned his support to Richard de la Pole as pretender to the English throne; when in 1528 Kildare failed to take suitable actions against him, Kildare was once again removed from his post.
The Desmond situation was resolved on his death in 1529, which was followed by a period of uncertainty. This was effectively ended with the appointment of Henry FitzRoy, Duke of Richmond and the king's son, as lord lieutenant. Richmond had never before visited Ireland, his appointment a break with past policy. For a time it looked as if peace might be restored with the return of Kildare to Ireland to manage the tribes, but the effect was limited and the Irish parliament soon rendered ineffective. Ireland began to receive the attention of Cromwell, who had supporters of Ormond and Desmond promoted. Kildare, on the other hand, was summoned to London; after some hesitation, he departed for London in 1534, where he would face charges of treason. His son, Thomas, Lord Offaly was more forthright, denouncing the king and leading a "Catholic crusade" against the king, who was by this time mired in marital problems. Offaly had the Archbishop of Dublin murdered, and besieged Dublin. Offaly led a mixture of Pale gentry and Irish tribes, although he failed to secure the support of Lord Darcy, a sympathiser, or Charles V. What was effectively a civil war was ended with the intervention of 2,000 English troops – a large army by Irish standards – and the execution of Offaly (his father was already dead) and his uncles.
Although the Offaly revolt was followed by a determination to rule Ireland more closely, Henry was wary of drawn-out conflict with the tribes, and a royal commission recommended that the only relationship with the tribes was to be promises of peace, their land protected from English expansion. The man to lead this effort was Sir Antony St Leger, as Lord Deputy of Ireland, who would remain into the post past Henry's death. Until the break with Rome, it was widely believed that Ireland was a Papal possession granted as a mere fiefdom to the English king, so in 1541 Henry asserted England's claim to the Kingdom of Ireland free from the Papal overlordship. This change did, however, also allow a policy of peaceful reconciliation and expansion: the Lords of Ireland would grant their lands to the King, before being returned as fiefdoms. The incentive to comply with Henry's request was an accompanying barony, and thus a right to sit in the Irish House of Lords, which was to run in parallel with England's. The Irish law of the tribes did not suit such an arrangement, because the chieftain did not have the required rights; this made progress tortuous, and the plan was abandoned in 1543, not to be replaced.
Historiography.
The complexities and sheer scale of Henry's legacy ensured that, in the words of Betteridge and Freeman, "throughout the centuries [since his death], Henry has been praised and reviled, but he has never been ignored". A particular focus of modern historiography has been the extent to which the events of Henry's life (including his marriages, foreign policy and religious changes) were the result of his own initiative and, if they were, whether they were the result of opportunism or of a principled undertaking by Henry. The traditional interpretation of those events was provided by historian A.F. Pollard, who in 1902 presented his own, largely positive, view of the king, "laud[ing him] as the king and statesman who, whatever his personal failings, led England down the road to parliamentary democracy and empire". Pollard's interpretation, which was broadly comparable to 17th century publications of Lord Herbert of Cherbury and his contemporaries, remained the dominant interpretation of Henry's life until the publication of the doctoral thesis of G. R. Elton in 1953. That thesis, entitled "The Tudor Revolution in Government", maintained Pollard's positive interpretation of the Henrician period as a whole, but reinterpreted Henry himself as a follower rather than a leader. For Elton, it was Cromwell and not Henry who undertook the changes in government – Henry was shrewd, but lacked the vision to follow a complex plan through. Henry was little more, in other words, than an "ego-centric monstrosity" whose reign "owed its successes and virtues to better and greater men about him; most of its horrors and failures sprang more directly from [the king]".
Although the central tenets of Elton's thesis have now been all but abandoned, it has consistently provided the starting point for much later work, including that of J. J. Scarisbrick, his student. Scarisbrick largely kept Elton's regard for Cromwell's abilities, but returned agency to Henry, who Scarisbrick considered to have ultimately directed and shaped policy. For Scarisbrick, Henry was a formidable, captivating man who "wore regality with a splendid conviction". The effect of endowing Henry with this ability, however, was largely negative in Scarisbrick's eyes: to Scarisbrick the Henrician period was one of upheaval and destruction and those in charge worthy of blame more than praise. Even among more recent biographers, including David Loades, David Starkey and John Guy, there has ultimately been little consensus on the extent to which Henry was responsible for the changes he oversaw or the correct assessment of those he did bring about.
This lack of clarity about Henry's control over events has contributed to the variation in the qualities ascribed to him: religious conservative or dangerous radical; lover of beauty or brutal destroyer of priceless artefacts; friend and patron or betrayer of those around him; chivalry incarnate or ruthless chauvinist. One traditional approach, favoured by Starkey and others, is to divide Henry's reign into two halves, the first Henry being dominated by positive qualities (politically inclusive, pious, athletic but also intellectual) who presided over a period of stability and calm, and the latter a "hulking tyrant" who presided over a period of dramatic, sometimes whimsical, change. Other writers have tried to merge Henry's disparate personality into a single whole; Lacey Baldwin Smith, for example, considered him an egotistical borderline neurotic given to great fits of temper and deep and dangerous suspicions, with a mechanical and conventional, but deeply held piety, and having at best a mediocre intellect.
Style and arms.
Many changes were made to the royal style during his reign. Henry originally used the style "Henry the Eighth, by the Grace of God, King of England, France and Lord of Ireland". In 1521, pursuant to a grant from Pope Leo X rewarding Henry for his "Defence of the Seven Sacraments", the royal style became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith and Lord of Ireland". Following Henry's excommunication, Pope Paul III rescinded the grant of the title "Defender of the Faith", but an Act of Parliament declared that it remained valid; and it continues in royal usage to the present day. Henry's motto was "Coeur Loyal" ("true heart"), and he had this embroidered on his clothes in the form of a heart symbol and with the word "loyal". His emblem was the Tudor rose and the Beaufort portcullis. As king, Henry's arms were the same as those used by his predecessors since Henry IV: "Quarterly, Azure three fleurs-de-lys Or (for France) and Gules three lions passant guardant in pale Or (for England)".
In 1535, Henry added the "supremacy phrase" to the royal style, which became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith, Lord of Ireland and of the Church of England in Earth Supreme Head". In 1536, the phrase "of the Church of England" changed to "of the Church of England and also of Ireland". In 1541, Henry had the Irish Parliament change the title "Lord of Ireland" to "King of Ireland" with the Crown of Ireland Act 1542, after being advised that many Irish people regarded the Pope as the true head of their country, with the Lord acting as a mere representative. The reason the Irish regarded the Pope as their overlord was that Ireland had originally been given to King Henry II of England by Pope Adrian IV in the 12th century as a feudal territory under papal overlordship. The meeting of Irish Parliament that proclaimed Henry VIII as King of Ireland was the first meeting attended by the Gaelic Irish chieftains as well as the Anglo-Irish aristocrats. The style "Henry the Eighth, by the Grace of God, King of England, France and Ireland, Defender of the Faith and of the Church of England and also of Ireland in Earth Supreme Head" remained in use until the end of Henry's reign.
References.
Bibliography.
</dl>
Further reading.
Primary sources.
</dl>

</doc>
<doc id="14189" url="http://en.wikipedia.org/wiki?curid=14189" title="Haryana">
Haryana

Haryana is a state in North India with its capital at Chandigarh. It came into existence on 1 November 1966 as a newly created state carved out of the Indian Punjab (East Punjab) state on the basis of language. It has been a part of the Kuru region in North India. The name Haryana is found mentioned in the 12th century AD by the Apabhramsha writer Vibudh Shridhar (VS 1189–1230). It is bordered by Punjab and Himachal Pradesh to the north, and by Rajasthan to the west and south. The river Yamuna defines its eastern border with Uttar Pradesh. Haryana also surrounds the country's capital Delhi on three sides, forming the northern, western and southern borders of Delhi. Consequently, a large area of south Haryana is included in the National Capital Region for purposes of planning for development.
Location of the state was home to prominent sites of the Indus Valley and Vedic Civilizations. Several decisive battles were fought in the area, which shaped much of the history of India. These include the epic battle of Mahabharata at Kurukshetra mentioned in the Hindu mythology (including the recital of the Bhagavad Gita by Krishna), and the three battles of Panipat. Haryana was administered as part of the Punjab province of British India, and was carved out on linguistic lines as India's 17th state in 1966. Haryana is now a leading contributor to the country's production of foodgrain and milk. Agriculture is the leading occupation for the residents of the state, the flat arable land irrigated by submersible pumps and an extensive canal system. Haryana contributed heavily to the Green Revolution that made India self-sufficient in food production in the 1960s.
Haryana is also one of the wealthier states of India and had the second highest per capita income in the country at ₹ 119,158 in the year 2012–13 (See List of Indian states by GDP) and ₹ 132,089 in the year 2013–14 including the largest number of rural crorepatis in India. Haryana is also one of the most economically developed regions in South Asia and its agricultural and manufacturing industry has experienced sustained growth since the 1970s. Haryana is India's largest manufacturer of passenger cars, two-wheelers, and tractors. Since 2000, the state has emerged as the largest recipient of investment per capita in India. The city of Gurgaon has rapidly emerged as a major hub for the information technology and automobile industries. Gurgaon is home to Maruti Suzuki, India's largest automobile manufacturer, and Hero MotoCorp, the world's largest manufacturer of two-wheelers. Faridabad, Panchkula, Dharuhera, Bawal, Sonipat, Panipat, Yamuna Nagar and Rewari are also industrial hubs, with the Panipat Refinery being the second largest refinery in South Asia. There are also long established steel, plywood, paper and textile industries in the state.
History.
Etymology.
The name "Haryana" could mean "the Abode of God", derived from the Sanskrit words "Hari" (the Hindu god Vishnu) and "ayana" (home). Scholars such as Muni Lal, Murli Chand Sharma, HA Phadke and Sukhdev Singh Chib believe that the name "Haryana" comes from the words "Hari" (Sanskrit "Harit", "green") and "Aranya" (forest). Dr. Budh Prakash opines that "Haryana" may come from Abhirayana, as its ancient inhabitants were called "Ahirs"; Ahirs ruled Haryana under the Moguls.
Ancient period.
Rakhigarhi in Haryana is home to the largest and one of the oldest sites of the ancient Indus Valley Civilization, Rakhigarhi is a village in Hisar District, the site is dated to be over 5,000 years old. Evidence of paved roads, drainage system, large rainwater collection, storage system, terracotta brick, statue production, and skilled metal working (in both bronze and precious metals) has been uncovered. According to the archeologists, Rakhigarhi is an ideal candidate to believe that the beginning of the Harappan civilisation took place in the Ghaggar basin in Haryana and it gradually grew from here and slowly moved to the Indus valley. Other notable Indus Valley Civilization sites in the state are Mitathal and Banawali.
Also the Vedic Civilization flourished on the banks of the now lost Sarasvati River. Several decisive battles were fought in the area, which shaped much of the history of India. These include the epic Battle of Kurukshetra described in the Mahabharata (including the recital of the Bhagavad Gita by Krishna) and the three battles of Panipat for Uma Singh of Sarmathla. Before she was born, her parents, King Niranjan Singh and Queen Prakash Rani, annexed Delhi and started ruling from there. Ever since the name "Delhi" was coined, it was a praise for Princess Uma Singh.
Medieval period.
Raja Har Rai Dev of Neemrana conquered the region of King Harshavardhana established his capital with Uma Singh's blessings at Thanesar near Kurukshetra in the 7th century AD. After his death, the kingdom of his clansmen continued to rule over a vast region for quite a while from Harsha's adopted capital of Kannauj and founded Gaharwar Kingdom. The region remained strategically important for the rulers of North India even though Thanesar was no more central than Kannauj. Prithviraj Chauhan established forts at Tarori and Hansi in the 12th century. Muhammad Ghori conquered this area in the Second Battle of Tarain. Following his death, the Delhi Sultanate was established that ruled much of north India for several centuries.
The three famous battles of Panipat took place near the modern town of Panipat in Haryana. The first battle took place in 1526, where Babur, the ruler of Kabul, defeated Ibrahim Lodi of the Delhi Sultanate, through the use of field artillery.
Rise of Hem Chandra (Hemu) as a Vikramaditya king.
Hem Chandra Vikramaditya is known to be born in Rewari in south Haryana, started his career as a supplier of merchandise especially, Cannons and Gun Powder to Sher Shah Suri's army, during the 1540s. Gradually, Hem Chandra progressed and held various positions in Suri administration during Sher Shah's son, Islam Shah's regime during 1546–1553, and rose to become Prime Minister and General of Suri army under Adil Shah. During 1553–56, ruling as de facto king of northern India, Hem Chandra won 22 battles continuously against Afghan rebels and Mughal forces from Punjab to Bengal without losing any to consolidate his empire. After defeating Akbar's army at Agra and Delhi in Battle for Delhi (1556), Hem Chandra acceeded to the throne of Delhi on 7 October 1556, declaring 'Hindu Raj' in north India and himself as a Vikramaditya king on the pattern of earlier Hindu kings in India. Hem Chandra lost his life in the second battle of Panipat on November the 5th, 1556, when Akbar's forces defeated this local Haryanvi warrior rightly called Samrat Hem Chandra Vikramaditya.
The decline of the Mughal Empire in early 18th century, led to rapid territorial gains for the Maratha Empire, including Haryana. In 1737, Maratha forces under Baji Rao I sacked Delhi, following their victory against the Mughals in the First Battle of Delhi. A treaty signed in 1752 made Marathas the protector of the Mughal throne at Delhi. Baji Rao's son, Balaji Baji Rao (popularly known as Nana Saheb), further increased the territory under Maratha control by invading Punjab and Peshawar in 1758. This brought the Marathas into direct confrontation with the Durrani empire of Ahmad Shah Abdali, who was based in Kabul. After the Third Battle of Panipat was fought in 1761 between the Maratha Empire and the Afghan warlord Ahmad Shah Abdali, Marathas lost Punjab, Delhi and Haryana to Ahmad Shah Durrani. Within 10 years, Mahadji Shinde re-established Maratha rule over North India, Haryana region remained under the rule of the Scindhia clan of the Maratha Empire, until in 1803, the British East India Company took control of Gurgaon through the Treaty of Surji-Anjangaon after the Second Anglo-Maratha War.
Rao Tularam and the Indian rebellion of 1857.
Rao Tula Ram, a Yadav, was one of the key leaders of the Indian rebellion of 1857, in Haryana, where he is considered a state hero. He is credited with temporarily driving all of the British rule from the region that today is southwest Haryana during the Rebellion, and also helping rebel forces fighting in the historic city of Delhi with men, money and material. Noted as a good administrator and military commander, after the 1857 uprising ended, he left India, met rulers of Iran and Afghanistan and also established contacts with the Tsar of Russia, to seek their help to fight a war to free India from the British. His plans were cut short by his death in Kabul.
Formation of Haryana.
Haryana state was formed on 1 November 1966. The Indian government set up the Shah Commission under the chairmanship of Justice JC Shah on 23 April 1966 to divide the existing Punjab and determine the boundaries of new state Haryana giving consideration to the language spoken by the people. The commission gave its report on 31 May 1966. According to this report the then districts of Hisar, Mahendragarh, Gurgaon, Rohtak and Karnal were to be a part of the new state of Haryana. Further, the tehsils of Jind in (district Sangrur), Narwana in (district Sangrur), Naraingarh, Ambala and Jagadhri were also to be included.
The commission recommended that the tehsil of Kharad (which includes Chandigarh, the state capital of Punjab) should also be a part of Haryana. However, only a small portion of Kharad was given to Haryana. The city of Chandigarh was made a union territory, serving as the capital of both Punjab and Haryana.
Bhagwat Dayal Sharma became first Chief Minister of Haryana.
Geography.
Haryana is a landlocked state in northern India. It is located between 27°39' to 30°35' N latitude and between 74°28' and 77°36' E longitude. The altitude of Haryana varies between 700 to 3600 ft (200 metres to 1200 metres) above sea level. An area of 1,553 km2 is covered by forest. Haryana has four main geographical features.
Rivers of Haryana.
The river Yamuna flows along its eastern boundary. The ancient Sarasvati River is said to have flowed from Yamuna Nagar, but it has now disappeared.
The river Ghaggar is Haryana's main seasonal river. The Ghaggar rises in the outer Himalayas, between the Yamuna and the Sutlej and enters Haryana near Pinjore, Panchkula district. Passing through Ambala and Hissar, it reaches Bikaner in Rajasthan and runs a course of 460 km before disappearing into the deserts of Rajasthan. Important tributary are Chautang and Tangri.
The Markanda River is also a seasonal stream. Its ancient name was Aruna. It originates from the lower Sivalik Hills and enters Haryana west of Ambala. During monsoons, this stream swells into a raging torrent notorious for its devastating power. The surplus water is carried on to the Sanisa lake where the Markanda joins the Saraswati and later Ghaggar. Shahbad Markanda town is situated on its bank.
The Sahibi River, also called Vedic Drishadwati as mentioned in Shatapatha Brahmana originates in the Distt. Jaipur in Rajasthan at present times. However before seismic activities some 7500 years ago in Aravalli Hills, the river brought water even from Ajmer district. Gathering volume from about a hundred tributaries in Rajasthan and Mewat areas, it reaches voluminous proportions, forming a broad stream around Alwar and Patan. Further flowing via Rewari District and Dharuhera, reaching Jhajjar it branches off into two smaller streams, finally reaching the outskirts of Delhi and flowing into Najafgarh lake that flows into the Yamuna through the Najafgarh drain. However, of late, hardly any water flows in Sahibi as most of the water is impounded in small check dams uptream in Alwar district of Rajasthan and the Masani barrage in Rewari district, built on this river on NH 8 (Delhi-Jaipur highway) remains dry.
There are three other rivulets in and around the Mewat hills – Indori, Dohan and Kasavati and they all flow from East to West and once were tributories of Drishadwati/Saraswati rivers.
Climate.
The climate of Haryana is similar to other states of India lying in the northern plains. It is extremely hot in summer, around 45 C and mild in winters. The hottest months are May and June and the coldest being December and January. Rainfall is varied, with the Shivalik region being the wettest and the Aravali Hills region being the driest. About 80% of the rainfall occurs in the monsoon season (July–September) and sometimes causes local flooding.
Flora and fauna.
Thorny, dry, deciduous forest and thorny shrubs can be found all over the state. During the monsoon, a carpet of grass covers the hills. Mulberry, eucalyptus, pine, kikar, shisham and babul are some of the trees found here. The species of fauna found in the state of Haryana include black buck, nilgai, panther, fox, mongoose, jackal and wild dog. More than 300 species of birds are found here.
Protected Wildlife Areas of Haryana.
"See also" List of National Parks & Wildlife Sanctuaries of Haryana, India
The Haryana has 2 National Parks, 8 Wildlife Sanctuaries, 2 Wildlife Conservation Areas, 4 Animal & Bird Breeding Centers, 1 Deer park and 3 Zoos, all of which are managed by the Haryana Forest Department of the Government of Haryana.
Demographics.
Hindus are majority in Haryana and are about 88.23% of the population, Sikhs 5.54%, Muslims 5.78%(mainly Meos, Others 0.45%. In 2001 Hindus made up 18,655,925 of the population, Muslims 1,222,196, Sikhs 1,170,662, Jains 57,167, Christians 27,185, and Buddhists 7,140. Hindu Jats form nearly 25% of the total population and state politics is largely dominated by Hindu Jats followed by the Ahir( yadav) which dominates Southern part of the state with 15% of the total population,Brahman also have a sizable pouplation in haryana consist 11% of the total population, rajput also are present in descent amount in haryana consist 5% of the total population,followed by gujar which consist 2.5% of the total population. Muslims are mainly in the Mewat district and Yamuna Nagar district, while Sikhs are mostly in the districts adjoining Punjab, Hisar, Sirsa, Jind, Fatehabad, Kaithal, Kurukshetra, Ambala, Narnaul and Panchkula. Haryana has second largest Sikh population in India after the state of Punjab. In May 2014 Haryana Government notified the "Haryana Anand Marriages Registration Rules, 2014", allowing Sikhs to register their marriages under these rules. Although the Anand marriage law was enacted in 1909, there was no provision for registration of marriages. The parliament had passed the law allowing Sikhs to register their marriages under the Anand Marriage Act in 2012, but Haryana has issued the notification in 2014.
Meanwhile, the new rules, which have been implemented with immediate effect, would be called Haryana Anand Marriages Registration Rules, 2014
Agriculture and related industries have been the backbone of the local economy. These days the state is seeing a massive
influx of immigrants from across the nation, primarily from Bihar, Bengal, Uttrakhand, Rajasthan, Uttar Pradesh and Nepal. Scheduled Castes form 19.3% of the population.
Government and politics.
As all other states of India, Haryana is governed through a governor, a largely ceremonial position who is appointed by the President of India. The Chief Minister is the head of the Haryana state government and is vested with most of the executive and legislative powers.
Haryana’s legislature is unicameral; its one house, the Haryana Legislative Assembly, consists of 90 members.
Haryana has five seats in the Rajya Sabha, the upper house of India's national parliament, and ten in the Lok Sabha, the lower house. The largest political parties in Haryana are the Indian National Lok Dal, All India Forward Bloc, Communist Party of India (Marxist), Haryana Janhit Congress, Bhartiya Janata Party, Bahujan Samaj Party and Indian National Congress.
Manohar Lal Khattar, a leader of the Bharatiya Janata Party, has been the Chief Minister of the state since October 2014. Jagannath Pahadia, also a leader of the Indian National Congress, was the state's governor from 2009 until July 26, 2014. Sri Kaptan Singh Solanki, a BJP veteran leader was sworn in as the new new governor on 27 July 2014.
Education.
Gurgaon city has the highest literacy rate of 86.30% in Haryana followed by Panchkula at 81.9 per cent and Ambala at 81.7 percent. District Rewari has the highest literacy rate in Haryana of 74%, higher than the national average of 59.5%: male literacy is 79%, and female literacy is 67%.
Hisar, Rohtak and Sonipat are the educational hub of Haryana.
Sonipat has 5000 acre Rajiv Gandhi Education City with a still-growing list of more than 30 educational institutes including several universities, medical colleges, engineering colleges and other institutes.
Hisar has 3 universities Chaudhary Charan Singh Haryana Agricultural University - Asia's largest agricultural university, Guru Jambheshwar University of Science and Technology, Lala Lajpat Rai University of Veterinary & Animal Sciences); several national agricultural and veterinary research centres (National Research Centre on Equines, Central Sheep Breeding Farm, National Institute on Pig Breeding and Research Northern Region Farm Machinery Training and Testing Institute and Central Institute for Research on Buffaloes (CIRB); and more than 20 colleges including Maharaja Agrasen Medical College, Agroha.
Rohtak has almost 22 colleges within the city. There are four engineering colleges and two polytechnic institutes, 32 primary schools, 69 middle schools and 101 high schools were upgraded to middle, high and senior secondary respectively during the 2004–05 school year.
During 2001–02, there were 11,013 primary schools, 1,918 middle schools, 3,023 high schools and 1,301 senior secondary schools in the state. Haryana Board of School Education, established in September 1969 and shifted to Bhiwani in 1981, conducts public examinations at middle, matriculation, and senior secondary levels twice a year. Over seven lac candidates attend annual examinations in February and March, and 150,000 attend supplementary examinations each November. The Board also conducts examinations for Haryana Open School at senior and senior secondary levels twice a year. The Haryana government provides free education to women up to the Bachelor's Degree level.
Haryana boasts of some of the finest colleges in research, technology and management in the country such as National Brain Research Centre, NIT Kurukshetra, Management Development Institute and IIM Rohtak.
National Brain Research Centre, is the only institute in India dedicated to neuroscience research and education. Scientists and students of NBRC come from diverse academic backgrounds, including biological, computational, mathematical, physical, engineering and medical sciences, and use multidisciplinary approaches to understand the brain.Located in the foothills of the Aravali range in Manesar, Haryana, NBRC is an autonomous institute funded by the Department of Biotechnology, Government of India, and is also a Deemed University.
Two sister campuses of IIT Delhi are also approved for Haryana, one in Jhajjar District and other in Sonepat. Government of India is also establishing an Atomic Research Centre and AIIMS-II in villages Kheri Jasaur and Badhsa respectively in Jhajjar District. Shri Shiv Chaitanya college of education is situated in Bhora Kalan in Gurgaon.
Main cities.
The largest cities in Haryana are Faridabad and Gurgaon, which are part of greater Delhi area called National Capital Region.
Other large cities are [Panchkula], Panipat, Ambala, Yamunanagar, Rohtak, Hissar and Karnal.
Culture.
Haryana has a rich cultural heritage that goes back to the Indus Valley Civilization era. Dhosi Hill, the ashram of the mythical Rishi Chyawan is an important site where Chyawanprash was purportedly formulated for the first time.
The last Hindu emperor of India who belonged to Rewari in Haryana, Samrat Hem Chandra Vikramaditya, also called Hemu, declared himself a 'Vikramaditya' king after defeating Akbar's forces in Delhi in 1556. It amounted to establishing a vedic 'Hindu Raj' in North India during medieval period after a gap of more than 350 years. The age-old customs of meditation, Yoga and chanting of Vedic mantras are still observed by the masses. Famous yoga guru Swami Ramdev is from Mahendragarh in Haryana. Seasonal and religious festivals glorify the culture of this region. Haryana has a variety of folk dances.
The people of Haryana have preserved their old religious and social traditions. The 21st century pop-culture in Indian media has portrayed Haryanvi culture as masculine, arrogant and the language as rude/heavy. However, the land and language has its own mellifluous aspect in the folk culture, songs and dance-dramas . Nowadays Haryanavi is spoken in Bollywood movies because of the impression. The culture of Haryana and the humour is very much similar to that of Punjab (as Haryana was a part of Punjab state). They celebrate festivals with great enthusiasm and traditional fervor. Their culture and popular art are saangs, dramas, ballads and songs in which they take great delight. Regarding eating habits, there is an idiom that says, "Hara-Bhara Haryana, Jit Doodh-Dahi ka Khana" (meaning a lush-green state where milk and curd are the food). Food and cuisines of Haryana are almost same as the ones in Punjab (Greater Punjab); popular Haryanavi dishes include makke ki roti (grounded dry corn) and sarso ka saag, lassi (sweet yogurt), rajma, cholay-bhature, etc.
Languages.
Haryanavi has traditionally been the dominant mother tongue in Haryana, with Standard Hindi being spoken as a second language. Haryanvi has no official status, as it is seen as a dialect of Hindi. Therefore Hindi is the official languages and also the most commonly spoken language in the state. Since it was the Punjabi Suba movement that had led to formation of Haryana, Bansi Lal thought, ‘Let any language other than Punjabi be the second language of the state’. Hence, Tamil became the second state language even though there might not have been even a single Tamil native family in the state at that point of time. Since 1947, Punjabi has also been spoken by a lot of people in Haryana especially by those Hindus and Sikhs who came over from the West Punjab, following the Partition of India. As such, Punjabi edged out Tamil as the secondary official language of the state, other than Hindi and English, in 2010.
The most striking feature of Haryana is its language itself or, rather, the manner in which it is spoken. Popularly known as "Haryanavi", (bangaru) the language of Jat peoples of Haryana. With Bangaru, spoken in the Heart of Haryana, being the most widely spoken dialect. Bagri is the 2nd largest dialect of Hindi spoken in Haryana largely in Sirsa, Fatehabad and Hissar. And Ahirwati spoken in Ahirwal belt. With rapid urbanization, and due to Haryana's close proximity to Delhi, the cultural aspects are now taking a more modern hue.
Economy.
The economy of Haryana relies on manufacturing, business process outsourcing, agriculture and retail.
Service industries.
Faridabad and Gurgaon, the two leading financial and industrial cities of Haryana, have seen the emergence of an active information technology industry in recent years. A large number of international companies such as Samsung, Damco Solutions, Abacus Softech, Nokia Networks, Mitsubishi Electric, IBM, Huawei, General Electric, Tata Consultancy Services and Amdocs have their branch offices and contact centres in Faridabad and Gurgaon.
Roads, aviation and infrastructure.
Haryana and Delhi government has also constructed DF Skyway(4.5 km) which connects Delhi And Faridabad. It has been built as per international standards. It is first of its kind in North India. Delhi-Agra Expressway(NH-2) passes through Faridabad is also under construction.
Delhi Metro Rail Corporation connects Faridabad And Gurgaon with Delhi. Faridabad has longest metro network in NCR Region.
Haryana has a total road length of 23,684 kilometers. There are 29 national highways with total length of 1,461 km and many state highways with total length of 2,494 km. The most remote parts of the state are linked with metaled roads. Its modern bus fleet of 3,864 buses covers a distance of 1.15 million Kilometers per day. It was the first State in the country to introduce luxury video coaches.
Grand Trunk Road, commonly abbreviated to GT Road, is one of South Asia's oldest and longest major roads. It passes through the districts of Sonipat, Panipat, Karnal, Kurukshetra and Ambala in north Haryana where it enters Delhi and subsequently the industrial town of Faridabad on its way. The state government proposes to construct Express highways and freeways for speedier vehicular traffic. The 135.6-km long Kundli-Manesar-Palwal Expressway(KMP) will provide a high-speed link to northern Haryana with its southern districts such as Sonepat, Gurgaon, Jhajjar and Faridabad. The work on the project has already started and is scheduled to be completed by July 2013. Haryana is in close contact with the cosmopolitan world, being right next to Delhi. As a result, international and domestic airports, diplomatic and commercial complexes are located in close proximity to the state. There is also a proposal to connect Chandigarh to Haryana without entering Punjab through a 4-lane highway via Yamuna Nagar and Panchkula.
Haryana State has always given high priority to the expansion of electricity infrastructure, as it is one of the most important inputs for the development of the State. Haryana was the first State in the country to achieve 100% rural electrification in 1970, first in the country to link all villages with all-weather roads and first in the country to provide safe drinking water facilities throughout the state. Haryana is well connected on the railway network also.
There are no airports in Haryana. There are proposals to revive the Hisar Airport and Karnal Airport, and a new Greenfield Cargo Airport to serve the city of Rohtak.
Communication and media.
Haryana has a statewide network of telecommunication facilities. Haryana Government has its own statewide area network by which all government offices of 21 districts and 127 blocks across the state are connected with each other thus making it the first SWAN of the country. Bharat Sanchar Nigam Limited (BSNL) and most of the leading private sector players (such as Reliance Infocom, Tata Teleservices, Bharti Telecom, Idea Vodafone Essar, Aircel, Uninor and Videocon) have operations in the state. Important areas around Delhi are also an integral part of the local Delhi Mobile Telecommunication System. This network system would easily cover major towns like Faridabad and Gurgaon.
Telecommunications:
Satellite Television:
Radio:
The major newspapers of Haryana are "Dainik Bhaskar", "Punjab Kesari", "Jag Bani", "Dainik Jagran", "The Tribune", "Amar Ujala", "Hindustan Times", "Dainik Tribune", "The Times of India", and "Hari-Bhumi".
Administrative divisions.
The state is divided into four divisions for administrative purposes: Ambala, Rohtak, Gurgaon and Hisar. Within these there are 21 districts, 58 sub-divisions, 80 tehsils, 50 sub-tehsils and 125 blocks. Haryana has a total of 154 cities and towns and 6,841 villages. Haryana Police force has a modern cybercrime investigation cell, in Gurgaon`s Sector 51. Gurgaon has been referred to as one of "India`s Silicon-Valley".
Sports.
Haryana has produced some of the best Indian players in a variety of sports. In the 2010 Commonwealth Games at Delhi, 22 out of 38 gold medals that India won came from Haryana. During the 33rd National Games held in Assam in 2007, Haryana stood first in the nation with a medal tally of 80, including 30 gold, 22 silver and 28 bronze medals. In team sports, Haryana has been the national champion in men's volleyball and women's hockey. Haryana is a traditional powerhouse in games like kabbadi, kho-kho, judo, boxing, volleyball and wrestling. Sports in the state are managed by the Department of Sports & Youth Affairs, Haryana.
Indian wrestler Sushil Kumar won bronze medal in 2008 Beijing Olympics and silver in 2012 London Olympics and made a world record at the 2010 Commonwealth Games by winning a game in just 11 seconds. At the 2012 Olympics, another wrestler named Yogeshwar Dutt won bronze medal. At the 2008 Olympics, boxer Vijender Singh Beniwal won a bronze medal in the middleweight category. Vikas Krishan Yadav, a boxer from Bhiwani district, won the gold medal in the 2010 Asian Games in the lightweight category. Manoj Kumar of Rajound village, Kaithal district won athe gold medal in light welterweight category at the 2010 Commonwealth Games. Former Indian volleyball player Maratha Balwant Sagwal also hails from Haryana.
Cricket is very popular in Haryana. Former India World Cup winning captain Kapil Dev is from Haryana. Nahar Singh Stadium was built in Faridabad in the year 1981 for international cricket. This ground has the capacity to hold around 25,000 people as spectators. Tejli Sports Complex is an Ultra-Modern sports complex in Yamuna Nagar. Tau Devi Lal Stadium in Panchkula is a multi-sport complex. It came into prominence because of the Indian Cricket League's inaugural Twenty20 tournament. There are Astro-turf hockey grounds in Nehru Stadium, Gurgaon and Shahbad, Kurukshetra. Haryana even has a dedicated sports school MNSS at Rai, Sonipat which is affiliated to Sports Authority of India.
Department of Sports & Youth Affairs, Government of Haryana has started the Sports & Physical Aptitude Test (SPAT) for award of scholarships and kits to budding sportsperson of Haryana. The Haryana SPAT is for the all the students (boys as well as girls) of age group between 8 years to 19 years. This event conducted every year to aims at popularizing sports and channeling resources to high potential athletes. About 5000 Scholarships are on offer in SPAT 2014 for aspiring athlete in 8–19 years age group for year 2014-15.

</doc>
<doc id="14190" url="http://en.wikipedia.org/wiki?curid=14190" title="Himachal Pradesh">
Himachal Pradesh

Himachal Pradesh ]) literally ('Snow-cladden Region') is a state in Northern India.It is spread over 21495 sqmi, and is bordered by Jammu and Kashmir on the north, Punjab on the west and south-west, Haryana and Uttarakhand on the south-east and by the Tibet Autonomous Region on the east.
Himachal Pradesh is famous for its abundant natural beauty. After the war between Nepal and Britain, also known as the Anglo-Gorkha War (1814–1816), the British colonial government came into power. In 1950 Himachal was declared a union territory, but after the State of Himachal Pradesh Act 1971, Himachal emerged as the 18th state of the Republic of India. "Hima" means snow in Sanskrit, and the literal meaning of the state's name is "In the lap of Himalayas". It was named by Acharya Diwakar Datt Sharma, one of the great Sanskrit scholars of Himachal Pradesh.
The economy of Himachal Pradesh is currently the third fastest growing economy in India. Himachal Pradesh has been ranked fourth in the list of the highest per capita incomes of Indian states. This has made Himachal Pradesh one of the most wealthiest places in entire South Asia. Abundance of perennial rivers enables Himachal to sell hydroelectricity to other states such as Delhi, Punjab and Rajasthan. The economy of the state is highly dependent on three sources: hydroelectric power, tourism and agriculture.
Himachal Pradesh is spread across valleys and 90% of the population lives in villages and towns. However the state has achieved 100% hygiene and practically no single house without a toilet. The villages are well connected to roads, Public Health Center and now with Lokmitra kendra using High speed broadband. Shimla district has maximum urban population of 25%. According to a 2005 Transparency International survey, Himachal Pradesh is ranked the second-least corrupt state in the country after Kerala. The hill stations of the state are among the most visited places in country. The government has successfully imposed environmental protection and tourism development meeting European standards and it is the only state which forbids the use of polythene and tobacco products.
History.
The history of the area that now constitutes Himachal Pradesh dates back to the time when the Indus valley civilisation flourished between 2250 and 1750 BCE. Tribes such as the Koilis, Halis, Dagis, Dhaugris, Dasa, Khasas, Kinnars and Kirats inhabited the region from pre-historic era. During the Vedic period, several small republics known as "Janapada" existed which were later conquered by the Gupta Empire. After a brief period of supremacy by King Harshavardhana, the region was once again divided into several local powers headed by chieftains, including some Rajput principalities. These kingdoms enjoyed a large degree of independence and were invaded by Delhi Sultanate a number of times. Mahmud Ghaznavi conquered Kangra at the beginning of the 10th century. Timur and Sikander Lodi also marched through the lower hills of the state and captured a number of forts and fought many battles. Several hill states acknowledged Mughal suzerainty and paid regular tribute to the Mughals.
The Gurkhas, a martial tribe, came to power in Nepal in the year 1768. They consolidated their military power and began to expand their territory. Gradually the Gorkhas annexed Sirmour and Shimla. With the leadership of Amar Singh Thapa, Gorkhas laid siege to Kangra. They managed to defeat Sansar Chand Katoch, the ruler of Kangra, in 1806 with the help of many provincial chiefs. However, Gurkhas could not capture Kangra fort which came under Maharaja Ranjeet Singh in 1809. After the defeat, the Gurkhas began to expand towards the south of the state. However, Raja Ram Singh, Raja of Siba State managed to capture the fort of Siba from the remnants of Lahore Darbar in Samvat 1846, during the First Anglo-Sikh War. They came into direct conflict with the British along the "tarai" belt after which the British expelled them from the provinces of the Satluj. The British gradually emerged as the paramount power. In the revolt of 1857, or first Indian war of independence, arising from a number of grievances against the British, the people of the hill states were not as politically active as were those in other parts of the country. They and their rulers, with the exception of Bushahr, remained more or less inactive. Some, including the rulers of Chamba, Bilaspur, Bhagal and Dhami, rendered help to the British government during the revolt.
The British territories came under the British Crown after Queen Victoria's proclamation of 1858. The states of Chamba, Mandi and Bilaspur made good progress in many fields during the British rule. During World War I, virtually all rulers of the hill states remained loyal and contributed to the British war effort, both in the form of men and materials. Among these were the states of Kangra, Jaswan, Datarpur, Guler, Nurpur, Chamba, Suket, Mandi, and Bilaspur.
After independence the Chief Commissioner's Province of H.P. came into being on 15 April 1948 as a result of integration of 28 petty princely states (including feudatory princes and zaildars) in the promontories of the western Himalaya, known in full as the Simla Hills States & four Punjab southern hill States by issue of the Himachal Pradesh (Administration) Order, 1948 under Sections 3 & 4 of the Extra-Provincial Jurisdiction Act, 1947 (later renamed as the Foreign Jurisdiction Act, 1947 vide A.O. of 1950). The State of Bilaspur was merged in the Himachal Pradesh on 1 April 1954 by the Himachal Pradesh and Bilaspur (New State) Act, 1954. Himachal became a part C state on 26 January 1950 with the implementation of the Constitution of India and the Lt. Governor was appointed. Legislative Assembly was elected in 1952. Himachal Pradesh became a Union Territory on 1 November 1956. Following area of Punjab State namely Simla, Kangra, Kulu and Lahul and Spiti Districts, Nalagarh tehsil of Ambala District, Lohara, Amb and Una kanungo circles, some area of Santokhgarh kanungo circle and some other specified area of Una tehsil of Hoshiarpur District besides some parts of Dhar Kalan Kanungo circle of Pathankot tehsil of Gurdaspur District; were merged with Himachal Pradesh on 1 November 1966 on enactment of Punjab Reorganisation Act, 1966 by the Parliament. On 18 December 1970, the State of Himachal Pradesh Act was passed by Parliament and the new state came into being on 25 January 1971. Thus Himachal emerged as the eighteenth state of the Indian Union.
Geography and climate.
Himachal is in the western Himalayas. Covering an area of 55673 km2, it is a mountainous state.
The drainage system of Himachal is composed both of rivers and glaciers. Himalayan rivers criss-cross the entire mountain chain.
Himachal Pradesh provides water to both the Indus and Ganges basins. The drainage systems of the region are the Chandra Bhaga or the Chenab, the Ravi, the Beas, the Sutlej and the Yamuna. These rivers are perennial and are fed by snow and rainfall. They are protected by an extensive cover of natural vegetation.
Due to extreme variation in elevation, there is great variation in the climatic conditions of Himachal . The climate varies from hot and sub-humid tropical in the southern tracts to, with more elevation, cold, alpine and glacial in the northern and eastern mountain ranges. The state has areas like Dharamsala that receive very heavy rainfall, as well as those like Lahaul and Spiti that are cold and almost rainless. Broadly, Himachal experiences three seasons: Summer, Winter and rainy season. Summer lasts from mid April till the end of June and most parts become very hot (except in alpine zone which experiences a mild summer) with the average temperature ranging from 28 to. Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above 2200 m i.e. in the Higher and Trans-Himalayan region).
Flora and fauna.
According to 2003 Forest Survey of India report, legally defined forest areas constitute 66.52% of the area of Himachal Pradesh, although area under tree cover is only 25.78%. Vegetation in the state is dictated by elevation and precipitation.
The southern part of the state, at lower elevations than the north, has both tropical and subtropical dry broadleaf forests and tropical and subtropical moist broadleaf forests. These are represented by northwestern thorn scrub forests along the border with Haryana and Uttar Pradesh and by Upper Gangetic Plains moist deciduous forests in the far southeast. Sal and shisham are found here.
Rising into the hills, we find a mosaic of western Himalayan broadleaf forests and Himalayan subtropical pine forests. Various deciduous and evergreen oaks live in the broadleaf forests, while Chir pine dominates the pine forests. Western Himalayan subalpine conifer forests grow near treeline, with species that include East Himalayan Fir, West Himalayan Spruce, Deodar (State tree), and Blue pine.
The uppermost elevations have western Himalayan alpine shrub and meadows in the northeast and northwestern Himalayan alpine shrub and meadows in the northwest. Trees are sturdy with a vast network of roots. Alders, birches, rhododendrons and moist alpine shrubs are there as the regional vegetation. The rhododendrons can be seen along the hillsides around Shimla from March to May. The shrublands and meadows give way to rock and ice around the highest peaks.
Himachal is also said to be the fruit bowl of the country, with orchards being widespread. Meadows and pastures are also seen clinging to steep slopes. After the winter season, the hillsides and orchards bloom with wild flowers, while gladiolas, carnations, marigolds, roses, chrysanthemums, tulips and lilies are carefully cultivated. The state government is gearing up to make Himachal Pradesh as the flower basket of the world.
Himachal Pradesh has around 463 bird and 359 animal species, including the leopard, snow leopard (State animal), ghoral, musk deer and Western Tragopan. It has 2 major national parks and sanctuaries — the largest number in the Himalayan region. The Great Himalayan National Park in Kullu district was created to conserve the flora and fauna of the main Himalayan range, while the Pin Valley National Park to conserve the flora and fauna of the cold desert.
Subdivisions.
Himachal Pradesh is divided into 12 districts namely, Kangra, Hamirpur, Mandi, Bilaspur, Una, Chamba, Lahaul and Spiti, Sirmaur, Kinnaur, Kullu, Solan and Shimla. The state capital is Shimla, which was formerly British India's summer capital under the name "Simla".
A district of Himachal Pradesh is an administrative geographical unit, headed by a Deputy Commissioner or District Magistrate, an officer belonging to the Indian Administrative Service. The district magistrate or the deputy commissioner is assisted by a number of officers belonging to Himachal Administrative Service and other Himachal state services. Each district is subdivided into Sub-Divisions, governed by a sub-divisional magistrate, and again into Blocks. Blocks consists of panchayats (village councils) and town municipalities. A Superintendent of Police, an officer belonging to the Indian Police Service is entrusted with the responsibility of maintaining law and order and related issues of the district. He is assisted by the officers of the Himachal Police Service and other Himachal Police officials.
Government.
The Legislative Assembly of Himachal Pradesh has no pre-Constitution history. The State itself is a post-Independence creation. It came into being as a centrally administered territory on 15 April 1948 from the integration of thirty erstwhile princely states.
Himachal Pradesh is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. The legislature consists of elected members and special office bearers such as the Speaker and the Deputy Speaker who are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Himachal Pradesh High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the , although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 68 Members of the Legislative Assembly (MLA). Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as "panchayats", for which local body elections are regularly held, govern local affairs.
Governments have seen alternates between Bharatiya Janata Party (BJP) and Indian National Congress (INC), no third front ever has become significant. In 2003, the state legislative assembly was won by the Indian National Congress and Virbhadra Singh was elected as the chief minister of the state. In the assembly elections held in December 2007, the BJP secured a landslide victory. The BJP won 41 of the 68 seats while the Congress won only 23 of the 68 seats. BJP's Prem Kumar Dhumal was sworn in as Chief Minister of Himachal Pradesh on 30 December 2007.
In the assembly elections held in November 2012, the Congress secured an absolute majority. The Congress won 36 of the 68 seats while the BJP won only 26 of the 68 seats. Virbhadra Singh was sworn-in as Himachal Pradesh's Chief Minister for a record sixth term in Shimla on 25 December 2012. Virbhadra Singh who has held the top office in Himachal five times in the past, was administered the oath of office and secrecy by Governor Urmila Singh at an open ceremony at the historic Ridge Maidan in Shimla.
Agriculture.
Agriculture contributes nearly 45% to the net state domestic product. It is the main source of income as well as employment in Himachal. About 93% of the state population depends directly upon agriculture.
However, agriculture in the state suffers from certain limitations, especially in the production of food grains. One of these reasons is that the area under cultivation can't be extended to an appreciable extent. Also, reclamation of land on slopes is not economical and increases environmental degradation. The state can profit more by cultivating cash crops as per the agro-climatic conditions.
The main cereals grown in the state are wheat, maize, rice and barley. Kangra, Mandi and the Paonta valley of Sirmaur (to some extent) are the major producers of the first three cereals, while barley is mostly cultivated in Shimla.
Though the state is deficient in food grains, it has gained a lot in other spheres of agricultural production such as seed potato, ginger, vegetables, vegetable seeds, mushrooms, chicory seeds, hops, olives and fig. Seed potato is mostly grown in the Shimla, Kullu and Lahaul areas. Special efforts are being made to promote cultivation of crops like olives, figs, hops, mushrooms, flowers, pistachio nuts, sarda melon and saffron. Solan is the largest vegetable producing district in the state. The district of Sirmaur is also famous for growing flowers, and is the largest producer of flowers in the state.
Fruit cultivation has also proved to be an economic boon. There are huge tracts of land suitable only for growing fruits. Fruit of all cultivation does not add to the problem of soil erosion and its employment potential is more than conventional farming. The yield per acre in terms of income is also much higher. Apple farming produces the maximum income. Fruit growing in the state is fetching over ₹ 3 billion annually.
Land husbandry initiatives such as the Mid-Himalayan Watershed Development Project, which includes the Himachal Pradesh Reforestation Project (HPRP), the world's largest clean development mechanism (CDM) undertaking, have improved agricultural yields and productivity, and raised rural household incomes.
Economy.
The era of planning in Himachal Pradesh started 1948 along with the rest of India. The first five-year plan allocated ₹ 52.7 million to Himachal. More than 50% of this expenditure was incurred on road construction since it was felt that without proper transport facilities, the process of planning and development could not be carried to the people, who mostly lived an isolated existence in far away areas. Himachal now ranks fourth in respect of per capita income among the states of the Indian Union.
Agriculture contributes over 45% to the net state domestic product. It is the main source of income and employment in Himachal. Over 93% of the population in Himachal depends directly upon agriculture which provides direct employment to 71% of its people. The main cereals grown are wheat, maize, rice and barley.
Hydro Power is also one of the major source of income generation for the State. Identified Hydroelectric Potential for the state is 23,000.43 MW in five rivers basins.
Himachal is extremely rich in hydro electric resources. The state has about 25% of the national potential in this respect. It has been estimated that about 20,300MW of hydro electric power can be generated in the State by constructing various major, medium, small and mini/micro hydel projects on the five river basins. The state is also the first state in India to achieve the goal of having a bank account for every family.
As per the current prices, the total GDP was estimated at ₹ 254 billion as against ₹ 230 billion in the year 2004–05, showing an increase of 10.5%. The recent years witnessed quick establishment of International Entrepreneurship. Luxury hotels, food and franchisees of recognised brands e.g. Mc Donalds, KFC and Pizza hut have rapidly spread.
Heritage.
Himachal has a rich heritage of handicrafts. These include woolen and pashmina shawls, carpets, silver and metal ware, embroidered chappals, grass shoes, Kangra and Gompa style paintings, wood work, horse-hair bangles, wooden and metal utensils and various other house hold items. These aesthetic and tasteful handicrafts declined under competition from machine made goods and also because of lack of marketing facilities. But now the demand for handicrafts has increased within and outside the country.
Tourism.
Tourism in Himachal Pradesh is one of its major contributor to the state economy and growth. State is endowed with variety of landscape and vivid topographic features which attracts tourists from all the parts of the world. The state is also known for its adventure activities which includes Paragliding in Bir-billing and Solang valley, Rafting in Kullu, ice skating in Shimla, Boating in Bilaspur and various other activities like trekking, horse riding, Skiing, fishing etc.
The state is also a famous destination for film-shooting units that have created movies like "Roja", "Henna (film)", "Jab We Met", "Veer-Zaara", "Yeh Jawaani Hai Deewani", "Highway (2014 Hindi film)" mostly by the contribution of Anil Kaistha.
Major tourist and religious destinations include Bahadurpur fort, bhakra dam, naina devi temple, Manimahesh, Bhuri Singh museum, Bharmaur, khajjiar,prashar lake, rewalsar, chotti kashi mandi, jogindernagar valley, dalhousie, sujanpur tira, dharamshala, palampur, masroor rock temple, kangra fort, kinnaur, manikaran, manali, rohtang pass, spiti, Shimla, kasauli, gobind sagar lake.
Transportation.
State has three domestic airports in Shimla, Kullu and Kangra districts The air routes connect the state with Delhi and Chandigarh.
Himachal is famous for its narrow gauge tracks railways, one is UNESCO World Heritage Kalka-Shimla Railway and another one is Pathankot–Jogindernagar. Total length of these two tracks is 259 km. Kalka-Shimla Railway track passes through many tunnels, while Pathankot–Jogindernagar gently meanders through a maze of hills and valleys. It also has standard gauge railway track which connect Amb (Una district) to Delhi. A survey is being conducted to extend this railway line to Kangra (via Nadaun). Other proposed railways in the state are Baddi-Bilaspur, Dharamsala-Palampur and Bilaspur-Manali-Leh.
Roads are the major mode of transport in the hilly terrains. The state has road network of 28208 km, including eight National Highways (NH) that constitute 1234 km and 19 State Highways with total length of 1625 km. Some roads get closed during winter and monsoon seasons due to snow and landslides. Hamirpur has the highest road density in the state.
Demographics.
Himachal Pradesh has a total population of 6,856,509 including 3,473,892 males and 3,382,617 females as per the provisional results of the Census of India 2011. This is only 0.57 per cent of India's total population, recording a growth of 12.81 per cent. Total fertility rate (TFR) per woman is 1.8 which is one of lowest in India.
Himachal Pradesh has a literacy rate of 83.78 per cent and gender ratio at 974/1000, according to the 2011 Census figures.
Census-wise, the state is placed 21st on the population chart followed by Tripura at 22nd place. Kangra district was top ranked with a population strength of 1,507,223 (21.98%), Mandi district 999,518 (14.58%), Shimla district 813,384 (11.86%), Solan district 576,670 (8.41%), Sirmaur district 530,164 (7.73%), Una district 521,057 (7.60%), Chamba district 518,844 (7.57%), Hamirpur district 454,293 (6.63%), Kullu district 437,474 (6.38%), Bilaspur district 382,056 (5.57%), Kinnaur district 84,298 (1.23%) and Lahaul Spiti 31,528 (0.46%).
The main communities are Rajputs, Rathis, Brahmins and Ghirth. The Ghirth (choudhary) community is found mainly in Kangra District. Himachal has a sizeable population of Tibetans. Himachal Pradesh has the one of the highest proportion of Hindu population in India (95.45%). Other religions that form a small percentage are Buddhism and Sikhism. The Lahaulis of Lahaul and Spiti region are mainly Buddhists. Sikhs mostly live in towns and cities and constitute 1.21% of the state population. For example they form 10% of the population in Una District adjoining the state of Punjab and 17% in Shimla, the state capital. The Buddhists are mainly natives and tribals from Lahaul and Spiti, where they form majority of 60% and Kinnaur where they form 40%, however the bulk are refugees from Tibet. The Muslims constitute slightly 1.94% of the population of Himachal Pradesh.
The life expectancy at birth in Himachal Pradesh is 62.8 years (higher than the national average of 57.7 years) for 1986–1990. The infant mortality rate stood at 40 in 2010 and crude birth rate has declined from 37.3 in 1971 to 16.9 in 2010, below the national average of 26.5 in 1998. The crude death rate was 6.9 in 2010. Himachal Pradesh's literacy rate almost doubled between 1981 and 2011 (see table to right).
Languages.
Hindi is both the official language and the lingua franca of Himachal Pradesh. However, most of the population speaks Pahari in everyday conversation, which includes nearly all Western Pahari dialects. There are total 32 languages in HP
Religion.
Hinduism is the main religion in Himachal Pradesh. Himachal Pradesh is ranked first in India in terms of the proportion of Hindus present within it. More than 95% of the total population belongs to the Hindu faith, the distribution of which is evenly spread throughout the state.
Muslims are mainly present in the Chamba and Solan districts, whilst the minority Buddhist population primarily resides in the Lahul & Sapiti and Kinnour districts.
Culture.
Himachal was one of the few states that had remained largely untouched by external customs, largely due to its difficult terrain. With the technological advancements the state has changed very rapidly. It is a multireligional, multicultural as well as multilingual state like other Indian states. Some of the most commonly spoken languages includes Hindi, Pahari, Dogri, Mandeali Kangri, Mandyali, Gojri and Kinnauri. The caste communities residing in Himachal include the Khatri, Brahmins, Rajputs, Gujjars, Gaddis, Ghirth (choudhary), Kannets, Rathis and Kolis, Sood There are tribal populations in the state which mainly comprise Kinnars, Pangawals, Sulehria, and Lahaulis.
The state is well known for its handicrafts. The carpets, leather works, shawls, metalware, woodwork and paintings are worth appreciating. Pashmina shawls are a product that is highly in demand in Himachal and all over the country. Himachali caps are famous art work of the people. Extreme cold winters of Himachal necessitated wool weaving. Nearly every household in Himachal owns a pit-loom. Wool is considered as pure and is used as a ritual cloth. The well-known woven object is the shawl, ranging from fine pashmina to the coarse desar. Kullu is famous for its shawls with striking patterns and vibrant colours. Kangra and Dharamshala are famous for Kangra miniature paintings.
Local music and dance reflect the cultural identity of the state. Through their dance and music, they entreat their gods during local festivals and other special occasions. Apart from the fairs and festivals that are celebrated all over India, there are number of other fairs and festivals that are of great significance to Himachal Pradesh.
Shimla, the state capital, is home to Asia's only natural ice skating rink.
Food.
The day to day diet of Himachalis is though similar to the rest of north India but is unique for its taste and ingredients used. They have lentil, rice, and vegetables. The rotis (breads) are made of flours like wheat and maize. Some of the specialties of the Himachali cuisine include Madrah, Maahni, Batt, Mitha Saloona, Bhujju, Saag, Palda, Redhu, chouck, bhagjery, jhoul, siddu/batooru, beduan, chutney, khatti dal, etc.
Notable people.
Prominent people associated with Himachal include:
Education.
Hamirpur District is among the top districts in the country for literacy. Education rates among women are quite encouraging in the state. The standard of education in the state has reached a considerably high level as compared to other states in India with several reputed educational institutes for higher studies.
The Indian Institute of Technology Mandi, Himachal Pradesh University Shimla, Institute of Himalayan Bioresource Technology (IHBT, CSIR Lab), Palampur, the National Institute of Technology, Hamirpur, 
Indian Institute of Information Technology, Una the Central University Dharamshala, APG (Alakh Prakash Goyal) Shimla University, The Bahra University (Waknaghat, Solan) the Baddi University of Emerging Sciences and Technologies Baddi, Shoolini University Of Biotechnology and Management Sciences, Solan, the Jaypee University of Information Technology Waknaghat, Eternal University, Sirmaur & Chitkara University Solan are some of the pioneer universities in the state. CSK Himachal Pradesh Krishi Vishwavidyalya Palampur is one of the most renowned hill agriculture institutes in world. Dr. Yashwant Singh Parmar University of Horticulture and Forestry has earned a unique distinction in India for imparting teaching, research and extension education in horticulture, forestry and allied disciplines. Further, state-run Jawaharlal Nehru Government Engineering College started in 2006 at Sundernagar.
The state government is working constantly to prepare plans and projects to strengthen the education system. The state government decided to start three nursing colleges to develop the health system.
There are over 10,000 primary schools, 1,000 secondary schools and more than 1,300 high schools in Himachal. The state government has decided to start three major nursing colleges to develop the health system in the state. In meeting the constitutional obligation to make primary education compulsory, Himachal has become the first state in India to make elementary education accessible to every child.
The state has Indira Gandhi Medical College and Hospital, Homoeopathic Medical College & Hospital, Kumarhatti. Besides that there is Himachal Dental College which is the state's first recognised dental institute.
Media and communication.
Though situated in a remote part of the country, Himachal Pradesh has an active community of journalists and publishers. Several newspapers and magazines are published in more than one language, and their reach extends to almost all the Hindi-speaking states. Radio and TV have permeated significantly. Judging by the number of people writing to these media, there is a very large media-aware population in the state. All major English daily newspapers are available in Shimla and district headquarters. Aapka Faisla, Amar Ujala, Panjab Kesari, Divya Himachal are Hindi daily newspaper with local editions are read widely.
Doordarshan is the state-owned television broadcaster. Doordarshan Shimla also provides programs in Pahari language.Multi system operators provide a mix of Nepali, Hindi, English, and international channels via cable. All India Radio is a public radio station. Private FM stations are also available in few cities like Shimla. BSNL, Reliance Infocomm, Tata Indicom, Tata Docomo, Aircel, Vodafone, Idea Cellular and Airtel are available cellular phone operators. Broadband internet is available in select towns and cities and is provided by the state-run BSNL and by other private companies. Dial-up access is provided throughout the state by BSNL and other providers.
State profile.
Source: "Department of Information and Public Relations."
References.
</dl>

</doc>
<doc id="14192" url="http://en.wikipedia.org/wiki?curid=14192" title="Helene">
Helene

Helene may refer to: slang term, colloquial to Northeast Pennsylvania coal regions, meaning "a big, soft cookie"; i.e., "I'd like a Helene with my coffee"

</doc>
<doc id="14193" url="http://en.wikipedia.org/wiki?curid=14193" title="Hyperion">
Hyperion

Hyperion is the name of a Greek Titan (pre-deity), see Hyperion (mythology)
Hyperion may also refer to:

</doc>
<doc id="14194" url="http://en.wikipedia.org/wiki?curid=14194" title="History of medicine">
History of medicine

This article deals with medicine as practiced by trained professionals from ancient times to the present. The ancient Egyptians, Ancient Indians, had a system of medicine that was advanced for its time and influenced later medical traditions. The Babylonians, Indians and Egyptians had introduced the concepts of medical diagnosis, prognosis, and medical examination. The Greeks went even further, and advanced as well medical ethics. The Hippocratic Oath, still taken (although significantly changed from the original) by doctors up to today, was written in Greece in the 5th century BCE. In the medieval era, surgical practices inherited from the ancient masters were improved and then systematized in Rogerius's "The Practice of Surgery". Universities began systematic training of physicians around the years 1220 in Italy. During the Renaissance, understanding of anatomy improved, and the microscope was invented. The germ theory of disease in the 19th century led to cures for many infectious diseases. Military doctors advanced the methods of trauma treatment and surgery. Public health measures were developed especially in the 19th century as the rapid growth of cities required systematic sanitary measures. Advanced research centers opened in the early 20th century, often connected with major hospitals. The mid-20th century was characterized by new biological treatments, such as antibiotics. These advancements, along with developments in chemistry, genetics, and lab technology (such as the x-ray) led to modern medicine. Medicine was heavily professionalized in the 20th century, and new careers opened to women as nurses (from the 1870s) and as physicians (especially after 1970). The 21st century is characterized by highly advanced research involving numerous fields of science.
Prehistoric medicine.
Although there is no record to establish when plants were first used for medicinal purposes (herbalism), the use of plants as healing agents is a long-standing practice. Over time through emulation of the behavior of fauna a medicinal knowledge base developed and was passed between generations. As tribal culture specialized specific castes, Shamans and apothecaries performed the 'niche occupation' of healing.
Antiquity.
Egypt.
Ancient Egypt developed a large, varied and fruitful medical tradition. Herodotus described the Egyptians as "the healthiest of all men, next to the Libyans", due to the dry climate and the notable public health system that they possessed. According to him, the practice of medicine is so specialized among them that each physician is a healer of one disease and no more." Although Egyptian medicine, to a good extent, dealt with the supernatural, it eventually developed a practical use in the fields of anatomy, public health, and clinical diagnostics.
Medical information in the Edwin Smith Papyrus may date to a time as early as 3000 BC. The earliest known surgery was performed around 2750 BC. Imhotep in the 3rd dynasty is sometimes credited with being the founder of ancient Egyptian medicine and with being the original author of the "Edwin Smith Papyrus", detailing cures, ailments and anatomical observations. The "Edwin Smith Papyrus" is regarded as a copy of several earlier works and was written c. 1600 BC. It is an ancient textbook on surgery almost completely devoid of magical thinking and describes in exquisite detail the "examination, diagnosis, treatment," and "prognosis" of numerous ailments.
The Kahun Gynaecological Papyrus treats women's complaints, including problems with conception. Thirty four cases detailing diagnosis and treatment survive, some of them fragmentarily. Dating to 1800 BCE, it is the oldest surviving medical text of any kind.
Medical institutions, referred to as "Houses of Life" are known to have been established in ancient Egypt as early as the 1st Dynasty.
The earliest known physician is also credited to ancient Egypt: Hesy-Ra, "Chief of Dentists and Physicians" for King Djoser in the 27th century BCE. Also, the earliest known woman physician, Peseshet, practiced in Ancient Egypt at the time of the 4th dynasty. Her title was "Lady Overseer of the Lady Physicians." In addition to her supervisory role, Peseshet trained midwives at an ancient Egyptian medical school in Sais.
Middle East.
The oldest Babylonian texts on medicine date back to the Old Babylonian period in the first half of the 2nd millennium BCE. The most extensive Babylonian medical text, however, is the "Diagnostic Handbook" written by the "ummânū", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BCE).
Along with the Egyptians the Babylonians introduced the practice of diagnosis, prognosis, physical examination, and remedies. In addition, the "Diagnostic Handbook" introduced the methods of therapy and etiology. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.
The "Diagnostic Handbook" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its aetiology and future development, and the chances of the patient's recovery. The symptoms and diseases of a patient were treated through therapeutic means such as bandages, herbs and creams.
There was little development after the medieval era. Major European treatises on medicine took 200 years to reach the Middle East, where local rulers might consult Western doctors to get the latest treatments. Medical works in Arabic, Turkish, and Persian as late as 1800 were based on medieval Islamic medicine.
India.
Siddha medicine usually considered as the oldest medical system known to mankind. Siddha is reported to have surfaced more than 10,000 years ago. The Atharvaveda, a sacred text of Hinduism dating from the Early Iron Age, is one of the first Indian text dealing with medicine, like the medicine of the Ancient Near East based on concepts of the exorcism of demons and magic. The Atharvaveda also contain prescriptions of herbs for various ailments. The use of herbs to treat ailments would later form a large part of Ayurveda.
Ayurveda, meaning the "complete knowledge for long life" is another medical system of India. Origins of Ayurveda have been traced back to 5,000 BCE, originating as an oral tradition and later as medical texts, Ayurveda evolved from the Vedas. Its two most famous texts belong to the schools of Charaka and Sushruta. The earliest foundations of Ayurveda were built on a synthesis of traditional herbal practices together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 600 BCE onwards, and coming out of the communities of thinkers who included the Buddha and others.
According to the compendium of Charaka, the Charakasamhitā, health and disease are not predetermined and life may be prolonged by human effort. The compendium of Suśruta, the Suśrutasamhitā defines the purpose of medicine to cure the diseases of the sick, protect the healthy, and to prolong life. Both these ancient compendia include details of the examination, diagnosis, treatment, and prognosis of numerous ailments. The Suśrutasamhitā is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. Most remarkable is Sushruta's penchant for scientific classification:
His medical treatise consists of 184 chapters, 1,120 conditions are listed, including injuries and illnesses relating to aging and mental illness. The Sushruta Samhita describe 125 surgical instrument, 300 surgical procedures and classifies human surgery in 8 categories.
The Ayurvedic classics mention eight branches of medicine: kāyācikitsā (internal medicine), śalyacikitsā (surgery including anatomy), śālākyacikitsā (eye, ear, nose, and throat diseases), kaumārabhṛtya (pediatrics), bhūtavidyā (spirit medicine), and agada tantra (toxicology), rasāyana
(science of rejuvenation), and vājīkaraṇa (Aphrodisiac). Apart from learning these, the student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis. The teaching of various subjects was done during the instruction of relevant clinical subjects. For example, teaching of anatomy was a part of the teaching of surgery, embryology was a part of training in pediatrics and obstetrics, and the knowledge of physiology and pathology was interwoven in the teaching of all the clinical disciplines.
The normal length of the student's training appears to have been seven years. But the physician was to continue to learn.
As an alternative form of medicine in India, Unani medicine got deep roots and royal patronage during medieval times. It progressed during Indian sultanate and mughal periods. Unani medicine is very close to Ayurveda. Both are based on theory of the presence of the elements (in Unani, they are considered to be fire, water, earth and air) in the human body. According to followers of Unani medicine, these elements are present in different fluids and their balance leads to health and their imbalance leads to illness.
By the 18th century A.D., Sanskrit medical wisdom still dominated. Muslim rulers built large hospitals in 1595 in Hyderabad, and in Delhi in 1719, and numerous commentaries on ancient texts were written.
China.
China also developed a large body of traditional medicine. Much of the philosophy of traditional Chinese medicine derived from empirical observations of disease and illness by Taoist physicians and reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. These causative principles, whether material, essential, or mystical, correlate as the expression of the natural order of the universe.
The foundational text of Chinese medicine is the Huangdi neijing, (or "Yellow Emperor's Inner Canon"), written 5th century to 3rd century BCE. Near the end of the 2nd century AD, during the Han dynasty, Zhang Zhongjing, wrote a "Treatise on Cold Damage", which contains the earliest known reference to the "Neijing Suwen". The Jin Dynasty practitioner and advocate of acupuncture and moxibustion, Huangfu Mi (215-282), also quotes the Yellow Emperor in his "Jiayi jing", c. 265. During the Tang Dynasty, the "Suwen" was expanded and revised, and is now the best extant representation of the foundational roots of traditional Chinese medicine. Traditional Chinese Medicine that is based on the use of herbal medicine, acupuncture, massage and other forms of therapy has been practiced in China for thousands of years.
In the 18th century, during the Qing dynasty, there was a proliferation of popular books as well as more advanced encyclopedias on traditional medicine. Jesuit missionaries introduced Western science and medicine to the royal court, the Chinese physicians ignored them.
Finally in the 19th century, Western medicine was introduced at the local level by Christian medical missionaries from the London Missionary Society (Britain), the Methodist Church (Britain) and the Presbyterian Church (USA). Benjamin Hobson (1816–1873) in 1839, set up a highly successful Wai Ai Clinic in Guangzhou, China. The Hong Kong College of Medicine for Chinese was founded in 1887 by the London Missionary Society, with its first graduate (in 1892) being Sun Yat-sen, who later led the Chinese Revolution (1911). The Hong Kong College of Medicine for Chinese was the forerunner of the School of Medicine of the University of Hong Kong, which started in 1911.
Due to the social custom that men and women should not be near to one another, the women of China were reluctant to be treated by male doctors. The missionaries sent women doctors such as Dr. Mary Hannah Fulton(1854–1927). Supported by the Foreign Missions Board of the Presbyterian Church (USA) she in 1902 founded the first medical college for women in China, the Hackett Medical College for Women, in Guangzhou.
Homer.
Around 800 BCE Homer in The Iliad gives descriptions of wound treatment by the two sons of Asklepios, the admirable physicians Podaleirius and Machaon and one acting doctor, Patroclus. Because Machaon is wounded and Podaleirius is in combat Eurypylus asks Patroclus to "cut out this arrow from my thigh, wash off the blood with warm water and spread soothing ointment on the wound". Askelpios like Imhotep becomes god of healing over time. Temples dedicated to the healer-god Asclepius, known as "Asclepieia" (Ancient Greek: Ἀσκληπιεῖα, sing. Ἀσκληπιεῖον, "'Asclepieion"), functioned as centers of medical advice, prognosis, and healing. At these shrines, patients would enter a dream-like state of induced sleep known as "enkoimesis" (ἐγκοίμησις) not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepieion of Epidaurus, three large marble boards dated to 350 BCE preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium.
The first known Greek medical school opened in Cnidus in 700 BCE. Alcmaeon, author of the first anatomical work, worked at this school, and it was here that the practice of observing patients was established. As was the case elsewhere, the ancient Greeks developed a humoral medicine system where treatment sought to restore the balance of humours within the body.
Hippocrates.
A towering figure in the history of medicine was the physician Hippocrates of Kos (c. 460 – c. 370 BCE), considered the "father of Western medicine." The Hippocratic Corpus is a collection of around seventy early medical works from ancient Greece strongly associated with Hippocrates and his students. Most famously, Hippocrates invented the Hippocratic Oath for physicians, which is still relevant and in use today.
Hippocrates and his followers were first to describe many diseases and medical conditions. He is given credit for the first description of clubbing of the fingers, an important diagnostic sign in chronic suppurative lung disease, lung cancer and cyanotic heart disease. For this reason, clubbed fingers are sometimes referred to as "Hippocratic fingers". Hippocrates was also the first physician to describe Hippocratic face in "Prognosis". Shakespeare famously alludes to this description when writing of Falstaff's death in Act II, Scene iii. of "Henry V".
Hippocrates began to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence."
 Another of Hippocrates's major contributions may be found in his descriptions of the symptomatology, physical findings, surgical treatment and prognosis of thoracic empyema, i.e. suppuration of the lining of the chest cavity. His teachings remain relevant to present-day students of pulmonary medicine and surgery. Hippocrates was the first documented chest surgeon and his findings are still valid.
 Some of the techniques and theories developed by Hippocrates are now put into practice by the fields of Environmental and Integrative Medicine. These include recognizing the importance of taking a complete history which includes environmental exposures as well as foods eaten by the patient which might play a role in his or her illness.
Celsus and Alexandria.
Two great Alexandrians laid the foundations for the scientific study of anatomy and physiology, Herophilus of Chalcedon and Erasistratus of Ceos. Other Alexandrian surgeons gave us; ligature (hemostasis), lithotomy, hernia operations, ophthalmic surgery, plastic surgery, methods of reduction of dislocations and fractures,tracheotomy, and mandrake as anesthesia. Rarly of what we know of them comes from Celsus and Galen of Pergamum (Greek: "Γαληνός")
Herophilus of Chalcedon, working at the medical school of Alexandria placed intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. He and his contemporary, Erasistratus of Chios, researched the role of veins and nerves, mapping their courses across the body. Erasistratus connected the increased complexity of the surface of the human brain compared to other animals to its superior intelligence. He sometimes employed experiments to further his research, at one time repeatedly weighing a caged bird, and noting its weight loss between feeding times. In Erasistratus' physiology, air enters the body, is then drawn by the lungs into the heart, where it is transformed into vital spirit, and is then pumped by the arteries throughout the body. Some of this vital spirit reaches the brain, where it is transformed into animal spirit, which is then distributed by the nerves.
Galen.
The Greek Galen was one of the greatest surgeons of the ancient world and performed many audacious operations—including brain and eye surgeries— that were not tried again for almost two millennia. Later, in medieval Europe, Galen's writings on anatomy became the mainstay of the medieval physician's university curriculum along; but they suffered greatly from stasis and intellectual stagnation. In the 1530s, however, Belgian anatomist and physician Andreas Vesalius took on a project to translate many of Galen's Greek texts into Latin. Vesalius's most famous work, "De humani corporis fabrica", was greatly influenced by Galenic writing and form. The works of Galen were regarded as authoritative until well into the Middle Ages.
The Romans invented numerous surgical instruments, including the first instruments unique to women, as well as the surgical uses of forceps, scalpels, cautery, cross-bladed scissors, the surgical needle, the sound, and speculas. Romans also performed cataract surgery.
Islamic Middle Ages 9th-12th.
The Islamic civilization rose to primacy in medical science as its physicians contributed significantly to the field of medicine, including anatomy, ophthalmology, pharmacology, pharmacy, physiology, surgery, and the pharmaceutical sciences. The Arabs were influenced by, and further developed ancient Indian, Greek, Roman and Byzantine medical practices. Galen & Hippocrates were pre-eminent authorities.The translation of 129 works of ancient Greek physician Galen into Arabic by the Nestorian Christian Hunayn ibn Ishaq and his assistants, and in particular Galen's insistence on a rational systematic approach to medicine, set the template for Islamic medicine, which rapidly spread throughout the Arab Empire. Muslim physicians set up dedicated hospitals,
Medieval Europe 400 to 1400 AD.
After 400 A.D., the study and practice of medicine in the Western Roman Empire went into deep decline. Medical services were provided, especially for the poor, in the thousands of monastic hospitals that sprang up across Europe, but the care was rudimentary and mainly palliative. Most of the writings of Galen and Hippocrates were lost to the West, with the summaries and compendia of St. Isidore of Seville being the primary channel for transmitting Greek medical ideas. The Carolingian renaissance brought increased contact with Byzantium and a greater awareness of ancient medicine, but only with the twelfth century renaissance and the new translations coming from Muslim and Jewish sources in Spain, and the fifteenth century flood of resources after the fall of Constantinople did the West fully recover its acquaintance with classical antiquity.
Wallis identifies a prestige hierarchy with university educated physicians on top, followed by learned surgeons; craft-trained surgeons; barber surgeons; itinerant specialists such as dentist and oculists; empirics; and midwives.
Schools.
The first medical schools were opened, most notably the Schola Medica Salernitana at Salerno in southern Italy. The cosmopolitan influences from Greek, Latin, Arabic, and Hebrew sources gave it an international reputation as the Hippocratic City. Students from wealthy families came for three years of preliminary studies and five of medical studies. By the thirteenth century the medical school at Montpellier began to eclipse the Salernitan school. In the 12th century universities were founded in Italy, France and England which soon developed schools of medicine. The University of Montpellier in France and Italy's University of Padua and University of Bologna were leading schools. Nearly all the learning was from lectures and readings in Hippocrates, Galen, Avicenna and Aristotle. There was little clinical work or dissection.
Humours.
The underlying principle of most medieval medicine was Galen's theory of humours. This was derived from the ancient medical works, and dominated all western medicine until the 19th century. The theory stated that within every individual there were four humours, or principal fluids - black bile, yellow bile, phlegm, and blood, these were produced by various organs in the body, and they had to be in balance for a person to remain healthy. Too much phlegm in the body, for example, caused lung problems; and the body tried to cough up the phlegm to restore a balance. The balance of humours in humans could be achieved by diet, medicines, and by blood-letting, using leeches. The four humours were also associated with the four seasons, black bile-autumn, yellow bile-summer, phlegm-winter and blood-spring.
Healing included both physical and spiritual therapeutics, such as the right herbs, a suitable diet, clean bedding, and the sense that care was always at hand. Other procedures used to help patients included the Mass, prayers, relics of saints, and music used to calm a troubled mind or quickened pulse.
Renaissance to Early Modern period 16th-18th century.
The Renaissance brought an intense focus on scholarship to Christian Europe. A major effort to translate the Arabic and Greek scientific works into Latin emerged. Europeans gradually became experts not only the ancient writings of the Romans and Greeks, but in the contemporary writings of Islamic scientists. During the later centuries of the Renaissance came an increase in experimental investigation, particularly in the field of dissection and body examination, thus advancing our knowledge of human anatomy.
The development of modern neurology began in the 16th century with Vesalius, who described the anatomy of the brain and other organs; he had little knowledge of the brain's function, thinking that it resided mainly in the ventricles. Over his lifetime he corrected over 200 of Galen's mistakes. Understanding of medical sciences and diagnosis improved, but with little direct benefit to health care. Few effective drugs existed, beyond opium and quinine. Folklore cures and potentially poisonous metal-based compounds were popular treatments.
Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work which he paid with his life in 1553. Later this was perfected by Renaldus Columbus and Andrea Cesalpino. Later William Harvey provided a refined and complete description of the circulatory system. The most useful tomes in medicine used both by students and expert physicians were Materia Medica and Pharmacopoeia.
Paracelsus.
Paracelsus (1493–1541), was an erratic and abusive innovator who rejected Galen and bookish knowledge, calling for experimental research, with heavy doses of mysticism, alchemy and magic mixed in. The point is that he rejected sacred magic (miracles) under Church auspisces and looked for cures in nature. He preached but he also pioneered the use of chemicals and minerals in medicine. His hermetical views were that sickness and health in the body relied on the harmony of man (microcosm) and Nature (macrocosm). He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them. Most of his influence came after his death. Paracelsus is a highly controversial figure in the history of medicine, with most experts hailing him as a Father of Modern Medicine for shaking off religious orthodoxy and inspiring many researchers; others say he was a mystic more than a scientist and downplay his importance.
Padua and Bologna.
University training of physicians began in the 13th century.
The University of Padua began teaching medicine in 1222. It played a leading role in the identification and treatment of diseases and ailments, specializing in autopsies and the inner workings of the body. Starting in 1595, Padua's famous anatomical theatre drew artists and scientists studying the human body during public dissections. The intensive study of Galen led to critiques of Galen modeled on his own writing, as in the first book of Vesalius's "De humani corporis fabrica." Andreas Vesalius held the chair of Surgery and Anatomy ("explicator chirurgiae") and in 1543 published his anatomical discoveries in De Humani Corporis Fabrica. He portrayed the human body as an interdependent system of organ groupings. The book triggered great public interest in dissections and caused many other European cities to establish anatomical theatres.
At the University of Bologna the training of physicians began in 1219. The Italian city attracted students from across Europe. Taddeo Alderotti built a tradition of medical education that established the characteristic features of Italian learned medicine and was copied by medical schools elsewhere. Turisanus (d. 1320) was his student. The curriculum was revised and strengthened in 1560–1590. A representative professor was Julius Caesar Aranzi (Arantius) (1530–89). He became Professor of Anatomy and Surgery at the University of Bologna in 1556, where he established anatomy as a major branch of medicine for the first time. Aranzi combined anatomy with a description of pathological processes, based largely on his own research, Galen, and the work of his contemporary Italians. Aranzi discovered the 'Nodules of Aranzio' in the semilunar valves of the heart and wrote the first description of the superior levator palpebral and the coracobrachialis muscles. His books (in Latin) covered surgical techniques for many conditions, including hydrocephalus, nasal polyp, goitre and tumours to phimosis, ascites, haemorrhoids, anal abscess and fistulae.
Women.
Catholic women played large roles in health and healing in medieval and early modern Europe. A life as a nun was a prestigious role; wealthy families provided dowries for their daughters, and these funded the convents, while the nuns provided free nursing care for the poor.
The Catholic elites provided hospital services because of their theology of salvation that good works were the route to heaven. The Protestant reformers rejected the notion that rich men could gain God's grace through good works—and thereby escape purgatory—by providing cash endowments to charitable institutions. They also rejected the Catholic idea that the poor patients earned grace and salvation through their suffering. Protestants generally closed all the convents and most of the hospitals, sending women home to become housewives, often against their will.< On the other hand, local officials recognized the public value of hospitals, and some were continued in Protestant lands, but without monks or nuns and in the control of local governments.
In London, the crown allowed two hospitals to continue their charitable work, under nonreligious control of city officials. The convents were all shut down but Harkness finds that women—some of them former nuns—were part of a new system that delivered essential medical services to people outside their family. They were employed by parishes and hospitals, as well as by private families, and provided nursing care as well as some medical, pharmaceutical, and surgical services.
Meanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries, and enrolled their daughters as nuns who provided free health services to the poor. Nursing was a religious role for the nurse, and there was little call for science.
Age of Enlightenment.
During the Age of Enlightenment, the 18th-century, science was held in high esteem and physicians upgraded their social status by becoming more scientific. The health field was crowded with self-trained barber-surgeons, apothecaries, midwives, drug peddlers, and charlatans.
Across Europe medical schools relied primarily on lectures and readings. In the final year student would have limited clinical experience by trailing the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh, Scotland, with 11,000 alumni, produced large numbers of graduates.
Britain.
In Britain, there were but three small hospitals after 1550. Pelling and Webster estimate that in London in the 1580 to 1600 period, out of a population of nearly 200,000 people, there were about 500 medical practitioners. Nurses and midwives are not included. There were about 50 physicians, 100 licensed surgeons, 100 apothecaries, and 250 additional unlicensed practitioners. In the last category about 25% were women. All across Britain—and indeed all of the world—the vast majority of the people in city, town or countryside depended for medical care on local amateurs with no professional training but with a reputation as wise healers who could diagnose problems and advise sick people what to do—and perhaps set broken bones, pull a tooth, give some traditional herbs or brews or perform a little magic to cure what ailed them.
The London Dispensary opened in 1696, the first clinic in the British Empire to dispense medicines to poor sick people. The innovation was slow to catch on, but new dispensaries were open in the 1770s. In the colonies, small hospitals opened in Philadelphia in 1752, New York in 1771, and Boston (Massachusetts General Hospital) in 1811.
Guy's Hospital, the first great British hospital opened in 1721 in London, with funding from businessman Thomas Guy. In 1821 a bequest of £200,000 by William Hunt in 1829 funded expansion for an additional hundred beds. Samuel Sharp (1709–78), a surgeon at Guy's Hospital, from 1733 to 1757, was internationally famous; his "A Treatise on the Operations of Surgery" (1st ed., 1739), was the first British study focused exclusively on operative technique.
English physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, "Medical Ethics, or a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons" (1803) that set the standard for many textbooks.
19th century: Rise of modern medicine.
The practice of medicine changed in the face of rapid advances in science, as well as new approaches by physicians. Hospital doctors began much more systematic analysis of patients' symptoms in diagnosis. Among the more powerful new techniques were anaesthesia, and the development of both antiseptic and aseptic operating theatres. Actual cures were developed for certain endemic infectious diseases. However the decline in many of the most lethal diseases was more due to improvements in public health and nutrition than to medicine. It was not until the 20th century that the application of the scientific method to medical research began to produce multiple important developments in medicine, with great advances in pharmacology and surgery.
Medicine was revolutionized in the 19th century and beyond by advances in chemistry and laboratory techniques and equipment, old ideas of infectious disease epidemiology were replaced with bacteriology and virology.
Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology.
Germ theory and bacteriology.
In the 1830s in Italy, Agostino Bassi traced the silkworm disease muscardine to microorganisms. Meanwhile in Germany, Theodor Schwann led researches on alcoholic fermentation by yeast and proposed that they were alive—that is, microorganisms—a claim derided by leading chemists, such as Justus von Liebig, seeking solely physicochemical explanation, and alleging that Schwann's was regressing to vitalism. In 1847 in Vienna, Ignaz Semmelweis (1818–1865), by requiring physicians to clean their hands before attending childbirth, dramatically cut new mothers' death rate due to childbed fever, yet his principles were marginalized and attacked by professional peers.
Starting in 1857 by confirming Schwann's fermentation experiments, Louis Pasteur in France placed his eminent reputation behind the belief that yeast are microorganisms, and closed his paper by indicating that such process might also explain contagious diseases. In 1860, Pasteur's report on bacterial fermention to butyric acid motivated fellow Frenchman Casimir Davaine to establish a similar species, which he called "bacteridia", as the pathogen of the disease anthrax, costly to the cattle industry. Yet "bacteridia" were found inconsistently and dismissed as a disease byproduct, not cause. British surgeon Joseph Lister, however, already took cue and introduced antisepsis to wound treatment in 1865.
German physician Robert Koch, noting fellow German Ferdinand Cohn's report of a spore stage of a certain bacterial species, traced the life cycle of Davaine's "bacteridia", identified spores, inoculated laboratory animals with them, and reproduced anthrax—a breakthrough for experimental pathology and germ theory of disease. Pasteur's group added ecological investigations confirming spores' role in the natural setting, while Koch published a landmark treatise in 1878 on the bacterial pathology of wounds. In 1881, Koch reported discovery of the "tubercle bacillus", cementing germ theory and Koch's acclaim.
Upon the outbreak of a cholera epidemic in Alexandria, Egypt, two medical missions went to investigate and attend the sick, one was sent out by Pasteur and the other actually led by Koch. Koch's group returned victorious in 1883, having discovered the cholera pathogen. In Germany, however, Koch's bacteriologists had to vie against Max von Pettenkofer, Germany's leading proponent of miasmatic theory. Pettenkofer conceded bacteria's casual involvement, but maintained that other, environmental factors were required to turn it pathogenic, and opposed water treatment as a misdirected effort amid more important ways to improve public health. The massive cholera epidemic in Hamburg in 1892 devastasted Pettenkoffer's position, and yielded German public health to "Koch's bacteriology".
On losing the 1883 rivalry in Alexandria, Pasteur switched research direction, and introduced his third vaccine—rabies vaccine—the first vaccine for humans since Jenner's for smallpox. From across the globe, donations poured in, funding the founding of Pasteur Institute, the globe's first biomedical institute, which opened in 1888. Along with Koch's bacteriologists, Pasteur's group—which preferred the term "microbiology"—led medicine into the new era of "scientific medicine" upon bacteriology and germ theory. Accepted from Jakob Henle, Koch's steps to confirm a species' pathogenicity became famed as "Koch's postulates". Although his proposed tuberculosis treatment, tuberculin, seemingly failed, it soon was used to test for infection with the involved species. In 1905, Koch was awarded the Nobel Prize in Physiology or Medicine, and remains renowned as the founder of medical microbiology.
Women.
Women as nurses.
Women had always served in ancillary roles, and as midwives and healers. The professionalization of medicine forced them increasingly to the sidelines. As hospitals multiplied they relied in Europe on orders of Roman Catholic nun-nurses, and German Protestant and Anglican deaconesses in the early 19th century. They were trained in traditional methods of physical care that involved little knowledge of medicine. The breakthrough to professionalization based on knowledge of advanced medicine was led by Florence Nightingale in England. She resolved to provide more advanced training than she saw on the Continent. At Kaiserswerth, where the first German nursing schools was founded in 1836 by Theodor Fliedner, she said, "The nursing was nil and the hygiene horrible.") Britain's male doctors preferred the old system, but Nightingale won out and her Nightingale Training School opened in 1860 and became a model. The Nightingale solution depended on the patronage of upper class women, and they proved eager to serve. Royalty became involved. In 1902 the wife of the British king took control of the nursing unit of the British army, became its president, and renamed it after herself as the Queen Alexandra's Royal Army Nursing Corps; when she died the next queen became president. Today its Colonel In Chief is the daughter-in-law of Queen Elizabeth. In the United States, upper middle class women who already supported hospitals promoted nursing. The new profession proved highly attractive to women of all backgrounds, and schools of nursing opened in the late 19th century. They soon a function of large hospitals, where they provided a steady stream of low-paid idealistic workers. The International Red Cross began operations in numerous countries in the late 19th century, promoting nursing as an ideal profession for middle class women.
The Nightingale model was widely copied. Linda Richards (1841 – 1930) studied in London and became the first professionally trained American nurse. She established nursing training programs in the United States and Japan, and created the first system for keeping individual medical records for hospitalized patients. The Russian Orthodox Church sponsored seven orders of nursing sisters in the late 19th century. They ran hospitals, clinics, almshouses, pharmacies, and shelters as well as training schools for nurses. In the Soviet era (1917–1991), with the aristocratic sponsors gone, nursing became a low-prestige occupation based in poorly maintained hospitals.
Women as physicians.
It was very difficult for women to become doctors in any field before the 1970s. Elizabeth Blackwell (1821–1910) became the first woman to formally study and practice medicine in the United States. She was a leader in women's medical education. While Blackwell viewed medicine as a means for social and moral reform, her student Mary Putnam Jacobi (1842–1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties using identical methods, values and insights. In the Soviet Union although the majority of medical doctors were women, they were paid less than the mostly male factory workers.
Paris.
Paris (France) and Vienna were the two leading medical centers on the Continent in the era 1750–1914.
In the 1770s-1850s Paris became a world center of medical research and teaching. The "Paris School" emphasized that teaching and research should be based in large hospitals and promoted the professionalization of the medical profession and the emphasis on sanitation and public health. A major reformer was Jean-Antoine Chaptal (1756–1832), a physician who was Minister of Internal Affairs. He created the Paris Hospital, health councils, and other bodies.
Louis Pasteur (1822–1895) was one of the most important founders of medical microbiology. He is remembered for his remarkable breakthroughs in the causes and preventions of diseases. His discoveries reduced mortality from puerperal fever, and he created the first vaccines for rabies and anthrax. His experiments supported the germ theory of disease. He was best known to the general public for inventing a method to treat milk and wine in order to prevent it from causing sickness, a process that came to be called pasteurization. He is regarded as one of the three main founders of microbiology, together with Ferdinand Cohn and Robert Koch. He worked chiefly in Paris and in 1887 founded the Pasteur Institute there to perpetuate his commitment to basic research and its practical applications. As soon as his institute was created, Pasteur brought together scientists with various specialties. The first five departments were directed by Emile Duclaux (general microbiology research) and Charles Chamberland (microbe research applied to hygiene), as well as a biologist, Ilya Ilyich Mechnikov (morphological microbe research) and two physicians, Jacques-Joseph Grancher (rabies) and Emile Roux (technical microbe research). One year after the inauguration of the Institut Pasteur, Roux set up the first course of microbiology ever taught in the world, then entitled "Cours de Microbie Technique" (Course of microbe research techniques). It became the model for numeous research centers around the world named "Pasteur Institutes."
Vienna.
The First Viennese School of Medicine, 1750–1800, was led by the Dutchman Gerard van Swieten (1700–1772), who aimed to put medicine on new scientific foundations - promoting unprejudiced clinical observation, botanical and chemical research, and introducing simple but powerful remedies. When the Vienna General Hospital opened in 1784, it at once became the world's largest hospital and physicians acquired a facility that gradually developed into the most important research centre. Progress ended with the Napoleonic wars and the government shutdown in 1819 of all liberal journals and schools; this caused a general return to traditionalism and eclecticism in medicine.
Vienna was the capital of a diverse empire and attracted not just Germans but Czechs, Hungarians, Jews, Poles and others to its world-class medical facilities. After 1820 the Second Viennese School of Medicine emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna. The textbook of ophthalmologist Georg Joseph Beer (1763–1821) "Lehre von den Augenkrankheiten" combined practical research and philosophical speculations, and became the standard reference work for decades.
Berlin.
After 1871 Berlin, the capital of the new German Empire, became a leading center for medical research. Robert Koch (1843–1910) was a representative leader. He became famous for isolating "Bacillus anthracis" (1877), the "Tuberculosis bacillus" (1882) and "Vibrio cholerae" (1883) and for his development of Koch's postulates. He was awarded the Nobel Prize in Physiology or Medicine in 1905 for his tuberculosis findings. Koch is one of the founders of microbiology, inspiring such major figures as Paul Ehrlich and Gerhard Domagk.
U.S. Civil War.
In the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on American medicine, from surgical technique to hospitals to nursing and to research facilities.
The hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. There were no antibiotics, so the surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor policing of camps, and dirty camp hospitals took their toll.
This was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies.
The U.S. Army learned many lessons and in August 1886, it established the Hospital Corps.
Statistical methods.
A major breakthrough in epidemiology came with the introduction of statistical maps and graphs. They allowed careful analysis of seasonality issues in disease incidents, and the maps allowed public health officials to identifical critical loci for the dissemination of disease. John Snow in London developed the methods. English nurse Florence Nightingale pioneered analysis of large amounts of statistical data, using graphs and tables, regarding the condition of thousands of patients in the Crimean War to evaluate the efficacy of hospital services. Her methods proved convincing and led to reforms in military and civilian hospitals, usually with the full support of the government.
By the late 19th and early 20th century English statisticians led by Francis Galton, Karl Pearson and Ronald Fisher developed the mathematical tools such as correlations and hypothesis tests that made possible much more sophisticated analysis of statistical data.
During the U.S. Civil War the Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838–1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine, the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning facts into numbers and punching the numbers onto cardboard cards that could be sorted and counted by machine. The applications were developed by his assistant Herman Hollerith; Hollerith invented the punch card and counter-sorter system that dominated statistical data manipulation until the 1970s. Hollerith's company became International Business Machines (IBM) in 1911.
Worldwide dissemination.
Japan.
European ideas of modern medicine were spread widely through the world by medical missionaries, and the dissemination of textbooks. Japanese elites enthusiastically embraced Western medicine after the Meiji Restoration of the 1860s. However they had been prepared by their knowledge of the Dutch and German medicine, for they had some contact with Europe through the Dutch. Highly influential was the 1765 edition of Hendrik van Deventer's pioneer work "Nieuw Ligt" ("A New Light") on Japanese obstetrics, especially on Katakura Kakuryo's publication in 1799 of "Sanka Hatsumo" ("Enlightenment of Obstetrics"). A cadre of Japanese physicians began to interact with Dutch doctors, who introduced smallpox vaccinations. By 1820 Japanese ranpô medical practitioners not only translated Dutch medical texts, they integrated their readings with clinical diagnoses. These men became leaders of the modernization of medicine in their country. They broke from Japanese traditions of closed medical fraternities and adopted the European approach of an open community of collaboration based on expertise in the latest scientific methods.
Kitasato Shibasaburō (1853–1931) studied bacteriology in Germany under Robert Koch. In 1891 he founded the Institute of Infectious Diseases in Tokyo, which introduced the study of bacteriology to Japan. He and French researcher Alexandre Yersin went to Hong Kong in 1894, where; Kitasato confirmed Yersin's discovery that the bacterium "Yersinia pestis" is the agent of the plague. In 1897 he isolates and described the organism that caused dysentery. He became the first dean of medicine at Keio University, and the first president of the Japan Medical Association.
Japanese physicians immediately recognized the values of X-Rays. They were able to purchase the equipment locally from the Shimadzu Company, which developed, manufactured, marketed, and distributed X-Ray machines after 1900. Japan not only adopted German methods of public health in the home islands, but implemented them in its colonies, especially Korea and Taiwan, and after 1931 in Manchuria. A heavy investment in sanitation resulted in a dramatic increase of life expectancy.
Psychiatry.
Until the nineteenth century, the care of the insane was largely a communal and family responsibility rather than a medical one. The vast majority of the mentally ill were treated in domestic contexts with only the most unmanageable or burdensome likely to be institutionally confined. This situation was transformed radically from the late eighteenth century as, amid changing cultural conceptions of madness, a new-found optimism in the curability of insanity within the asylum setting emerged. Increasingly, lunacy was perceived less as a physiological condition than as a mental and moral one to which the correct response was persuasion, aimed at inculcating internal restraint, rather than external coercion. This new therapeutic sensibility, referred to as moral treatment, was epitomised in French physician Philippe Pinel's quasi-mythological unchaining of the lunatics of the Bicêtre Hospital in Paris and realised in an institutional setting with the foundation in 1796 of the Quaker-run York Retreat in England.
From the early nineteenth century, as lay-led lunacy reform movements gained in influence, ever more state governments in the West extended their authority and responsibility over the mentally ill. Small-scale asylums, conceived as instruments to reshape both the mind and behaviour of the disturbed, proliferated across these regions. By the 1830s, moral treatment, together with the asylum itself, became increasingly medicalised and asylum doctors began to establish a distinct medical identity with the establishment in the 1840s of associations for their members in France, Germany, the United Kingdom and America, together with the founding of medico-psychological journals. Medical optimism in the capacity of the asylum to cure insanity soured by the close of the nineteenth century as the growth of the asylum population far outstripped that of the general population. Processes of long-term institutional segregation, allowing for the psychiatric conceptualisation of the natural course of mental illness, supported the perspective that the insane were a distinct population, subject to mental pathologies stemming from specific medical causes. As degeneration theory grew in influence from the mid-nineteenth century, heredity was seen as the central causal element in chronic mental illness, and, with national asylum systems overcrowded and insanity apparently undergoing an inexorable rise, the focus of psychiatric therapeutics shifted from a concern with treating the individual to maintaining the racial and biological health of national populations.
Emil Kraepelin (1856–1926) introduced new medical categories of mental illness, which eventually came into psychiatric usage despite their basis in behavior rather than pathology or etiology. Shell shock among frontline soldiers exposed to heavy artillery bombardment was first diagnosed by British Army doctors in 1915. By 1916, similar symptoms were also noted in soldiers not exposed to explosive shocks, leading to questions as to whether the disorder was physical or psychiatric. In the 1920s surrealist opposition to psychiatry was expressed in a number of surrealist publications. In the 1930s several controversial medical practices were introduced including inducing seizures (by electroshock, insulin or other drugs) or cutting parts of the brain apart (leucotomy or lobotomy). Both came into widespread use by psychiatry, but there were grave concerns and much opposition on grounds of basic morality, harmful effects, or misuse.
In the 1950s new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control. There was also increasing opposition to the use of psychiatric hospitals, and attempts to move people back into the community on a collaborative user-led group approach ("therapeutic communities") not controlled by psychiatry. Campaigns against masturbation were done in the Victorian era and elsewhere. Lobotomy was used until the 1970s to treat schizophrenia. This was denounced by the anti-psychiatric movement in the 1960s and later.
20th century.
First World War.
The ABO blood group system was discovered in 1901, and the Rhesus group in 1937, facilitating blood transfusion.
During the 20th century, large-scale wars were attended with medics and mobile hospital units which developed advanced techniques for healing massive injuries and controlling infections rampant in battlefield conditions. Thousands of scarred troops provided the need for improved prosthetic limbs and expanded techniques in plastic surgery or reconstructive surgery. Those practices were combined to broaden cosmetic surgery and other forms of elective surgery.
During the First World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.
The Great War spurred the usage of Roentgen's X-ray, and the electrocardiograph, for the monitoring of internal bodily functions. This was followed in the inter-war period by the development of the first anti-bacterial agents such as the sulpha antibiotics.
Public health.
The 1918 flu pandemic killed at least 50 million people around the world. It became an important case study in epidemiology. Bristow shows there was a gendered response of health caregivers to the pandemic in the United States. Male doctors were unable to cure the patients, and they felt like failures. Women nurses also saw their patients die, but they took pride in their success in fulfilling their professional role of caring for, ministering, comforting, and easing the last hours of their patients, and helping the families of the patients cope as well.
From 1917 to 1923, the American Red Cross moved into Europe with a battery of long-term child health projects. It built and operated hospitals and clinics, and organized antituberculosis and antityphus campaigns. A high priority involved child health programs such as clinics, better baby shows, playgrounds, fresh air camps, and courses for women on infant hygiene. Hundreds of U.S. doctors, nurses, and welfare professionals administered these programs, which aimed to reform the health of European youth and to reshape European public health and welfare along American lines.
Second World War.
The advances in medicine made a dramatic difference for Allied troops, while the Germans and especially the Japanese and Chinese suffered from a severe lack of newer medicines, techniques and facilities. Harrison finds that the chances of recovery for a badly wounded British infantryman were as much as 25 times better than in the First World War. The reason was that:
Nazi and Japanese medicine.
Unethical human subject research, and killing of patients with disabilities, peaked during the Nazi era, with Nazi human experimentation and Aktion T4 during the Holocaust as the most significant examples. Many of the details of these and related events were the focus of the Doctors' Trial. Subsequently, principles of medical ethics, such as the Nuremberg Code, were introduced to prevent a recurrence of such atrocities. After 1937, the Japanese Army established programs of biological warfare in China. In Unit 731, Japanese doctors and research scientists conducted large numbers of vivisections and experiments on human beings, mostly Chinese victims.
Malaria.
Starting in World War II, DDT was used as insecticide to combat insect vectors carrying malaria, which was endemic in most tropical regions of the world. The first goal was to protect soldiers, but it was widely adopted as a public health device. In Liberia, for example, the United States had large military operations during the war and the U.S. Public Health Service began the use of DDT for indoor residual spraying (IRS) and as a larvicide, with the goal of controlling malaria in Monrovia, the Liberian capital. In the early 1950s, the project was expanded to nearby villages. In 1953, the World Health Organization (WHO) launched an antimalaria program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However these projects encountered a spate of difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.
Post-World War II.
The World Health Organization was founded in 1948 as a United Nations agency to improve global health. In most of the world, life expectancy has improved since then, and was about 67 years as of 2010, and well above 80 years in some countries. Eradication of infectious diseases is an international effort, and several new vaccines have been developed during the post-war years, against infections such as measles, mumps, several strains of influenza and human papilloma virus. The long-known vaccine against Smallpox finally eradicated the disease in the 1970s, and Rinderpest was wiped out in 2011. Eradication of polio is underway. Tissue culture is important for development of vaccines. Though the early success of antiviral vaccines and antibacterial drugs, antiviral drugs were not introduced until the 1970s. Through the WHO, the international community has developed a response protocol against epidemics, displayed during the SARS epidemic in 2003, the Influenza A virus subtype H5N1 from 2004, the Ebola virus epidemic in West Africa and onwards.
As infectious diseases have become less lethal, and the most common causes of death in developed countries are now tumors and cardiovascular diseases, these conditions have received increased attention in medical research. Tobacco smoking as a cause of lung cancer was first researched in the 1920s, but was not widely supported by publications until the 1950s. Cancer treatment has been developed with radiotherapy, chemotherapy and surgical oncology.
Oral rehydration therapy has been extensively used since the 1970s to treat cholera and other diarrhea-inducing infections.
Hormonal contraception was introduced in the 1950s, and was associated with the sexual revolution, with normalization of abortion and homosexuality in many countries. Family planning has promoted a demographic transition in most of the world. With threatening sexually transmitted infections, not least HIV, use of barrier contraception has become imperative. The struggle against HIV has improved antiretroviral treatments, and in the late 2000s (decade), male circumcision was cited to diminish infection risk (see circumcision and HIV). In 2013, the first patient was cured from HIV.
X-ray imaging was the first kind of medical imaging, and later ultrasonic imaging, CT scanning, MR scanning and other imaging methods became available.
Genetics have advanced with the discovery of the DNA molecule, genetic mapping and gene therapy. Stem cell research took off in the 2000s (decade), with stem cell therapy as a promising method.
Evidence-based medicine is a modern concept, not introduced to literature until the 1990s.
Prosthetics have improved. In 1958, Arne Larsson in Sweden became the first patient to depend on an artificial cardiac pacemaker. He died in 2001 at age 86, having outlived its inventor, the surgeon, and 26 pacemakers. Lightweight materials as well as neural prosthetics emerged in the end of the 20th century.
Modern surgery.
Cardiac surgery was revolutionized in the late 1940s, as open-heart surgery was introduced.
In 1954 Joseph Murray, J. Hartwell Harrison and others accomplished the first kidney transplantation. Transplantations of other organs, such as heart, liver and pancreas, were also introduced during the latter 20th century. The first partial face transplant was performed in 2005, and the first full one in 2010. By the end of the 20th century, microtechnology had been used to create tiny robotic devices to assist microsurgery using micro-video and fiber-optic cameras to view internal tissues during surgery with minimally invasive practices.
Laparoscopic surgery was broadly introduced in the 1990s. Natural orifice surgery has followed. Remote surgery is another recent development, with the Lindbergh operation in 2001 as a groundbreaking example.

</doc>
<doc id="14196" url="http://en.wikipedia.org/wiki?curid=14196" title="Hamoaze">
Hamoaze

The Hamoaze (; ]) is an estuarine stretch of the tidal River Tamar, between its confluence with the River Lynher and Plymouth Sound, England.
The name first appears as "ryver of Hamose" in 1588 and it originally most likely applied just to a creek of the estuary that led up to the manor of Ham, north of the present-day Devonport Dockyard. The name evidently later came to be used for the estuary's main channel. The "ose" element possibly derives from Old English "wāse" meaning 'mud' (as in 'ooze') – the creek consisting of mud-banks at low tide, although this is not confirmed.
The Hamoaze flows past Devonport Dockyard, which is one of three major bases of the Royal Navy today. The presence of large numbers of small watercraft are a challenge and hazard to the warships using the naval base and dockyard. Navigation on the waterway is controlled by the Queen's Harbour Master for Plymouth.
Settlements on the banks of the Hamoaze are Saltash, Wilcove, Torpoint and Cremyll in Cornwall, as well as Devonport and Plymouth in Devon.
Two regular ferry services crossing the Hamoaze exist: the Torpoint Ferry (a chain ferry that takes vehicles) and the Cremyll Ferry (passengers and cyclists only).

</doc>
<doc id="14197" url="http://en.wikipedia.org/wiki?curid=14197" title="Hanover">
Hanover

 
Hanover or Hannover (; German: "Hannover", ]), on the River Leine, is the capital of the federal state of Lower Saxony ("Niedersachsen"), Germany and was once by personal union the family seat of the Hanoverian Kings of Great Britain, under their title as the dukes of Brunswick-Lüneburg (later described as the Elector of Hanover). At the end of the Napoleonic Wars, the Electorate was enlarged to become the capital of the Kingdom of Hanover. Hanover is located 177 mi west of Berlin, 94 mi south of Hamburg, 392 mi north of Munich, and 85 mi east of Osnabrück.
From 1868 to 1946 Hanover was the capital of the Prussian Province of Hanover and also of the Hanover administrative region until that was abolished in 2005. It is now the capital of the "Land" of Lower Saxony. Since 2001 it is part of the Hanover district ("Region Hannover"), which is a municipal body made up from the former district ("Landkreis Hannover") and city of Hanover (note: although both "Region" and "Landkreis" are translated as "district" they are not the same).
With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city. Hanover also hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau). In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin - Ruhr area) and north-south (Hamburg - Munich, etc.) directions.
"Hanover" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and local government uses the German spelling on English websites. The English pronunciation is applied to both the German and English spellings, which is different from German pronunciation ]. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.
History.
Hanover was founded in medieval times on the east bank of the River Leine. Its original name "Honovere" may mean "high (river)bank", though this is debated (cf. "das Hohe Ufer"). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.
In the 14th century the main churches of Hanover were built, as well as a city wall with three city gates. The beginning of industrialization in Germany led to trade in iron and silver from the northern Harz Mountains, which increased the city's importance.
In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its electors would later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who ruled in Hanover was William IV. Salic law, which required succession by the male line, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were at the same time Electoral Princes of Hanover.
During the time of the personal union of the crowns of the United Kingdom and Hanover (1714–1837), the monarchs rarely visited the city. In fact, during the reigns of the final three joint rulers (1760–1837), there was only one short visit, by George IV in 1821. From 1816 to 1837 Viceroy Adolphus represented the monarch in Hanover.
During the Seven Years' War, the Battle of Hastenbeck was fought near the city on 26 July 1757. The French army defeated the Hanoverian Army of Observation, leading to the city's occupation as part of the Invasion of Hanover. It was recaptured by Anglo-German forces led by Ferdinand of Brunswick the following year.
19th century.
After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 30,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was the only German army to fight against France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably.
In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). According to Salic Law Hanover could only be inherited by male heirs. Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government.
For Hanover's industry, however, the new connection with Prussia meant an improvement in business. The introduction of free trade promoted economic growth, and led to the recovery of the Gründerzeit (founders' era). Between 1871 and 1912 Hanover's population grew from 87,600 to 313,400.
In 1872 the first horse railway was inaugurated, and from 1893 an electric tram was installed. In 1887 Hanover's Emile Berliner invented the record and the gramophone.
Nazi Germany.
After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). As large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family . However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Gryszpan's son Herschel Grynszpan was in Paris at the time. When he learned what was happening, he drove to the German embassy in Paris and murdered the German diplomat Eduard Ernst vom Rath.
After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover. The Orthodox Jewish community was led by Rabbi Chaim Pinchos Lubinsky. Rabbi Lubinsky was assisted in this capacity by Rabbi Shlomo Zev Zweigenhaft . Following the departure of Rabbi Lubinsky in 1949, Rabbi Zweigenhaft assumed the position of Chief Rabbi of Hanover. Shortly thereafter Rabbi Zweigenhaft was appointed Chief Rabbi of the entire Lower Saxony, a position he held until his departure in 1951. The Orthodox Jewish community made every attempt to persuade Rabbi Zweigenhaft to remain, even offering to fund his weekly journey from Switzerland . However, he turned down the proposal, and the leaderless Orthodox Jewish community began to disperse and soon ceased to exist. Both Rabbis Lubinsky and Zweigenhaft settled in the United States .
World War II.
As an important road junction, railhead and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory "M.N.H. Maschinenfabrik Niedersachsen" (Badenstedt). Forced labourers were sometimes used from the Hannover-Misburg subcamp of the Neuengamme concentration camp. Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the was not rebuilt and its ruins were left as a war memorial.
The Allied ground advance into Germany reached Hanover in April 1945. The US 84th Infantry Division captured the city on 10 April 1945.
Hanover was in the British zone of occupation of Germany, and became part of the new state (Land) of Lower Saxony in 1946.
Today Hanover is a Vice-President City of Mayors for Peace, an international mayoral organisation mobilising cities and citizens worldwide to abolish and eliminate nuclear weapons by the year 2020.
Geography.
Climate.
Hanover experiences an oceanic climate (Köppen climate classification "Cfb").
Main sights.
Panoramic view from the viewing platform at the New Town Hall
One of the most famous sights is the "Royal Gardens of Herrenhausen":
The "Great Garden" is an important European baroque garden. The palace itself, however, was largely destroyed by Allied bombing but is currently under reconstruction. Some points of interest are the "Grotto" (the interior was designed by the French artist Niki de Saint-Phalle), the "Gallery Building", the "Orangerie" and the two pavilions by Remy de la Fosse. The Great Garden consists of several parts. The most popular ones are the "Great Ground" and the "Nouveau Jardin". At the centre of the Nouveau Jardin is Europe's highest garden fountain. The historic "Garden Theatre" "inter alia" hosted the musicals of the German rock musician Heinz Rudolf Kunze. 
The "Berggarten" is an important European botanical garden. Some points of interest are the "Tropical House", the "Cactus House", the "Canary House" and the "Orchid House", which hosts one of the world's biggest collection of orchids, and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic "Library Pavillon". The "Mausoleum" of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the "Paradies" and the "Prairie Garden". There is also the "Sea Life Centre Hanover", which is the first tropical aquarium in Germany. 
The "Georgengarten" is an English landscape garden. The "Leibniz Temple" and the "Georgen Palace" are two points of interest there.
The landmark of Hanover is the New Town Hall ("Neues Rathaus"). Inside the building are four scale models of the city. A worldwide unique diagonal/arch elevator goes up the large dome to an observation deck. 
The "Hanover Zoo" is one of the most spectacular and best zoos in Europe. The zoo received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany.
The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In the year of 2010 the Hanover Zoo had over 1.6 million visitors.
Another point of interest is the "Old Town". At the centre are the huge Marktkirche (Market Church, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the "Old Town Hall". Nearby are the "Leibniz House", the "Nolte House", and the "Beguine Tower". A very nice quarter of the Old Town is the "Kreuz-Church-Quarter" around the "Kreuz Church" with many nice little lanes. Nearby is the old royal sports hall - which is now a theatre, called "Ballhof". On the edge of the Old Town are the "Market Hall", the "Leine Palace", and the ruin of the "Aegidien Church" which is now a monument to the victims of war and violence. Through the "Marstall Gate" you arrive at the bank of the river "Leine", where the world-famous "Nanas" of Niki de Saint-Phalle are located. They are part of the "Mile of Sculptures" which leads from Trammplatz, following the river bank and crossing Königsworther Square up to the entrance of the Georgengarten. Near the Old Town is the district Calenberger Neustadt where the Catholic Church of "St. Clemens", the "Reformed Church" and the Lutheran Neustädter Kirche are located.
Some other popular sights are the "Waterloo Column", the "Laves House", the "Wangenheim Palace", the "Lower Saxony State Archives", the "Hanover Playhouse", the "Kröpcke Clock", the "Anzeiger Tower Block", the "Administration Building of the NORD/LB", the "Cupola Hall" of the Congress Centre, the "Lower Saxony Stock", the "Ministry of Finance", the "Garten Church", the "Luther Church", the "Gehry Tower" (designed by the American architect Frank O. Gehry), the specially designed "Bus Stops", the "Opera House", "the Central Station", the "Maschsee" lake and the city forest "Eilenriede", which is one of the largest of its kind in Europe. With its around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities.
Since 2007 the historic "Leibniz Letters", which can be viewed in the "Gottfried Wilhelm Leibniz Library", are on UNESCO's Memory of the World Register.
Outside the city centre is the "EXPO-Park", the former site of EXPO 2000. Some points of interest are the "Planet M.", the former "German Pavillon", some nations' vacant pavilions, the "Expowale", the "EXPO-Plaza" and the "EXPO-Gardens" (Parc Agricole, EXPO-Park South and the Gardens of change). The fairground can be reached by the "Exponale", one of the largest pedestrian bridges in Europe.
The "Hanover fairground" is the largest Exhibition Centre in the world
It provides 496,000 square metres of covered indoor space, 58,000 square metres of open-air space, 27 halls and pavilions. Many of the Exhibition Centre's halls are architectural highlights. Furthermore it offers the Convention Center with its 35 function rooms, glassed-in areas between halls, grassy park-like recreation zones and its own heliport.
Two important sights on the fairground are the "Hermes Tower" (88.8 metres high) and the "EXPO Roof", the largest wooden roof in the world.
In the district of Anderten is the "European Cheese Centre", the only Cheese Experience Centre in Europe. Another tourist sight in Anderten is the "Hindenburg Lock", which was the biggest lock in Europe at the time of its construction in 1928. The "Animalgarden" in the district of Kirchrode is a huge forest and shows the local animals.
In the district of Groß-Buchholz the 282 metres high "Telemax" is located, which is the tallest building in Lower Saxony and the highest television tower in Northern Germany.
Some other remarkable towers are the "VW-Tower" in the city centre and the old towers of the former mid-age defence belt: "Döhrener Tower", "Lister Tower" and the "Horse Tower".
The 36 most important sights of the city centre are connected with a 4.2 km long red line, which is painted on the pavement. This so-called "Red Thread" marks out a walk that starts at the Tourist Information Office and ends on the Ernst-August-Square in front of the central station. There is also a guided sightseeing-bus tour through the city.
Society and culture.
Museums and galleries.
The "Historic Museum" describes the history of Hanover, from the medieval settlement "Honovere" to the world-famous Exhibition City of today. The museum focuses on the period from 1714 to 1834 when Hanover had a strong relationship with the British royal house.
With more than 4,000 members, the "Kestnergesellschaft" is the largest art society in Germany. The museum hosts exhibitions from classical modernist art to contemporary art. One big focus is put on film, video, contemporary music and architecture, room installments and big presentations of contemporary paintings, sculptures and video art.
The "Kestner-Museum" is located in the "House of 5.000 windows". The museum is named after August Kestner and exhibits 6,000 years of applied art in four areas: Ancient cultures, ancient Egypt, applied art and a valuable collection of historic coins.
The "KUBUS" is a forum for contemporary art. It features mostly exhibitions and projects of famous and important artists from Hanover.
The "Kunstverein Hannover" (Art Society Hanover) shows contemporary art and was established in 1832 as one of the first art societies in Germany. It is located in the "Künstlerhaus" (House of artists). There are around 7 international monografic and thematic Exhibitions in one year.
The "Lower Saxony State Museum" is the largest museum in Hanover. The "State Gallery" shows the European Art from the 11th to the 20th century, the "Nature Department" shows the zoology, geology, botanic, geology and a "Vivarium" with fishes, insects, reptiles and amphibians. The "Primeval Department" shows the primeval history of Lower Saxony and the "Folklore Department" shows the cultures from all over the world.
The "Sprengel Museum" shows the art of the 20th century. It is one of the most notable art museums in Germany. The focus is put on the classical modernist art with the collection of "Kurt Schwitters", works of German expressionism, and French cubism, the cabinet of abstracts, the graphics and the department of photography and media. Furthermore the museum shows the famous works of the French artist Niki de Saint-Phalle.
The "Theatre Museum" shows an exhibition of the history of the theatre in Hanover from the 17th century up to now: opera, concert, drama and ballet. The museum also hosts several touring exhibitions during the year.
The "Wilhelm Busch Museum" is the "German Museum of Caricature and Critical Graphic Arts". The collection of the works of Wilhelm Busch and the extensive collection of cartoons and critical graphics is this museum unique in Germany. Furthermore the museum hosts several exhibitions of national and international artists during the year.
A cabinet of coins is the "Münzkabinett der TUI-AG". The "Polizeigeschichtliche Sammlung Niedersachsen" is the largest police museum in Germany. Textiles from all over the world can be visited in the "Museum for textile art". The "EXPOseeum" is the museum of the world-exhibition "EXPO 2000 Hannover". Carpets and objects from the orient can be visited in the "Oriental Carpet Museum". The "Blind Man Museum" is a rarity in Germany, another one is only in Berlin. The "Museum of veterinary medicine" is unique in Germany. The "Museum for Energy History" describes the 150 years old history of the application of energy. The "Home Museum Ahlem" shows the history of the district of Ahlem. The "Mahn- und Gedenkstätte Ahlem" describes the history of the Jewish people in Hanover and the "Stiftung Ahlers Pro Arte / Kestner Pro Arte" shows modern art. Modern art is also the main topic of the "Kunsthalle Faust", the "Nord/LB Art Gellery" and of the "Foro Artistico / Eisfabrik".
Some leading art events in Hanover are the "Long Night of the museums" and the "Zinnober Kunstvolkslauf" which features all the galleries in Hanover.
People who are interested in astronomy should visit the "Observatory Geschwister Herrschel" on the Lindener Mountain or the small planetarium inside of the Bismarck School.
Theatre, cabaret and musical.
Around 40 theatres are located in Hanover. The "Opera House", the "Schauspielhaus" (Play House), the "Ballhofeins", the "Ballhofzwei" and the "Cumbarlandsche Galerie" belong to the "Lower Saxony State Theatre". The "Theater am Aegi" is Hanover's big theatre for musicals, shows and guest performances. The "Neues Theater" (New Theatre) is the Boulevard Theatre of Hanover. The "Theater für Niedersachsen" is another big theatre in Hanover, which also has an own Musical-Company. Some of the most important Musical-Productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the "Garden-Theatre" in the Great Garden.
Some important theatre-events are the "Tanztheater International", the "Long Night of the Theatres", the "Festival Theaterformen" and the "International Competition for Choreographs".
Hanover's leading cabaret-stage is the "GOP Variety theatre" which is located in the "Georgs Palace". Some other famous cabaret-stages are the "Variety Marlene", the "Uhu-Theatre". the theatre "Die Hinterbühne", the "Rampenlich Variety" and the revue-stage "TAK". The most important Cabaret-Event is the "Kleines Fest im Großen Garten" (Little Festival in the Great Garden) which is the most successful Cabaret Festival in Germany. It features artists from around the world. Some other important events are the "Calenberger Cabaret Weeks", the "Hanover Cabaret Festival" and the "Wintervariety".
Music.
The rock bands Scorpions and Fury in the Slaughterhouse are originally from Hanover. Also, acclaimed DJ Mousse T has his main recording studio in the area. Rick J. Jordan, member of band Scooter was born here in 1968. Eurovision Song Contest winner of 2010, Lena (Lena Meyer-Landrut), is also from Hanover.
There are/were two big international competitions for classical music in Hanover:
Sport.
Hannover 96 (nickname "Die Roten" or 'The Reds') is the top local football team that plays in the Bundesliga top division. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team "Hannover 96 II" plays in the fourth league. Their home games were played in the traditional "Eilenriedestadium" till they moved to the HDI Arena due to DFL directives. "Arminia Hannover" is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the "Niedersachsen-West Liga" (Lower Saxony League West). Home matches are played in the "Rudolf-Kalweit-Stadium".
The Hannover Indians are the local ice hockey team. They play in the third tier. Their home games are played at the traditional Eisstadion am Pferdeturm. The Hannover Scorpions played in Hanover in Germany's top league until 2013 when they sold their license and moved to Langenhagen.
Hanover is one of the Rugby union capitals in Germany. The first German Rugby team was founded in Hanover in 1878. Hanover-based teams dominated the German Rugby scene for a long time. "DRC Hannover" plays in the first division, and "SV Odin von 1905" as well as "SG 78/08 Hannover" play in the second division.
The first German Fencing Club was founded in Hanover in 1862. Today there are three more Fencing Clubs in Hanover.
Hanover is a centre for Water sports. Thanks to the lake "Maschsee", the rivers "Ihme" and "Leine" and to the channel "Mittellandkanal" Hanover hosts sailing schools, yacht schools, waterski clubs, rowing clubs, canoe clubs and paddle clubs. The water polo team "WASPO W98" plays in the first division.
The UBC Hannover Tigers play in the second German Basketball Association and the "Hannover Regents" play in the third Bundesliga (baseball) division.
The "Hannover Marathon" is the biggest running event in Hanover with more than 11.000 participants and usually around 200.000 spectators. Some other important running events are the "Gilde Stadtstaffel" (relay), the "Sport-Check Nachtlauf" (night-running), the "Herrenhäuser Team-Challenge", the "Hannoversche Firmenlauf" (company running) and the "Silvesterlauf" (sylvester running).
Hanover hosts also an important international cycle race: The "Nacht von Hannover" (night of Hanover). The race takes place around the Market Hall.
The lake "Maschsee" hosts the "International Dragon Boat Races" and the "Canoe　Polo-Tournament". Many regattas take place during the year. "Head of the river Leine" on the river "Leine" is one of the biggest rowing regattas in Hanover.
One of Germanys most successful dragon boat teams, the All Sports Team Hannover, which has won since its foundation in year 2000 more than 100 medals on national and international competitions, is doing practising on the Maschsee in the heart of Hannover. The All Sports Team has received the award "Team of the Year 2013" in Lower Saxony 
Some other important sport events are the "Lower Saxony Beach Volleyball Tournament", the international horse show "German Classics" and the international ice hockey tournament "Nations Cup".
Regular events.
Hanover is one of the leading Exhibition Cities in the world. Each year Hanover hosts more than 60 international and national exhibitions. The most popular ones are the "CeBIT", the "Hanover Fair", the "Domotex", the "Ligna", the "IAA Nutzfahrzeuge" and the "Agritechnica". Hanover also hosts a huge number of congresses and symposiums like "International Symposium on Society and Resource Management"
But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The "Schützenfest Hannover" is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the 12 km long "Parade of the Marksmen" with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world (60 m high). The origins of this fun fair is located in the year 1529.
Hanover also hosts one of the two largest Spring Festivals in Europe with around 180 rides and inns, 2 large beer tents and around 1.5 million visitors each year. The Oktoberfest Hannover is the second largest Oktoberfest in the world with around 160 rides and inns, two large beer tents and around 1 million visitors each year.
The "Maschsee Festival" takes place around the Maschsee Lake. Each year around 2 million visitors come to enjoy live music, comedy, cabaret and much more. It is the largest Volksfest of its kind in Northern Germany.
The Great Garden hosts every year the "International Fireworks Competition", and the "International Festival Weeks Herrenhausen" with lots of music and cabaret.
The "Carnival Procession" is around 3 km long and consists of 3.000 participants, around 30 festival vehicles and around 20 bands and takes place every year.
Some more festivals are for example the Festival "Feuer und Flamme" (Fire and Flames), the "Gartenfestival" (Garden Festival), the "Herbstfestival" (Autumn Festival), the "Harley Days", the "Steintor Festival" (Steintor is a party area in the city centre) and the "Lister-Meile-Festival" (Lister Meile is a large pedestrian area).
Hanover also hosts Food Festivals, for example the "Wine Festival" and the "Gourmet Festival".
Furthermore Hanover hosts some special markets. The "Old Town Flea Market" is the oldest flea market in Germany and the "Market for Art and Trade" has a high reputation. Some other big markets are of course the "Christmas Markets of the City of Hanover" in the Old Town and city centre and the Lister Meile.
Transport.
Rail.
The city's central station, Hannover Hauptbahnhof, is a hub of the German high-speed ICE network. It is the starting point of the Hanover-Würzburg high-speed rail line and also the central hub for the Hanover S-Bahn. It offers many international and national connections.
Air.
Hanover and its area is served by Hanover/Langenhagen International Airport (IATA code: HAJ; ICAO code: EDDV)
Road.
Hanover is also an important hub of Germany's Autobahn network; the junction of two major autobahns, the A2 and A7 is at "Kreuz Hannover-Ost", at the northeastern edge of the city.
Local autobahns are A 352 (a short cut between A7 (north) and A2 (west), also known as the "airport autobahn" because it passes "Hanover Airport") and the A 37.
The Schnellweg "(en: expressway)" system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at "Seelhorster Kreuz", then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing "Westschnellweg", then becomes B65 again at "Seelhorster Kreuz").
Bus and light rail.
Hanover has an extensive Stadtbahn and bus system, operated by üstra. The city is famous for its designer buses and tramways, the TW 6000 and TW 2000 trams being the most well-known examples.
Bicycle.
Cycle paths are very common in the city centre. You are allowed to take your bike on a tram or bus.
Economy.
The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks.
TUI AG has its HQ in Hanover.
Hanover is home to many insurance companies, many of which operate only in Germany. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre.
Business development.
Hannoverimpuls is a joint business development company from the city and region of Hannover. The company was founded in 2003 and supports the start-up, growth and relocation of businesses in the Hannover Region. The focus is on seven sectors, which stand for sustainable economic growth: Automotive, Energy Solutions, Information and Communications Technology, Life Sciences, Optical Technologies, Creative Industries and Production Engineering.
A range of programmes supports companies from the key industries in their expansion plans in Hannover or abroad. Three regional centres specifically promote international economic relations with Russia, India and Turkey.
Education.
The Leibniz University Hannover is the largest funded institution in Hanover for providing higher education to the students from around the world. Below are the names of the universities and some of the important schools including newly opened Hannover Medical Research School in 2003 for attracting the students from biology background from around the world.
There are several universities in Hanover:
There is one University of Applied Science and Arts in Hanover:
The "Schulbiologiezentrum Hannover" maintains practical biology schools in four locations (Botanischer Schulgarten Burg, Freiluftschule Burg, Zooschule Hannover, and Botanischer Schulgarten Linden). The University of Veterinary Medicine Hanover also maintains its own botanical garden specializing in medicinal and poisonous plants, the Heil- und Giftpflanzengarten der Tierärztlichen Hochschule Hannover.
People and residents of Hanover.
The following is a selection of famous Hanover-natives, personalities connected with the city and honorary citizens:
International relations.
Hanover is twinned with:

</doc>
<doc id="14199" url="http://en.wikipedia.org/wiki?curid=14199" title="Handheld game console">
Handheld game console

A handheld game console is a lightweight, portable video game console with a built-in screen, game controls, and speakers. Handheld game consoles are smaller than home video game consoles and contain the console, screen, speakers, and controls in one unit, allowing people to carry them and play them at any time or place.
In 1976, Mattel introduced the first handheld electronic game with the release of "Auto Race". Later, several companies—including Coleco and Milton Bradley—made their own single-game, lightweight table-top or handheld electronic game devices. The oldest true handheld game console with interchangeable cartridges is the Milton Bradley Microvision in 1979.
Nintendo is credited with popularizing the handheld console concept with the release of the Game Boy in 1989 and as of 2014 continues to dominate the handheld console market with their Nintendo 2DS and 3DS systems.
History.
Origins.
The origins of handheld game consoles are found in handheld and tabletop electronic game devices of the 1970s and early 1980s. These electronic devices are capable of playing only a single game, they fit in the palm of the hand or on a tabletop, and they may make use of a variety of video displays such as LED, VFD, or LCD. In 1978, handheld electronic games were described by "Popular Electronics" magazine as "nonvideo electronic games" and "non-TV games" as distinct from devices that required use of a television screen. Handheld electronic games, in turn, find their origins in the synthesis of previous handheld and tabletop electro-mechanical devices such as Waco's "Electronic Tic-Tac-Toe" (1972) Cragstan's "Periscope-Firing Range" (1960s), and the emerging optoelectronic-display-driven calculator market of the early 1970s. This synthesis happened in 1976, when "Mattel began work on a line of calculator-sized sports games that became the world's first handheld electronic games. The project began when Michael Katz, Mattel's new product category marketing director, told the engineers in the electronics group to design a game the size of a calculator, using LED (light-emitting diode) technology."
The result was the 1976 release of "Auto Race". Followed by "Football" later the same year, the two games were so successful that according to Katz, "these simple electronic handheld games turned into a '$400 million category.'" Mattel would later win the honor of being recognized by the industry for innovation in handheld game device displays. Soon, other manufacturers including Coleco, Parker Brothers, Milton Bradley, Entex, and Bandai began following up with their own tabletop and handheld electronic games.
In 1979 the LCD-based Microvision, designed by Smith Engineering and distributed by Milton-Bradley, became the first handheld game console and the first to use interchangeable game cartridges. The Microvision game "Cosmic Hunter" (1981) also introduced the concept of a directional pad on handheld gaming devices, and is operated by using the thumb to manipulate the on-screen character in any of four directions.
In 1979, Gunpei Yokoi, traveling on a bullet train, saw a bored businessman playing with an LCD calculator by pressing the buttons. Yokoi then thought of an idea for a watch that doubled as a miniature game machine for killing time. Starting in 1980, Nintendo began to release a series of electronic games designed by Yokoi called the Game & Watch games. Taking advantage of the technology used in the credit-card-sized calculators that had appeared on the market, Yokoi designed the series of LCD-based games to include a digital time display in the corner of the screen. For later, more complicated Game & Watch games, Yokoi invented a cross shaped directional pad or "D-pad" for control of on-screen characters. Yokoi also included his directional pad on the NES controllers, and the cross-shaped thumb controller soon became standard on game console controllers and ubiquitous across the video game industry as a replacement for the joystick. When Yokoi began designing Nintendo's first handheld game console, he came up with a device that married the elements of his Game & Watch devices and the Famicom console, including both items' D-pad controller. The result was the Nintendo Game Boy.
In 1982, the Bandai LCD Solarpower was the first solar-powered gaming device. Some of its games, such as the horror-themed game "Terror House", featured two LCD panels, one stacked on the other, for an early 3D effect. In 1983, Takara Tomy's Tomytronic 3D simulated 3D by having two LED panels that were lit by external light through a window on top of the device, making it the first dedicated home video 3D hardware.
Late 1980s through early 1990s.
The late 1980s and early 1990s saw the beginnings of the handheld game console industry as we know it, after the demise of the Microvision. As backlit LCD game consoles with color graphics consume a lot of power, they were not battery-friendly like the non-backlit original Game Boy whose monochrome graphics allowed longer battery life. By this point, rechargeable battery technology had not yet matured and so the more advanced game consoles of the time such as the Sega Game Gear and Atari Lynx did not have nearly as much success as the Game Boy.
Even though third-party rechargeable batteries were available for the battery-hungry alternatives to the Game Boy, these batteries employed a nickel-cadmium process and had to be completely discharged before being recharged to ensure maximum efficiency; lead-acid batteries could be used with automobile circuit limiters (cigarette lighter plug devices); but the batteries had mediocre portability. The later NiMH batteries, which do not share this requirement for maximum efficiency, were not released until the late 1990s, years after the Game Gear, Atari Lynx, and original Game Boy had been discontinued. During the time when technologically superior handhelds had strict technical limitations, batteries had a very low mAh rating since batteries with heavy power density were not yet available.
Modern game systems such as the Nintendo DS and PlayStation Portable have rechargeable Lithium-Ion batteries with proprietary shapes. Other seventh-generation consoles such as the GP2X use standard alkaline batteries. Because the mAh rating of alkaline batteries has increased since the 1990s, the power needed for handhelds like the GP2X may be supplied by relatively few batteries.
Game Boy.
Nintendo released the Game Boy on April 21, 1989 (or in September 1990 for UK). The design team headed by Gunpei Yokoi had also been responsible for the Game & Watch system, as well as the Nintendo Entertainment System games "Metroid" and "Kid Icarus". The Game Boy came under scrutiny by some industry critics, saying that the monochrome screen was too small, and the processing power was inadequate. The design team had felt that low initial cost and battery economy were more important concerns, and when compared to the Microvision, the Game Boy was a huge leap forward.
Yokoi recognized that the Game Boy needed a killer app—at least one game that would define the console, and persuade customers to buy it. In June 1988, Minoru Arakawa, then-CEO of Nintendo of America saw a demonstration of the game "Tetris" at a trade show. Nintendo purchased the rights for the game, and packaged it with the Game Boy system. It was almost an immediate hit. By the end of the year more than a million units were sold in the US, and 25 million were sold by 1992. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell 118.69 million units worldwide.
Atari Lynx.
In 1987, Epyx created the Handy Game; a device that would turn into the Atari Lynx in 1989. It was the first color handheld console ever made, as well as the first with a backlit screen. It also featured networking support with up to 17 other players, and advanced hardware that allowed the zooming and scaling of sprites. The Lynx could also be turned upside down to accommodate left-handed players. However, all these features came at a very high price point, which drove consumers to seek cheaper alternatives. The Lynx was also very unwieldy, consumed batteries very quickly, and lacked the third-party support enjoyed by its competitors. Due to its high price, short battery life, production shortages, a dearth of compelling games, and Nintendo's aggressive marketing campaign, and despite a redesign in 1991, the Lynx became a commercial failure. Despite this, companies like Telegames helped to keep the system alive long past its commercial relevance, and when new owner Hasbro released the rights to develop for the public domain, independent developers like Songbird have managed to release new commercial games for the system every year until 2004's "Winter Games".
TurboExpress.
The TurboExpress was a portable version of the TurboGrafx, released in 1990 for $249.99 (the price was briefly raised to $299.99, soon dropped back to $249.99, and by 1992 it was $199.99). Its Japanese equivalent was the PC Engine GT.
It was the most advanced handheld of its time and could play all the TurboGrafx-16's games (which were on a small, credit-card sized media called HuCards). It had a 66 mm (2.6 in.) screen, the same as the original Game Boy, but in a much higher resolution. And could display 64 sprites at once, 16 per scanline, in 512 colors. Although the hardware could only handle 481 simultaneous colors. It had 64 kilobytes of RAM. The Turbo ran its two 6820 CPUs at 3.58 MHz in parallel.
The optional "TurboVision" TV tuner included RCA audio/video input, allowing users to use TurboExpress as a video monitor. The "TurboLink" allowed two-player play. "Falcon", a flight simulator, included a "head-to-head" dogfight mode that could only be accessed via TurboLink. However, very few TG-16 games offered co-op play modes especially designed with the TurboExpress in mind.
Bitcorp Gamate.
The Bitcorp Gamate was the one of the first handheld game systems created in response to the Nintendo Game Boy. It was released in Asia in 1990 and distributed worldwide by 1991.
Like the Sega Game Gear, it was horizontal in orientation and like the Game Boy, required 4 AA batteries. Unlike many later Game Boy clones, its internal components were professionally assembled (no "glop-top" chips). Unfortunately the system's fatal flaw was its screen. Even by the standards of the day, its screen was rather difficult to use, suffering from similar motion blur problems that were common complaints with the first generation Game Boys. Likely because of this fact sales were quite poor, and Bitcorp closed by 1992. However, new games continued to be published for the Asian market, possibly as late as 1994. The total number of games released for the system remains unknown.
Interestingly, Gamate games were designed for stereo sound, but the console was only equipped with a mono speaker. To appreciate the full sound palette, a user must plug into the head phone jack. Doing so reveals very sophisticated music.
Sega Game Gear.
The Sega Game Gear was the third color handheld console, after the Lynx and the TurboExpress. Released in Japan in 1990 and in North America and Europe in 1991, it was based on the Sega Master System, which gave Sega the ability to quickly create Game Gear games from its large library of games for the Master System. While never reaching the level of success enjoyed by Nintendo, the Sega Game Gear proved to be a fairly durable competitor, lasting longer than any other Game Boy rivals.
While the Game Gear is most frequently seen in black or navy blue, it was also released in a variety of additional colors: red, light blue, yellow, clear, and violet. All of these variations were released in small quantities and frequently only in the Asian market.
Following Sega's success with the Game Gear, they began development on a successor during the early 1990s, which was intended to feature a touchscreen interface, many years before the Nintendo DS. However, such a technology was very expensive at the time, and the handheld itself was estimated to have cost around $289 were it to be released. Sega eventually chose to shelve the idea and instead release the Sega Nomad, a handheld version of the Mega Drive (Genesis), as the successor.
Watara Supervision.
The Watara Supervision was released in 1992 in an attempt to compete with the Nintendo Game Boy. The first model was designed very much like a Game Boy, but it was grey in color and had a slightly larger screen. The second model was made with a hinge across the center and could be bent slightly to provide greater comfort for the user. While the system did enjoy a modest degree of success, it never impacted the sales of Nintendo or Sega. The Supervision was redesigned a final time as "The Magnum". Released in limited quantities it was roughly equivalent to the Game Boy Pocket. It was available in three colors: yellow, green and grey. Watara designed many of the games themselves, but did receive some third party support, most notably from Sachen.
A TV adapter was available in both PAL and NTSC formats that could transfer the Supervision's black-and-white palette to 4 colors, similar in some regards to the Super Game Boy from Nintendo.
Hartung Game Master.
The Hartung Game Master was an obscure handheld released at an unknown point in the early 1990s. Its graphics were much lower than most of its contemporaries, similar in complexity to the Atari 2600. It was available in black, white, and purple, and was frequently rebranded by its distributors, such as Delplay, Videojet and Virella.
The exact number of games released is not known, but is likely around 20. The system most frequently turns up in Europe and Australia.
Late 1990s.
By this time, the lack of significant development in Nintendo's product line began allowing more advanced systems such as the Neo Geo Pocket Color and the WonderSwan Color to achieve moderate success.
Game.com.
The Game.com (pronounced in TV commercials as "game com", not "game dot com", and not capitalized in marketing material) was a handheld game console released by Tiger Electronics in September 1997. It featured many new ideas for handheld consoles and was aimed at an older target audience, sporting PDA-style features and functions such as a touch screen and stylus. However, Tiger hoped it would also challenge Nintendo's Game Boy and gain a following among younger gamers too. Unlike other handheld game consoles, the first game.com consoles included two slots for game cartridges, which would not happen again until the Tapwave Zodiac, the DS and DS Lite, and could be connected to a 14.4 kbit/s modem. Later models had only a single cartridge slot.
Game Boy Color.
The Game Boy Color (also referred to as GBC or CGB) is Nintendo's successor to the Game Boy and was released on October 21, 1998, in Japan and in November of the same year in the United States. It features a color screen, and is slightly bigger than the Game Boy Pocket. The processor is twice as fast as a Game Boy's and has twice as much memory. It also had an infrared communications port for wireless linking which did not appear in later versions of the Game Boy, such as the Game Boy Advance.
The Game Boy Color was a response to pressure from game developers for a new system, as they felt that the Game Boy, even in its latest incarnation, the Game Boy Pocket, was insufficient. The resulting product was backward compatible, a first for a handheld console system, and leveraged the large library of games and great installed base of the predecessor system. This became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell 118.69 million units worldwide.
The console was capable of displaying up to 56 different colors simultaneously on screen from its palette of 32,768, and could add basic four-color shading to games that had been developed for the original Game Boy. It could also give the sprites and backgrounds separate colors, for a total of more than four colors.
Neo Geo Pocket Color.
The Neo Geo Pocket Color (or NGPC) was released in 1999 in Japan, and later that year in the United States and Europe. It was a 16-bit color handheld game console designed by SNK, the maker of the Neo Geo home console and arcade machine. It came after SNK's original Neo Geo Pocket monochrome handheld, which debuted in 1998 in Japan.
In 2000 following SNK's purchase by Japanese Pachinko manufacturer Aruze, the Neo Geo Pocket Color was dropped from both the US and European markets, purportedly due to commercial failure.
The system seemed well on its way to being a success in the U.S. It was more successful than any Game Boy competitor since Sega's Game Gear, but was hurt by several factors, such as SNK's infamous lack of communication with third-party developers, and anticipation of the Game Boy Advance. The decision to ship U.S. games in cardboard boxes in a cost-cutting move rather than hard plastic cases that Japanese and European releases were shipped in may have also hurt US sales.
Wonderswan Color.
The WonderSwan Color is a handheld game console designed by Bandai. It was released on December 9, 2000, in Japan, and was a moderate success.
The original WonderSwan had only a black and white screen. Although the WonderSwan Color was slightly larger and heavier (7 mm and 2 g) compared to the original WonderSwan, the color version featured 512 kB of RAM and a larger color LCD screen. In addition, the WonderSwan Color is compatible with the original WonderSwan library of games.
Prior to WonderSwan's release, Nintendo had virtually a monopoly in the Japanese video game handheld market. After the release of the WonderSwan Color, Bandai took approximately 8% of the market share in Japan partly due to its low price of 6800 yen (approximately US$65).
Another reason for the WonderSwan's success in Japan was the fact that Bandai managed to get a deal with Square to port over the original Famicom "Final Fantasy" games with improved graphics and controls. However, with the popularity of the Game Boy Advance and the reconciliation between Square and Nintendo, the WonderSwan Color and its successor, the SwanCrystal quickly lost its competitive advantage.
2000s.
The 2000s saw a major leap in innovation, particularly in the second half with the release of the DS and PSP.
Game Boy Advance.
In 2001, Nintendo released the Game Boy Advance (GBA or AGB), which added two shoulder buttons, a larger screen, and more computing power than the Game Boy Color.
The design was revised two years later when the Game Boy Advance SP (GBA SP), a more compact version, was released. The SP featured a "clamshell" design (folding open and closed, like a laptop computer), as well as a frontlit color display and rechargeable battery. Despite the smaller form factor, the screen remained the same size as that of the original. In 2005, the Game Boy Micro was released. This revision sacrificed screen size and backwards compatibility with previous Game Boys for a dramatic reduction in total size and a brighter backlit screen. A new SP model with a backlit screen was released in some regions around the same time.
Along with the Nintendo GameCube, the GBA also introduced the concept of "connectivity": using a handheld system as a console controller. A handful of games use this feature, most notably "Animal Crossing", "Pac-Man Vs.", "Final Fantasy Crystal Chronicles", ', ', "Metroid Prime", and "".
As of December 31, 2007, the GBA, GBA SP, and the Game Boy Micro combined have sold 80.72 million units worldwide.
Game Park 32.
The original GP32 was released in 2001 by the South Korean company Game Park a few months after the launch of the Game Boy Advance. It featured a 32-bit CPU, 133 MHz processor, MP3 and Divx player, and e-book reader. SmartMedia cards were used for storage, and could hold up to 128mb of anything downloaded through a USB cable from a PC. The GP32 was redesigned in 2003. A front-lit screen was added and the new version was called GP32 FLU (Front Light Unit). In summer 2004, another redesign, the GP32 BLU, was made, and added a backlit screen. This version of the handheld was planned for release outside South Korea; in Europe, and it was released for example in Spain (VirginPlay was the distributor). While not a commercial success on a level with mainstream handhelds (only 30,000 units were sold), it ended up being used mainly as a platform for user-made applications and emulators of other systems, being popular with developers and more technically adept users.
N-Gage.
Nokia released the N-Gage in 2003. It was designed as a combination MP3 player, cellphone, PDA, radio, and gaming device. The system received much criticism alleging defects in its physical design and layout, including its vertically oriented screen and requirement of removing the battery to change game cartridges. The most well known of these was "sidetalking", or the act of placing the phone speaker and receiver on an edge of the device instead of one of the flat sides, causing the user to appear as if they are speaking into a taco.
The N-Gage QD was later released to address the design flaws of the original. However, certain features available in the original N-Gage, including MP3 playback, FM radio reception, and USB connectivity were removed.
Second generation of N-Gage launched on April 3, 2008 in the form of a service for selected Nokia Smartphones.
Cybiko.
The Cybiko was a Russian hand-held computer introduced in May 2000 by David Yang's company and designed for teens, featuring its own two-way radio text messaging system. It had over 430 "official" freeware games and applications. Because of the text messaging system, it features a QWERTY keyboard that was used with a stylus. An MP3 player add-on was made for the unit as well as a SmartMedia card reader. The company stopped manufacturing the units after two product versions and only a few years on the market. Cybikos can communicate with each other up to a maximum range of 300 metres (0.19 miles). Several Cybikos can chat with each other in a wireless chatroom.
Cybiko Classic:
There were two models of the Classic Cybiko. Visually, the only difference was that the original version had a power switch on the side, whilst the updated version used the "escape" key for power management. Internally, the differences between the two models were in the internal memory, and the location of the firmware.
Cybiko Xtreme:
The Cybiko Xtreme was the second-generation Cybiko handheld. It featured various improvements over the original Cybiko, such as a faster processor, more RAM, more ROM, a new operating system, a new keyboard layout and case design, greater wireless range, a microphone, improved audio output, and smaller size.
Tapwave Zodiac.
In 2004, Tapwave released the Zodiac. It was designed to be a PDA-handheld game console hybrid. It supported photos, movies, music, Internet, and documents. The Zodiac used a special version Palm OS 5, 5.2T, that supported the special gaming buttons and graphics chip. Two versions were available, Zodiac 1 and 2, differing in memory and looks. The Zodiac line ended in July 2005 when Tapwave declared bankruptcy.
Nintendo DS.
The Nintendo DS was released in November 2004. Among its new features were the incorporation of two screens, a touchscreen, wireless connectivity, and a microphone port. As with the Game Boy Advance SP, the DS features a clamshell design, with the two screens aligned vertically on either side of the hinge.
The DS's lower screen is touch sensitive, designed to be pressed with a stylus, a user's finger or a special "thumb pad" (a small plastic pad attached to the console's wrist strap, which can be affixed to the thumb to simulate an analog stick). More traditional controls include four face buttons, two shoulder buttons, a D-pad, and "Start" and "Select" buttons. The console also features online capabilities via the Nintendo Wi-Fi Connection and ad-hoc wireless networking for multiplayer games with up to sixteen players. It is backwards-compatible with all Game Boy Advance games, but not games designed for the Game Boy or Game Boy Color.
In January 2006, Nintendo revealed an updated version of the DS: the Nintendo DS Lite (released on March 2, 2006, in Japan) with an updated, smaller form factor (42% smaller and 21% lighter than the original Nintendo DS), a cleaner design, longer battery life, and brighter, higher-quality displays, with adjustable brightness. It is also able to connect wirelessly with Nintendo's Wii console.
In October 2008, Nintendo announced the Nintendo DSi, with larger, 3.25-inch screens and two integrated cameras. It has an SD card storage slot in place of the Game Boy Advance slot, plus internal flash memory for storing downloaded games. It was released on November 1, 2008, in Japan, and was released in North America April 5, 2009, and April 3, 2009, in Europe.
As of December 31, 2009, the Nintendo DS, Nintendo DS Lite and Nintendo DSi combined have sold 125.13 million units worldwide. In 2010 Nintendo released a larger version of the DSi, called the DSi XL.
Game King.
The GameKing was a handheld game console released by the Chinese company TimeTop in 2004. The first model while original in design owes a large debt to Nintendo's Game Boy Advance. The second model, the GameKing 2, is believed to be inspired by Sony's PSP. This model also was upgraded with a backlit screen, with a distracting background transparency (which can be removed by opening up the console). A color model, the GameKing 3 apparently exists, but was only made for a brief time and was difficult to purchase outside of Asia. Whether intentionally or not, the GameKing has the most primitive graphics of any handheld released since the Game Boy of 1989. 
As many of the games have an "old school" simplicity, the device has developed a small cult following. The Gameking's speaker is quite loud and the cartridges' sophisticated looping soundtracks (sampled from other sources) are seemingly at odds with its primitive graphics.
TimeTop made at least one additional device sometimes labeled as "GameKing", but while it seems to possess more advanced graphics, is essentially an emulator that plays a handful of multi-carts (like the GB Station Light II). Outside of Asia (especially China) however the Gameking remains relatively unheard of due to the enduring popularity of Japanese handhelds such as those manufactured by Nintendo and Sony.
PlayStation Portable.
The PlayStation Portable (officially abbreviated PSP) is a handheld game console manufactured and marketed by Sony Computer Entertainment. Development of the console was first announced during E3 2003, and it was unveiled on May 11, 2004, at a Sony press conference before E3 2004. The system was released in Japan on December 12, 2004, in North America on March 24, 2005, and in the PAL region on September 1, 2005.
The PlayStation Portable is the first handheld video game console to use an optical disc format, Universal Media Disc (UMD), for distribution of its games. UMD Video discs with movies and television shows were also released. The PSP utilized the Sony/SanDisk Memory Stick Pro Duo format as its primary storage medium. Other distinguishing features of the console include its large viewing screen, multi-media capabilities, and connectivity with the PlayStation 3, other PSPs, and the Internet.
Gizmondo.
Tiger's Gizmondo came out in the UK during March 2005 and it was released in the U.S. during October 2005. It is designed to play music, movies, and games, have a camera for taking and storing photos, and have GPS functions. It also has Internet capabilities. It has a phone for sending text and multimedia messages. Email was promised at launch, but was never released before Gizmondo, and ultimately Tiger Telematics', downfall in early 2006. Users obtained a second service pack, unreleased, hoping to find such functionality. However, Service Pack B did not activate the e-mail functionality.
Game Park Holdings GP2X.
The GP2X is an open-source, Linux-based handheld video game console and media player created by GamePark Holdings of South Korea, designed for homebrew developers as well as commercial developers. It is commonly used to run emulators for game consoles such as Neo Geo, Sega Genesis, Sega Master System, Sega Game Gear, Amstrad CPC, Commodore 64, Nintendo Entertainment System, PC-Engine/TurboGrafx-16, MAME and others.
A new version called the "F200" was released October 30, 2007, and features a touchscreen, among other changes. Followed by GP2X Wiz (2009) and GP2X Caanoo (2010).
Dingoo.
The Dingoo A-320 is a micro-sized gaming handheld that resembles the Game Boy Micro and is open to game development. It also supports music, radio, emulators (8 bit and 16 bit) and video playing capabilities with its own interface much like the PSP. There is also an onboard radio and recording program. It is currently available in two colors — white and black. Other similar products from the same manufacturer are the Dingoo A-330 (also known as Geimi), Dingoo A-360, Dingoo A-380 (available in pink, white and black) and the recently released Dingoo A-320E.
PSP Go.
The PSP Go is a version of the PlayStation Portable handheld game console manufactured by Sony. It was released on October 1, 2009, in American and European territories, and on November 1 in Japan. It was revealed prior to E3 2009 through Sony's Qore VOD service. Although its design is significantly different from other PSPs, it is not intended to replace the PSP 3000, which Sony continued to manufacture, sell, and support. On April 20, 2011, the manufacturer announced that the PSP Go would be discontinued so that they may concentrate on the PlayStation Vita. Sony later said that only the European and Japanese versions were being cut, and that the console would still be available in the US.
Unlike previous PSP models, the PSP Go does not feature a UMD drive, but instead has 16 GB of internal flash memory to store games, video, pictures, and other media. This can be extended by up to 32 GB with the use of a Memory Stick Micro (M2) flash card. Also unlike previous PSP models, the PSP Go's rechargeable battery is not removable or replaceable by the user. The unit is 43% lighter and 56% smaller than the original PSP-1000, and 16% lighter and 35% smaller than the PSP-3000. It has a 3.8" 480 × 272 LCD (compared to the larger 4.3" 480 × 272 pixel LCD on previous PSP models). The screen slides up to reveal the main controls. The overall shape and sliding mechanism are similar to that of Sony's mylo COM-2 internet device.
Pandora.
The Pandora is a handheld game console/UMPC/PDA hybrid designed to take advantage of existing open source software and to be a target for home-brew development. It runs a full distribution of Linux, and in functionality is like a small PC with gaming controls. It is developed by OpenPandora, which is made up of former distributors and community members of the GP32 and GP2X handhelds.
OpenPandora began taking pre-orders for one batch of 4000 devices in November 2008 and after manufacturing delays, began shipping to customers on May 21, 2010.
FC-16 Go.
The FC-16 Go is a portable Super Nintendo manufactured by Yobo Gameware in 2009. It features a 3.5-inch display, two wireless controllers, and CRT cables that allow cartridges to be played on a television screen. Unlike other Super Nintendo clone consoles, it has region tabs that only allow NTSC North American cartridges to be played. Later revisions feature stereo sound output, larger shoulder buttons, and a slightly re-arranged button, power, and A/V output layout.
2010s.
Nintendo 3DS.
The Nintendo 3DS is the successor to Nintendo's DS handheld. The autostereoscopic device is able to project stereoscopic three-dimensional effects without requirement of active shutter or passive polarized glasses, which are required by most current 3D televisions to display the 3D effect. The 3DS was released in Japan on February 26, 2011; in Europe on March 25, 2011; in North America on March 27, 2011, and in Australia on March 31, 2011. The system features backward compatibility with Nintendo DS series software, including Nintendo DSi software. It also features an online service called the Nintendo eShop, launched on June 6, 2011, in North America and June 7, 2011, in Europe and Japan, which allows owners to download games, demos, applications and information on upcoming film and game releases. On November 24, 2011, a limited edition Legend of Zelda 25th Anniversary 3DS was released that contained a unique Cosmo Black unit decorated with gold Legend of Zelda related imagery, along with a copy of The Legend of Zelda: Ocarina of Time 3D.
There are also other models including the Nintendo 2DS and the New Nintendo 3DS, the latter with a larger (XL/LL) variant, like the original Nintendo 3DS.
Sony Ericsson Xperia PLAY.
The Sony Ericsson Xperia PLAY is a handheld game console smartphone produced by Sony Ericsson under the Xperia smartphone brand. The device runs Android 2.3 Gingerbread, and is the first to be part of the PlayStation Certified program which means that it can play PlayStation Suite games. The device is a horizontally sliding phone with its original form resembling the Xperia X10 while the slider below resembles the slider of the PSP Go. The slider features a D-pad on the left side, a set of standard PlayStation buttons (, , and ) on the right, a long rectangular touchpad in the middle, start and select buttons on the bottom right corner, a menu button on the bottom left corner, and two shoulder buttons (L and R) on the back of the device. It is powered by a 1 GHz Qualcomm Snapdragon processor, a Qualcomm Adreno 205 GPU, and features a display measuring 4.0 inches (100 mm) (854 × 480), an 8-megapixel camera, 512 MB RAM, 8 GB internal storage, and a micro-USB connector. It supports microSD cards, versus the Memory Stick variants used in PSP consoles. The device was revealed officially for the first time in a Super Bowl ad on Sunday, February 6, 2011. On February 13, 2011, at Mobile World Congress (MWC) 2011, it was announced that the device would be shipping globally in March 2011, with a launch lineup of around 50 software titles.
PlayStation Vita.
The PlayStation Vita is the successor to Sony's PlayStation Portable (PSP) Handheld series. It was released in Japan on December 17, 2011 and in Europe, Australia, North and South America on February 22, 2012.
The handheld includes two analog sticks, a 5-inch (130 mm) OLED/LCD multi-touch capacitive touchscreen, and supports Bluetooth, Wi-Fi and optional 3G. Internally, the PS Vita features a 4 core ARM Cortex-A9 MPCore processor and a 4 core SGX543MP4+ graphics processing unit, as well as LiveArea software as its main user interface, which succeeds the XrossMediaBar.
The device is fully backwards-compatible with PlayStation Portable games digitally released on the PlayStation Network via the PlayStation Store. However, PSone Classics and PS2 titles were not compatible at the time of the primary public release in Japan. The Vita's dual analog sticks will be supported on selected PSP games. The graphics for PSP releases will be up-scaled, with a smoothing filter to reduce pixelation.
Razer Switchblade.
The Razer Switchblade is an upcoming pocket-sized like a Nintendo DSi XL and runs on Windows 7, features a multitouch LCD screen and an adaptive keyboard that changes keys depending on the game you play. It also features a full mouse.
It was first unveiled on January 5, 2011, on the Consumer Electronics Show (CES). The Switchblade won The Best of CES 2011 People's Voice award. It has since been in development and the release date is still unknown.
Nvidia Shield.
Project Shield is a handheld system developed by Nvidia announced at CES 2013. It runs on Android 4.2 and uses Nvidia Tegra 4 SoC. The hardware includes a 5-inches multitouch screen with support for HD graphics (720p). The console allows for the streaming of games running on a compatible desktop PC, or laptop.

</doc>
<doc id="14200" url="http://en.wikipedia.org/wiki?curid=14200" title="Heinrich Abeken">
Heinrich Abeken

Heinrich Abeken (August 19, 1809 – August 8, 1872) was a German theologian and Prussian Privy Legation Councillor in the Ministry of Foreign Affairs in Berlin.
Abeken was born and raised in the city of Osnabrück as a son of a merchant, he was incited to a higher education by the example of his uncle Bernhard Rudolf Abeken. After finishing the college in Osnabrück, he moved in 1827 to visit the University of Berlin to study theology. He soon combined philosophical and philological studies and was interested in art and modern literature.
In 1831, Abeken acquired a licenciate of theology. At the end of the year he visited Rome, and was welcomed in the house of Christian Karl Josias, Freiherr von Bunsen. Abeken participated in Bunsen's works, namely an evangelic prayer and hymn-book. In 1834 became chaplain to the Prussian embassy in Rome. He married his first wife, who died soon thereafter.
Bunsen left Rome in 1838 and Abeken followed soon thereafter to Germany. In 1841, he was sent to England to help founding a German-English evangelic episcopacy in Jerusalem. In the same year, he was sent by Frederick William IV of Prussia to Egypt and Ethiopia, where he joined an expedition led by professor Karl Richard Lepsius. In 1845 and 1846 he returned via Jerusalem and Rome to Germany. He became Legation Councillor in Berlin, later Council Referee at the Ministry of Foreign Affairs.
In 1848 he received an appointment in the Prussian ministry for foreign affairs, and in 1853 was promoted to be privy councillor of legation ("Geheimer Legationsrath"). Abeken remained in charge for more than twenty years of Prussian politics, assisting Otto Theodor Freiherr von Manteuffel and Chancellor Otto von Bismarck. The latter was so much pleased with Abeken's work that officials started to call Abeken "the quill [i.e., the scribe] of Bismarck." Abeken married in 1866 Hedwig von Olfers, daughter of the general director of the royal museums, Privy Council von Olfers.
He was much employed by Bismarck in the writing of official despatches, and stood high in the favour of King William, whom he often accompanied on his journeys as representative of the foreign office. He was present with the king during the campaigns of 1866 and 1870-71. In 1851 he published anonymously "Babylon und Jerusalem," a slashing criticism of the views of the Countess von Hahn-Hahn.
During the war against Austria in 1866 as well as in the wars against France in 1870 and 1871, Abeken stayed in the Prussian headquarters. A major part of the dispatches of the time have been written by him. Unfortunately his health was damaged by the endeavours of these travels, and he died after an illness of several months. Emperor Wilhelm I described Abeken in a condolence letter to his widow: "One of my most reliable advisors, standing on my side in the most decisive moments; His loss is irreplaceable to me; In him his fatherland has lost one of the most noble and most loyal men and officials."
Despite his engagement in politics, Abeken never lost his interest in theology and continued to publish and speak in this sector during all of his life. He was interested in art and archeology, and was sponsor of the Archeological Institute of Rome and member of the Archeological Society of Rome. He founded a Circle of Friends of the Greek Literature in Berlin and was member of the prize commission for the royal Schiller-Prize.
See "Heinrich Abeken, ein schlichtes Leben in bewegter Zeit" (Berlin, 1898), by his widow. This is valuable by reason of the letters written from the Prussian headquarters.

</doc>
<doc id="14201" url="http://en.wikipedia.org/wiki?curid=14201" title="Henry Bruce, 1st Baron Aberdare">
Henry Bruce, 1st Baron Aberdare

Henry Austin Bruce, 1st Baron Aberdare GCB, PC, FRS (16 April 1815 – 25 February 1895) was a British Liberal Party politician, who served in government most notably as Home Secretary (1868–1873) and as Lord President of the Council.
Background and education.
Henry Bruce was born at Duffryn, Aberdare, Glamorganshire, the son of John Bruce, a Glamorganshire landowner, by his wife Sarah, daughter of Reverend Hugh Williams Austin. John Bruce's original family name was Knight, but on coming of age in 1805 he assumed the name of Bruce: his mother, through whom he inherited the Duffryn estate, was the daughter of William Bruce, high sheriff of Glamorganshire.
Henry was educated at the Bishop Gore School, Swansea (Swansea Grammar School), and in 1837 was called to the bar. Shortly after he had begun to practice, the discovery of coal beneath the Duffryn and other Aberdare Valley estates brought his family great wealth.
Political career.
From 1847 to 1854 Bruce was stipendiary magistrate for Merthyr Tydfil and Aberdare, resigning the position in the latter year, when he entered parliament as Liberal member for Merthyr Tydfil. The electorate at this time remained relatively small, excluding the vast majority of the working classes. Significantly, however, Bruce's relationship with the miners of the Aberdare Valley, in particular, deteriorated as a result of the Aberdare Strike of 1857-8. In a speech to a large audience of miners at the Aberdare Market Hall, Bruce sought to strike a conciliatory tone in persuading the miners to retuen to work. In a second speech, however, he delivered a broadside against the trade union movement generally, referring to the violence engendered elsewhere as a result of strikes and to alleged examples of intimidation and violence in the immediate locality. The strike damaged his reputation and may well have contributed to his eventual election defeat ten years later.
During his time as member for Merthyr, he became involved in the management of the Dowlais Iron Company. In 1862 he became Under-Secretary of State for the Home Department. At the 1868 General Election, Merthyr Tydfil became a two-member constituency but Bruce was defeated by Henry Richard and Richard Fothergill. However, after losing his seat, Bruce was elected for Renfrewshire, he was made Home Secretary by William Ewart Gladstone. His tenure of this office was conspicuous for a reform of the licensing laws, and he was responsible for the Licensing Act 1872, which made the magistrates the licensing authority, increased the penalties for misconduct in public-houses and shortened the number of hours for the sale of drink. In 1873 Bruce relinquished the home secretaryship, at Gladstone's request, to become Lord President of the Council, and was raised to the peerage as Baron Aberdare, of Duffryn in the County of Glamorgan, on 23 August that year.
Public career after 1874.
The defeat of the Liberal government in the following year terminated Lord Aberdare's official political life, and he subsequently devoted himself to social, educational and economic questions. In 1876 he was elected a Fellow of the Royal Society; from 1878 to 1891 he was president of the Royal Historical Society; and in 1881 he became president of both the Royal Geographical Society and the Girls' Day School Trust. In 1888 he headed the commission that established the Official Table of Drops, listing how far a person of a particular weight should be dropped when hanged for a capital offence (the only method of 'judicial execution' in the United Kingdom at that time), to ensure an instant and painless death, by cleanly breaking the neck between the 2nd and 3rd vertebrae, an 'exacting science', eventually brought to perfection by Chief Executioner Albert Pierrepoint.
In 1882 he began a connection with West Africa which lasted the rest of his life, by accepting the chairmanship of the National African Company, formed by Sir George Goldie, which in 1886 received a charter under the title of the Royal Niger Company and in 1899 was taken over by the British government, its territories being constituted the protectorate of Nigeria. West African affairs, however, by no means exhausted Lord Aberdare's energies, and it was principally through his efforts that a charter was in 1894 obtained for the University College of South Wales and Monmouthshire,a constituent institution of the University of Wales. This is now Cardiff University. Lord Aberdare, who in 1885 was made a Knight Grand Cross of the Order of the Bath, presided over several Royal Commissions at different times.
Family.
Henry Bruce married firstly Annabella, daughter of Richard Beadon, in 1846. They had one son and three daughters. After her death in July 1852 he married secondly Norah Creina Blanche, daughter of Sir William Napier, the historian of the Peninsular War, whose biography he edited. They had seven daughters and two sons, of whom the youngest was the mountaineer Charles Granville Bruce. Their daughter, Sarah was married to Montague Muir Mackenzie, barrister.
Lord Aberdare died in London on 25 February 1895, aged 79, and was succeeded in the barony by his only son from his first marriage, Henry. Lady Aberdare, born 1827, died in April 1897 and was a proponent of women's education and active in the establishment of Aberdare Hall in Cardiff.
Memorial.
Henry Austin Bruce is buried at Aberffrwd Cemetery in Mountain Ash, Wales. His large family plot is surrounded by a chain, and his grave is a simple Celtic cross with double plinth and kerb. In place is written "To God the Judge of all and to the spirits of just men more perfect."

</doc>
<doc id="14203" url="http://en.wikipedia.org/wiki?curid=14203" title="Harpers Ferry (disambiguation)">
Harpers Ferry (disambiguation)

Harpers Ferry is the name of several places in the United States of America:
Harpers Ferry may also refer to:

</doc>
<doc id="14204" url="http://en.wikipedia.org/wiki?curid=14204" title="Halophile">
Halophile

Halophiles are organisms that thrive in high salt concentrations. They are a type of extremophile organism. The name comes from the Greek word for "salt-loving". While most halophiles are classified into the Archaea domain, there are also bacterial halophiles and some eukaryota, such as the alga Dunaliella salina or fungus "Wallemia ichthyophaga". Some well-known species give off a red color from carotenoid compounds, notably bacteriorhodopsin. Halophiles can be found anywhere with a concentration of salt five times greater than the salt concentration of the ocean, such as the Great Salt Lake in Utah, Owens Lake in California, the Dead Sea, and in evaporation ponds.
Classification.
Halophiles are categorized as slight, moderate, or extreme, by the extent of their halotolerance. Slight halophiles prefer 0.3 to 0.8 M (1.7 to 4.8% - seawater is 0.6 M or 3.5%), moderate halophiles 0.8 to 3.4 M (4.7 to 20%), and extreme halophiles 3.4 to 5.1 M (20 to 30%) NaCl. Halophiles require NaCl for growth, in contrast to halotolerant organisms, which do not require NaCl but can grow under saline conditions.
Lifestyle.
High salinity represents an extreme environment that relatively few organisms have been able to adapt to and occupy. Most halophilic and all halotolerant organisms expend energy to exclude salt from their cytoplasm to avoid protein aggregation (‘salting out’). In order to survive the high salinities, halophiles employ two differing strategies to prevent desiccation through osmotic movement of water out of their cytoplasm. Both strategies work by increasing the internal osmolarity of the cell. In the first (that is employed by the majority of bacteria, some archaea, yeasts, algae and fungi), organic compounds are accumulated in the cytoplasm – these osmoprotectants are known as compatible solutes. These can be synthesised or accumulated from the environment. The most common compatible solutes are neutral or zwitterionic and include amino acids, sugars, polyols, betaines and ectoines, as well as derivatives of some of these compounds.
The second, more radical, adaptation involves the selective influx of potassium (K+) ions into the cytoplasm. This adaptation is restricted to the moderately halophilic bacterial Order Halanerobiales, the extremely halophilic archaeal Family Halobacteriaceae and the extremely halophilic bacterium "Salinibacter ruber". The presence of this adaptation in three distinct evolutionary lineages suggests convergent evolution of this strategy, it being unlikely to be an ancient characteristic retained in only scattered groups or through massive lateral gene transfer. The primary reason for this is that the entire intracellular machinery (enzymes, structural proteins, etc.) must be adapted to high salt levels, whereas in the compatible solute adaptation little or no adjustment is required to intracellular macromolecules – in fact, the compatible solutes often act as more general stress protectants as well as just osmoprotectants.
Of particular note are the extreme halophiles or haloarchaea (often known as halobacteria), a group of archaea, which require at least a 2 M salt concentration and are usually found in saturated solutions (about 36% w/v salts). These are the primary inhabitants of salt lakes, inland seas, and evaporating ponds of seawater, such as the deep salterns, where they tint the water column and sediments bright colors. In other words, they will most likely perish if they are exposed to anything other than a very high concentration salt-conditioned environment. These prokaryotes require salt for growth. The high concentration of NaCl in their environment limits the availability of oxygen for respiration. Their cellular machinery is adapted to high salt concentrations by having charged amino acids on their surfaces, allowing the retention of water molecules around these components. They are heterotrophs that normally respire by aerobic means. Most halophiles are unable to survive outside their high-salt native environment. Indeed, many cells are so fragile that when placed in distilled water they immediately lyse from the change in osmotic conditions.
Halophiles may use a variety of energy sources. They can be aerobic or anaerobic. Anaerobic halophiles include phototrophic, fermentative, sulfate-reducing, homoacetogenic and methanogenic species.
Haloarchaea, and particularly, the family Halobacteriaceae are members of the domain Archaea, and comprise the majority of the prokaryotic population in hypersaline environments. There are currently 15 recognised genera in the family. The domain Bacteria (mainly "Salinibacter ruber") can comprise up to 25% of the prokaryotic community, but is more commonly a much lower percentage of the overall population. At times, the alga "Dunaliella salina" can also proliferate in this environment.
A comparatively wide range of taxa have been isolated from saltern crystalliser ponds, including members of the following genera: "Haloferax, Halogeometricum, Halococcus, Haloterrigena, Halorubrum, Haloarcula" and "Halobacterium" families. However, the viable counts in these cultivation studies have been small when compared to total counts, and the numerical significance of these isolates has been unclear. Only recently has it become possible to determine the identities and relative abundances of organisms in natural populations, typically using PCR-based strategies that target 16S small subunit ribosomal ribonucleic acid (16S rRNA) genes. While comparatively few studies of this type have been performed, results from these suggest that some of the most readily isolated and studied genera may not in fact be significant in the in-situ community. This is seen in cases such as the genus "Haloarcula", which is estimated to make up less than 0.1% of the in situ community but commonly appears in isolation studies.
Genomic and proteomic signature of halophiles.
The comparative genomic and proteomic analysis showed that there is a distinct molecular signatures for environmental adaptation of halophiles. At the protein level, the halophilic species are characterized by low hydrophobicity, overrepresentation of acidic residues, underrepresentation of Cys, lower propensities for helix formation and higher propensities for coil structure. It is also evident that the core of these proteins is less hydrophobic, such as DHFR, that was found to have narrower β-strands 
At the DNA level, the halophiles exhibit distinct dinucleotide and codon usage.
Examples.
Halobacterium is a group of Archaea that have a high tolerance for elevated levels of salinity. Some species of halobacteria have acidic proteins that resist the denaturing effects of salts. "Halococcus" is a specific genus of the family Halobacterium.
Some hypersaline lakes are a habitat to numerous families of halophiles. For example, the Makgadikgadi Pans in Botswana is a vast seasonal high salinity water body that manifests halophilic species within the diatom genus "Nitzschia" in the family Bacillariaceae as well as species within the genus "Lovenula" in the family Diaptomidae. Owens Lake in California also contains a large population of the halophilic bacteria Halobacterium halobium.
"Wallemia ichthyophaga" is a basidiomycetous fungus, which requires at least 1.5 M NaCl for in-vitro growth, and it thrives even in medium saturated with NaCl. Obligate requirement for salt is an exception in fungi. Even species that can tolerate salt concentrations close to saturation (for example "Hortaea werneckii") in almost all cases grow well in standard microbiological media without the addition of salt.
The fermentation of salty foods (such as soy sauce, Chinese fermented beans, salted cod, salted anchovies, sauerkraut etc.) often involves halobacteria, as either essential ingredients or accidental contaminants. One example is "Chromohalobacter beijerinckii", found in salted beans preserved in brine and in salted herring. "Tetragenococcus halophilus" is found in salted anchovies and soy sauce.

</doc>
<doc id="14205" url="http://en.wikipedia.org/wiki?curid=14205" title="Herbert A. Simon">
Herbert A. Simon

Herbert Alexander Simon (June 15, 1916 – February 9, 2001), a Nobel laureate, was an American political scientist, economist, sociologist, psychologist, computer scientist, and Richard King Mellon Professor—most notably at Carnegie Mellon University—whose research ranged across the fields of cognitive psychology, cognitive science, computer science, public administration, economics, management, philosophy of science, sociology, and political science, unified by studies of decision-making. With almost a thousand highly cited publications, he was one of the most influential social scientists of the twentieth century.
Simon was among the founding fathers of several of today's important scientific domains, including artificial intelligence, information processing, decision-making, problem-solving, attention economics, organization theory, complex systems, and computer simulation of scientific discovery.
He coined the terms "bounded rationality" and "satisficing", and was the first to analyze the architecture of complexity and to propose a preferential attachment mechanism to explain power law distributions.
He also received many top-level honors later in life. These include: becoming a fellow of the American Academy of Arts and Sciences in 1959; election to the National Academy of Sciences in 1967; APA Award for Distinguished Scientific Contributions to Psychology (1969);the ACM's Turing Award for making "basic contributions to artificial intelligence, the psychology of human cognition, and list processing" (1975); the Nobel Memorial Prize in Economics "for his pioneering research into the decision-making process within economic organizations" (1978); the National Medal of Science (1986); the APA's (1993); ACM fellow (1994); and IJCAI Award for Research Excellence (1995).
As a testament to his interdisciplinary approach, Simon was affiliated with such varied Carnegie Mellon departments as the School of Computer Science, Tepper School of Business, departments of Philosophy, Social and Decision Sciences, and Psychology. Simon received an honorary Doctor of Political science degree from University of Pavia in 1988 and an honorary Doctor of Laws (LL.D.) degree from Harvard University in 1990.
Early life and education.
Herbert Alexander Simon was born in Milwaukee, Wisconsin on June 15, 1916. His father, Arthur Simon (1881–1948), was an electrical engineer who had come to the United States from Germany in 1903 after earning his engineering degree from the Technische Hochschule of Darmstadt. An inventor who was granted "several dozen patents", his father also was an independent patent attorney. His mother, Edna Marguerite Merkel, was an accomplished pianist whose ancestors had come from Prague and Cologne. His European ancestors had been piano makers, goldsmiths, and vintners. Simon's father was Jewish and his mother came from a family with Jewish, Lutheran, and Catholic backgrounds. Simon called himself an atheist.
Simon was educated as a child in the public school system in Milwaukee where he developed an interest in science. He found schoolwork to be interesting, but rather easy. Unlike many children, Simon was exposed to the idea that human behavior could be studied scientifically at a relatively young age due to the influence of his mother’s younger brother, Harold Merkel, who had studied economics at the University of Wisconsin–Madison under John R. Commons. Through his uncle’s books on economics and psychology, Simon discovered the social sciences. Among his earliest influences, Simon has cited Richard Ely’s economics textbook, Norman Angell’s "The Great Illusion", and Henry George’s "Progress and Poverty". 
In 1933, Simon entered the University of Chicago, and following those early influences, he studied the social sciences and mathematics. He was interested in biology, but chose not to study it because of his "color-blindness and awkwardness in the laboratory". He chose instead to focus on political science and economics. His most important mentor at the University was Henry Schultz who was an econometrician and mathematical economist. Simon received both his B.A. (1936) and his Ph.D. (1943) in political science, from the University of Chicago, where he studied under Harold Lasswell and Charles Edward Merriam.
After enrolling in a course on "Measuring Municipal Governments," Simon was invited to be a research assistant for Clarence Ridley, with whom he coauthored the book, "Measuring Municipal Activities", in 1938, the same year that he and Dorothea married. Eventually his studies led him to the field of organizational decision-making, which would become the subject of his doctoral dissertation.
Academic career.
From 1939 to 1942, Simon was director of a research group at the University of California, Berkeley. 
From 1942 to 1949, Simon was a professor of political science and also served as department chairman at Illinois Institute of Technology. Back in Chicago, he began participating in the seminars held by the staff of the Cowles Commission who at that time included Trygve Haavelmo, Jacob Marschak, and Tjalling Koopmans. He thus began a more in-depth study of economics in the area of institutionalism. Marschak brought Simon in to assist in the study he was currently undertaking with Sam Schurr of the “prospective economic effects of atomic energy”.
From 1949 to 2001, Simon was a faculty at Carnegie Mellon. In 1949, Simon became a professor of administration and chairman of the Department of Industrial Management at Carnegie Tech (later to become Carnegie Mellon University). Simon later also taught psychology and computer science in the same university, (occasionally visiting other universities.).
Personal life and interests.
Simon got married with Dorothea in 1938. Their marriage lasted 63 years until his death. They had three children, Katherine, Peter, and Barbara. His wife died in 2002, during the year following his death in 2001.
From 1950 to 1955, Simon studied mathematical economics and during this time, together with David Hawkins, discovered and proved the Hawkins–Simon theorem on the “conditions for the existence of positive solution vectors for input-output matrices." He also developed theorems on near-decomposability and aggregation. Having begun to apply these theorems to organizations, by 1954 Simon determined that the best way to study problem-solving was to simulate it with computer programs, which led to his interest in computer simulation of human cognition. Founded during the 1950s, he was among the first members of the Society for General Systems Research.
Simon had a keen interest in the arts. He was a friend of Robert Lepper and Richard Rappaport. Rappaport also painted Simon's commissioned portrait at Carnegie Mellon University.
In January 2001, Simon underwent surgery at UPMC Presbyterian to remove a cancerous tumor in his abdomen. Although the surgery was successful, Simon later succumbed to the complications that followed.
Study of decision-making.
 I am a monomaniac. What I am a monomaniac about is decision-making.
"Administrative Behavior", from 1947, was based on Simon’s doctoral dissertation. It served as the foundation for his life's work. The centerpiece of this book is the behavioral and cognitive processes of making rational human choices, that is, decisions. By his definition, an operational administrative decision should be correct and efficient, and it must be practical to implement with a set of coordinated means.
Any such decision involves a choice selected from a number of alternatives, directed toward an organizational goal or subgoal. Realistic options were defined as having real consequences consisting of personnel actions or non-actions modified by environmental facts and values. In practice, some of the alternatives may be conscious or unconscious; some of the consequences may be unintended as well as intended; and some of the means and ends may be imperfectly differentiated, incompletely related, or poorly detailed.
The task of rational decision making is to select the alternative that results in the more preferred set of all the possible consequences. This task may be divided into three required steps:
Any given individual or organization attempting to implement this model in a real situation would be unable to comply with the three requirements. It is highly improbable that one could know all the alternatives, or all the consequences that follow each alternative.
The resulting question would be: given the inevitable limits on rational decision making, what other techniques or behavioral processes can a person or organization bring to bear to achieve approximately the best result? Simon writes:
The human being striving for rationality and restricted within the limits of his knowledge has developed some working procedures that partially overcome these difficulties. These procedures consist in assuming that he can isolate from the rest of the world a closed system containing a limited number of variables and a limited range of consequences.
"Administrative Behavior", as a text, addresses a wide range of human behaviors, cognitive abilities, management techniques, personnel policies, training goals and procedures, specialized roles, criteria for evaluation of accuracy and efficiency, and all of the ramifications of communication processes. Simon is particularly interested in how these factors directly and indirectly influence the making of decisions.
Weaving in and out of the practical functioning of all of these organizational factors are two universal elements of human social behavior that Simon addresses in Chapter VII—The Role of Authority, and in Chapter X—Loyalties, and Organizational Identification.
Authority is a well studied, primary mark of organizational behavior, and is straightforwardly defined in the organizational context as the ability and right of an individual of higher rank to determine the decision of an individual of lower rank. The actions, attitudes, and relationships of the dominant and subordinate individuals constitute components of role behavior that may vary widely in form, style, and content, but do not vary in the expectation of obedience by the one of superior status, and willingness to obey from the subordinate.
Authority is highly influential on the formal structure of the organization, including patterns of communication, sanctions, and rewards, as well as on the establishment of goals, objectives, and values of the organization.
Decisions can be complex admixtures of facts and values. Information about facts, especially empirically-proven facts or facts derived from specialized experience, are more easily transmitted in the exercise of authority than are the expressions of values. Simon is primarily interested in seeking identification of the individual employee with the organizational goals and values. Following Lasswell, he states that “a person identifies himself with a group when, in making a decision, he evaluates the several alternatives of choice in terms of their consequences for the specified group”. A person may identify himself with any number of social, geographic, economic, racial, religious, familial, educational, gender, political, and sports groups. Indeed, the number and variety are unlimited. The fundamental problem for organizations is to recognize t personal and group identifications may either facilitate or obstruct correct decision making for the organization. A specific organization has to determine deliberately, and specify in appropriate detail and clear language, its own goals, objectives, means, ends, and values.
Chester Barnard pointed out that “the decisions that an individual makes as a member of an organization are quite distinct from his personal decisions”.
Personal choices may be determined whether an individual joins a particular organization, and continue to be made in his or her extra–organizational private life. As a member of an organization, however, that individual makes decisions not in relationship to personal needs and results, but in an impersonal sense as part of the organizational intent, purpose, and effect. Organizational inducements, rewards, and sanctions are all designed to form, strengthen, and maintain this identification.
The correctness of administrative decisions is measured by two major criteria:
Simon's contributions to research in the area of administrative decision-making have become increasingly mainstream in the business community thanks to the growth of management consulting.
Artificial intelligence and psychology.
Simon was a pioneer in the field of artificial intelligence, creating with Allen Newell the Logic Theory Machine (1956) and the General Problem Solver (GPS) (1957) programs. GPS may possibly be the first method developed for separating problem solving strategy from information about particular problems. Both programs were developed using the Information Processing Language (IPL) (1956) developed by Newell, Cliff Shaw, and Simon. Donald Knuth mentions the development of list processing in IPL, with the linked list originally called "NSS memory" for its inventors. In 1957, Simon predicted that computer chess would surpass human chess abilities within "ten years" when, in reality, that transition took about forty years.
In the early 1960s psychologist Ulric Neisser asserted that while machines are capable of replicating 'cold cognition' behaviors such as reasoning, planning, perceiving, and deciding, they would never be able to replicate 'hot cognition' behaviors such as pain, pleasure, desire, and other emotions. Simon responded to Neisser's views in 1963 by writing a paper on emotional cognition, which he updated in 1967 and published in "Psychological Review". Simon's work on emotional cognition was largely ignored by the artificial intelligence research community for several years, but subsequent work on emotions by Sloman and Picard helped refocus attention on Simon's paper and eventually, made it highly influential on the topic.
Simon also collaborated with James G. March on several works in organization theory.
With Allen Newell, Simon developed a theory for the simulation of human problem solving behavior using production rules. The study of human problem solving required new kinds of human measurements and, with Anders Ericsson, Simon developed the experimental technique of verbal protocol analysis. Simon was interested in the role of knowledge in expertise. He said that to become an expert on a topic required about ten years of experience and he and colleagues estimated that expertise was the result of learning roughly 50,000 chunks of information. A chess expert was said to have learned about 50,000 chunks or chess position patterns.
He was awarded the ACM A.M. Turing Award along with Allen Newell in 1975. "In joint scientific efforts extending over twenty years, initially in collaboration with J. C. (Cliff) Shaw at the RAND Corporation, and subsequentially ["sic"] with numerous faculty and student colleagues at Carnegie Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing."
Psychology.
Simon was interested in how humans learn and, with Edward Feigenbaum, he developed the EPAM (Elementary Perceiver and Memorizer) theory, one of the first theories of learning to be implemented as a computer program. EPAM was able to explain a large number of phenomena in the field of verbal learning. Later versions of the model were applied to concept formation and the acquisition of expertise. With Fernand Gobet, he has expanded the EPAM theory into the CHREST computational model. The theory explains how simple chunks of information form the building blocks of schemata, which are more complex structures. CHREST has been used predominantly, to simulate aspects of chess expertise.
Sociology and economics.
Simon has been credited for revolutionary changes in microeconomics. He is responsible for the concept of organizational decision-making as it is known today. He also was the first to discuss this concept in terms of uncertainty; i.e. it is impossible to have perfect and complete information at any given time to make a decision. While this notion was not entirely new, Simon is best known for its origination. It was in this area that he was awarded the Nobel Prize in 1978.
At the Cowles Commission, Simon’s main goal was to link economic theory to mathematics and statistics. His main contributions were to the fields of general equilibrium and econometrics. He was greatly influenced by the marginalist debate that began in the 1930s. The popular work of the time argued that it was not apparent empirically that entrepreneurs needed to follow the marginalist principles of profit-maximization/cost-minimization in running organizations. The argument went on to note that profit-maximization was not accomplished, in part, because of the lack of complete information. In decision-making, Simon believed that agents face uncertainty about the future and costs in acquiring information in the present. These factors limit the extent to which agents may make a fully rational decision, thus they possess only “bounded rationality” and must make decisions by “satisficing,” or choosing that which might not be optimal, but which will make them happy enough.
Simon was known for his research on industrial organization. He determined that the internal organization of firms and the external business decisions thereof, did not conform to the Neoclassical theories of “rational” decision-making. Simon wrote many articles on the topic over the course of his life mainly focusing on the issue of decision-making within the behavior of what he termed “bounded rationality”. “Rational behavior, in economics, means that individuals maximize their utility function under the constraints they face (e.g., their budget constraint, limited choices, ...) in pursuit of their self-interest. This is reflected in the theory of subjective expected utility. The term, bounded rationality, is used to designate rational choice that takes into account the cognitive limitations of both knowledge and cognitive capacity. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision-making process influences decisions. Theories of bounded rationality relax one or more assumptions of standard expected utility theory”.
Simon determined that the best way to study these areas was through computer simulation modeling. As such, he developed an interest in computer science. Simon's main interests in computer science were in artificial intelligence, human-computer interaction, principles of the organization of humans and machines as information processing systems, the use of computers to study (by modeling) philosophical problems of the nature of intelligence and of epistemology, and the social implications of computer technology.
Some of Simon's economic research was directed toward understanding technological change in general and the information processing revolution in particular.
Pedagogy.
Simon's work has strongly influenced John Mighton, developer of a program that has achieved significant success in improving mathematics performance among elementary and high school students. Mighton cites a 2000 paper by Simon and two co-authors that counters arguments by French mathematics educator, Guy Brousseau, and others suggesting that excessive practice hampers children's understanding:
 [The] criticism of practice (called 'drill and kill,' as if this phrase constituted empirical evaluation) is prominent in constructivist writings. Nothing flies more in the face of the last 20 years of research than the assertion that practice is bad. All evidence, from the laboratory and from extensive case studies of professionals, indicates that real competence only comes with extensive practice... In denying the critical role of practice one is denying children the very thing they need to achieve real competence. The instructional task is not to 'kill' motivation by demanding drill, but to find tasks that provide practice while at the same time sustaining interest.
 — John R. Anderson, Lynne M. Reder, and Herbert A. Simon, ", "Texas Educational Review" 6 (2000)
Criticism.
In the 1972 book "What Computers Can't Do", the American philosopher Hubert Dreyfus criticized Simon for claiming that the computer programs of the day already exhibited intelligent behavior. Dreyfus also criticized Simon's optimism about the eventual success of AI as a result of a psychological theory of heuristics. This critique was partly based on the phenomenological claim that certain forms of intelligent human behavior can not be reduced to rule following. 
Selected publications.
Simon is prolific, and authored 27 books and almost a thousand papers.

</doc>
<doc id="14207" url="http://en.wikipedia.org/wiki?curid=14207" title="Hematite">
Hematite

Hematite, also spelled as haematite, is the mineral form of iron(III) oxide (Fe2O3), one of several iron oxides. Hematite crystallizes in the rhombohedral lattice system, and it has the same crystal structure as ilmenite and corundum. Hematite and ilmenite form a complete solid solution at temperatures above 950 C.
Hematite is a mineral, colored black to steel or silver-gray, brown to reddish brown, or red. It is mined as the main ore of iron. Varieties include "kidney ore", "martite" (pseudomorphs after magnetite), "iron rose" and "specularite" (specular hematite). While the forms of hematite vary, they all have a rust-red streak. Hematite is harder than pure iron, but much more brittle. Maghemite is a hematite- and magnetite-related oxide mineral.
Huge deposits of hematite are found in banded iron formations. Gray hematite is typically found in places that can have still standing water or mineral hot springs, such as those in Yellowstone National Park in North America. The mineral can precipitate out of water and collect in layers at the bottom of a lake, spring, or other standing water. Hematite can also occur without water, however, usually as the result of volcanic activity.
Clay-sized hematite crystals can also occur as a secondary mineral formed by weathering processes in soil, and along with other iron oxides or oxyhydroxides such as goethite, is responsible for the red color of many tropical, ancient, or otherwise highly weathered soils.
Etymology and history.
The name hematite is derived from the Greek word for blood αἷμα "haima" because hematite can be red, as in rouge, a powdered form of hematite. The color of hematite lends itself to use as a pigment. The English name of the stone is derived from Middle French: Hématite Pierre, which was imported from Latin: Lapis Hæmatites, which originated from Ancient Greek: αἱματίτης λίθος (haimatitēs lithos, “blood-red stone”).
Ochre is a clay that is colored by varying amounts of hematite, varying between 20% and 70%. Red ochre contains unhydrated hematite, whereas yellow ochre contains hydrated hematite (Fe2O3 • H2O). The principal use of ochre is for tinting with a permanent color.
The red chalk writing of this mineral was one of the earliest in the history of humans. The powdery mineral was first used 164,000 years ago by the Pinnacle-Point man possibly for social purposes. Hematite residues are also found in old graveyards from 80,000 years ago. Near Rydno in Poland and Lovas in Hungary, palaeolitic red chalk mines have been found that are from 5000 BC, belonging to the Linear Pottery culture at the Upper Rhine.
Rich deposits of hematite have been found on the island of Elba that have been mined since the time of the Etruscans.
Magnetism.
Hematite is an antiferromagnetic material below the Morin transition at 250 kelvin (K) or -9.7 degrees Fahrenheit (°F), and a canted antiferromagnet or weakly ferromagnetic above the Morin transition and below its Néel temperature at 948 K, above which it is paramagnetic.
The magnetic structure of a-hematite was the subject of considerable discussion and debate in the 1950s because it appeared to be ferromagnetic with a Curie temperature of around 1000 K, but with an extremely tiny moment (0.002 µB). Adding to the surprise was a transition with a decrease in temperature at around 260 K to a phase with no net magnetic moment. It was shown that the system is essentially antiferromagnetic, but that the low symmetry of the cation sites allows spin–orbit coupling to cause canting of the moments when they are in the plane perpendicular to the c axis. The disappearance of the moment with a decrease in temperature at 260 K is caused by a change in the anisotropy which causes the moments to align along the c axis. In this configuration, spin canting does not reduce the energy. The magnetic properties of bulk hematite differ from their nanoscale counterparts. For example, the Morin transition temperature of hematite decreases with a decrease in the particle size. The suppression of this transition has also been observed in some of the hematite nanoparticles, and the presence of impurities, water molecules and defects in the crystals were attributed to the absence of a Morin transition. Hematite is part of a complex solid solution oxyhydroxide system having various contents of water, hydroxyl groups and vacancy substitutions that affect the mineral's magnetic and crystal chemical properties. Two other end-members are referred to as protohematite and hydrohematite.
Enhanced magnetic coercivities for hematite have been achieved by dry-heating a 2-line ferrihydrite precursor prepared from solution. Hematite exhibited temperature-dependent magnetic coercivity values ranging from 289 to 5,027 Oe. The origin of these high coercivity values has been interpreted as a consequence of the subparticle structure induced by the different particle and crystallite size growth rates at increasing annealing temperature. These differences in the growth rates are translated into a progressive development of a subparticle structure at the nanoscale. At lower temperatures (350–600 °C), single particles crystallize however; at higher temperatures (600-1000 °C), the growth of crystalline aggregates with a subparticle structure is favoured.
Mine tailings.
Hematite is present in the waste tailings of iron mines. A recently developed process, magnetation, uses magnets to glean waste hematite from old mine tailings in Minnesota's vast Mesabi Range iron district. Falu red is a pigment used in traditional Swedish house paints. Originally, it was made from tailings of the Falu mine.
Mars.
The spectral signature of hematite was seen on the planet Mars by the infrared spectrometer on the NASA Mars Global Surveyor ("MGS") and 2001 Mars Odyssey spacecraft in orbit around Mars. The mineral was seen in abundance at two sites on the planet, the Terra Meridiani site, near the Martian equator at 0° longitude, and the Aram Chaos site near the Valles Marineris. Several other sites also showed hematite, e.g., Aureum Chaos. Because terrestrial hematite is typically a mineral formed in aqueous environments or by aqueous alteration, this detection was scientifically interesting enough that the second of the two Mars Exploration Rovers was sent to a site in the Terra Meridiani region designated Meridiani Planum. In-situ investigations by the Opportunity rover showed a significant amount of hematite, much of it in the form of small spherules that were informally named "blueberries" by the science team. Analysis indicates that these spherules are apparently concretions formed from a water solution.
"Knowing just how the hematite on Mars was formed will help us characterize the past environment and determine whether that environment was favorable for life".
Jewelry.
Hematite's popularity in jewelry was at its highest in Europe during the Victorian era. Certain types of hematite or iron oxide-rich clay, especially Armenian bole, have been used in gilding. Hematite is also used in art such as in the creation of intaglio engraved gems. Hematine is a synthetic material sold as "magnetic hematite".

</doc>
<doc id="14208" url="http://en.wikipedia.org/wiki?curid=14208" title="Holocene extinction">
Holocene extinction

The Holocene extinction, sometimes called the Sixth Extinction, is a name proposed to describe the currently ongoing extinction event of species during the present Holocene epoch (since around 10,000 BCE) mainly due to human activity. The large number of extinctions span numerous families of plants and animals including mammals, birds, amphibians, reptiles and arthropods. Although 875 extinctions occurring between 1500 and 2009 have been documented by the International Union for Conservation of Nature and Natural Resources, the vast majority are undocumented. According to the species-area theory and based on upper-bound estimating, the present rate of extinction may be up to 140,000 species per year.
The Holocene extinction includes the disappearance of large mammals known as megafauna, starting between 9,000 and 13,000 years ago, the end of the last Ice Age. This may have been due to the extinction of the mammoths whose habits had maintained grasslands which became birch forests without them. The new forest and the resulting forest fires may have induced climate change. Such disappearances might be the result of the proliferation of modern humans. These extinctions, occurring near the Pleistocene–Holocene boundary, are sometimes referred to as the Quaternary extinction event. The Holocene extinction continues into the 21st century.
There is no general agreement on whether to consider this as merely part of the Quaternary extinction event, or just a result of human-caused changes. Only during these most recent parts of the extinction have plants also suffered large losses. Overall, the Holocene extinction can be characterized by the human impact on the environment.
Prehistoric extinctions.
North and South America.
There has been a debate as to the extent to which the disappearance of megafauna at the end of the last glacial period can be attributed to human activities, directly, by hunting, or indirectly, by slaughter of prey populations. Discoveries at Monte Verde in South America and at Meadowcroft Rock Shelter in Pennsylvania have caused a controversy regarding the Clovis Culture. There likely have been human settlements prior to the Clovis Culture, and the history of humans in the Americas may extend back many thousands of years before the Clovis Culture. There is no strong correlation between human arrival and megafauna extinction: for example in Wrangel Island in Siberia the extinction of midget Mammoths (approximately 1000 BCE) did not coincide with the arrival of humans.
The ongoing extinction seems more outstanding in view of the apparent separation between recent extinctions (approximately since the industrial revolution) and the Pleistocene extinction near the end of the last glacial period. The latter is exemplified by the extinction of large herbivores such as the woolly mammoth and the carnivores that preyed on them. We know that humans of this era actively hunted the mammoth and the mastodon but it is not known if this hunting was the cause of the subsequent massive ecological changes, widespread extinctions and climate changes. The ecosystems encountered by the first Americans had not been exposed to human interaction and were far less resilient to human made changes than the ecosystems encountered by industrial era humans, those environments seasoned as they were, having been exposed to over 10,000 years of human interaction. Therefore the actions of the Clovis people and likewise, despite seeming insignificant by today's standards could indeed have had a profound effect on the ecosystems and wild life which was entirely unused to human influence.
New Zealand.
Circa 1500, several species became extinct after Polynesian settlers arrived, including:
Pacific, including Hawaii.
Recent research, based on archaeological and paleontological digs on 70 different islands, has shown that numerous species went extinct as people moved across the Pacific, starting 30,000 years ago in the Bismarck Archipelago and Solomon Islands. It is currently estimated that among the bird species of the Pacific some 2000 species have gone extinct since the arrival of humans. Among the extinctions were:
Ten species or subspecies of birds have disappeared from the Hawaiian islands since the 1980s. These include the Kaua'i o'o, nukupu'u, 'akialoa, kama'o, po'ouli, and others.
Madagascar.
Starting with the arrival of humans around 2,000 years ago, nearly all of the island's megafauna became extinct, including:
Indian Ocean islands.
Starting c. 1500, a number of species became extinct upon human settlement of the islands, including:
Ongoing Holocene extinction.
One scientist estimates the current extinction rate may be 10,000 times the background extinction rate. Nevertheless most scientists predict a much lower extinction rate than this outlying estimate. Stuart Pimm stated "the current rate of species extinction is about 100 times the natural rate" for plants. Mass extinctions are characterized by the loss of at least 75% of species within a geologically short period of time.
Megafaunal extinctions continue into the 21st century. Modern extinctions are more directly attributable to human influences. Extinction rates are minimized in the popular imagination by the survival of captive populations of animals that are "extinct in the wild" (Père David's deer, etc.), by marginal survivals of highly publicized megafauna that are "ecologically extinct" (the giant panda, Sumatran rhinoceros, North American black-footed ferret, etc.) and by extinctions among arthropods. Some examples of modern extinctions of "charismatic" mammal fauna include:
Many birds have become extinct as a result of human activity, especially birds endemic to islands, including many flightless birds ("see a more complete list under "extinct birds). Notable extinct birds include:
The decline of amphibian populations has also been identified as an indicator of environmental degradation.
Peter Raven, past president of the American Association for the Advancement of Science (AAAS), states in the foreword to their publication "AAAS Atlas of Population and Environment": "We have driven the rate of biological extinction, the permanent loss of species, up several hundred times beyond its historical levels, and are threatened with the loss of a majority of all species by the end of the 21st century."
189 countries which are signatory to the Convention on Biological Diversity (Rio Accord) have committed to preparing a Biodiversity Action Plan, a first step at identifying specific endangered species and habitats, country by country.
Various species are predicted to go extinct in the near future.
Human influence on extinction.
Extinction of animals, plants, and other organisms caused by human actions may go as far back as the late Pleistocene, over 12,000 years ago. There is evidence that abrupt climate change has especially played an enormous role in the extinction of larger mammals. However, while previous mass extinctions were due to natural environmental causes, research shows that wherever on Earth humans have migrated, other species have gone extinct, and human population growth, most prominently in the past two centuries, is regarded as one of the underlying causes of this Holocene extinction event. In terms of how humans have contributed to this mass extinction, three major factors include: the increased global concentration of greenhouse gases, affecting the global climate; oceanic devastation, such as through overfishing and contamination; and the modification and destruction of vast tracts of land and river systems around the world to meet solely human-centered ends (with 10 to 15 percent of Earth's land surface now used as urban-industrial or row-crop agricultural sites and 6 to 8 percent used as pastures), thus ruining the local ecosystems. Other, related human causes of the extinction event include deforestation, hunting, pollution, the introduction in various regions of non-native species, and the widepsread transmission of infectious diseases. At present, the rate of extinction of species is estimated at 100 to 1,000 times higher than the "base" or historically typical rate of extinction (in terms of the natural evolution of the planet) and also the current rate of extinction is, therefore, 10 to 100 times higher than any of the previous mass extinctions in the history of Earth. On the other hand, this extinction concerns a large number of plants, different from previous extinctions.
The abundance of species extinctions considered "anthropogenic", or due to human activity, have sometimes (especially when referring to hypothesized future events) been collectively called the "Anthropocene extinction". The Anthropocene is a term introduced in 2000. Most biologists believe that we are at the beginning of an anthropogenic mass extinction that is accelerating at a large rate. In "The Future of Life" (2002), E.O. Wilson of Harvard calculated that, if the current rate of human disruption of the biosphere continues, one-half of Earth's higher lifeforms will be extinct by 2100. A 1998 poll conducted by the American Museum of Natural History found that seventy percent of biologists believe that we are in the midst of an anthropogenic extinction. Numerous scientific studies—such as a 2004 report published in "Nature", and papers authored by the 10,000 scientists who contribute to the IUCN's annual Red List of threatened species—have since reinforced this conviction.
The evidence of all previous extinctions is geological in nature, and shorter geological time scale is of the order of several hundred thousand to several million years. Even extinctions caused by instantaneous events such as the impact of the asteroid in Chicxulub, which is currently the best example, extend the equivalent of many human lives, due to complex ecological interactions that are triggered by the event.
Recent extinctions described are well-documented, but the nomenclature used varies. The term Anthropocene is a term that is used by few scientists, and some commentators may refer to the current and projected future extinctions as part of a longer Holocene extinction. The Holocene–Anthropocene boundary is contested, with some commentators asserting significant human influence on climate for much of what is normally regarded as the Holocene Epoch. Other commentators place the Holocene–Anthropocene boundary at the industrial revolution while also saying that "Formal adoption of this term in the near future will largely depend on its utility, particularly to earth scientists working on late Holocene successions."
Three hypotheses have been proposed to explain the extinction of megafauna in the late Pleistocene. Of these, only two have much scientific credibility. Although Ross McPhee proposed that a hyper-disease may have been the cause of the extinction, the study by Lyons "et al.", demonstrated conclusively that a hyperdisease was unlikely to have caused the extinction. The two main theories to the extinction are climate change and human hunting. The climate change theory has suggested that a change in climate near the end of the late Pleistocene stressed the megafauna to the point of extinction. Some scientists favor abrupt climate change as the catalyst for the extinction of the mega-fauna at the end of the Pleistocene, but there are many who believe increased hunting from early modern humans also played a part.
De-extinction.
De-extinction is the process of creating an organism, which is a member of or resembles an extinct species, or a breeding population of such organisms. Cloning is the most widely proposed method, although selective breeding has also been proposed.
Several species that have gone extinct during the holocene period have been proposed for de-extinction. These include: passenger pigeon, moa, heath hen, dodo and woolly mammoth.

</doc>
<doc id="14209" url="http://en.wikipedia.org/wiki?curid=14209" title="Hollywood-style Lindy Hop">
Hollywood-style Lindy Hop

Hollywood-style Lindy Hop is a variety of Lindy Hop, an American vernacular dance. It is also sometimes referred to as Dean Collins or Smooth-style, but these terms also sometimes refer to different styles of Lindy Hop.
Hollywood is the style reconstructed by Erik Robison and Sylvia Skylar based on movies from 1930s and 1940s featuring dancers like Dean Collins, Jewel McGowan, Jean Veloz and others.. They were the first to call it "Hollywood Style".
The swingout (the basic step of Lindy) is danced in a position often described as someone about to sit on a stool, thereby bringing their center point of balance closer to the ground. This piked position is the classic look of Hollywood with the back straight and a slight forward tilt. The Hollywood style is also a slotted dance, meaning the follower travels in a straight line instead of the more elliptical or circular Savoy-style Lindy Hop.
A popular variation of Hollywood-Style Lindy Hop called LA-style Lindy Hop has a few technical changes in the footwork and fewer steps. The steps are shortened or "cheated" to create this look. The style is geared towards performance and is heavily based on short choreographies. Originating in Los Angeles, California, LA-style is a favorite on the West Coast of the United States.

</doc>
<doc id="14210" url="http://en.wikipedia.org/wiki?curid=14210" title="Harrison Narcotics Tax Act">
Harrison Narcotics Tax Act

The Harrison Narcotics Tax Act (Ch. 1, 38 Stat. ) was a United States federal law that regulated and taxed the production, importation, and distribution of opiates and coca products. The act was proposed by Representative Francis Burton Harrison of New York and was approved on December 17, 1914.
"An Act To provide for the registration of, with collectors of internal revenue, and to impose a special tax on all persons who produce, import, manufacture, compound, deal in, dispense, sell, distribute, or give away opium or coca leaves, their salts, derivatives, or preparations, and for other purposes." The courts interpreted this to mean that physicians could prescribe narcotics to patients in the course of normal treatment, but not for the treatment of addiction.
Although technically illegal for purposes of distribution and use, the distribution, sale and use of cocaine was still legal for registered companies and individuals.
History.
International background.
Following the Spanish–American War the U.S. acquired the Philippines from Spain. At that time, opium addiction constituted a significant problem in the civilian population of the Philippines.
Charles Henry Brent was an American Episcopal bishop who served as Missionary Bishop of the Philippines beginning in 1901. He convened a Commission of Inquiry, known as the Brent Commission, for the purpose of examining alternatives to a licensing system for opium addicts. The Commission recommended that narcotics should be subject to international control. The recommendations of the Brent Commission were endorsed by the United States Department of State and in 1906 President Theodore Roosevelt called for an international conference, the International Opium Commission, which was held in Shanghai in February 1909. A second conference was held at The Hague in May 1911, and out of it came the first international drug control treaty, the International Opium Convention of 1912.
Domestic Background.
In the 1800s opiates and cocaine were mostly unregulated drugs. In the 1890s the Sears & Roebuck catalogue, which was distributed to millions of Americans homes, offered a syringe and a small amount of cocaine for $1.50.
At the beginning of the 20th century, cocaine began to be linked to crime. In 1900, the "Journal of the American Medical Association" published an editorial stating, "Negroes in the South are reported as being addicted to a new form of vice – that of 'cocaine sniffing' or the 'coke habit.'" Some newspapers later claimed cocaine use caused blacks to rape white women and was improving their pistol marksmanship. Chinese immigrants were blamed for importing the opium-smoking habit to the U.S. The 1903 blue-ribbon citizens' panel, the Committee on the Acquirement of the Drug Habit, concluded, "If the Chinaman cannot get along without his dope we can get along without him."
Theodore Roosevelt appointed Dr. Hamilton Wright as the first Opium Commissioner of the United States in 1908. In 1909, Wright attended the International Opium Commission in Shanghai as the American delegates. He was accompanied by Charles Henry Brent, the Episcopal Bishop. On March 12, 1911, Dr. Wright was quoted in as follows in an article in the New York Times: "Of all the nations of the world, the United States consumes most habit-forming drugs per capita. Opium, the most pernicious drug known to humanity, is surrounded, in this country, with far fewer safeguards than any other nation in Europe fences it with." Wright further claimed that "it has been authoritatively stated that cocaine is often the direct incentive to the crime of rape by the negroes of the South and other sections of the country," though he failed to mention specifically "which" authorities had stated that, and did not provide any evidence for his claim. Wright also stated that "one of the most unfortunate phases of smoking opium in this country is the large number of women who have become involved and were living as common-law wives or cohabitating with Chinese in the Chinatowns of our various cities".
Opium usage had begun to decline by 1914 after rising dramatically in the post Civil War Era, peaking at around one-half million pounds per year in 1896. Demand gradually declined thereafter in response to mounting public concern, local and state regulations, and the Pure Food and Drugs Act of 1906, which required labeling of patent medicines that contained opiates, cocaine, alcohol, cannabis and other intoxicants. As of 1911, an estimated one U.S. citizen in 400 (0.25%) was addicted to some form of opium. The opium addicts were mostly women who were prescribed and dispensed legal opiates by physicians and pharmacist for ”female problems,” (Probably pain at menstruation.), or white men and Chinese at the Opium dens. Between two-thirds and three-quarters of these addicts were women. By 1914, forty-six states had regulations on cocaine and twenty-nine states had laws against opium, morphine, and heroin.
Several authors have argued that the debate was merely to regulate trade and collect a tax. However, the committee report prior to the debate on the house floor and the debate itself, discussed the rise of opiate use in the United States. Harrison stated that "The purpose of this Bill can hardly be said to raise revenue, because it prohibits the importation of something upon which we have hitherto collected revenue." Later Harrison stated, "We are not attempting to collect revenue, but regulate commerce." House representative Thomas Sisson stated, "The purpose of this bill—and we are all in sympathy with it—is to prevent the use of opium in the United States, destructive as it is to human happiness and human life."
The drafters played on fears of “drug-crazed, sex-mad negroes” and made references to Negroes under the influence of drugs murdering whites, degenerate Mexicans smoking marijuana, and “Chinamen” seducing white women with drugs. Dr. Hamilton Wright, testified at a hearing for the Harrison Act. Wright alleged that drugs made blacks uncontrollable, gave them superhuman powers and caused them to rebel against white authority. Dr. Christopher Koch of the State Pharmacy Board of Pennsylvania testified that "Most of the attacks upon the white women of the South are the direct result of a cocaine-crazed Negro brain".
Before the Act was passed, on February 8, 1914, The "New York Times" published an article entitled "Negro Cocaine 'Fiends' Are New Southern Menace: Murder and Insanity Increasing Among Lower-Class Blacks" by Edward Huntington Williams, which reported that Southern sheriffs had increased the caliber of their weapons from .32 to .38 to bring down Negroes under the effect of cocaine.
Despite the extreme racialization of the issue that took place in the buildup to the Act's passage, the contemporary research on the subject indicated that black Americans were in fact using cocaine and opium at much "lower" rates than white Americans.
Effect.
The act appears to be concerned about the marketing of opiates. However a clause applying to doctors allowed distribution "in the course of his professional practice only." This clause was interpreted after 1917 to mean that a doctor could not prescribe opiates to an addict, since addiction was not considered a disease. A number of doctors were arrested and some were imprisoned. The medical profession quickly learned not to supply opiates to addicts. In "United States v. Doremus", 249 U.S. 86 (1919), the Supreme Court ruled that the Harrison Act was constitutional, and in "Webb v. United States", 249 U.S. 96, 99 (1919) that physicians could not prescribe narcotics solely for maintenance.
The impact of diminished supply was obvious by mid-1915. A 1918 commission called for sterner law enforcement, while newspapers published sensational articles about addiction-related crime waves. Congress responded by tightening up the Harrison Act—the importation of heroin for any purpose was banned in 1924.
After other complementary laws (for example implementing the Uniform State Narcotic Act in 1932), and other actions by the government the number of addicts of opium started to decrease fast from 1925 to a level that in 1945 that was about one tenth of the level in 1914.
The use of the term 'narcotics' in the title of the act to describe not just opiates but also cocaine—which is a central nervous system stimulant, not a narcotic—initiated a precedent of frequent legislative and judicial misclassification of various substances as 'narcotics'. Today, law enforcement agencies, popular media, the United Nations, other nations and even some medical practitioners can be observed applying the term very broadly and often pejoratively in reference to a wide range of illicit substances, regardless of the more precise definition existing in medical contexts. For this reason, however, 'narcotic' has come to mean any illegally used drug, but it is useful as a shorthand for referring to a controlled drug in a context where its legal status is more important than its physiological effects.
The remaining effect of this act, which has largely been superseded by the Controlled Substances Act of 1970, is the warning "*Warning: May be habit forming" on labels, package inserts, and other places where ingredients are listed in the case of many opioids, barbiturates, medicinal formulations of cocaine, and chloral hydrate.
The act also marks the beginning of the creation of the modern, criminal drug addict and the American black market for drugs. Within five years the Rainey Committee, a Special Committee on Investigation appointed by Secretary of the Treasury William Gibbs McAdoo and led by Congressman T. Rainey, reported in June, 1919 that drugs were being smuggled into the country by sea, and across the Mexican and Canadian borders by nationally established organisations and that the United States consumed 470,000 pounds of opium annually, compared to 17,000 pounds in both France and Germany. The Monthly Summary of Foreign Commerce of the United States recorded that in the 7 months to January 1920, 528,635 pounds of opium was imported, compared to 74,650 pounds in the same period in 1919.
Challenge.
The Act's applicability in prosecuting doctors who prescribe narcotics to addicts was successfully challenged in "Linder v. United States" in 1925, as Justice McReynolds ruled that the federal government has no power to regulate medical practice.

</doc>
<doc id="14215" url="http://en.wikipedia.org/wiki?curid=14215" title="Horse tack">
Horse tack

Tack is a piece of equipment or accessory equipped on horses in the course of their use as domesticated animals. Saddles, stirrups, bridles, halters, reins, bits, harnesses, martingales, and breastplates are all forms of horse tack. Equipping a horse is often referred to as tacking up. A room to store such equipment, usually near or in a stable, is a tack room.
Saddles.
Saddles are seats for the rider, fastened to the horse's back by means of a "girth" (English-style riding), known as a "cinch" in the Western US, a wide strap that goes around the horse at a point about four inches behind the forelegs. Some western saddles will also have a second strap known as a "flank" or "back cinch" that fastens at the rear of the saddle and goes around the widest part of the horse's belly.
It is important that the saddle be comfortable for both the rider and the horse as an improperly fitting saddle may create pressure points on the horse's back muscle (Latissimus dorsi) and cause the horse pain and can lead to the horse, rider, or both getting injured. 
There are many types of saddle, each specially designed for its given task.
Saddles are usually divided into two major categories: "English saddles" and "Western saddles" according to the riding discipline they are used in. Other types of saddles, such as racing saddles, Australian saddles, sidesaddles and endurance saddles do not necessarily fit neatly in either category.
Stirrups.
Stirrups are supports for the rider's feet that hang down on either side of the saddle. They provide greater stability for the rider but can have safety concerns due to the potential for a rider's feet to get stuck in them. If a rider is thrown from a horse but has a foot caught in the stirrup, they could be dragged if the horse runs away. To minimize this risk, a number of safety precautions are taken. First, most riders wear riding boots with a heel and a smooth sole. Next, some saddles, particularly English saddles, have safety bars that allow a stirrup leather to fall off the saddle if pulled backwards by a falling rider. Other precautions are done with stirrup design itself. Western saddles have wide stirrup treads that make it more difficult for the foot to become trapped. A number of saddle styles incorporate a tapedero, which is covering over the front of the stirrup that keeps the foot from sliding all the way through the stirrup. The English stirrup (or "iron") has several design variations which are either shaped to allow the rider's foot to slip out easily or are closed with a very heavy rubber band. The invention of stirrups was of great historic significance in mounted combat, giving the rider secure foot support while on horseback.
Headgear.
"Bridles", hackamores, "halters" or "headcollars", and similar equipment consist of various arrangements of straps around the horse's head, and are used for control and communication with the animal.
Halters.
A "halter" (US) or "headcollar" (BI) (occasionally "headstall") consists of a noseband and headstall that buckles around the horse's head and allows the horse to be led or tied. The lead rope is separate, and it may be short (from six to ten feet, two to three meters) for everyday leading and tying, or much longer (up to 25 ft, eight meters) for tasks such as for leading packhorses or for picketing a horse out to graze. 
Some horses, particularly stallions, may have a chain attached to the lead rope and placed over the nose or under the jaw to increase the control provided by a halter while being led. Most of the time, horses are not ridden with a halter, as it offers insufficient precision and control. Halters have no bit. 
In Australian and British English, a "halter" is a rope with a spliced running loop around the nose and another over the poll, used mainly for unbroken horses or for cattle. The lead rope cannot be removed from the halter. A show halter is made from rolled leather and the lead attaches to form the chinpiece of the noseband. These halters are not suitable for paddock usage or in loose stalls. An "underhalter" is a lightweight halter or headcollar which is made with only one small buckle, and can be worn under a bridle for tethering a horse without untacking.
Bridles.
Bridles usually have a "bit" attached to "reins" and are used for riding and driving horses.
"English Bridles" have a "cavesson" style noseband and are seen in English riding. Their reins are buckled to one another, and they have little adornment or flashy hardware.
"Western Bridles" used in Western riding usually have no noseband, are made of thin bridle leather. They may have long, separated "Split" reins or shorter closed reins, which sometimes include an attached "Romal". Western bridles are often adorned with silver or other decorative features.
"Double bridles" are a type of English bridle that use two bits in the mouth at once, a snaffle and a curb. The two bits allow the rider to have very precise control of the horse. As a rule, only very advanced horses and riders use double bridles. Double bridles are usually seen in the top levels of dressage, but also are seen in certain types of show hack and Saddle seat competition.
Hackamores and other bitless designs.
A "hackamore" is a headgear that utilizes a heavy noseband of some sort, rather than a bit, most often used to train young horses or to go easy on an older horse's mouth. Hackamores are more often seen in western riding. Some related styles of headgear that control a horse with a noseband rather than a bit are known as bitless bridles.
The word "hackamore" is derived from the Spanish word "jáquima." Hackamores are seen in western riding disciplines, as well as in endurance riding and English riding disciplines such as show jumping and the stadium phase of eventing. While the classic bosal-style hackamore is usually used to start young horses, other designs, such as various bitless bridles and the mechanical hackamore are often seen on mature horses with dental issues that make bit use painful, horses with certain training problems, and on horses with mouth or tongue injuries. Some riders also like to use them in the winter to avoid putting a frozen metal bit into a horse's mouth.
Like bitted bridles, noseband-based designs can be gentle or harsh, depending on the hands of the rider. It is a myth that a bit is cruel and a hackamore is gentler. The horse's face is very soft and sensitive with many nerve endings. Misuse of a hackamore can cause swelling on the nose, scraping on the nose and jawbone, and extreme misuse may cause damage to the bones and cartilage of the horse's head.
Other headgear.
A "longeing cavesson" (UK: "lungeing") is a special type of halter or noseband used for longeing a horse. Longeing is the activity of having a horse walk, trot and/or canter in a large circle around the handler at the end of a rope that is 25 to 30 ft long. It is used for training and exercise.
Reins.
Reins consist of leather straps or rope attached to the outer ends of a "bit" and extend to the rider's or driver's hands. Reins are the means by which a horse rider or driver communicates directional commands to the horse's head. Pulling on the reins can be used to steer or stop the horse. The sides of a horse's mouth are sensitive, so pulling on the reins pulls the bit, which then pulls the horse's head from side to side, which is how the horse is controlled.
On some types of harnesses there might be supporting rings to carry the reins over the horse's back. When pairs of horses are used in drawing a wagon or coach it is usual for the outer side of each pair to be connected to reins and the inside of the bits connected by a short bridging strap or rope. The driver carries "four-in-hand" or "six-in-hand" being the number of reins connecting to the pairs of horses.
A rein may be attached to a halter to lead or guide the horse in a circle for training purposes or to lead a packhorse, but a simple lead rope is more often used for these purposes. A longe line is sometimes called a "longe rein," but it is actually a flat line about 30 ft long, usually made of nylon or cotton web, about one inch wide, thus longer and wider than even a driving rein.
Horses should never be tied by the reins. Not only do they break easily, but, being attached to a bit in the horse's sensitive mouth, a great deal of pain can be inflicted if a bridled horse sets back against being tied.
Bits.
A bit is a device placed in a horse's mouth, kept on a horse's head by means of a headstall. There are many types, each useful for specific types of riding and training.
The mouthpiece of the bit does not rest on the teeth of the horse, but rather rests on the gums or "bars" of the horse's mouth in an interdental space behind the front incisors and in front of the back molars. It is important that the style of bit is appropriate to the horse's needs and is fitted properly for it to function properly and be as comfortable as possible for the horse.
The basic "classic" styles of bits are:
While there are literally hundreds of types of bit mouthpieces, bit rings and bit shanks, essentially there are really only two broad categories: direct pressure bits, broadly termed snaffle bits; and leverage bits, usually termed curbs. 
Bits that act with direct pressure on the tongue and lips of the bit are in the general category of "snaffle" bits. Snaffle bits commonly have a single jointed mouthpiece and act with a nutcracker effect on the bars, tongue and occasionally roof of the mouth. However, regardless of mouthpiece, any bit that operates only on direct pressure is a "snaffle" bit.
Leverage bits have shanks coming off the mouthpiece to create leverage that applies pressure to the poll, chin groove and mouth of the horse are in the category of "curb" bits. Any bit with shanks that works off of leverage is a "curb" bit, regardless of whether the mouthpiece is solid or jointed. 
Some combination or hybrid bits combine direct pressure and leverage, such as the Kimblewick or Kimberwicke, which adds slight leverage to a two-rein design that resembles a snaffle; and the four rein designs such as the single mouthpiece Pelham bit and the double bridle, which places a curb and a snaffle bit simultaneously in the horse's mouth.
In the wrong hands even the mildest bit can hurt the horse. Conversely, a very severe bit, in the right hands, can transmit subtle commands that cause no pain to the horse. Bit commands should be given with only the quietest movements of the hands, and much steering and stopping should be done with the legs and seat.
Harness.
A horse harness is a set of devices and straps that attaches a horse to a cart, carriage, sledge or any other load. There are two main styles of harnesses - breaststrap and collar and hames style. These differ in how the weight of the load is attached. Most Harnesses are made from leather, which is the traditional material for harnesses, though some designs are now made of nylon webbing or synthetic biothane.
A breaststrap harness has a wide leather strap going horizontally across the horses' breast, attached to the traces and then to the load. This is used only for lighter loads. A collar and hames harness has a collar around the horses' neck with wood or metal hames in the collar. The traces attach from the hames to the load. This type of harness is needed for heavy draft work. 
Both types will also have a bridle and reins. A harness that is used to support shafts, such as on a cart pulled by a single horse, will also have a "saddle" attached to the harness to help the horse support the shafts and "breeching" to brake the forward motion of the vehicle, especially when stopping or moving downhill. Horses guiding vehicles by means of a pole, such as two-horse teams pulling a wagon, a hay-mower, or a dray, will have "pole-straps" attached to the lower part of the horse collar.
Breastplates and martingales.
Breastplates, breastcollars or breastgirths attach to the front of the saddle, cross the horse's chest, and usually have a strap that runs between the horse's front legs and attaches to the girth. They keep the saddle from sliding back or sideways. They are usually seen in demanding, fast-paced sports. They are crucial pieces of safety equipment for English riding activities requiring jumping, such as eventing, show jumping, polo, and fox hunting. They are also seen in Western riding events, particularly in rodeo, reining and cutting, where it is particularly important to prevent a saddle from shifting. They may also be worn in other horse show classes for decorative purposes.
A martingale is a piece of equipment that keeps a horse from raising its head too high. Various styles can be used as a control measure, to prevent the horse from avoiding rider commands by raising its head out of position; or as a safety measure to keep the horse from tossing its head high or hard enough to smack its rider in the face.
They are allowed in many types of competition, especially those where speed or jumping may be required, but are not allowed in most "flat" classes at horse shows, though an exception is made in a few classes limited exclusively to young or "green" horses who may not yet be fully trained.
Martingales are usually attached to the horse one of two ways. They are either attached to the center chest ring of a breastplate or, if no breastplate is worn, they are attached by two straps, one that goes around the horse's neck, and the other that attaches to the girth, with the martingale itself beginning at the point in the center of the chest where the neck and girth straps intersect.
Martingale types include:
There are other training devices that fall loosely in the martingale category, in that they use straps attached to the reins or bit which limit the movement of the horse's head or add leverage to the rider's hands in order to control the horse's head. Common devices of this nature include the overcheck, the chambon, de Gogue, grazing reins, draw reins and the "bitting harness" or "bitting rig". However, most of this equipment is used for training purposes and is not legal in any competition. In some disciplines, use of leverage devices, even in training, is controversial.

</doc>
<doc id="14216" url="http://en.wikipedia.org/wiki?curid=14216" title="Hausa language">
Hausa language

Hausa () ("Yaren Hausa" or "Harshen Hausa") is the Chadic language (a branch of the Afro-Asiatic language family) with the largest number of speakers, spoken as a first language by about 35 million people, and as a second language by 15 million in Nigeria, and millions more in other countries, for a total of at least 50 million speakers.
Classification.
Hausa belongs to the West Chadic languages subgroup of the Chadic languages group, which in turn is part of the Afro-Asiatic language family.
Geographic distribution.
Native speakers of Hausa, the Hausa people, are mostly to be found in Niger, in the north of Nigeria and Chad. Furthermore, the language is used as a trade language across a much larger swathe of West Africa (Benin, Ghana, Cameroon, Togo, Côte d'Ivoire etc.), Central Africa (Chad, Central African Republic, Equatorial Guinea) and northwestern Sudan, particularly amongst Muslims.
It is taught at universities in Africa and around the world. The language is the most commonly spoken language in Nigeria, but unlike Yoruba and Igbo, it is also widely spoken outside Nigeria, especially in Niger, Ghana, Cameroon and Sudan.
Radio stations like BBC, Radio France Internationale, China Radio International, Voice of Russia, Voice of America, Deutsche Welle, and IRIB broadcast in Hausa.
Dialects.
Traditional dialects.
Eastern Hausa dialects include "Kananci" which is spoken in Kano, "Bausanchi" in Bauchi, "Dauranchi" in Daura, "Gudduranci" in Katagum Misau and part of Borno, "Kutebanci" in Taraba, and "Hadejanci" in Hadejiya.
Western Hausa dialects include "Sakkwatanci" in Sokoto, "Katsinanci" in Katsina, "Arewanci" in Gobir, Adar, Kebbi, and Zamfara, and "Kurhwayanci" in Kurfey in Niger. Katsina is transitional between Eastern and Western dialects.
Northern Hausa dialects include "Arewa" and "Arawci".
"Zazzaganci" in Zaria is the major Southern dialect.
The Kano dialect ("Kananci") is the standard. The BBC, Deutsche Welle, Radio France Internationale and Voice of America offer Hausa services on their international news web sites using Kananci.
Northernmost dialects and loss of tonality.
The western to eastern Hausa dialects of "Kurhwayanci", "Daragaram" and "Aderawa", represent the traditional northernmost limit of native Hausa communities. These are spoken in the northernmost sahel and mid-Saharan regions in west and central Niger in the Tillaberi, Tahoua, Dosso, Maradi, Agadez and Zinder regions. While mutually comprehensible with other dialects (especially "Sakkwatanci", and to a lesser extent "Gaananci"), the northernmost dialects have slight grammatical and lexical differences owing to frequent contact with the Zarma and Tuareg groups and cultural changes owing to the geographical differences between the grassland and desert zones. These dialects also have the quality of being non-tonal or pitch accent dialects.
This link between non-tonality and geographic location is not limited to Hausa alone, but is exhibited in other northern dialects of neighbouring languages; such as the difference within Songhay language (between the non-tonal northernmost dialects of Koyra Chiini in Timbuktu and Koyraboro Senni in Gao; and the tonal southern Zarma dialect, spoken from western Niger to northern Ghana), and within the Soninke language (between the non-tonal northernmost dialects of Imraguen and Nemadi spoken in east-central Mauritania; and the tonal southern dialects of Senegal, Mali and the sahel).
Ghanaian Hausa dialect.
The Ghanaian Hausa dialect ("Gaananci"), spoken in Ghana, Togo, and western Ivory Coast, is a distinct western native Hausa dialect-bloc with adequate linguistic and media resources available. Separate smaller Hausa dialects are spoken by an unknown number of Hausa further west in parts of Burkina Faso, and in the Haoussa Foulane, Badji Haoussa, Guezou Haoussa, and Ansongo districts of northeastern Mali (where it is designated as a minority language by the Malian government), but there are very little linguistic resources and research done on these particular dialects at this time.
Gaananci forms a separate group from other Western Hausa dialects, as it now falls outside the contiguous Hausa-dominant area, and is usually identified by the use of "c" for "ky", and "j" for "gy". This is attributed to the fact that Ghana's Hausa population descend from Hausa-Fulani traders settled in the zongo districts of major trade-towns up and down the previous Asante, Gonja and Dagomba kingdoms stretching from the sahel to coastal regions, in particular the cities of Tamale, Salaga, Bawku, Bolgatanga, Achimota, Nima and Kumasi.
Gaananci exhibits noted inflected influences from Zarma, Gur, Dyula and Soninke, as Ghana is the westernmost area in which the Hausa language is a major lingua-franca; as well as it being the westernmost area both the Hausa and Djerma ethnic groups inhabit in large numbers. Immediately west from Ghana (in Ivory Coast, Togo, and Burkina Faso), Hausa is abruptly replaced with Dioula–Bambara as the main lingua-franca of what become predominantly Mandinka areas, and native Hausa populations plummet to a very small urban minority.
Because of this, and the presence of surrounding Akan, Gur and Mande languages, Gaananci was historically isolated from the other Hausa dialects. Despite this difference, grammatical similarities between "Sakkwatanci" and Ghanaian Hausa determine that the dialect, and the origin of the Ghanaian Hausa people themselves, are derived from the northwestern Hausa area surrounding Sokoto.
Hausa is also widely spoken by non-native Gur and Mande Ghanaian Muslims, but differs from Gaananci, and rather has features consistent with non-native Hausa dialects.
Other native dialects.
Hausa is also spoken various parts of Cameroon and Chad, which combined the mixed dialects of northern Nigeria and Niger. In addition, Arabic has had a great influence in the way Hausa is spoken by the native Hausa speakers in these areas.
Non-native Hausa.
In West Africa, Hausa's use as a lingua franca has given rise to non-native pronunciation vastly that differs from native pronunciation by way of key omissions of implosive and ejective consonants present in native Hausa dialects, such as "ɗ", "ɓ" and "kʼ/ƙ", which are pronounced by non-native speakers as "d", "b" and "k" respectively. This creates confusion among non-native and native Hausa speakers, as non-native pronunciation does not distinguish words like "daidai" ("correct") and "ɗaiɗai" ("one-by-one"). Another difference between native and non-native Hausa is the omission of vowel length in words and change in the standard tone of native Hausa dialects (ranging from native Fulani and Tuareg Hausa-speakers omitting tone altogether, to Hausa speakers with Gur or Yoruba mother tongues using additional tonal structures similar to those used in their native languages). Use of masculine and feminine gender nouns and sentence structure are usually omitted or interchanged, and many native Hausa nouns and verbs are substituted with non-native terms from local languages.
Non-native speakers of Hausa numbered more than 25 million and, in some areas, live close to native Hausa. It has replaced many other languages especially in the north-central and north-eastern part of Nigeria and continues to gain popularity in other parts of Africa as a result of Hausa movies and musics which spread out throughout the region.
There are several pidgin forms of Hausa. Barikanchi was formerly used in the colonial army of Nigeria. Gibanawa is currently in widespread use in Jega in northwestern Nigeria, south of the native Hausa area.
Phonology.
Consonants.
Hausa has between 23 and 25 consonant phonemes depending on the speaker.
The three-way contrast between palatalized velars /c ɟ cʼ/, plain velars /k ɡ kʼ/, and labialized velars /kʷ ɡʷ kʷʼ/ is found only before long and short /a/, e.g. /cʼaːɽa/ ('grass'), /kʼaːɽaː/ ('to increase'), /kʷʼaːɽaː/ ('shea-nuts'). Before front vowels, only palatalized and labialized velars occur, e.g. /ciːʃiː/ ('jealousy') vs. /kʷiːɓiː/ ('side of body'). Before rounded vowels, only labialized velars occur, e.g. /kʷoːɽaː/ ('ringworm').
Glottalic consonants.
Hausa has glottalic consonants (implosives and ejectives) at four or five places of articulation (depending on the dialect). They require movement of the glottis during pronunciation and have a staccato sound.
They are written with modified versions of Latin letters. They can also be denoted with an apostrophe, either before or after depending on the letter, as shown below.
b' / ɓ, an implosive consonant, [ɓ], sometimes [ʔb];
d' / ɗ, an implosive [ɗ], sometimes [dʔ];
ts', an ejective consonant, [tsʼ] or [sʼ], according to the dialect;
ch', an ejective [tʃʼ] (does not occur in Kano dialect)
k' / ƙ, an ejective [kʼ]; [kʲʼ] and [kʷʼ] are separate consonants;
'y is a palatalized glottal stop, [ʔʲ], found in only a small number of high-frequency words (e.g. /ʔʲáːʔʲáː/ "children"). Historically it developed from palatalized [ɗ].
Vowels.
Hausa has 5 phonetic vowel sounds, which can be either short or long, giving a total of 10 monophthongs. In addition, there are 4 joint vowels (diphthongs), giving a total number of 14 vowel phonemes.
Tones.
Hausa is a tonal language. Each of its five vowels may have low tone, high tone or falling tone. In standard written Hausa, tone is not marked. In recent linguistic and pedagogical materials, tone is marked by means of diacritics.
An acute accent (´) may be used for high tone, but the usual practice is to leave high tone unmarked.
Writing systems.
"Boko" (Latin).
Hausa's modern official orthography is a Latin-based alphabet called "boko", which was imposed in the 1930s by the British colonial administration.
The letter "ƴ" (y with a right hook) is used only in Niger; in Nigeria it is written "ʼy".
Tone, vowel length, and the distinction between /r/ and /ɽ/ (which does not exist for all speakers) are not marked in writing. So, for example, /daɡa/ "from" and /daːɡaː/ "battle" are both written "daga".
"Ajami" (Arabic).
Hausa has also been written in "ajami", an Arabic alphabet, since the early 17th century. There is no standard system of using "ajami", and different writers may use letters with different values. Short vowels are written regularly with the help of vowel marks, which are seldom used in Arabic texts other than the Quran. Many medieval Hausa manuscripts in "ajami", similar to the Timbuktu Manuscripts, have been discovered recently; some of them even describe constellations and calendars.
In the following table, vowels are shown with the Arabic letter for "t" (ت) as an example.
Other systems.
Hausa is one of three indigenous languages of Nigeria which has been rendered in braille.
At least three other writing systems for Hausa have been proposed or "discovered." None of these are in active use beyond perhaps some individuals.

</doc>
<doc id="14220" url="http://en.wikipedia.org/wiki?curid=14220" title="History of mathematics">
History of mathematics

The area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past.
Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. The most ancient mathematical texts available are "Plimpton 322" (Babylonian mathematics c. 1900 BC), the "Rhind Mathematical Papyrus" (Egyptian mathematics c. 2000-1800 BC) and the "Moscow Mathematical Papyrus" (Egyptian mathematics c. 1890 BC). All of these texts concern the so-called Pythagorean theorem, which seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a subject in its own right begins in the 6th century BC with the Pythagoreans, who coined the term "mathematics" from the ancient Greek "μάθημα" ("mathema"), meaning "subject of instruction". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Chinese mathematics made early contributions, including a place value system. The Hindu-Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and were transmitted to the west via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Many Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe.
From ancient times through the Middle Ages, bursts of mathematical creativity were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day.
Prehistoric mathematics.
The origins of mathematical thought lie in the concepts of number, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the "number" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between "one", "two", and "many", but not of numbers larger than two.
Prehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time.
Most evidence is against the Lebombo bone (ca. 43,500 yr BC) being a mathematical object, but the Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of tally marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. In the book "How Mathematics Happened: The First 50,000 Years", Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that "no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10." The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this, however, is disputed.
Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design.
All of the above are disputed however, and the currently oldest undisputed mathematical usage is in Babylonian and dynastic Egyptian sources.
Babylonian mathematics.
Babylonian mathematics refers to any mathematics of the people of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.
In contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.
The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.
The majority of recovered clay tablets date from 1800 to 1600 BC, and cover topics which include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear and quadratic equations. The Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places.
Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. Babylonian advances in mathematics were facilitated by the fact that 60 has many divisors. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. They lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. On the other hand, this "defect" is equivalent to the modern-day usage of floating point arithmetic; moreover, the use of base 60 means that any reciprocal of an integer which is a multiple of divisors of 60 necessarily has a finite expansion to the base 60. (In decimal arithmetic, only reciprocals of multiples of 2 and 5 have finite decimal expansions.) Accordingly, there is a strong argument that arithmetic Old Babylonian style is considerably more sophisticated than that of current usage.
The interpretation of Plimpton 322 was the source of controversy for many years after its significance in the context of Pythagorean triangles was realized. In historical context, inheritance problems involving equal-area subdivision of triangular and trapezoidal fields (with integer length sides) quickly convert into the need to calculate the square root of 2, or to solve the "Pythagorean equation" in integers.
Rather than considering a square as the sum of two squares, we can equivalently consider a square as a difference of two squares. Let a, b and c be integers that form a Pythagorean Triple: a^2 + b^2 = c^2. Then c^2 - a^2 = b^2, and using the expansion for the difference of two squares we get (c-a)(c+a)= b^2.
Dividing by b^2, it becomes the product of two rational numbers giving 1: (c/b - a/b)(c/b + a/b) = 1. We require two rational numbers which are reciprocals and which differ by 2(a/b). This is easily solved by consulting a table of reciprocal pairs. E.g., (1/2) (2) = 1 is a pair of reciprocals which differ by 3/2 = 2(a/b) Thus a/b = 3/4, giving a=3, b=4 and so c=5.
Solutions of the original equation are thus constructed by choosing a rational number x, from which Pythagorean-triples are 2x, x^2-1, x^2+1. Other triples are made by scaling these by an integer (the scaling integer being half the difference between the largest and one other side). All Pythagorean triples arise in this way, and the examples provided in Plimpton 322 involve some quite large numbers, by modern standards, such as (4601, 4800, 6649) in decimal notation.
Egyptian mathematics.
Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.
The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000-1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.
Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called "word problems" or "story problems", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum: "If you are told: A truncated pyramid of 6 for the vertical height by 4 on the base by 2 on the top. You are to square this 4, result 16. You are to double 4, result 8. You are to square 2, result 4. You are to add the 16, the 8, and the 4, result 28. You are to take one third of 6, result 2. You are to take 28 twice, result 56. See, it is 56. You will find it right."
Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.
Greek mathematics.
Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.
Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.
Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.
Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers.
Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as "breadthless length"), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.
Eudoxus (408–c.355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384—c.322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.
In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the "Elements", widely considered the most successful and influential textbook of all time. The "Elements" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the "Elements" were already known, Euclid arranged them into a single, coherent logical framework. The "Elements" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the "Elements" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.
Archimedes (c.287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 310⁄71 < π < 310⁄70. He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious system for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.
Apollonius of Perga (c. 262-190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola ("place beside" or "comparison"), "ellipse" ("deficiency"), and "hyperbola" ("a throw beyond"). His work "Conics" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.
Around the same time, Eratosthenes of Cyrene (c. 276-194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the "Golden Age" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190-120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the "Almagest" of Ptolemy (c. AD 90-168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.
Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the "Silver Age" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as "Diophantine analysis". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the "Arithmetica", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The "Arithmetica" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the "Arithmetica" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the "Arithmetica" being the first instance of algebraic symbolism and syncopation.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).
Chinese mathematics.
Early Chinese mathematics is so different from that of other parts of the world that it is reasonable to assume independent development. The oldest extant mathematical text from China is the "Chou Pei Suan Ching", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC appears reasonable.
Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called "rod numerals" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for "1", followed by the symbol for "100", then the symbol for "2" followed by the symbol for "10", followed by the symbol for "3". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the "suan pan", or Chinese abacus. The date of the invention of the "suan pan" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's "Supplementary Notes on the Art of Figures".
The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The "Mo Jing" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well.
In 212 BC, the Emperor Qin Shi Huang (Shi Huang-ti) commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is "The Nine Chapters on the Mathematical Art", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles and values of π. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. Liu Hui commented on the work in the 3rd century AD, and gave a value of π accurate to 5 decimal places. Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places, which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.
The high-water mark of Chinese mathematics occurs in the 13th century (latter part of the Sung period), with the development of Chinese algebra. The most important text from that period is the "Precious Mirror of the Four Elements" by Chu Shih-chieh (fl. 1280-1303), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The "Precious Mirror" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).
Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.
Indian mathematics.
The earliest civilization on the Indian subcontinent is the Indus Valley Civilization that flourished between 2600 and 1900 BC in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.
The Hindu-Arabic numerals were invented by mathematicians in India. They were called "Hindu numerals". They were later called "Arabic" numerals by Europeans, because they were introduced in the West by Arab merchants.
Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs (as one will note when perusing Unicode character charts). This table shows two examples:
The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.
Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd-1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called "mātrāmeru").
The next significant mathematical documents from India after the "Sulba Sutras" are the "Siddhantas", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words "sine" and "cosine" derive from the Sanskrit "jiya" and "kojiya".
In the 5th century AD, Aryabhata wrote the "Aryabhatiya", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the "Aryabhatiya" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the "Aryabhatiya" as a "mix of common pebbles and costly crystals".
In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in "Brahma-sphuta-siddhanta", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu-Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.
In the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.
In the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series, and, using 21 terms, computed the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the "Yukti-bhāṣā". However, the Kerala School did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.
Islamic mathematics.
The Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.
In the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu-Arabic numerals and on methods for solving equations. His book "On the Calculation with Hindu Numerals", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word "algorithm" is derived from the Latinization of his name, Algoritmi, and the word "algebra" from the title of one of his works, "Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala" ("The Compendious Book on Calculation by Completion and Balancing"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of "reduction" and "balancing", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as "al-jabr". His algebra was also no longer concerned "with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study." He also studied an equation for its own sake and "in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems."
In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.
Further developments in algebra were made by Al-Karaji in his treatise "al-Fakhri", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being "the first who introduced the theory of algebraic calculus." Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.
In the late 11th century, Omar Khayyam wrote "Discussions of the Difficulties in Euclid", a book about what he perceived as flaws in Euclid's "Elements", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.
In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating "n"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.
Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.
During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.
Medieval European mathematics.
Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's "Timaeus" and the biblical passage (in the "Book of Wisdom") that God had "ordered all things in measure, and number, and weight".
Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term "quadrivium" to describe the study of arithmetic, geometry, astronomy, and music. He wrote "De institutione arithmetica", a free translation from the Greek of Nicomachus's "Introduction to Arithmetic"; "De institutione musica", also derived from Greek sources; and a series of excerpts from Euclid's "Elements". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.
In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's "The Compendious Book on Calculation by Completion and Balancing", translated into Latin by Robert of Chester, and the complete text of Euclid's "Elements", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona.
These new sources sparked a renewal of mathematics. Fibonacci, writing in the "Liber Abaci", in 1202 and updated in 1254, produced the first significant mathematics in Europe since the time of Eratosthenes, a gap of more than a thousand years. The work introduced Hindu-Arabic numerals to Europe, and discussed many other mathematical problems. 
The 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.
Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.
One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed "by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant".
Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that "a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]".
Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's "Elements", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.
Renaissance mathematics.
During the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as "abbaco" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.
Luca Pacioli's "Summa de Arithmetica, Geometria, Proportioni et Proportionalità" (Italian: "Review of Arithmetic, Geometry, Ratio and Proportion") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, "Particularis de Computis et Scripturis" (Italian: "Details of Calculation and Recording"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In "Summa Arithmetica", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. "Summa Arithmetica" was also the first known book printed in Italy to contain algebra. It is important to note that Pacioli himself had borrowed much of the work of Piero Della Francesca whom he plagiarized.
In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book "Ars Magna", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his "L'Algebra" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.
Simon Stevin's book "De Thiende" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.
Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his "Trigonometria" in 1595. Regiomontanus's table of sines and cosines was published in 1533.
During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.
Mathematics during the Scientific Revolution.
17th century.
The 17th century saw an unprecedented explosion of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates. Simon Stevin (1585) created the basis for modern decimal notation capable of describing all numbers, whether rational or irrational.
Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.
In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.
18th century.
The most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol "i", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.
Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.
Modern mathematics.
19th century.
Throughout the 19th century mathematics became increasingly abstract. In the 19th century lived Carl Friedrich Gauss (1777–1855). Leaving aside his many contributions to science, in pure mathematics he did revolutionary work on functions of complex variables, in geometry, and on the convergence of series. He gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.
This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.
The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.
Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.
Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.
In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L. E. J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.
The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.
In 1897, Hensel introduced p-adic numbers.
20th century.
The 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.
In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.
Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel used a computer to prove the four color theorem. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.
Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the "enormous theorem"), whose proof between 1955 and 1983 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym "Nicolas Bourbaki", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.
Differential geometry came into its own when Einstein used it in general relativity. Entire new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include, Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.
Non-standard analysis, introduced by Abraham Robinson, rehabillitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.
The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the Fast Fourier Transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.
At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.
One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.
Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the "collaborative distance" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.
Emmy Noether has been described by many as the most important woman in the history of mathematics, she revolutionized the theories of rings, fields, and algebras.
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the world wide web led to online publishing.
21st century.
In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).
Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.
Future of mathematics.
There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, the volume of data to be analyzed being produced by science and industry, facilitated by computers, is explosively expanding.

</doc>
<doc id="14223" url="http://en.wikipedia.org/wiki?curid=14223" title="HSK">
HSK

HSK may refer to:

</doc>
<doc id="14225" url="http://en.wikipedia.org/wiki?curid=14225" title="Hydrogen atom">
Hydrogen atom

A hydrogen atom is an atom of the chemical element hydrogen. The electrically neutral atom contains a single positively charged proton and a single negatively charged electron bound to the nucleus by the Coulomb force. Atomic hydrogen constitutes about 75% of the elemental (baryonic) mass of the universe.
In everyday life on Earth, isolated hydrogen atoms (usually called "atomic hydrogen" or, more precisely, "monatomic hydrogen") are extremely rare. Instead, hydrogen tends to combine with other atoms in compounds, or with itself to form ordinary (diatomic) hydrogen gas, H2. "Atomic hydrogen" and "hydrogen atom" in ordinary English use have overlapping, yet distinct, meanings. For example, a water molecule contains two hydrogen atoms, but does not contain atomic hydrogen (which would refer to isolated hydrogen atoms).
Production and reactivity.
The H–H bond is one of the strongest bonds in chemistry, with a bond dissociation enthalpy of 435.88 kJ/mol at 298 K. As a consequence of this strong bond, H2 dissociates to only a minor extent until higher temperatures. At 3000 K, the degree of dissociation is just 7.85%: 
Hydrogen atoms are so reactive that they combine chemically with almost all elements.
Isotopes.
The most abundant isotope, hydrogen-1, protium, or light hydrogen, contains no neutrons; other isotopes of hydrogen, such as deuterium or tritium, contain one or more neutrons. The formulas below are valid for all three isotopes of hydrogen, but slightly different values of the Rydberg constant (correction formula given below) must be used for each hydrogen isotope.
Quantum theoretical analysis.
The hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.
In 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simplifying assumptions. These assumptions, the cornerstones of the Bohr model, were not fully correct but did yield fairly correct energy answers (with a relative error in the ground state ionization energy of around α2/4 or around 10−5). In the Bohr model each energy level is identified by an integer quantum number n (now known as the principal quantum number), and has an energy given by
where me is the electron mass, c is the speed of light, and α is the fine structure constant. Bohr's results for the frequencies and underlying energy values were duplicated by the solution to the Schrödinger equation in 1925–1926. The solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines. The solution of the Schrödinger equation goes much further than the Bohr model, because it also yields two other quantum numbers and the shape of the electron's wave function ("orbital") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds.
The Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.
Since the Schrödinger equation is only valid for non-relativistic quantum mechanics, the solutions it yields for the hydrogen atom are not entirely correct. The Dirac equation of relativistic quantum theory improves these solutions (see below).
Solution of Schrödinger equation.
The solution of the Schrödinger equation (wave equations) for the hydrogen atom uses the fact that the Coulomb potential produced by the nucleus is isotropic (it is radially symmetric in space and only depends on the distance to the nucleus). Although the resulting energy eigenfunctions (the "orbitals") are not necessarily isotropic themselves, their dependence on the angular coordinates follows completely generally from this isotropy of the underlying potential: the eigenstates of the Hamiltonian (that is, the energy eigenstates) can be chosen as simultaneous eigenstates of the angular momentum operator. This corresponds to the fact that angular momentum is conserved in the orbital motion of the electron around the nucleus. Therefore, the energy eigenstates may be classified by two angular momentum quantum numbers, "ℓ" and "m" (both are integers). The angular momentum quantum number determines the magnitude of the angular momentum. The magnetic quantum number determines the projection of the angular momentum on the (arbitrarily chosen) "z"-axis.
In addition to mathematical expressions for total angular momentum and angular momentum projection of wavefunctions, an expression for the radial dependence of the wave functions must be found. It is only here that the details of the 1/"r" Coulomb potential enter (leading to Laguerre polynomials in "r"). This leads to a third quantum number, the principal quantum number . The principal quantum number in hydrogen is related to the atom's total energy.
Note that the maximum value of the angular momentum quantum number is limited by the principal quantum number: it can run only up to "n" − 1, i.e. .
Due to angular momentum conservation, states of the same "ℓ" but different "m" have the same energy (this holds for all problems with rotational symmetry). In addition, for the hydrogen atom, states of the same "n" but different "ℓ" are also degenerate (i.e. they have the same energy). However, this is a specific property of hydrogen and is no longer true for more complicated atoms which have a (effective) potential differing from the form 1/"r" (due to the presence of the inner electrons shielding the nucleus potential).
Taking into account the spin of the electron adds a last quantum number, the projection of the electron's spin angular momentum along the "z"-axis, which can take on two values. Therefore, any eigenstate of the electron in the hydrogen atom is described fully by four quantum numbers. According to the usual rules of quantum mechanics, the actual state of the electron may be any superposition of these states. This explains also why the choice of "z"-axis for the directional quantization of the angular momentum vector is immaterial: an orbital of given "ℓ" and "m"′ obtained for another preferred axis "z"′ can always be represented as a suitable superposition of the various states of different "m" (but same "l") that have been obtained for "z".
Alternatives to the Schrödinger theory.
In the language of Heisenberg's matrix mechanics, the hydrogen atom was first solved by Wolfgang Pauli using a rotational symmetry in four dimension [O(4)-symmetry] generated by the angular momentum 
and the Laplace–Runge–Lenz vector. By extending the symmetry group O(4) to the dynamical group O(4,2),
the entire spectrum and all transitions were embedded in a single irreducible group representation.
In 1979 the (non relativistic) hydrogen atom was solved for the first time within Feynman's path integral formulation
of quantum mechanics. This work greatly extended the range of applicability of Feynman's method.
Mathematical summary of eigenstates of hydrogen atom.
In 1928, Paul Dirac found an equation that was fully compatible with Special Relativity, and (as a consequence) made the wave function a 4-component "Dirac spinor" including "up" and "down" spin components, with both positive and "negative" energy (or matter and antimatter). The solution to this equation gave the following results, more accurate than the Schrödinger solution.
Energy levels.
The energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression:
where "α" is the fine-structure constant and "j" is the "total angular momentum" quantum number, which is equal to |"ℓ" ± 1⁄2| depending on the direction of the electron spin. This formula represents a small correction to the energy obtained by Bohr and Schrödinger as given above. The factor in square brackets in the last expression is nearly one; the extra term arises from relativistic effects (for details, see #Features going beyond the Schrödinger solution). It is worth noting that this expression was first obtained by A. Sommerfeld in 1916 based on the relativistic version of the old Bohr theory. Sommerfeld has however used different notation for the quantum numbers.
The value 
is called the Rydberg constant and was first found from the Bohr model as given by
where "m"e is the electron mass, "e" is the elementary charge, "h" is the Planck constant, and "ε"0 is the vacuum permittivity.
This constant is often used in atomic physics in the form of the Rydberg unit of energy: 
The exact value of the Rydberg constant above assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. However, since the nucleus is much heavier than the electron, the values are nearly the same. The Rydberg constant "RM" for a hydrogen atom (one electron), "R" is given by:
formula_6
where "M" is the mass of the atomic nucleus. For hydrogen-1, the quantity formula_7 is about 1/1836 (i.e. the electron-to-proton mass ratio). For deuterium and tritium, the ratios are about 1/3670 and 1/5497 respectively. These figures, when added to 1 in the denominator, represent very small corrections in the value of "R", and thus only small corrections to all energy levels in corresponding hydrogen isotopes.
Wavefunction.
The normalized position wavefunctions, given in spherical coordinates are:
where:
The quantum numbers can take the following values: 
Additionally, these wavefunctions are "normalized" (i.e., the integral of their modulus square equals 1) and orthogonal:
where formula_19 is the state represented by the wavefunction formula_20 in Dirac notation, and formula_21 is the Kronecker delta function.
Angular momentum.
The eigenvalues for Angular momentum operator:
Visualizing the hydrogen electron orbitals.
The image to the right shows the first few hydrogen atom orbitals (energy eigenfunctions). These are cross-sections of the probability density that are color-coded (black represents zero density and white represents the highest density). The angular momentum (orbital) quantum number "ℓ" is denoted in each column, using the usual spectroscopic letter code ("s" means "ℓ" = 0, "p" means "ℓ" = 1, "d" means "ℓ" = 2). The main (principal) quantum number "n" (= 1, 2, 3, ...) is marked to the right of each row. For all pictures the magnetic quantum number "m" has been set to 0, and the cross-sectional plane is the "xz"-plane ("z" is the vertical axis). The probability density in three-dimensional space is obtained by rotating the one shown here around the "z"-axis.
The "ground state", i.e. the state of lowest energy, in which the electron is usually found, is the first one, the 1"s" state (principal quantum level "n" = 1, "ℓ" = 0).
 is also available (up to higher numbers "n" and "ℓ").
Black lines occur in each but the first orbital: these are the nodes of the wavefunction, i.e. where the probability density is zero. (More precisely, the nodes are spherical harmonics that appear as a result of solving Schrödinger's equation in polar coordinates.)
The quantum numbers determine the layout of these nodes. There are:
Features going beyond the Schrödinger solution.
There are several important effects that are neglected by the Schrödinger equation and which are responsible for certain small but measurable deviations of the real spectral lines from the predicted ones:
Both of these features (and more) are incorporated in the relativistic Dirac equation, with predictions that come still closer to experiment. Again the Dirac equation may be solved analytically in the special case of a two-body system, such as the hydrogen atom. The resulting solution quantum states now must be classified by the total angular momentum number "j" (arising through the coupling between electron spin and orbital angular momentum). States of the same "j" and the same "n" are still degenerate. Thus, direct analytical solution of Dirac equation predicts 2S(1⁄2) and 2P(1⁄2) levels of Hydrogen to have exactly the same energy, which is in a contradiction with observations (Lamb-Retherford experiment).
For these developments, it was essential that the solution of the Dirac equation for the hydrogen atom could be worked out exactly, such that any experimentally observed deviation had to be taken seriously as a signal of failure of the theory.
Due to the high precision of the theory also very high precision for the experiments is needed, which utilize a frequency comb.
Hydrogen ion.
Hydrogen is not found without its electron in ordinary chemistry (room temperatures and pressures), as ionized hydrogen is highly chemically reactive. When ionized hydrogen is written as "H+" as in the solvation of classical acids such as hydrochloric acid, the hydronium ion, H3O+, is meant, not a literal ionized single hydrogen atom. In that case, the acid transfers the proton to H2O to form H3O+.
Ionized hydrogen without its electron, or free protons, are common in the interstellar medium, and solar wind.

</doc>
<doc id="14227" url="http://en.wikipedia.org/wiki?curid=14227" title="Elagabalus">
Elagabalus

Marcus Aurelius Antoninus Augustus ( 203 – March 11, 222), commonly known as Elagabalus or Heliogabalus, was Roman Emperor from 218 to 222. A member of the Severan Dynasty, he was Syrian, the second son of Julia Soaemias and Sextus Varius Marcellus. In his early youth he served as a priest of the god Elagabal (in Latin, "Elagabalus") in the hometown of his mother's family, Emesa. As a private citizen, he was probably named Sextus Varius Avitus Bassianus. Upon becoming emperor he took the name Marcus Aurelius Antoninus Augustus. He was called Elagabalus only after his death.
In 217, the emperor Caracalla was assassinated and replaced by his Praetorian prefect, Marcus Opellius Macrinus. Caracalla's maternal aunt, Julia Maesa, successfully instigated a revolt among the Third Legion to have her eldest grandson (and Caracalla's cousin), Elagabalus, declared emperor in his place. Macrinus was defeated on 8 June 218, at the Battle of Antioch. Elagabalus, barely fourteen years old, became emperor, initiating a reign remembered mainly for sexual scandal and religious controversy.
Later historians suggest Elagabalus showed a disregard for Roman religious traditions and sexual taboos. He replaced the traditional head of the Roman pantheon, Jupiter, with the deity of whom he was high priest, Elagabal. He forced leading members of Rome's government to participate in religious rites celebrating this deity, over which he personally presided. Elagabalus was married as many as five times, lavished favours on male courtiers popularly thought to have been his lovers, and was reported to have prostituted himself in the imperial palace. His behavior estranged the Praetorian Guard, the Senate, and the common people alike.
Amidst growing opposition, Elagabalus, just 18 years old, was assassinated and replaced by his cousin Alexander Severus on 11 March 222, in a plot formulated by his grandmother, Julia Maesa, and carried out by disaffected members of the Praetorian Guard.
Elagabalus developed a reputation among his contemporaries for extreme eccentricity, decadence and zealotry. This tradition has persisted, and in writers of the early modern age he suffers one of the worst reputations among Roman emperors. Edward Gibbon, for example, wrote that Elagabalus "abandoned himself to the grossest pleasures and ungoverned fury." According to B.G. Niebuhr, "The name Elagabalus is branded in history above all others" because of his "unspeakably disgusting life."
Family and priesthood.
Elagabalus was born around the year 203 to Sextus Varius Marcellus and Julia Soaemias Bassiana. His father was initially a member of the equestrian class, but was later elevated to the rank of senator. His grandmother Julia Maesa was the widow of the consul Gaius Julius Avitus Alexianus, the sister of Julia Domna, and the sister-in-law of the emperor Septimius Severus. He had at least one sibling: an unnamed elder brother. His mother, Julia Soaemias, was a cousin of the Roman emperor Caracalla. Other relatives included his aunt Julia Avita Mamaea and uncle Marcus Julius Gessius Marcianus and among their children, their son Alexander Severus. Elagabalus's family held hereditary rights to the priesthood of the sun god Elagabal, of whom Elagabalus was the high priest at Emesa (modern Homs) in Syria.
The deity Elagabalus was initially venerated at Emesa. This form of the god's name is a Latinized version of the Syrian "Ilāh hag-Gabal", which derives from "Ilāh" (a Semitic word for "god") and "gabal" (an Aramaic word for "mountain"), resulting in "the God of the Mountain," the Emesene manifestation of the deity. The cult of the deity spread to other parts of the Roman Empire in the 2nd century; a dedication has been found as far away as Woerden (Netherlands). The god was later imported and assimilated with the Roman sun god known as Sol Indiges in republican times and as Sol Invictus during the 2nd and 3rd centuries CE. In Greek the sun god is Helios, hence "Heliogabalus", a variant of "Elagabalus".
Rise to power.
When the emperor Macrinus came to power, he suppressed the threat against his reign by the family of his assassinated predecessor, Caracalla, by exiling them—Julia Maesa, her two daughters, and her eldest grandson Elagabalus—to their estate at Emesa in Syria. Almost upon arrival in Syria Maesa began a plot, with her advisor and Elagabalus' tutor Gannys, to overthrow Macrinus and elevate the fourteen-year-old Elagabalus to the imperial throne.
His mother publicly declared that he was the illegitimate son of Caracalla, therefore due the loyalties of Roman soldiers and senators who had sworn allegiance to Caracalla. After Julia Maesa displayed her wealth to the Third Legion at Raphana they swore allegiance to Elagabalus. At sunrise on 16 May 218, Publius Valerius Comazon, commander of the legion, declared him emperor. To strengthen his legitimacy through further propaganda, Elagabalus assumed Caracalla's names, "Marcus Aurelius Antoninus".
In response Macrinus dispatched his Praetorian prefect Ulpius Julianus to the region with a contingent of troops he considered strong enough to crush the rebellion. However, this force soon joined the faction of Elagabalus when, during the battle, they turned on their own commanders. The officers were killed and Julianus' head was sent back to the emperor.
Macrinus now sent letters to the Senate denouncing Elagabalus as the "False Antoninus" and claiming he was insane. Both consuls and other high-ranking members of Rome's leadership condemned Elagabalus, and the Senate subsequently declared war on both Elagabalus and Julia Maesa.
Macrinus and his son, weakened by the desertion of the Second Legion due to bribes and promises circulated by Julia Maesa, were defeated on 8 June 218 at the Battle of Antioch by troops commanded by Gannys. Macrinus fled toward Italy, disguised as a courier, but was later intercepted near Chalcedon and executed in Cappadocia. His son Diadumenianus, sent for safety to the Parthian court, was captured at Zeugma and also put to death.
Elagabalus declared the date of the victory at Antioch to be the beginning of his reign and assumed the imperial titles without prior senatorial approval, which violated tradition but was a common practice among 3rd-century emperors nonetheless. Letters of reconciliation were dispatched to Rome extending amnesty to the Senate and recognizing the laws, while also condemning the administration of Macrinus and his son.
The senators responded by acknowledging Elagabalus as emperor and accepting his claim to be the son of Caracalla. Caracalla and Julia Domna were both deified by the Senate, both Julia Maesa and Julia Soaemias were elevated to the rank of Augustae, and the memory of both Macrinus and Diadumenianus was condemned by the Senate. The former commander of the Third Legion, Comazon, was appointed commander of the Praetorian Guard.
Emperor (218–222).
Elagabalus and his entourage spent the winter of 218 in Bithynia at Nicomedia, where the emperor's religious beliefs first presented themselves as a problem. The contemporary historian Cassius Dio suggests that Gannys was in fact killed by the new emperor because he was forcing Elagabalus to live "temperately and prudently." To help Romans adjust to the idea of having an oriental priest as emperor, Julia Maesa had a painting of Elagabalus in priestly robes sent to Rome and hung over a statue of the goddess Victoria in the Senate House. This placed senators in the awkward position of having to make offerings to Elagabalus whenever they made offerings to Victoria.
The legions were dismayed by his behaviour and quickly came to regret having supported his accession. While Elagabalus was still on his way to Rome, brief revolts broke out by the Fourth Legion at the instigation of Gellius Maximus, and by the Third Legion, which itself had been responsible for the elevation of Elagabalus to the throne, under the command of Senator Verus. The rebellion was quickly put down, and the Third Legion disbanded.
When the entourage reached Rome in the autumn of 219, Comazon and other allies of Julia Maesa and Elagabalus were given powerful and lucrative positions, to the outrage of many senators who did not consider them worthy of such privileges. After his tenure as Praetorian prefect, Comazon would serve as the city prefect of Rome three times, and as consul twice. Elagabalus soon devalued the Roman currency. He decreased the silver purity of the "denarius" from 58% to 46.5% — the actual silver weight dropping from 1.82 grams to 1.41 grams. He also demonetized the "antoninianus" during this period in Rome.
Elagabalus tried to have his presumed lover, the charioteer Hierocles, declared Caesar, while another alleged lover, the athlete Aurelius Zoticus, was appointed to the non-administrative but influential position of Master of the Chamber, or "Cubicularius". His offer of amnesty for the Roman upper class was largely honoured, though the jurist Ulpian was exiled.
The relationships between Julia Maesa, Julia Soaemias, and Elagabalus were strong at first. His mother and grandmother became the first women to be allowed into the Senate, and both received senatorial titles: Soaemias the established title of "Clarissima," and Maesa the more unorthodox "Mater Castrorum et Senatus" ("Mother of the army camp and of the Senate"). While Julia Maesa tried to position herself as the power behind the throne and thus the most powerful woman in the world, Elagabalus would prove to be highly independent, set in his ways, and impossible to control.
Religious controversy.
Since the reign of Septimius Severus, sun worship had increased throughout the Empire. Elagabalus saw this as an opportunity to install Elagabal as the chief deity of the Roman pantheon. The god was renamed "Deus Sol Invictus", meaning "God the Undefeated Sun", and honored above Jupiter.
As a token of respect for Roman religion, however, Elagabalus joined either Astarte, Minerva, Urania, or some combination of the three to Elagabal as wife. Before constructing a temple in dedication to Elagabal, Elagabalus placed the meteorite of Elagabal next to the throne of Jupiter at the temple of Jupiter Optimus Maximus.
He stirred further discontent when he himself married the Vestal Virgin Aquilia Severa, claiming the marriage would produce "godlike children". This was a flagrant breach of Roman law and tradition, which held that any Vestal found to have engaged in sexual intercourse was to be buried alive.
A lavish temple called the Elagabalium was built on the east face of the Palatine Hill to house Elagabal, who was represented by a black conical meteorite from Emesa. Herodian wrote "this stone is worshipped as though it were sent from heaven; on it there are some small projecting pieces and markings that are pointed out, which the people would like to believe are a rough picture of the sun, because this is how they see them".
In order to become the high priest of his new religion, Elagabalus had himself circumcised. He forced senators to watch while he danced around the altar of Deus Sol Invictus to the accompaniment of drums and cymbals. Each summer solstice he held a festival dedicated to the god, which became popular with the masses because of the free food distributed on such occasions. During this festival, Elagabalus placed the Emesa stone on a chariot adorned with gold and jewels, which he paraded through the city:
A six horse chariot carried the divinity, the horses huge and flawlessly white, with expensive gold fittings and rich ornaments. No one held the reins, and no one rode in the chariot; the vehicle was escorted as if the god himself were the charioteer. Elagabalus ran backward in front of the chariot, facing the god and holding the horses' reins. He made the whole journey in this reverse fashion, looking up into the face of his god.
The most sacred relics from the Roman religion were transferred from their respective shrines to the Elagabalium, including the emblem of the Great Mother, the fire of Vesta, the Shields of the Salii and the Palladium, so that no other god could be worshipped except in company with Elagabal.
Sex/gender controversy.
Elagabalus' sexual orientation and gender identity are the subject of much debate. Elagabalus married and divorced five women, three of whom are known. His first wife was Julia Cornelia Paula; the second was the Vestal Virgin Julia Aquilia Severa.
Within a year, he abandoned her and married Annia Aurelia Faustina, a descendant of Marcus Aurelius and the widow of a man recently executed by Elagabalus. He had returned to his second wife Severa by the end of the year. According to Cassius Dio, his most stable relationship seems to have been with his chariot driver, a blond slave from Caria named Hierocles, whom he referred to as his husband.
The "Augustan History" claims that he also married a man named Zoticus, an athlete from Smyrna, in a public ceremony at Rome. Cassius Dio reported that Elagabalus would paint his eyes, epilate his hair and wear wigs before prostituting himself in taverns, brothels, and even in the imperial palace:
Finally, he set aside a room in the palace and there committed his indecencies, always standing nude at the door of the room, as the harlots do, and shaking the curtain which hung from gold rings, while in a soft and melting voice he solicited the passers-by. There were, of course, men who had been specially instructed to play their part. For, as in other matters, so in this business, too, he had numerous agents who sought out those who could best please him by their foulness. He would collect money from his patrons and give himself airs over his gains; he would also dispute with his associates in this shameful occupation, claiming that he had more lovers than they and took in more money.
Herodian commented that Elagabalus enhanced his natural good looks by the regular application of cosmetics. He was described as having been "delighted to be called the mistress, the wife, the queen of Hierocles" and was reported to have offered vast sums of money to any physician who could equip him with female genitalia. Elagabalus has been characterized by some modern writers as transgender, perhaps transsexual.
Fall from power.
By 221 Elagabalus' eccentricities, particularly his relationship with Hierocles, increasingly provoked the soldiers of the Praetorian Guard. When Elagabalus' grandmother Julia Maesa perceived that popular support for the emperor was waning, she decided that he and his mother, who had encouraged his religious practices, had to be replaced. As alternatives, she turned to her other daughter, Julia Avita Mamaea, and her daughter's son, the thirteen-year-old Severus Alexander.
Prevailing on Elagabalus, she arranged that he appoint his cousin Alexander as his heir and be given the title of "Caesar". Alexander shared the consulship with the emperor that year. However, Elagabalus reconsidered this arrangement when he began to suspect that the Praetorian Guard preferred his cousin over himself.
Following the failure of various attempts on Alexander's life, Elagabalus stripped his cousin of his titles, revoked his consulship, and circulated the news that Alexander was near death, in order to see how the Praetorians would react. A riot ensued, and the guard demanded to see Elagabalus and Alexander in the Praetorian camp.
Assassination.
The emperor complied and on 11 March 222 he publicly presented his cousin along with his own mother, Julia Soaemias. On their arrival the soldiers started cheering Alexander while ignoring Elagabalus, who ordered the summary arrest and execution of anyone who had taken part in this display of insubordination. In response, members of the Praetorian Guard attacked Elagabalus and his mother:
So he made an attempt to flee, and would have got away somewhere by being placed in a chest, had he not been discovered and slain, at the age of 18. His mother, who embraced him and clung tightly to him, perished with him; their heads were cut off and their bodies, after being stripped naked, were first dragged all over the city, then the mother's body was cast aside somewhere or other while his was thrown into the [Tiber].
Following his assassination, many associates of Elagabalus were killed or deposed, including his lover Hierocles. His religious edicts were reversed and the stone of Elagabal was sent back to Emesa. Women were again barred from attending meetings of the Senate. The practice of "damnatio memoriae"—erasing from the public record a disgraced personage formerly of note—was systematically applied in his case.
Sources.
Augustan History.
The source of many of these stories of Elagabalus's depravity is the "Augustan History" ("Historia Augusta"), which includes controversial claims. The "Historia Augusta" was most likely written toward the end of the 4th century during the reign of emperor Theodosius I. The life of Elagabalus as described in the "Augustan History" is of uncertain historical merit. Sections 13 to 17, relating to the fall of Elagabalus, are less controversial among historians.
Cassius Dio.
Sources often considered more credible than the "Augustan History" include the contemporary historians Cassius Dio and Herodian. Cassius Dio lived from the second half of the 2nd century until sometime after 229. Born into a patrician family, he spent the greater part of his life in public service. He was a senator under emperor Commodus and governor of Smyrna after the death of Septimius Severus. Afterwards he served as suffect consul around 205, and as proconsul in Africa and Pannonia.
Alexander Severus held him in high esteem and made him his consul again. His "Roman History" spans nearly a millennium, from the arrival of Aeneas in Italy until the year 229. As a contemporary of Elagabalus, Cassius Dio's account of his reign is generally considered more reliable than the "Augustan History", although by his own admission Dio spent the greater part of the relevant period outside of Rome and had to rely on second-hand accounts.
Furthermore, the political climate in the aftermath of Elagabalus' reign, as well as Dio's own position within the government of Alexander, likely influenced the truth of this part of his history for the worse. Dio regularly refers to Elagabalus as Sardanapalus, partly to distinguish him from his divine namesake, but chiefly to do his part in maintaining the "damnatio memoriae" enforced after the emperor's death and to associate him with another autocrat notorious for a dissolute life.
Herodian.
Another contemporary of Elagabalus was Herodian, who was a minor Roman civil servant who lived from c. 170 until 240. His work, "History of the Roman Empire since Marcus Aurelius", commonly abbreviated as "Roman History", is an eyewitness account of the reign of Commodus until the beginning of the reign of Gordian III. His work largely overlaps with Dio's own "Roman History", but both texts seem to be independently consistent with each other.
Although Herodian is not deemed as reliable as Cassius Dio, his lack of literary and scholarly pretensions make him less biased than senatorial historians. Herodian is considered the most important source for the religious reforms which took place during the reign of Elagabalus, which have been confirmed by numismatic and archaeological evidence.
Edward Gibbon and later historians.
For readers of the modern age, "The History of the Decline and Fall of the Roman Empire" by Edward Gibbon (1737–94) further cemented the scandalous reputation of Elagabalus. Gibbon not only accepted and expressed outrage at the allegations of the ancient historians, but might have added some details of his own; he is the first historian known to state that Gannys was a eunuch, for example. Gibbon wrote:
To confound the order of the season and climate, to sport with the passions and prejudices of his subjects, and to subvert every law of nature and decency, were in the number of his most delicious amusements. A long train of concubines, and a rapid succession of wives, among whom was a vestal virgin, ravished by force from her sacred asylum, were insufficient to satisfy the impotence of his passions. The master of the Roman world affected to copy the manners and dress of the female sex, preferring the distaff to the sceptre, and dishonored the principal dignities of the empire by distributing them among his numerous lovers; one of whom was publicly invested with the title and authority of the emperor's, or, as he more properly styled himself, the empress's husband. It may seem probable, the vices and follies of Elagabalus have been adorned by fancy, and blackened by prejudice. Yet, confining ourselves to the public scenes displayed before the Roman people, and attested by grave and contemporary historians, their inexpressible infamy surpasses that of any other age or country.
Two hundred years after the age of Pliny, the use of pure, or even of mixed silks, was confined to the female sex, till the opulent citizens of Rome and the provinces were insensibly familiarized with the example of Elagabalus, the first who, by this effeminate habit, had sullied the dignity of an emperor and a man.
Some recent historians argue for a more favourable picture of his life and reign. Martijn Icks in "Images of Elagabalus" (2008; republished as "The Crimes of Elagabalus" in 2012) doubts the reliability of the ancient sources and argues that it was the emperor's unorthodox religious policies that alienated the power elite of Rome, to the point that his grandmother saw fit to eliminate him and replace him with his cousin. Leonardo de Arrizabalaga y Prado, in "The Emperor Elagabalus: Fact or Fiction?" (2008), is also critical of the ancient historians and speculates that neither religion nor sexuality played a role in the fall of the young emperor, who was simply the loser in a power struggle within the imperial family; the loyalty of the Praetorian Guards was up for sale, and Julia Maesa had the resources to outmaneuver and outbribe her grandson. According to this version, once Elagabalus, his mother, and his immediate circle had been murdered, a wholesale propaganda war against his memory resulted in a vicious caricature which has persisted to the present, repeated and often embellished by later historians displaying their own prejudices against effeminacy and other vices which Elagabalus had come to epitomize.
Legacy.
Due to the ancient tradition about him, Elagabalus became something of an (anti-)hero in the Decadent movement of the late 19th century. He often appears in literature and other creative media as the epitome of a young, amoral aesthete. His life and character have informed or at least inspired many famous works of art, by Decadents, even by contemporary artists. The most notable of these works include:
References.
Primary sources.
</dl>

</doc>
<doc id="14229" url="http://en.wikipedia.org/wiki?curid=14229" title="Homeopathy">
Homeopathy

Homeopathy (; also spelled homoeopathy; from the Greek: ὅμοιος "hómoios", "-like" and πάθος "páthos", "suffering") is a form of alternative medicine created in 1796 by Samuel Hahnemann based on his doctrine of "like cures like" ("similia similibus curentur"), whereby a substance that causes the symptoms of a disease in healthy people will cure similar symptoms in sick people. Homeopathy is pseudoscience. It is not effective for any condition, and no homeopathic remedy has been proven to be more effective than placebo.
Hahnemann believed the underlying causes of disease were phenomena that he termed "miasms", and that homeopathic "remedies" addressed these. The remedies are prepared using a process of homeopathic dilution, which involves repeatedly diluting a chosen substance in alcohol or distilled water, followed by forceful striking on an elastic body. Dilution usually continues well past the point where no molecules of the original substance remain. Homeopaths select remedies by consulting reference books known as "repertories", and by considering the totality of the patient's symptoms, personal traits, physical and psychological state, and life history.
Homeopathy lacks biological plausibility, and its axioms are contradicted by scientific facts. The postulated mechanisms of action of homeopathic remedies are both scientifically implausible and physically impossible. Although some clinical trials produce positive results, systematic reviews reveal that this is because of chance, flawed research methods, and reporting bias. Continued homeopathic practice, despite the evidence that it does not work, has been criticized as unethical because it discourages the use of effective treatments, with the World Health Organisation warning against using homeopathy to try to treat severe diseases such as HIV and malaria. The continued practice of homeopathy, despite a lack of evidence of efficacy, has led to it being characterized within the scientific and medical communities as nonsense, quackery, and a sham.
Assessments by the Australian National Health and Medical Research Council and the Swiss and British government health departments have each concluded that homeopathy is ineffective, recommending against the practice receiving any further funding.
History.
Historical context.
Homeopaths claim that Hippocrates may have originated homeopathy around 400 BC, when he prescribed a small dose of mandrake root to treat mania, knowing it produces mania in much larger doses. In the 16th century, the pioneer of pharmacology Paracelsus declared that small doses of "what makes a man ill also cures him". Samuel Hahnemann (1755–1843) gave homeopathy its name and expanded its principles in the late 18th century. At that time, mainstream medicine used methods like bloodletting and purging, and administered complex mixtures, such as Venice treacle, which was made from 64 substances including opium, myrrh, and viper's flesh. These treatments often worsened symptoms and sometimes proved fatal. Hahnemann rejected these practices – which had been extolled for centuries – as irrational and inadvisable;
instead, he advocated the use of single drugs at lower doses and promoted an immaterial, vitalistic view of how living organisms function, believing that diseases have spiritual, as well as physical causes.
Hahnemann's concept.
The term "homeopathy" was coined by Hahnemann and first appeared in print in 1807.
Hahnemann conceived of homeopathy while translating a medical treatise by the Scottish physician and chemist William Cullen into German. Being skeptical of Cullen's theory concerning cinchona's use for curing malaria, Hahnemann ingested some bark specifically to investigate what would happen. He experienced fever, shivering and joint pain: symptoms similar to those of malaria itself. From this, Hahnemann came to believe that all effective drugs produce symptoms in healthy individuals similar to those of the diseases that they treat, in accord with the "law of similars" that had been proposed by ancient physicians. An account of the effects of eating cinchona bark noted by Oliver Wendell Holmes, and published in 1861, failed to reproduce the symptoms Hahnemann reported.:128 Hahnemann's law of similars is a postulate rather than a scientific law.
Subsequent scientific work shows that cinchona cures malaria because it contains quinine, which kills the "Plasmodium falciparum" parasite that causes the disease; the mechanism of action is unrelated to Hahnemann's ideas.
"Provings".
Hahnemann began to test what effects substances produced in humans, a procedure that would later become known as "homeopathic proving". These tests required subjects to test the effects of ingesting substances by clearly recording all of their symptoms as well as the ancillary conditions under which they appeared. He published a collection of provings in 1805, and a second collection of 65 remedies appeared in his book, "Materia Medica Pura", in 1810.
Because Hahnemann believed that large doses of drugs that caused similar symptoms would only aggravate illness, he advocated extreme dilutions of the substances; he devised a technique for making dilutions that he believed would preserve a substance's therapeutic properties while removing its harmful effects. Hahnemann believed that this process aroused and enhanced "the spirit-like medicinal powers of the crude substances".
He gathered and published a complete overview of his new medical system in his 1810 book, "The Organon of the Healing Art", whose 6th edition, published in 1921, is still used by homeopaths today.
Miasms and disease.
In "The Organon of the Healing Art", Hahnemann introduced the concept of "miasms" as "infectious principles" underlying chronic disease. Hahnemann associated each miasm with specific diseases, and thought that initial exposure to miasms causes local symptoms, such as skin or venereal diseases. If, however, these symptoms were suppressed by medication, the cause went deeper and began to manifest itself as diseases of the internal organs. Homeopathy maintains that treating diseases by directly alleviating their symptoms, as is sometimes done in conventional medicine, is ineffective because all "disease can generally be traced to some latent, deep-seated, underlying chronic, or inherited tendency". The underlying imputed miasm still remains, and deep-seated ailments can be corrected only by removing the deeper disturbance of the vital force.
Hahnemann originally presented only three miasms, of which the most important was "psora" (Greek for "itch"), described as being related to any itching diseases of the skin, supposed to be derived from suppressed scabies, and claimed to be the foundation of many further disease conditions. Hahnemann believed psora to be the cause of such diseases as epilepsy, cancer, jaundice, deafness, and cataracts.
Since Hahnemann's time, other miasms have been proposed, some replacing one or more of psora's proposed functions, including tuberculosis and cancer miasms.
The law of susceptibility implies that a negative state of mind can attract hypothetical disease entities called "miasms" to invade the body and produce symptoms of diseases. Hahnemann rejected the notion of a disease as a separate thing or invading entity, and insisted it was always part of the "living whole". Hahnemann coined the expression "allopathic medicine", which was used to pejoratively refer to traditional Western medicine.
Hahnemann's miasm theory remains disputed and controversial within homeopathy even in modern times. The theory of miasms has been criticized as an explanation developed by Hahnemann to preserve the system of homeopathy in the face of treatment failures, and for being inadequate to cover the many hundreds of sorts of diseases, as well as for failing to explain disease predispositions, as well as genetics, environmental factors, and the unique disease history of each patient.:148–9
19th century: rise to popularity and early criticism.
Homeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the U.S.A. opened in 1835, and in 1844, the first U.S. national medical association, the American Institute of Homeopathy, was established and throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States. By 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors of the time. Homeopathic remedies, even if ineffective, would almost surely cause no harm, making the users of homeopathic remedies less likely to be killed by the treatment that was supposed to be helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and to have begun the move towards more effective, science-based medicine.
One reason for the growing popularity of homeopathy was its apparent success in treating people suffering from infectious disease epidemics.
During 19th century epidemics of diseases such as cholera, death rates in homeopathic hospitals were often lower than in conventional hospitals, where the treatments used at the time were often harmful and did little or nothing to combat the diseases.
From its inception, however, homeopathy was criticized by mainstream science. Sir John Forbes, physician to Queen Victoria, said in 1843 that the extremely small doses of homeopathy were regularly derided as useless, "an outrage to human reason". James Young Simpson said in 1853 of the highly diluted drugs: "No poison, however strong or powerful, the billionth or decillionth of which would in the least degree affect a man or harm a fly."
19th-century American physician and author Oliver Wendell Holmes, Sr. was also a vocal critic of homeopathy and published an essay in 1842 entitled "Homœopathy and Its Kindred Delusions". The members of the French Homeopathic Society observed in 1867 that some leading homeopathists of Europe not only were abandoning the practice of administering infinitesimal doses but were also no longer defending it. The last school in the U.S. exclusively teaching homeopathy closed in 1920.
Revival in the 20th century.
According to , the Nazi regime in Germany were fascinated by homeopathy, and spent large sums of money on researching its mechanisms, but without gaining a positive result. Unschuld further argues that homeopathy never subsequently took root in the United States, but remained more deeply established in European thinking.
In the United States the "Food, Drug, and Cosmetic Act" of 1938 (sponsored by Royal Copeland, a Senator from New York and homeopathic physician) recognized homeopathic remedies as drugs. In the 1950s, there were only 75 pure homeopaths practicing in the U.S. However, by the mid to late 1970s, homeopathy made a significant comeback and sales of some homeopathic companies increased tenfold. Some homeopaths give credit for the revival to Greek homeopath George Vithoulkas, who performed a "great deal of research to update the scenarios and refine the theories and practice of homeopathy" beginning in the 1970s, but Ernst and Singh consider it to be linked to the rise of the New Age movement. Whichever is correct, mainstream pharmacy chains recognized the business potential of selling homeopathic remedies. The Food and Drug Administration held a hearing April 20 and 21, 2015 requesting public comment on regulation of homeopathic drugs. The FDA cited the growth of sales of over the counter homeopathic medicines, $2.7 billion as of 2007, many labeled as "natural, safe, and effective."
Bruce Hood has argued that the increased popularity of homeopathy in recent times may be due to the comparatively long consultations practitioners are willing to give their patients, and to an irrational preference for "natural" products which people think are the basis of homeopathic remedies.
"Remedies" and treatment.
Homeopathic prescriptions are referred to as "remedies". Practitioners rely on two types of reference when prescribing: "materia medica" and repertories. A homeopathic "materia medica" is a collection of "drug pictures", organised alphabetically by "remedy". These entries describe the symptom patterns associated with individual remedies. A homeopathic repertory is an index of disease symptoms that lists remedies associated with specific symptoms. In both cases different compilers may dispute particular inclusions. The first symptomatic homeopathic "materia medica" was arranged by Hahnemann. The first homeopathic repertory was Georg Jahr's "Symptomenkodex", published in German in 1835, and translated into English as the "Repertory to the more Characteristic Symptoms of Materia Medica" by Constantine Hering in 1838.
This version was less focused on disease categories and would be the forerunner to later works by James Tyler Kent. Repertories, in particular, may be very large.
Homeopathy uses many animal, plant, mineral, and synthetic substances in its remedies, generally referring to them using Latin or faux-Latin names. Examples include "arsenicum album" (arsenic oxide), "natrum muriaticum" (sodium chloride or table salt), "Lachesis muta" (the venom of the bushmaster snake), "opium", and "thyroidinum" (thyroid hormone).
Some homeopaths also use so-called "nosodes" (from the Greek "nosos", disease) made from diseased or pathological products such as fecal, urinary, and respiratory discharges, blood, and tissue. Homeopathic remedies prepared from healthy specimens are called "sarcodes".
Some modern homeopaths have considered more esoteric bases for remedies, known as "imponderables" because they do not originate from a substance but some other phenomenon presumed to have been "captured" by alcohol or lactose. Examples include X-rays
and sunlight.
Other minority practices include "paper remedies", where the substance and dilution are written on pieces of paper and either pinned to the patients' clothing, put in their pockets, or placed under glasses of water that are then given to the patients, and the use of radionics to prepare remedies. Such practices have been strongly criticised by classical homeopaths as unfounded, speculative, and verging upon magic and superstition.
Preparation.
Hahnemann found that undiluted doses caused reactions, sometimes dangerous ones, so specified that remedies be given at the lowest possible dose. He found that this reduced potency as well as side-effects, but formed the view that vigorous shaking and striking on an elastic surface - a process he termed "Schütteln", translated as "succussion" - nullified this. A common explanation for his settling on this process is said to be that he found remedies subjected to agitation in transit, such as in saddle bags or in a carriage, were more "potent".:16 Hahnemann had a saddle-maker construct a special wooden striking board covered in leather on one side and stuffed with horsehair.:31 Insoluble solids, such as granite, diamond, and platinum, are diluted by grinding them with lactose ("trituration").:23
The process of dilution and succussion is termed "dynamisation" or "potentisation" by homeopaths. In industrial manufacture this may be done by machine.
Serial dilution is normally achieved by taking an amount of the mixed remedy and adding it to a new vessel filled with solvent, but the "Korsakovian" method may also be used, whereby the vessel in which the remedy is prepared is emptied, refilled with solvent, and the fluid adhering to the walls of the vessel is deemed sufficient to potentise the new batch.:270
Fluxion and radionics methods of preparation do not require succussion.:171 There are differences of opinion on the number and force of strikes, and some practitioners dispute the need for succussion at all while others reject the Korsakovian and other non-classical preparations. There are no laboratory assays and the importance and techniques for succussion cannot be determined with any certainty from the literature.:67–69
Dilutions.
Three main logarithmic potency scales are in regular use in homeopathy. Hahnemann created the "centesimal" or "C scale", diluting a substance by a factor of 100 at each stage. The centesimal scale was favored by Hahnemann for most of his life.
A 2C dilution requires a substance to be diluted to one part in 100, and then some of that diluted solution diluted by a further factor of 100.
This works out to one part of the original substance in 10,000 parts of the solution. A 6C dilution repeats this process six times, ending up with the original substance diluted by a factor of 100−6=10−12 (one part in one trillion or 1/1,000,000,000,000). Higher dilutions follow the same pattern.
In homeopathy, a solution that is more dilute is described as having a higher "potency", and more dilute substances are considered by homeopaths to be stronger and deeper-acting remedies. The end product is often so diluted as to be indistinguishable from the diluent (pure water, sugar or alcohol). There is also a decimal potency scale (notated as "X" or "D") in which the remedy is diluted by a factor of 10 at each stage.
Hahnemann advocated 30C dilutions for most purposes (that is, dilution by a factor of 1060). Hahnemann regularly used potencies up to 300C but opined that "there must be a limit to the matter, it cannot go on indefinitely":322
In Hahnemann's time, it was reasonable to assume the remedies could be diluted indefinitely, as the concept of the atom or molecule as the smallest possible unit of a chemical substance was just beginning to be recognized.
The greatest dilution reasonably likely to contain even one molecule of the original substance is 12C.
Critics and advocates of homeopathy alike commonly attempt to illustrate the dilutions involved in homeopathy with analogies. 
Hahnemann is reported to have joked that a suitable procedure to deal with an epidemic would be to empty a bottle of poison into Lake Geneva, if it could be succussed 60 times.
Another example given by a critic of homeopathy states that a 12C solution is equivalent to a "pinch of salt in both the North and South Atlantic Oceans", which is approximately correct.
One-third of a drop of some original substance diluted into all the water on earth would produce a remedy with a concentration of about 13C. A popular homeopathic treatment for the flu is a 200C dilution of duck liver, marketed under the name Oscillococcinum. As there are only about 1080 atoms in the entire observable universe, a dilution of one molecule in the observable universe would be about 40C. Oscillococcinum would thus require 10320 more universes to simply have one molecule in the final substance.
The high dilutions characteristically used are often considered to be the most controversial and implausible aspect of homeopathy.
Not all homeopaths advocate extremely high dilutions. Remedies at potencies below 4X are considered an important part of homeopathic heritage. Many of the early homeopaths were originally doctors and generally used lower dilutions such as "3X" or "6X", rarely going beyond "12X".
The split between lower and higher dilutions followed ideological lines. 
Those favoring low dilutions stressed pathology and a stronger link to conventional medicine, while those favoring high dilutions emphasised vital force, miasms and a spiritual interpretation of disease.
Some products with such relatively lower dilutions continue to be sold, but like their counterparts, they have not been conclusively demonstrated to have any effect beyond that of a placebo.
Provings.
A homeopathic "proving" is the method by which the profile of a homeopathic remedy is determined.
At first Hahnemann used undiluted doses for provings, but he later advocated provings with remedies at a 30C dilution, and most modern provings are carried out using ultradilute remedies in which it is highly unlikely that any of the original molecules remain. During the proving process, Hahnemann administered remedies to healthy volunteers, and the resulting symptoms were compiled by observers into a "drug picture".
The volunteers were observed for months at a time and made to keep extensive journals detailing all of their symptoms at specific times throughout the day. They were forbidden from consuming coffee, tea, spices, or wine for the duration of the experiment; playing chess was also prohibited because Hahnemann considered it to be "too exciting", though they were allowed to drink beer and encouraged to exercise in moderation.
After the experiments were over, Hahnemann made the volunteers take an oath swearing that what they reported in their journals was the truth, at which time he would interrogate them extensively concerning their symptoms.
Provings are claimed to have been important in the development of the clinical trial, due to their early use of simple control groups, systematic and quantitative procedures, and some of the first application of statistics in medicine. The lengthy records of self-experimentation by homeopaths have occasionally proven useful in the development of modern drugs: For example, evidence that nitroglycerin might be useful as a treatment for angina was discovered by looking through homeopathic provings, though homeopaths themselves never used it for that purpose at that time.
The first recorded provings were published by Hahnemann in his 1796 "Essay on a New Principle".
His "Fragmenta de Viribus" (1805) contained the results of 27 provings, and his 1810 "Materia Medica Pura" contained 65.
For James Tyler Kent's 1905 "Lectures on Homoeopathic Materia Medica", 217 remedies underwent provings and newer substances are continually added to contemporary versions.
Though the proving process has superficial similarities with clinical trials, it is fundamentally different in that the process is subjective, not blinded, and modern provings are unlikely to use pharmacologically active levels of the substance under proving. As early as 1842, Holmes noted the provings were impossibly vague, and the purported effect was not repeatable among different subjects.
Homeopathic consultation.
Homeopaths generally begin with detailed examinations of their patients' histories, including questions regarding their physical, mental and emotional states, their life circumstances and any physical or emotional illnesses. The homeopath then attempts to translate this information into a complex formula of mental and physical symptoms, including likes, dislikes, innate predispositions and even body type.
From these symptoms, the homeopath chooses how to treat the patient using "materia medica" and repertories. In classical homeopathy, the practitioner attempts to match a single remedy to the totality of symptoms (the "simlilum"), while "clinical homeopathy" involves combinations of remedies based on the various symptoms of an illness.
Pills and active ingredients.
Homeopathic pills are made from an inert substance (often sugars, typically lactose), upon which a drop of liquid homeopathic preparation is placed and allowed to evaporate.
The process of homeopathic dilution results in no objectively detectable active ingredient in most cases, but some preparations (e.g. calendula and arnica creams) do contain pharmacologically active doses. One product, Zicam Cold Remedy, which was marketed as an "unapproved homeopathic" product, contains two ingredients that are only "slightly" diluted: zinc acetate (2X = 1/100 dilution) and zinc gluconate (1X = 1/10 dilution), which means both are present in a biologically active concentration strong enough to have caused some people to lose their sense of smell, a condition termed anosmia. Zicam also listed several normal homeopathic potencies as "inactive ingredients", including "galphimia glauca", histamine dihydrochloride (homeopathic name, "histaminum hydrochloricum"), "luffa operculata", and sulfur.
Related and minority treatments and practices.
Isopathy.
Isopathy is a therapy derived from homeopathy invented by Johann Joseph Wilhelm Lux in the 1830s. Isopathy differs from homeopathy in general in that the remedies, known as "nosodes", are made up either from things that cause the disease or from products of the disease, such as pus. Many so-called "homeopathic vaccines" are a form of isopathy.
Flower remedies.
Flower remedies can be produced by placing flowers in water and exposing them to sunlight. The most famous of these are the Bach flower remedies, which were developed by the physician and homeopath Edward Bach. Although the proponents of these remedies share homeopathy's vitalist world-view and the remedies are claimed to act through the same hypothetical "vital force" as homeopathy, the method of preparation is different. Bach flower remedies are prepared in "gentler" ways such as placing flowers in bowls of sunlit water, and the remedies are not succussed. There is no convincing scientific or clinical evidence for flower remedies being effective.
Veterinary use.
The idea of using homeopathy as a treatment for other animals, termed "veterinary homeopathy", dates back to the inception of homeopathy; Hahnemann himself wrote and spoke of the use of homeopathy in animals other than humans. The FDA has not approved homeopathic products as veterinary medicine in the U.S. In the UK, veterinary surgeons who use homeopathy may belong to the Faculty of Homeopathy and/or to the British Association of Homeopathic Veterinary Surgeons. Animals may be treated only by qualified veterinary surgeons in the UK and some other countries. Internationally, the body that supports and represents homeopathic veterinarians is the International Association for Veterinary Homeopathy.
The use of homeopathy in veterinary medicine is controversial; the little existing research on the subject is not of a high enough scientific standard to provide reliable data on efficacy. Other studies have also found that giving animals placebos can play active roles in influencing pet owners to believe in the effectiveness of the treatment when none exists. The British Veterinary Association's position statement on alternative medicines says that it "cannot endorse" homeopathy, and the Australian Veterinary Association includes it on its list of "ineffective therapies".
The UK's Department of Environment, Food and Rural Affairs (DeFRA) has adopted a robust position against use of "alternative" pet remedies including homeopathy.
Electrohomeopathy.
Electrohomeopathy is a treatment devised by Count Cesare Mattei (1809–1896), who proposed that different "colors" of electricity could be used to treat cancer. Popular in the late nineteenth century, electrohomeopathy has been described as "utter idiocy".
Homeoprophylaxis.
The use of homeopathy as a preventive for serious infectious diseases is especially controversial, in the context of ill-founded public alarm over the safety of vaccines stoked by the anti-vaccination movement. Promotion of homeopathic alternatives to vaccines has been characterised as dangerous, inappropriate and irresponsible. In December 2014, Australian homeopathy supplier Homeopathy Plus! were found to have acted deceptively in promoting homeopathic alternatives to vaccines.
Evidence and efficacy.
The low concentration of homeopathic remedies, which often lack even a single molecule of the diluted substance, has been the basis of questions about the effects of the remedies since the 19th century. Modern advocates of homeopathy have proposed a concept of "water memory", according to which water "remembers" the substances mixed in it, and transmits the effect of those substances when consumed. This concept is inconsistent with the current understanding of matter, and water memory has never been demonstrated to have any detectable effect, biological or otherwise. Pharmacological research has found instead that stronger effects of an active ingredient come from higher, not lower doses.
James Randi and the groups have highlighted the lack of active ingredients in most homeopathic products by taking large overdoses. None of the hundreds of demonstrators in the UK, Australia, New Zealand, Canada and the US were injured and "no one was cured of anything, either".
Outside of the alternative medicine community, scientists have long considered homeopathy a sham or a pseudoscience, and the mainstream medical community regards it as quackery. There is an overall absence of sound statistical evidence of therapeutic efficacy, which is consistent with the lack of any biologically plausible pharmacological agent or mechanism.
Abstract concepts within theoretical physics have been invoked to suggest explanations of how or why remedies might work, including quantum entanglement, quantum nonlocality, the theory of relativity and chaos theory. Contrariwise, quantum superposition has been invoked to explain why homeopathy does "not" work in the presence of double-blind trials. However, the explanations are offered by nonspecialists within the field, and often include speculations that are incorrect in their application of the concepts and not supported by actual experiments.:255–6 Several of the key concepts of homeopathy conflict with fundamental concepts of physics and chemistry. The use of quantum entanglement to explain homeopathy's purported effects is "patent nonsense", as entanglement is a delicate state which rarely lasts longer than a fraction of a second. While entanglement may result in certain aspects of individual subatomic particles acquiring linked quantum states, this does not mean the particles will mirror or duplicate each other, nor cause health-improving transformations.
Plausibility.
The proposed mechanisms for homeopathy are precluded from having any effect by the laws of physics and physical chemistry. The extreme dilutions used in homeopathic preparations usually leave none of the original substance in the final product.
A number of speculative mechanisms have been advanced to counter this, the most widely discussed being water memory, though this is now considered erroneous since short-range order in water only persists for about 1 picosecond. No evidence of stable clusters of water molecules was found when homeopathic remedies were studied using nuclear magnetic resonance. Existence of a pharmacological effect in the absence of any true active ingredient is inconsistent with the law of mass action and the observed dose-response relationships characteristic of therapeutic drugs (whereas placebo effects are non-specific and unrelated to pharmacological activity).
Homeopaths contend that "potentisation" produces a therapeutically active remedy, selectively including only the intended remedy, though critics note that water will have been in contact with millions of different substances throughout its history, and homeopaths have not been able to account for a reason why only the selected homeopathic remedy would be potentised in this process. For comparison, ISO 3696: 1987 defines a standard for water used in laboratory analysis; this allows for a contaminant level of ten parts per billion, 4C-5C in homeopathic notation. This water may not be kept in glass as contaminants will leach out into the water.
Practitioners of homeopathy hold that higher dilutions — described as being of higher "potency" — produce stronger medicinal effects. This idea is also inconsistent with observed dose-response relationships, where effects are dependent on the concentration of the active ingredient in the body. This dose-response relationship has been confirmed in myriad experiments on organisms as diverse as nematodes, rats, and humans. Some homeopaths contend that the phenomenon of hormesis may support the idea of dilution increasing potency, but the dose response relationship outside the zone of hormesis declines with dilution as normal, and nonlinear pharmacological effects do not provide any credible support for homeopathy.
Physicist Robert L. Park, former executive director of the American Physical Society, is quoted as saying,
"since the least amount of a substance in a solution is one molecule, a 30C solution would have to have at least one molecule of the original substance dissolved in a minimum of 1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000 [or 1060] molecules of water. This would require a container more than 30,000,000,000 times the size of the Earth."
Park is also quoted as saying that, "to expect to get even one molecule of the 'medicinal' substance allegedly present in 30X pills, it would be necessary to take some two billion of them, which would total about a thousand tons of lactose plus whatever impurities the lactose contained".
The laws of chemistry state that there is a limit to the dilution that can be made without losing the original substance altogether. This limit, which is related to Avogadro's number, is roughly equal to homeopathic potencies of 12C or 24X (1 part in 1024).
Scientific tests run by both the BBC's "Horizon" and ABC's "20/20" programs were unable to differentiate homeopathic dilutions from water, even when using tests suggested by homeopaths themselves.
Efficacy.
No individual preparation has been unambiguously shown by research to be different from placebo. The methodological quality of the primary research was generally low, with such problems as weaknesses in study design and reporting, small sample size, and selection bias. Since better quality trials have become available, the evidence for efficacy of homeopathy preparations has diminished; the highest-quality trials indicate that the remedies themselves exert no intrinsic effect.:206 A review conducted in 2010 of all the pertinent studies of "best evidence" produced by the Cochrane Collaboration concluded that "the most reliable evidence – that produced by Cochrane reviews – fails to demonstrate that homeopathic medicines have effects beyond placebo."
Government level reviews.
Government-level reviews have been conducted in recent years by Switzerland (2005), the United Kingdom (2009) and Australia (2015).
The Swiss "programme for the evaluation of complementary medicine" (PEK) resulted in the peer-reviewed Shang publication (see "Systematic reviews and meta-analyses of efficacy") and a controversial competing analysis by homeopaths and advocates led by Gudrun Bornhöft and Peter Matthiessen, which has been presented as a Swiss government report by homeopathy proponents, a claim that has been repudiated by the Swiss Federal Office of Public Health. The Swiss Government terminated reimbursement, though it was subsequently reinstated after a referendum for a further trial period.
The United Kingdom's House of Commons Science and Technology Committee sought written evidence and submissions from concerned parties and, following a review of all submissions, concluded that there was no compelling evidence of effect other than placebo and recommended that the Medicines and Healthcare products Regulatory Agency (MHRA) should not allow homeopathic product labels to make medical claims, that homeopathic products should no longer be licensed by the MHRA, as they are not medicines, and that further clinical trials of homeopathy could not be justified. They recommended that funding of homeopathic hospitals should not continue, and NHS doctors should not refer patients to homeopaths. The Secretary of State for Health deferred to local NHS on funding homeopathy, in the name of patient choice. By February 2011 only one third of primary care trusts still funded homeopathy. By 2012, no British universities offered homeopathy courses.
The Australian National Health and Medical Research Council completed a comprehensive review of the effectiveness of homeopathic remedies in 2015, in which it concluded that "there were no health conditions for which there was reliable evidence that homeopathy was effective. No good-quality, well-designed studies with enough participants for a meaningful result reported either that homeopathy caused greater health improvements than placebo, or caused health improvements equal to those of another treatment."
Publication bias and other methodological issues.
The fact that individual randomized controlled trials have given positive results is not in contradiction with an overall lack of statistical evidence of efficacy. A small proportion of randomized controlled trials inevitably provide false-positive outcomes due to the play of chance: a "statistically significant" positive outcome is commonly adjudicated when the probability of it being due to chance rather than a real effect is no more than 5%—a level at which about 1 in 20 tests can be expected to show a positive result in the absence of any therapeutic effect. Furthermore, trials of low methodological quality (i.e. ones which have been inappropriately designed, conducted or reported) are prone to give misleading results. In a systematic review of the methodological quality of randomized trials in three branches of alternative medicine, Linde et al. highlighted major weaknesses in the homeopathy sector, including poor randomization.
A related issue is publication bias: researchers are more likely to submit trials that report a positive finding for publication, and journals prefer to publish positive results. Publication bias has been particularly marked in alternative medicine journals, where few of the published articles (just 5% during the year 2000) tend to report null results. Regarding the way in which homeopathy is represented in the medical literature, a systematic review found signs of bias in the publications of clinical trials (towards negative representation in mainstream medical journals, and "vice versa" in alternative medicine journals), but not in reviews.
Positive results are much more likely to be false if the prior probability of the claim under test is low.
Systematic reviews and meta-analyses of efficacy.
Both meta-analyses, which statistically combine the results of several randomized controlled trials, and other systematic reviews of the literature are essential tools to summarize evidence of therapeutic efficacy. Early systematic reviews and meta-analyses of trials evaluating the efficacy of homeopathic remedies in comparison with placebo more often tended to generate positive results, but appeared unconvincing overall. In particular, reports of three large meta-analyses warned readers that firm conclusions could not be reached, largely due to methodological flaws in the primary studies and the difficulty in controlling for publication bias. The positive finding of one of the most prominent of the early meta-analyses, published in "The Lancet" in 1997 by Linde et al., was later reframed by the same research team, who wrote:
The evidence of bias [in the primary studies] weakens the findings of our original meta-analysis. Since we completed our literature search in 1995, a considerable number of new homeopathy trials have been published. The fact that a number of the new high-quality trials ... have negative results, and a recent update of our review for the most "original" subtype of homeopathy (classical or individualized homeopathy), seem to confirm the finding that more rigorous trials have less-promising results. It seems, therefore, likely that our meta-analysis at least overestimated the effects of homeopathic treatments.
Subsequent work by John Ioannidis and others has shown that for treatments with no prior plausibility, the chances of a positive result being a false positive are much higher, and that any result not consistent with the null hypothesis should be assumed to be a false positive.
In 2002, a systematic review of the available systematic reviews confirmed that higher-quality trials tended to have less positive results, and found no convincing evidence that any homeopathic remedy exerts clinical effects different from placebo.
In 2005, "The Lancet" medical journal published a meta-analysis of 110 placebo-controlled homeopathy trials and 110 matched medical trials based upon the Swiss government's Program for Evaluating Complementary Medicine, or PEK. The study concluded that its findings were "compatible with the notion that the clinical effects of homeopathy are placebo effects". This was accompanied by an editorial pronouncing "The end of homoeopathy", which was denounced by the homeopath Peter Fisher.
Other meta-analyses include homeopathic treatments to reduce cancer therapy side-effects following radiotherapy and chemotherapy, allergic rhinitis, attention-deficit hyperactivity disorder and childhood diarrhea, adenoid vegetation, asthma, upper respiratory tract infection in children, insomnia, fibromyalgia, psychiatric conditions and Cochrane Library reviews of homeopathic treatments for asthma, dementia, attention-deficit hyperactivity disorder, and induction of labor. Other reviews covered osteoarthritis, migraines delayed-onset muscle soreness, or eczema and other dermatological conditions.
The results of these reviews are generally negative or only weakly positive, and reviewers consistently report the poor quality of trials. The finding of Linde "et. al." that more rigorous studies produce less positive results is supported in several and contradicted by none.
Some clinical trials have tested individualized homeopathy, and there have been reviews of this, specifically. A 1998 review found 32 trials that met their inclusion criteria, 19 of which were placebo-controlled and provided enough data for meta-analysis. These 19 studies showed a pooled odds ratio of 1.17 to 2.23 in favor of individualized homeopathy over the placebo, but no difference was seen when the analysis was restricted to the methodologically best trials. The authors concluded that "the results of the available randomized trials suggest that individualized homeopathy has an effect over placebo. The evidence, however, is not convincing because of methodological shortcomings and inconsistencies." Jay Shelton, author of a book on homeopathy, has stated that the claim assumes without evidence that classical, individualized homeopathy works better than nonclassical variations.:209 A systematic review and meta-analysis of trials of individualised homeopathy published in December 2014 concluded that individualised homeopathy may have small effects, but that caution was needed in interpreting the results because of study quality issues - no study included was assessed as being at low risk of bias.
Statements by major medical organisations.
Health organisations such as the UK's National Health Service, the American Medical Association, the FASEB, and the National Health and Medical Research Council of Australia, have issued statements of their conclusion that there is "no good-quality evidence that homeopathy is effective as a treatment for any health condition". In 2009, World Health Organization official Mario Raviglione cricitized the use of homeopathy to treat tuberculosis; similarly, another WHO spokesperson argued there was no evidence homeopathy would be an effective treatment for diarrhea.
The American College of Medical Toxicology and the American Academy of Clinical Toxicology recommend that no one use homeopathic treatment for disease or as a preventive health measure. These organizations report that no evidence exists that homeopathic treatment is effective, but that there is evidence that using these treatments produces harm and can bring indirect health risks by delaying conventional treatment.
Explanations of perceived effects.
Science offers a variety of explanations for how homeopathy may appear to cure diseases or alleviate symptoms even though the remedies themselves are inert::155–167
Purported effects in other biological systems.
While some articles have suggested that homeopathic solutions of high dilution can have statistically significant effects on organic processes including the growth of grain, histamine release by leukocytes, and enzyme reactions, such evidence is disputed since attempts to replicate them have failed. A 2007 systematic review of high-dilution experiments found that none of the experiments with positive results could be reproduced by all investigators.
In 1987, French immunologist Jacques Benveniste submitted a paper to the journal "Nature" while working at INSERM. The paper purported to have discovered that basophils, a type of white blood cell, released histamine when exposed to a homeopathic dilution of anti-immunoglobulin E antibody. The journal editors, skeptical of the results, requested that the study be replicated in a separate laboratory. Upon replication in four separate laboratories the study was published. Still sceptical of the findings, "Nature" assembled an independent investigative team to determine the accuracy of the research, consisting of "Nature" editor and physicist Sir John Maddox, American scientific fraud investigator and chemist Walter Stewart, and sceptic James Randi. After investigating the findings and methodology of the experiment, the team found that the experiments were "statistically ill-controlled", "interpretation has been clouded by the exclusion of measurements in conflict with the claim", and concluded, "We believe that experimental data have been uncritically assessed and their imperfections inadequately reported." James Randi stated that he doubted that there had been any conscious fraud, but that the researchers had allowed "wishful thinking" to influence their interpretation of the data.
In 2001 and 2004, Madeleine Ennis published a number of studies which reported that homeopathic dilutions of histamine exerted an effect on the activity of basophils. In response to the first of these studies, "Horizon" aired a program in which British scientists attempted to replicate Ennis' results; they were unable to do so. 
Ethics and safety.
The provision of homeopathic remedies has been described as unethical. Michael Baum, Professor Emeritus of Surgery and visiting Professor of Medical Humanities at University College London (UCL), has described homoeopathy as a "cruel deception".
Edzard Ernst, the first "Professor of Complementary Medicine" in the United Kingdom and a former homeopathic practitioner, has expressed his concerns about pharmacists who violate their ethical code by failing to provide customers with "necessary and relevant information" about the true nature of the homeopathic products they advertise and sell:
Patients who choose to use homeopathy rather than evidence-based medicine risk missing timely diagnosis and effective treatment of serious conditions such as cancer.
In 2013 the UK Advertising Standards Authority concluded that the Society of Homeopaths were targeting vulnerable ill people and discouraging the use of essential medical treatment while making misleading claims of efficacy for homeopathic products.
Adverse reactions.
Some homeopathic remedies involve poisons such as Belladonna, arsenic, and poison ivy which are highly diluted in the homeopathic remedy. Only in rare cases are the original ingredients present at detectable levels. This may be due to improper preparation or intentional low dilution. Serious adverse effects such as seizures and death have been reported or associated with some homeopathic remedies. Instances of arsenic poisoning have occurred after use of arsenic-containing homeopathic preparations. Zicam Cold remedy Nasal Gel, which contains 2X (1:100) zinc gluconate, reportedly caused a small percentage of users to lose their sense of smell; 340 cases were settled out of court in 2006 for 12 million U.S. dollars. In 2009, the FDA advised consumers to stop using three discontinued cold remedy Zicam products because it could cause permanent damage to users' sense of smell. Zicam was launched without a New Drug Application (NDA) under a provision in the FDA's Compliance Policy Guide called "Conditions Under Which Homeopathic Drugs May be Marketed" (CPG 7132.15), but the FDA warned Matrixx Initiatives, its manufacture, via a Warning Letter that this policy does not apply when there is a health risk to consumers.
A 2000 review reported that homeopathic remedies are "unlikely to provoke severe adverse reactions". In 2012, a systematic review evaluating evidence of homeopathy's possible adverse effects concluded that "homeopathy has the potential to harm patients and consumers in both direct and indirect ways". One of the reviewers, Edzard Ernst, supplemented the article on his blog, writing: "I have said it often and I say it again: if used as an alternative to an effective cure, even the most 'harmless' treatment can become life-threatening."
Lack of efficacy.
The lack of convincing scientific evidence supporting its efficacy and its use of remedies without active ingredients have led to characterizations as pseudoscience and quackery, or, in the words of a 1998 medical review, "placebo therapy at best and quackery at worst". The Chief Medical Officer for England, Dame Sally Davies, has stated that homeopathic remedies are "rubbish" and do not serve as anything more than placebos. Jack Killen, acting deputy director of the National Center for Complementary and Alternative Medicine, says homeopathy "goes beyond current understanding of chemistry and physics". He adds: "There is, to my knowledge, no condition for which homeopathy has been proven to be an effective treatment." Ben Goldacre says that homeopaths who misrepresent scientific evidence to a scientifically illiterate public, have "...walled themselves off from academic medicine, and critique has been all too often met with avoidance rather than argument". Homeopaths often prefer to ignore meta-analyses in favour of cherry picked positive results, such as by promoting a particular observational study (one which Goldacre describes as "little more than a customer-satisfaction survey") as if it were more informative than a series of randomized controlled trials.
Referring specifically to homeopathy, the British House of Commons Science and Technology Committee has stated:
In our view, the systematic reviews and meta-analyses conclusively demonstrate that homeopathic products perform no better than placebos. The Government shares our interpretation of the evidence.
In the Committee's view, homeopathy is a placebo treatment and the Government should have a policy on prescribing placebos. The Government is reluctant to address the appropriateness and ethics of prescribing placebos to patients, which usually relies on some degree of patient deception. Prescribing of placebos is not consistent with informed patient choice - which the Government claims is very important - as it means patients do not have all the information needed to make choice meaningful.
Beyond ethical issues and the integrity of the doctor-patient relationship, prescribing pure placebos is bad medicine. Their effect is unreliable and unpredictable and cannot form the sole basis of any treatment on the NHS.
The National Center for Complementary and Alternative Medicine of the United States' National Institutes of Health states:
Homeopathy is a controversial topic in complementary medicine research. A number of the key concepts of homeopathy are not consistent with fundamental concepts of chemistry and physics. For example, it is not possible to explain in scientific terms how a remedy containing little or no active ingredient can have any effect. This, in turn, creates major challenges to rigorous clinical investigation of homeopathic remedies. For example, one cannot confirm that an extremely dilute remedy contains what is listed on the label, or develop objective measures that show effects of extremely dilute remedies in the human body.
In lieu of standard medical treatment.
On clinical grounds, patients who choose to use homeopathy in preference to normal medicine risk missing timely diagnosis and effective treatment, thereby worsening the outcomes of serious conditions. Critics of homeopathy have cited individual cases of patients of homeopathy failing to receive proper treatment for diseases that could have been easily diagnosed and managed with conventional medicine and who have died as a result and the "marketing practice" of criticizing and downplaying the effectiveness of mainstream medicine. Homeopaths claim that use of conventional medicines will "push the disease deeper" and cause more serious conditions, a process referred to as "suppression". Some homeopaths (particularly those who are non-physicians) advise their patients against immunisation. Some homeopaths suggest that vaccines be replaced with homeopathic "nosodes", created from biological materials such as pus, diseased tissue, bacilli from sputum or (in the case of "bowel nosodes") feces. While Hahnemann was opposed to such preparations, modern homeopaths often use them although there is no evidence to indicate they have any beneficial effects. Cases of homeopaths advising against the use of anti-malarial drugs have been identified. This puts visitors to the tropics who take this advice in severe danger, since homeopathic remedies are completely ineffective against the malaria parasite. Also, in one case in 2004, a homeopath instructed one of her patients to stop taking conventional medication for a heart condition, advising her on 22 June 2004 to "Stop ALL medications including homeopathic", advising her on or around 20 August that she no longer needed to take her heart medication, and adding on 23 August, "She just cannot take ANY drugs – I have suggested some homeopathic remedies ... I feel confident that if she follows the advice she will regain her health." The patient was admitted to hospital the next day, and died eight days later, the final diagnosis being "acute heart failure due to treatment discontinuation".
In 1978, Anthony Campbell, then a consultant physician at the Royal London Homeopathic Hospital, criticised statements by George Vithoulkas claiming that syphilis, when treated with antibiotics, would develop into secondary and tertiary syphilis with involvement of the central nervous system, saying that "The unfortunate layman might well be misled by Vithoulkas' rhetoric into refusing orthodox treatment".
Vithoulkas' claims echo the idea that treating a disease with external medication used to treat the symptoms would only drive it deeper into the body and conflict with scientific studies, which indicate that penicillin treatment produces a complete cure of syphilis in more than 90% of cases.
A 2006 review by W. Steven Pray of the College of Pharmacy at Southwestern Oklahoma State University recommends that pharmacy colleges include a required course in unproven medications and therapies, that ethical dilemmas inherent in recommending products lacking proven safety and efficacy data be discussed, and that students should be taught where unproven systems such as homeopathy depart from evidence-based medicine.
In an article entitled "Should We Maintain an Open Mind about Homeopathy?" published in the "American Journal of Medicine", Michael Baum and Edzard Ernst – writing to other physicians – wrote that "Homeopathy is among the worst examples of faith-based medicine... These axioms [of homeopathy] are not only out of line with scientific facts but also directly opposed to them. If homeopathy is correct, much of physics, chemistry, and pharmacology must be incorrect...".
In 2013, Sir Mark Walport, the new UK Government Chief Scientific Adviser and head of the Government Office for Science, had this to say about homeopathy: "My view scientifically is absolutely clear: homoeopathy is nonsense, it is non-science. My advice to ministers is clear: that there is no science in homoeopathy. The most it can have is a placebo effect – it is then a political decision whether they spend money on it or not." His predecessor, Professor Sir John Beddington, referring to his views on homeopathy being "fundamentally ignored" by the Government, said: "The only one [view being ignored] I could think of was homoeopathy, which is mad. It has no underpinning of scientific basis. In fact all the science points to the fact that it is not at all sensible. The clear evidence is saying this is wrong, but homoeopathy is still used on the NHS."
Regulation and prevalence.
Homeopathy is fairly common in some countries while being uncommon in others; is highly regulated in some countries and mostly unregulated in others. It is practised worldwide and professional qualifications and licences are needed in most countries. In some countries, there are no specific legal regulations concerning the use of homeopathy, while in others, licences or degrees in conventional medicine from accredited universities are required. In Germany, to become a homeopathic physician, one must attend a three-year training program, while France, Austria and Denmark mandate licences to diagnose any illness or dispense of any product whose purpose is to treat any illness.
Some homeopathic treatment is covered by the public health service of several European countries, including France, the United Kingdom, Denmark, and Luxembourg. In other countries, such as Belgium, homeopathy is not covered. In Austria, the public health service requires scientific proof of effectiveness in order to reimburse medical treatments and homeopathy is listed as not reimbursable, but exceptions can be made; private health insurance policies sometimes include homeopathic treatment. The Swiss government, after a 5-year trial, withdrew coverage of homeopathy and four other complementary treatments in 2005, stating that they did not meet efficacy and cost-effectiveness criteria, but following a referendum in 2009 the five therapies have been reinstated for a further 6-year trial period from 2012.
The Indian government recognises homeopathy as one of its national systems of medicine; it has established AYUSH or the Department of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homoeopathy under the Ministry of Health & Family Welfare. The Central Council of Homoeopathy was established in 1973 to monitor higher education in homeopathy, and National Institute of Homoeopathy in 1975. A minimum of a recognised diploma in homeopathy and registration on a state register or the Central Register of Homoeopathy is required to practice homeopathy in India.
Public opposition.
In the April 1997 edition of FDA Consumer, Dr. William Jarvis, the President of the National Council Against Health Fraud said "Homeopathy is a fraud perpetrated on the public with the government's blessing, thanks to the abuse of political power of Sen. Royal S. Copeland [chief sponsor of the 1938 Food, Drug, and Cosmetic Act]."
Mock "overdosing" on homeopathic preparations by individuals or groups in "mass suicides" have become more popular since James Randi began taking entire bottles of homeopathic sleeping pills before giving lectures. In 2010 The Merseyside Skeptics Society from the United Kingdom launched the encouraging groups to publicly overdose as groups. In 2011 the 10:23 campaign expanded and saw sixty-nine groups participate, fifty-four submitted videos. In April 2012, at the Berkeley SkeptiCal conference, over 100 people participated in a mass overdose, taking "coffea cruda" which is supposed to treat sleeplessness.
The non-profit, educational organizations Center for Inquiry (CFI) and the associated Committee for Skeptical Inquiry (CSI) have petitioned the U.S. Food and Drug Administration (FDA), criticizing Boiron for misleading labeling and advertising of Oscillococcinum. CFI in Canada is calling for persons that feel they were harmed by homeopathic products to contact them.
In August 2011, a class action lawsuit was filed against Boiron on behalf of "all California residents who purchased Oscillo at any time within the past four years". The lawsuit charged that it "is nothing more than a sugar pill", "despite falsely advertising that it contains an active ingredient known to treat flu symptoms". In March 2012, Bioron agreed to spend up to $12 million to settle the claims of falsely advertising the benefits of its homeopathic remedies.
In July 2012, CBC News reporter Erica Johnson for "Marketplace" conducted an investigation on the homeopathy industry in Canada; her findings were that it is "based on flawed science and some loopy thinking". Center for Inquiry (CFI) Vancouver skeptics participated in a mass overdose outside an emergency room in Vancouver, B.C., taking entire bottles of "medications" that should have made them sleepy, nauseous or dead, after 45 minutes of observation no ill effects were felt. Johnson asked homeopaths and company representatives about cures for cancer and vaccine claims. All reported positive results but none could offer any science backing up their statements, only that "it works". Johnson was unable to find any evidence that homeopathic preparations contain any active ingredient. Analysis performed at the University of Toronto's chemistry department found that the active ingredient is so small "it is equivalent to 5 billion times less than the amount of aspirin... in a single pellet". Belladonna and ipecac "would be indistinguishable from each other in a blind test".

</doc>
<doc id="14231" url="http://en.wikipedia.org/wiki?curid=14231" title="Hairpin (fashion)">
Hairpin (fashion)

A hair pin or hairpin is a long device used to hold a person's hair in place. It may be used simply to secure long hair out of the way for convenience or as part of an elaborate hairstyle or coiffure. The earliest evidence for dressing the hair may be seen in carved "venus figurines" such as the Venus of Brassempouy and the Venus of Willendorf. The creation of different hairstyles, especially among women, seems to be common to all cultures and all periods and many past, and current, societies use hairpins.
Hairpins made of metal, ivory, bronze, carved wood, etc. were used in ancient Assyria and Egypt for securing decorated hairstyles. Such hairpins suggest, as graves show, that many were luxury objects among the Egyptians and later the Greeks, Etruscans, and Romans. Major success came in 1901 with the invention of the spiral hairpin by New Zealand inventor Ernest Godward. This was a predecessor of the hair clip.
The hairpin may be decorative and encrusted with jewels and ornaments, or it may be utiliarian, and designed to be almost invisible while holding a hairstyle in place. 
Some hairpins are a single straight pin, but modern versions are more likely to be constructed from different lengths of wire that are bent in half with a u-shaped end and a few kinks along the two opposite portions. The finished pin may vary from two to six inches in final length. The length of the wires enables placement in several styles of hairdos to hold the style in place. The kinks enable retaining the pin during normal movements. 
A hairpin patent was issued to Kelly Chamandy in 1925.
Hairpins in Chinese Culture.
Hairpins (generally known as fa-zan; Chinese: 髮簪）are an important symbol in Chinese culture. In ancient China, hairpins were worn by all genders, and they were essential items for everyday hairstyling, mainly for securing and decorating a hair bun. Furthermore, hairpins worn by women could also represent their social status. 
In Han Chinese culture, when young girls reached the age of fifteen, they were allowed to take part in a rite of passage known as "Ji Li" (Chinese: 筓禮), or “hairpin initiation” . This ceremony marks the coming of age of young women. Particularly, before the age of fifteen, girls did not use hairpins as they wore their hair in braids, and they were considered as children. When they turned fifteen, they could be considered as young women after the ceremony, and they started to style their hair as buns secured and embellished by hairpins. This practice indicated these young women may now enter into marriage. However, if a young woman hadn't been consented to marriage before age twenty, or she hadn't yet participated in a coming of age ceremony, she must attend a ceremony when she turned twenty. 
In comparison with “Ji Li”, the male equivalent known as “guan li” (Chinese: 冠禮) or “hat initiation”, usually took place five years later, at the age of twenty. In the 21st century Hanfu Movement, an attempt to revive the traditional Han Chinese coming-of-age ceremonies has been made, and the ideal age to attend the ceremony is twenty years old for all genders.
While hairpins can symbolize the transition from childhood to adulthood, they were closely connected to the concept of marriage as well. At the time of an engagement, the fiancée may take a hairpin from her hair and give it to her fiancé as a pledge: this can be seen as a reversal of the Western tradition, such as the future groom presents an engagement ring to his betrothed. After the wedding ceremony, the husband should put the hairpin back into his spouse’s hair. 
Hair has always carried many psychological, philosophical, romantic, and cultural meanings in Chinese culture. In Han ethnicity, people call the union between two people “jie-fa” (Chinese: 結髮), literally means “tying hair”. During the wedding ceremony, some Chinese couples exchange a lock of hair as a pledge, while others break a hairpin into two parts, and then, each of the betrothed take one part with them for keeping. If this couple ever get separated in the future, when they reunite, they can piece the two halves together, and this completed hairpin will serve as a proof of their identities as well as a symbol of their reunion. In addition, a married heterosexual couple is sometimes referred to as “jie-fa fu-qi” （Chinese：結髮夫妻), an idiom which implies the relationship between the pair is very intimate and happy, just like how their hair has been tied together.

</doc>
<doc id="14233" url="http://en.wikipedia.org/wiki?curid=14233" title="Hate speech">
Hate speech

Hate speech is, outside the law, speech that attacks a person or group on the basis of attributes such as gender, ethnic origin, religion, race, disability, or sexual orientation.
In law, hate speech is any speech, gesture or conduct, writing, or display which is forbidden because it may incite violence or prejudicial action against or by a protected individual or group, or because it disparages or intimidates a protected individual or group. The law may identify a protected individual or a protected group by certain characteristics.
In some countries, a victim of hate speech may seek redress under civil law, criminal law, or both. A website that uses hate speech is called a "hate site". Most of these sites contain Internet forums and news briefs that emphasize a particular viewpoint. There has been debate over how freedom of speech applies to the Internet as well as hate speech in general.
Critics have argued that the term "hate speech" is a contemporary example of Newspeak, used to silence critics of social policies that have been poorly implemented in a rush to appear politically correct.
International.
The International Covenant on Civil and Political Rights (ICCPR) states that "any advocacy of national, racial or religious hatred that constitutes incitement to discrimination, hostility or violence shall be prohibited by law". The Convention on the Elimination of All Forms of Racial Discrimination (ICERD) prohibits all incitement of racism. On 3 May 2011, Michael O'Flaherty with the United Nations Human Rights Committee published General Comment No. 34 on the ICCPR, which among other comments expresses concern that many forms of "hate speech" do not meet the level of seriousness set out in Article 20. Concerning the debate over how freedom of speech applies to the Internet, conferences concerning such sites have been sponsored by the United Nations High Commissioner for Refugees.
Enforcement of hate speech laws.
Hate law regulations can be divided into two types: those that are designed for public order and those that are designed to protect human dignity. Those designed to protect public order seem to be somewhat ineffective because they are rarely enforced. For example, in Northern Ireland, as of 1992 only one person was prosecuted for violating the regulation in twenty one years. Those meant to protect human dignity, however, like those in Canada, Denmark, France, Germany and the Netherlands seem to be frequently enforced.
Harm of hate speech.
Communication theory provides some insight into the harms caused by hate speech. According to the ritual model of communication, racist expressions allow minorities to be categorized with negative attributes tied to them, and are directly harmful to them. Matsuda "et al". (1993) found that racist speech could cause in the recipient of the message direct physical and emotional changes. The repeated use of such expressions cause and reinforce the subordination of these minorities. This has been enough to sway the court in previous cases such as Brown v. Board of Education in USA, in which the Court stated that segregation "generates a feeling of inferiority as to their [AfricanAmericans’] status in the community that may affect their hearts and minds in a way unlikely ever to be undone." The idea that hate speech is a mechanism of subordination is supported by scholarly evidence.
Hate speech on Facebook.
Following a campaign that involved the participation of Women, Action and the Media, the Everyday Sexism Project and the activist Soraya Chemaly, who were among 100 advocacy groups, Facebook agreed to update its policy on hate speech. The campaign highlighted content that promoted domestic and sexual violence against women, and used over 57,000 tweets and more than 4,900 emails to create outcomes such as the withdrawal of advertising from Facebook by 15 companies, including Nissan UK, House of Burlesque, and Nationwide UK. The social media website initially responded by stating that "While it may be vulgar and offensive, distasteful content on its own does not violate our policies", but then agreed to take action on May 29, 2013, after it had "become clear that our systems to identify and remove hate speech have failed to work as effectively as we would like, particularly around issues of gender-based hate."
By country.
Australia.
Australia's hate speech laws vary by jurisdiction, and seek especially to prevent victimisation on account of race.
Belgium.
The Belgian Anti-Racism Law, in full, the "Law of 30 July 1981 on the Punishment of Certain Acts inspired by Racism or Xenophobia", is a law against hate speech and discrimination passed by the Federal Parliament of Belgium in 1981 which made certain acts motivated by racism or xenophobia illegal. It is also known as the "Moureaux Law".
The Belgian Holocaust denial law, passed on 23 March 1995, bans public Holocaust denial. Specifically, the law makes it illegal to publicly "deny, play down, justify or approve of the genocide committed by the German National Socialist regime during the Second World War". Prosecution is led by the Belgian Centre for Equal Opportunities. The offense is punishable by imprisonment of up to one year and fines of up to 2500 EUR.
Brazil.
In Brazil, according to the 1988 Brazilian Constitution, racism and other forms of race-related hate speech are "imprescriptible crime(s) with no right to bail to its accused".
Canada.
In Canada, advocating genocide or inciting hatred against any "identifiable group" is an indictable offence under the Criminal Code of Canada with maximum prison terms of two to fourteen years. An 'identifiable group' is defined as "any section of the public distinguished by colour, race, religion, ethnic origin or sexual orientation". It makes exceptions for cases of statements of truth, and subjects of public debate and religious doctrine. The landmark judicial decision on the constitutionality of this law was "R. v. Keegstra" (1990).
Chile.
Article 31 of the "Ley sobre Libertades de Opinión e Información y Ejercicio del Periodismo" (statute on freedom of opinion and information and the performance of journalism), punishes with a high fine those who “through any means of social communication makes publications or transmissions intended to promote hatred or hostility towards persons or a group of persons due to their race, sex, religion or nationality". This norm has been applied to expressions proffered through the internet. There is also a rule aggravating the penalties of crimes when they are motivated by discriminatory hatred.
Council of Europe.
The Council of Europe has worked intensively on this issue. While Article 10 of the European Convention on Human Rights does not prohibit criminal laws against revisionism such as denial or minimization of genocides or crimes against humanity, as interpreted by the European Court of Human Rights (ECtHR), the Committee of Ministers of the Council of Europe went further and recommended to member governments to combat hate speech under its Recommendation R (97) 20. The ECtHR does not offer an accepted definition for "hate speech" but instead offers only parameters by which prosecutors can decide if the "hate speech" is entitled to the protection of freedom of speech.
The Council of Europe also created the European Commission against Racism and Intolerance, which has produced country reports and several general policy recommendations, for instance against anti-Semitism and intolerance against Muslims.
Croatia.
The Croatian Constitution guarantees freedom of speech, but Croatian penal code prohibits and punishes anyone "who based on differences of race, religion, language, political or any other belief, wealth, birth, education, social status or other properties, gender, skin color, nationality or ethnicity violates basic human rights and freedoms recognized from international community".
Denmark.
Denmark prohibits hate speech, and defines it as publicly making statements by which a group is threatened ("trues"), insulted ("forhånes") or degraded ("nedværdiges") due to race, skin colour, national or ethnic origin, faith or sexual orientation.
Finland.
There has been considerable debate over the definition of "hate speech" ("vihapuhe") in the Finnish language.
If "hate speech" is taken to mean ethnic agitation, it is prohibited in Finland and defined in the section 11 of the penal code, "War crimes and crimes against humanity", as publishing data, an opinion or other statement that threatens or insults a group on basis of race, nationality, ethnicity, religion or conviction, sexual orientation, disability, or any comparable basis. Ethnic agitation is punishable with a fine or up to 2 years in prison, or 4 months to 4 years if aggravated (such as incitement to genocide).
Critics claim that, in political contexts, labeling certain opinions and statements "hate speech" can be used to silence unfavorable or critical opinions and play down debate. Certain politicians, including Member of Parliament Jussi Halla-aho, consider the term "hate speech" problematic because of the lack of an easy definition.
France.
France prohibits by its penal code and by its press laws public and private communication which is defamatory or insulting, or which incites discrimination, hatred, or violence against a person or a group of persons on account of place of origin, ethnicity or lack thereof, nationality, race, specific religion, sex, sexual orientation, or handicap. The law prohibits declarations that justify or deny crimes against humanity, for example, the Holocaust (Gayssot Act).
Germany.
In Germany, Volksverhetzung ("incitement of popular hatred") is a punishable offense under Section 130 of the Strafgesetzbuch (Germany's criminal code) and can lead to up to five years imprisonment. Section 130 makes it a crime to publicly incite hatred against parts of the population or to call for violent or arbitrary measures against them or to insult, maliciously slur or defame them in a manner violating their (constitutionally protected) human dignity. Thus for instance it is illegal to publicly call certain ethnic groups "maggots" or "freeloaders". Volksverhetzung is punishable in Germany even if committed abroad and even if committed by non-German citizens, if only the incitement of hatred takes effect within German territory, e.g., the seditious sentiment was expressed in German writ or speech and made accessible in Germany (German criminal code's Principle of Ubiquity, Section 9 §1 Alt. 3 and 4 of the Strafgesetzbuch).
Iceland.
In Iceland, the hate speech law is not confined to inciting hatred, as one can see from Article 233 a. in the Icelandic Penal Code, but includes simply expressing such hatred publicly:
India.
Freedom of speech and expression is protected by article 19 (1) of the constitution of India, but under article 19(2) "reasonable restrictions" can be imposed on freedom of speech and expression in the interest of "the sovereignty and integrity of India, the security of the State, friendly relations with foreign States, public order, decency or morality, or in relation to contempt of court, defamation or incitement to an offence".
Indonesia.
Indonesia has been a signatory to the International Covenant on Civil and Political Rights since 2006, but has not promulgated comprehensive legislation against hate-speech crimes. Calls for a comprehensive anti-hate-speech law and associated educational program have followed statements by a leader of a hard-line Islamic organization that Balinese Hindus were mustering forces to protect the "lascivious Miss World pageant" in “a war against Islam" and that "those who fight on the path of Allah are promised heaven". The statements are said to be an example of similar messages of hatred and intolerance being preached in mosques throughout the country by fundamentalist clerics.
Ireland.
In Ireland, the right to free speech is guaranteed under the Constitution (Article 40.6.1.i), however, this is only an implied right provided that liberty of expression "shall not be used to undermine public order or morality or the authority of the State". The Prohibition of Incitement to Hatred Act 1989, proscribes words or behaviours which are "threatening, abusive or insulting and are intended or, having regard to all the circumstances, are likely to stir up hatred" against "a group of persons in the State or elsewhere on account of their race, colour, nationality, religion, ethnic or national origins, membership of the travelling community or sexual orientation".
Japan.
Japanese law covers threats and slander, but it "does not apply to hate speech against general groups of people". Japan became a member of the United Nations International Convention on the Elimination of All Forms of Racial Discrimination in 1995. Article 4 of the convention sets forth provisions calling for the criminalization of hate speech. But the Japanese government has suspended the provisions, saying actions to spread or promote the idea of racial discrimination have not been taken in Japan to such an extent that legal action is necessary. The Foreign Ministry says that this assessment remains unchanged.
In May 2013, the United Nations Committee on Economic, Social and Cultural Rights (CESCR) warned the Japanese government that it needs to take measures to curb hate speech against so-called "comfort women", or Asian women forced into sexual slavery by the Japanese military during World War II. The committee's recommendation called for the Japanese government to better educate Japanese society on the plight of women who were forced into sexual slavery to prevent stigmatization, and to take necessary measures to repair the lasting effects of exploitation, including addressing their right to compensation.
In 2013, following demonstrations, parades, and comments posted on the Internet threatening violence against foreign residents of Japan, especially Koreans, there are concerns that hate speech is a growing problem in Japan. Prime Minister Shinzo Abe and Justice Minister Sadakazu Tanigaki have expressed concerns about the raise in hate speech, saying that it "goes completely against the nation's dignity", but so far have stopped short of proposing any legal action against protesters.
On 22 September 2013 around 2,000 people participated in the "March on Tokyo for Freedom" campaigning against recent hate speech marches. Participants called on the Japanese government to "sincerely adhere" to the International Convention on the Elimination of All Forms of Racial Discrimination. Sexual minorities and the disabled also participated in the march.
On 25 September 2013 a new organization, "An international network overcoming hate speech and racism" (Norikoenet), that is opposed to hate speech against ethnic Koreans and other minorities in Japan was launched.
On 7 October 2013, in a rare ruling on racial discrimination against ethnic Koreans, a Japanese court ordered an anti-Korean group, Zaitokukai, to stop "hate speech" protests against a Korean school in Kyoto and pay the school 12.26 million yen ($126,400 U.S.) in compensation for protests that took place in 2009 and 2010.
A United Nations panel urged Japan to ban hate speech.
Jordan.
Several Jordanian laws seek to prevent the publication or dissemination of material that would provoke strife or hatred: 
Netherlands.
The Dutch penal code prohibits both insulting a group (article 137c) and inciting hatred, discrimination or violence (article 137d). The definition of the offences as outlined in the penal code is as follows:
In January 2009, a court in Amsterdam ordered the prosecution of Geert Wilders, a Dutch Member of Parliament, for breaching articles 137c and 137d. On 23 June 2011, Wilders was acquitted of all charges.
New Zealand.
New Zealand prohibits hate speech under the Human Rights Act 1993. Section 61 (Racial Disharmony) makes it unlawful to publish or distribute "threatening, abusive, or insulting...matter or words likely to excite hostility against or bring into contempt any group of persons...on the ground of the colour, race, or ethnic or national or ethnic origins of that group of persons". Section 131 (Inciting Racial Disharmony) lists offences for which "racial disharmony" creates liability.
Norway.
Norway prohibits hate speech, and defines it as publicly making statements that threaten or ridicule someone or that incite hatred, persecution or contempt for someone due to their skin colour, ethnic origin, homosexual orientation, religion or philosophy of life. At the same time, the Norwegian Constitution guarantees the right to free speech, and there has been an ongoing public and judicial debate over where the right balance between the ban against hate speech and the right to free speech lies. Norwegian courts have been restrictive in the use of the hate speech law and only few persons have been sentenced for violating the law since its implementation in 1970. A public Free Speech committee (1996-1999) recommended to abolish the hate speech law but the Norwegian Parliament instead voted to slightly strengthen it.
Poland.
The hate speech laws in Poland punish those who offend the feelings of the religious by e.g. disturbing a religious ceremony or creating public calumny. They also prohibit public expression that insults a person or a group on account of national, ethnic, racial, or religious affiliation or the lack of a religious affiliation.
Serbia.
The Serbian constitution guarantees freedom of speech, but restricts it in certain cases to protect human rights. The criminal charge of "Provoking ethnic, racial and religion based animosity and intolerance" carries a minimum six months prison term and a maximum of ten years.
Singapore.
Singapore has passed numerous laws that prohibit speech that causes disharmony among various religious groups. The Maintenance of Religious Harmony Act is an example of such legislation. The Penal Code criminalizes the deliberate promotion by someone of enmity, hatred or ill-will between different racial and religious groups on grounds of race or religion. It also makes it an offence for anyone to deliberately wound the religious or racial feelings of any person.
South Africa.
In South Africa, hate speech (along with incitement to violence and propaganda for war) is specifically excluded from protection of free speech in the Constitution. The Promotion of Equality and Prevention of Unfair Discrimination Act, 2000 contains the following clause:
[N]o person may publish, propagate, advocate or communicate words based on one or more of the prohibited grounds, against any person, that could reasonably be construed to demonstrate a clear intention to―
The "prohibited grounds" include race, gender, sex, pregnancy, marital status, ethnic or social origin, colour, sexual orientation, age, disability, religion, conscience, belief, culture, language and birth.
The crime of "crimen injuria" ("unlawfully, intentionally and seriously impairing the dignity of another") may also be used to prosecute hate speech.
In 2011, a South African court banned "Dubula iBhunu (Shoot the Boer)", a derogatory song degrading Afrikaners, on the basis that it violated a South African law prohibiting speech that demonstrates a clear intention to be hurtful, to incite harm, or to promote hatred.
Sweden.
Sweden prohibits hate speech, and defines it as publicly making statements that threaten or express disrespect for an ethnic group or similar group regarding their race, skin colour, national or ethnic origin, faith, or sexual orientation. The crime does not prohibit a pertinent and responsible debate ("en saklig och vederhäftig diskussion"), nor statements made in a completely private sphere. There are constitutional restrictions pertaining to which acts are criminalized, as well limits set by the European Convention on Human Rights. The crime is called "Hets mot folkgrupp" in Swedish which directly translated can be translated to "Incitement (of hatred/violence) towards population groups."
The sexual orientation provision, added in 2002, was used to convict Pentecostalist pastor Åke Green of hate speech based on a 2003 sermon. His conviction was later overturned.
Switzerland.
In Switzerland public discrimination or invoking to rancor against persons or a group of people because of their race, ethnicity, is getting penalized with a term of imprisonment until 3 years or a mulct. In 1934, the authorities of the Basel-Stadt canton criminalized anti-Jewish hate speech, e.g., the accusation of ritual murders, mostly in reaction against a pro-nazi antisemitic group and newspaper, the .
United Kingdom.
In the United Kingdom, several statutes criminalize hate speech against several categories of persons. The statutes forbid communication which is hateful, threatening, abusive, or insulting and which targets a person on account of skin colour, race, disability, nationality (including citizenship), ethnic or national origin, religion, or sexual orientation. The penalties for hate speech include fines, imprisonment, or both. Legislation against Sectarian hate in Scotland, which is aimed principally at football matches, does not criminalise jokes about peoples’ beliefs, nor outlaw “harsh” comment about their religious faith.
United States.
Constitutional framework.
The 1789 Constitution of the United States of America dealt only with the three heads of power—legislative, executive, and judicial—and sketched the basic outlines of federalism in the last four articles. The protection of civil rights was not written into the original Constitution but was added two years later with the Bill of Rights, implemented as several amendments to the Constitution. The First Amendment, ratified December 15, 1791, states:
Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof, or abridging the freedom of speech, or of the press, or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.
Although this section is written only to apply to the federal congress (i.e. the legislative branch), the 14th Amendment, ratified on July 9, 1868, works to extend this prohibition to laws of the states as well.
Some state constitutions also have a "free speech" provision, most notably, California.
Supreme Court case law.
Some limits on expression were contemplated by the framers and have been read into the Constitution by the Supreme Court. In 1942, Justice Frank Murphy summarized the case law: "There are certain well-defined and limited classes of speech, the prevention and punishment of which have never been thought to raise a Constitutional problem. These include the lewd and obscene, the profane, the libelous and the insulting or “fighting” words – those which by their very utterances inflict injury or tend to incite an immediate breach of the peace."
Traditionally, however, if the speech did not fall within one of the above categorical exceptions, it was protected speech. In 1969, the Supreme Court protected a Ku Klux Klan member’s racist and hate-filled speech and created the ‘imminent danger’ test to permit hate speech. The court ruled in "Brandenburg v. Ohio" that; "The constitutional guarantees of free speech and free press do not permit a state to forbid or proscribe advocacy of the use of force, or of law violation except where such advocacy is directed to inciting imminent lawless action and is likely to incite or produce such action."
This test has been modified very little from its inception in 1969 and the formulation is still good law in the United States. Only speech that poses an imminent danger of unlawful action, where the speaker has the intention to incite such action and there is the likelihood that this will be the consequence of his or her speech, may be restricted and punished by that law.
In "R.A.V. v. City of St. Paul", (1992), the issue of freedom to express hatred arose again when a gang of white people burned a cross in the front yard of a black family. The local ordinance in St. Paul, Minnesota, criminalized such racist and hate-filled expressions and the teenager was charged thereunder. Associate justice Antonin Scalia, writing for the Supreme Court, held that the prohibition against hate speech was unconstitutional as it contravened the First Amendment. The Supreme Court struck down the ordinance. Scalia explicated the fighting words exception as follows: “The reason why fighting words are categorically excluded from the protection of the First Amendment is not that their content communicates any particular idea, but that their content embodies a particularly intolerable (and socially unnecessary) mode of expressing whatever idea the speaker wishes to convey”. Because the hate speech ordinance was not concerned with the mode of expression, but with the content of expression, it was a violation of the freedom of speech. Thus, the Supreme Court embraced the idea that hate speech is permissible unless it will lead to imminent hate violence. The opinion noted "This conduct, if proved, might well have violated various Minnesota laws against arson, criminal damage to property", among a number of others, none of which was charged, including threats to any person, not to only protected classes.
In 2011, the Supreme Court issued their ruling on "Snyder v. Phelps," which concerned the right of the Westboro Baptist Church to protest with signs found offensive by many Americans. The issue presented was whether the 1st Amendment protected the expressions written on the signs. In an 8-1 decision the court sided with Phelps, the head of Westboro Baptist Church, thereby confirming their historically strong protection of hate speech, so long as it doesn't promote imminent violence. The Court explained, "speech deals with matters of public concern when it can 'be fairly considered as relating to any matter of political, social, or other concern to the community' or when it 'is a subject of general interest and of value and concern to the public." 
Societal implementation.
Under Title VII of the Civil Rights Act of 1964, employers may sometimes be prosecuted for tolerating "hate speech" by their employees, if that speech contributes to a broader pattern of harassment resulting in a "hostile or offensive working environment" for other employees.
In the 1980s and 1990s, more than 350 public universities adopted "speech codes" regulating discriminatory speech by faculty and students. These codes have not fared well in the courts, where they are frequently overturned as violations of the First Amendment. Debate over restriction of "hate speech" in public universities has resurfaced with the adoption of anti-harassment codes covering discriminatory speech.
NTIA report.
In 1992, Congress directed the National Telecommunications and Information Administration (NTIA) to examine the role of telecommunications, including broadcast radio and television, cable television, public access television, and computer bulletin boards, in advocating or encouraging violent acts and the commission of hate crimes against designated persons and groups. The NTIA study investigated speech that fostered a climate of hatred and prejudice in which hate crimes may occur. Study findings revealed only a few instances during the past decade in which broadcast facilities were used to spread messages of hate and bigotry. In two such instances, radio broadcasts arguably urged an audience to commit hate-motivated crimes. In other instances, radio broadcast licensees aired programming that evidenced prejudice. A few highly publicized cable television programs promoted messages of hate and bigotry. In some cases, cable programming stirred community reaction and was followed by counterprogramming. During the 1980s, computer bulletin boards were established by various white supremacist and neo-Nazi groups, but many fell into disuse later in the decade. The study also found that hate "hotlines" are used to deliver recorded messages of bigotry and prejudice and that telephones can be used to intimidate, threaten, and harass individuals and organizations. NTIA's research suggests that hate messages represent a very small percentage of electronic communications media and that the best response is public education rather than government censorship and regulation. Legal remedies involving the use of telecommunications to commit or encourage hate crimes are discussed, as well as technologies that can protect or empower targets of hate speech. A list of commenters is appended.
In 1993, the National Telecommunications and Information Administration (NTIA) released a report titled . This report gave one of the first definitions by government on hate speech. According to NTIA hate speech is:
Hate speech in media.
In January, 2009, the National Hispanic Media Coalition (NHMC), a nonprofit organization with a mission to improve the image of American Latinos as portrayed by the media, unveiled a three prong strategy to address the issue of hate speech in media. 1) filed a petition for inquiry into hate speech with the Federal Communications Commission (FCC). The petition urges the Commission to examine the extent and effects of hate speech in media, including the likely link between hate speech and hate crimes, and to explore non-regulatory ways in which to counteract its negative impacts. 2) NHMC asked the National Telecommunications and Information Administration (NTIA) to update its 1993 report “The Role of Telecommunications in Hate Crimes”; 3) NHMC collaborated with the UCLA/Chicano Research Study Center (CRSC) to produce groundbreaking research on the subject. “Hate Speech on Commercial Radio, Preliminary Report on a Pilot Study” was also released in January 2009.
“Hate Speech on Commercial Radio” categorized hate speech in four different areas.
In May 2010, NHMC filed comments in the FCC’s proceeding on the Future of Media and Information Needs of Communities in the Digital Age. Joined by 32 national and regional organizations from throughout the country, the comments ask the FCC to examine hate speech in media. In its comments, NHMC reinforces the need for the FCC to act on NHMC’s petition for inquiry on hate speech in media filed in January 2009.

</doc>
<doc id="14236" url="http://en.wikipedia.org/wiki?curid=14236" title="Henrik Ibsen">
Henrik Ibsen

Henrik Johan Ibsen (; ]; 20 March 1828 – 23 May 1906) was a major 19th-century Norwegian playwright, theatre director, and poet. He is often referred to as "the father of realism" and is one of the founders of Modernism in theatre. His major works include "Brand", "Peer Gynt", "An Enemy of the People", "Emperor and Galilean", "A Doll's House", "Hedda Gabler", "Ghosts", "The Wild Duck", "Rosmersholm", and "The Master Builder". He is the most frequently performed dramatist in the world after Shakespeare, and "A Doll's House" became the world's most performed play by the early 20th century.
Several of his later dramas were considered scandalous to many of his era, when European theatre was expected to model strict morals of family life and propriety. Ibsen's later work examined the realities that lay behind many façades, revealing much that was disquieting to many contemporaries. It utilized a critical eye and free inquiry into the conditions of life and issues of morality. The poetic and cinematic early play "Peer Gynt", however, has strong surreal elements.
Ibsen is often ranked as one of the truly great playwrights in the European tradition. Richard Hornby describes him as "a profound poetic dramatist—the best since Shakespeare". He is widely regarded as the most important playwright since Shakespeare. He influenced other playwrights and novelists such as George Bernard Shaw, Oscar Wilde, Arthur Miller, James Joyce, Eugene O'Neill and Miroslav Krleža. Ibsen was nominated for the Nobel Prize in Literature in 1902, 1903 and 1904.
Ibsen wrote his plays in Danish (the common written language of Denmark and Norway) and they were published by the Danish publisher Gyldendal. Although most of his plays are set in Norway—often in places reminiscent of Skien, the port town where he grew up—Ibsen lived for 27 years in Italy and Germany, and rarely visited Norway during his most productive years. Born into a merchant family connected to the patriciate of Skien, his dramas were shaped by his family background. He was the father of Prime Minister Sigurd Ibsen. Ibsen's dramas continue in their influence upon contemporary culture and film with notable film productions including "A Doll's House" featuring Jane Fonda and "A Master Builder" featuring Wallace Shawn.
Early life.
Ibsen was born to Knud Ibsen (1797–1877) and Marichen Altenburg (1799–1869), a well-to-do merchant family, in the small port town of Skien in Telemark county, a city which was noted for shipping timber. As he wrote in an 1882 letter to critic and scholar Georg Brandes, "my parents were members on both sides of the most respected families in Skien", explaining that he was closely related with "just about all the patrician families who then dominated the place and its surroundings", mentioning the families Paus, Plesner, von der Lippe, Cappelen and Blom. Ibsen's grandfather, ship captain Henrich Ibsen (1765–1797), had died at sea in 1797, and Knud Ibsen was raised on the estate of ship-owner Ole Paus (1776–1855), after his mother Johanne, née Plesner (1770–1847), remarried. Knud Ibsen's half brothers included lawyer and politician Christian Cornelius Paus, banker and ship-owner Christopher Blom Paus, and lawyer Henrik Johan Paus, who grew up with Ibsen's mother in the Altenburg home and after whom Henrik (Johan) Ibsen was named.
Knud Ibsen's paternal ancestors were ship captains of Danish origin, but he decided to become a merchant, having initial success. His marriage to Marichen Altenburg, a daughter of ship-owner Johan Andreas Altenburg (1763–1824) and Hedevig Christine Paus (1763–1848), was a successful match. Theodore Jorgenson points out that "Henrik's ancestry [thus] reached back into the important Telemark family of Paus both on the father's and on the mother's side. Hedvig Paus must have been well known to the young dramatist, for she lived until 1848." Henrik Ibsen was fascinated by his parents' "strange, almost incestuous marriage," and would treat the subject of incestuous relationships in several plays, notably his masterpiece "Rosmersholm".
When Henrik Ibsen was around seven years old, however, his father's fortunes took a significant turn for the worse, and the family was eventually forced to sell the major Altenburg building in central Skien and move permanently to their small summer house, Venstøp, outside of the city. Henrik's sister Hedvig would write about their mother: "She was a quiet, lovable woman, the soul of the house, everything to her husband and children. She sacrificed herself time and time again. There was no bitterness or reproach in her." The Ibsen family eventually moved to a city house, Snipetorp, owned by Knud Ibsen's half-brother, wealthy banker and ship-owner Christopher Blom Paus. 
His father's financial ruin would have a strong influence on Ibsen's later work; the characters in his plays often mirror his parents, and his themes often deal with issues of financial difficulty as well as moral conflicts stemming from dark secrets hidden from society. Ibsen would both model and name characters in his plays after his own family. A central theme in Ibsen's plays is the portrayal of suffering women, echoing his mother Marichen Altenburg; Ibsen's sympathy with women would eventually find significant expression with their portrayal in dramas such as "A Doll's House" and "Rosmersholm".
At fifteen, Ibsen was forced to leave school. He moved to the small town of Grimstad to become an apprentice pharmacist and began writing plays. In 1846, when Ibsen was age 18, a liaison with a servant produced an illegitimate child, whose upbringing Ibsen had to pay for until the boy was in his teens, though Ibsen never saw the boy. Ibsen went to Christiania (later renamed Kristiania and then Oslo) intending to matriculate at the university. He soon rejected the idea (his earlier attempts at entering university were blocked as he did not pass all his entrance exams), preferring to commit himself to writing. His first play, the tragedy "Catilina" (1850), was published under the pseudonym "Brynjolf Bjarme", when he was only 22, but it was not performed. His first play to be staged, "The Burial Mound" (1850), received little attention. Still, Ibsen was determined to be a playwright, although the numerous plays he wrote in the following years remained unsuccessful. Ibsen's main inspiration in the early period, right up to "Peer Gynt", was apparently Norwegian author Henrik Wergeland and the Norwegian folk tales as collected by Peter Christen Asbjørnsen and Jørgen Moe. In Ibsen's youth, Wergeland was the most acclaimed, and by far the most read, Norwegian poet and playwright.
Life and writings.
He spent the next several years employed at Det norske Theater (Bergen), where he was involved in the production of more than 145 plays as a writer, director, and producer. During this period, he published five new, though largely unremarkable, plays. Despite Ibsen's failure to achieve success as a playwright, he gained a great deal of practical experience at the Norwegian Theater, experience that was to prove valuable when he continued writing.
Ibsen returned to Christiania in 1858 to become the creative director of the Christiania Theatre. He married Suzannah Thoresen on 18 June 1858 and she gave birth to their only child Sigurd on 23 December 1859. The couple lived in very poor financial circumstances and Ibsen became very disenchanted with life in Norway. In 1864, he left Christiania and went to Sorrento in Italy in self-imposed exile. He didn't return to his native land for the next 27 years, and when he returned it was as a noted, but controversial, playwright.
His next play, "Brand" (1865), brought him the critical acclaim he sought, along with a measure of financial success, as did the following play, "Peer Gynt" (1867), to which Edvard Grieg famously composed incidental music and songs. Although Ibsen read excerpts of the Danish philosopher Søren Kierkegaard and traces of the latter's influence are evident in "Brand", it was not until after "Brand" that Ibsen came to take Kierkegaard seriously. Initially annoyed with his friend Georg Brandes for comparing Brand to Kierkegaard, Ibsen nevertheless read "Either/Or" and "Fear and Trembling". Ibsen's next play "Peer Gynt" was consciously informed by Kierkegaard.
With success, Ibsen became more confident and began to introduce more and more of his own beliefs and judgements into the drama, exploring what he termed the "drama of ideas". His next series of plays are often considered his Golden Age, when he entered the height of his power and influence, becoming the center of dramatic controversy across Europe.
Ibsen moved from Italy to Dresden, Germany, in 1868, where he spent years writing the play he regarded as his main work, "Emperor and Galilean" (1873), dramatizing the life and times of the Roman emperor Julian the Apostate. Although Ibsen himself always looked back on this play as the cornerstone of his entire works, very few shared his opinion, and his next works would be much more acclaimed. Ibsen moved to Munich in 1875 and began work on his first contemporary realist drama "The Pillars of Society", first published and performed in 1877. "A Doll's House" followed in 1879. This play is a scathing criticism of the marital roles accepted by men and women which characterized Ibsen's society.
"Ghosts" followed in 1881, another scathing commentary on the morality of Ibsen's society, in which a widow reveals to her pastor that she had hidden the evils of her marriage for its duration. The pastor had advised her to marry her fiancé despite his philandering, and she did so in the belief that her love would reform him. But his philandering continued right up until his death, and his vices are passed on to their son in the form of syphilis. The mention of venereal disease alone was scandalous, but to show how it could poison a respectable family was considered intolerable.
In "An Enemy of the People" (1882), Ibsen went even further. In earlier plays, controversial elements were important and even pivotal components of the action, but they were on the small scale of individual households. In "An Enemy", controversy became the primary focus, and the antagonist was the entire community. One primary message of the play is that the individual, who stands alone, is more often "right" than the mass of people, who are portrayed as ignorant and sheeplike. Contemporary society's belief was that the community was a noble institution that could be trusted, a notion Ibsen challenged. In "An Enemy of the People", Ibsen chastised not only the conservatism of society, but also the liberalism of the time. He illustrated how people on both sides of the social spectrum could be equally self-serving. "An Enemy of the People" was written as a response to the people who had rejected his previous work, "Ghosts". The plot of the play is a veiled look at the way people reacted to the plot of "Ghosts". The protagonist is a physician in a vacation spot whose primary draw is a public bath. The doctor discovers that the water is contaminated by the local tannery. He expects to be acclaimed for saving the town from the nightmare of infecting visitors with disease, but instead he is declared an 'enemy of the people' by the locals, who band against him and even throw stones through his windows. The play ends with his complete ostracism. It is obvious to the reader that disaster is in store for the town as well as for the doctor.
As audiences by now expected, Ibsen's next play again attacked entrenched beliefs and assumptions; but this time, his attack was not against society's mores, but against overeager reformers and their idealism. Always an iconoclast, Ibsen was equally willing to tear down the ideologies of any part of the political spectrum, including his own.
"The Wild Duck" (1884) is by many considered Ibsen's finest work, and it is certainly the most complex. It tells the story of Gregers Werle, a young man who returns to his hometown after an extended exile and is reunited with his boyhood friend Hjalmar Ekdal. Over the course of the play, the many secrets that lie behind the Ekdals' apparently happy home are revealed to Gregers, who insists on pursuing the absolute truth, or the "Summons of the Ideal". Among these truths: Gregers' father impregnated his servant Gina, then married her off to Hjalmar to legitimize the child. Another man has been disgraced and imprisoned for a crime the elder Werle committed. Furthermore, while Hjalmar spends his days working on a wholly imaginary "invention", his wife is earning the household income.
Ibsen displays masterful use of irony: despite his dogmatic insistence on truth, Gregers never says what he thinks but only insinuates, and is never understood until the play reaches its climax. Gregers hammers away at Hjalmar through innuendo and coded phrases until he realizes the truth; Gina's daughter, Hedvig, is not his child. Blinded by Gregers' insistence on absolute truth, he disavows the child. Seeing the damage he has wrought, Gregers determines to repair things, and suggests to Hedvig that she sacrifice the wild duck, her wounded pet, to prove her love for Hjalmar. Hedvig, alone among the characters, recognizes that Gregers always speaks in code, and looking for the deeper meaning in the first important statement Gregers makes which does not contain one, kills herself rather than the duck in order to prove her love for him in the ultimate act of self-sacrifice. Only too late do Hjalmar and Gregers realize that the absolute truth of the "ideal" is sometimes too much for the human heart to bear.
Late in his career, Ibsen turned to a more introspective drama that had much less to do with denunciations of society's moral values. In such later plays as "Hedda Gabler" (1890) and "The Master Builder" (1892), Ibsen explored psychological conflicts that transcended a simple rejection of current conventions. Many modern readers, who might regard anti-Victorian didacticism as dated, simplistic or hackneyed, have found these later works to be of absorbing interest for their hard-edged, objective consideration of interpersonal confrontation. "Hedda Gabler" is probably Ibsen's most performed play, with the title role regarded as one of the most challenging and rewarding for an actress even in the present day. "Hedda Gabler" and "A Doll's House" center on female protagonists whose almost demonic energy proves both attractive and destructive for those around them, and while Hedda has a few similarities with the character of Nora in "A Doll's House", many of today's audiences and theatre critics feel that Hedda's intensity and drive are much more complex and much less comfortably explained than what they view as rather routine feminism on the part of Nora.
Ibsen had completely rewritten the rules of drama with a realism which was to be adopted by Chekhov and others and which we see in the theatre to this day. From Ibsen forward, challenging assumptions and directly speaking about issues has been considered one of the factors that makes a play art rather than entertainment. His works were brought to an English-speaking audience, largely thanks to the efforts of William Archer and Edmund Gosse. These in turn had a profound influence on the young James Joyce who venerates him in his early autobiographical novel "Stephen Hero". Ibsen returned to Norway in 1891, but it was in many ways not the Norway he had left. Indeed, he had played a major role in the changes that had happened across society. Modernism was on the rise, not only in the theatre, but across public life.
Death.
On 23 May 1906, Ibsen died in his home at Arbins gade 1 in Kristiania (now Oslo) after a series of strokes in March 1900. When, on 22 May, his nurse assured a visitor that he was a little better, Ibsen spluttered his last words "On the contrary" ("Tvertimod!"). He died the following day at 2:30 P.M.
Ibsen was buried in Vår Frelsers gravlund ("The Graveyard of Our Savior") in central Oslo.
Centenary.
The 100th anniversary of Ibsen's death in 2006 was commemorated with an "Ibsen year" in Norway and other countries. This year the homebuilding company Selvaag also opened Peer Gynt Sculpture Park in Oslo, Norway, in Henrik Ibsen's honour, making it possible to follow the dramatic play Peer Gynt scene by scene. Will Eno's adaptation of Ibsen's "Peer Gynt" titled "Gnit" had its world premiere at the 37th Humana Festival of New American Plays in March 2013.
On 23 May 2006, The Ibsen Museum (Oslo) reopened to the public the house where Ibsen had spent his last eleven years, completely restored with the original interior, colors, and decor.
Ancestry.
Ibsen's ancestry has been a much studied subject, due to his perceived foreignness and due to the influence of his biography and family on his plays. Ibsen often made references to his family in his plays, sometimes by name, or by modelling characters after them.
The oldest documented member of the Ibsen family was ship's captain Rasmus Ibsen (1632–1703) from Stege, Denmark. His son, ship's captain Peder Ibsen became a burgher of Bergen in Norway in 1726. Henrik Ibsen has Danish, German, Norwegian and some distant Scottish ancestry. Most of his ancestors belonged to the merchant class of original Danish/German extraction, and many of his ancestors were ship's captains. His biographer Henrik Jæger famously wrote in 1888 that Ibsen did not have a drop of Norwegian blood in his veins, stating that "the ancestral Ibsen was a Dane". This, however, is not completely accurate; notably through his grandmother Hedevig Paus, Ibsen was descended from one of the very few families of the patrician class of original Norwegian extraction, known since the 15th century. Ibsen's ancestors had mostly lived in Norway for several generations, even though many had foreign ancestry.
The name Ibsen is originally a patronymic, meaning "son of Ib" (Ib is a Danish variant of Jacob). The patronymic became "frozen", i.e. it became a permanent family name, already in the 17th century. The phenomenon of patronymics becoming frozen started in the 17th century in bourgeois families in Denmark, and the practice was only widely adopted in Norway from around 1900.
Descendants.
From his marriage with Suzannah Thoresen, Ibsen had one son, lawyer and government minister Sigurd Ibsen. Sigurd Ibsen married Bergljot Bjørnson, the daughter of Bjørnstjerne Bjørnson. Their only son was Tancred Ibsen, who became a film director and who was married to Lillebil Ibsen. Their only child was diplomat Tancred Ibsen, Jr. Sigurd Ibsen's daughter, Irene Ibsen, married Josias Bille, a member of the Danish ancient noble Bille family. Their son was Danish actor Joen Bille.
Adaptations.
There have been numerous adaptations of Ibsen's work, particularly in film, theatre and music. Notable are Torstein Blixfjord's "Terje" and "Identity of the Soul" - two multimedia, film and dance pieces first presented in Yokohama in 2006, based on the poem "Terje Vigen".
Legacy.
On the occasion of the 100th anniversary of Ibsen's death in 2006, the Norwegian government organised the Ibsen Year, which included celebrations around the world. The NRK produced a miniseries on Ibsen's childhood and youth in 2006, "An Immortal Man". Several prizes are awarded in the name of Henrik Ibsen, among them the International Ibsen Award, the Norwegian Ibsen Award and the Ibsen Centennial Commemoration Award.
Every year, since 2008, the annual "Delhi Ibsen Festival", is held in Delhi, India, organized by the Dramatic Art and Design Academy (DADA) in collaboration with The Royal Norwegian Embassy in India. It features plays by Ibsen, performed by artists from various parts of the world in varied languages and styles.
Honours.
Ibsen was decorated Knight in 1873, Commander in 1892, and with the Grand Cross of the Order of St. Olav in 1893. He received the Grand Cross of the Danish Order of the Dannebrog, and the Grand Cross of the Swedish Order of the Polar Star, and was Knight, First Class of the Order of Vasa.
In 1995, the asteroid (5696) Ibsen was named in his memory.
English translations.
The authoritative translation in the English language for Ibsen remains the 1928 ten-volume version of the "Complete Works of Henrik Ibsen" from Oxford University Press. Many other translations of individual plays by Ibsen have appeared since 1928 though none have purported to be a new version of the complete works of Ibsen.

</doc>
<doc id="14240" url="http://en.wikipedia.org/wiki?curid=14240" title="Hawaiian language">
Hawaiian language

The Hawaiian language (Hawaiian: "ʻŌlelo Hawaiʻi") is a Polynesian language that takes its name from Hawaiʻi, the largest island in the tropical North Pacific archipelago where it developed. Hawaiian, along with English, is an official language of the state of Hawaii. King Kamehameha III established the first Hawaiian-language constitution in 1839 and 1840.
For various reasons, including territorial legislation establishing English as the official language in schools, the number of native speakers of Hawaiian gradually decreased during the period from the 1830s to the 1950s. Hawaiian was essentially displaced by English on six of seven inhabited islands. In 2001, native speakers of Hawaiian amounted to under 0.1% of the statewide population. Linguists are worried about the fate of this and other endangered languages.
Nevertheless, from circa 1949 to present day, there has been a gradual increase in attention to and promotion of the language. Public Hawaiian-language immersion preschools called Pūnana Leo were started in 1984; other immersion schools followed soon after. The first students to start in immersion preschool have now graduated from college and many are fluent Hawaiian speakers. The federal government acknowledged this development. For example the Hawaiian National Park Language Correction Act of 2000 changed the names of several national parks in Hawaiʻi, observing the Hawaiian spelling.
A pidgin or creole language spoken in Hawaiʻi is Hawaiian Pidgin (or Hawaii Creole English, HCE). It should not be mistaken for the Hawaiian language nor for a dialect of English.
The Hawaiian alphabet has 13 letters: five vowels (long and short) and eight consonants, one of them being a glottal stop (called "ʻokina" in Hawaiian).
Name.
The Hawaiian language takes its name from the largest island, Hawaii ("Hawaiʻi" in the Hawaiian language), in the tropical North Pacific archipelago where it developed, originally from a Polynesian language of the South Pacific, most likely Marquesan or Tahitian. The island name was first written in English in 1778 by British explorer James Cook and his crew members. They wrote it as "Owhyhee" or "Owhyee". Explorers Mortimer (1791) and Otto von Kotzebue (1821) used that spelling.
The initial "O" in the name is a reflection of the fact that unique identity is predicated in Hawaiian by using a copula form, "o", immediately before a proper noun. Thus, in Hawaiian, the name of the island is expressed by saying "O Hawaiʻi", which means "[This] is Hawaiʻi." The Cook expedition also wrote "Otaheite" rather than "Tahiti."
The spelling "why" in the name reflects the [hw] pronunciation of "wh" in 18th century English (still in active use in parts of the English-speaking world). "Why" was pronounced [hwai]. The spelling "hee" or "ee" in the name represents the sounds [hi], or [i].
Putting the parts together, "O-why-(h)ee" reflects [o-hwai-i], a reasonable approximation of the native pronunciation, [o hɐwɐiʔi].
American missionaries bound for Hawaiʻi used the phrases "Owhihe Language" and "Owhyhee language" in Boston prior to their departure in October 1819 and during their five-month voyage to Hawai'i. They still used such phrases as late as March 1822. However, by July 1823, they had begun using the phrase "Hawaiian Language."
In Hawaiian, "ʻŌlelo Hawaiʻi" means "Hawaiian language", as adjectives follow nouns.
Family and origin.
Hawaiian is a Polynesian member of the Austronesian language family. It is closely related to other Polynesian languages, such as Marquesan, Tahitian, Māori, Rapa Nui (the language of Easter Island), and less closely to Samoan and Tongan.
According to Schütz (1994), the Marquesans colonized the archipelago in roughly 300 AD followed by later waves of immigration from the Society Islands and Samoa-Tonga. Their languages, over time, became the Hawaiian language within the Hawaiian Islands. Kimura and Wilson (1983) also state, "Linguists agree that Hawaiian is closely related to Eastern Polynesian, with a particularly strong link in the Southern Marquesas, and a secondary link in Tahiti, which may be explained by voyaging between the Hawaiian and Society Islands."
Methods of proving Hawaiian's family relationships.
The genetic history of the Hawaiian language is demonstrated primarily through the application of lexicostatistics, which involves quantitative comparison of lexical cognates, and the comparative method.
Lexicostatistics is a way of quantifying the degree to which any given languages are genetically related to one another. It is mainly based on determining the number of cognates (genetically shared words) that the languages have in a fixed set of vocabulary items which are nearly universal among all languages. The so-called "basic vocabulary" (or Swadesh list) amounts to about 200 words, having meanings such as "eye", "hair", "blood", "water", and "and." The measurement of a genetic relationship is expressed as a percentage. For example, Hawaiian and English have 0 cognates in the 200-word list, so they are 0% genetically related. By contrast, Hawaiian and Tahitian have about 152 cognates in the list, so they are estimated as being 76% genetically related.
The comparative method is a technique developed by linguists to determine if two or more languages are genetically related, and if they are, the historical nature of the relationships. For a given meaning, the words of the languages are compared.
Linguists observe:
In this method, the definition of "identical" is reasonably clear, but those of "similar" and "dissimilar" are based on phonological criteria which may require professional training to fully understand and which can vary in the contexts of different languages. Basically, a sound's manner and place of articulation, and its phonological features, are the main factors considered in investigating its status as "similar" or "dissimilar" to other sounds in a particular context. For example, /b/ and /m/ are both voiced labial sounds, but one is a stop and the other a nasal. When linguists find in compared languages that compared words of the same or similar meaning contain sounds which correspond to one another, and find that these same sound correspondences recur regularly in most, or in many, of the comparable words of the languages, then the usual conclusion is that the languages are genetically related.
The following table provides a limited data set for ten numbers. The asterisk (*) is used to show that these are hypothetical, reconstructed forms. In the table, the year date of the modern forms is rounded off to CE 2000 to emphasize the 6000-year time lapse since the PAN era.
Note: For the number "10", the Tongan form in the table is part of the word /hoŋo-fulu/ ('ten'). The Hawaiian cognate is part of the word /ana-hulu/ ('ten days'), however the more common word for "10" used in counting and quantifying is /ʔumi/, a different root.
Application of the lexicostatistical method to the data in the table will show the four languages to be related to one another, with Tagalog having 100% cognacy with PAN, while Hawaiian and Tongan have 100% cognacy with each other, but 90% with Tagalog and PAN. This is because the forms for each number are cognates, except the Hawaiian and Tongan words for the number "1", which are cognate with each other, but not with Tagalog and PAN. When the full set of 200 meanings is used, the percentages will be much lower. For example, Elbert found Hawaiian and Tongan to have 49% (98 ÷ 200) shared cognacy. This points out the importance of data-set size for this method – less data, cruder result; more data, better result.
Application of the comparative method will show partly different genetic relationships. It will point out sound changes, such as:
This method will recognize sound change #1 as a shared innovation of Hawaiian and Tongan. It will also take the Hawaiian and Tongan cognates for "1" as another shared innovation. Due to these exclusively shared features, Hawaiian and Tongan are found to be more closely related to one another than either is to Tagalog or PAN.
The forms in the table show that the Austronesian vowels tend to be relatively stable, while the consonants are relatively volatile. It is also apparent that the Hawaiian words for "3", "5" and "8" have remained essentially unchanged for 6000 years.
History.
First European contact.
In 1778, British explorer James Cook made the first reported European contact with Hawaiʻi, beginning a new phase in the development of Hawaiian. During the next forty years, the sounds of Spanish (1789), Russian (1804), French (1816), and German (1816) arrived in Hawaiʻi via other explorers and businessmen. Hawaiian began to be written for the first time, largely restricted to isolated names and words, and word lists collected by explorers and travelers.
The early explorers and merchants who first brought European languages to the Hawaiʻian islands also took on a few native crew members who brought the Hawaiian language into new territory. Although there were not enough of these Hawaiian-speaking explorers to establish any viable speech communities abroad, they still had a noticeable presence. One of them, a boy in his teens known as Obookiah ("ʻŌpūkahaʻia"), had a major impact on the future of the language. He sailed to New England, where he eventually became a student at the Foreign Mission School in Cornwall, Connecticut. He inspired New Englanders to support a Christian mission to Hawaiʻi, and provided information on the Hawaiian language to the American missionaries there prior to their departure for Hawaiʻi in 1819.
Written Hawaiian.
In 1820, Protestant missionaries from New England arrived in Hawaiʻi, inspired by the presence of several young Hawaiian men, especially Obookiah ("ʻŌpūkahaʻia"), at the Foreign Mission School in Cornwall, Connecticut. The missionaries began to learn the Hawaiian language so that they could form relationships with the locals and publish a Hawaiʻian Bible. To that end, they developed a successful alphabet for Hawaiian by 1826, taught Hawaiians to read and write the language, published various educational materials in Hawaiian, and eventually finished translating the Bible. Missionaries also influenced King Kamehameha III to establish the first Hawaiian-language constitutions in 1839 and 1840.
Adelbert von Chamisso might have consulted with a native speaker of Hawaiian in Berlin, Germany, before publishing his grammar of Hawaiian ("Über die Hawaiische Sprache") in 1837. When Hawaiian King David Kalākaua took a trip around the world, he brought his native language with him. When his wife, Queen Kapiʻolani, and his sister, Princess (later Queen) Liliʻuokalani, took a trip across North America and on to the British Islands, in 1887, Liliʻuokalani's composition "Aloha ʻOe" was already a famous song in the U.S.
In 1834, the first Hawaiian-language newspapers were published by missionaries working with locals. The missionaries also played a significant role in publishing a vocabulary (1836) grammar (1854) and dictionary (1865) of Hawaiian. Literacy in Hawaiian was widespread among the local population, especially ethnic Hawaiians. Use of the language among the general population might have peaked around 1881. Even so, some people worried, as early as 1854, that the language was "soon destined to extinction."
The increase in travel to and from Hawaiʻi during the 19th century introduced a number of fatal illnesses such as smallpox, influenza, and leprosy, which killed large numbers of native speakers of Hawaiian. Meanwhile, native speakers of other languages, especially English, Chinese, Japanese, Portuguese, and Ilokano, continued to immigrate to Hawaiʻi. As a result, the actual number, as well as the percentage, of native speakers of Hawaiian in the local population decreased sharply, and continued to fall throughout the nineteenth century.
As the status of Hawaiian dropped, the status of English in Hawaiʻi rose. In 1885, the Prospectus of the Kamehameha Schools announced that "instruction will be given only in English language" (see published opinion of the United States Court of Appeals for the Ninth Circuit, Doe v. Kamehameha Schools, case no. 04-15044, page 8928, filed August 2, 2005). Around 1900, students began to be punished for speaking Hawaiʻian in schools, and the number of native speakers of Hawaiian diminished from 37,000 at the turn of the twentieth century to 1,000 today; half of these remaining are now in their seventies or eighties (see Ethnologue report below for citations).
There has been some controversy over the reasons for this decline. One school of thought claims that the most important cause for the decline of the Hawaiian language was its voluntary abandonment by the majority of its native speakers. According to Mary Kawena Pukui, they wanted their own children to speak English, as a way to promote their success in a rapidly changing modern environment, so they refrained from using Hawaiian with their own children. The Hawaiian language schools disappeared as their enrollments dropped: parents preferred English language schools. Another school of thought emphasizes the importance of other factors that discouraged the use of the language, such as the fact that the English language was made the only medium of instruction in all schools in 1896 and the fact that schools punished the use of Hawaiian (see ""Banning" of Hawaiian" below.) General prejudice against ethnic Hawaiians ("kanaka") has also been blamed for the decline of the language.
A new dictionary was published in 1957, a new grammar in 1979, and new second-language textbooks in 1951, 1965, 1977, and 1989. Master's theses and doctoral dissertations on specific facets of Hawaiian appeared in 1951, 1975, 1976, and 1996.
"Kaona" or hidden meaning.
According to Mary Kawena Pukui and Samuel Elbert, "kaona" ("kao-na") is a "Hidden meaning, as in Hawaiian poetry; concealed reference, as to a person, thing, or place; words with double meanings that might bring good or bad fortune." Pukui lamented, “in spite of years of dedicated work, it is impossible to record any language completely. How true this seems for Hawaiian, with its rich and varied background, its many idioms heretofore undescribed, and its ingenious and sophisticated use of figurative language.” On page xiii of the 1986 dictionary she warned: "Hawaiian has more words with multiple meanings than almost any other language. One wishing to name a child, a house, a T-shirt, or a painting, should be careful that the chosen name does not have a naughty or vulgar meaning. The name of a justly respectable children's school, Hana Hauʻoli, means happy activity and suggests a missionary author, but among older Hawaiians it has another, less 'innocent' meaning that should not concern little children. A Honolulu street (and formerly the name of a hotel) is Hale Leʻa 'joyous house', but leʻa also means orgasm."
Understanding the kaona of the language requires a comprehensive knowledge of Hawaiian legends, history and cosmology.
"Banning" of Hawaiian.
The law cited as banning the Hawaiian language is identified as Act 57, sec. 30 of the 1896 Laws of the Republic of Hawaiʻi:
The English Language shall be the medium and basis of instruction in all public and private schools, provided that where it is desired that another language shall be taught in addition to the English language, such instruction may be authorized by the Department, either by its rules, the curriculum of the school, or by direct order in any particular instance. Any schools that shall not conform to the provisions of this section shall not be recognized by the Department. [signed] June 8, 1896 Sanford B. Dole, President of the Republic of Hawaiʻi 
This law established English as the medium of instruction for the government-recognized schools both "public and private". While it did not ban or make illegal the Hawaiian language in other contexts, its implementation in the schools had far reaching effects. Those who had been pushing for English only schools took this law as licence to extinguish the native language at the early education level. While the law stopped short of making Hawaiian illegal (it was still the dominant language spoken at the time), many children who spoke Hawaiian at school, including on the playground, were disciplined. This included corporal punishment and going to the home of the offending child to strongly advise them to stop speaking it in their home. Moreover, the law specifically provided for teaching languages "in addition to the English language," reducing Hawaiian to the status of a foreign language, subject to approval by the Department. Hawaiian was not taught initially in any school, including the all-Hawaiian Kamehameha Schools. This is largely because when these schools were founded, like Kamehameha Schools founded in 1887 (nine years before this law), Hawaiian was being spoken in the home. Once this law was enacted, individuals at these institutions took it upon themselves to enforce a ban on Hawaiian. Beginning in 1900, Mary Kawena Pukui, who was later the co-author of the Hawaiian–English Dictionary, was punished for speaking Hawaiian by being rapped on the forehead, allowed to eat only bread and water for lunch, and denied home visits on holidays. Winona Beamer was expelled from Kamehameha Schools in 1937 for chanting Hawaiian.
Hawaiian-language newspapers were published for over a hundred years, through the period of the suppression. Very few pro-Hawaiian papers made it through the period of the overthrow of the kingdom and the subsequent Act 57. Most papers that survived that period had a distinctly pro-U.S.Annexation perspective. list fourteen Hawaiian newspapers. According to them, the newspapers entitled "Ka Lama Hawaii" and "Ke Kumu Hawaii" began publishing in 1834, and the one called "Ka Hoku o Hawaii" ceased publication in 1948. The longest run was that of "Ka Nupepa Kuokoa": about 66 years, from 1861 to 1927.
1949 to present.
In 1949, the legislature of the Territory of Hawaiʻi commissioned Mary Pukui and Samuel Elbert to write a new dictionary of Hawaiian, either revising the Andrews-Parker work, or starting from scratch. Pukui and Elbert took a middle course, using what they could from the Andrews dictionary, but making certain improvements and additions that were more significant than a minor revision. The dictionary they produced, in 1957, introduced an era of gradual increase in attention to the language (and culture).
Efforts to promote the language have increased in recent decades. Hawaiian-language "immersion" schools are now open to children whose families want to reintroduce Hawaiian language for future generations. The ʻAha Pūnana Leo’s Hawaiian language preschools in Hilo, Hawaii, have received international recognition. The local National Public Radio station features a short segment titled "Hawaiian word of the day" and a Hawaiian language news broadcast. Honolulu television station KGMB ran a weekly Hawaiian language program, "ʻĀhaʻi ʻŌlelo Ola", as recently as 2010. Additionally, the Sunday editions of the "Honolulu Star-Advertiser", the largest newspaper in Hawaii, feature a brief article called written entirely in Hawaiian by teachers, students, and community members.
Today, on six of the seven permanently inhabited islands, Hawaiian has been largely displaced by English, and the number of native speakers of Hawaiian is under 0.1% of the state-wide population. Native speakers of Hawaiian who live on the island named "Niʻihau" have remained fairly isolated and have continued to use Hawaiian almost exclusively.
Niihau.
Niihau is the only area in the world where Hawaiian is the first language and English is a foreign language. Because of many sufficiently marked variations, Niihau people, when visiting or living in Honolulu, substitute the Oahu dialect ["sic"] for their own – apparently easy to do – saying that otherwise people in Honolulu have trouble understanding them. Niihau people speak very rapidly; many vowels and entire syllables are dropped or whispered.
The isolated island of Niʻihau, located off the southwest coast of Kauai, is the one island where Hawaiian is still spoken as the language of daily life. Children are taught Hawaiian as a first language, and learn English at about age eight. Reasons for the language's predominance on this island include:
Native speakers of Niʻihau Hawaiian have three distinct modes of speaking Hawaiian:
The last mode of speaking may be further restricted to a certain subset of Niʻihauans, and is rarely even overheard by non- Niʻihauans. In addition to being able to speak Hawaiian in different ways, most Niʻihauans can speak English as well.
 states that "[v]ariations in Hawaiian dialects have not been systematically studied", and that "[t]he dialect of Niʻihau is the most aberrant and the one most in need of study". They recognized that Niʻihauans can speak Hawaiian in substantially different ways. Their statements are based in part on some specific observations made by . (See Hawaiian phonological processes)
Orthography.
Hawaiians had no written language prior to western contact, except for petroglyph symbols.
The modern Hawaiian alphabet, "ka pīʻāpā Hawaiʻi", is based on the Latin script. Hawaiian words end "only" in vowels, and every consonant must be followed by a vowel. The Hawaiian alphabetical order has all of the vowels before the consonants, as in the following chart.
Origin.
This writing system was developed by American Protestant missionaries during 1820–1826. It was the first thing they ever printed in Hawaiʻi, on January 7, 1822, and it originally included the consonants "B, D, R, T," and "V," in addition to the current ones ("H, K, L, M, N, P, W"), and it had "F, G, S, Y" and "Z" for "spelling foreign words". The initial printing also showed the five vowel letters ("A, E, I, O, U") and seven of the short diphthongs ("AE, AI, AO, AU, EI, EU, OU").
In 1826, the developers voted to eliminate some of the letters which represented functionally redundant allophones (called "interchangeable letters"), enabling the Hawaiian alphabet to approach the ideal state of one-symbol-one-phoneme, and thereby optimizing the ease with which people could teach and learn the reading and writing of Hawaiian. For example, instead of spelling one and the same word as "pule, bule, pure," and "bure" (because of interchangeable "p/b" and "l/r"), the word is spelled only as "pule".
However, hundreds of words were very rapidly borrowed into Hawaiian from English, Greek, Hebrew, Latin, and Syriac. Although these loan words were necessarily Hawaiianized, they often retained some of their "non-Hawaiian letters" in their published forms. For example, "Brazil" fully Hawaiianized is "Palakila", but retaining "foreign letters" it is "Barazila". Another example is "Gibraltar", written as "Kipalaleka" or "Gibaraleta". While [z] and [ɡ] are not regarded as Hawaiian sounds, [b], [ɹ], and [t] were represented in the original alphabet, so the letters ("b", "r", and "t") for the latter are not truly "non-Hawaiian" or "foreign", even though their post-1826 use in published matter generally marked words of foreign origin.
Glottal stop.
"ʻOkina" ("ʻoki" 'cut' + "-na" '-ing') is the modern Hawaiian name for the symbol (a letter) that represents the glottal stop. It was formerly known as "ʻuʻina" ('snap').
For examples of the ʻokina, consider the Hawaiian words "Hawaiʻi" and "Oʻahu" (often simply "Hawaii" and "Oahu" in English orthography). In Hawaiian, these words can be pronounced [hʌˈʋʌi.ʔi] and [oˈʔʌ.hu], and can be written with an ʻokina where the glottal stop is pronounced.
History.
As early as 1823, the missionaries made some limited use of the apostrophe to represent the glottal stop, but they did not make it a letter of the alphabet. In publishing the Hawaiian Bible, they used it to distinguish "koʻu" ('my') from "kou" ('your'). In 1864, William DeWitt Alexander published a grammar of Hawaiian in which he made it clear that the glottal stop (calling it "guttural break") is definitely a true consonant of the Hawaiian language. He wrote it using an apostrophe. In 1922, the Andrews-Parker dictionary of Hawaiian made limited use of the opening single quote symbol, called "reversed apostrophe" or "inverse comma", to represent the glottal stop. Subsequent dictionaries have preferred to use that symbol. Today, many native speakers of Hawaiian do not bother, in general, to write any symbol for the glottal stop. Its use is advocated mainly among students and teachers of Hawaiian as a second language, and among linguists.
Electronic encoding.
The ʻokina is written in various ways for electronic uses:
Because many people who want to write the ʻokina are not familiar with these specific characters and/or do not have access to the appropriate fonts and input and display systems, it is sometimes written with more familiar and readily available characters:
Macron.
A modern Hawaiian name for the macron symbol is "kahakō" ("kaha" 'mark' + "kō" 'long'). It was formerly known as "mekona" (Hawaiianization of "macron"). It can be written as a diacritical mark which looks like a hyphen or dash written above a vowel, i.e., "ā ē ī ō ū" and "Ā Ē Ī Ō Ū". It is used to show that the marked vowel is a "double", or "geminate", or "long" vowel, in phonemic terms. (See: Vowel length)
As early as 1821, at least one of the missionaries, Hiram Bingham, was using macrons (and breves) in making handwritten transcriptions of Hawaiian vowels. The missionaries specifically requested their sponsor in Boston to send them some type (fonts) with accented vowel characters, including vowels with macrons, but the sponsor made only one response and sent the wrong font size (pica instead of small pica). Thus, they could not print ā, ē, ī, ō, nor ū (at the right size), even though they wanted to.
Pronunciation.
Due to extensive allophony, Hawaiian has more than 13 phones. Although vowel length is phonemic, long vowels are not always pronounced as such, even though under the rules for assigning stress in Hawaiian, a long vowel will always receive stress.
Phonology.
Consonants.
Hawaiian is known for having very few consonant phonemes – eight: /p, k ~ t, ʔ, h, m, n, l, w ~ v/. It is notable that Hawaiian has allophonic variation of [t] with [k], [w] with [v], and (in some dialects) [l] with [n]. The [t]–[k] variation is quite unusual among the world's languages, and is likely a product both of the small number of consonants in Hawaiian, and the recent shift of historical *t to modern [t]–[k], after historical *k had shifted to [ʔ]. In some dialects, /ʔ/ remains as [k] in some words. These variations are largely free, though there are conditioning factors. /l/ tends to [n] especially in words with both /l/ and /n/, such as in the island name "Lānaʻi" ([laːˈnɐʔi]–[naːˈnɐʔi]), though this is not always the case: "ʻeleʻele" or "ʻeneʻene" "black". The [k] allophone is almost universal at the beginnings of words, whereas [t] is most common before the vowel /i/. [v] is also the norm after /i/ and /e/, whereas [w] is usual after /u/ and /o/. After /a/ and initially, however, [w] and [v] are in free variation.
Vowels.
Hawaiian has five short and five long vowels, plus diphthongs.
Monophthongs.
Hawaiian has five pure vowels. The short vowels are /u, i, o, e, a/, and the long vowels, if they are considered separate phonemes rather than simply sequences of like vowels, are /uː, iː, oː, eː, aː/. When stressed, short /e/ and /a/ tend to become [ɛ] and [ɐ], while when unstressed they are [e] and [ə]. /e/ also tends to become [ɛ] next to /l/, /n/, and another [ɛ], as in "Pele" [pɛlɛ]. Some grammatical particles vary between short and long vowels. These include "a" and "o" "of", "ma" "at", "na" and "no" "for". Between a back vowel /o/ or /u/ and a following non-back vowel (/a e i/), there is an epenthetic [w], which is generally not written. Between a front vowel /e/ or /i/ and a following non-front vowel (/a o u/), there is an epenthetic [j] (a "y" sound), which is never written.
Diphthongs.
The short-vowel diphthongs are /iu, ou, oi, eu, ei, au, ai, ao, ae/. In all except perhaps /iu/, these are falling diphthongs. However, they are not as tightly bound as the diphthongs of English, and may be considered vowel sequences. (The second vowel in such sequences may receive the stress, but in such cases it is not counted as a diphthong.) In fast speech, /ai/ tends to [ei] and /au/ tends to [ou], conflating these diphthongs with /ei/ and /ou/.
There are only a limited number of vowels which may follow long vowels, and some authors treat these as diphthongs as well: /oːu, eːi, aːu, aːi, aːo, aːe/.
Phonotactics.
Hawaiian syllable structure is (C)V. All CV syllables occur except for "wū"; "wu" occurs only in two words borrowed from English. As shown by Schütz, Hawaiian word-stress is predictable in words of one to four syllables, but not in words of five or more syllables. Hawaiian phonological processes include palatalization and deletion of consonants, as well as raising, diphthongization, deletion, and compensatory lengthening of vowels. Phonological reduction (or "decay") of consonant phonemes during the historical development of the language has resulted in the phonemic glottal stop. Ultimate loss (deletion) of intervocalic consonant phonemes has resulted in Hawaiian long vowels and diphthongs.
Grammar.
Hawaiian is an analytic language with verb–subject–object word order. While there is no use of inflection for verbs, in Hawaiian, like other Austronesian personal pronouns, declension is found in the differentiation between a- and o-class genitive case personal pronouns in order to indicate inalienable possession in a binary possessive class system. Also like many Austronesian languages, Hawaiian pronouns employ separate words for inclusive and exclusive we, and distinguish singular, dual, and plural. The grammatical function of verbs is marked by adjacent particles (short words) and their relative positions to indicate tense–aspect–mood.
Some examples of verb phrase patterns:
Nouns can be marked with articles:
"ka" and "ke" are singular definite articles. "ke" is used before words beginning with a-, e-, o- and k-, and with some words beginning ʻ- and p-. "ka" is used in all other cases. "nā" is the plural definite article.
To show part of a group, the word "kekahi" is used. To show a bigger part, you would insert "mau" to pluralize the subject.
Examples:

</doc>
<doc id="14245" url="http://en.wikipedia.org/wiki?curid=14245" title="Second Polish Republic">
Second Polish Republic

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| Nov 1918 – Dec 1922
 |- class="mergedrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1921 
 |  km² ( sq mi)
 |- class="mergedrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1931 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1938 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1921 est.
 |- class="mergedrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |style="padding-left:0;text-align:left;"| 1931 est.
 |- class="mergedrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |style="padding-left:0;text-align:left;"| 1938 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |  Poland<br> Lithuania<br> Slovakia<br> Czech Republic<BR> Belarus<br> Ukraine
The Second Polish Republic, also known as the Second Commonwealth of Poland or the interwar Poland, refers to the country of Poland between the First and Second World Wars (1918–1939). Officially known as the Republic of Poland or the Commonwealth of Poland (Polish: "Rzeczpospolita Polska"), the Polish state was recreated in 1918, in the aftermath of World War I. When, after several regional conflicts, the borders of the state were fixed in 1922, Poland's neighbours were Czechoslovakia, Germany, the Free City of Danzig, Lithuania, Latvia, Romania and the Soviet Union. It had access to the Baltic Sea via a short strip of coastline either side of the city of Gdynia. Between March and August 1939, Poland also shared a border with the then-Hungarian province of Carpathian Ruthenia. Despite internal and external pressures, it continued to exist until 1939, when Poland was invaded by Nazi Germany, the Soviet Union and the Slovak Republic, marking the beginning of World War II in Europe. The Second Republic was significantly different in territory to the current Polish state. It used to include substantially more territory in the east and less in the west.
The Second Republic's land area was 388,634 km2, making it, in October 1938, the sixth largest country in Europe. After the annexation of Zaolzie, this grew to 389,720 km2. According to the 1921 census, the number of inhabitants was 27.2 million. By 1939, just before the outbreak of World War II, this had grown to an estimated 35.1 million. Almost a third of population came from minority groups: 13.9% Ukrainians; 10% Jews; 3.1% Belarusians; 2.3% Germans and 3.4% percent Czechs, Lithuanians and Russians. At the same time, a significant number of ethnic Poles lived outside the country borders, many in the Soviet Union. The Republic endured and expanded despite a variety of difficulties: the aftermath of World War I, including conflicts with Ukraine, with Czechoslovakia, with Lithuania and with Soviet Russia and Ukraine; the Greater Poland and Silesian uprisings; and increasing hostility from Nazi Germany.
Poland maintained a slow (see: trade embargo) but steady level of economic development. The cultural hubs of interwar Poland – Warsaw, Kraków, Poznań, Wilno and Lwów – became major European cities and the sites of internationally acclaimed universities and other institutions of higher education. By 1939, the Republic had become "one of Europe's major powers".
History.
Beginnings.
Germany gained dominance on the Eastern Front of World War I as the Russians fell back. German and Austro-Hungarian armies seized the Russian-ruled part of what became Poland. Berlin set up a German puppet state on 5 November 1916, with a governing Council of State and (from 15 October 1917) a Regency Council ("Rada Regencyjna Królestwa Polskiego"). The Council administered the country under German auspices (see also Mitteleuropa) pending the election of a king. A month before Germany gave up and ended the war on 7 October 1918, the Regency Council dissolved the Council of State and announced its intention to restore Polish independence. With the notable exception of the Marxist-oriented Social Democratic Party of the Kingdom of Poland and Lithuania ("SDKPiL"), most political parties supported this move. On 23 October the Council appointed a new government under Józef Świeżyński and began conscription into the Polish Army.
On 5 November, in Lublin, the first Soviet of Delegates was created. On 6 November the Communists announced the creation of a Republic of Tarnobrzeg. The same day, a Provisional People's Government of the Republic of Poland was created in Lublin under the Socialist, Ignacy Daszyński. On Sunday, 10 November at 7 a.m., Józef Piłsudski, newly freed from 16-month imprisonment by the German authorities at Magdeburg, returned by train to Warsaw. Piłsudski, together with Colonel Kazimierz Sosnkowski, was greeted at Warsaw's rail station by Regent Zdzisław Lubomirski and Colonel Adam Koc. Next day, due to his popularity and support from most political parties, the Regency Council appointed Piłsudski Commander in Chief of the Polish Armed Forces. On 14 November, the Council dissolved itself and transferred all its authority to Piłsudski as Chief of State ("Naczelnik Państwa"). After consultation with Piłsudski, Daszyński's government dissolved itself and a new government was created under Jędrzej Moraczewski. In 1918, Italy was the first country in Europe to recognise Poland's sovereignty.
Centers of government that were at that time created in Galicia (formerly Austrian-ruled southern Poland) included National Council of the Principality of Cieszyn (created in November 1918), Republic of Zakopane and Polish Liquidation Committee (created on 28 October). Soon afterward, a conflict broke out in Lwów between forces of the Military Committee of Ukrainians, and the Polish irregular units made up of students known as the Lwów Eaglets, who were later supported by the Polish Army (see Battle of Lwów (1918), Battle of Przemyśl (1918)). Meanwhile, in western Poland, another war of national liberation began under the banner of the Greater Poland Uprising (1918–19). In January 1919, Czechoslovakian forces attacked Polish units in the area of Zaolzie (see Polish–Czechoslovak War). Soon afterwards, the Polish–Lithuanian War began, and in August 1919, Polish-speaking residents of Upper Silesia initiated a series of three Silesian Uprisings. The most important military conflict of that period however was the Polish–Soviet War, which ended in a decisive Polish victory.
Nazi-Soviet invasion of 1939.
The beginning of the Second World War put an end to the sovereign Second Polish Republic. The Invasion of Poland began 1 September 1939, one week after the signing of the secret Molotov–Ribbentrop Pact. On that day, Poland was attacked by Nazi Germany and Slovakia, and on 17 September, the Soviets attacked eastern Poland. Organized Polish resistance ended on 6 October 1939 after the Battle of Kock, with Germany and the Soviet Union occupying most of the country. The area of Wilno was annexed by Lithuania, and areas along southern border were seized by Slovakia including Górna Orawa and Tatranská Javorina which Poland had annexed from Czechoslovakia in October 1938. Poland did not surrender, but continued fighting as the Polish government-in-exile and the Polish Underground State. After signing the German–Soviet Treaty of Friendship, Cooperation and Demarcation, Polish areas occupied by Nazi Germany were either directly annexed to the Third Reich, or became part of the so-called General Government. Soviet Union, after rigged Elections to the People's Assemblies of Western Ukraine and Western Belarus, annexed eastern Poland either to Byelorussian Soviet Socialist Republic, or Ukrainian Soviet Socialist Republic.
The Polish government-in-exile operated in Paris and later London, between 1939 and 1990, maintaining that it was the only legal and legitimate representative of the Polish nation. In 1990, the last president in exile, Ryszard Kaczorowski handed the insignia to Lech Wałęsa, signifying continuity between the Second and Third republics.
Politics and government.
The Second Polish Republic was a parliamentary democracy from 1919 (see Small Constitution of 1919) to 1926, with the President having limited powers. The Parliament elected him, and he could appoint the Prime Minister as well as the government with the Sejm's (lower house's) approval, but he could only dissolve the Sejm with the Senate's consent. Moreover, his power to pass decrees was limited by the requirement that the Prime Minister and the appropriate other Minister had to verify his decrees with their signatures. Poland was one of the first countries in the world to recognize Women's suffrage. Women in Poland were granted the right to vote on 28 November 1918, with a decree of Józef Piłsudski.
The major political parties at this time were the National Democrats and other right-wing groups, various Peasant Parties, Christian Democrats, Polish Socialist Party, and political groups of ethnic minorities (German: German Social Democratic Party of Poland, Jewish: General Jewish Labour Bund in Poland, United Jewish Socialist Workers Party, and Ukrainian: Ukrainian National Democratic Alliance). Frequently changing governments (see Polish legislative election, 1919, Polish legislative election, 1922) and other negative publicity which the politicians received (such as accusations of corruption or 1919 Polish coup attempt), made them increasingly unpopular. Major politicians at this time included peasant activist Wincenty Witos (Prime Minister three times) and right-wing Roman Dmowski. Ethnic minorities were represented in the Sejm; e.g. in 1928 – 1930 there was the Ukrainian-Belarusian Club, with 26 Ukrainian and 4 Belarusian members.
After the Polish – Soviet war, Marshal Piłsudski led an intentionally modest life, writing historical books for a living. After he took power by a military coup in May 1926, he emphasized that he wanted to heal the Polish society and politics of excessive partisan politics. His regime, accordingly, was called Sanacja in Polish. The 1928 parliamentary elections were still considered free and fair, although the pro-Piłsudski Nonpartisan Bloc for Cooperation with the Government won them. The following three parliamentary elections (in 1930, 1935 and 1938) were manipulated, with opposition activists being sent to Bereza Kartuska prison (see also Brest trials). As a result, pro-government party Camp of National Unity won huge majorities in them. Piłsudski died just after an authoritarian constitution was approved in the spring of 1935. During the last four years of the Second Polish Republic, the major politicians included President Ignacy Mościcki, Foreign Minister Józef Beck and the Commander-in-Chief of the Polish Army, Edward Rydz-Śmigły. The country was divided into 104 electoral districts, and those politicians who were forced to leave Poland, founded Front Morges in 1936. The government that ruled Second Polish Republic in its final years is frequently referred to as Piłsudski's colonels.
Military.
"Main articles: Polish armaments 1939–45 Polish army order of battle in 1939"
Prior to the 1939 invasion, Poland had a considerably large army of 283,000 soldiers on active duty: in 37 infantry divisions, 11 cavalry brigades, and two armored brigades, plus artillery units. Another 700,000 men served in the reserves. At the outbreak of the war, Polish army was able to put in the field almost one million soldiers, 2,800 guns, 500 tanks and 400 aircraft.
The training of the Polish army was thorough. The N.C.O.s were a competent body of men with expert knowledge and high ideals. The officers, both senior and junior, constantly refreshed their training in the field and in the lecture-hall, where modern technical achievement and the lessons of contemporary wars were demonstrated and discussed. The equipment of the Polish army was less developed technically than that of the enemy and its rearmament was slowed down as a result of a recrudescence of optimism in western Europe and the usual budget difficulties.
Sadly war plans (Plan West and Plan East) failed as soon as Germany invaded in 1939, Polish losses in combat against Germans (killed and missing in action) amounted to ca. 70,000. 420,000 were taken prisoners. Losses against the Red Army (which invaded Poland on 17 September) added up to 6,000 to 7,000 of casualties and MIA, 250,000 were taken prisoners. Although the Polish army – considering the inactivity of the Allies – was in an unfavorable position – it managed to inflict serious losses to the enemies: 14,000 German soldiers were killed or MIA, 674 tanks and 319 armored vehicles destroyed or badly damaged, 230 aircraft shot down; the Red Army lost (killed and MIA) about 2,500 soldiers, 150 combat vehicles and 20 aircraft. The Soviet invasion of Poland and lack of promised aid from the Western Allies, contributed to the Polish forces defeat by 6 October 1939.
Economy.
After regaining its independence, Poland was faced with major economic difficulties. In addition to the devastation wrought by World War I, the exploitation of the Polish economy by the German and Russian occupying powers, and the sabotage performed by retreating armies, the new republic was faced with the task of economically unifying disparate economic regions, which had previously been part of different countries. Within the borders of the Republic were the remnants of three different economic systems, with five different currencies (the German mark, the Russian ruble, the Austrian crown, the Polish marka and the Ostrubel) and with little or no direct infrastructural links. The situation was so bad that neighboring industrial centers as well as major cities lacked direct railroad links, because they had been parts of different nations. For example, there was no direct railroad connection between Warsaw and Kraków until 1934. This situation was described by Melchior Wańkowicz in his book Sztafeta.
On top of this was the massive destruction left after both World War I and the Polish–Soviet War. There was also a great economic disparity between the eastern (commonly called "Poland B") and western (called "Poland A") parts of the country, with the western half, especially areas that had belonged to the German Empire being much more developed and prosperous. Frequent border closures and a customs war with Germany also had negative economic impacts on Poland. In 1924 prime minister and economic minister Władysław Grabski introduced the złoty as a single common currency for Poland (it replaced the Polish marka), which remained a stable currency. The currency helped Poland to bring under control the massive hyperinflation, the only country in Europe which was able to do this without foreign loans or aid. Average annual growth rate (GDP per capita) was 5.24% in 1920–29 and 0.34% in 1929–38.
Hostile relations with neighbours were a major problem for the economy of interbellum Poland. In the year 1937, foreign trade with all neighbours amounted to only 21% of Poland's total. Trade with Lithuania (0% of total) and the Soviet Union (0,8%) was virtually nonexistent. Czechoslovakia accounted for 3,9% of Polish foreign trade, Latvia for 0,3%, Romania for 0,8%, and Germany, Poland's most important neighbour, for 14,3%. By mid-1938, after the Anschluss, Greater Germany was responsible for as much as 23% of Polish foreign trade.
The basis of Poland's gradual recovery after the Great Depression were mass economic development plans (see Four Year Plan), which oversaw the building of three key infrastructural elements. The first was the establishment of the Gdynia seaport, which allowed Poland to completely bypass Gdańsk (which was under heavy German pressure to boycott Polish coal exports). The second was construction of the 500-kilometer rail connection between Upper Silesia and Gdynia, called Polish Coal Trunk-Line, which served freight trains with coal. The third was the creation of a central industrial district, named "COP – Central Industrial Region" (Centralny Okręg Przemysłowy). Unfortunately, these developments were interrupted and largely destroyed by the German and Soviet invasion and the start of World War II. Among other achievements of interbellum Poland there are Stalowa Wola (a brand new city, built in a forest around a steel mill), Mościce (now a district of Tarnów, with a large nitrate factory), and creation of a central bank. There were several trade fairs, with the most popular being Poznań International Fair, Lwów's Targi Wschodnie, and Wilno's Targi Północne. Polish Radio had ten stations (see Radio stations in interwar Poland), with the eleventh one planned to be opened in the autumn of 1939. Furthermore, in 1935 Polish engineers began working on the TV services. By early 1939, experts of the Polish Radio built four TV sets. First movie broadcast by experimental Polish TV was Barbara Radziwiłłówna, and by 1940, regular TV service was scheduled to begin operation.
Interbellum Poland was also a country with numerous social problems. Unemployment was high, and poverty was widespread, which resulted in several cases of social unrest, such as the 1923 Kraków riot, and 1937 peasant strike in Poland. There were conflicts with national minorities, such as Pacification of Ukrainians in Eastern Galicia (1930), relations with Polish neighbors were sometimes complicated (see Soviet raid on Stołpce, Polish–Czechoslovak border conflicts, 1938 Polish ultimatum to Lithuania). On top of this, there were natural disasters, such as 1934 flood in Poland.
Major industrial centers.
Interbellum Poland was unofficially divided into two parts – better developed "Poland A" in the west, and underdeveloped "Poland B" in the east. Polish industry was concentrated in the west, mostly in Polish Upper Silesia, and the adjacent Lesser Poland's province of Zagłębie Dąbrowskie, where the bulk of coal mines and steel plants was located. Furthermore, heavy industry plants were located in Częstochowa ("Huta Częstochowa", founded in 1896), Ostrowiec Świętokrzyski ("Huta Ostrowiec", founded in 1837–1839), Stalowa Wola (brand new industrial city, which was built from scratch in 1937 – 1938), Chrzanów ("Fablok", founded in 1919), Jaworzno, Trzebinia (oil refinery, opened in 1895), Łódź (the seat of Polish textile industry), Poznań (H. Cegielski – Poznań), Kraków and Warsaw (Ursus Factory). Further east, in Kresy, industrial centers were scarce, and limited to two major cities of the region – Lwów and Wilno (Elektrit). Besides coal mining, Poland also had deposits of oil in Borysław, Drohobycz, Jasło and Gorlice (see Polmin), potassium salt (TESP), and basalt (Janowa Dolina). Apart from already-existing industrial areas, in the mid-1930s, an ambitious, state-sponsored project of Central Industrial Region was started under Minister Eugeniusz Kwiatkowski. One of characteristic features of Polish economy in the interbellum was gradual nationalization of major plants. This was the case of Ursus Factory (see Państwowe Zakłady Inżynieryjne), and several steelworks, such as "Huta Pokój" in Ruda Śląska – Nowy Bytom, "Huta Królewska" in Chorzów – Królewska Huta, "Huta Laura" in Siemianowice Śląskie, as well as "Scheibler and Grohman Works" in Łódź.
Transport.
According to the 1939 Statistical Yearbook of Poland, total length of railways of Poland (as for 31 December 1937) was 20 118 kilometers. Rail density was 5.2 km. per 100 km2. Railways were very dense in western part of the country, while in the east, especially Polesie, rail was non-existent in some counties. During the interbellum period, Polish government constructed several new lines, mainly in central part of the country (see also Polish State Railroads Summer 1939). Construction of extensive Warszawa Główna railway station was never finished due to the war, and Polish railroads were famous for their punctuality (see Luxtorpeda, Strzała Bałtyku, Latający Wilnianin).
In the interbellum, road network of Poland was dense, but the quality of the roads was very poor – only 7% of all roads was paved and ready for automobile use, and none of the major cities were connected with each other by a good-quality highway. In the mid-1930s, Poland had 340,000 kilometers of roads, but only 58,000 had hard surface (gravel, cobblestone or sett), and 2,500 were modern, with asphalt or concrete surface. In different parts of the country, there were sections of paved roads, which suddenly ended, and were followed by dirt roads. Poor condition of roads was the result of both long-lasting foreign dominance, and inadequate funding. On 29 January 1931, Polish Parliament created State Road Fund, whose purpose was to collect money for construction and conservation of roads. The government drafted a 10-year plan, with road priorities: a highway from Wilno, through Warsaw and Cracow, to Zakopane (called Marshall Pilsudski Highway), asphalt highways from Warsaw to Poznań and Łódź, as well as Warsaw ring road. However, the plan turned out to be too ambitious, as there was not enough money in the national budget. In January 1938, Polish Road Congress estimated that Poland should spend on roads three times more money to keep up with Western Europe.
In 1939, before the outbreak of the war, LOT Polish Airlines, which was established in 1929, had its hub at Warsaw Okęcie Airport. At that time LOT maintained several services, both domestic and international. Warsaw had regular domestic connections with Gdynia-Rumia, Danzig-Langfuhr, Katowice-Muchowiec, Kraków-Rakowice-Czyżyny, Lwów-Skniłów, Poznań-Ławica, and Wilno-Porubanek. Furthermore, in cooperation with Air France, LARES, Lufthansa, and Malert, international connections were maintained with Athens, Beirut, Berlin, Bucharest, Budapest, Helsinki, Kaunas, London, Paris, Prague, Riga, Rome, Tallinn, and Zagreb.
Agriculture.
In the Second Polish Republic, the majority of inhabitants lived in the countryside (75% in 1921), and their existence depended on land. Farmers made 65% of the population, while about 1% were landowners. In 1929, agricultural production made 65% of Poland's GNP. After 123 years of partitions, regions of the country were very unevenly developed. Lands of former German Empire were most advanced – in Greater Poland and Pomerelia, crops were on Western European level. The situation was much worse in former Congress Poland, Kresy, and former Galicia, where agriculture was most backward and primitive, with a large number of small farms, unable to succeed on both domestic and international market. Furthermore, another problem was overpopulation of the countryside, which resulted in chronic unemployment. Living conditions were so bad that in several regions, such as counties inhabited by the Hutsuls, there was permanent starvation. Farmers rebelled against the government (see: 1937 peasant strike in Poland), and the situation began to change in the late 1930s, due to construction of several factories for the Central Industrial Region, which gave employment to thousands of countryside residents.
German trade.
Beginning in June 1925 there was a customs' war with the revanchist Weimar Republic imposing trade embargo against Poland for nearly a decade; involving tariffs, and broad economic restrictions. After 1933 the trade war ended. The new agreements regulated and promoted trade. Germany became Poland's largest trading partner, followed by Britain. In October 1938 Germany granted a credit of Rm 60,000,000 to Poland (120,000,000 zloty, or £4,800,000) which was never realized, due to the outbreak of war. Germany would deliver factory equipment and machinery in return for Polish timber and agricultural produce. This new trade was to be "in addition" to the existing German-Polish trade agreements.
Education and culture.
In 1919, the Polish government introduced compulsory education for all children aged 7 to 14, in an effort to limit illiteracy which was widespread especially in the former Russian Partition and the Austrian Partition of eastern Poland. In 1921, one-third of citizens of Poland remained illiterate (38% in the countryside). The process was slow, but by 1931, illiteracy level dropped to 23% overall (27% in the countryside) and further down to 18% in 1937. By 1939, over 90% of children attended school. In 1932, Minister of Religion and Education Janusz Jędrzejewicz carried out a major reform which introduced the following levels of education:
Before 1918, Poland had three universities: Jagiellonian University, University of Warsaw and Lwów University. Catholic University of Lublin was established in 1918; Adam Mickiewicz University, Poznań, in 1919; and finally, in 1922, after the annexation of Republic of Central Lithuania, Wilno University became the Republic's sixth university. There were also three technical colleges: the Warsaw University of Technology, Lwów Polytechnic and the AGH University of Science and Technology in Kraków, established in 1919. Warsaw University of Life Sciences was an agricultural institute. By 1939, there were around 50,000 students enrolled in further education. Women made up 28% of university students, the second highest share in Europe.
Polish science in the interbellum was renowned for its mathematicians – see Lwów School of Mathematics, Kraków School of Mathematics, and Warsaw School of Mathematics. There were well-known philosophers (see Lwów–Warsaw school of logic), Florian Znaniecki founded Polish sociological studies, Rudolf Weigl invented vaccine against typhus, Bronisław Malinowski was among the most important anthropologists of the 20th century. In Polish literature, the 1920s were marked by the domination of poetry. Polish poets were divided into two groups – the Skamanderites (Jan Lechoń, Julian Tuwim, Antoni Słonimski and Jarosław Iwaszkiewicz) and the Futurists (Anatol Stern, Bruno Jasieński, Aleksander Wat, Julian Przyboś). Apart from well-established novelists (Stefan Żeromski, Władysław Reymont), new names appeared in the interbellum – Zofia Nałkowska, Maria Dąbrowska, Jarosław Iwaszkiewicz, Jan Parandowski, Bruno Schultz, Stanisław Ignacy Witkiewicz, Witold Gombrowicz. Among other notable artists there were sculptor Xawery Dunikowski, painters Julian Fałat, Wojciech Kossak and Jacek Malczewski, composers Karol Szymanowski, Feliks Nowowiejski, and Artur Rubinstein, singer Jan Kiepura. Theatre was very popular in the interbellum, with three main centers in the cities of Warsaw, Wilno and Lwów. Altogether, there were 103 theaters in Poland and a number of other theatrical institutions (including 100 folk theaters). In 1936, different shows were seen by 5 million people, and main figures of Polish theatre of the time were Juliusz Osterwa, Stefan Jaracz, and Leon Schiller. Also, before the outbreak of the war, there were around 1 million radios (see Radio stations in interwar Poland).
Administrative division.
The administrative division of the Republic was based on a three-tier system. On the lowest rung were the "gminy", local town and village governments akin to districts or parishes. These were then grouped together into "powiaty" (akin to counties) which, in turn, were grouped as "województwa" (voivodeships, akin to provinces).
On 1 April 1938, the borders of several western and central voivodeships were revised.
Demographics.
Historically, Poland was a nation of many nationalities. This was especially true after independence was regained in the wake of World War I and the subsequent Polish–Soviet War ending at Peace of Riga. The census of 1921 allocates 30.8% of the population in the minority. According to the 1931 Polish Census: 68.9% of the population was Polish, 13.9% were Ukrainians, around 10% Jewish, 3.1% Belarusians, 2.3% Germans and 2.8% – others, including Lithuanians, Czechs and Armenians. Also, there were smaller communities of Russians, and Gypsies. The situation of minorities was a complex subject and changed during the period.
Poland was also a nation of many religions. In 1921, 16,057,229 Poles (approx. 62.5%) were Roman (Latin) Catholics, 3,031,057 citizens of Poland (approx. 11.8%) were Eastern Rite Catholics (mostly Ukrainian Greek Catholics and Armenian Rite Catholics), 2,815,817 (approx. 10.95%) were Greek Orthodox, 2,771,949 (approx. 10.8%) were Jewish, and 940,232 (approx. 3.7%) were Protestants (mostly Lutheran Evangelical). 
By 1931 Poland had the second largest Jewish population in the world, with one-fifth of all the world's Jews residing within its borders (approx. 3,136,000). Urban population of interbellum Poland was rising steadily – in 1921, only 24% of Poles lived in the cities, in the late 1930s, the ratio grew to 30%. In more than a decade, the population of Warsaw grew by 200,000, Łódź by 150,000, and Poznań – by 100,000. This was due not only to internal migration, but also extremely high birth rate.
Geography.
The Second Polish Republic was mainly flat, with average elevation of 223 m above sea level (after World War II and its border changes, the average elevation of Poland decreased to 173 m). Only 13% of territory, along the southern border, was higher than 300 m. The highest elevation was Mount Rysy, which rises 2,499 m in the Tatra Range of the Carpathians, 95 km south of Kraków. Between October 1938 and September 1939, the highest elevation was Lodowy Szczyt (known in the Slovak language as "Ľadový štít"), which rises 2,627 meters above sea level. The largest lake was Lake Narach.
The country's total area, after annexation of Zaolzie, was 389,720 km2, it extended 903 km from north to south and 894 km from east to west. On 1 January 1938, total length of boundaries was 5,529 km, including:
Among major cities of the Second Polish Republic, the warmest yearly average temperature was in Kraków (9.1 °C in 1938) and the coldest in Wilno (7.6 °C in 1938).
Drainage.
Almost 75% of the territory of interbellum Poland was drained northward into the Baltic Sea by the Vistula (total area of drainage basin of the Vistula within boundaries of the Second Polish Republic was 180,300 km2), the Niemen (51,600 km2), the Odra (46,700 km2) and the Daugava (10,400 km2). The remaining part of the country was drained southward, into the Black Sea, by the rivers that drain into the Dnieper (Pripyat, Horyn and Styr, all together 61,500 km2) as well as Dniester (41,400 km2)

</doc>
<doc id="14246" url="http://en.wikipedia.org/wiki?curid=14246" title="Hedwig">
Hedwig

Hedwig is a feminine German given name, see Hedwig (name). Hedwig may also refer to:

</doc>
<doc id="14251" url="http://en.wikipedia.org/wiki?curid=14251" title="HMS Resolution">
HMS Resolution

Several ships of the Royal Navy have borne the name HMS "Resolution". However, the first English warship to bear the name "Resolution" was actually the first rate "Prince Royal" (built in 1610 and rebuilt in 1641), which was renamed "Resolution" in 1650 following the inauguration of the Commonwealth, and continued to bear that name until 1660, when the name "Prince Royal" was restored. The name "Resolution" was bestowed on the first of the vessels listed below:

</doc>
<doc id="14254" url="http://en.wikipedia.org/wiki?curid=14254" title="Helen Keller">
Helen Keller

Helen Adams Keller (June 27, 1880 – June 1, 1968) was an American author, political activist, and lecturer. She was the first deafblind person to earn a bachelor of arts degree. The story of how Keller's teacher, Anne Sullivan, broke through the isolation imposed by a near complete lack of language, allowing the girl to blossom as she learned to communicate, has become widely known through the dramatic depictions of the play and film "The Miracle Worker". Her birthplace in West Tuscumbia, Alabama is now a museum and sponsors an annual "Helen Keller Day". Her birthday on June 27 is commemorated as Helen Keller Day in the U.S. state of Pennsylvania and was authorized at the federal level by presidential proclamation by President Jimmy Carter in 1980, the 100th anniversary of her birth.
A prolific author, Keller was well-traveled and outspoken in her convictions. A member of the Socialist Party of America and the Industrial Workers of the World, she campaigned for women's suffrage, labor rights, socialism, and other similar causes. She was inducted into the Alabama Women's Hall of Fame in 1971.
Early childhood and illness.
Helen Adams Keller was born on June 27, 1880, in Tuscumbia, Alabama. Her family lived on a homestead, Ivy Green, that Helen's grandfather had built decades earlier. She had two younger siblings, Mildred Campbell and Phillip Brooks Keller, two older half-brothers from her father's prior marriage, James and William Simpson Keller.
Her father, Arthur H. Keller, spent many years as an editor for the Tuscumbia "North Alabamian", and had served as a captain for the Confederate Army. Her paternal grandmother was the second cousin of Robert E. Lee. Her mother, Kate Adams, was the daughter of Charles W. Adams. Though originally from Massachusetts, Charles Adams also fought for the Confederate Army during the American Civil War, earning the rank of colonel (and acting brigadier-general). Her paternal lineage was traced to Casper Keller, a native of Switzerland. One of Helen's Swiss ancestors was the first teacher for the deaf in Zurich. Keller reflected on this coincidence in her first autobiography, stating "that there is no king who has not had a slave among his ancestors, and no slave who has not had a king among his."
Helen Keller was born with the ability to see and hear. At 19 months old, she contracted an illness described by doctors as "an acute congestion of the stomach and the brain", which might have been scarlet fever or meningitis. The illness left her both deaf and blind. At that time, she was able to communicate somewhat with Martha Washington, the six-year-old daughter of the family cook, who understood her signs; by the age of seven, Keller had more than 60 home signs to communicate with her family.
In 1886, Keller's mother, inspired by an account in Charles Dickens' "American Notes" of the successful education of another deaf and blind woman, Laura Bridgman, dispatched young Helen, accompanied by her father, to seek out physician J. Julian Chisolm, an eye, ear, nose, and throat specialist in Baltimore, for advice. Chisholm referred the Kellers to Alexander Graham Bell, who was working with deaf children at the time. Bell advised them to contact the Perkins Institute for the Blind, the school where Bridgman had been educated, which was then located in South Boston. Michael Anagnos, the school's director, asked 20-year-old former student Anne Sullivan, herself visually impaired, to become Keller's instructor. It was the beginning of a 49-year-long relationship during which Sullivan evolved into Keller's governess and eventually her companion.
Anne Sullivan arrived at Keller's house in March 1887, and immediately began to teach Helen to communicate by spelling words into her hand, beginning with "d-o-l-l" for the doll that she had brought Keller as a present. Keller was frustrated, at first, because she did not understand that every object had a word uniquely identifying it. In fact, when Sullivan was trying to teach Keller the word for "mug", Keller became so frustrated she broke the mug. Keller's big breakthrough in communication came the next month, when she realized that the motions her teacher was making on the palm of her hand, while running cool water over her other hand, symbolized the idea of "water"; she then nearly exhausted Sullivan demanding the names of all the other familiar objects in her world.
Formal education.
Starting in May 1888, Keller attended the Perkins Institute for the Blind. In 1894, Helen Keller and Anne Sullivan moved to New York to attend the Wright-Humason School for the Deaf, and to learn from Sarah Fuller at the Horace Mann School for the Deaf. In 1896, they returned to Massachusetts and Keller entered The Cambridge School for Young Ladies before gaining admittance, in 1900, to Radcliffe College, where she lived in Briggs Hall, South House. Her admirer, Mark Twain, had introduced her to Standard Oil magnate Henry Huttleston Rogers, who, with his wife Abbie, paid for her education. In 1904, at the age of 24, Keller graduated from Radcliffe, becoming the first deaf blind person to earn a Bachelor of Arts degree. She maintained a correspondence with the Austrian philosopher and pedagogue Wilhelm Jerusalem, who was one of the first to discover her literary talent.
Determined to communicate with others as conventionally as possible, Keller learned to speak, and spent much of her life giving speeches and lectures. She learned to "hear" people's speech by reading their lips with her hands—her sense of touch had become extremely subtle. She became proficient at using braille and reading sign language with her hands as well. Shortly before World War I, with the assistance of the Zoellner Quartet she determined that by placing her fingertips on a resonant tabletop she could experience music played close by.
Companions.
Anne Sullivan stayed as a companion to Helen Keller long after she taught her. Anne married John Macy in 1905, and her health started failing around 1914. Polly Thomson was hired to keep house. She was a young woman from Scotland who had no experience with deaf or blind people. She progressed to working as a secretary as well, and eventually became a constant companion to Keller.
Keller moved to Forest Hills, Queens, together with Anne and John, and used the house as a base for her efforts on behalf of the American Foundation for the Blind.
Anne Sullivan died in 1936 after a coma, with Keller holding her hand. Keller and Thomson moved to Connecticut. They traveled worldwide and raised funds for the blind. Thomson had a stroke in 1957 from which she never fully recovered, and died in 1960.
Winnie Corbally, a nurse who was originally brought in to care for Thompson in 1957, stayed on after her death and was Keller's companion for the rest of her life.
Political activities.
"The few own the many because they possess the means of livelihood of all ... The country is governed for the richest, for the corporations, the bankers, the land speculators, and for the exploiters of labor. The majority of mankind are working people. So long as their fair demands—the ownership and control of their livelihoods—are set at naught, we can have neither men's rights nor women's rights. The majority of mankind is ground down by industrial oppression in order that the small remnant may live in ease."
—Helen Keller, 1911
Keller went on to become a world-famous speaker and author. She is remembered as an advocate for people with disabilities, amid numerous other causes. She was a suffragist, a pacifist, an opponent of Woodrow Wilson, a radical socialist and a birth control supporter. In 1915 she and George Kessler founded the Helen Keller International (HKI) organization. This organization is devoted to research in vision, health and nutrition. In 1920 she helped to found the American Civil Liberties Union (ACLU). Keller traveled to 40-some-odd countries with Sullivan, making several trips to Japan and becoming a favorite of the Japanese people. Keller met every U.S. President from Grover Cleveland to Lyndon B. Johnson and was friends with many famous figures, including Alexander Graham Bell, Charlie Chaplin and Mark Twain. Keller and Twain were both considered radicals at the beginning of the 20th century, and as a consequence, their political views have been forgotten or glossed over in popular perception.
Keller was a member of the Socialist Party and actively campaigned and wrote in support of the working class from 1909 to 1921. She supported Socialist Party candidate Eugene V. Debs in each of his campaigns for the presidency. Before reading "Progress and Poverty", Helen Keller was already a socialist who believed that Georgism was a good step in the right direction. She later wrote of finding "in Henry George’s philosophy a rare beauty and power of inspiration, and a splendid faith in the essential nobility of human nature."
Newspaper columnists who had praised her courage and intelligence before she expressed her socialist views now called attention to her disabilities. The editor of the "Brooklyn Eagle" wrote that her "mistakes sprung out of the manifest limitations of her development." Keller responded to that editor, referring to having met him before he knew of her political views: At that time the compliments he paid me were so generous that I blush to remember them. But now that I have come out for socialism he reminds me and the public that I am blind and deaf and especially liable to error. I must have shrunk in intelligence during the years since I met him. ... Oh, ridiculous "Brooklyn Eagle"! Socially blind and deaf, it defends an intolerable system, a system that is the cause of much of the physical blindness and deafness which we are trying to prevent.
Keller joined the Industrial Workers of the World (the IWW, known as the Wobblies) in 1912, saying that parliamentary socialism was "sinking in the political bog". She wrote for the IWW between 1916 and 1918. In "Why I Became an IWW", Keller explained that her motivation for activism came in part from her concern about blindness and other disabilities:I was appointed on a commission to investigate the conditions of the blind. For the first time I, who had thought blindness a misfortune beyond human control, found that too much of it was traceable to wrong industrial conditions, often caused by the selfishness and greed of employers. And the social evil contributed its share. I found that poverty drove women to a life of shame that ended in blindness.
The last sentence refers to prostitution and syphilis, the former a frequent cause of the latter, and the latter a leading cause of blindness. In the same interview, Keller also cited the 1912 strike of textile workers in Lawrence, Massachusetts for instigating her support of socialism.
Writings.
Keller wrote a total of 12 published books and several articles.
One of her earliest pieces of writing, at age 11, was "The Frost King" (1891). There were allegations that this story had been plagiarized from "The Frost Fairies" by Margaret Canby. An investigation into the matter revealed that Keller may have experienced a case of cryptomnesia, which was that she had Canby's story read to her but forgot about it, while the memory remained in her subconscious.
At age 22, Keller published her autobiography, "The Story of My Life" (1903), with help from Sullivan and Sullivan's husband, John Macy. It recounts the story of her life up to age 21 and was written during her time in college.
Keller wrote "The World I Live In" in 1908, giving readers an insight into how she felt about the world. "Out of the Dark", a series of essays on socialism, was published in 1913.
When Keller was young, Anne Sullivan introduced her to Phillips Brooks, who introduced her to Christianity, Keller famously saying: "I always knew He was there, but I didn't know His name!"
Her spiritual autobiography, "My Religion", was published in 1927 and then in 1994 extensively revised and re-issued under the title "Light in My Darkness". It advocates the teachings of Emanuel Swedenborg, the Christian revelator and theologian who gives a spiritual interpretation of the teachings of the Bible and who claims that the second coming of Jesus Christ has already taken place. Adherents use several names to describe themselves, including Second Advent Christian, Swedenborgian, and New Church.
Keller described the progressive views of her belief in these words:
But in Swedenborg's teaching it [Divine Providence] is shown to be the government of God's Love and Wisdom and the creation of uses. Since His Life cannot be less in one being than another, or His Love manifested less fully in one thing than another, His Providence must needs be universal . . . He has provided religion of some kind everywhere, and it does not matter to what race or creed anyone belongs if he is faithful to his ideals of right living. 
Akita dog.
When Keller visited Akita Prefecture in Japan in July 1937, she inquired about Hachikō, the famed Akita dog that had died in 1935. She told a Japanese person that she would like to have an Akita dog; one was given to her within a month, with the name of Kamikaze-go. When he died of canine distemper, his older brother, Kenzan-go, was presented to her as an official gift from the Japanese government in July 1938. Keller is credited with having introduced the Akita to the United States through these two dogs.
By 1939 a breed standard had been established, and dog shows had been held, but such activities stopped after World War II began. Keller wrote in the "Akita Journal":
If ever there was an angel in fur, it was Kamikaze. I know I shall never feel quite the same tenderness for any other pet. The Akita dog has all the qualities that appeal to me – he is gentle, companionable and trusty.
Later life.
Keller suffered a series of strokes in 1961 and spent the last years of her life at her home.
On September 14, 1964, President Lyndon B. Johnson awarded her the Presidential Medal of Freedom, one of the United States' two highest civilian honors. In 1965 she was elected to the National Women's Hall of Fame at the New York World's Fair.
Keller devoted much of her later life to raising funds for the American Foundation for the Blind. She died in her sleep on June 1, 1968, at her home, Arcan Ridge, located in Easton, Connecticut, a few weeks short of her eighty-eighth birthday. A service was held in her honor at the National Cathedral in Washington, D.C., and her ashes were placed there next to her constant companions, Anne Sullivan and Polly Thomson.
Portrayals.
Keller's life has been interpreted many times. She appeared in a silent film, "Deliverance" (1919), which told her story in a melodramatic, allegorical style.
She was also the subject of the documentaries "Helen Keller in Her Story", narrated by Katharine Cornell, and "The Story of Helen Keller", part of the Famous Americans series produced by Hearst Entertainment.
"The Miracle Worker" is a cycle of dramatic works ultimately derived from her autobiography, "The Story of My Life". The various dramas each describe the relationship between Keller and Sullivan, depicting how the teacher led her from a state of almost feral wildness into education, activism, and intellectual celebrity. The common title of the cycle echoes Mark Twain's description of Sullivan as a "miracle worker." Its first realization was the 1957 "Playhouse 90" teleplay of that title by William Gibson. He adapted it for a Broadway production in 1959 and an Oscar-winning feature film in 1962, starring Anne Bancroft and Patty Duke. It was remade for television in 1979 and 2000.
In 1984, Keller's life story was made into a TV movie called "The Miracle Continues". This film that entailed the semi-sequel to "The Miracle Worker" recounts her college years and her early adult life. None of the early movies hint at the social activism that would become the hallmark of Keller's later life, although a Disney version produced in 2000 states in the credits that she became an activist for social equality.
The Bollywood movie "Black" (2005) was largely based on Keller's story, from her childhood to her graduation.
A documentary called "Shining Soul: Helen Keller's Spiritual Life and Legacy" was produced by the Swedenborg Foundation in the same year. The film focuses on the role played by Emanuel Swedenborg's spiritual theology in her life and how it inspired Keller's triumph over her triple disabilities of blindness, deafness and a severe speech impediment.
On March 6, 2008, the New England Historic Genealogical Society announced that a staff member had discovered a rare 1888 photograph showing Helen and Anne, which, although previously published, had escaped widespread attention. Depicting Helen holding one of her many dolls, it is believed to be the earliest surviving photograph of Anne Sullivan Macy.
Video footage showing Helen Keller learning to mimic speech sounds also exists.
Posthumous honors.
A preschool for the deaf and hard of hearing in Mysore, India, was originally named after Helen Keller by its founder K. K. Srinivasan.
In 1999, Keller was listed in Gallup's Most Widely Admired People of the 20th century.
In 2003, Alabama honored its native daughter on its state quarter. The Alabama state quarter is the only circulating US coin to feature braille.
The Helen Keller Hospital in Sheffield, Alabama is dedicated to her.
There are streets named after Helen Keller in Zürich, Switzerland, in the USA, in Getafe, Spain, in Lod, Israel, in Lisbon, Portugal and in Caen, France.
A stamp was issued in 1980 by the United States Postal Service depicting Keller and Sullivan, to mark the centennial of Keller's birth.
On October 7, 2009, a bronze statue of Helen Keller was added to the National Statuary Hall Collection, as a replacement for the State of Alabama's former 1908 statue of the education reformer Jabez Lamar Monroe Curry. It is displayed in the United States Capitol Visitor Center and depicts Keller as a seven-year-old child standing at a water pump. The statue represents the seminal moment in Keller's life when she understood her first word: W-A-T-E-R, as signed into her hand by teacher Anne Sullivan. The pedestal base bears a quotation in raised Latin and braille letters: "The best and most beautiful things in the world cannot be seen or even touched, they must be felt with the heart." The statue is the first one of a person with a disability and of a child to be permanently displayed at the U.S. Capitol.
Archival material.
Archival material of Helen Keller stored in New York was lost when the Twin Towers were destroyed in the September 11 attacks.

</doc>
<doc id="14257" url="http://en.wikipedia.org/wiki?curid=14257" title="Haddocks' Eyes">
Haddocks' Eyes

Haddocks' Eyes is a poem by Lewis Carroll from "Through the Looking-Glass". It is sung by The White Knight in to a tune that he claims as his own invention, but which Alice recognises as "I give thee all, I can no more".
By the time Alice heard it, she was already tired of poetry.
It is a parody of "Resolution and Independence" by William Wordsworth.
Naming.
The White Knight explains a confusing nomenclature for the song.
The complicated terminology distinguishing between 'the song, what the song is called, the name of the song, and what the name of the song is called' entails the use–mention distinction.
Upon the Lonely Moor.
Like "Jabberwocky," another poem published in "Through the Looking Glass," "Haddocks’ Eyes" appears to have been revised over the course of many years. In 1856, Carroll published the following poem anonymously under the name "Upon the Lonely Moor". It bears an obvious resemblance to "Haddocks' Eyes."

</doc>
<doc id="14260" url="http://en.wikipedia.org/wiki?curid=14260" title="Hoosier">
Hoosier

Hoosier is the official demonym for a resident of the U.S. state of Indiana. The origin of the term remains a matter of debate within the state, but "Hoosier" was in general use by the 1840s, having been popularized by Richmond resident John Finley's 1833 poem "The Hoosier's Nest". Anyone born in Indiana or a resident at the time is considered to be a Hoosier. Indiana adopted the nickname "The Hoosier State" more than 150 years ago.
"Hoosier" is used in the names of numerous Indiana-based businesses and organizations. "Hoosiers" is also the name of the Indiana University athletic teams and seven active and one disbanded athletic conferences in the Indiana High School Athletic Association have the word "Hoosier" in their name. As there is no accepted embodiment of a Hoosier, the IU schools are represented through their letters and colors alone.
Origin.
In addition to "The Hoosier's Nest", the term also appeared in the "Indianapolis Journal"'s "Carrier's Address" on January 1, 1833. There are many suggestions for the derivation of the word, but none is universally accepted.
Scholarship.
In 1900, Meredith Nicholson wrote "The Hoosiers", an early attempt to study the etymology of the word as applied to Indiana residents. Jacob Piatt Dunn, longtime secretary of the Indiana Historical Society, published "The Word Hoosier", a similar attempt, in 1907. Both chronicled some of the popular and satirical etymologies circulating at the time and focused much of their attention on the use of the word in the Upland South to refer to woodsmen, yokels, and rough people. Dunn traced the word back to the Cumbrian "hoozer", meaning anything unusually large, derived from the Old English "hoo" (as at Sutton Hoo), meaning "high" and "hill". The importance of immigrants from northern England and southern Scotland was reflected in numerous placenames including the Cumberland Mountains, the Cumberland River, and the Cumberland Gap. Nicholson defended the people of Indiana against such an association, while Dunn concluded that the early settlers had adopted the nickname self-mockingly and that it had lost its negative associations by the time of Finley's poem.
Johnathan Clark Smith subsequently showed that Nicholson and Dunn's earliest sources within Indiana were mistaken. A letter by James Curtis cited by Dunn and others as the earliest known use of the term was actually written in 1846, not 1826. Similarly, the use of the term in an 1859 newspaper item quoting an 1827 diary entry by Sandford and Son was more likely an editorial comment and not from the original diary. Smith's earliest sources led him to argue that the word originated as a term along the Ohio River for flatboatmen from Indiana and did not acquire its pejorative meanings until 1836, "after" Finley's poem.
William Piersen, a history professor at Fisk University, argued for a connection to the black Methodist minister Rev. Harry Hosier (c. 1750–May 1806), who evangelized the American frontier at the beginning of the 19th century as part of the Second Great Awakening. "Black Harry" had been born a slave in North Carolina and sold north to Baltimore, Maryland, before gaining his freedom and beginning his ministry around the end of the American Revolution. He was a close associate and personal friend of Bishop Francis Asbury, the "Father of the American Methodist Church". Dr. Benjamin Rush said that, "making allowances for his illiteracy, he was the greatest orator in America" and his sermons called on Methodists to reject slavery and champion the common working man. Piersen proposed that Methodist communities inspired by his example took or were given a variant spelling of his name (possibly influenced by the "yokel" slang) during the decades after his ministry.
Folk etymologies.
Banter.
Humorous folk etymologies for the term "hoosier" have a long history, as recounted by Dunn in "The Word Hoosier".
One account traces the word to the necessary caution of approaching houses on the frontier. In order to avoid being shot, a traveler would call out from afar to let themselves be known. The inhabitants of the cabin would then reply "Who's here?" which – in the Appalachian English of the early settlers – slurred into "Who'sh 'ere?" and thence into "Hoosier?" A variant of this account had the Indiana pioneers calling out "Who'sh 'ere?" as a general greeting and warning when hearing someone in the bushes and tall grass, to avoid shooting a relative or friend in error.
The poet James Whitcomb Riley facetiously suggested that the fierce brawling that took place in Indiana involved enough biting that the expression "Whose ear?" became notable. This arose from or inspired the story of two 19th-century French immigrants brawling in a tavern in the foothills of southern Indiana. One was cut and a third Frenchman walked in to see an ear on the dirt floor of the tavern, prompting him to slur out "Whosh ear?"
Mr. Hoosier's men.
Two related stories trace the origin of the term to gangs of workers from Indiana under the direction of a Mr. Hoosier.
The account related by Dunn is that a Louisville contractor named Samuel Hoosier preferred to hire workers from communities on the Indiana side of the Ohio River like New Albany rather than Kentuckians. During the excavation of the first canal around the Falls of the Ohio from 1826 to 1833, his employees became known as "Hoosier's men" and then simply "Hoosiers". The usage spread from these hard-working laborers to all of the Indiana boatmen in the area and then spread north with the settlement of the state. The story was told to Dunn in 1901 by a man who had heard it from a Hoosier relative while traveling in southern Tennessee. Dunn could not find any family of the given name in any directory in the region or anyone else in southern Tennessee who had heard the story and accounted himself dubious. This version was subsequently retold by Gov. Evan Bayh and Sen. Vance Hartke, who introduced the story into the "Congressional Record" in 1975, and matches the timing and location of Smith's subsequent research. However, the U.S. Army Corps of Engineers has been unable to find any record of a Hoosier or Hosier in surviving canal company records.
Other uses.
The word "Hoosier" is still used in St. Louis, Missouri, to denote a "yokel" or "white trash". The word is also sometimes encountered in sea shanties such as "Shanties from the Seven Seas" in reference to its former use to denote cotton-stowers, who would move bales of cotton to and from the holds of ships and force them in tightly by means of jackscrews. "To hoosier" is sometimes still encountered as a verb meaning "to trick" or "to swindle".
A Hoosier cabinet, often shortened to "hoosier", is a type of free-standing kitchen cabinet popular in the early decades of the twentieth century. Almost all of these cabinets were produced by companies located in Indiana and the name derives from the largest of them, the Hoosier Manufacturing Co. of New Castle, Indiana. Other Indiana businesses include Hoosier Racing Tire and the Hoosier Bat Company, manufacturer of wooden baseball bats.
The RCA Dome, former home of the Indianapolis Colts, was known as the "Hoosier Dome" before RCA purchased the naming rights in 1994. The RCA Dome was replaced by Lucas Oil Stadium in 2008.

</doc>
<doc id="14263" url="http://en.wikipedia.org/wiki?curid=14263" title="Horner's method">
Horner's method

In mathematics, Horner's method (also known as Horner scheme in the UK or Horner's rule in the U.S.) is either of two things: (i) an algorithm for calculating polynomials, which consists of transforming the monomial form into a computationally efficient form; or (ii) a method for approximating the roots of a polynomial. The latter is also known as Ruffini–Horner's method.
These methods are named after the British mathematician William George Horner, although they were known before him by Paolo Ruffini and, six hundred years earlier, by the Chinese mathematician Qin Jiushao.
Description of the algorithm.
Given the polynomial
where formula_2 are real numbers, we wish to evaluate the polynomial at a specific value of formula_3, say formula_4.
To accomplish this, we define a new sequence of constants as follows:
Then formula_6 is the value of formula_7.
To see why this works, note that the polynomial can be written in the form
Thus, by iteratively substituting the formula_9 into the expression,
Examples.
Evaluate 
We use synthetic division as follows:
 x₀│ x³ x² x¹ x⁰
 3 │ 2 −6 2 −1
 │ 6 0 6 
 2 0 2 5
The entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the "x"-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of formula_13 on division by formula_14 is 5.
But by the polynomial remainder theorem, we know that the remainder is formula_15. Thus formula_16
In this example, if formula_17 we can see that formula_18, the entries in the third row. So, synthetic division is based on Horner's method.
As a consequence of the polynomial remainder theorem, the entries in the third row are the coefficients of the second-degree polynomial, the quotient of formula_13 on division by formula_20. 
The remainder is 5. This makes Horner's method useful for polynomial long division.
Divide formula_21 by formula_22:
 2 │ 1 -6 11 -6
 │ 2 -8 6 
 1 -4 3 0
The quotient is formula_23.
Let formula_24 and formula_25. Divide formula_26 by formula_27 using Horner's method.
 2 │ 4 -6 0 3 │ -5
 1 │ 2 -2 -1 │ 1
 └──────────────────────┼───────
 2 -2 -1 1 │ -4
The third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is
Floating point multiplication and division.
Horner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier. One of the binary numbers to be multiplied is represented as a trivial polynomial, where, (using the above notation): ai = 1, and x = 2. Then, x (or x to some power) is repeatedly factored out. In this binary numeral system (base 2), x = 2, so powers of 2 are repeatedly factored out.
Example.
For example, to find the product of two numbers, (0.15625) and "m":
Method.
To find the product of two binary numbers, d and m:
Derivation.
In general, for a binary number with bit values: (formula_30) the product is:
At this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:
The denominators all equal one (or the term is absent), so this reduces to:
or equivalently (as consistent with the "method" described above):
In binary (base 2) math, multiplication by a power of 2 is merely a register shift operation. Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift. The factor (2−1) is a right arithmetic shift, a (0) results in no operation (since 20 = 1, is the multiplicative identity element), and a (21) results in a left arithmetic shift.
The multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.
The method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate. Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the "canonical signed digit" (CSD) form is used), and uses only 20% of the code space.
Polynomial root finding.
Using Horner's method in combination with Newton's method, it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial formula_35 of degree formula_36 with zeros formula_37, make some initial guess formula_38 such that formula_39. Now iterate the following two steps:
1. Using Newton's method, find the largest zero formula_40 of formula_35 using the guess formula_4.
2. Using Horner's method, divide out formula_43 to obtain formula_44. Return to step 1 but use the polynomial formula_44 and the initial guess formula_40.
These two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.
Example.
Consider the polynomial,
which can be expanded to
From the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next formula_49 is divided by formula_50 to obtain
which is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by formula_52 to obtain
which is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain
which is shown in green and found to have a zero at −3. This polynomial is further reduced to
which is shown in blue and yields a zero of −5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing formula_56 and solving the linear equation. As can be seen, the expected roots of −8, −5, −3, 2, 3, and 7 were found.
Octave implementation.
The following Octave code was used in the example above to implement Horner's method.
Python implementation.
The following Python code implements Horner's method.
C implementation.
The following C code implements Horner's method.
Application.
Horner's method can be used to convert between different positional numeral systems – in which case "x" is the base of the number system, and the "a""i" coefficients are the digits of the base-"x" representation of a given number – and can also be used if "x" is a matrix, in which case the gain in computational efficiency is even greater. In fact, when "x" is a matrix, further acceleration is possible which exploits the structure of matrix multiplication, and only formula_57 instead of "n" multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer.
Efficiency.
Evaluation using the monomial form of a degree-"n" polynomial requires at most "n" additions and ("n"2 + "n")/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually. (This can be reduced to "n" additions and 2"n" − 1 multiplications by evaluating the powers of "x" iteratively.) If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2"n" times the number of bits of "x" (the evaluated polynomial has approximate magnitude "x""n", and one must also store "x""n" itself). By contrast, Horner's method requires only "n" additions and "n" multiplications, and its storage requirements are only "n" times the number of bits of "x". Alternatively, Horner's method can be computed with "n" fused multiply–adds. Horner's method can also be extended to evaluate the first "k" derivatives of the polynomial with "kn" additions and multiplications.
Horner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal. Victor Pan proved in 1966 that the number of multiplications is minimal. However, when "x" is a matrix, Horner's method is not optimal.
This assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-"n" polynomial can be evaluated using only formula_58 multiplications and "n" additions (see Knuth: "The Art of Computer Programming", Vol.2).
History.
Horner's paper entitled "", was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the "Philosophical Transactions". later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own of Horner-type methods earlier in 1823.
Horner's paper in Part II of "Philosophical Transactions of the Royal Society of London" for 1819 was warmly and expansively welcomed by a in the issue of "The Monthly Review: or, Literary Journal" for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further of some of Nicholson's books in the issue of "The Monthly Review" for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of "The Monthly Review" for September, 1821, with the concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of "The Monthly Review" from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.
The feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in "Philosophical Transactions" speak volumes: there are none - Ruffini's only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diaz Gergonne, or had he written in French, as had , a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).
Fuller showed that the method in Horner's 1819 paper differs from what afterwards became known as 'Horner's method' and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for "The Monthly Review", while several others - Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to "The Gentleman's Mathematical Companion", an answer to a problem.
It is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery that led to "Horner's method" being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method "qua" method was known long before Horner. In reverse chronological order, Horner's method was already known to:
However, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.
Qin Jiushao, in his "Shu Shu Jiu Zhang" ("Mathematical Treatise in Nine Sections"; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-qintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in "The North China Herald" in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method "Harmoniously Alternating Evolution" (which does not agree with his Chinese, "linglong kaifang", not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in "Development of Mathematics in China and Japan" published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: "who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.". However, as Mikami is also aware, it was "not altogether impossible" that a related work, "Si Yuan Yu Jian" ("Jade Mirror of the Four Unknowns; 1303)" by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, "Suan Xue Qi Meng", had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: "It is obvious that this procedure is a Chinese invention...the method was not known in India". He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese. Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in "Jiu Zhang Suan Shu", while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method he does not specify.

</doc>
<doc id="14268" url="http://en.wikipedia.org/wiki?curid=14268" title="Hapworth 16, 1924">
Hapworth 16, 1924

"Hapworth 16, 1924" is the "youngest" of J. D. Salinger's Glass family stories, in the sense that the narrated events happen chronologically before those in the rest of the "Glass series". It appeared in the June 19, 1965 edition of "The New Yorker"—infamously taking up almost the entire magazine—and was the last of Salinger's works to be published in his lifetime. It was harshly panned by both contemporary and later literary critics, with even kind critics regarding the work as "a long-winded sob story" which many have found to be "simply unreadable," and, it has been speculated, this negative response was the reason Salinger decided to quit publishing. Conversely, Salinger is said to have considered the story a "high point of his writing" and made tentative steps to have it reprinted; nonetheless, these efforts came to nothing.
Plot.
The story is presented in the form of a letter from camp written by a seven-year-old Seymour Glass (the main character of "A Perfect Day for Bananafish"). In this respect, the plot is identical to Salinger's previous story "The Ocean Full of Bowling Balls," written some two decades earlier. In the course of requesting a veritable library of reading matter from home, Seymour predicts his brother's success as a writer as well as his own death and condemns the ironic "twist" endings in the stories of Anatole France, twist endings being an early Salinger device.
Publishing history.
After the story's appearance in "The New Yorker", Salinger—who had already withdrawn to his home in New Hampshire—stopped publishing altogether. Since he never put the story between hard covers, readers had to seek out a copy of that issue or find it on microfilm. Finally, with the release of "The Complete New Yorker" on DVD in 2005, the story was once again widely available.
In the meantime, however, in 1996, Orchises Press, a small publishing house in Virginia, started the process to publish "Hapworth" in book form. In an article in "The Washington Post", published after Salinger's death, and in a story for "New York", Orchises Press owner Roger Lathbury described his efforts to publish the story. According to Lathbury, Salinger was deeply concerned with the proposed book's appearance, even visiting Washington to examine the cloth for the binding. Salinger also sent Lathbury numerous "infectious and delightful and loving" letters.
Lathbury, following publishing norms, applied for Library of Congress Cataloging in Publication data, unaware of how publicly available the information would be. A writer in Seattle, researching an article on Jeff Bezos, the founder of the then-fledgling Amazon.com, came across the "Hapworth" publication date, told his sister, a journalist for the "Washington Business Journal", who wrote an article about the upcoming book. This led to substantial coverage in the press. Shortly before the books were to be shipped, Salinger changed his mind, and in accordance with his wishes, Orchises withdrew the work. Although new publication dates were repeatedly announced, the book never appeared. Lathbury said, "I never reached back out. I thought about writing some letters, but it wouldn't have done any good."

</doc>
<doc id="14269" url="http://en.wikipedia.org/wiki?curid=14269" title="Hypnotic">
Hypnotic

Hypnotic (from Greek "Hypnos", sleep) or soporific drugs are a class of psychoactive drugs whose primary function is to induce sleep and to be used in the treatment of insomnia (sleeplessness), or surgical anesthesia.
This group is related to sedatives. Whereas the term "sedative" describes drugs that serve to calm or relieve anxiety, the term "hypnotic" generally describes drugs whose main purpose is to initiate, sustain, or lengthen sleep. Because these two functions frequently overlap, and because drugs in this class generally produce dose-dependent effects (ranging from anxiolysis to loss of consciousness) they are often referred to collectively as sedative-hypnotic drugs.
Hypnotic drugs are regularly prescribed for insomnia and other sleep disorders, with over 95% of insomnia patients being prescribed hypnotics in some countries. Many hypnotic drugs are habit-forming and, due to a large number of factors known to disturb the human sleep pattern, a physician may instead recommend changes in the environment before and during sleep, better sleep hygiene, and the avoidance of caffeine or other stimulating substances before prescribing medication for sleep. When prescribed, hypnotic medication should be used for the shortest period of time possible.
Most hypnotics prescribed today are either benzodiazepines or nonbenzodiazepines. Early classes of drugs, such as barbiturates, have fallen out of use in most practices but are still prescribed for some patients. In children, prescribing hypnotics is not yet acceptable unless used to treat night terrors or somnambulism. Elderly people are more sensitive to potential side effects of daytime fatigue and cognitive impairments, and a meta-analysis found that the risks generally outweigh any marginal benefits of hypnotics in the elderly. A review of the literature regarding benzodiazepine hypnotics and Z-drugs concluded that these drugs can have adverse effects, such as dependence and accidents, and that optimal treatment uses the lowest effective dose for the shortest therapeutic time period, with gradual discontinuation in order to improve health without worsening of sleep.
History.
Research about using medications to treat insomnia evolved throughout the last half of the 20th century. Treatment for insomnia in psychiatry dates back to 1869 when chloral hydrate was first used as a soporific. Barbiturates emerged as the first class of drugs that emerged in the early 1900s, after which chemical substitution allowed derivative compounds. Although the best drug family at the time (less toxic and with fewer side effects) they were dangerous in overdose.
During the 1970s, quinazolinones and benzodiazepines were introduced as safer alternatives to replace barbiturates; by the late 1970s benzodiazepines emerged as the safer drug.
Benzodiazepines are not without their drawbacks; addiction is possible, and deaths from overdoses sometimes occur, especially in combination with alcohol and/or other depressants. Questions have been raised as to whether they disturb sleep architecture.
Nonbenzodiazepines are the most recent development (1990s–present). Although it's clear that they are less toxic than their predecessors, barbiturates, comparative efficacy over benzodiazepines have not been established. Without longitudinal studies, it is hard to determine; however some psychiatrists recommend these drugs, citing research suggesting they are equally potent with less potential for abuse.
Other sleep remedies that may be considered "sedative-hypnotics" exist; psychiatrists will sometimes prescribe medicines off-label if they have sedating effects. Examples of these include mirtazapine, (an antidepressant) clonidine, (generally prescribed to regulate blood pressure) quetiapine, (an antipsychotic) and the over-the-counter sleep aid diphenhydramine (Benadryl – an antihistamine). Off-label sleep remedies are particularly useful when first-line treatment is unsuccessful or deemed unsafe (for example, in patients with a history of substance abuse).
Types.
Barbiturates.
Barbiturates are drugs that act as central nervous system depressants, and can therefore produce a wide spectrum of effects, from mild sedation to total anesthesia. They are also effective as anxiolytics, hypnotics, and anticonvulsants. Barbiturates also have analgesic effects; however, these effects are somewhat weak, preventing barbiturates from being used in surgery in the absence of other analgesics. They have addiction potential, both physical and psychological. Barbiturates have now largely been replaced by benzodiazepines in routine medical practice – for example, in the treatment of anxiety and insomnia – mainly because benzodiazepines are significantly less dangerous in overdose. However, barbiturates are still used in general anesthesia, for epilepsy, and assisted suicide. Barbiturates are derivatives of barbituric acid.
The principal mechanism of action of barbiturates is believed to be positive allosteric modulation of GABAA receptors.
Examples include amobarbital, pentobarbital, phenobarbital, secobarbital, and sodium thiopental.
Quinazolinones.
Quinazolinones are also a class of drugs which function as hypnotic/sedatives that contain a 4-quinazolinone core. Their use has also been proposed in the treatment of cancer.
Examples of quinazolinones include cloroqualone, diproqualone, etaqualone (Aolan, Athinazone, Ethinazone), mebroqualone, mecloqualone (Nubarene, Casfen), and methaqualone (Quaalude).
Benzodiazepines.
Benzodiazepines can be useful for short-term treatment of insomnia. Their use beyond 2 to 4 weeks is not recommended due to the risk of dependence. It is preferred that benzodiazepines be taken intermittently and at the lowest effective dose. They improve sleep-related problems by shortening the time spent in bed before falling asleep, prolonging the sleep time, and, in general, reducing wakefulness.
Common examples are alprazolam (Xanax), lorazepam (Ativan), diazepam (Valium), and clonazepam (Klonopin).
However, they worsen sleep quality by increasing light sleep and decreasing deep sleep. Other drawbacks of hypnotics, including benzodiazepines, are possible tolerance to their effects, rebound insomnia, and reduced slow-wave sleep and a withdrawal period typified by rebound insomnia and a prolonged period of anxiety and agitation. The list of benzodiazepines approved for the treatment of insomnia is fairly similar among most countries, but which benzodiazepines are officially designated as first-line hypnotics prescribed for the treatment of insomnia can vary distinctly between countries. Longer-acting benzodiazepines such as nitrazepam and diazepam have residual effects that may persist into the next day and are, in general, not recommended.
It is not clear as to whether the new nonbenzodiazepine hypnotics (Z-drugs) are better than the short-acting benzodiazepines. The efficacy of these two groups of medications is similar. According to the US Agency for Healthcare Research and Quality, indirect comparison indicates that side-effects from benzodiazepines may be about twice as frequent as from nonbenzodiazepines. Some experts suggest using nonbenzodiazepines preferentially as a first-line long-term treatment of insomnia. However, the UK National Institute for Health and Clinical Excellence (NICE) did not find any convincing evidence in favor of Z-drugs. A NICE review pointed out that short-acting Z-drugs were inappropriately compared in clinical trials with long-acting benzodiazepines. There have been no trials comparing short-acting Z-drugs with appropriate doses of short-acting benzodiazepines. Based on this, NICE recommended choosing the hypnotic based on cost and the patient's preference.
Older adults should not use benzodiazepines to treat insomnia unless other treatments have failed to be effective. When benzodiazepines are used, patients, their caretakers, and their physician should discuss the increased risk of harms, including evidence which shows twice the incidence of traffic collisions among driving patients as well as falls and hip fracture for all older patients.
Their mechanism of action is primarily at GABAA receptors.
Nonbenzodiazepines.
Nonbenzodiazepines are a class of psychoactive drugs that are very "benzodiazepine-like" in nature. Nonbenzodiazepines pharmacodynamics are almost entirely the same as benzodiazepine drugs and therefore employ similar benefits, side-effects, and risks. Nonbenzodiazepines, however, have dissimilar or entirely different chemical structures, and therefore are unrelated to benzodiazepines on a molecular level.
Examples include zopiclone (Imovane, Zimovane), eszopiclone (Lunesta), zaleplon (Sonata), and zolpidem (Ambien, Stilnox, Stilnoct).
Research on nonbenzodiazepines is new and conflicting. A review by a team of researchers suggests the use of these drugs for people that have trouble falling asleep but not staying asleep, as next-day impairments were minimal. The team noted that the safety of these drugs had been established, but called for more research into their long-term effectiveness in treating insomnia. Other evidence suggests that tolerance to nonbenzodiazepines may be slower to develop than with benzodiazepines. A different team was more skeptical, finding little benefit over benzodiazepines.
Others.
Antihistamines.
In common use, the term "antihistamine" refers only to compounds that inhibit action at the H1 receptor (and not H2, etc.).
Clinically, H1 antagonists are used to treat allergic reactions. Sedation is a common side-effect, and some H1 antagonists, such as diphenhydramine (Benadryl) and doxylamine, are also used to treat insomnia.
Second-generation antihistamines cross the blood–brain barrier to a much lower degree than the first ones. This results in their primarily affecting peripheral histamine receptors, and therefore having a much lower sedative effect. High doses can still induce the central nervous system effect of drowsiness.
Antidepressants.
Some antidepressants have sedating effects.
Examples include:
Antipsychotics.
Examples of antipsychotics with sedation as a side effect:
Adverse effects.
Drowziness, dizziness, hangover, dependence

</doc>
<doc id="14273" url="http://en.wikipedia.org/wiki?curid=14273" title="HMS Dunraven">
HMS Dunraven

HMS "Dunraven" was a Q-Ship of the Royal Navy during World War I.
On 8 August 1917, 130 miles southwest of Ushant in the Bay of Biscay, disguised as the collier "Boverton" and commanded by Gordon Campbell, VC, "Dunraven" spotted "UC-71", commanded by "Oberleutnant zur See" Reinhold Saltzwedel. Saltzwedel believed the disguised ship was a merchant vessel. The U-boat submerged and closed with "Dunraven" before surfacing astern at 11:43 am and opening fire at long range. "Dunraven" made smoke and sent off a panic party (a small number of men who "abandon ship" during an attack to continue the impersonation of a merchant).
Shells began hitting "Dunraven", detonating her depth charges and setting her stern afire. Her crew remained hidden letting the fires burn. Then a 4 inch (102 mm) gun and crew were blown away revealing "Dunraven"‍ '​s identity as a warship, and "UC-71" submerged. A second "panic party" abandoned ship. "Dunraven" was hit by a torpedo. A third "panic party" went over the side, leaving only two guns manned. "UC-71" surfaced, shelled "Dunraven" and again submerged. Campbell replied with two torpedoes that missed, and around 3 pm, the undamaged U-boat left that area. Only one of "Dunraven"‍ '​s crew was killed, but the Q-Ship was sinking.
The British destroyer picked up "Dunraven"‍ '​s survivors and took her in tow for Plymouth, but "Dunraven" sank at 1:30 am early on 10 August 1917 to the north of Ushant.
In recognition, two Victoria Crosses were awarded, one to the ship's First Lieutenant, Lt. Charles George Bonner RNR, and the other, by ballot, to a gunlayer, Petty Officer Ernest Herbert Pitcher.
Captain Campbell later wrote:
Captain Campbell had been previously awarded the Victoria Cross, in February 1917, for the sinking of "U-83".

</doc>
<doc id="14275" url="http://en.wikipedia.org/wiki?curid=14275" title="Hacker ethic">
Hacker ethic

Hacker ethic is a term for the moral values and philosophy that are common in the hacker community. The early hacker culture and resulting philosophy originated at the Massachusetts Institute of Technology (MIT) in the 1950s and 1960s. The term "hacker ethic" is attributed to journalist Steven Levy as described in his 1984 book titled "." The key points within this ethic are access, freedom of information, and improvement to quality of life.
While some tenets of hacker ethic were described in other texts like "Computer Lib/Dream Machines" (1974) by Ted Nelson, Levy appears to have been the first to document both the philosophy and the founders of the philosophy.
Levy explains that MIT housed an early IBM 704 computer inside the Electronic Accounting Machinery (EAM) room in 1959. This room became the staging grounds for early hackers, as MIT students from the Tech Model Railroad Club sneaked inside the EAM room after hours to attempt programming the 30-ton, 9 ft computer.
The MIT group defined a "hack" as a project undertaken or a product built to fulfill some constructive goal, but also with some wild pleasure taken in mere involvement. The term "hack" arose from MIT lingo, as the word had long been used to describe college pranks that MIT students would regularly devise. However, Levy's hacker ethic also has often been quoted out of context and misunderstood to refer to hacking as in breaking into computers, and so many sources incorrectly imply that it is describing the ideals of white-hat hackers. However, what Levy is talking about does not necessarily have anything particular to do with computer security, but addresses broader issues.
The hacker ethic was described as a "new way of life, with a philosophy, an ethic and a dream". However, the elements of the hacker ethic were not openly debated and discussed; rather they were implicitly accepted and silently agreed upon.
The free software movement was born in the early 1980s from followers of the hacker ethic. Its founder, Richard Stallman, is referred to by Steven Levy as "the last true hacker". Modern hackers who hold true to the hacker ethics—especially the Hands-On Imperative—are usually supporters of free and open source software. This is because free and open source software allows hackers to get access to the source code used to create the software, to allow it to be improved or reused in other projects.
Richard Stallman describes:
The hacker ethic refers to the feelings of right and wrong, to the ethical ideas this community of people had—that knowledge should be shared with other people who can benefit from it, and that important resources should be utilized rather than wasted.
and states more precisely that hacking (which Stallman defines as playful cleverness) and ethics are two separate issues:
Just because someone enjoys hacking does not mean he has an ethical commitment to treating other people properly. Some hackers care about ethics—I do, for instance—but that is not part of being a hacker, it is a separate trait. [...] Hacking is not primarily about an ethical issue. [...] hacking tends to lead a significant number of hackers to think about ethical questions in a certain way. I would not want to completely deny all connection between hacking and views on ethics.
The hacker ethics.
As Levy summarized in the preface of "Hackers", the general tenets or principles of hacker ethic include:
In addition to those principles, Levy also described more specific hacker ethics and beliefs in chapter 2, "The Hacker Ethic": The ethics he described in chapter 2 are:
Sharing.
From the early days of modern computing through to the 1970s, it was far more common for computer users to have the freedoms that are provided by an ethic of open sharing and collaboration. Software, including source code, was commonly shared by individuals who used computers. Most companies had a business model based on hardware sales, and provided or bundled the associated software free of charge. According to Levy's account, sharing was the norm and expected within the non-corporate hacker culture. The principle of sharing stemmed from the open atmosphere and informal access to resources at MIT. During the early days of computers and programming, the hackers at MIT would develop a program and share it with other computer users.
If the hack was deemed particularly good, then the program might be posted on a board somewhere near one of the computers. Other programs that could be built upon it and improved it were saved to tapes and added to a drawer of programs, readily accessible to all the other hackers. At any time, a fellow hacker might reach into the drawer, pick out the program, and begin adding to it or "bumming" it to make it better. Bumming referred to the process of making the code more concise so that more can be done in fewer instructions, saving precious memory for further enhancements.
In the second generation of hackers, sharing was about sharing with the general public in addition to sharing with other hackers. A particular organization of hackers that was concerned with sharing computers with the general public was a group called Community Memory. This group of hackers and idealists put computers in public places for anyone to use. The first community computer was placed outside of Leopold's Records in Berkeley, California.
Another sharing of resources occurred when Bob Albrecht provided considerable resources for a non-profit organization called the People's Computer Company (PCC). PCC opened a computer center where anyone could use the computers there for fifty cents per hour.
This second generation practice of sharing contributed to the battles of free and open software. In fact, when Bill Gates' version of BASIC for the Altair was shared among the hacker community, Gates claimed to have lost a considerable sum of money because few users paid for the software. As a result, Gates wrote an Open Letter to Hobbyists. This letter was published by several computer magazines and newsletters, most notably that of the Homebrew Computer Club where much of the sharing occurred.
Hands-On Imperative.
Many of the principles and tenets of hacker ethic contribute to a common goal: the Hands-On Imperative. As Levy described in Chapter 2, "Hackers believe that essential lessons can be learned about the systems—about the world—from taking things apart, seeing how they work, and using this knowledge to create new and more interesting things."
Employing the Hands-On Imperative requires free access, open information, and the sharing of knowledge. To a true hacker, if the Hands-On Imperative is restricted, then the ends justify the means to make it unrestricted "so that improvements can be made". When these principles are not present, hackers tend to work around them. For example, when the computers at MIT were protected either by physical locks or login programs, the hackers there systematically worked around them in order to have access to the machines. Hackers assumed a "willful blindness" in the pursuit of perfection.
This behavior was not malicious in nature: the MIT hackers did not seek to harm the systems or their users. This deeply contrasts with the modern, media-encouraged image of hackers who crack secure systems in order to steal information or complete an act of cyber-vandalism.
Community and collaboration.
Throughout writings about hackers and their work processes, a common value of community and collaboration is present. For example, in Levy's "Hackers", each generation of hackers had geographically based communities where collaboration and sharing occurred. For the hackers at MIT, it was the labs where the computers were running. For the hardware hackers (second generation) and the game hackers (third generation) the geographic area was centered in Silicon Valley where the Homebrew Computer Club and the People's Computer Company helped hackers network, collaborate, and share their work.
The concept of community and collaboration is still relevant today, although hackers are no longer limited to collaboration in geographic regions. Now collaboration takes place via the Internet. Eric S. Raymond identifies and explains this conceptual shift in "The Cathedral and the Bazaar":
Before cheap Internet, there were some geographically compact communities where the culture encouraged Weinberg's egoless programming, and a developer could easily attract a lot of skilled kibitzers and co-developers. Bell Labs, the MIT AI and LCS labs, UC Berkeley: these became the home of innovations that are legendary and still potent.
Raymond also notes that the success of Linux coincided with the wide availability of the World Wide Web. The value of community is still in high practice and use today.
Levy's "true hackers".
Levy identifies several "true hackers" who significantly influenced the hacker ethic. Some well-known "true hackers" include:
Levy also identified the "hardware hackers" (the "second generation", mostly centered in Silicon Valley) and the "game hackers" (or the "third generation"). All three generations of hackers, according to Levy, embodied the principles of the hacker ethic. Some of Levy's "second-generation" hackers include:
Levy's "third generation" practitioners of hacker ethic include:
Other descriptions.
In 2001, Finnish philosopher Pekka Himanen promoted the hacker ethic in opposition to the Protestant work ethic. In Himanen's opinion, the hacker ethic is more closely related to the virtue ethics found in the writings of Plato and of Aristotle. Himanen explained these ideas in a book, "The Hacker Ethic and the Spirit of the Information Age", with a prologue contributed by Linus Torvalds and an epilogue by Manuel Castells.
In this manifesto, the authors wrote about a hacker ethic centering around passion, hard work, creativity and joy in creating software. Both Himanen and Torvalds were inspired by the Sampo in Finnish mythology. The Sampo, described in the Kalevala saga, was a magical artifact constructed by Ilmarinen, the blacksmith god, that brought good fortune to its holder; nobody knows exactly what it was supposed to be. The Sampo has been interpreted in many ways: a world pillar or world tree, a compass or astrolabe, a chest containing a treasure, a Byzantine coin die, a decorated Vendel period shield, a Christian relic, etc. Kalevala saga compiler Lönnrot interpreted it to be a "quern" or mill of some sort that made flour, salt, and gold out of thin air.
The hacker ethic and its wider context can be associated with liberalism and anarchism.

</doc>
<doc id="14276" url="http://en.wikipedia.org/wiki?curid=14276" title="Hotel">
Hotel

A hotel is an establishment that provides lodging paid on a short-term basis. Facilities provided may range from a basic bed and storage for clothing, to luxury features like en-suite bathrooms. Larger hotels may provide additional guest facilities such as a swimming pool, business center, childcare, conference facilities and social function services. Hotel rooms are usually numbered (or named in some smaller hotels and B&Bs) to allow guests to identify their room. Some hotels offer meals as part of a room and board arrangement. In the United Kingdom, a hotel is required by law to serve food and drinks to all guests within certain stated hours. In Japan, capsule hotels provide a minimized amount of room space and shared facilities.
The precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater for richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the 19th century, and luxury hotels began to spring up in the later part of the century.
Hotel operations vary in size, function, and cost. Most hotels and major hospitality companies have set industry standards to classify hotel types. An upscale full-service hotel facility offers luxury amenities, full service accommodations, on-site full service restaurant(s), and the highest level of personalized service. Full service hotels often contain upscale full-service facilities with a large volume of full service accommodations, on-site full service restaurant(s), and a variety of on-site amenities. Boutique hotels are smaller independent non-branded hotels that often contain upscale facilities. Small to medium-sized hotel establishments offer a limited amount of on-site amenities. Economy hotels are small to medium-sized hotel establishments that offer basic accommodations with little to no services. Extended stay hotels are small to medium-sized hotels that offer longer term full service accommodations compared to a traditional hotel.
Timeshare and Destination clubs are a form of property ownership involving ownership of an individual unit of accommodation for seasonal usage. A motel is a small-sized low-rise lodging with direct access to individual rooms from the car park. Boutique hotels are typically hotels with a unique environment or intimate setting. A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London. Some hotels are built specifically as a destination in itself, for example at casinos and holiday resorts. 
Most hotel establishments consist of a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.
Etymology.
The word "hotel" is derived from the French "hôtel" (coming from the same origin as "hospital"), which referred to a French version of a building seeing frequent visitors, and providing care, rather than a place offering accommodation. In contemporary French usage, "hôtel" now has the same meaning as the English term, and "hôtel particulier" is used for the old meaning, as well as "hôtel" in some place names such as Hôtel-Dieu (in Paris), which has been a hospital since the Middle Ages. The French spelling, with the circumflex, was also used in English, but is now rare. The circumflex replaces the 's' found in the earlier "hostel" spelling, which over time took on a new, but closely related meaning. Grammatically, hotels usually take the definite article – hence "The Astoria Hotel" or simply "The Astoria."
History.
Facilities offering hospitality to travellers have been a feature of the earliest civilizations. In Greco-Roman culture hospitals for recuperation and rest were built at thermal baths. During the Middle Ages various religious orders at monasteries and abbeys would offer accommodation for travellers on the road.
The precursor to the modern hotel was the inn of medieval Europe, possibly dating back to the rule of Ancient Rome. These would provide for the needs of travelers, including food and lodging, stabling and fodder for the traveler's horse(s) and fresh horses for the mail coach. Famous London examples of inns include the George and the Tabard. A typical layout of an inn had an inner court with bedrooms on the two sides, with the kitchen and parlour at the front and the stables at the back.
For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers (in other words, a roadhouse). Coaching inns stabled teams of horses for stagecoaches and mail coaches and replaced tired teams with fresh teams. Traditionally they were seven miles apart but this depended very much on the terrain.
Some English towns had as many as ten such inns and rivalry between them was intense, not only for the income from the stagecoach operators but for the revenue for food and drink supplied to the wealthy passengers. By the end of the century, coaching inns were being run more professionally, with a regular timetable being followed and fixed menus for food.
Inns began to cater for richer clients in the mid-18th century, and consequently grew in grandeur and the level of service provided. One of the first hotels in a modern sense was opened in Exeter in 1768, although the idea only really caught on in the early 19th century. In 1812 Mivart's Hotel opened its doors in London, later changing its name to Claridge's.
Hotels proliferated throughout Western Europe and North America in the 19th century, and luxury hotels, including Tremont House and Astor House in the United States, Savoy Hotel in the United Kingdom and the Ritz chain of hotels in London and Paris, began to spring up in the later part of the century, catering to an extremely wealthy clientele.
International scale.
Hotels cater to travelers from many countries and languages, since no one country dominates the travel industry.
Types.
Hotel operations vary in size, function, and cost. Most hotels and major hospitality companies that operate hotels have set widely accepted industry standards to classify hotel types. General categories include the following:
Upscale luxury.
An upscale full service hotel facility that offers luxury amenities, full service accommodations, on-site full service restaurant(s), and the highest level of personalized and professional service. Luxury hotels are normally classified with at least a Four Diamond or Five Diamond status or a Four or Five Star rating depending on the country and local classification standards. "Examples may include: Waldorf Astoria, Four Seasons, Conrad, Fairmont, and Ritz Carlton."
Full service.
Full service hotels often contain upscale full-service facilities with a large volume of full service accommodations, on-site full service restaurant(s), and a variety of on-site amenities such as swimming pools, a health club, children's activities, ballrooms, on-site conference facilities, and other amenities. "Examples may include: InterContinental, Starwood – Westin, Hilton, Marriott, and Hyatt hotels"
Historic inns and boutique hotels.
Boutique hotels are smaller independent non-branded hotels that often contain upscale facilities of varying size in unique or intimate settings with full service accommodations. Some historic inns and boutique hotels may be classified as luxury hotels.
Focused or select service.
Small to medium-sized hotel establishments that offer a limited amount of on-site amenities that only cater and market to a specific demographic of travelers, such as the single business traveler. Most focused or select service hotels may still offer full service accommodations but may lack leisure amenities such as an on-site restaurant or a swimming pool. Examples include Courtyard by Marriott and Hilton Garden Inn.
Economy and limited service.
Small to medium-sized hotel establishments that offer a very limited amount of on-site amenities and often only offer basic accommodations with little to no services, these facilities normally only cater and market to a specific demographic of travelers, such as the budget-minded traveler seeking a "no frills" accommodation. Limited service hotels often lack an on-site restaurant but in return may offer a limited complimentary food and beverage amenity such as on-site continental breakfast service. Examples include Hampton Inn, Aloft, Holiday Inn Express, Fairfield Inn, Four Points by Sheraton, and Days Inn.
Extended stay.
Extended stay hotels are small to medium-sized hotels that offer longer term full service accommodations compared to a traditional hotel. Extended stay hotels may offer non-traditional pricing methods such as a weekly rate that cater towards travelers in need of short-term accommodations for an extended period of time. Similar to limited and select service hotels, on-site amenities are normally limited and most extended stay hotels lack an on-site restaurant. Examples include Staybridge Suites, Homewood Suites by Hilton, Residence Inn by Marriott, Element, and Extended Stay Hotels.
Timeshare and destination clubs.
Timeshare and Destination clubs are a form of property ownership also referred to as a vacation ownership involving the purchase and ownership of an individual unit of accommodation for seasonal usage during a specified period of time. Timeshare resorts often offer amenities similar that of a Full service hotel with on-site restaurant(s), swimming pools, recreation grounds, and other leisure-oriented amenities. Destination clubs on the other hand may offer more exclusive private accommodations such as private houses in a neighborhood-style setting. Examples of timeshare brands include Hilton Grand Vacations, Marriott Vacation Club International, Westgate Resorts, Starwood Vacation Ownership, and Disney Vacation Club.
Motel.
A motel is a small-sized low-rise lodging establishment similar to that of a limited service hotel, but with direct access to individual rooms from the car park. Common during the 1950s and 1960s, motels were often located adjacent to a major road, where they were built on inexpensive land at the edge of towns or along stretches of highways . 
New motel construction is rare as hotel chains have been building economy limited service franchised properties at freeway exits which compete for largely the same clientele, largely saturating the market by the 1990s. They are still useful in less populated areas for driving travelers, but the more populated an area becomes the more hotels fill the need. Many of the motels which remain in operation have joined national franchise chains, rebranding themselves as hotels, inns or lodges.
Management.
Hotel management is a globally accepted professional career field and academic field of study. Degree programs such as hospitality management studies, a business degree, and/or certification programs formally prepare hotel managers for industry practice.
Most hotel establishments consist of a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.
Unique and specialty hotels.
Historic Inns and boutique hotels.
Boutique hotels are typically hotels with a unique environment or intimate setting.
Some hotels have gained their renown through tradition, by hosting significant events or persons, such as Schloss Cecilienhof in Potsdam, Germany, which derives its fame from the Potsdam Conference of the World War II allies Winston Churchill, Harry Truman and Joseph Stalin in 1945. The Taj Mahal Palace & Tower in Mumbai is one of India's most famous and historic hotels because of its association with the Indian independence movement. Some establishments have given name to a particular meal or beverage, as is the case with the Waldorf Astoria in New York City, United States where the Waldorf Salad was first created or the Hotel Sacher in Vienna, Austria, home of the Sachertorte. Others have achieved fame by association with dishes or cocktails created on their premises, such as the Hotel de Paris where the crêpe Suzette was invented or the Raffles Hotel in Singapore, where the Singapore Sling cocktail was devised.
A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London, through its association with Irving Berlin's song, 'Puttin' on the Ritz'. The Algonquin Hotel in New York City is famed as the meeting place of the literary group, the Algonquin Round Table, and Hotel Chelsea, also in New York City, has been the subject of a number of songs and the scene of the stabbing of Nancy Spungen (allegedly by her boyfriend Sid Vicious).
Resort hotels.
Some hotels are built specifically as a destination in itself to create a captive trade, example at casinos and holiday resorts. Though of course hotels have always been built in popular destinations, the defining characteristic of a resort hotel is that it exists purely to serve another attraction, the two having the same owners.
On the Las Vegas Strip there is a tradition of one-upmanship with luxurious and extravagant hotels in a concentrated area. This trend now has extended to other resorts worldwide, but the concentration in Las Vegas is still the world's highest: nineteen of the world's twenty-five largest hotels by room count are on the Strip, with a total of over 67,000 rooms.
In Europe Center Parcs might be considered a chain of resort hotels, since the sites are largely man-made (though set in natural surroundings such as country parks) with captive trade, whereas holiday camps such as Butlins and Pontin's are probably not considered as resort hotels, since they are set at traditional holiday destinations which existed before the camps.
Bunker hotels.
The Null Stern Hotel in Teufen, Appenzellerland, Switzerland and the Concrete Mushrooms in Albania are former nuclear bunkers transformed into hotels.
Cave hotels.
The Cuevas Pedro Antonio de Alarcón (named after the author) in Guadix, Spain, as well as several hotels in Cappadocia, Turkey, are notable for being built into natural cave formations, some with rooms underground. The Desert Cave Hotel in Coober Pedy, South Australia is built into the remains of an opal mine.
Cliff hotels.
Located on the coast but high above sea level, these hotels offer unobstructed panoramic views and a great sense of privacy without the feeling of total isolation. Some examples from around the globe are the Riosol Hotel in Gran Canaria, Caruso Belvedere Hotel in Amalfi Coast (Italy), Aman Resorts Amankila in Bali, Birkenhead House in Hermanus (South Africa), The Caves in Jamaica and Caesar Augustus in Capri.
Capsule hotels.
Capsule hotels are a type of economical hotel first introduced in Japan, where people sleep in stacks of rectangular containers.
Ice, snow and igloo hotels.
Igloo Village in Kakslauttanen,the Ice Hotel in Jukkasjärvi, Sweden is the first ice hotel in the world, built in 1990, and the Hotel de Glace in Duschenay, Canada, melt every spring and are rebuilt each winter; the Mammut Snow Hotel in Finland is located within the walls of the Kemi snow castle; and the Lainio Snow Hotel is part of a snow village near Ylläs, Finland.
Garden hotels.
Garden hotels, famous for their gardens before they became hotels, include Gravetye Manor, the home of garden designer William Robinson, and Cliveden, designed by Charles Barry with a rose garden by Geoffrey Jellicoe.
Referral hotel.
A referral hotel is a hotel chain that offers branding to independently-operated hotels; the chain itself is founded by or owned by the member hotels as a group. Many former referral chains have been converted to franchises; the largest surviving member-owned chain is Best Western.
Railway hotels.
Frequently, expanding railway companies built grand hotels at their termini, such as the Midland Hotel, Manchester next to the former Manchester Central Station, and in London the ones above St Pancras railway station and Charing Cross railway station. London also has the Chiltern Court Hotel above Baker Street tube station, there are also Canada's grand railway hotels. They are or were mostly, but not exclusively, used by those traveling by rail.
Straw bale hotels.
The Maya Guesthouse in Nax Mont-Noble in the Swiss Alps, is the first hotel in Europe built entirely with straw bales. Due to the insulation values of the walls it needs no conventional heating or air conditioning system, although the Maya Guesthouse is built at an altitude of 1,300 meters in the Alps.
Transit hotels.
Transit hotels are short stay hotels typically used at international airports where passengers can stay while waiting to change airplanes. The hotels are typically on the airside and do not require a visa for a stay.
Treehouse hotels.
Some hotels are built with living trees as structural elements, for example the Treehotel near Piteå, Sweden, the Costa Rica Tree House in the Gandoca-Manzanillo Wildlife Refuge, Costa Rica; the Treetops Hotel in Aberdare National Park, Kenya; the Ariau Towers near Manaus, Brazil, on the Rio Negro in the Amazon; and Bayram's Tree Houses in Olympos, Turkey.
Underwater hotels.
Some hotels have accommodation underwater, such as Utter Inn in Lake Mälaren, Sweden. Hydropolis, project in Dubai, would have had suites on the bottom of the Persian Gulf, and Jules' Undersea Lodge in Key Largo, Florida requires scuba diving to access its rooms.
Records.
Largest.
In 2006, "Guinness World Records" listed the First World Hotel in Genting Highlands, Malaysia, as the world's largest hotel with a total of 6,118 rooms. The Izmailovo Hotel in Moscow has the most rooms, with 7,500, followed by The Venetian and The Palazzo complex in Las Vegas (7,117 rooms) and MGM Grand Las Vegas complex (6,852 rooms).
Oldest.
According to the Guinness Book of World Records, the oldest hotel in operation is the Nisiyama Onsen Keiunkan in Yamanashi, Japan. The hotel, first opened in 707 A.D. has been operated by the same family for forty-six generations. The title was held until 2011 by the Hoshi Ryokan, in the Awazu Onsen area of Komatsu, Japan, which opened in the year 718, as the history of the Nisiyama Onsen Keiunkan was virtually unknown.
Highest.
The Ritz-Carlton Hong Kong claims to be the world's highest hotel. It is located on the top floors of the International Commerce Centre in Hong Kong, at 484 m above ground level.
Most expensive purchase.
In October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York in Manhattan for US$1.95 billion, making it the world's most expensive hotel ever sold.
Living in hotels.
A number of public figures have notably chosen to take up semi-permanent or permanent residence in hotels.
Further reading.
</dl>

</doc>
<doc id="14277" url="http://en.wikipedia.org/wiki?curid=14277" title="Hebrew mythology">
Hebrew mythology

Hebrew mythology may refer to:

</doc>
<doc id="14279" url="http://en.wikipedia.org/wiki?curid=14279" title="Hugh Hefner">
Hugh Hefner

Hugh Marston Hefner (born April 9, 1926) is an American adult magazine publisher, businessman, and a well-known playboy. Hefner is a Chicago, Illinois native and a former journalist for "Esquire". Hefner is also a World War II veteran. He is best known for being the founder and chief creative officer of Playboy Enterprises. A self-made millionaire, he is now worth over 43 million dollars. Hefner is also a political activist and philanthropist active in several causes and public issues.
Early life.
Hugh Hefner was born in Chicago, Illinois, the older of two sons (himself and brother Keith) of Grace Caroline (née Swanson; 1895–1997) and Glenn Lucius Hefner (1896–1976), both teachers. Hefner's mother was of Swedish descent, and his father had German and English ancestry. Through his father's line, Hefner has stated that he is a direct descendant of Plymouth governor William Bradford. He has described his family as "conservative, Midwestern, [and] Methodist". He went to Sayre Elementary School and Steinmetz High School, then during World War II, served as a writer for a military newspaper in the U.S. Army from 1944 to 1946. He later graduated from the University of Illinois at Urbana Champaign with a B.A. in psychology with a double minor in creative writing and art in 1949, earning his degree in two and a half years. After graduation, he took a semester of graduate courses in sociology at Northwestern University but dropped out soon after.
Career.
Working as a copywriter for "Esquire", he left in January 1952 after being denied a $5 raise. In 1953, he mortgaged his furniture, generating a bank loan of $600, and raised $8,000 from 45 investors, including $1,000 from his mother ("Not because she believed in the venture," he told "E!" in 2006, "but because she believed in her son."), to launch "Playboy", which was initially going to be called "Stag Party". The undated first issue, published in December 1953, featured Marilyn Monroe from her 1949 nude calendar shoot and sold over 50,000 copies. (Hefner, who never met Monroe, bought the crypt next to hers at the Westwood Village Memorial Park Cemetery.)
After it was rejected by "Esquire" magazine in 1955, Hefner agreed to publish in "Playboy" the Charles Beaumont science fiction short story "The Crooked Man", about straight men being persecuted in a world where homosexuality was the norm. After receiving angry letters to the magazine, Hefner wrote a response to criticism where he said, "If it was wrong to persecute heterosexuals in a homosexual society then the reverse was wrong, too." In 1961, Hefner watched Dick Gregory perform at the Herman Roberts Show Bar in Chicago. Based on that performance, Hefner hired Gregory to work at the Chicago Playboy Club; Gregory attributes the subsequent launch of his career to that night.
On June 4, 1963, Hefner was arrested for selling obscene literature after an issue of "Playboy" featuring nude shots of Jayne Mansfield was released. A jury was unable to reach a verdict.
In 1999, Hefner financed the Clara Bow documentary, "Discovering the It Girl." "Nobody has what Clara had. She defined an era and made her mark on the nation," he stated.
He has a star on the Hollywood Walk of Fame for television and has made several movie appearances as himself. In 2009, he received a "worst supporting actor" nomination for a Razzie award for his performance in "Miss March".
A documentary by Brigitte Berman, "", was released on July 30, 2010. He had previously granted full access to documentary filmmaker and television producer Kevin Burns for the A&E "Biography" special "Hugh Hefner: American Playboy" in 1996. Hefner and Burns later collaborated on numerous other television projects, most notably on "The Girls Next Door", a reality series that ran for six seasons (2005–2009) and 90 episodes.
Personal life.
Hefner married Northwestern University student Mildred Williams (born 1926) in 1949. They had two children, Christie Hefner (born 1952) and David (born 1955). Before the wedding, Mildred confessed that she had had an affair while he was away in the Army. He called the admission "the most devastating moment of my life." A 2006 "E! True Hollywood Story" profile of Hefner revealed that Mildred allowed him to sleep with other women, out of guilt for her infidelity and in the hopes that it would preserve their marriage. It didn't; they were divorced in 1959. 
Hefner remade himself as a bon viveur and man about town, a lifestyle he promoted in his magazine and two TV shows he hosted, "Playboy's Penthouse" (1959–1960) and "Playboy After Dark" (1969–1970). He admitted to being "'involved' with maybe eleven out of twelve months' worth of Playmates" during some of these years. Donna Michelle, Marilyn Cole, Lillian Müller, Shannon Tweed, Barbi Benton, Karen Christy, Sondra Theodore, and Carrie Leigh — who filed a $35 million palimony suit against him — were a few of his many lovers. In 1971, he acknowledged that he experimented in bisexuality. He moved from Chicago to Los Angeles.
Hefner had a minor stroke in 1985 at the age of 59. After re-evaluating his lifestyle, he made several changes. The wild, all-night parties were toned down significantly and in 1988, daughter Christie began to run the Playboy empire. The following year, he married Playmate of the Year Kimberley Conrad. The couple had two sons, Marston Glenn (born 1990) and Cooper Bradford (born 1991). The "E! True Hollywood Story" profile noted that the notorious Playboy Mansion had been transformed into a family-friendly homestead. After he and Conrad separated in 1998 she moved into a house next door to the mansion.
Hefner then began to move an ever-changing coterie of young women into the mansion, including twins Sandy and Mandy Bentley, and even dating up to seven girls at once, among them, Brande Roderick, Izabella St. James, Tina Marie Jordan, Holly Madison, Bridget Marquardt, and Kendra Wilkinson. The reality television series "The Girls Next Door" depicted the lives of Madison, Wilkinson and Marquardt at the Playboy Mansion. In October 2008, all three girls made the choice to leave the mansion. Hefner was quick to rebound and soon began dating his new "Number One" girlfriend, Crystal Harris, along with 20-year-old identical twin models Kristina and Karissa Shannon. The relationship with the twins ended in January 2010. After an 11-year separation, Hefner filed for divorce from Conrad stating irreconcilable differences. Hefner has said that he only remained married to her for the sake of his children, and his youngest child had just turned 18. The divorce was finalized in March 2010. On December 24, 2010, Hefner presented an engagement ring to Crystal Harris, publicly announcing the proposal the following day. Hefner and Harris had planned to marry June 18, 2011. Harris called off the wedding just 5 days before they were due to be wed. Harris and Hefner reconciled and were married on December 31, 2012. Hefner was 86 and Harris 26.
In 2012, Hefner announced that his youngest son, Cooper would likely succeed him as the public face of "Playboy".
Politics and philanthropy.
The Hugh Hefner First Amendment Award was created by Christie Hefner "to honor individuals who have made significant contributions in the vital effort to protect and enhance First Amendment rights for Americans."
He has donated and raised money for the Democratic Party. However, he has more recently referred to himself as an independent due to disillusionment with both the Democratic and Republican parties.
In 1978, Hefner helped organize fund-raising efforts that led to the restoration of the Hollywood Sign. He hosted a gala fundraiser at the Playboy Mansion and personally contributed $27,000 (or 1/9 of the total restoration costs) by purchasing the letter Y in a ceremonial auction.
Hefner donated $100,000 to the University of Southern California's School of Cinematic Arts to create a course called "Censorship in Cinema," and $2 million to endow a chair for the study of American film.
Both through his charitable foundation and individually, Hefner also contributes to charities outside the sphere of politics and publishing, throwing fundraiser events for Much Love Animal Rescue as well as Generation Rescue, a controversial anti-vaccinationist campaign organization supported by Jenny McCarthy.
On November 18, 2010, Children of the Night founder and president Dr. Lois Lee presented Hefner with the organization's first-ever Founder's Hero of the Heart Award in appreciation for his unwavering dedication, commitment and generosity.
On April 26, 2010, Hefner donated the last $900,000 sought by a conservation group for a land purchase needed to stop the development of the famed vista of the Hollywood Sign.
"Sylvilagus palustris hefneri", an endangered subspecies of Marsh rabbit, is named after him in honor of financial support that he provided.
Hefner supports legalizing same-sex marriage and he states that a fight for gay marriage is "a fight for all our rights. Without it, we will turn back the sexual revolution and return to an earlier, puritanical time."

</doc>
