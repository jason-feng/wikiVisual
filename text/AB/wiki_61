<doc id="21147" url="http://en.wikipedia.org/wiki?curid=21147" title="Natural selection">
Natural selection

Natural selection is the gradual process by which heritable biological traits become either more or less common in a population as a function of the effect of inherited traits on the differential reproductive success of organisms interacting with their environment. It is a key mechanism of evolution. The term "natural selection" was popularised by Charles Darwin, who intended it to be compared with artificial selection, now more commonly referred to as selective breeding.
Variation exists within all populations of organisms. This occurs partly because random mutations arise in the genome of an individual organism, and these mutations can be passed to offspring. Throughout the individuals’ lives, their genomes interact with their environments to cause variations in traits. (The environment of a genome includes the molecular biology in the cell, other cells, other individuals, populations, species, as well as the abiotic environment.) Individuals with certain variants of the trait may survive and reproduce more than individuals with other, less successful, variants. Therefore the population evolves. Factors that affect reproductive success are also important, an issue that Charles Darwin developed in his ideas on sexual selection and fecundity selection, for example.
Natural selection acts on the phenotype, or the observable characteristics of an organism, but the genetic (heritable) basis of any phenotype that gives a reproductive advantage may become more common in a population (see allele frequency). Over time, this process can result in populations that specialise for particular ecological niches (microevolution) and may eventually result in the emergence of new species (macroevolution). In other words, natural selection is an important process (though not the only process) by which evolution takes place within a population of organisms. Natural selection can be contrasted with artificial selection, in which humans intentionally choose specific traits (although they may not always get what they want). In natural selection there is no intentional choice. In other words, artificial selection is teleological and natural selection is not teleological.
Natural selection is one of the cornerstones of modern biology. The term was introduced by Darwin in his influential 1859 book "On the Origin of Species", in which natural selection was described as analogous to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favoured for reproduction. The concept of natural selection was originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, nothing was known of modern genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical and molecular genetics is termed the "modern evolutionary synthesis". Natural selection remains the primary explanation for adaptive evolution.
General principles.
Natural variation occurs among the individuals of any population of organisms. Many of these differences do not affect survival or reproduction, but some differences may improve the chances of survival and reproduction of a particular individual. A rabbit that runs faster than others may be more likely to escape from predators, and algae that are more efficient at extracting energy from sunlight will grow faster. Something that increases an organism's chances of survival will often also include its reproductive rate; however, sometimes there is a trade-off between survival and current reproduction. Ultimately, what matters is total lifetime reproduction of the organism.
The peppered moth exists in both light and dark colours in the United Kingdom, but during the industrial revolution, many of the trees on which the moths rested became blackened by soot, giving the dark-coloured moths an advantage in hiding from predators. This gave dark-coloured moths a better chance of surviving to produce dark-coloured offspring, and in just fifty years from the first dark moth being caught, nearly all of the moths in industrial Manchester were dark. The balance was reversed by the effect of the Clean Air Act 1956, and the dark moths became rare again, demonstrating the influence of natural selection on peppered moth evolution.
If the traits that give these individuals a reproductive advantage are also heritable, that is, passed from parent to child, then there will be a slightly higher proportion of fast rabbits or efficient algae in the next generation. This is known as "differential reproduction". Even if the reproductive advantage is very slight, over many generations any heritable advantage will become dominant in the population. In this way the natural environment of an organism "selects" for traits that confer a reproductive advantage, causing gradual changes or evolution of life. This effect was first described and named by Charles Darwin.
The concept of natural selection predates the understanding of genetics, the mechanism of heredity for all known life forms. In modern terms, selection acts on an organism's phenotype, or observable characteristics, but it is the organism's genetic make-up or genotype that is inherited. The phenotype is the result of the genotype and the environment in which the organism lives (see Genotype-phenotype distinction).
This is the link between natural selection and genetics, as described in the modern evolutionary synthesis. Although a complete theory of evolution also requires an account of how genetic variation arises in the first place (such as by mutation and sexual reproduction) and includes other evolutionary mechanisms (such as genetic drift and gene flow), natural selection appears to be the most important mechanism for creating complex adaptations in nature.
Nomenclature and usage.
The term "natural selection" has slightly different definitions in different contexts. It is most often defined to operate on heritable traits, because these are the traits that directly participate in evolution. However, natural selection is "blind" in the sense that changes in phenotype (physical and behavioural characteristics) can give a reproductive advantage regardless of whether or not the trait is heritable (non heritable traits can be the result of environmental factors or the life experience of the organism).
Following Darwin's primary usage the term is often used to refer to both the evolutionary consequence of blind selection and to its mechanisms. It is sometimes helpful to explicitly distinguish between selection's mechanisms and its effects; when this distinction is important, scientists define "natural selection" specifically as "those mechanisms that contribute to the selection of individuals that reproduce", without regard to whether the basis of the selection is heritable. This is sometimes referred to as "phenotypic natural selection".
Traits that cause greater reproductive success of an organism are said to be selected for, whereas those that reduce success are selected against. Selection for a trait may also result in the selection of other correlated traits that do not themselves directly influence reproductive advantage. This may occur as a result of pleiotropy or gene linkage.
Fitness.
The concept of fitness is central to natural selection. In broad terms, individuals that are more "fit" have better potential for survival, as in the well-known phrase "survival of the fittest". However, as with natural selection above, the precise meaning of the term is much more subtle. Modern evolutionary theory defines fitness not by how long an organism lives, but by how successful it is at reproducing. If an organism lives half as long as others of its species, but has twice as many offspring surviving to adulthood, its genes will become more common in the adult population of the next generation.
Though natural selection acts on individuals, the effects of chance mean that fitness can only really be defined "on average" for the individuals within a population. The fitness of a particular genotype corresponds to the average effect on all individuals with that genotype. Very low-fitness genotypes cause their bearers to have few or no offspring on average; examples include many human genetic disorders like cystic fibrosis.
Since fitness is an averaged quantity, it is also possible that a favourable mutation arises in an individual that does not survive to adulthood for unrelated reasons. Fitness also depends crucially upon the environment. Conditions like sickle-cell anaemia may have low fitness in the general human population, but because the sickle-cell trait confers immunity from malaria, it has high fitness value in populations that have high malaria infection rates.
Types of selection.
Natural selection can act on any heritable phenotypic trait, and selective pressure can be produced by any aspect of the environment, including sexual selection and competition with members of the same or other species. However, this does not imply that natural selection is always directional and results in adaptive evolution; natural selection often results in the maintenance of the status quo by eliminating less fit variants.
Selection can be classified according to its effect on a trait. Stabilising selection acts to hold a trait at a stable optimum, and in the simplest case all deviations from this optimum are selectively disadvantageous. Directional selection acts during transition periods, when the current mode of the trait is sub-optimal, and alters the trait towards a single new optimum. Disruptive selection also acts during transition periods when the current mode is sub-optimal, but alters the trait in more than one direction. In particular, if the trait is quantitative and univariate then both higher and lower trait levels are favoured. Disruptive selection can be a precursor to speciation.
Selection can also be classified according to its effect on allele frequency. Positive selection acts to increase the frequency of an allele. Negative selection acts to decrease the frequency of an allele. Note that for a diallelic locus, positive selection on one allele perforce implies negative selection on the other allele.
Selection can also be classified according to its effect on genetic diversity. Purifying selection acts to remove genetic variation from the population (and is opposed by "de novo" mutation, which introduces new variation). Balancing selection acts to maintain genetic variation in a population (even in the absence of "de novo" mutation). Mechanisms include negative frequency-dependent selection (of which heterozygous advantage is a special case), and spatial and/or temporal fluctuations in the strength and direction of selection.
Selection can also be classified according to the stage of an organism’s life cycle at which it acts. The use of terminology differs here. Some recognise just two types of selection: viability selection (or survival selection) which acts to improve the probability of survival of the organism, and fecundity selection (or fertility selection, or reproductive selection) which acts to improve the rate of reproduction, given successful survival. Others split the life cycle into further components of selection (see figure). Thus viability and survival selection may be defined separately and respectively as acting to improve the probability of survival before and after reproductive age is reached, while fecundity selection may be split into additional sub-components including sexual selection, "gametic selection" (acting on gamete survival) and "compatibility selection" (acting on zygote formation).
Selection can also be classified according to the level or unit of selection. "Individual selection" acts at the level of the individual, in the sense that adaptions are ‘for’ the benefit of the individual, and result from selection among individuals. Gene selection acts directly at the level of the gene. In many situations, this is simply a different way of describing individual selection. However, in some cases (e.g. kin selection and intragenomic conflict), gene-level selection provides a more apt explanation of the underlying process. Group selection acts at the level of groups of organisms. The mechanism assumes that groups replicate and mutate in an analogous way to genes and individuals. There is an ongoing debate over the degree to which group selection occurs in nature.
Finally, selection can be classified according to the resource being competed for. Sexual selection results from competition for mates. Sexual selection can be "intrasexual", as in cases of competition among individuals of the same sex in a population, or "intersexual", as in cases where one sex controls reproductive access by choosing among a population of available mates. Typically, sexual selection proceeds via fecundity selection, sometimes at the expense of viability. Ecological selection is natural selection via any other means than sexual selection. Alternatively, natural selection is sometimes defined as synonymous with ecological selection, and sexual selection is then classified as a separate mechanism to natural selection. This accords with Darwin’s usage of these terms, but ignores the fact that mate competition and mate choice are natural processes.
Note that types of selection often act in concert. Thus stabilising selection typically proceeds via negative selection on rare alleles, leading to purifying selection, while directional selection typically proceeds via positive selection on an initially rare favoured allele.
Sexual selection.
It is useful to distinguish between "ecological selection" and "sexual selection". Ecological selection covers any mechanism of selection as a result of the environment (including relatives, e.g. kin selection, competition, and infanticide), while "sexual selection" refers specifically to competition for mates.
Sexual selection can be "intrasexual", as in cases of competition among individuals of the same sex in a population, or "intersexual", as in cases where one sex controls reproductive access by choosing among a population of available mates. Most commonly, intrasexual selection involves male–male competition and intersexual selection involves female choice of suitable males, due to the generally greater investment of resources for a female than a male in a single offspring. However, some species exhibit sex-role reversed behaviour in which it is males that are most selective in mate choice; the best-known examples of this pattern occur in some fishes of the family "Syngnathidae", though likely examples have also been found in amphibian and bird species.
Some features that are confined to one sex only of a particular species can be explained by selection exercised by the other sex in the choice of a mate, for example, the extravagant plumage of some male birds. Similarly, aggression between members of the same sex is sometimes associated with very distinctive features, such as the antlers of stags, which are used in combat with other stags. More generally, intrasexual selection is often associated with sexual dimorphism, including differences in body size between males and females of a species.
Examples of natural selection.
A well-known example of natural selection in action is the development of antibiotic resistance in microorganisms. Since the discovery of penicillin in 1928, antibiotics have been used to fight bacterial diseases. Natural populations of bacteria contain, among their vast numbers of individual members, considerable variation in their genetic material, primarily as the result of mutations. When exposed to antibiotics, most bacteria die quickly, but some may have mutations that make them slightly less susceptible. If the exposure to antibiotics is short, these individuals will survive the treatment. This selective elimination of maladapted individuals from a population is natural selection.
These surviving bacteria will then reproduce again, producing the next generation. Due to the elimination of the maladapted individuals in the past generation, this population contains more bacteria that have some resistance against the antibiotic. At the same time, new mutations occur, contributing new genetic variation to the existing genetic variation. Spontaneous mutations are very rare, and advantageous mutations are even rarer. However, populations of bacteria are large enough that a few individuals will have beneficial mutations. If a new mutation reduces their susceptibility to an antibiotic, these individuals are more likely to survive when next confronted with that antibiotic.
Given enough time and repeated exposure to the antibiotic, a population of antibiotic-resistant bacteria will emerge. This new changed population of antibiotic-resistant bacteria is optimally adapted to the context it evolved in. At the same time, it is not necessarily optimally adapted any more to the old antibiotic free environment. The end result of natural selection is two populations that are both optimally adapted to their specific environment, while both perform substandard in the other environment.
The widespread use and misuse of antibiotics has resulted in increased microbial resistance to antibiotics in clinical use, to the point that the methicillin-resistant "Staphylococcus aureus" (MRSA) has been described as a "superbug" because of the threat it poses to health and its relative invulnerability to existing drugs. Response strategies typically include the use of different, stronger antibiotics; however, new strains of MRSA have recently emerged that are resistant even to these drugs.
This is an example of what is known as an evolutionary arms race, in which bacteria continue to develop strains that are less susceptible to antibiotics, while medical researchers continue to develop new antibiotics that can kill them. A similar situation occurs with pesticide resistance in plants and insects. Arms races are not necessarily induced by man; a well-documented example involves the spread of a gene in the butterfly "Hypolimnas bolina" suppressing male-killing activity by Wolbachia bacteria parasites on the island of Samoa, where the spread of the gene is known to have occurred over a period of just five years 
Evolution by means of natural selection.
A prerequisite for natural selection to result in adaptive evolution, novel traits and speciation, is the presence of heritable genetic variation that results in fitness differences. Genetic variation is the result of mutations, recombinations and alterations in the karyotype (the number, shape, size and internal arrangement of the chromosomes). Any of these changes might have an effect that is highly advantageous or highly disadvantageous, but large effects are very rare. In the past, most changes in the genetic material were considered neutral or close to neutral because they occurred in noncoding DNA or resulted in a synonymous substitution. However, recent research suggests that many mutations in non-coding DNA do have slight deleterious effects. Although both mutation rates and average fitness effects of mutations are dependent on the organism, estimates from data in humans have found that a majority of mutations are slightly deleterious.
By the definition of fitness, individuals with greater fitness are more likely to contribute offspring to the next generation, while individuals with lesser fitness are more likely to die early or fail to reproduce. As a result, alleles that on average result in greater fitness become more abundant in the next generation, while alleles that in general reduce fitness become rarer. If the selection forces remain the same for many generations, beneficial alleles become more and more abundant, until they dominate the population, while alleles with a lesser fitness disappear. In every generation, new mutations and re-combinations arise spontaneously, producing a new spectrum of phenotypes. Therefore, each new generation will be enriched by the increasing abundance of alleles that contribute to those traits that were favoured by selection, enhancing these traits over successive generations.
Some mutations occur in so-called regulatory genes. Changes in these can have large effects on the phenotype of the individual because they regulate the function of many other genes. Most, but not all, mutations in regulatory genes result in non-viable zygotes. Examples of nonlethal regulatory mutations occur in HOX genes in humans, which can result in a cervical rib or polydactyly, an increase in the number of fingers or toes. When such mutations result in a higher fitness, natural selection will favour these phenotypes and the novel trait will spread in the population.
Established traits are not immutable; traits that have high fitness in one environmental context may be much less fit if environmental conditions change. In the absence of natural selection to preserve such a trait, it will become more variable and deteriorate over time, possibly resulting in a vestigial manifestation of the trait, also called evolutionary baggage. In many circumstances, the apparently vestigial structure may retain a limited functionality, or may be co-opted for other advantageous traits in a phenomenon known as preadaptation. A famous example of a vestigial structure, the eye of the blind mole rat, is believed to retain function in photoperiod perception.
Speciation.
Speciation requires selective mating, which result in a reduced gene flow. Selective mating can be the result of 1. Geographic isolation, 2. Behavioural isolation, or 3. Temporal isolation. For example, a change in the physical environment (geographic isolation by an extrinsic barrier) would follow number 1, a change in camouflage for number 2 or a shift in mating times (i.e., one species of deer shifts location and therefore changes its "rut") for number 3.
Over time, these subgroups might diverge radically to become different species, either because of differences in selection pressures on the different subgroups, or because different mutations arise spontaneously in the different populations, or because of founder effects – some potentially beneficial alleles may, by chance, be present in only one or other of two subgroups when they first become separated. A lesser-known mechanism of speciation occurs via hybridisation, well-documented in plants and occasionally observed in species-rich groups of animals such as cichlid fishes. Such mechanisms of rapid speciation can reflect a mechanism of evolutionary change known as punctuated equilibrium, which suggests that evolutionary change and in particular speciation typically happens quickly after interrupting long periods of stasis.
Genetic changes within groups result in increasing incompatibility between the genomes of the two subgroups, thus reducing gene flow between the groups. Gene flow will effectively cease when the distinctive mutations characterising each subgroup become fixed. As few as two mutations can result in speciation: if each mutation has a neutral or positive effect on fitness when they occur separately, but a negative effect when they occur together, then fixation of these genes in the respective subgroups will lead to two reproductively isolated populations. According to the biological species concept, these will be two different species.
Historical development.
Pre-Darwinian theories.
Several ancient philosophers expressed the idea that nature produces a huge variety of creatures, randomly, and that only those creatures that manage to provide for themselves and reproduce successfully survive; well-known examples include Empedocles and his intellectual successor, the Roman poet Lucretius. Empedocles' idea that organisms arose entirely by the incidental workings of causes such as heat and cold was criticised by Aristotle in Book II of "Physics". He posited natural teleology in its place. He believed that form was achieved for a purpose, citing the regularity of heredity in species as proof. Nevertheless, he acceded that new types of animals, monstrosities (τερας), can occur in very rare instances ("Generation of Animals", Book IV). As quoted in Darwin's Origin of Species, Aristotle considered whether different forms, e.g. of teeth, might have appeared accidentally, but only the useful forms survived:
So what hinders the different parts (of the body) from having this merely accidental relation in nature? as the teeth, for example, grow by necessity, the front ones sharp, adapted for dividing, and the grinders flat, and serviceable for masticating the food; since they were not made for the sake of this, but it was the result of accident. And in like manner as to other parts in which there appears to exist an adaptation to an end. Wheresoever, therefore, all things together (that is all the parts of one whole) happened like as if they were made for the sake of something, these were preserved, having been appropriately constituted by an internal spontaneity; and whatsoever things were not thus constituted, perished and still perish.—Aristotle, "Physicae Auscultationes"(lib.2, cap.8, s.2)
But he rejected this possibility in the next paragraph:
...Yet it is impossible that this should be the true view. For teeth and all other natural things either invariably or normally come about in a given way; but of not one of the results of chance or spontaneity is this true. We do not ascribe to chance or mere coincidence the frequency of rain in winter, but frequent rain in summer we do; nor heat in the dog-days, but only if we have it in winter. If then, it is agreed that things are either the result of coincidence or for an end, and these cannot be the result of coincidence or spontaneity, it follows that they must be for an end; and that such things are all due to nature even the champions of the theory which is before us would agree. Therefore action for an end is present in things which come to be and are by nature.—Aristotle, Aristotle, "Physicae Auscultationes"(lib.2, cap.8, s.2)
The struggle for existence was later described by Islamic writer Al-Jahiz in the 9th century, who argued that environmental factors influence animals to develop new characteristics to ensure survival.
The classical arguments were reintroduced in the 18th century by Pierre Louis Maupertuis and others, including Charles Darwin's grandfather Erasmus Darwin. While these forerunners had an influence on Darwinism, they later had little influence on the trajectory of evolutionary thought after Charles Darwin.
Until the early 19th century, the prevailing view in Western societies was that differences between individuals of a species were uninteresting departures from their Platonic idealism (or typus) of created kinds. However, the theory of uniformitarianism in geology promoted the idea that simple, weak forces could act continuously over long periods of time to produce radical changes in the Earth's landscape. The success of this theory raised awareness of the vast scale of geological time and made plausible the idea that tiny, virtually imperceptible changes in successive generations could produce consequences on the scale of differences between species.
Early 19th-century evolutionists such as Jean Baptiste Lamarck suggested the inheritance of acquired characteristics as a mechanism for evolutionary change; adaptive traits acquired by an organism during its lifetime could be inherited by that organism's progeny, eventually causing transmutation of species. This theory has come to be known as Lamarckism and was an influence on the anti-genetic ideas of the Stalinist Soviet biologist Trofim Lysenko.
Between 1835 and 1837, zoologist Edward Blyth also contributed specifically to the area of variation, artificial selection, and how a similar process occurs in nature (see Edward Blyth#On natural selection). In fact, Charles Darwin showed his high regards for Blyth's ideas in the first chapter on variation of "On the Origin of Species" that he wrote, "Mr. Blyth, whose opinion, from his large and varied stores of knowledge, I should value more than that of almost any one, ..."
Darwin's theory.
In 1859, Charles Darwin set out his theory of evolution by natural selection as an explanation for adaptation and speciation. He defined natural selection as the "principle by which each slight variation [of a trait], if useful, is preserved". The concept was simple but powerful: individuals best adapted to their environments are more likely to survive and reproduce. As long as there is some variation between them and that variation is heritable, there will be an inevitable selection of individuals with the most advantageous variations. If the variations are inherited, then differential reproductive success will lead to a progressive evolution of particular populations of a species, and populations that evolve to be sufficiently different eventually become different species.
Darwin's ideas were inspired by the observations that he had made on the "Beagle" voyage, and by the work of a political economist, the Reverend Thomas Malthus, who in "An Essay on the Principle of Population", noted that population (if unchecked) increases exponentially, whereas the food supply grows only arithmetically; thus, inevitable limitations of resources would have demographic implications, leading to a "struggle for existence". When Darwin read Malthus in 1838 he was already primed by his work as a naturalist to appreciate the "struggle for existence" in nature and it struck him that as population outgrew resources, "favourable variations would tend to be preserved, and unfavourable ones to be destroyed. The result of this would be the formation of new species."
Here is Darwin's own summary of the idea, which can be found in the fourth chapter of the "Origin":
Once he had his theory "by which to work", Darwin was meticulous about gathering and refining evidence as his "prime hobby" before making his idea public. He was in the process of writing his "big book" to present his researches when the naturalist Alfred Russel Wallace independently conceived of the principle and described it in an essay he sent to Darwin to forward to Charles Lyell. Lyell and Joseph Dalton Hooker decided (without Wallace's knowledge) to present his essay together with unpublished writings that Darwin had sent to fellow naturalists, and "On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection" was read to the Linnean Society announcing co-discovery of the principle in July 1858. Darwin published a detailed account of his evidence and conclusions in "On the Origin of Species" in 1859. In the 3rd edition of 1861 Darwin acknowledged that others — a notable one being William Charles Wells in 1813, and Patrick Matthew in 1831 — had proposed similar ideas, but had neither developed them nor presented them in notable scientific publications.
Darwin thought of natural selection by analogy to how farmers select crops or livestock for breeding, which he called "artificial selection"; in his early manuscripts he referred to a "Nature", which would do the selection. At the time, other mechanisms of evolution such as evolution by genetic drift were not yet explicitly formulated, and Darwin believed that selection was likely only part of the story: "I am convinced that [it] has been the main, but not exclusive means of modification." In a letter to Charles Lyell in September 1860, Darwin regretted the use of the term "Natural Selection", preferring the term "Natural Preservation".
For Darwin and his contemporaries, natural selection was in essence synonymous with evolution by natural selection. After the publication of "On the Origin of Species", educated people generally accepted that evolution had occurred in some form. However, natural selection remained controversial as a mechanism, partly because it was perceived to be too weak to explain the range of observed characteristics of living organisms, and partly because even supporters of evolution balked at its "unguided" and non-progressive nature, a response that has been characterised as the single most significant impediment to the idea's acceptance.
However, some thinkers enthusiastically embraced natural selection; after reading Darwin, Herbert Spencer introduced the term "survival of the fittest", which became a popular summary of the theory. The fifth edition of "On the Origin of Species" published in 1869 included Spencer's phrase as an alternative to natural selection, with credit given: "But the expression often used by Mr. Herbert Spencer, of the Survival of the Fittest, is more accurate, and is sometimes equally convenient." Although the phrase is still often used by non-biologists, modern biologists avoid it because it is tautological if "fittest" is read to mean "functionally superior" and is applied to individuals rather than considered as an averaged quantity over populations.
Modern evolutionary synthesis.
Natural selection relies crucially on the idea of heredity, but developed before the basic concepts of genetics. Although the Austrian monk Gregor Mendel (1822-1884), the father of modern genetics, was a contemporary of Darwin's, his work would lie in obscurity until the early 20th century. Only after the 20th-century integration of Darwin's theory of evolution with a complex statistical appreciation of Gregor Mendel's "re-discovered" laws of inheritance did scientists generally come to accept natural selection.
The work of Ronald Fisher (who developed the required mathematical language and wrote "The Genetical Theory of Natural Selection"), J.B.S. Haldane (who introduced the concept of the "cost" of natural selection), Sewall Wright (who elucidated the nature of selection and adaptation), Theodosius Dobzhansky (who established the idea that mutation, by creating genetic diversity, supplied the raw material for natural selection: see "Genetics and the Origin of Species"), William Hamilton (who conceived of kin selection), Ernst Mayr (who recognised the key importance of reproductive isolation for speciation: see "Systematics and the Origin of Species") and many others together formed the modern evolutionary synthesis. This synthesis cemented natural selection as the foundation of evolutionary theory, where it remains today.
Genetic basis of natural selection.
The idea of natural selection predates the understanding of genetics. We now have a much better idea of the biology underlying heritability, which is the basis of natural selection.
Genotype and phenotype.
Natural selection acts on an organism's phenotype, or physical characteristics. Phenotype is determined by an organism's genetic make-up (genotype) and the environment in which the organism lives. Often, natural selection acts on specific traits of an individual, and the terms phenotype and genotype are used narrowly to indicate these specific traits.
When different organisms in a population possess different versions of a gene for a certain trait, each of these versions is known as an allele. It is this genetic variation that underlies phenotypic traits. A typical example is that certain combinations of genes for eye colour in humans that, for instance, give rise to the phenotype of blue eyes. (On the other hand, when all the organisms in a population share the same allele for a particular trait, and this state is stable over time, the allele is said to be "fixed" in that population.)
Some traits are governed by only a single gene, but most traits are influenced by the interactions of many genes. A variation in one of the many genes that contributes to a trait may have only a small effect on the phenotype; together, these genes can produce a continuum of possible phenotypic values.
Directionality of selection.
When some component of a trait is heritable, selection will alter the frequencies of the different alleles, or variants of the gene that produces the variants of the trait. Selection can be divided into three classes, on the basis of its effect on allele frequencies.
Directional selection occurs when a certain allele has a greater fitness than others, resulting in an increase of its frequency. This process can continue until the allele is fixed and the entire population shares the fitter phenotype. It is directional selection that is illustrated in the antibiotic resistance example above.
Far more common is stabilising selection (which is commonly confused with "purifying selection"), which lowers the frequency of alleles that have a deleterious effect on the phenotype – that is, produce organisms of lower fitness. This process can continue until the allele is eliminated from the population. Purifying selection results in functional genetic features, such as protein-coding genes or regulatory sequences, being conserved over time due to selective pressure against deleterious variants.
Finally, a number of forms of balancing selection exist, which do not result in fixation, but maintain an allele at intermediate frequencies in a population. This can occur in diploid species (that is, those that have homologous pairs of chromosomes) when heterozygote individuals, who have different alleles on each chromosome at a single genetic locus, have a higher fitness than homozygote individuals that have two of the same alleles. This is called heterozygote advantage or over-dominance, of which the best-known example is the malarial resistance observed in heterozygous humans who carry only one copy of the gene for sickle cell anaemia. Maintenance of allelic variation can also occur through disruptive or diversifying selection, which favours genotypes that depart from the average in either direction (that is, the opposite of over-dominance), and can result in a bimodal distribution of trait values. Finally, balancing selection can occur through frequency-dependent selection, where the fitness of one particular phenotype depends on the distribution of other phenotypes in the population. The principles of game theory have been applied to understand the fitness distributions in these situations, particularly in the study of kin selection and the evolution of reciprocal altruism.
Selection and genetic variation.
A portion of all genetic variation is functionally neutral in that it produces no phenotypic effect or significant difference in fitness; the hypothesis that this variation accounts for a large fraction of observed genetic diversity is known as the neutral theory of molecular evolution and was originated by Motoo Kimura. When genetic variation does not result in differences in fitness, selection cannot "directly" affect the frequency of such variation. As a result, the genetic variation at those sites will be higher than at sites where variation does influence fitness. However, after a period with no new mutation, the genetic variation at these sites will be eliminated due to genetic drift.
Mutation selection balance.
Natural selection results in the reduction of genetic variation through the elimination of maladapted individuals and consequently of the mutations that caused the maladaptation. At the same time, new mutations occur, resulting in a mutation-selection balance. The exact outcome of the two processes depends both on the rate at which new mutations occur and on the strength of the natural selection, which is a function of how unfavourable the mutation proves to be. Consequently, changes in the mutation rate or the selection pressure will result in a different mutation-selection balance.
Genetic linkage.
Genetic linkage occurs when the loci of two alleles are "linked", or in close proximity to each other on the chromosome. During the formation of gametes, recombination of the genetic material results in reshuffling of the alleles. However, the chance that such a reshuffle occurs between two alleles depends on the distance between those alleles; the closer the alleles are to each other, the less likely it is that such a reshuffle will occur. Consequently, when selection targets one allele, this automatically results in selection of the other allele as well; through this mechanism, selection can have a strong influence on patterns of variation in the genome.
Selective sweeps occur when an allele becomes more common in a population as a result of positive selection. As the prevalence of one allele increases, linked alleles can also become more common, whether they are neutral or even slightly deleterious. This is called "genetic hitchhiking". A strong selective sweep results in a region of the genome where the positively selected haplotype (the allele and its neighbours) are in essence the only ones that exist in the population.
Whether a selective sweep has occurred or not can be investigated by measuring linkage disequilibrium, or whether a given haplotype is overrepresented in the population. Normally, genetic recombination results in a reshuffling of the different alleles within a haplotype, and none of the haplotypes will dominate the population. However, during a selective sweep, selection for a specific allele will also result in selection of neighbouring alleles. Therefore, the presence of a block of strong linkage disequilibrium might indicate that there has been a 'recent' selective sweep near the centre of the block, and this can be used to identify sites recently under selection.
Background selection is the opposite of a selective sweep. If a specific site experiences strong and persistent purifying selection, linked variation will tend to be weeded out along with it, producing a region in the genome of low overall variability. Because background selection is a result of deleterious new mutations, which can occur randomly in any haplotype, it does not produce clear blocks of linkage disequilibrium, although with low recombination it can still lead to slightly negative linkage disequilibrium overall.
Competition.
In the context of natural selection, competition is an interaction between organisms or species in which the fitness of one is lowered by the presence of another. Limited supply of at least one resource (such as food, water, and territory) used by both can be a factor. Competition; both within and between species is an important topic in ecology, especially community ecology. Competition is one of many interacting biotic and abiotic factors that affect community structure. Competition among members of the same species is known as intraspecific competition, while competition between individuals of different species is known as interspecific competition. Competition is not always straightforward, and can occur in both a direct and indirect fashion.
According to the competitive exclusion principle, species less suited to compete for resources should either adapt or die out, although competitive exclusion is rarely found in natural ecosystems. According to evolutionary theory, this competition within and between species for resources plays a very relevant role in natural selection, however, competition may play less of a role than expansion among larger clades, this is termed the 'Room to Roam' hypothesis.
In evolutionary contexts, competition is related to the concept of r/K selection theory, which relates to the selection of traits which promote success in particular environments. The theory originates from work on island biogeography by the ecologists Robert MacArthur and E. O. Wilson.
In r/K selection theory, selective pressures are hypothesised to drive evolution in one of two stereotyped directions: "r"- or "K"-selection. These terms, r and K, are derived from standard ecological algebra, as illustrated in the simple Verhulst equation of population dynamics:
where "r" is the growth rate of the population ("N"), and "K" is the carrying capacity of its local environmental setting. Typically, r-selected species exploit empty niches, and produce many offspring, each of whom has a relatively low probability of surviving to adulthood. In contrast, K-selected species are strong competitors in crowded niches, and invest more heavily in much fewer offspring, each of whom has a relatively high probability of surviving to adulthood.
Impact of the idea.
Darwin's ideas, along with those of Adam Smith and Karl Marx, had a profound influence on 19th century thought. Perhaps the most radical claim of the theory of evolution through natural selection is that "elaborately constructed forms, so different from each other, and dependent on each other in so complex a manner" evolved from the simplest forms of life by a few simple principles. This claim inspired some of Darwin's most ardent supporters—and provoked the most profound opposition. The radicalism of natural selection, according to Stephen Jay Gould, lay in its power to "dethrone some of the deepest and most traditional comforts of Western thought". In particular, it challenged long-standing beliefs in such concepts as a special and exalted place for humans in the natural world and a benevolent creator whose intentions were reflected in nature's order and design.
In the words of the philosopher Daniel Dennett, "Darwin's dangerous idea" of evolution by natural selection is a "universal acid," which cannot be kept restricted to any vessel or container, as it soon leaks out, working its way into ever-wider surroundings. Thus, in the last decades, the concept of natural selection has spread from evolutionary biology into virtually all disciplines, including evolutionary computation, quantum darwinism, evolutionary economics, evolutionary epistemology, evolutionary psychology, and cosmological natural selection. This unlimited applicability has been called Universal Darwinism.
Emergence of natural selection.
How life originated from inorganic matter (abiogensis) remains an unresolved problem in biology. One prominent hypothesis is that life first appeared in the form of short self-replicating RNA polymers. (see RNA world hypothesis) On this view, “life” may have come into existence when RNA chains first experienced the basic conditions, as conceived by Darwin, for natural selection to operate. These conditions are: heritability, variation of type, and competition for limited resources. Fitness of an early RNA replicator (its per capita rate of increase) would likely have been a function of adaptive capacities that were intrinsic (i.e. determined by the nucleotide sequence) and the availability of resources. The three primary adaptive capacities could logically have been: (1) the capacity to replicate with moderate fidelity (giving rise to both heritability and variation of type), (2) the capacity to avoid decay, and (3) the capacity to acquire and process resources. These capacities would have been determined initially by the folded configurations (including those configurations with ribozyme activity) of the RNA replicators that, in turn, would have been encoded in their individual nucleotide sequences. Competitive success among different RNA replicators would have depended on the relative values of their adaptive capacities.
Cell and molecular biology.
In the 19th century, Wilhelm Roux, a founder of modern embryology, wrote a book entitled « Der Kampf der Teile im Organismus » (The struggle of parts in the organism) in which he suggested that the development of an organism results from a Darwinian competition between the parts of the embryo, occurring at all levels, from molecules to organs. In recent years, a modern version of this theory has been proposed by . According to this , stochasticity at the molecular level generates diversity in cell types whereas cell interactions impose a characteristic order on the developing embryo.
Social and psychological theory.
The social implications of the theory of evolution by natural selection also became the source of continuing controversy. Friedrich Engels, a German political philosopher and co-originator of the ideology of communism, wrote in 1872 that "Darwin did not know what a bitter satire he wrote on mankind when he showed that free competition, the struggle for existence, which the economists celebrate as the highest historical achievement, is the normal state of the animal kingdom". Interpretation of natural selection as necessarily "progressive", leading to increasing "advances" in intelligence and civilisation, was used as a justification for colonialism and policies of eugenics, as well as to support broader sociopolitical positions now described as Social Darwinism. Konrad Lorenz won the Nobel Prize in Physiology or Medicine in 1973 for his analysis of animal behaviour in terms of the role of natural selection (particularly group selection). However, in Germany in 1940, in writings that he subsequently disowned, he used the theory as a justification for policies of the Nazi state. He wrote "... selection for toughness, heroism, and social utility...must be accomplished by some human institution, if mankind, in default of selective factors, is not to be ruined by domestication-induced degeneracy. The racial idea as the basis of our state has already accomplished much in this respect." Others have developed ideas that human societies and culture evolve by mechanisms analogous to those that apply to evolution of species.
More recently, work among anthropologists and psychologists has led to the development of sociobiology and later of evolutionary psychology, a field that attempts to explain features of human psychology in terms of adaptation to the ancestral environment. The most prominent example of evolutionary psychology, notably advanced in the early work of Noam Chomsky (born 1928) and later by Steven Pinker (born 1954), is the hypothesis that the human brain has adapted to acquire the grammatical rules of natural language. Other aspects of human behaviour and social structures, from specific cultural norms such as incest avoidance to broader patterns such as gender roles, have been hypothesised to have similar origins as adaptations to the early environment in which modern humans evolved. By analogy to the action of natural selection on genes, the concept of memes – "units of cultural transmission", or culture's equivalents of genes undergoing selection and recombination – has arisen, first described in this form by Richard Dawkins in 1976 and subsequently expanded upon by philosophers such as Daniel Dennett (born 1942) as explanations for complex cultural activities, including human consciousness.
Information and systems theory.
In 1922, Alfred Lotka proposed that natural selection might be understood as a physical principle that could be described in terms of the use of energy by a system, a concept that was later developed by Howard Odum as the maximum power principle whereby evolutionary systems with selective advantage maximise the rate of useful energy transformation. Such concepts are sometimes relevant in the study of applied thermodynamics.
The principles of natural selection have inspired a variety of computational techniques, such as "soft" artificial life, that simulate selective processes and can be highly efficient in 'adapting' entities to an environment defined by a specified fitness function. For example, a class of heuristic optimisation algorithms known as genetic algorithms, pioneered by John Holland in the 1970s and expanded upon by David E. Goldberg, identify optimal solutions by simulated reproduction and mutation of a population of solutions defined by an initial probability distribution. Such algorithms are particularly useful when applied to problems whose solution landscape is very rough or has many local minima.

</doc>
<doc id="21148" url="http://en.wikipedia.org/wiki?curid=21148" title="Netherlands">
Netherlands

The Netherlands (; Dutch: "Nederland" ]) is the main constituent country of the Kingdom of the Netherlands. It is a small, densely populated country, lying mainly in Western Europe, but also including three islands in the Caribbean (Bonaire, Sint Eustatius and Saba). The European part of the Netherlands borders Germany to the east, Belgium to the south, and the North Sea to the northwest, sharing maritime borders with Belgium, the United Kingdom and Germany. The largest and most important cities in the Netherlands are Amsterdam, The Hague and Rotterdam. Amsterdam is the country's capital, while The Hague holds the Dutch seat of government and parliament. The port of Rotterdam is the largest port in Europe – as large as the next three largest combined.
The Netherlands' name literally means "Low Country", influenced by its low land and flat geography, with only about 50% of its land exceeding one metre above sea level. Most of the areas below sea level are man-made. Since the late 16th century, large areas (polders) have been reclaimed from the sea and from lakes, amounting to nearly 17% of the country's current land mass.
With a population density of 406 people per km² – 497 if water is excluded – the Netherlands is a very densely populated country for its size. Only Bangladesh, South Korea and Taiwan both have a larger population and a higher population density. Nevertheless, the Netherlands is the world's second largest exporter of food and agriculture products, after the United States. This is due to the fertility of the soil and the mild climate.
The Netherlands was one of the first countries in the world to have an elected parliament, and since 1848 it has been governed as a parliamentary democracy and a constitutional monarchy, organised as a unitary state. The Netherlands has a long history of social tolerance and is generally regarded as a liberal country, having legalised abortion, prostitution and euthanasia, while maintaining a progressive drugs policy. In 2001 it became the world's first country to legalize same-sex marriage.
The Netherlands is a founding member of the EU, Eurozone, G-10, NATO, OECD, WTO and a part of the trilateral Benelux economic union. The country is host to the Organisation for the Prohibition of Chemical Weapons and five international courts: the Permanent Court of Arbitration, the International Court of Justice, the International Criminal Tribunal for the Former Yugoslavia, the International Criminal Court and the Special Tribunal for Lebanon. The first four are situated in The Hague, as is the EU's criminal intelligence agency Europol and judicial co-operation agency Eurojust. This has led to the city being dubbed "the world's legal capital". The Netherlands is also a part of the Schengen Area.
The Netherlands has a market-based mixed economy, ranking 17th of 177 countries according to the Index of Economic Freedom. It had the thirteenth-highest per capita income in the world in 2013 according to the International Monetary Fund. In 2013, the United Nations World Happiness Report ranked the Netherlands as the fourth happiest country in the world, reflecting its high quality of life.
Etymology.
The Netherlands in its entirety is often referred to by the much older designation "Holland", though this strictly refers only to North and South Holland, two of the nation's twelve provinces, that were created out of the former County of Holland. That county was economically and politically the most important county in the region. Historically, Holland often served as a metonym for the entire country. Referring to the Netherlands as Holland is an example of "pars pro toto" and is considered either incorrect or informal, depending on the context, but is more acceptable when referring to the national football team.
"De Lage landen" (The Low Countries) is a geographical designation of the general area of Belgium, the Netherlands, and Luxembourg, and is also known for the more geopolitical term Benelux. Depending on the context, the designation "Low Countries" sometimes extended with the former Burgundian and Habsburg possessions that are now part of northern France (French Flanders, French Hainaut, Artois, Picardy to the Somme) and the former Luxembourg region around Diedenhoven and Germany (east Frisia, Julich, Cleves, Bentheim, Lingen, the region around Geldern, around Bitburg, some municipalities east of the eastern provinces which were annexed by Prussia in 1815, etc.). The Netherlands has about the same meaning as the Low Countries, but of a more historiographical nature.
In the fifteenth century the name "the Netherlands" ("de Nederlanden") came into use. Unlike "France" and "England", it had no ethnic origin, but it was originally a geographical term which denoted only the difference with a higher ground. This was already in practice among the Romans, who made a distinction between the Roman provinces of Germania Inferior and Germania Superior, referring to the downstream and upstream location of these two provinces. Place names with "Neder" and "Nieder" are still used in various places in Dutch and German language areas. Also terms like lower Rhine and lower Meuse were commonly used (vs. middle Rhine or upper Rhine).
"Niderlant" was in the late Middle Ages the region between the Meuse and the Rhine, the Lower Rhine Area now included. The area known as "Oberland" (High country) was considered to begin approximately at the nearby higher located Cologne. By extension, the term could also be applied to the delta of the Schelde, Meuse and Rhine, and then would occur in the plural form. Due to the great importance of the Low Countries, the name was increasingly used specifically for this area. From about 1490, the Burgundian-Habsburg provinces were also indicated. Besides Flanders, "the Netherlands" was, from the mid-sixteenth century on, probably the most commonly used name.
History.
Prehistory (before 500 BC).
The prehistory of the area that is now the Netherlands was largely shaped by the sea and the rivers that constantly shifted the low-lying geography. The oldest human (Neanderthal) traces in the Netherlands were found in higher soils, near Maastricht, from 250,000 years ago. After the end of the Ice Age, various Paleolithic groups inhabited the area, and around 8000 BC Mesolithic tribes resided in Friesland and Drenthe, where the oldest canoe in the world was recovered. Autochthonous hunter-gatherers from the Swifterbant culture are attested from around 5600 BC onwards. They are strongly linked to rivers and open water and were related to the southern Scandinavian Ertebølle culture (5300–4000 BC). To the west, the same tribes might have built hunting camps to hunt winter game. People made the switch to animal husbandry sometime between 4800 BC and 4500 BC. Agricultural transformation took place very gradually, between 4300 BC and 4000 BC. The farming Funnelbeaker culture extended from Denmark through northern Germany into the northern Netherlands, and erected the dolmens, large stone grave monuments found in Drenthe (built between 4100 BC and 3200 BC). To the southwest, the Vlaardingen culture (around 2600 BC), an apparently more primitive culture of hunter-gatherers survived well into the Neolithic period. Around 2950 BC there was a quick and smooth transition from the Funnelbeaker farming culture to the pan-European Corded Ware pastoralist culture. The Bell Beaker culture was also present in the Netherlands, that apparently arose out of the Corded Ware culture.
Copper finds show that there was trade with other areas in Europe, as natural copper is not found in Dutch soil. The Bronze age probably started somewhere around 2000 BC and lasted until around 800 BC. The many finds in Drenthe of rare and valuable objects, suggest that it was a trading centre in the Bronze Age. The Bell Beaker cultures (2700–2100 BC) locally developed into the Bronze Age Barbed-Wire Beaker culture (2100–1800 BC). In the second millennium BC, the region was the boundary between the Atlantic and Nordic horizons, roughly divided by the course of the Rhine. In the north, the Elp culture (c. 1800 BC to 800 BC) was a Bronze Age archaeological culture having earthenware pottery of low quality as a marker. The initial phase was characterised by tumuli (1800–1200 BC) that were strongly tied to contemporary tumuli in northern Germany and Scandinavia, and were apparently related to the Tumulus culture (1600–1200 BC) in central Europe. This phase was followed by a subsequent change featuring Urnfield (cremation) burial customs (1200–800 BC). The southern region became dominated by the Hilversum culture (1800–800 BC), which apparently inherited cultural ties with Britain of the previous Barbed-Wire Beaker culture.
The Iron Age brought a measure of prosperity. Iron ore was available throughout the country, including bog iron extracted from the ore in peat bogs in the north, the natural iron-bearing balls found in the Veluwe and the red iron ore near the rivers in Brabant. Smiths travelled from small settlement to settlement with bronze and iron, fabricating tools on demand, including axes, knives, pins, arrowheads and swords. Some evidence even suggests the making of Damascus steel swords using an advanced method of forging that combined the flexibility of iron with the strength of steel. The King's grave of Oss dating from around 500 BC was found in a burial mound, the largest of its kind in western Europe and containing an iron sword with an inlay of gold and coral.
Germanic groups and Romans (500 BC – 410 AD).
 Deteriorating climate in Scandinavia around 850 BC and later faster around 650 BC might have triggered migration of the Germanic tribes. By the time this migration was complete, around 250 BC, a few general cultural and linguistic groupings had emerged. The North Sea Germanic (or Ingvaeones) inhabited the northern part of the Low Countries. They would later develop into the Frisii and the early Saxons. A second grouping, the Weser-Rhine Germanic (or Istvaeones), extended along the middle Rhine and Weser and inhabited the Low Countries south of the great rivers. This group consisted of tribes that would eventually develop into the Salian Franks. Also the Celtic La Tène culture (c. 450 BC up to the Roman conquest) had expanded over a wide range, including into the southern area of the Low Countries. Some scholars have speculated that even a third ethnic identity and language, neither Germanic nor Celtic, survived in the Netherlands until the Roman period, the Iron Age Nordwestblock culture, that eventually was being absorbed by the Celts to the south and the Germanic peoples from the east.
During the Gallic Wars, the area south of the Oude Rijn and west of the Rhine was conquered by Roman forces under Julius Caesar from 57 BC to 53 BC. Caesar describes two main tribes living in what is now the southern Netherlands: the Menapii and the Eburones. The Rhine became fixed around 12AD as Rome's northern frontier. Notable towns would arise along the Limes Germanicus: Nijmegen and Voorburg. At first part of Gallia Belgica, the area south of the Limes became part of the Roman province of Germania Inferior. The area to the north of the Rhine, inhabited by the Frisii, remained outside Roman rule (but not its presence and control), while the border tribes Batavi and Cananefates served in the Roman cavalry. The Batavi rose against the Romans in the Batavian rebellion of 69AD, but were eventually defeated. The Batavi later merged with other tribes into the confederation of the Salian Franks, whose identity emerged at the first half of the third century. Salian Franks appear in Roman texts as both allies and enemies. The Salian Franks were forced by the confederation of the Saxons from the east to move over the Rhine into Roman territory in the fourth century. From their new base in west Flanders and southwest Netherlands, they were raiding the English Channel. Roman forces pacified the region, but did not expel the Franks, who continued to be feared at least until the time of Julian the Apostate (358), when Salian Franks were allowed to settle as "foederati" in Toxandria. After deteriorating climate conditions and the Romans withdrew, the Frisii disappeared from the northern Netherlands, probably forced to resettle within Roman territory as "laeti" in c. 296. Coastal lands remained largely unpopulated for the next two centuries.
Early Middle Ages (411–1000).
After Roman government in the area collapsed, the Franks expanded their territories in numerous kingdoms. By the 490s, Clovis I had conquered and united all these territories in the southern Netherlands in one Frankish kingdom, and from there continued his conquests into Gaul. During this expansion, Franks migrating to the south eventually adopted the Vulgar Latin of the local population. A widening cultural divide grew with the Franks remaining in their original homeland in the north (i.e. southern Netherlands and Flanders), who kept on speaking Old Frankish, which by the Ninth century had evolved into Old Low Franconian or Old Dutch. A Dutch-French language boundary came into existence.
To the north of the Franks, climatic conditions on the coast improved, so the abandoned land of the ancient Frisii was during the Migration Period resettled again, mostly by Saxons, but also by Angles, Jutes and ancient Frisii. Many moved on to England and came to be known as Anglo-Saxons, but those who stayed would be referred to as Frisians, named after the ancient inhabitants of the Frisii. Frisian was spoken along the entire southern North Sea coast, and it is still the closest to English related living language after Scots. By the Seventh-century a Frisian Kingdom (650–734) under King Aldegisel and King Redbad emerged with Utrecht as its centre of power, while Dorestad was a Frisian flourishing trading place. Between 600 and around 719 the city was often fought over between the Frisians and the Franks. In 734, at the Battle of the Boarn, the Frisians in the Netherlands were after a series of wars defeated by the Franks. With the approval of the Franks, Anglo-Saxon missionaries converted the Frisian people to Christianity. Willibrord established the Archdiocese of Utrecht and became bishop of the Frisians. However, his successor Boniface was murdered by the Frisians in Dokkum, in 754.
The Frankish Carolingian empire – that had the Roman Empire as its example – would eventually include much of Western Europe, but got divided into three parts in 843. Most of what is today the Netherlands became part of Middle Francia; Flanders became part of West Francia. Situated between – and wanted by – the realms of West and East Francia, Middle Francia was a weak kingdom that comprised territories from Frisia in the north to the Kingdom of Italy in the south. Then, the middle kingdom was partitioned; the lands north of the Alps passed to Lothair II and consecutively were named Lotharingia. After he died in 869, Lotharingia was partitioned, in Upper- and Lower Lotharingia. The latter part comprising of the Low Countries, that technically became part of East Francia in 870, although it was effectively under the control of Vikings, who raided the largely defenceless Frisian and Frankish towns lying on the Frisian coast and along the rivers. Around 850, Lothair I acknowledged the Viking Rorik of Dorestad as ruler of most of Frisia. Around 879, another Viking terrorised the Frisian lands. He became known as Godfrid, Duke of Frisia, but he was assassinated in 885, after which Gerolf of Holland assumed lordship in Frisia and Viking rule came to an end. The Viking raids made the sway of French and German lords in the area weak. Resistance to the Vikings, if any, came from local nobles, who gained in stature as a result, and that lay the basis for the disintegration of Lower Lotharingia into (semi-)independent states.
High Middle Ages (1000–1384).
The Holy Roman Empire (the successor state of East Francia) ruled much of the Low Countries in the 10th and 11th century, but was not able to maintain political unity. Powerful local nobles turned their cities, counties and duchies into private kingdoms, that felt little sense of obligation to the emperor. Holland, Hainaut, Flanders, Gelre, Brabant, and the Utrecht were in a state of almost continual war or paradoxically formed personal unions. The language and culture of most of the people who lived in the County of Holland were originally Frisian. As Frankish settlement progressed from Flanders and Brabant, the area quickly became Old Low Franconian (or Old Dutch). The rest of Frisia in the north (now Friesland and Groningen) continued to maintain its independence and had its own institutions (collectively called the "Frisian freedom") and resented the imposition of the feudal system.
Around 1000 AD, due to several agricultural developments, the economy started to develop at a fast pace, and the higher productivity allowed workers to farm more land or to become tradesmen. Towns grew around monasteries and castles, and a mercantile middle class began to develop in these urban areas, especially in Flanders and later also Brabant. Wealthy cities started to buy certain privileges for themselves from the sovereign. In practice, this meant that Brugge and Antwerp became quasi-independent republics in their own right and would later develop into some of the most important cities and ports in Europe.
Around 1100 AD, farmers from Flanders and Utrecht began draining and cultivating uninhabited swampy land in the western Netherlands, and made the emergence of the County of Holland as center of power possible. The title of Count of Holland were fought over in the Hook and Cod Wars (Dutch: "Hoekse en Kabeljauwse twisten") between 1350 and 1490. The Cod faction consisted of the more progressive cities, while the Hook faction consisted of the conservative noblemen. These noblemen invited the Duke Philip the Good of Burgundy – who was also Count of Flanders – to conquer Holland.
Burgundian and Habsburg Netherlands (1384–1581).
Most of the Imperial and French fiefs in what is now the Netherlands and Belgium were united in a personal union by Philip the Good, duke of Burgundy in 1433. The House of Valois-Burgundy and their Habsburg heirs would rule the Low Countries in the period from 1384 to 1581. Before the Burgundian union, the Dutch identified themselves by the town they lived in or their local duchy or county. The Burgundian period is when the road to nationhood began. The new rulers defended Dutch trading interests, that developed rapidly. The fleets of the County of Holland defeated the fleets of the Hanseatic League several times. Amsterdam grew and in the 15th century became the primary trading port in Europe for grain from the Baltic region. Amsterdam distributed grain to the major cities of Belgium, Northern France and England. This trade was vital, because Holland could no longer produce enough grain to feed itself. Land drainage had caused the peat of the former wetlands to reduce to a level that was too low for drainage to be maintained.
Under Habsburg, Charles V, ruler of the Holy Roman Empire and King of Spain, all fiefs in the current Netherlands region were united into the Seventeen Provinces, which also included most of present-day Belgium, Luxembourg, and some adjacent land in what is now France and Germany. In 1568, the Eighty Years' War between the Provinces and their Spanish ruler began. In 1579, the northern half of the Seventeen Provinces forged the Union of Utrecht in which they committed to support each other in their defence against the Spanish army. The Union of Utrecht is seen as the foundation of the modern Netherlands. In 1581, the northern provinces adopted the Act of Abjuration, the declaration of independence in which the provinces officially deposed Philip II of Spain as reigning monarch in the northern provinces.
The protestant Queen Elizabeth I of England sympathised with the Dutch struggle against the Spanish, and sent an army of 7,600 soldiers to aid the Dutch in their war with the Catholic Spanish. The English army under command of Robert Dudley, 1st Earl of Leicester was of no real benefit to the Dutch rebellion. Philip II, the son of Charles V, was not prepared to let them go easily, and war continued until 1648, when Spain under King Philip IV finally recognised the independence of the seven north-western provinces in the Peace of Münster. Parts of the southern provinces became "de facto" colonies of the new republican-mercantile empire.
Dutch Republic (1581–1795).
After declaring their independence, the provinces of Holland, Zeeland, Groningen, Friesland, Utrecht, Overijssel, and Gelderland formed a confederation. All these duchies, lordships and counties were autonomous and had their own government, the States-Provincial. The States General, the confederal government, were seated in The Hague and consisted of representatives from each of the seven provinces. The sparsely populated region of Drenthe was part of the republic too, although it was not considered one of the provinces. Moreover, the Republic had come to occupy during the Eighty Years' War a number of so-called Generality Lands in Flanders, Brabant and Limburg. Their population was mainly Roman Catholic, and these areas did not have a governmental structure of their own, and were used as a buffer zone between the Republic and the Spanish-controlled Southern Netherlands.
In the Dutch Golden Age, spanning much of the 17th century, the Dutch Empire grew to become one of the major seafaring and economic powers. Science, military, and art (especially painting) were among the most acclaimed in the world. By 1650, the Dutch owned 16,000 merchant ships. The Dutch East India Company and the Dutch West India Company established colonies and trading posts all over the world. The Dutch settlement in North America began with the founding of New Amsterdam on the southern part of Manhattan in 1614. In South Africa, the Dutch settled the Cape Colony in 1652. Dutch colonies in South America were established along the many rivers in the fertile Guyana plains, among them Colony of Surinam (now Suriname). In Asia, the Dutch established the Dutch East Indies (now Indonesia), and the only western trading post in Japan, Dejima.
Many economic historians regard the Netherlands as the first thoroughly capitalist country in the world. In early modern Europe it had the wealthiest trading city (Amsterdam) and the first full-time stock exchange. The inventiveness of the traders led to insurance and retirement funds as well as phenomena such as the boom-bust cycle, the world's first asset-inflation bubble, the tulip mania of 1636–1637, and the world's first bear raider, Isaac le Maire, who forced prices down by dumping stock and then buying it back at a discount. The republic went into a state of general decline in the later 18th century, with economic competition from England and long-standing rivalries between the two main factions in Dutch society, the republican "Staatsgezinden" and the supporters of the stadtholder the "Prinsgezinden", as main factors.
Batavian Republic and kingdom (1795–1890).
With the armed support of revolutionary France, Dutch republicans proclaimed the Batavian Republic, modelled after the French Republic and rendering the Netherlands a unitary state in 19 January 1795. The stadtholder William V of Orange had fled to England. But from 1806 to 1810, Kingdom of Holland was set up by Napoleon Bonaparte as a puppet kingdom governed by his brother Louis Bonaparte to control the Netherlands more effectively. However, King Louis Bonaparte tried to serve Dutch interests instead of his brother's, and he was forced to abdicate on 1 July 1810. The Emperor sent in an army and the Netherlands became part of the French Empire until the autumn of 1813, when Napoleon was defeated in the Battle of Leipzig.William Frederick, son of the last stadtholder, returned to the Netherlands in 1813 and proclaimed himself Sovereign Prince of the Netherlands. Two years later, the Congress of Vienna added the southern Netherlands to the north to create a strong country on the northern border of France. William Frederick raised this United Netherlands to the status of a kingdom and proclaimed himself King William I. In addition, William became hereditary Grand Duke of Luxembourg in exchange for his German possessions. However, the Southern Netherlands had been culturally separate from the north since 1581, and rebelled. The south gained independence in 1830 as Belgium, while the personal union between Luxembourg and the Netherlands was severed in 1890, when William III died with no surviving male heirs. Ascendancy laws prevented his daughter Queen Wilhelmina from becoming the next Grand Duchess.
The Belgian Revolution at home and the Java War in the Dutch East Indies brought the Netherlands to the brink of bankruptcy. However, the Cultivation System was introduced in 1830; in the Dutch East Indies, 20% of village land had to be devoted to government crops for export. The policy brought the Dutch enormous wealth and made the colony self-sufficient. On the other hand, the colonies in the West Indies (Dutch Guiana and Curaçao and Dependencies), relied heavily on African slaves in which the Dutch part is estimated at 5–7 percent, or more than half a million Africans. The Netherlands abolished slavery in 1863. Furthermore, slaves in Suriname would be fully free only in 1873, since the law stipulated that there was to be a mandatory 10-year transition. The Dutch were also one of the last European countries to industrialise, in the second half of the 19th century.
World Wars and beyond (1890–present).
The Netherlands were able to remain neutral during World War I. In part, because the import of goods through the Netherlands proved essential to German survival, until the blockade by the British Royal Navy in 1916. That changed in World War II, when Nazi Germany invaded the Netherlands on 10 May 1940. The Rotterdam Blitz forced the main element of the Dutch army to surrender 4 days later. During the occupation, over 100,000 Dutch Jews were rounded up and transported to Nazi extermination camps of whom only a few survived. Dutch workers were conscripted for forced labour in Germany, civilians who resisted were killed in reprisal for attacks on German soldiers, and the countryside was plundered for food. Although there were thousands of Dutch who risked their lives by hiding Jews from the Germans, local fascists joined the Waffen SS, fighting on the Eastern Front. Political collaborators were members of the fascist NSB, the only legal political party in the occupied Netherlands. On 8 December 1941, the Dutch government-in-exile in London declared war on Japan, but could not prevent the Japanese occupation of the Dutch East Indies (Indonesia). In 1944–45, the First Canadian Army, which included Canadian, British and Polish troops, was responsible for liberating much of the Netherlands. But soon after VE day, the Dutch fought a colonial war against the new republic of Indonesia. 
In 1954, the Charter for the Kingdom of the Netherlands reformed the political structure of the Netherlands, which was a result of international pressure to carry out decolonisation. The Dutch colonies of Surinam and Curaçao and Dependencies and the European country became all constituent countries within the Kingdom, on a basis of equality. Before the reform was completed, Indonesia declared its independence in August 1945 (recognised in 1949), and thus has never been part of the Kingdom. Suriname followed in 1975. After the war the Netherlands left behind also an era of neutrality and gained closer ties with neighboring states. The Netherlands was one of the founding members of the Benelux, the NATO, and the European Coal and Steel Community, which would evolve into the EEC (Common Market) and later the European Union.
Government-encouraged emigration efforts to reduce population density prompted some 500,000 Dutch people to leave the country after the war. The 1960s and 1970s were a time of great social and cultural change, such as rapid "ontzuiling" (literally: depillarisation), a term that describes the decay of the old divisions along political and religious lines. Youths, and students in particular, rejected traditional mores and pushed for change in matters such as women's rights, sexuality, disarmament and environmental issues. On 10 October 2010, the Netherlands Antilles was dissolved. Referendums were held on each island to determine their future status. As a result the islands of Bonaire, Sint Eustatius and Saba (the BES islands) were to obtain closer ties with the Netherlands. This led to the incorporation of these three islands into the country of the Netherlands as "special municipalities" upon the dissolution of the Netherlands Antilles. The special municipalities are collectively known as the Caribbean Netherlands.
Geography of the Netherlands.
The European Netherlands.
The European area of the Netherlands lies between latitudes 50° and 54° N, and longitudes 3° and 8° E.
The Netherlands is geographically a very low and flat country, with about 26% of its area and 21% of its population located below sea level, and only about 50% of its land exceeding one metre above sea level. The country is for the most part flat, with the exception of foothills in the far southeast, up to a height of no more than 321 metres, and some low hill ranges in the central parts. Most of the areas below sea level are man-made, caused by peat extraction or achieved through land reclamation. Since the late 16th century, large polder areas are preserved through elaborate drainage systems that include dikes, canals and pumping stations. Nearly 17% of the country's land area is reclaimed from the sea and from lakes.
Much of the country was originally formed by the estuaries of three large European rivers: the Rhine ("Rijn"), the Meuse ("Maas") and the Scheldt ("Schelde"), as well as their distributaries. The south-western part of the Netherlands is to this day a river delta of these three rivers, the Rhine-Meuse-Scheldt delta.
The Netherlands is divided into north and south parts by the Rhine, the Waal, its main distributary branch, and the Meuse. In the past these rivers functioned as a natural barrier between fiefdoms and hence historically created a cultural divide, as is evident in some phonetic traits that are recognisable on either side of what the Dutch call their "Great Rivers" ("de Grote Rivieren"). Another significant branch of the Rhine, the IJssel river, discharges into Lake IJssel, the former Zuiderzee ('southern sea'). Just like the previous, this river forms a linguistic divide: people to the northeast of this river speak Dutch Low Saxon dialects (except for the province of Friesland, which has its own language).
Floods.
Over the centuries, the Dutch coastline has changed considerably as a result of natural disasters and human intervention. Most notable in terms of land loss was the storm of 1134, which created the archipelago of Zeeland in the south-west.
On 14 December 1287, St. Lucia's flood affected the Netherlands and Germany killing more than 50,000 people in one of the most destructive floods in recorded history. The St. Elizabeth flood of 1421 and the mismanagement in its aftermath destroyed a newly reclaimed polder, replacing it with the 72 km2 "Biesbosch" tidal floodplains in the south-centre. The huge North Sea flood of early February 1953 caused the collapse of several dikes in the south-west of the Netherlands; more than 1,800 people drowned in the flood. The Dutch government subsequently instituted a large-scale programme, the "Delta Works", to protect the country against future flooding, which was completed over a period of more than thirty years.
The impact of disasters was to an extent increased through human activity. Relatively high-lying swampland was drained to be used as farmland. The drainage caused the fertile peat to contract and ground levels to drop, upon which groundwater levels were lowered to compensate for the drop in ground level, causing the underlying peat to contract further. Additionally, until the 19th century peat was mined, dried, and used for fuel, further exacerbating the problem. Centuries of extensive and poorly controlled peat extraction lowered an already low land surface by several metres. Even in flooded areas, peat extraction continued through turf dredging.
Because of the flooding, farming was difficult, which encouraged foreign trade, the result of which was that the Dutch were involved in world affairs since the early 14th/15th century.
To guard against floods, a series of defences against the water were contrived. In the first millennium AD, villages and farmhouses were built on man-made hills called "terps". Later, these terps were connected by dikes. In the 12th century, local government agencies called "waterschappen" ("water boards") or "hoogheemraadschappen" ("high home councils") started to appear, whose job it was to maintain the water level and to protect a region from floods; these agencies continue to exist. As the ground level dropped, the dikes by necessity grew and merged into an integrated system. By the 13th century windmills had come into use to pump water out of areas below sea level. The windmills were later used to drain lakes, creating the famous polders. 
In 1932 the "Afsluitdijk" ("Closure Dike") was completed, blocking the former "Zuiderzee" (Southern Sea) from the North Sea and thus creating the IJsselmeer (IJssel Lake). It became part of the larger Zuiderzee Works in which four polders totalling 2500 km2 were reclaimed from the sea.
The Netherlands is one of the countries that may suffer most from climate change. Not only is the rising sea a problem, but erratic weather patterns may cause the rivers to overflow.
Delta works.
After the 1953 disaster, the Delta Works were constructed, a comprehensive set of civil works throughout the Dutch coast. The project started in 1958 and was largely completed in 1997 with the completion of the Maeslantkering. New projects have been periodically started since to renovate and renew the Delta Works. A main goal of the Delta project was to reduce the risk of flooding in South Holland and Zeeland to once per 10,000 years (compared to 1 per 4000 years for the rest of the country). This was achieved by raising 3000 km of outer sea-dykes and 10000 km of inner, canal, and river dikes, and by closing off the sea estuaries of the Zeeland province. New risk assessments occasionally show problems requiring additional Delta project dyke reinforcements. The Delta project is considered by the American Society of Civil Engineers as one of the seven wonders of the modern world.
It is anticipated that global warming in the 21st century will result in a rise in sea level which, despite popular belief, will possibly not overwhelm the measures the Netherlands has taken to control floods. Even more specifically, the Netherlands is the only country in the world actively preparing for a sea level rise. A politically neutral Delta Commission has formulated an action plan to cope with a sea level rise of 1.10 m and a simultaneous land height decline of 10 cm. The plan foresees in the reinforcement of the existing coastal defenses like dikes and dunes with 1.30 m of additional flood protection. Climate change will not only threaten the Netherlands from the sea side, but could also alter rain fall patterns and river run-off. To protect the country from river flooding, another program is already being executed. The Room for the River plan grants more flow space to rivers, protects the major populated areas and allows for periodic flooding of indefensible lands. The few residents that lived in these so-called "overflow areas" have been moved to higher ground, with some of that ground having been raised above anticipated flood levels.
Protecting the country against floods is one element of climate change. The other is that the pressure of the sea water on ground water will increase. As a result, the fresh water table will be pushed more inland, resulting in more brackish or saline groundwater in the coastal provinces. Due to this change, some drinking water areas will be forced to apply desalination despite the apparent abundance of water. It will also affect agriculture. The greenhouses can continue their production by becoming more water efficient (they are already disconnected from the groundwater, thereby not becoming more saline), though they will need to become more energy and water efficient. The push of more brackish water into the mainland will also cause changes in flora and fauna.
Climate.
The predominant wind direction in the Netherlands is south-west, which causes a moderate maritime climate, with cool summers and mild winters, and typically high humidity. This is especially true close to the Dutch coastline, where temperatures can be more than 10 C-change higher (in winter) or lower (in summer) than in the (south) east of the country.
The following tables are based on mean measurements by the KNMI weather station in De Bilt between 1981 and 2010:
Ice days (maximum temperature below 0 C) usually occur from December until February, with the occasional rare ice day prior to or after that period. Freezing days (minimum temperature below 0 C) occur much more often, usually ranging from mid-November to late March, but not rarely measured as early as mid October and as late as mid May. If one chooses the height of measurement to be 10 cm above ground instead of 150 cm, one may even find such temperatures in the middle of the summer. On average, snow can occur from November to April, but sometimes occurs in May or October too.
Warm days (maximum temperature above 20 C) in De Bilt are usually found in April to October, but in some parts of the country these warm days can also occur in March, or even sometimes in November or February (usually not in De Bilt, however). Summer days (maximum temperature above 25 C) are usually measured in De Bilt from May until September, tropical days (maximum temperature above 30 C) are rare and usually occur only in June to August.
Precipitation throughout the year is distributed relatively equally each month. Summer and autumn months tend to gather a little more precipitation than the other months, mainly because of the intensity of the rainfall rather than the frequency of rain days (this is especially the case in summer, when lightning is also much more frequent).
The number of sunshine hours is affected by the fact that because of the geographical latitude, the length of the days varies between barely eight hours in December and nearly 17 hours in June.
Nature.
The Netherlands has 20 national parks and hundreds of other nature reserves, that include lakes, heathland, woods, dunes and other habitats. Most of these are owned by Staatsbosbeheer, the national department for forestry and nature conservation and Natuurmonumenten (literally 'Natures monuments'), a private organisation that buys, protects and manages nature reserves. The Dutch part of the Wadden Sea in the north, with its tidal flats and wetlands, is rich in biological diversity, and was declared a UNESCO World Heritage Nature Site in 2009.
The Oosterschelde, formerly the northeast estuary of the river Scheldt was designated a national park in 2002, thereby making it the largest national park in the Netherlands at an area of 370 km2. It consists primarily of the salt waters of the Oosterschelde, but also includes mud flats, meadows, and shoals. Because of the large variety of sea life, including unique regional species, the park is popular with Scuba divers. Other activities include sailing, fishing, cycling, and bird watching.
Phytogeographically, the Netherlands is shared between the Atlantic European and Central European provinces of the Circumboreal Region within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of the Netherlands belongs to the ecoregion of Atlantic mixed forests. In 1871, the last old original natural woods were cut down, and most woods today are planted monocultures of trees like Scots Pine and trees that are not native to the Netherlands. These woods were planted on anthropogenic heaths and sand-drifts (overgrazed heaths) (Veluwe).
Caribbean islands.
The Caribbean Netherlands are designated as special municipalities of the Netherlands. The islands form a part of the Lesser Antilles. Within this island group,
The islands of the Caribbean Netherlands enjoy a tropical climate with warm weather all year round. The Leeward Antilles are warmer and drier than the Windward islands. In summer, the Windward Islands can be subject to hurricanes.
Government.
The Netherlands has been a constitutional monarchy since 1815 and a parliamentary democracy since 1848. The Netherlands is described as a consociational state. Dutch politics and governance are characterised by an effort to achieve broad consensus on important issues, within both the political community and society as a whole. In 2010, "The Economist" ranked the Netherlands as the 10th most democratic country in the world.
The monarch is the head of state, at present King Willem-Alexander. Constitutionally, the position is equipped with limited powers. By law, the king (the title queen has no constitutional significance) has the right to be periodically briefed and consulted on government affairs. Depending on the personalities and relationships of the king and the ministers, the king might have "influence" beyond the "power" granted by the constitution.
The executive power is formed by the council of Ministers, the deliberative council of the Dutch cabinet. The cabinet usually consists of 13 to 16 ministers and a varying number of state secretaries. One to three ministers are ministers without portfolio. The head of government is the Prime Minister of the Netherlands, who often is the leader of the largest party of the coalition. The Prime Minister is a "primus inter pares", with no explicit powers beyond those of the other ministers. Mark Rutte has been Prime Minister since October 2010; the Prime Minister had been the leader of the largest party continuously since 1973.
The cabinet is responsible to the bicameral parliament, the States General, which also has legislative powers. The 150 members of the House of Representatives, the Lower House, are elected in direct elections on the basis of party-list proportional representation. These are held every four years, or sooner in case the cabinet falls (for example: when one of the chambers carries a motion of no confidence, the cabinet offers its resignation to the monarch). The States-Provincial are directly elected every four years as well. The members of the provincial assemblies elect the 75 members of the Senate, the upper house, which has the power to reject laws, but not propose or amend them.
Political culture.
Both trade unions and employers organisations are consulted beforehand in policymaking in the financial, economic and social areas. They meet regularly with government in the Social-Economic Council. This body advises government and its advice cannot be put aside easily.
The Netherlands has a long tradition of social tolerance. In the 18th century, while the Dutch Reformed Church was the state religion, Catholicism, other forms of Protestantism, such as Baptists and Lutherans, and Judaism were tolerated but discriminated against.
In the late 19th century this Dutch tradition of religious tolerance transformed into a system of pillarisation, in which religious groups coexisted separately and only interacted at the level of government. This tradition of tolerance influences Dutch criminal justice policies on recreational drugs, prostitution, LGBT rights, euthanasia, and abortion, which are among the most liberal in the world.
Political parties.
Because of the multi-party system, no single party has held a majority in parliament since the 19th century, and coalition cabinets had to be formed. Since suffrage became universal in 1919, the Dutch political system has been dominated by three families of political parties: the strongest of which were the Christian democrats, currently represented by the Christian Democratic Appeal (CDA); second were the social democrats, represented by the Labour Party (PvdA); and third were the liberals, of which the right wing People's Party for Freedom and Democracy (VVD) is the main representative.
These parties co-operated in coalition cabinets in which the Christian democrats had always been a partner: so either a centre-left coalition of the Christian democrats and social democrats was ruling or a centre-right coalition of Christian democrats and liberals. In the 1970s, the party system became more volatile: the Christian democratic parties lost seats, while new parties became successful, such as the radical democrat and progressive liberal D66.
In the 1994 election, the CDA lost its dominant position. A "purple" cabinet was formed by VVD, D66, and PvdA. In the 2002 elections, this cabinet lost its majority, because of an increased support for the CDA and the rise of the right LPF, a new political party, around Pim Fortuyn, who was assassinated a week before the elections. A short-lived cabinet was formed by CDA, VVD, and LPF, which was led by the CDA leader Jan Peter Balkenende. After the 2003 elections, in which the LPF lost most of its seats, a cabinet was formed by CDA, VVD, and D66. The cabinet initiated an ambitious programme of reforming the welfare state, the healthcare system, and immigration policy.
In June 2006, the cabinet fell after D66 voted in favour of a motion of no confidence against the Minister of Immigration and Integration, Rita Verdonk, who had instigated an investigation of the asylum procedure of Ayaan Hirsi Ali, a VVD MP. A caretaker cabinet was formed by CDA and VVD, and general elections were held on 22 November 2006. In these elections, the CDA remained the largest party and the Socialist Party made the largest gains. The formation of a new cabinet took three months, resulting in a coalition of CDA, PvdA, and ChristianUnion.
On 20 February 2010, the cabinet fell when the PvdA refused to prolong the involvement of the Dutch Army in Uruzgan, Afghanistan. Snap elections were held on 9 June 2010, with devastating results for the previously largest party, the CDA, which lost about half of its seats, resulting in 21 seats. The VVD became the largest party with 31 seats, closely followed by the PvdA with 30 seats. The big winner of the 2010 elections was Geert Wilders, whose right wing PVV, the ideological successor to the LPF, more than doubled its number of seats. Negotiation talks for a new government resulted in a minority government, led by VVD (a first) in coalition with CDA, which was sworn in on 14 October 2010. This unprecedented minority government was supported by PVV, but proved ultimately to be unstable, when on 21 April 2012, Wilders, leader of PVV, unexpectedly 'torpedoed seven weeks of austerity talks' on new austerity measures, paving the way for early elections.
VVD and PvdA were the big winners of the elections. Since 5 November 2012 they have formed the second Rutte cabinet.
Administrative divisions.
The Netherlands is divided into twelve provinces, each under a Commissioner of the King ("Commissaris van de Koning"), except for Limburg province where the position is named Governor ("Gouverneur"). All provinces are divided into municipalities ("gemeenten"), of which there are 393.
The country is also subdivided into 24 water districts, governed by a water board ("waterschap" or "hoogheemraadschap"), each having authority in matters concerning water management. The creation of water boards actually pre-dates that of the nation itself, the first appearing in 1196. The Dutch water boards are among the oldest democratic entities in the world still in existence. Direct elections of the water boards take place every 4 years.
The administrative structure on the 3 BES islands, also known as the Caribbean Netherlands, is different. These islands have the status of "openbare lichamen (public bodies)" and are generally referred to as "special municipalities". They are not part of a province.
Foreign relations.
The history of Dutch foreign policy has been characterised by its neutrality. Since the Second World War the Netherlands has become a member of a large number of international organisations, most prominently the UN, NATO and the EU. The Dutch economy is very open and relies on international trade.
The foreign policy of the Netherlands is based on four basic commitments: to Atlantic co-operation, to European integration, to international development and to international law. One of the more controversial international issues surrounding the Netherlands is its liberal policy towards soft drugs.
During and after the Dutch Golden Age, the Dutch people built up a commercial and colonial empire. The most important colonies were the current Surinam and Indonesia. Indonesia became independent quickly after the Second World War. Surinam became independent in 1975. The historical ties inherited from its colonial past still influence the foreign relations of the Netherlands. In addition, many people from these countries are living permanently in the Netherlands.
Military.
The Netherlands has one of the oldest standing armies in Europe; it was first established as such by Maurice of Nassau. The Dutch army was used throughout the Dutch Empire. After the defeat of Napoleon, the Dutch army was transformed into a conscription army. The army was unsuccessfully deployed during the Belgian revolution in 1830. After 1830, it was deployed mainly in the Dutch colonies, as the Netherlands remained neutral in European wars (including the First World War), until the Netherlands was invaded in the Second World War and quickly defeated by the Wehrmacht in May 1940.
The Netherlands abandoned its neutrality in 1948 when it signed the Treaty of Brussels, and later became a founding member of NATO in 1949. The Dutch military was therefore part of the NATO strength in Cold War Europe, deploying its army to several bases in Germany. More than 3.000 Dutch soldiers were assigned to the 2nd Infantry Division of the United States Army during the Korean War. In 1996 conscription was suspended, and the Dutch army was once again transformed into a professional army. Since the 1990s the Dutch army has been involved in the Bosnian War and the Kosovo War, it held a province in Iraq after the defeat of Saddam Hussein, and it was engaged in Afghanistan.
The military is composed of four branches, all of which carry the prefix "Koninklijke" (Royal):
General Tom Middendorp is the current Commander of the Netherlands armed forces.
All military specialities except the submarine service and the Royal Netherlands Marine Corps ("Korps Mariniers") are open to women. The Korps Commandotroepen, the Special Operations Force of the Netherlands Army, is open to women, but because of the extremely high physical demands for initial training, it is almost impossible for women to become a commando. The Dutch Ministry of Defence employs more than 70,000 personnel, including over 20,000 civilians and over 50,000 military personnel. In April 2011 the government announced a major reduction in its military because of a cut in government expenditure, including a decrease in the number of tanks, fighter aircraft, naval ships and senior officials.
Economy.
The Netherlands has a developed economy and has been playing a special role in the European economy for many centuries. Since the 16th century, shipping, fishing, agriculture, trade, and banking have been leading sectors of the Dutch economy. The Netherlands has a high level of economic freedom. The Netherlands is one of the top countries in the Global Enabling Trade Report (3rd in 2014).
As of 2013, the key trading partners of the Netherlands were Germany, Belgium, UK, United States, France, Italy, China and Russia. The Netherlands is one of the world's 10 leading exporting countries. Foodstuffs form the largest industrial sector. Other major industries include chemicals, metallurgy, machinery, electrical goods, and tourism (in 2012 the Netherlands welcomed 11.7 million international tourists). Examples of international companies operating in Netherlands include Randstad, Unilever, Heineken, financial services (ING, ABN AMRO, Rabobank), chemicals (DSM, AKZO), petroleum refining (Shell), electronical machinery (Philips, ASML), and car navigation (TomTom).
The Netherlands has the 17th-largest economy in the world, and ranks 10th in GDP (nominal) per capita. Between 1997 and 2000 annual economic growth (GDP) averaged nearly 4%, well above the European average. Growth slowed considerably from 2001 to 2005 with the global economic slowdown, but accelerated to 4.1% in the third quarter of 2007. In May 2013, inflation was at 2.8% per year. In April 2013, unemployment was at 8.2% (or 6.7% following the ILO definition) of the labour force.
In Q3 and Q4 2011, the Dutch economy contracted by 0.4% and 0.7%, respectively, because of European Debt Crisis, while in Q4 the Eurozone economy shrunk by 0.3%. The Netherlands also has a relatively low GINI coefficient of 0.326. Despite ranking 7th in GDP per capita, UNICEF ranked the Netherlands 1st in child well-being. On the Index of Economic Freedom Netherlands is the 13th most free market capitalist economy out of 157 surveyed countries.
Amsterdam is the financial and business capital of the Netherlands. The Amsterdam Stock Exchange (AEX), part of Euronext, is the world's oldest stock exchange and is one of Europe's largest bourses. It is situated near Dam Square in the city's centre. As a founding member of the euro, the Netherlands replaced (for accounting purposes) its former currency, the "gulden" (guilder), on 1 January 1999, along with 15 other adopters of the euro. Actual euro coins and banknotes followed on 1 January 2002. One euro was equivalent to 2.20371 Dutch guilders. In the Caribbean Netherlands, the United States dollar is used instead of the euro.
The Dutch location gives it prime access to markets in the UK and Germany, with the port of Rotterdam being the largest port in Europe. Other important parts of the economy are international trade (Dutch colonialism started with co-operative private enterprises such as the VOC), banking and transport. The Netherlands successfully addressed the issue of public finances and stagnating job growth long before its European partners. Amsterdam is the 5th-busiest tourist destination in Europe with more than 4.2 million international visitors. Since the enlargement of the EU large numbers of migrant workers have arrived in the Netherlands from central and eastern Europe.
Of economic importance is BrabantStad, a partnership between the municipalities of Breda, Eindhoven, Helmond, 's-Hertogenbosch and Tilburg and the province of North Brabant. BrabantStad is the fastest growing economic region in the Netherlands, with Brainport as one of the three national top regions and as a top region in the world. The region lies within the Eindhoven-Leuven-Aachen Triangle (ELAT). The partnership aims to form an urban network and to make North Brabant explicitly known as a leading knowledge region within Europe. With a total of 1.5 million people and 20% of the industrial production in the Netherlands is BrabantStad one of the major economical important, metropolitan regions of the Netherlands. Of all the money that goes to research and development in the Netherlands, one third is spent in Eindhoven. A quarter of the jobs in the region are in technology and ICT.
Of all European patent applications in the field of physics and electronics about eight per cent is from North Brabant. In the extended region, BrabantStad is part of the Eindhoven-Leuven-Aachen Triangle (ELAT). This economic cooperation agreement between three cities in three countries has created one of the most innovative regions in the European Union (measured in terms of money invested in technology and knowledge economy). The economic success of this region is important for the international competitiveness of the Netherlands; Together Amsterdam (airport), Rotterdam (seaport), and Eindhoven (Brainport) form the foundation of the Dutch economy.
The Netherlands continues to be one of the leading European nations for attracting foreign direct investment and is one of the five largest investors in the United States. The economy experienced a slowdown in 2005, but in 2006 recovered to the fastest pace in six years on the back of increased exports and strong investment. The pace of job growth reached 10-year highs in 2007. The Netherlands is the fifth-most competitive economy in the world, according to the World Economic Forum's Global Competitiveness Report.
Apart from coal and gas, the country has no mining resources. The last coal mine has been closed in 1974. The Groningen gas field, one of the largest natural gas fields in the world, is situated near Slochteren. Exploitation of this field has resulted in €159 billion in revenue since the mid-1970s. The field is operated by government-owned Gasunie and output is jointly exploited by the government, Royal Dutch Shell and Exxon Mobil through NAM (Nederlandse Aardolie Maatschappij). "Gas extraction has resulted in increasingly strong earth tremors, some measuring as much as 3.6 on the Richter scale. The cost of damage repairs, structural improvements to buildings, and compensation for home value decreases has been estimated at 6.5 billion euros. Around 35,000 homes are said to be affected."
Agriculture.
The Dutch agricultural sector is highly mechanised, and has a strong focus on international exports. It employs about 4% of the Dutch labour force but produces large surpluses for the food-processing industry and accounts for 21 percent of the Dutch total export value. The Dutch rank second worldwide in value of agricultural exports, behind only the United States, with exports earning €80.7 billion in 2014, up from €75.4 billion in 2012.
The Netherlands has, at some time in recent history, supplied one quarter of all of the world's exported tomatoes, and trade of one-third of the world's exports of chilis, tomatoes and cucumbers goes through the country. The Netherlands also exports one-fifteenth of the world's apples.
Aside from that, a significant portion of Dutch agricultural exports consists of fresh-cut plants, flowers, and flower bulbs, with the Netherlands exporting two-thirds of the world's total.
Transport.
Mobility on Dutch roads has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car. Around half of all trips in the Netherlands are made by car, 25% by bicycle, 20% walking, and 5% by public transport. With a total road network of 139,295 km, which includes 2,758 km of expressways, the Netherlands has one of the densest road networks in the world—much denser than Germany and France, but still not as dense as Belgium.
About 13% of all distance is travelled by public transport, the majority of which by train. Like in many other European countries, the Dutch rail network of 3,013 route km is also rather dense. The network is mostly focused on passenger rail services and connects virtually all major towns and cities. Trains are frequent, with one or two trains per hour on lesser lines, two to four trains per hour on average, and up to eight trains an hour on the busiest lines.
Cycling is a ubiquitous mode of transport in the Netherlands. Almost as many kilometres are covered by bicycle as by train. The Dutch are estimated to have at least 18 million bicycles, which makes more than one per capita, and twice as many as the ca. 9 million motor vehicles on the road. In 2013, the European Cyclists' Federation ranked both the Netherlands and Denmark as the most bike-friendly countries in Europe, but more of the Dutch (31%) than of the Danes (19%) list the bike as their main mode of transport for daily activities. Cycling infrastructure is comprehensive. Busy roads have received some 35,000 km of dedicated cycle tracks, physically segregated from motorised traffic. Busy junctions are often equipped with bicycle-specific traffic lights. There are large bicycle parking facilities, particularly in city centres and at train stations.
Rotterdam has the largest port in Europe, with the rivers Meuse and Rhine providing excellent access to the hinterland upstream reaching to Basel, Switzerland, and into France. As of 2013, Rotterdam was the world's eighth largest container port handling 440.5 million metric tonnes of cargo annually. The port's main activities are petrochemical industries and general cargo handling and transshipment. The harbour functions as an important transit point for bulk materials and between the European continent and overseas. From Rotterdam goods are transported by ship, river barge, train or road. In 2007, the Betuweroute, a new fast freight railway from Rotterdam to Germany, was completed.
Schiphol airport, just southwest of Amsterdam, is the main international airport in the Netherlands, and the fifth busiest airport in Europe in terms of passengers.
As part of its commitment to environmental sustainability, the Dutch government initiated a plan to establish over 200 recharging stations for electric vehicles across the country by 2015. The rollout will be undertaken by Switzerland-based power and automation company ABB and Dutch startup Fastned, and will aim to provide at least one station within a 50-kilometre radius (30 miles) from every home in the Netherlands.
Demographics.
The Netherlands had an estimated population of 16,785,403 on 30 April 2013. It is the 10th most populous country in Europe and the 63rd most populous country in the world. Between 1900 and 1950, the country's population almost doubled from 5.1 to 10.0 million people. From 1950 to 2000, the population further increased from 10.0 to 15.9 million people, but the rate of population growth was less than that of the previous fifty years. The estimated growth rate in 2013[ [update]] is 0.44%.
The fertility rate in the Netherlands is 1.78 children per woman (2013 est), which is high compared with many other European countries, but below the rate of 2.1 children per woman required for natural population replacement. Life expectancy is high in the Netherlands: 83.21 years for newborn girls and 78.93 for boys (2013 est). The country has a migration rate of 1.99 migrants per 1,000 inhabitants per year.
The majority of the population of the Netherlands is ethnically Dutch. A 2005 estimate counted: 80.9% Dutch, 2.4% Indonesian, 2.4% German, 2.2% Turkish, 2.0% Surinamese, 1.9% Moroccan, 0.8% Antillean and Aruban, and 7.4% others. Some 150.000 to 200.000 people living in the Netherlands are Expatriates, mostly concentrated in and around Amsterdam and The Hague, now constituting almost 10% of the population of these cities.
The Dutch are the tallest people in the world, with an average height of 1.81 m for adult males and 1.67 m for adult females in 2009. People in the south are on average about 2 cm shorter than those in the north.
Dutch people, or descendants of Dutch people, are also found in migrant communities worldwide, notably in Canada, Australia, Brazil, South Africa and the United States. According to the 2006 US Census, more than 5 million Americans claim total or partial Dutch ancestry. There are close to 3 million Dutch-descended Afrikaners living in South Africa. In 1940, there were 290,000 Europeans and Eurasians in Indonesia, but most have since left the country. According to Eurostat, in 2010 there were 1.8 million foreign-born residents in the Netherlands, corresponding to 11.1% of the total population. Of these, 1.4 million (8.5%) were born outside the EU and 0.428 million (2.6%) were born in another EU Member State.
The Netherlands is the 24th most densely populated country in the world, with 404.6 PD/sqkm—or 497 PD/sqkm if only the land area is counted. The Randstad is the country's largest conurbation located in the west of the country and contains the four largest cities: Amsterdam in the province North Holland, Rotterdam and The Hague in the province South Holland, and Utrecht in the province Utrecht. The Randstad has a population of 7 million inhabitants and is the 6th largest metropolitan area in Europe.
Language.
The official language is Dutch, which is spoken by the vast majority of the inhabitants. Besides Dutch, West Frisian is recognized as a second official language in the northern province of Friesland ("Fryslân" in West Frisian). West Frisian has a formal status for government correspondence in that province.
In the European part of the Netherlands two other regional languages are recognized under the European Charter for Regional or Minority Languages.
The first of these regional languages is Low Saxon ("Nedersaksisch" in Dutch) is recognised. Low Saxon consists of several dialects spoken in the north and east, like Twents in the region of Twente, and Drents in the province of Drenthe. Secondly, Limburgish is also recognised as regional language. It consists of Dutch varieties of Meuse-Rhenish Franconian languages and is spoken in the south-eastern province of Limburg.
The dialects most spoken in the Netherlands are the Brabantian-Hollandic dialects.
English has a formal status in the special municipalities of Saba and Sint Eustatius. It is widely spoken on these islands. Papiamento has a formal status in the special municipality of Bonaire. Yiddish and the Romani language were recognised in 1996 as non-territorial languages.
The Netherlands has a tradition of learning foreign languages, formalized in Dutch education laws. Some 87% of the total population indicate they are able to converse in English, 70% in German, and 29% in French. English is a mandatory course in all secondary schools. In most lower level secondary school educations ("vmbo"), one additional modern foreign language is mandatory during the first two years.
In higher level secondary schools ("havo" and "vwo"), two additional modern foreign languages are mandatory during the first three years. Only during the last three years in "vwo" one foreign language is mandatory. Besides English, the standard modern languages are French and German, although schools can replace one of these modern languages with Spanish, Turkish, Arabic, or Russian. Additionally, schools in the Frisia region teach and have exams in Frisian, and schools across the country teach and have exams in classical Greek and Latin for secondary school (called gymnasium or vwo+).
Religion.
Beliefs in the Netherlands (dec 2014) .
   Atheism
 (25%)   Agnostic
 (31%)   Ietsism
 (27%)   Theism
 (17%)
Religious adherence in the Netherlands (2013)
   No religion
 (55.1%)   Roman Catholic
 (23.7%)   Protestant
 (10.2%)   Islam
 (5%)  Other religion (6%)
The Netherlands was a predominantly Christian society until late into the 20th century, with a strong demarcation (pillarisation) between roughly the Catholic south on one side and the Calvinist north on the other side. In the 1960s, this started to diminish. Although religious diversity remains, there has been a decline of religious adherence. The Netherlands is one of the most secular countries in Western Europe, with only 39% being religiously affiliated (31% for those aged under 35), and fewer than 5.6% visiting religious services regularly (meaning once or more per month) in 2010. Religion is in the Netherlands generally considered a personal matter which is not supposed to be propagated in public.
In a December 2014 survey by the VU University Amsterdam was concluded that for the first time there are more atheists (25%) than theists (17%) in the Netherlands. The rest of the population being agnostic (31%) or ietsist (27%). Since 1989, the unaffiliated have become mainstream. According to the most recent statistics (2013) approximately 34% of the Dutch people adhere to the two historical Christian traditions of their country (23.7% the Catholic Church and 10.2% the Protestant Church in the Netherlands). Meanwhile, Muslims in the country constitute 5% of the total population, and 6% are adherents of other faiths (including Hinduism 0,6%, Judaism 0,1%, Buddhism 0,4%, minor Christian communities (4%), Ethnic religions, and New religious movements). Approximately 55% of the population has no religious affiliation.
Almost all Christian groups show a decrease in the number of members or less stable membership. However, in particular the loss of members of the two major churches, which are the Roman Catholic Church in the Netherlands, with a membership loss of more than 300,000 members between late 2005 and late 2010, and the Protestant Church in the Netherlands, with a membership loss of more than 150,000 members, cause the number of Christians in the Netherlands to have decreased: from approximately 7.132 million (44%) by the end of 2005 to 6.861 million (39%) by the end of 2010. Also atheism, ietsism, Agnosticism and Christian atheism are on the rise; the first three being widely accepted and the last one being more or less considered non-controversial. A countervailing trend is produced by a religious revival in the Protestant Bible Belt, and the growth of Muslim and Hindu communities resulting from immigration and high birth rates.
The SCP (Sociaal Cultureel Planbureau) expects the number of non-affiliated Dutch to be at 72% in 2020.
Different sources give very different percentages. A 2007 research "God in Nederland", based on in-depth interviews of 1132 people concluded that 61% of the Dutch are non-affiliated. Similar studies were done in 1966, 1979 and 1996, showing a steady decline of religious affiliation. That this trend is likely to continue is illustrated by the fact that in the age group under 35, 69% are non-affiliated. However, those who "do" identify with a religious denomination tend to be more profoundly religious than in the past. Religious belief is also regarded as a very personal affair, as is illustrated by the fact that 60% of self-described believers are not affiliated with any organised religion. There is a stronger stress on positive sides of belief, with Hell and the concept of damnation being pushed into the background.
Freedom of education has been guaranteed by the Dutch constitution since 1917, and schools run by religious groups (especially Catholic and Protestant) are funded by the government. All schools must meet strict quality criteria.
Three political parties in the Dutch parliament (CDA, ChristianUnion, and SGP) base their policy on Christian belief in varying degrees. Although the Netherlands is a secular state, in some municipalities where the Christian parties have the majority, the council meetings are opened by prayer.
All government layers and by far most companies give civil servants and employees one or more days off on Christian religious holidays, such as Christmas (2 days), Easter and Pentecost (for the Monday following the Sunday) and the Ascension of Jesus.
Roman Catholicism.
Currently, Roman Catholicism is the single largest religion of the Netherlands with around four million registered adherents which is 23.7% of the Dutch population in 2012. In the second half of the twentieth century a rapid secularization took place in the Catholic parts of the Netherlands. Most Catholics live in the southern provinces of North Brabant and Limburg, where they comprise a majority of the population in the diocese of Roermond in the province of Limburg. In the province of North Brabant Catholics are no longer a majority of the population as of 2010. The number of parishes in the Netherlands has dropped between 2003 and 2014 from 1525 to 842. Only 1–2% of the total population of Catholic area attend mass, and these churchgoers consist mostly of people over 65 years old.
In contrast to the rest of the country, the provinces of North Brabant and Limburg have historically been strongly Roman Catholic, and their people still largely consider the faith as a base for their cultural identity, though not necessarily a religious identity. The vast majority of the Catholic population in the Netherlands is now irreligious in practice. Research among Catholics in the Netherlands in 2007 shows that only 27% of the Dutch Catholics can be regarded as a theist, 55% as an ietsist / agnostic deist and 17% as agnostic or atheist.
Protestantism.
The Protestant Church in the Netherlands (PKN) follows with 10.2% of the population. It was formed in 2004 as a merger of the two major strands of Calvinism: the Dutch Reformed Church, the Reformed Churches in the Netherlands and a smaller Lutheran Church. Other Protestant churches, both orthodox Calvinist and liberal churches did not merge into the PKN.
The number of members falls on average by about 2.5% per year. This is caused primarily by the death of older members and little growth among the younger population.
Research in 2007 concludes that 42% of the members of the PKN are in fact non-theist. Furthermore, in the Protestant Church in the Netherlands (PKN) and several other smaller denominations of the Netherlands, one in six clergy are either agnostic or atheist.
While the Netherlands as a whole has become more secular, it still contains a Bible Belt, running from Zeeland to the northern parts of the province Overijssel, in which traditional Protestant beliefs remain fairly strong.
Humanism.
A research in 2003 shows that about 1.27 million people in the Netherlands express explicitly an affinity with secular humanism, which is about 9.4% of the total population.
Erasmus and Dirck Coornhert are important early representatives of humanism in the Netherlands in the 16th century.
In the 17th century, especially Spinoza and Hugo Grotius were important.
During the Age of Enlightenment (18th century), the importance of science and research increased sharply. Confidence in human understanding and logical reasoning was given shape in liberalism.
The German philosophers Ludwig Feuerbach and Kant and the evolution theory of Darwin, among other scientific theories in the 19th century, had an exceptionally strong influence and were a major step in the development of humanism in the Netherlands. The modern organized humanist movement began in the Netherlands in the mid-nineteenth century with the establishment of freethinkers association "De Dageraad" (Dawn). The members, including writer Multatuli and later Anton Constandse.
Marx' socialism had a significant influence on the Dutch humanism of the 20th century.
With the establishment of the humanistic associations "Humanitas" in 1945 and the "Humanistisch Verbond" in 1946, Dutch humanists organized themselves after the Second World War. When the Universal Declaration of Human Rights was adopted by the United Nations in 1948, the Dutch Humanist movements became involved with the establishment of the International Humanist and Ethical Union in 1952 (and since 1990 also the European Humanist Federation).
Pluralism.
Slightly more than half (52.8%) of the respondents to a research about humanism in 2003 affiliated with no religious or philosophical movement at all. In contrast 8% said to follow more than 1 movement. This form of pluralism occurs in all religious and philosophical Dutch movements, but is strongest among supporters of non-Western religions. 75% of Dutch Buddhists also affiliate with other religious or philosophical movements. Among followers of Hinduism in the Netherlands, this ratio is even higher, at 91%.
On the other hand, followers of Western religions and humanism, as well as movements in the 'other' category were least likely to affiliate with more than one religious or philosophical movement.
Within Western movements the people affiliating with humanism were most likely to also adhere to one or more other movements (47%). Most of these humanists adhere to Catholicism (27%), Protestantism (14%) or Buddhism (12%). Also 9% of Catholics, 6% of Protestants and 50% of the Buddhists counting themselves as humanists, as well as 25% of the Muslims, 55% of the Hindu, 19% of the Jews and 15% of the supporters of a movement other than these listed.
Minority religions.
In 2006, there were 850,000 Muslims.
As a result of different determination methods in 2012 this number had dropped to 825,000 (4% of the total Dutch population). Muslim numbers began to rise after the 1970s as the result of immigration. Some migrants from former Dutch colonies, such as Surinam and Indonesia, were sometimes Muslim, but migrant workers from Turkey and Morocco are the biggest part, as well as their children. During the 1990s, the Netherlands opened its borders for Muslim refugees from countries like Bosnia and Herzegovina, Iran, Iraq, Somalia, and Afghanistan. Muslims form a diverse group. Social tensions between native Dutch and migrant Muslims began to rise in the early 21st century, with the rise and murder of populist politician Pim Fortuyn by militant animal rights activist Volkert van der Graaf and the murder of Theo van Gogh by Mohammed Bouyeri. There are about 1,500 Ahmadi Muslims in the Netherlands.
The Netherlands has an estimated 250,000 Buddhists or people who feel strongly attracted by this religion, mainly ethnic Dutch people. There are approximately 200,000 Hindus, most of them of Surinamese origin. Sikhs are another religious minority in the Netherlands, numbering around 12,000, mainly located in or around Amsterdam. There are five gurudwaras in the Netherlands. The Association of Religion Data Archives (relying on World Christian Encyclopedia) estimated some 6,400 Bahá'ís in 2005.
Although the Holocaust deeply affected the Jewish community (killing about 75% of its 140,000 members at the time), it has managed to rebuild a vibrant and lively Jewish life for its approximately 45,000 current members. Around 10% of the population of Amsterdam was Jewish at the start of World War II.
Education.
Education in the Netherlands is compulsory between the ages of 5 and 16, and partially compulsory between the ages of 16 and 18.
All children in the Netherlands usually attend elementary school from (on average) ages 4 to 12. It comprises eight grades, the first of which is facultative. Based on an aptitude test, the 8th grade teacher's recommendation and the opinion of the pupil's parents or caretakers, a choice is made for one of the three main streams of secondary education (after completing a particular stream, a pupil may still continue in the penultimate year of the next stream):
Healthcare.
In 2014 the Netherlands has maintained its number one position at the top of the annual Euro health consumer index (EHCI), which compares healthcare systems in Europe, scoring 898 of a maximum 1,000 points. The Netherlands has been in the top three countries in each report published since 2005. On 48 indicators such as patient rights and information, accessibility, prevention and outcomes, the Netherlands secured its top position among 37 European countries for the fifth year in a row. 
The Netherlands was ranked first in a study in 2009 comparing the health care systems of the United States, Australia, Canada, Germany and New Zealand.
Ever since a major reform of the health care system in 2006, the Dutch system received more points in the Index each year. According to the HCP (Health Consumer Powerhouse), the Netherlands has 'a chaos system', meaning patients have a great degree of freedom from where to buy their health insurance, to where they get their healthcare service. But the difference between the Netherlands and other countries is that the chaos is managed. Healthcare decisions are being made in a dialogue between the patients and healthcare professionals.
Health insurance in the Netherlands is mandatory. Healthcare in the Netherlands is covered by two statutory forms of insurance:
While Dutch residents are automatically insured by the government for AWBZ, everyone has to take out their own basic healthcare insurance (basisverzekering), except those under 18 who are automatically covered under their parents' premium. If you don’t take out insurance, you risk a fine.
Insurers have to offer a universal package for everyone over the age of 18 years, regardless of age or state of health – it’s illegal to refuse an application or impose special conditions. 
In contrast to many other European systems, the Dutch government is responsible for the accessibility and quality of the healthcare system in the Netherlands, but not in charge of its management.
Healthcare in the Netherlands can be divided in several ways: three echelons, in somatic and mental health care and in 'cure' (short term) and 'care' (long term). Home doctors ("huisartsen", comparable to General Practitioners) form the largest part of the first echelon. Being referenced by a member of the first echelon is mandatory for access to the second and third echelon. The health care system is in comparison to other Western countries quite effective but not the most cost-effective.
Healthcare in the Netherlands is financed by a dual system that came into effect in January 2006. Long-term treatments, especially those that involve semi-permanent hospitalization, and also disability costs such as wheelchairs, are covered by a state-controlled mandatory insurance. This is laid down in the "Algemene Wet Bijzondere Ziektekosten" ("General Law on Exceptional Healthcare Costs") which first came into effect in 1968. In 2009 this insurance covered 27% of all health care expenses.
For all regular (short-term) medical treatment, there is a system of obligatory health insurance, with private health insurance companies. These insurance companies are obliged to provide a package with a defined set of insured treatments. This insurance covers 41% of all health care expenses.
Other sources of health care payment are taxes (14%), out of pocket payments (9%), additional optional health insurance packages (4%) and a range of other sources (4%). Affordability is guaranteed through a system of income-related allowances and individual and employer-paid income-related premiums.
A key feature of the Dutch system is that premiums may not be related to health status or age. Risk variances between private health insurance companies due to the different risks presented by individual policy holders are compensated through risk equalization and a common risk pool. Funding for all short-term health care is 50% from employers, 45% from the insured person and 5% by the government. Children under 18 are covered for free. Those on low incomes receive compensation to help them pay their insurance. Premiums paid by the insured are about 100 € per month (about US$127 in Aug. 2010 and in 2012 €150 or US$196,) with variation of about 5% between the various competing insurers, and deductible a year €220 US$288.
Culture.
Art, philosophy and literature.
The Netherlands has had many well-known painters. The 17th century, in which the Dutch Republic was prosperous, was the age of the "Dutch Masters", such as Rembrandt van Rijn, Johannes Vermeer, Jan Steen, Jacob van Ruysdael and many others. Famous Dutch painters of the 19th and 20th century were Vincent van Gogh and Piet Mondriaan. M. C. Escher is a well-known graphics artist. Willem de Kooning was born and trained in Rotterdam, although he is considered to have reached acclaim as an American artist.
The Netherlands is the country of philosophers Erasmus of Rotterdam and Spinoza. All of Descartes' major work was done in the Netherlands. The Dutch scientist Christiaan Huygens (1629–1695) discovered Saturn's moon Titan, argued that light travelled as waves, invented the pendulum clock and was the first physicist to use mathematical formulae. Antonie van Leeuwenhoek was the first to observe and describe single-celled organisms with a microscope.
In the Dutch Golden Age, literature flourished as well, with Joost van den Vondel and P. C. Hooft as the two most famous writers. In the 19th century, Multatuli wrote about the poor treatment of the natives in the Dutch colony, the current Indonesia. Important 20th century authors include Harry Mulisch, Jan Wolkers, Simon Vestdijk, Hella S. Haasse, Cees Nooteboom, Gerard (van het) Reve and Willem Frederik Hermans. Anne Frank's "Diary of a Young Girl" was published after she died in the Holocaust and translated from Dutch to all major languages.
The traditional Dutch architecture is especially valuated in Amsterdam, Delft and Leiden, with 17 and 18th century buildings along the canals. Smaller village architecture with wooden houses is found in Zaandam and Marken. Replicas of Dutch buildings can be found in Huis Ten Bosch, Nagasaki, Japan. A similar Holland Village is being built in Shenyang, China. Windmills, tulips, wooden shoes, cheese, Delftware pottery, and cannabis are among the items associated with the Netherlands by tourists.
The Netherlands has a long history of social tolerance and today is regarded as a liberal country, considering its drug policy and its legalisation of euthanasia. On 1 April 2001, the Netherlands became the first nation to legalise same-sex marriage.
Dutch value system and etiquette.
The Dutch have a code of etiquette which governs social behaviour and is considered important. Because of the international position of the Netherlands, many books have been written on the subject. Some customs may not be true in all regions and they are never absolute. In addition to those specific to the Dutch, many general points of European etiquette apply to the Dutch as well.
Dutch society is egalitarian, individualistic and modern. The people tend to view themselves as modest, independent and self-reliant. They value ability over dependency. The Dutch have an aversion to the non-essential.
Ostentatious behaviour is to be avoided. Accumulating money is fine, but public spending of large amounts of money is considered something of a vice and associated with being a show-off. A high lifestyle is considered wasteful and suspect with most people. The Dutch are proud of their cultural heritage, rich history in art and involvement in international affairs.
Dutch manners are open and direct with a no-nonsense attitude; informality combined with adherence to basic behaviour. This might be perceived as impersonal and patronising by other cultures, but is the norm in Dutch culture. According to a humorous source on Dutch culture, "Their directness gives many the impression that they are rude and crude—attributes they prefer to call 'openness"'.
A well known more serious source on Dutch etiquette is "Dealing with the Dutch" from Jacob Vossestein: "Dutch egalitarianism is the idea that people are equal, especially from a moral point of view, and accordingly, causes the somewhat ambiguous stance the Dutch have towards hierarchy and status." As always, manners differ between groups. Asking about basic rules will not be considered impolite. "What may strike you as being blatantly blunt topics and comments are no more embarrassing or unusual to the Dutch than discussing the weather".
The majority of the Dutch are irreligious and religion is in the Netherlands generally considered as a very personal matter which is not supposed to be propagated in public.
Music.
The Netherlands has multiple music traditions. Traditional Dutch music is a genre known as "Levenslied", meaning "Song of life", to an extent comparable to a French Chanson or a German Schlager. These songs typically have a simple melody and rhythm, and a straightforward structure of couplets and refrains. Themes can be light, but are often sentimental and include love, death and loneliness. Traditional musical instruments such as the accordion and the barrel organ are a staple of levenslied music, though in recent years many artists also use synthesizers and guitars. Artists in this genre include Jan Smit, Frans Bauer and André Hazes.
Contemporary Dutch rock and pop music (Nederpop) originated in the 1960s, heavily influenced by popular music from the United States and Britain. In the 1960s and 1970s the lyrics were mostly in English, and some tracks were instrumental. Bands such as Shocking Blue, (the) Golden Earring and Focus enjoyed international success. As of the 1980s, more and more pop musicians started working in the Dutch language, partly inspired by the huge success of the band Doe Maar. Today Dutch rock and pop music thrives in both languages, with some artists recording in either.
Current symphonic metal bands Epica and Within Temptation as well as jazz / pop singer Caro Emerald are having some international success. Contemporary local heroes include pop singer Anouk, country pop singer Ilse DeLange, in South Guelderish dialect singing folk band Rowwen Hèze, rock band BLØF
 and Dutch language duo Nick & Simon.
Early 1990s Dutch and Belgian house music came together in Eurodance project 2 Unlimited. Selling 18 million records, the two singers in the band are the most successful Dutch music artists to this day. Tracks like "Get Ready for This" are still popular themes of U.S. sports events, like the NHL. In the mid 1990s Dutch language rap and hip hop ("Nederhop") also came to fruition and has become popular in the Netherlands and Belgium. Artists with North African, Caribbean or Middle Eastern origins have strongly influenced this genre.
Since the 1990s Dutch electronic dance music (EDM) conquered the world in many forms, from trance, techno and gabber to hardstyle. Some of the world's best dance music DJs hail from the Netherlands, including Armin van Buuren, Tiësto, Hardwell, Oliver Heldens, Martin Garrix, Sander van Doorn and Afrojack; the first three of which have been ranked as best in the world by DJ Mag Top 100 DJs. The Amsterdam dance event (ADE) is the world's leading electronic music conference and the biggest club festival for the many electronic subgenres on the planet. These DJs also contribute to the world's mainstream pop music, as they frequently collaborate and produce for high profile international artists.
In classical music, Jan Sweelinck ranks as the Dutch most famous composer, with Louis Andriessen amongst the best known living Dutch classical composers. Notable violinists are Janine Jansen and André Rieu. The latter, together with his Johann Strauss Orchestra, has taken classical and waltz music on worldwide concert tours, the size and revenue of which are otherwise only seen from the world's biggest rock and pop music acts. Acclaimed harpist Lavinia Meijer in 2012 released an album with works from Philip Glass that she transcribed for harp, with approval of Glass himself. The Concertgebouw (completed in 1888) in Amsterdam is home to the Royal Concertgebouw Orchestra, considered one of the world's finest orchestras.
Aruba and the five main islands of the Netherlands Antilles are part of the Lesser Antilles island chain. Their music is a mixture of native, African and Dutch elements, and is closely connected with trends from neighboring islands like Barbados, Martinique, Trinidad and Tobago and Guadeloupe, as well as the mainland former Dutch possession of Suriname, which has exported kaseko music to great success on the islands. Curaçao and Bonaire likely have the most active and well-known music scenes. Curaçao is known for a kind of music called tumba, which is named after the conga drums that accompany it.
Film and television.
Some Dutch films – mainly by director Paul Verhoeven – have received international distribution and recognition, such as "Turkish Delight" (""Turks Fruit") (1973), "Soldier of Orange" ("Soldaat van Oranje") (1975), "Spetters" (1980) and "The Fourth Man" ("De Vierde Man"") (1983). Verhoeven then went on to direct big Hollywood movies like "RoboCop" and "Basic Instinct", and returned with Dutch film Black Book in 2006.
Other well-known Dutch film directors are Jan de Bont ("Speed"), Dick Maas ("De Lift"), Fons Rademakers ("The Assault"), documentary maker Bert Haanstra and Joris Ivens. Film director Theo van Gogh achieved international notoriety in 2004 when he was murdered in the streets of Amsterdam.
Internationally successful Dutch actors include Famke Janssen ("X-Men" films), Carice van Houten ("Game of Thrones"), Rutger Hauer ("Blade Runner"), Jeroen Krabbé and Derek de Lint.
The Netherlands has a well developed television market, with both multiple commercial and non-commercial broadcasters. Imported TV programmes, as well as interviews with responses in a foreign language, are virtually always shown with the original sound, and subtitled. The only exception are shows for children.
TV exports from the Netherlands mostly take the form of specific formats and franchises, most notably through internationally active TV production conglomerate Endemol, founded by Dutch media tycoons John de Mol and Joop van den Ende. Headquartered in Amsterdam, Endemol has around 90 companies in over 30 countries. Endemol and its subsidiaries create and run reality, talent, and game show franchises worldwide, including "Big Brother" and "Deal or No Deal". John de Mol later started his own company Talpa which created show franchises like "The Voice" and "Utopia".
Sports.
Approximately 4.5 million of the 16.8 million people in the Netherlands are registered to one of the 35,000 sports clubs in the country. About two-thirds of the population between 15 and 75 participates in sports weekly. Football is the most popular participant sport in the Netherlands, before field hockey and volleyball as the second and third most popular team sports. Tennis, gymnastics and golf are the three most widely engaged individual sports.
Organisation of sports began at the end of the 19th century and the beginning of the 20th century. Federations for sports were established (such as the speed skating federation in 1882), rules were unified and sports clubs came into existence. A Dutch National Olympic Committee was established in 1912. Thus far, the nation has won 266 medals at the Summer Olympic Games and another 110 medals at the Winter Olympic Games.
In international competition Dutch national teams and athletes are dominant in several fields of sport. The Netherlands women's hockey team is the most successful team in World Cup history. The Netherlands baseball team have won the European championship 20 times out of 32 events. Dutch K-1 kickboxers have won the K-1 World Grand Prix 15 times out of 19 tournaments.
The Dutch speed skaters' performance at the 2014 Winter Olympics, where they won 8 out of 12 events, 23 out of 36 medals, including 4 clean sweeps, is the most dominant performance in a single sport in Olympic history.
Motorcycle racing at the TT Assen Circuit has a long history. Assen is the only venue to have held a round of the Motorcycle World Championship every year since its creation in 1949. The circuit was purpose built for the Dutch TT in 1954, with previous events having been held on public roads.
Cuisine.
Originally, the country's cuisine has been shaped by the practices of fishing and farming, including the cultivation of the soil for growing crops and raising domesticated animals. Dutch cuisine is simple and straightforward, and contains many dairy products. Breakfast and lunch are typically bread with toppings, with cereal for breakfast as an alternative. Traditionally, dinner consists of potatoes, a portion of meat, and (seasonal) vegetables.
The Dutch diet was relatively high in carbohydrates and fat, reflecting the dietary needs of the labourers whose culture moulded the country. Without many refinements, it is best described as rustic, though many holidays are still celebrated with special foods. In the course of the twentieth century this diet changed and became much more cosmopolitan, with most global cuisines being represented in the major cities.
The Southern Dutch cuisine consists of the cuisines of the Dutch provinces of North Brabant and Limburg and the Flemish Region in Belgium. It is renowned for its many rich pastries, soups, stews and vegetable dishes and is often called Burgundian which is a Dutch idiom invoking the rich Burgundian court which ruled the Low Countries in the Middle Ages, renowned for its splendor and great feasts. It is the only Dutch culinary region that developed an haute cuisine.
In early 2014, Oxfam ranked the Netherlands as the country with the most nutritious, plentiful and healthy food, in a comparison of 125 countries.
Colonial heritage.
From the exploitations of the Dutch East India Company in the 17th century, to the colonisations in the 19th century, Dutch imperial possessions continued to expand, reaching their greatest extent by establishing a hegemony of the Dutch East Indies in the early 20th century. The Dutch East Indies, which later formed modern-day Indonesia, was one of the most valuable European colonies in the world and the most important one for the Netherlands. Over 350 years of mutual heritage has left a significant cultural mark on the Netherlands.
In the Dutch Golden Age of the 17th century, the Netherlands urbanised considerably, mostly financed by corporate revenue from the Asian trade monopolies. Social status was based on merchants' income, which reduced feudalism and considerably changed the dynamics of Dutch society. When the Dutch Royal Family was established in 1815, much of its wealth came from Colonial trade.
Universities such as the Royal Leiden University, founded in the 16th century, have developed into leading knowledge centres for South-east Asian and Indonesian studies. Leiden University has produced leading academics such as Christiaan Snouck Hurgronje, and still has academics who specialise in Indonesian languages and cultures. Leiden University and in particular KITLV are educational and scientific institutions that to this day share both an intellectual and historical interest in Indonesian studies. Other scientific institutions in the Netherlands include the Amsterdam Tropenmuseum, an anthropological museum with massive collections of Indonesian art, culture, ethnography and anthropology.
The traditions of the Royal Dutch East Indies Army (KNIL) are maintained by the Regiment Van Heutsz of the modern Royal Netherlands Army. A dedicated "Bronbeek Museum", a former home for retired KNIL soldiers, exists in Arnhem to this day.
A specific segment of Dutch literature called Dutch Indies literature still exists and includes established authors, such as Louis Couperus, the writer of "The Hidden Force", taking the colonial era as an important source of inspiration. One of the great masterpieces of Dutch literature is the book "Max Havelaar", written by Multatuli in 1860.
The majority of Dutchmen that repatriated to the Netherlands after and during the Indonesian revolution are Indo (Eurasian), native to the islands of the Dutch East Indies. This relatively large Eurasian population had developed over a period of 400 years and were classified by colonial law as belonging to the European legal community. In Dutch they are referred to as "Indische Nederlanders" or as Indo (short for Indo-European).
Including their second generation descendants, Indos are currently the largest foreign-born group in the Netherlands. In 2008, the Dutch Census Bureau for Statistics (CBS) registered 387,000 first- and second-generation Indos living in the Netherlands. Although considered fully assimilated into Dutch society, as the main ethnic minority in the Netherlands, these 'repatriants' have played a pivotal role in introducing elements of Indonesian culture into Dutch mainstream culture.
Practically every town in the Netherlands has a "Toko" (Dutch Indonesian Shop) or an Indonesian restaurant and many 'Pasar Malam' (Night market in Malay/Indonesian) fairs are organised throughout the year. Many Indonesian dishes and foodstuffs have become commonplace in the Netherlands. Rijsttafel, a colonial culinary concept, and dishes such as Nasi goreng and satay are very popular in the Netherlands.

</doc>
<doc id="21149" url="http://en.wikipedia.org/wiki?curid=21149" title="N.W.A">
N.W.A

N.W.A (an abbreviation of Niggaz Wit Attitudes) was an American hip hop group from Compton (California) that is widely considered to have been among the earliest and most significant popularizers of the gangsta rap and West Coast hip hop subgenres while also being credited by many as one of the seminal groups in the history of hip hop music. Active from 1986 to 1991, the rap group endured controversy due to their music's explicit lyrics that many viewed as being disrespectful of women, as well as its glorification of drugs, and crime. The group was subsequently banned from many mainstream American radio stations. In spite of this, the group has sold over 10 million units in the United States alone. The group was also known for their deep hatred of the police system, which sparked much controversy over the years.
The original lineup consisted of Arabian Prince, DJ Yella, Dr. Dre, Eazy-E, and Ice Cube. MC Ren joined in 1988, with Arabian Prince leaving the group later that same year. Ice Cube left in December 1989 over royalty disputes. Several members would later become platinum-selling solo artists in the 1990s. Their debut album "Straight Outta Compton" marked the beginning of the new gangsta rap era as the production and social commentary in their lyrics were revolutionary within the genre. "Rolling Stone" ranked N.W.A number 83 on their list of the "100 Greatest Artists of All Time." In October 2012, N.W.A were nominated for induction into the Rock and Roll Hall of Fame for the first time. In October 2013 the group was nominated for a second time.
History.
Formation and "Panic Zone" (1986–1988).
The group was assembled by Compton-based Eazy-E, who co-founded Ruthless Records with Jerry Heller. Eazy-E sought an introduction to Dr. Dre from Steve Yano, a Los Angeles entrepreneur who sold rap albums out of the Roadium Swap Meet in Gardena, CA and served as a clearing house for new music of that generation. Although initially rebuffed, Steve was impressed by Eazy-E's persistence and arranged a meeting with Dr. Dre. Initially, N.W.A consisted of Eazy-E and Dr. Dre, who in turn brought DJ Yella on board. Dre and Yella were both formerly members of the World Class Wreckin' Cru, as DJs and producers. Together with fellow producer Arabian Prince, Ice Cube was added to the roster after he had started out as a rapper for the group "C.I.A." Ruthless released the single "Panic Zone" in 1987 with Macola Records, which was later included on the compilation album "N.W.A. and the Posse". N.W.A was still in its developing stages, and is only credited on three of the eleven tracks, notably the uncharacteristic record "Panic Zone," "8-Ball," "Dopeman," which marked the first collaboration of Arabian Prince, DJ Yella, Dr. Dre, and Ice Cube. Mexican rapper Krazy-Dee co-wrote "Panic Zone," which was originally called "Hispanic Zone," but the title was later changed when Dr. Dre advised Krazy-Dee that the word "hispanic" would hinder sales. Also included was Eazy-E's solo track "Boyz-n-the-Hood." In 1988, rapper MC Ren joined the group.
"Straight Outta Compton", "Eazy-Duz-It" (1988–1989).
N.W.A released their debut studio album, "Straight Outta Compton", in 1988. With its famous opening salvo of three tracks, the group reflected the rising anger of the urban youth. The opening song "Straight Outta Compton" introduced the group, "Fuck tha Police" protested police brutality and racial profiling, and "Gangsta Gangsta" painted the worldview of the inner-city youth. While the group was later credited with pioneering the burgeoning subgenre of gangsta rap, N.W.A referred to their music as "reality rap." Dr. Dre and DJ Yella, as HighPowered Productions, composed the beats for each song, with Dre making occasional rapping appearances. Ice Cube and MC Ren wrote most of the group's lyrics, including "Fuck tha Police," perhaps the group's most notorious song, which brought them into conflict with various law enforcement agencies. Under pressure from Focus on the Family, Milt Ahlerich, an assistant director of the FBI, sent a letter to Ruthless and its distributing company Priority Records, advising the rappers that "advocating violence and assault is wrong and we in the law enforcement community take exception to such action." This letter can still be seen at the Rock and Roll Hall of Fame in Cleveland, Ohio. Policemen refused to provide security for the group's concerts, hurting their plans to tour. Nonetheless, the FBI's letter only served to draw more publicity to the group.
"Straight Outta Compton" was also one of the first albums to adhere to the new Parental Advisory label scheme, then still in its early stages: the label at the time consisted of "WARNING: Moderate impact coarse language and/or themes" only. However, the taboo nature of N.W.A's music was the most important factor of its mass appeal. Media coverage compensated for N.W.A's lack of airplay and their album eventually went double platinum. One month after "Straight Outta Compton", Eazy-E's solo debut "Eazy-Duz-It" was released. The album was dominated by Eazy's persona—MC Ren was the only guest rapper—but behind the scenes it was a group effort. Music was handled by Dr. Dre and DJ Yella; the lyrics were largely written by MC Ren, with contributions from Ice Cube and The D.O.C. The album was another double platinum success for Ruthless (in addition to girl group J.J. Fad in 1988 and singer Michel'le in 1989). 1989 saw the re-issue of "N.W.A and the Posse" and "Straight Outta Compton" on CD, and the release of The D.O.C.'s "No One Can Do It Better". His album was essentially a collaboration with Dr. Dre and notably free of "gangsta rap" content, however including the N.W.A posse cut "The Grand Finalé." It would become another #1 album for the record label.
"100 Miles And Runnin'" and "Niggaz4Life" (1989–1991).
Due to a financial conflict, Ice Cube left in December 1989 over royalty disputes; having written almost half of the lyrics on "Straight Outta Compton" himself, he felt he was not getting a fair share of the profits. He wasted little time putting together his solo debut, 1990's "AmeriKKKa's Most Wanted", but he avoided mentioning his former label mates. N.W.A's title track from their 1990 EP "100 Miles and Runnin'", however, included a diss of Ice Cube:
"We started with five, but yo / One couldn't take it—So now it's four / Cuz the fifth couldn't make it." The video for the song depicted the remaining members of N.W.A together in a jail cell, while an Ice Cube look-alike is released. Also heard on the EP (which found its way on the "Efil4zaggin" CD re-issue) was "Real Niggaz," a full-blown diss on Ice Cube where the remaining members accuse him of cowardice, and question his authenticity, longevity and originality: "How the fuck you think a rapper lasts / With your ass sayin' shit that was said in the past / Yo, be original, your shit is sloppy / Get off the dick, you motherfuckin' carbon-copy", and "We started out with too much cargo / So I'm glad we got rid of Benedict Arnold, yo." The song "100 Miles and Runnin'" was Dr. Dre's final uptempo recording, which had been a common feature of late 1980s hip hop. After this, he focused on a midtempo, synthesizer based sound which would become known as G-funk, starting with "Alwayz Into Somethin'" from "Efil4zaggin" in 1991. The G-funk style dominated both the West and East Coast hip hop music scene for several years to come. N.W.A is referenced on Ice Cube's 1990 EP, "Kill at Will", where he name-checks his former group (likely in a mocking manner) on the song "Jackin' For Beats." On "I Gotta Say What Up!!!", Ice Cube gives shout-outs to his rap peers at the time, among them Public Enemy, Geto Boys, and Sir Jinx. At the end of the track, in what appears to be an on-the-phone interview, Ice Cube is asked, "Since you went solo, whatever happened to the rest of your crew?" and the phone is abruptly hung up on the interviewer.
The group's second full-length release, 1991's "Efil4zaggin" ("Niggaz4Life" spelled backwards), re-established the band in the face of Ice Cube's continued solo success. The album is considered by many Dr. Dre's finest production work, and it heralded the beginning of the G-Funk era. It also showed a clear animosity towards their former member, and derogatory references to Ice Cube are found in several songs. The interlude "A Message to B.A." echoes the beginning of his song "Turn Off the Radio" from "AmeriKKKa's Most Wanted": Ice Cube is first addressed by the name Benedict Arnold (after the infamous traitor of the American Revolution) but then named outright in a torrent of abuse from both the group and its fans: "When we see yo' ass, we gon' cut yo' hair off and fuck you with a broomstick" spoken by MC Ren. The N.W.A–Ice Cube feud eventually escalated, both on record and in real life. "AmeriKKKa's Most Wanted" had avoided direct attacks on N.W.A, but on "Death Certificate", Ice Cube's second full-length release, he retaliated. He sampled and mocked the "Message to B.A." skit before embarking on a full-blown tirade, the infamous "No Vaseline." In a series of verses, Ice Cube verbally assaulted the group: "You lookin' like straight bozos / I saw it comin' that's why I went solo / Kept on stompin' / When y'all Muthafuckas moved Straight outta Compton / You got jealous when I got my own company / But I'm a man, and ain't nobody humpin' me." He also responded to members MC Ren, Dr. Dre, and Eazy-E individually to "100 Miles and Runnin'", claiming "I started off with too much cargo, dropped four niggaz / And now I'm makin' all the dough," using homophobic metaphors to describe their unequal business relationship with Jerry Heller, who became the target of harsh insults:
"Get rid of that devil real simple / Put a bullet to his temple / Cuz you can't be the 'Niggaz 4 Life' crew / With a white Jew tellin' you what to do." The song attracted controversy for its antisemitism (the beginning of such accusations against Ice Cube during his affiliation with the Nation of Islam), based on the bashing of Heller's religion; The track was omitted from the UK release, and later pressings included a censored version of the song. In September 1990, members of hip hop act Above the Law clashed with Ice Cube and his posse Da Lench Mob during the annual New Music Seminar conference, forcing the latter to flee the premises of Times Square's Marriott Marquis, the venue of the event. On January 27, 1991, Dr. Dre assaulted Dee Barnes, host of the hip hop show "Pump It Up", after its coverage of the N.W.A/Ice Cube beef. According to "Rolling Stone" reporter Alan Light:
He picked her up and "began slamming her face and the right side of her body repeatedly against a wall near the stairway" as his bodyguard held off the crowd. After Dre tried to throw her down the stairs and failed, he began kicking her in the ribs and hands. She escaped and ran into the women's rest room. Dre followed her and "grabbed her from behind by the hair and proceeded to punch her in the back of the head."
In response, Dre commented: "People talk all this shit, but you know, if somebody fucks with me, I'm gonna fuck with them. I just did it, you know. Ain't nothing you can do now by talking about it. Besides, it ain't no big thing—I just threw her through a door."
The end of N.W.A (1991–1995).
1991's "Niggaz4Life" would be the group's final album. After Dr. Dre, The D.O.C. and Michel'le departed from Ruthless to join Death Row Records and allegations over Eazy-E being coerced into signing away their contracts (while however retaining a portion of their publishing rights), a bitter rivalry ensued. Dr. Dre began the exchange with Death Row's first release, 1992's "Fuck Wit Dre Day (And Everybody's Celebratin')", and its accompanying video featured a character named "Sleazy-E" who ran around desperately trying to get money. The insults continued on "The Chronic" with "Bitches Ain't Shit." Eazy-E responded in 1993 with the EP "It's On (Dr. Dre) 187um Killa" on the tracks "Real Muthaphuckkin G's" and "It's On." Eazy-E accused Dr. Dre of being a homosexual, calling him a "she thang," and criticizing Dre's new image by calling him and Snoop "studio gangsters." The music video for "Real Muthaphuckkin G's" showed a still of Dre wearing make-up and a sequined jumpsuit. The photos dated back to Dr. Dre's World Class Wreckin' Cru days, when such fashion was common among West Coast electro hop artists, prior to N.W.A's popularization of gangsta rap. Eazy-E kept dissing Dre and Death Row on most of his songs until his AIDS-related death on March 26, 1995. All bad blood finally ceased within the rest of the group. Dr. Dre and Ice Cube would later express their re-evaluated feelings to their old friend on 1999's "What's The Difference" and "Chin Check," 2000's "Hello," and 2006's "Growin' Up" and in the 2011 music video "I Need a Doctor."
Reunions and legacy (1995–present).
Having both parted with Ruthless Records on bad terms, tensions between Ice Cube and Dr. Dre eased on the other hand. Ice Cube made a cameo appearance in Dr. Dre's "Let Me Ride" video in 1993. The two recorded the hit song "Natural Born Killaz" for Snoop Doggy Dogg's 1994 short film and soundtrack "Murder Was the Case" and they also planned an album which was to be titled "Heltah Skeltah". Later Ice Cube appeared on MC Ren's album "Ruthless for Life" on the track "Comin' After You." MC Ren appeared on Dre's 1999 album "2001", and the three remaining N.W.A emcees would reunite for "Hello" on Ice Cube's 2000 album "War & Peace Vol. 2 (The Peace Disc)". The West Coast and "gangsta" music scene had however fallen out of the spotlight since the death of Tupac Shakur in 1996, and it was only after Dr. Dre's successful patronage of Eminem and Dre's ensuing comeback album "2001" that the genre and its artists would regain the national spotlight. 2000's all-star Up In Smoke Tour would reunite much of the N.W.A and Death Row families, and during time spent on the road, Dre, Ice Cube, MC Ren, guest star Snoop Dogg and Eminem began recording in a mobile studio. A comeback album entitled "Not These Niggaz Again" was planned (and would include DJ Yella, who had not been present on the tour).
However, due to busy and conflicting schedules as well as the obstacles of coordinating three different record labels (Priority, No Limit and Interscope), obtaining the rights to the name N.W.A and endorsing the whole project to gain exclusive rights, the album never materialized. Only two tracks from these sessions would be released: "Chin Check" (with Snoop Dogg as a member of N.W.A) from 1999's "Next Friday" soundtrack and "Hello" from Ice Cube's 2000 album "War & Peace Vol. 2 (The Peace Disc)". Both songs would appear on N.W.A's remastered and re-released "Greatest Hits". There would also be partial reunions on "Set It Off," from Snoop Dogg's "Tha Last Meal" (2000), which featured MC Ren and Ice Cube as well as former Death Row "inmates," and The D.O.C.'s "The Shit," from his 2003 album "Deuce", which featured MC Ren, Ice Cube, Snoop Dogg and Six-Two. Dr. Dre and DJ Yella were present in the studio for the latter song. In addition to the "Greatest Hits" initially released by Priority in 1996, Capitol and Ruthless Records jointly released ' in 1998, a compilation that contained songs by other rap artists and only three songs from the actual group but various solo tracks from the five members. The success of the album prompted a second volume, "The N.W.A Legacy, Vol. 2", two years later. It emulated the format of its predecessor, containing only three genuine N.W.A tracks and many solo efforts by the crew members. In 2007, a new greatest hits package was released, entitled '.
In 2014 Ice Cube appeared on MC Ren's remix for Rebel Music. This was the first time the duo had worked together since the N.W.A reunion in 2000.
Biopic.
New Line Cinema representatives announced to "Entertainment Weekly's" "Hollywood Insider Blog" that N.W.A's story is in development to become a feature film for theatrical release in 2012. However, this has been delayed to sometime in 2014. Yet again, the film has a final date to release on August 14. According to the Internet Movie Database, the script was researched and written by filmmaker S. Leigh Savidge and radio veteran Alan Wenkus, who worked closely with Eazy-E's widow, Tomica Woods-Wright. Ice Cube, and Dr. Dre will act as producers of the film. As of September 2011, John Singleton was selected as director. Ice Cube and Singleton previously collaborated on "Boyz n the Hood", a movie that was nominated for an Academy Award, and Ice Cube also played the part of the character "Fudge" in Singleton's "Higher Learning". The cast also has yet to be disclosed. To date, MC Ren and DJ Yella have not commented on whether or not they will be involved in the production. Casting calls began in the summer of 2010. There have been rumors of Lil Eazy E playing his late father Eazy-E, and Ice Cube's son and fellow rapper O'Shea Jackson II playing his father as well. Ice Cube stated of the movie "We're taking it to the nooks and crannies, I think deeper than any other article or documentary on the group," he said. "These are the intimate conversations that helped forge N.W.A. To me, I think it's interesting to anybody who loves that era and I don't know any other movie where you can mix Gangster Rap, the F.B.I., L.A. riots, HIV, and fucking feuding with each other. This movie has everything from Darryl Gates and the battering ram."
In August 2012, F. Gary Gray was selected as director rather than Singleton. The film, named "Straight Outta Compton", had been picked up by Universal Pictures who hired Jonathan Herman in December 2013 to draft a new script and brought in Will Packer to executive produce. On February 21, 2014, director F. Gary Gray announced a March 9, 2014 open casting call for the film via his Twitter account. There were also open casting calls in Atlanta and Chicago. Rapper YG auditioned to play MC Ren in the film. The project was scheduled to start filming in April 2014 but was pushed backed due to casting delays.
On June 18, 2014, Universal officially announced that the N.W.A biopic "Straight Outta Compton" will be released August 14, 2015. It was also confirmed that Ice Cube's son, O'Shea Jackson, Jr., will play a younger version of his father in the movie. O'Shea, Jr. will join Jason Mitchell and Corey Hawkins who will portray group members Eazy-E and Dr. Dre, respectively, in the film. To round out the cast of N.W.A, Aldis Hodge will play MC Ren. Originally, YG 400 auditioned for the role, but was turned down and Neil Brown, Jr. will be DJ Yella. In early July 2014, casting directors for the N.W.A biopic issued a casting call for extras and vintage cars in the Los Angeles area for scenes in the movie. According to the casting call release, the film is scheduled to start filming in August 2014.
Influence.
Although the group disbanded in 1991, it left a lasting legacy on hip hop music in the following decades. Their influence (from their funky, bass-driven beats to their exaggerated lyrics) was evident throughout the 1990s and even into the present, and is often credited as bridging the White/Black American musical lines with their massive appeal to White America in the late 1980s. In Dr. Dre's 1999 single Forgot About Dre the line "Who you think brought you the O.G.s, Eazy-Es, Ice Cube, and The D.O.C.s, the Snoop Dogg, and the group that said 'Motherfuck the police'?" outlines N.W.A's importance in hip hop. In the same song, one of Eminem's verse says "So what do you say to somebody you hate or anyone tryna bring trouble your way, Wanna resolve things in a bloodier way, Then just study a tape of N.W.A" refers to the negative reception of N.W.A's works by the mainstream radio, considering their songs to be violent. A scene in 2005 single Hate It or Love It by Game featuring 50 Cent shows Tequan Richmond (portraying The Game) and Zachary Williams (playing 50 Cent in their youth) being caught spraypainting "N.W.A" on a wall, resulting in their subsequent arrest by two policemen. Also, the rapper Game has a tattoo that says N.W.A on the right side of his chest.

</doc>
<doc id="21150" url="http://en.wikipedia.org/wiki?curid=21150" title="Nibble">
Nibble

In computing, a nibble (often nybble or even nyble to match the vowels of byte) is a four-bit aggregation, or half an octet. It is also known as half-byte, quadruple, or tetrade.
In a networking or telecommunication context, the nibble is often called a semi-octet or a quartet.
As a nibble contains 4 bits, there are sixteen (24) possible values. A nibble can be represented by a single hexadecimal digit and called a "hex digit".
A full byte (octet) is represented by two hexadecimal digits; therefore, it is common to display a byte of information as two nibbles. Sometimes the set of all 256 byte values is represented as a table 16×16, which gives easily readable hexadecimal codes for each value.
History.
The term 'nibble' originates from its representing 'half a byte', with 'byte' a homophone of the English word 'bite'. 
In 2014, David B. Benson, a former professor of Washington State University, remembered that he playfully used (and may have possibly coined) the term nibble as "half a byte" and unit of storage required to hold a BCD decimal digit around 1958, when talking to a programmer of Los Alamos Scientific Laboratory.
The alternative spelling 'nybble' reflects the spelling of 'byte', as noted in editorials of "Kilobaud" and "Byte" in the early 1980s.
Another early recorded use of the term 'nybble' was in 1977 within the consumer-banking technology group at Citibank. It created a pre-ISO 8583 standard for transactional messages between cash machines and Citibank's data centres that used the basic informational unit 'NABBLE'.
The nibble is used to describe the amount of memory used to store a digit of a number stored in packed decimal format (BCD) within an IBM mainframe. This technique is used to make computations faster and debugging easier. An 8-bit byte is split in half and each nibble is used to store one decimal digit. The last (rightmost) nibble of the variable is reserved for the sign. Thus a variable which can store up to nine digits would be "packed" into 5 bytes. Ease of debugging resulted from the numbers being readable in a hex dump where two hex numbers are used to represent the value of a byte, as 16×16 = 28. For example, a five-byte BCD value of codice_1 codice_1 codice_1 codice_1 codice_1 represents a decimal value of codice_1.
Historically, there are cases where nybble was used for a group of bits fewer than 8 but not necessarily 4. In the Apple II microcomputer line, much of the disk drive control was implemented in software. Writing data to a disk was done by converting 256-byte pages into sets of 5-bit (later, 6-bit) nibbles and loading disk data required the reverse. Note that the term "byte" once had this ambiguity and meant a set of bits but not necessarily 8, hence the distinction of "bytes" and "octets". Today, the terms 'byte' and 'nibble' almost always refer to 8-bit and 4-bit collections respectively and are very rarely used to express any other sizes.
The term 'semi-nibble' is used to refer to a 2-bit collection or half a nibble but rarely so.
Table of nibbles.
The sixteen nibbles and their equivalents in other numeral systems:
Low and high nibbles.
The terms "low nibble" and "high nibble" are used to denote the nibbles containing, respectively, the less significant bits and the more significant bits within a byte. In graphical representations of bits within a byte, the leftmost bit could represent the most significant bit (MSB), corresponding to ordinary decimal notation in which the digit at the left of a number is the most significant. In such illustrations the four bits on the left end of the byte form the high nibble, and the remaining four bits form the low nibble.
Extracting a nibble from a byte.
In the C programming language:
where codice_7 must be a variable or constant of an integral data type, and only the least-significant byte of codice_7 is used.
For example, codice_9 and codice_10.
In Common Lisp:

</doc>
<doc id="21151" url="http://en.wikipedia.org/wiki?curid=21151" title="New wave music">
New wave music

New wave music is a musical genre of pop/rock created in the late-1970s to mid-1980s with ties to 1970s punk rock. The wide range of bands categorized under this term has been a source of much confusion and controversy. The new wave sound of the late 1970s moved away from the smooth blues and rock & roll sounds to create music with a twitchy, agitated feel, choppy rhythm guitars and fast tempos. Initially—as with the later post-punk—new wave was broadly analogous to punk rock before branching as a distinctly identified genre, incorporating electronic/experimental music, mod, disco and pop. It subsequently engendered subgenres and fusions, including New Romantic and gothic rock.
New wave differs from other movements with ties to first-wave punk as it displays characteristics common to pop music, rather than the more "arty" post-punk, though it incorporates much of the original punk rock sound and ethos while arguably exhibiting greater complexity in both music and lyrics. Common characteristics of new wave music, aside from its punk influences, include the use of synthesizers and electronic productions, the importance of styling and the arts, as well as a great amount of diversity.
New wave has been called one of the definitive genres of the 1980s, after it grew partially fixated on MTV (The Buggles' "Video Killed the Radio Star" music video was broadcast as the first music video to promote the channel's launch). and the popularity of several new wave artists, attributing the exposure that was given to them by the channel. In the mid-1980s, differences between new wave and other music genres began to blur. New wave has enjoyed resurgences since the 1990s, after a rising "nostalgia" for several new wave-influenced artists. The revivals in the 1990s and early 2000s were small, but became popular by 2004; subsequently, the genre has influenced a variety of other music genres. During the 2000s, a number of acts explored new wave and post-punk influences, such as The Strokes, Interpol, Franz Ferdinand, and The Killers. These acts were sometimes labeled "New New Wave".
The term "new wave".
The catch-all nature of new wave music has been a source of much confusion and controversy. The 1985 discography "Who's New Wave in Music" listed artists in over 130 separate categories. The "New Rolling Stone Encyclopedia of Rock" uses the term "virtually meaningless" in its definition of new wave, while AllMusic mentions "stylistic diversity".
New wave first circulated as a rock music genre in the early 1970s, used by critics like Nick Kent and Dave Marsh to classify such New York-based groups as the Velvet Underground and New York Dolls. It gained a much wider currency beginning in 1976 when it appeared in UK punk fanzines such as "Sniffin' Glue" and also in newsagent music weeklies such as "Melody Maker" and "New Musical Express". In a November 1976 article in "Melody Maker", Caroline Coon used Malcolm McLaren's term "new wave" to designate music by bands not exactly punk, but related to, and part of the same musical scene. The term was also used in that sense by music journalist Charles Shaar Murray in his comments about The Boomtown Rats. For a period of time in 1976 and 1977, the terms new wave and punk were somewhat interchangeable. By the end of 1977, "new wave" had replaced "punk" as the definition for new underground music in the UK.
"The New York Dolls arrived and galvanized the entire scene. Real glam trash. Beautiful. They proved it was possible to be trashy and good at the same time. Kicked everyone into action at a desperate moment. They saved us all. At that moment, I was drawing lines into New York and the Velvets, European avant garde and electronic music, previous generation's Brit Psychedelia plus a ragged sort of insulting glam. I guess this was the start of the New Wave. By the way, whoever coined that New Wave byline is my hero. Because a New Wave is precisely what it was – and precisely what was needed at that moment."
—John Foxx
In the United States, Sire Records chairman Seymour Stein, believing that the term "punk" would mean poor sales for Sire's acts who had frequently played the club CBGB, launched a "Don't Call It Punk" campaign designed to replace the term with "new wave". As radio consultants in the United States had advised their clients that punk rock was a fad, they settled on the term "new wave". Like the filmmakers of the French new wave movement (after whom the genre was named), its new artists were anti-corporate and experimental (e.g. Ramones and Talking Heads). At first, most U.S. writers exclusively used the term "new wave" for British punk acts. Starting in December 1976, "The New York Rocker", which was suspicious of the term "punk", became the first American journal to enthusiastically use the term starting with British acts, later appropriating it to acts associated with the CBGB scene.
Music historian Vernon Joynson states that new wave emerged in the UK in late 1976, when many bands began disassociating themselves from punk. Music that followed the anarchic garage band ethos of the Sex Pistols was distinguished as "punk", while music that tended toward experimentation, lyrical complexity or more polished production, came to be categorized as "new wave". In the U.S., the first new wavers were the not-so-punk acts associated with the New York club CBGB (e.g. Talking Heads, Mink DeVille and Blondie).
CBGB owner Hilly Kristal, referring to the first show of the band Television at his club in March 1974, said, "I think of that as the beginning of new wave." Furthermore, many artists who would have originally been classified as punk were also termed new wave. A 1977 Phonogram Records compilation album of the same name ("New Wave") features US artists including the Dead Boys, Ramones, Talking Heads and The Runaways.
United States and United Kingdom differences.
New wave is much more closely tied to punk and came and went more quickly in the United Kingdom than in the United States. At the time punk began, it was a major phenomenon in the United Kingdom and a minor one in the United States. Thus when new wave acts started getting noticed in America, punk meant little to the mainstream audience and it was common for rock clubs and discos to play British dance mixes and videos between live sets by American guitar acts.
Post-punk music developments in the UK became mainstream and were considered unique cultural events. By the early 1980s, British journalists largely had abandoned using the term "new wave" in favor of subgenre terms such as "synthpop". By 1983, the term of choice for the US music industry had become "new music", while to the majority of US fans it was still a "new wave" reacting to album-based rock.
United States use of the term as substitute for "synthpop".
"Bit by bit the last traces of Punk were drained from New Wave, as New Wave went from meaning Talking Heads to meaning the Cars to Squeeze to Duran Duran to, finally, Wham! "
—Music critic Bill Flannigan writing in 1989
While a consensus has developed that new wave proper ended in the mid-1980s, knocked out by various guitar-driven rock music reacting against new wave, for most of the remainder of the 1980s the term "new wave" was widely applied to nearly every new pop or pop rock artist that predominantly used synthesizers. 
In the United States during the 21st century, "new wave" was still used to describe artists such as Morrissey, Duran Duran, Cyndi Lauper, and Devo. Late 1970s new wave acts such as The Pretenders and The Cars were more likely to be found on classic rock playlists than on new wave playlists there. Reflecting its British origins, the 2004 study "Popular Music Genres: An Introduction" had one paragraph dedicated to 1970s new wave artists in its punk chapter while there was a 20-page chapter on early 1980s synthpop.
Styles and subgenres.
The new wave sound of the late 1970s represented a break from the smooth-oriented blues and rock & roll sounds of late 1960s to mid-1970s rock music. According to music journalist Simon Reynolds, the music had a twitchy, agitated feel to it. New wave musicians often played choppy rhythm guitars with fast tempos. Keyboards were common as were stop-and-start song structures and melodies. Reynolds noted that new wave vocalists sounded high-pitched, geeky and suburban. A nervous, nerdy persona was a common characteristic of new wave fans and acts such as Talking Heads, Devo and Elvis Costello. This took the forms of robotic or spastic dancing, jittery high-pitched vocals, and clothing fashions such as suits and big glasses that hid the body.
This seemed radical to audiences accustomed to post-counterculture forms such as disco dancing and macho "cock rock" which emphasized a "let it hang loose" philosophy, open sexuality and sexual bravado. The majority of American male new wave acts of the late 1970s were from Caucasian middle-class backgrounds, and theorized that these acts intentionally presented these exaggerated nerdy tendencies associated with their "whiteness" either to criticize it or to reflect who they were.
The British pub rock scene of the mid-1970s was the source of several new wave acts such as Ian Dury, Nick Lowe, Eddie and the Hot Rods and Dr. Feelgood.
Singer-songwriters who were "angry" and "intelligent" and who "approached pop music with the sardonic attitude and tense, aggressive energy of punk" such as Elvis Costello, Joe Jackson, and Graham Parker were also part of the new wave music scene.
A British revival of ska music on the 2 Tone label, led by The Specials were more politically oriented than other new wave genres. Madness, the The Beat/English Beat, and Selecter were associated with this revival.
The idea of rock music as a serious art form started in the late 1960s and was the dominant view of the genre at the time of new wave's arrival. New wave looked back or borrowed in various ways from the years just prior to this occurrence. One way this was done was by taking an ironic look at consumer and pop culture of the 1950s and early 1960s. The B-52's became most noted for a kitsch and camp presentation with their bouffant wigs, beach party and sci-fi movie references. Other groups that referenced the pre-progressive rock era were The Go-Go's, Blondie and Devo.
Power pop continued the guitar-based, singles-oriented British invasion sound of the mid-1960s into the 1970s and the present day. Although the name "power pop" had been around before punk (it is believed to be coined by Pete Townshend in 1967) it became widely associated with new wave when "Bomp" and "Trouser Press" magazines (respectively in March and April 1978) wrote cover stories touting power pop as a sound that could continue new wave's directness without the negativity associated with punk. Cheap Trick, The Romantics, The Records, Shoes, The Motors, The Only Ones, The Plimsouls, the dB's, The Beat, The Vapors, 20/20 and Squeeze were groups that found success playing this style. The Jam was the prime example of the mod sensibility of British power pop. By the end of 1979 a backlash had developed against power pop in general, particularly in regards to the Los Angeles scene. The skinny ties worn by a lot of LA power pop groups, epitomized by The Knack, became symbolic of the supposed lack of authenticity of the genre. Power pop's association with the genre has largely been forgotten.
The term "post-punk" was coined to describe groups such as Public Image Ltd, Siouxsie and the Banshees, Joy Division, Gang of Four, Wire, The Fall, Magazine, and The Cure, which were initially considered part of new wave but were more ambitious, serious and challenging, as well as darker and less pop-oriented. Some of these groups would later adopt synths. Although distinct, punk, new wave, and post-punk all shared common ground: an energetic reaction to what they perceived as the overproduced, uninspired popular music of the 1970s.
 The New Romantic scene had developed in the London nightclubs Billy's and The Blitz and was associated with bands such as Duran Duran, Japan, Ultravox, Visage, Adam and the Ants, Bow Wow Wow, Soft Cell, Spandau Ballet, ABC and Culture Club. They adopted their visual and musical style from David Bowie and Roxy Music.
Kraftwerk were acclaimed for their groundbreaking use of synthesizers. Their 1975 pop single "Autobahn" reached number 11 in the United Kingdom. In 1978, Gary Numan saw a synthesizer left by another music act and started playing around with it. By 1979 his band Tubeway Army had three albums and two singles in the British Top 20 and a No. 10 U.S. single. Numan's admitted amateurism and deliberate lack of emotion was a sea change from the masculine and professional image that professional synth players had in an era when elaborate, lengthy solos were the norm. His open desire to be a pop star broke from punk orthodoxy. The decreasing price and ease of use of the instrument led acts to follow in Kraftwerk and Numan's footsteps. While Tubeway Army also utilized conventional rock instruments, several acts that followed used only synthesizers. Synthpop (or "technopop" as it was described by the U.S. press) filled a void left by disco, and grew into a broad genre that included groups such as The Human League, Depeche Mode, Soft Cell, a-ha, New Order, Orchestral Manoeuvres in the Dark, Yazoo, Ultravox, Kajagoogoo, and the Thompson Twins.
In the early 1980s, new wave acts embraced a "crossover" of rock music with African and African-American styles. Adam and The Ants and Bow Wow Wow, both acts with ties to former Sex Pistols manager Malcolm McLaren, used Burundi-style drumming. The Talking Heads album "Remain in Light" was marketed and positivity reviewed as a breakthrough melding of new wave and African styles, although drummer Chris Frantz has said that he found out about this supposed African influence after the fact. The 1981 U.S. number 1 single "Rapture" by Blondie was an homage to rap music. The song name-checked rap artists and Fab 5 Freddie appeared in the video for the song. Second British Invasion acts were influenced by funk and disco. 
The genre produced numerous one-hit wonders.
"Rock en español".
New Wave also influenced bands in the Hispanosphere, some of whom have gained widespread success throughout Latin American nations, although lesser known in the English-speaking world. This movement, known as Rock en español borrowed heavily from the styles of American and British New Wave, while also incorporating Latin American rhythms, such as cumbia and salsa. Bands in this genre promoted the fact that this new genre was "Rock en tu idioma", or "Rock in your language," offering a Spanish-language alternative to the Hispanic culture. Influential bands in this genre include:
Reception in the United States.
In the summer of 1977 both "Time" and "Newsweek" magazines wrote favorable lead stories on the "punk/new wave" movement.<ref name=punk/newwave></ref> Acts associated with the movement received little or no radio airplay or music industry support. Small scenes developed in major cities. Continuing into the next year, public support remained limited to select elements of the artistic, bohemian and intellectual population, as arena rock and disco dominated the charts.
Starting in late 1978 and continuing into 1979, acts associated with punk and acts that mixed punk with other genres began to make chart appearances and receive airplay on rock stations and rock discos. Blondie, Talking Heads, The Police and The Cars would chart during this period. "My Sharona", a single from The Knack, was "Billboard" magazine's number one single of 1979. The success of "My Sharona" combined with the fact that new wave albums were much cheaper to produce during a time when the music industry was in its worst slump in decades, prompted record companies to rush out and sign new wave groups. New wave music scenes developed in Ohio and Athens, Georgia. 1980 saw brief forays into new wave-styled music by non-new wave artists Billy Joel and Linda Ronstadt.
Early in 1980, influential radio consultant Lee Abrams wrote a memo saying that with a few exceptions, "we're not going to be seeing many of the new wave circuit acts happening very big over here (in America). As a movement, we don't expect it to have much influence." Lee Ferguson, a consultant to KWST, said in an interview that Los Angeles radio stations were banning disc jockeys from using the term and noted, "Most of the people who call music new wave are the ones looking for a way not to play it." Despite the success of Devo's socially critical but widely misperceived song "Whip It", second albums by artists who had successful debut albums, along with newly signed artists, failed to sell, and radio pulled most new wave programming.
The arrival of MTV in 1981 would usher in new wave's most successful era in the United States. British artists, unlike many of their American counterparts, had learned how to use the music video early on. Several British acts signed to independent labels were able to outmarket and outsell American artists that were signed with major labels. Journalists labeled this phenomenon a "Second British Invasion". MTV continued its heavy rotation of videos by new wave-oriented acts until 1987, when it changed to a heavy metal and rock dominated format.
In a December 1982 Gallup poll, 14% of teenagers rated new wave music as their favorite type of music, making it the third most popular. New wave had its greatest popularity on the West Coast. Unlike other genres, race was not a factor in the popularity of new wave music, according to the poll.
 Urban Contemporary radio stations were the first to play dance-oriented new wave artists such as the B-52's, Culture Club, Duran Duran and ABC.
New wave soundtracks were used in mainstream "Brat Pack" films such as "Valley Girl", "Sixteen Candles", "Pretty In Pink," and "The Breakfast Club". John Hughes, the director of several of these films, was enthralled with British new wave music and placed songs from acts such as The Psychedelic Furs, Simple Minds, and Echo and The Bunnymen in his films, helping to keep new wave in the mainstream. Several of these songs remain standards of the era. Critics described the MTV acts of the period as shallow or vapid. The homophobic slurs "faggot" and "art fag" were openly used to describe new wave musicians Despite the criticism, the danceable quality of the music and the quirky fashion sense associated with new wave artists appealed to audiences.
The use of synthesizers by new wave acts influenced the development of house music in Chicago and techno in Detroit. In September 1988 Billboard launched their Modern Rock chart. While the acts on the chart reflected a wide variety of stylistic influences, new wave's legacy remained in the large influx of acts from Great Britain and acts that were popular in rock discos, as well as the charts name itself which reflected how new wave had been marketed as "modern". New wave's indie spirit would be crucial to the development of college rock and grunge/alternative rock in the latter half of the 1980s and beyond.
Post-1980s revivals and influence.
 In the aftermath of grunge, the British music press launched a campaign to promote the New Wave of New Wave. This campaign involved overtly punk and new wave-influenced acts such as Elastica but was eclipsed by Britpop. Other acts of note during the 1990s included No Doubt, Metric, Six Finger Satellite, and Brainiac. During that decade, the synthesizer-heavy dance sounds of British and European new wave acts influenced various incarnations of Euro disco and trance. Chris Martin was inspired to start Coldplay by a-ha.
During the 2000s, a number of acts emerged that mined a diversity of new wave and post-punk influences. Among these were The Strokes, Interpol, Yeah Yeah Yeahs, Franz Ferdinand, The Epoxies, She Wants Revenge, Bloc Party, Foals, Kaiser Chiefs, and The Killers. These acts were sometimes labeled "New New Wave". By 2004, these acts were described as "hot". The new wave revival reached its apex during the mid-2000s with acts such as The Sounds, The Ting Tings, Hot Chip, Passion Pit, The Presets, La Roux, Ladytron, Shiny Toy Guns, Hockey, Gwen Stefani and Ladyhawke. While some journalists and fans regarded this as a revival, others argued that the phenomenon was a continuation of the original movements.
The Drums are an example of the trend in the U.S. indie pop scene that employs both the sounds and attitudes of the British new wave era. A new wave-influenced genre called chillwave also developed in the late 2000s, exemplified by artists like Toro Y Moi, Neon Indian, Twin Shadow and Washed Out.
Disney Channel stars such as Jonas Brothers and Hannah Montana have been described as embracing new wave sounds, as have acts like Marina and the Diamonds. Hip-hop artists commonly sample 1980s synthpop, and R&B artists such as Rihanna have also embraced that subgenre.
In electronic music.
During the late 1990s, new wave received a sudden surge of attention when it was fused with electro and techno during the short-lived electroclash movement. It received popular attention from musical acts such as I-F, Peaches, Fischerspooner, and Vitalic, but largely faded as a genre when it was combined with tech house to form the electro house genre.
During the mid 2000s, new rave combined new wave with elements from several other genres, such as indie rock and electro house, and added aesthetic elements archetypal of a rave, such as light shows and glow sticks. Despite the term itself receiving controversy to the point where many affiliated reject it, new rave as a musical genre has been fronted by artists such as the Klaxons, NYPC, Shitdisco, and Hadouken!
Since the late 2000s, nostalgia for 80's new wave has seen a resurgence in the form of "synthwave", which is primarily characterized by new wave and soundtrack influences and a retrofuturistic, cyberpunk visual aesthetic. This term has been applied in particular to the music of artists such as Kavinsky and Power Glove, and to the visual styles and soundtracks of films and video games such as "Drive", ', "Hotline Miami" and '.

</doc>
<doc id="21160" url="http://en.wikipedia.org/wiki?curid=21160" title="Telecommunications in the Netherlands">
Telecommunications in the Netherlands

Communications in the Netherlands.
Mail.
The postal service in the Netherlands is performed by PostNL in most cases—which has, as of 2008, a monopoly on letters lighter than 50 g. The monopoly is planned to expire in 2009. PostNL's competitors include Selekt Mail and Sandd. Post offices that are owned by Postbank and TNT Post have been earmarked for closure between 2008 and 2013.
Postal codes in the Netherlands are formed of four digits then two letters (in capitals), separated by a space—1234 AB, for example.
Telephone.
Telephones - main lines in use:
8.000.000 (2007)
Telephones - mobile cellular subscribers:
17.200.000 (2007)
Telephone system:
general assessment: highly developed and well maintained
domestic: extensive fixed-line fiber-optic network; cellular telephone system is one of the largest in Europe with three major network operators utilizing the third generation of the Global System for Mobile Communications (GSM).
<br>"international:"
9 submarine cables; satellite earth stations - 3 Intelsat (1 Indian Ocean and 2 Atlantic Ocean), 1 Eutelsat, and 1 Inmarsat (Atlantic and Indian Ocean regions) (2004)
Non-geographical codes.
"Starting with"
Radio, television and internet.
Radio broadcast stations:
AM 4, FM 58, shortwave 3 (1998)
see 
Radios:
15.3 million (1996)
Television broadcast stations:
25
Televisions:
6.700.000 (2002, CBS)
Internet Service Providers (ISPs):
33 (2007)
Country code (Top level domain): .nl

</doc>
<doc id="21161" url="http://en.wikipedia.org/wiki?curid=21161" title="Transport in the Netherlands">
Transport in the Netherlands

The Netherlands is both a very densely populated and a highly developed country, in which transport is a key factor of the economy. Correspondingly it has a very dense and modern infrastructure, facilitating transport with road, rail, air and water networks. In its Global Competitiveness Report for 2014-2015, the World Economic Forum ranked the Dutch transport infrastructure fourth in the world.
With a total road network of 139,295 km, including 2,758 km of expressways, the Netherlands has one of the densest road networks in the world; much denser than Germany and France, but still not as dense as Belgium. The Dutch also have a well developed railway network, that connects most major towns and cities, as well as a comprehensive dedicated cycling infrastructure, featuring some 35,000 km of track physically segregated from motorised traffic.
The port of Rotterdam is the world's largest seaport outside East Asia, and by far the largest port of Europe. It connects with its hinterland in Germany, Switzerland and France through rivers Rhine and Meuse. Two thirds of all inland water freight shipping within the E.U., and 40% of containers, pass through the Netherlands.
Mobility in the Netherlands is considerable. On the roads it has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car. Around half of all trips in the Netherlands are made by car, 25% by bicycle, 20% walking, and 5% by public transport. Additionally, Dutch airports handled 58 million passengers in 2013. Excluding air travel, the Dutch journey more than 30 km a day on average, which takes them just over an hour.
In 2010, 1.65 billion tons of goods traffic was registered, half of which moved by sea and inland shipping, and 40% by road transport. The remainder was mostly by pipelines; rail transport only handles 2% of freight movements through the Netherlands.
Road transport.
With 139,295 km of public roads, the Netherlands has one of the most dense road networks in the world - much denser than Germany and France, but still not as dense as Belgium. In 2013, 5,191 km were national roads, 7,778 km were provincial roads, and 125,230 km were municipality and other roads.
Dutch roads include 2,758 km of motorways and expressways, and with a motorway density of 64 kilometres per 1,000 km², the country also has one of the densest motorway networks in the world.
The Netherlands' main highway net ("hoofdwegennet") - comparable to Britains net of trunk roads - consists of most of its 5,200 km of national roads, supplemented with the most prominent provincial roads. Although only about 2,500 km are fully constructed to motorway standards, much of the remainder are also expressways for fast motor vehicles only.
Mobility on Dutch roads has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car, meaning that while Dutch roads are numerous, they are also used with one of the highest intensities of any road network. Car ownership in the Netherlands is high but not exceptional, and slightly lower than in surrounding countries. Goods vehicles account for 20% of total traffic.
The busiest Dutch motorway is the A16 in Rotterdam, with a traffic volume of 232,000 vehicles per day. The widest Dutch motorway is the A15/A16 just south of Rotterdam with 16 lanes in a 4+4+4+4 setup.
Cycling.
Cycling is a ubiquitous mode of transport in the Netherlands. 27% of all trips are by bicycle - the highest modal share of any country in the world. Moreover: 31% of the Dutch list the bike as their main mode of transport for daily activities. Some 85% of the people own at least one bicycle. All in all the Dutch are estimated to have at least 18 million bikes, which makes more than one per capita, and twice as many as the ca. 9 million motor vehicles on the road. Almost as many passenger kilometres are covered by bicycle as by train.
Cycling infrastructure is comprehensive, and public policy, urban planning & laws are bike-friendly. Most roads except for motorways support cyclists, and bikeways are clearly signposted, well maintained and well lit. Dedicated cycle tracks are common on busy roads - some 35,000 km of track has been physically segregated from motor traffic, equal to a quarter of the country's entire road network. Busy junctions often give priority to cyclists, or they are equipped with cycle-specific traffic lights. There are large bicycle parking facilities, particularly in city centres and at train stations. In 2013, the European Cyclists' Federation ranked the Netherlands, together with Denmark as the most bike-friendly country in Europe. Helmets are neither officially encouraged nor frequently worn.
Rail transport.
Most distance travelled on Dutch public transport is by rail. Like many other European countries, the Netherlands has a dense railway network, totalling 6,830 kilometres of track, or 3,013 "route" km, three quarters of which has been electrified. The network is mostly focused on passenger transport and connects virtually all major towns and cities, counting as many train stations as there are municipalities in the Netherlands. The national rail infrastructure is managed by public task company ProRail, and a number of different operators have concessions to run their trains.
Public passenger rail transport is operated mainly by Nederlandse Spoorwegen (NS) ("Dutch Railways"); minor parts by Arriva, Syntus, Connexxion, Breng, DB Regio, NMBS, Veolia and DB Regionalbahn Westfalen. During week days almost all railway stations are serviced at least twice an hour in each direction. , Large parts of the network are serviced by two to four trains per hour on average. Heavily used tracks are serviced up to 8 trains an hour. Per kilometre of track, the Dutch rail network is the busiest in the European Union, handling over a million passengers a day.
In recent years, the four largest railway stations in the Netherlands, the central stations of each of the largest cities: Amsterdam, Rotterdam, The Hague and Utrecht, have all entered into major reconstruction and expansion. The rebuild of Rotterdam Central was the first to complete, reopening in March 2014.
Public transport.
For longer distances the main public transport in the Netherlands is the train. Long-distance buses are limited to a few missing railway connections. Regional / rural public transport, serving small(er) towns is by bus. Local / urban public transport is also generally by bus, but the three biggest cities (Amsterdam, Rotterdam and The Hague) all have extensive tram systems, that in each case also connect with adjacent cities in their respective urban agglomerations. Amsterdam and Rotterdam have several metro lines as well.
Additionally, Rotterdam, The Hague and suburbs in between are connected by a light rail system called RandstadRail, and one line of the Rotterdam metro system connects all the way to The Hague Central station. Utrecht has its own light rail system, called fast tram, connecting the city with adjacent Nieuwegein and IJsselstein. Arnhem is the only Dutch town that still operates a trolleybus system.
Due to the large amount of waterways in the Netherlands, not every road connection has been bridged, and there are still some ferries in operation. In the Rotterdam region, a water bus public transport service is operating as well.
Public transport operators are both the public transport companies run by the big cities: GVB (Amsterdam), RET (Rotterdam) and HTM (The Hague), as well as private enterprise companies like Arriva, Connexxion, Qbuzz, Syntus and Veolia.
Air travel.
Schiphol airport, just southwest of Amsterdam, is the main international airport in the Netherlands, and the fifth busiest airport in Europe in terms of passengers. Schiphol is the primary hub for Dutch flag carrier airline KLM and its regional affiliate KLM Cityhopper, as well as for other Dutch airlines Arkefly, Corendon Dutch Airlines, Martinair and Transavia.com. The airport also serves as a European hub for American carrier Delta Air Lines.
In other regions there are much smaller international airports, the most popular being Eindhoven Airport, Rotterdam The Hague Airport, Maastricht Aachen Airport and Groningen Airport. On the latter two, a considerable share of flights is seasonal in nature. For transport within the country, air travel is hardly used.
In 2013 Dutch airports handled 58 million passengers (41 million on European flights, and 17 million on intercontinental flights), and 1.6 million metric tons of airfreight.
Water transport.
Ports and Harbours.
The Netherlands has thirteen seaports, three of which have international significance. Handling 440 million metric tons of cargo in 2013, the port of Rotterdam is the biggest port of Europe - as big as the next three biggest combined, and the eighth largest in the world. The Amsterdam seaport is the second in the country, and the fifth largest in Europe. Additionally, since 1998 the ports of Flushing and Terneuzen are working as one, under the name of Zeeland Seaports. Handling 34 million metric tons of cargo in 2012, this is now the third biggest Dutch seaport. For comparison: the nearby port of London handled 44 million tons in that year.
Through the rivers Rhine and Meuse, Rotterdam has excellent access to its hinterland upstream, reaching to Germany, France and Switzerland. The port's main activities are petrochemical industries and general cargo handling and transshipment. The harbour functions as an important transit point for bulk materials and between the European continent and overseas. From Rotterdam goods are transported by ship, river barge, train or road. In 2007, the Betuweroute, a new fast freight railway from Rotterdam to Germany, was completed.
Three Dutch ports are deepwater ports, that can handle fully laden Panamax ships: Rotterdam, Zeeland Seaports and the port of IJmuiden. Besides Rotterdam, Amsterdam and Zeeland, the ports of Moerdijk and Vlaardingen also support container liner shipping. Other notable port cities are Dordrecht, Haarlem and Den Helder, as well as Groningen, which controls the seaports of Delfzijl and Eemshaven. Den Helder is home to the Netherlands' main naval base.
Merchant marine.
"note:" many Dutch-owned ships are also operating under the registry of Netherlands Antilles (1998 est.)
Inland waters & shipping.
6,237 km of rivers and canals are navigable for ships of 50 tons. Some 3,740 km of this consists of canals.
At least 4,326 km of waterways are usable by craft up to 400 metric ton capacity, and over 3,000 km are usable by ships up to 1,250 metric ton capacity. Although another source states that all of 6,230 km is navigable for craft up to 400 tons, and over 4,000 km is usable by ships up to 1,500 metric ton capacity.
The Dutch inland shipping fleet is the biggest in Europe. Consisting of some 7,000 vessels, it takes a share of 35% of the national total annual freight transport, and as much as 80% of bulk transport. Also two thirds of all inland water freight transports within the E.U., and 40% of inland container shipping, pass through the Netherlands. All in all the Netherlands has so many waterways that virtually all major industrial areas and population centres can be reached by water via inland ports (200) and transhipment terminals (350).
Punting and canal boats are very common, and are used by a lot of tourists.
Pipelines.
Crude oil 418 km; petroleum products 965 km; natural gas 10,230 km
Government.
Transport in the Netherlands falls under the Ministry of Infrastructure and the Environment.
Economics.
Although transport economics is much more than just the economy of the transport sector itself, the latter is much easier to quantify. In 2012 the Dutch goods transport and storage sectors by themselves accounted for almost 400,000 full-time jobs, employing some 500,000 people. Gross revenues totalled 77 billion euro, leading to results of 4.3 billion euro.

</doc>
<doc id="21162" url="http://en.wikipedia.org/wiki?curid=21162" title="Armed forces of the Netherlands">
Armed forces of the Netherlands

The Armed forces of the Netherlands consist of the Army, Navy, and Air Force.
The service branches consist of:
In addition, within the Kingdom of the Netherlands, there are small local conscript forces on the islands of Aruba (Arumil) and Curaçao (Antmil). These operate under the auspices of the Royal Netherlands Navy and Marines.
The military ranks of the Dutch armed forces have similarities with British and U.S. military ranks. The highest-ranking officer in the Dutch military is the Chief of the Netherlands Defence Staff, who is usually (but not necessarily) a four-star officer (NATO OF-9).
Foundation in law and purpose.
The Dutch armed forces exist by declaration in the constitution of the Netherlands. Article 97 of this constitution determines that the armed forces exist
Interestingly, this means that the role and responsibility of the Dutch military in international stability and peacekeeping is constitutionally determined.
The same article of the constitution determines that supreme command of the Dutch military resides with the Government of the Netherlands. This has been the case since the constitution was changed in 1983; before then, supreme command of the armed forces of the Netherlands was held by the King of the Netherlands.
In addition, a second major change in military affairs was made in 2003. Before then, all citizens of the Netherlands were tasked with the defense of the kingdom. In keeping with the move to a professional military, this article was dropped.
Military personnel.
The Netherlands' military is currently a fully professional military. Conscription in the Netherlands was suspended in 1996 with the exception of Aruba and Curaçao. All military branches and specialties, except for the submarine service and the Marine Corps, are open to female recruits.
The Korps Mariniers is not open to women because of the extremely high physical demands for initial training it is found impossible for women to become marines. The Dutch Ministry of Defence employs almost 70,000 personnel, including both civilian and military personnel.
The Dutch military is part of the NATO militaries and therefore conforms to the structure of a NATO military. It also uses conforming rank structures.
All Dutch military personnel, officers and enlisted personnel, are required to take an oath of allegiance. This oath is recorded in the law on General Military Personnel Regulations (Algemeen Militair Ambtenarenregelement) in Article 126a.
Unionized military.
Unlike many military organizations, Dutch military members are allowed to form and join unions.
There are four of these unions:
All unions represent both current and retired military personnel and/or civilian personnel.
Contemporary campaigns.
Since the 1990s, the Dutch military has been involved in four major military campaigns:
Afghanistan.
As part of Operation Enduring Freedom as a response to those attacks, the Netherlands deployed aircrafts as part of the European Participating Air Force (EPAF) in support of ground operations in Afghanistan as well as Dutch naval frigates to police the waters of the Middle East/Indian Ocean. The Netherlands deployed further troops and helicopters to Afghanistan in 2006 as part of a new security operation in the south of the country. Dutch ground and air forces totalled almost 2,000 personnel during 2006, taking part in combat operations alongside British and Canadian forces as part of NATO's ISAF force in the south.
The Netherlands announced in December 2007 that it would begin withdrawing its troops from Afghanistan, which were mainly in Uruzgan Province, in July 2010. "I do not have assurances that other countries will be ready to replace Netherlands troops, but I am certain that Dutch troops will leave in 2010," Foreign Minister Maxime Verhagen said. "I indicated that in writing ... to the NATO secretary general, who has confirmed it." In January 2009, Prime Minister Jan Peter Balkenende reiterated that the 1,600 Dutch troops in Afghanistan would end their mission in 2010, saying "We will stop in Uruzgan in 2010." He ruled out the possibility of the Netherlands keeping its troops in Afghanistan past 2010 with any force comparable to its former deployment.
In December 2009, reacting to three requests received from the side of the U.S. by Vice President Biden, the special American representative to Afghanistan Holbrooke and Secretary of State Clinton and a request by Secretary General of NATO Rasmussen as well, the Dutch government announced that the final decision on the continuation of the mission in Uruzgan would be on its agenda in March 2010. Two ministers from the Dutch Labour Party (PvdA), Koenders (Development Aid) and Bos (Finance and Vice PM) in the meantime pleaded termination, which was also the opinion of the majority of the Dutch parliament.
On 10 December 2009, the Dutch daily newspaper De Telegraaf reported that the government was exploring areas elsewhere in Afghanistan to set up a new mission. The northern province of Kunduz was mentioned, where at the moment German and Belgian troops were deployed. On 9 December, allegedly PM Balkenende (CDA), the vice-PM's Bos (PvdA) and Rouvoet (ChristenUnie) and the three involved ministers Verhagen (CDA, Foreign Affairs), Van Middelkoop (ChristenUnie, Defense) and Koenders (PvdA, (Development Aid) secretly discussed the future Dutch engagement in Afghanistan, together with Commander of the Forces general Van Uhm.
In early February 2010, the disagreement between the PvdA on the one hand and CDA and ChristenUnie on the other about a request from NATO, by improper channels, for a renewed Dutch commitment in Afghanistan, came to a head. CDA and ChristenUnie wanted the freedom to consider this request—in spite of the decisions by the Minister of Defence and the votes in Parliament—whereas PvdA and a majority of the parties in the Dutch parliament stood by the earlier decision and refused any consideration of further Dutch involvement in Afghanistan. Thus, on 20 February, the PvdA had no choice but to resign their ministers from the Cabinet, leading to a collapse of the Dutch government. As a result, the NATO request could not be considered and Dutch troops withdrew later in 2010 according to the schedule agreed in 2007.
On 1 August 2010 the Dutch military formally declared its withdrawal from its four-year mission in Afghanistan; most soldiers are expected to be back in the Netherlands by September, excepting those working on the reset, redistribution and repatriation of materiel and supplies. The AH-64 Apache and F-16 squadron will remain longer in Afghanistan to support the withdrawal process and transports. The Dutch contingent has been replaced by soldiers from the U.S., Australia, Slovakia, and Singapore.

</doc>
<doc id="21163" url="http://en.wikipedia.org/wiki?curid=21163" title="Foreign relations of the Netherlands">
Foreign relations of the Netherlands

The foreign policy of the Netherlands is based on four basic commitments: to the Atlantic cooperation, to European integration, to international development and to international law . While historically the Netherlands was a neutral state, since the second World War it became a member of a large number of international organisations. The Dutch economy is very open and relies on international trade. One of the more controversial international issues surrounding the Netherlands is its liberal policy towards soft drugs and position of the Netherlands as one of the major exporters of hard drugs. During and after its Golden Age, the Dutch built up a commercial and colonial empire, which fell apart quickly after the Second World War; the historical ties inherited from its colonial past still influence the foreign relations of the Netherlands
Policy.
The Dutch Government conducted a review of foreign policy main themes, organization, and funding in 1995. The document "The Foreign Policy of the Netherlands: A Review" outlined the new direction of Dutch foreign policy. The Netherlands prioritizes enhancing European integration, maintaining relations with neighboring states, ensuring European security and stability (mainly through the mechanism of NATO and emphasizing the important role the United States plays in the security of Europe), and participating in conflict management and peacekeeping missions. The foreign policy review also resulted in the reorganization of the Ministry of Foreign Affairs. Through the creation of regional departments, the Ministry coordinates tasks previously divided among the international cooperation, foreign affairs, and economic affairs sections.
Atlantic cooperation.
Dutch security policy is based primarily on membership in NATO, which the Netherlands co-founded in 1949. Because of Dutch participation in NATO nuclear weapons are believed to be stationed in the Netherlands, see possible nuclear weapons in the Netherlands.
The Dutch also pursue defense cooperation within Europe, both multilaterally - in the context of the Western European Union and the European Security and Defence Policy of the EU - and bilaterally, as in the German-Netherlands Corps. In recent years, the Dutch have become significant contributors to UN peacekeeping efforts around the world as well as to the Stabililzation Force in Bosnia and Herzegovina (SFOR) in Bosnia.
European integration.
The Dutch have been strong advocates of European integration, and most aspects of their foreign, economic, and trade policies are coordinated through the European Union (EU). The Netherlands' postwar customs union with Belgium and Luxembourg (the Benelux group) paved the way for the formation of the European Community (precursor to the EU), of which the Netherlands was a founding member. Likewise, the Benelux abolition of internal border controls was a model for the wider Schengen Accord, which today has 29 European signatories (including the Netherlands) pledged to common visa policies and free movement of people across common borders.
The Dutch stood at the cradle of the 1992 Maastricht Treaty and have been the architects of the Treaty of Amsterdam concluded in 1998. The Dutch have thus played an important role in European political and monetary integration; indeed, until the year 2003, Dutchman Wim Duisenberg headed the European Central Bank. In addition, Dutch financial minister Gerrit Zalm was the main critic of the violation of the Stability and Growth Pact by France and Germany in 2004 and 2005.
Third World development.
The Netherlands is among the world's leading aid donors, giving almost $8 billion, about 0.8% of its gross national income (GNI) in official development assistance (ODA). It is one of five countries worldwide that meets the longstanding UN ODA target of 0.7% ODA/GNI. The country consistently contributes large amounts of aid through multilateral channels, especially the United Nations Development Programme, the international financial institutions, and EU programs. A large portion of Dutch aid funds also are channeled through private ("co-financing") organizations that have almost total autonomy in choice of projects.
The Netherlands is a member of the European Bank for Reconstruction and Development, which recently initiated economic reforms in central Europe. The Dutch strongly support the Middle East peace process and in 1998 earmarked $29 million in contributions to international donor-coordinated activities for the occupied territories and also for projects in which they worked directly with Palestinian authorities. These projects included improving environmental conditions and support for multilateral programs in cooperation with local non-governmental organizations. In 1998, the Dutch provided significant amounts of aid to the former Yugoslavia and Africa. The Dutch consistently provide significant amounts of relief aid to victims of natural disasters, such as Hurricane Mitch in Central America, the 2004 tsunami in Southeast Asia, and more recent catastrophes in Pakistan and Burma.
Export assistance grants.
"Developing countries aspiring to purchase foreign goods and services to invest in, inter alia, port facilities, roads, public transport, health care, or drinking water facilities may be eligible for a special Dutch grant facility. The grant facility, known as ORET (a Dutch acronym for Ontwikkelingsrelevante Exporttransacties, or Development-Related Export) serves to award grants to governments of developing countries for making payments to foreign suppliers."
International law.
A centuries-old tradition of legal scholarship has made the Netherlands the home of the International Court of Justice; the Iran-United States Claims Tribunal; the International Criminal Tribunal for the former Yugoslavia; the International Criminal Tribunal for Rwanda; and the International Criminal Court (ICC). In addition it hosts the European police organization, Europol; and the Organisation for the Prohibition of Chemical Weapons.
International organizations.
As a relatively small country, the Netherlands generally pursues its foreign policy interests within the framework of multilateral organizations. The Netherlands is an active and responsible participant in the United Nations system as well as other multilateral organizations such as the Organization for Security and Cooperation in Europe, Organisation for Economic Co-operation and Development (OECD), World Trade Organization (WTO), and International Monetary Fund.
The Netherlands is one of the founding members of what today is the European Union. It was one of the first countries to start European integration, through the Benelux in 1944 and the European Coal and Steel Community in 1952. Being a small country with a history of neutrality it was the host country for the important Maastricht Treaty and Amsterdam Treaty and is the seat of the International Court of Justice.
International issues.
The country is one of the major producers of illicit amphetamines and other synthetic drugs. It also functions as an important gateway for cocaine, heroin, and hashish entering Europe. A large portion of the world's XTC consumption is supplied by illegal laboratories from The Netherlands.
The Dutch also work with the U.S. and other countries on international programs against drug trafficking and organized crime. The Dutch-U.S. cooperation focuses on joint anti-drug operations in the Caribbean, including an agreement establishing Forward Operating Locations on the Dutch Kingdom islands of Curaçao and Aruba. The Netherlands is a signatory to international counter-narcotics agreements, a member of the United Nations International Drug Control Program, the UN Commission on Narcotic Drugs, and is a contributor to international counter-narcotics.
From June 26 until December 22, 2006, two children, Ammar (12 - 13) and Sara (10 - 11), lived in the Dutch embassy in Damascus because of a child custody dispute between the Dutch mother, supported by Dutch law and the Hague Convention on the Civil Aspects of International Child Abduction, and the Syrian father, supported by Syrian law (Syria is no participant of this convention). The children had been living in Syria since 2004, after an alleged international child abduction by the father from the Netherlands to Syria, during a family contact in which he supposedly would visit Paris with them. The children fled to the embassy because they would like to live with their mother in the Netherlands. Minister of Foreign Affairs Ben Bot travelled to Damascus, negotiated and on December 22 the children finally could return to the Netherlands.
The father claims that the Dutch government has promised not to prosecute him for the abduction. However, a Dutch prosecutor claims that he is free to prosecute the father and may well do that, and that the Dutch have only retracted the international request to arrest him outside the Netherlands.
Former Colonies.
The Caribbean islands of Aruba, Curaçao, Sint Maarten, Bonaire, Sint Eustatius and Saba are dependencies of the Netherlands. The latter three are part of the Netherlands proper and are collectively known as the Caribbean Netherlands. Suriname and Indonesia became independent of the Netherlands in the period of decolonization: Suriname in 1975 and Indonesia in 1945 (it was not until 16 August 2005 that the Dutch government recognized 1945 - and not 1949 - as the country's year of independence).

</doc>
<doc id="21164" url="http://en.wikipedia.org/wiki?curid=21164" title="Drug policy of the Netherlands">
Drug policy of the Netherlands

The drug policy of the Netherlands officially has four major objectives:
By contrast, most other countries take the point of view that recreational drug use is detrimental to society and must therefore be outlawed. This has caused friction between the Netherlands and other countries about the policy for cannabis, most notably with France and Germany. As of 2004, Belgium seems to be moving toward the Dutch model and a few local German legislators are calling for experiments based on the Dutch model. Switzerland has had long and heated parliamentary debates about whether to follow the Dutch model on cannabis, most recently deciding against it in 2004; currently a ballot initiative is in the works on the question. A new law came into force in the Netherlands requiring people to have membership cards to gain entry to coffeeshops, these cards are only available to residents of the Netherlands who need to apply for the card, (known as a 'weed pass') this was promptly adopted by several provinces including the cities of Maastricht and Eindhoven, there were proposals for this to apply to Amsterdam in 2012, although after fierce opposition from the Mayor of Amsterdam and many coffeeshop owners in Amsterdam the Government decided to allow the individual provinces to determine their own policy, of which Amsterdam rejected the membership cards and therefore the entry to coffeeshops and the sale of cannabis remains permissible to anyone over the age of 18. By test, a few coffeeshops in the south of the Netherlands were already forced to handle this new law. Residents are complaining about growing criminality problems due to drug dealers in the streets.
If seen to fruition, which seems likely, the new laws will reduce tourism in the Netherlands dramatically and cost the exchequer millions in lost revenue and well-established business are forecast to go bankrupt. The club owners argue that the previous law opened the door for other European nations with relaxed attitudes on cannabis to capitalize on the niche in the market and take the valuable tourist resource. Maastricht’s association of coffee shop owners lost in June 2012 in a Dutch court a legal challenge of the new rules against the city's mayor. Amsterdam has decided to not enforce the new law and will continue to sell to tourists.
In the last few years drug tourism and certain strains of cannabis with higher concentrations of THC have challenged the former policy in the Netherlands and led to a more restrictive approach; for example, a ban on selling cannabis to tourists in coffee shops suggested to start late 2011.
In October 2011 the Dutch government proposed a new law to the Dutch parliament, that will put cannabis with 15% THC or more onto the list of hard drugs. If the law comes into effect, it would prohibit "coffee shops" from selling cannabis of that potency. The government finds motivation from its experts' assertions, that cannabis of that strength have an "unacceptable risk" associated with its usage. 
Today, about 80% of the "coffee shops" sell, among their products, such kind of cannabis.
While the legalization of cannabis remains controversial, the introduction of heroin-assisted treatment in 1998 has been lauded for considerably improving the health and social situation of opiate-dependent patients in the Netherlands.
Public health.
Large-scale dealing, production, import and export are prosecuted to the fullest extent of the law, even if it does not supply end users or "coffeeshops" with more than the allowed amounts. Exactly how coffeeshops get their supplies is rarely investigated, however. The average concentration of THC in the cannabis sold in coffeeshops has increased from 9% in 1998 to 18% in 2005. This means that less plant material has to be consumed to achieve the same effect. One of the reasons is plant breeding and use of greenhouse technology for illegal growing of cannabis in Netherlands.
The former minister of Justice Piet Hein Donner announced in June 2007 that cultivation of cannabis shall continue to be illegal.
Non-enforcement.
The drug policy of the Netherlands is marked by its distinguishing between so called "soft" and "hard drugs". An often used argument is that alcohol, which is claimed by some scientists as a hard drug, is legal and a soft drug can't be more dangerous to society if it's controlled. This may refer to the Prohibition in the 1920s, when the U.S. government decided to ban all alcohol. Prohibition created a golden opportunity for organized crime syndicates to smuggle alcohol, and as a result the syndicates were able to gain considerable power in some major cities.
Cannabis remains a controlled substance in the Netherlands and both possession and production for personal use are still misdemeanors, punishable by fines. Coffee shops are also technically illegal but are flourishing nonetheless. However, a policy of non-enforcement has led to a situation where reliance upon non-enforcement has become common, and because of this the courts have ruled against the government when individual cases were prosecuted.
This is because the Dutch Ministry of Justice applies a "gedoogbeleid" (tolerance policy) with regard to the category "soft drugs": an official set of guidelines telling public prosecutors under which circumstances offenders should not be prosecuted. This is a more official version of a common practice in other European countries wherein law enforcement sets priorities regarding offenses on which it is important enough to spend limited resources.
According to current "gedoogbeleid" the possession of a maximum amount of five grams cannabis for personal use is not prosecuted. Cultivation is treated in a similar way. Cultivation of 5 plants or less is usually not prosecuted when they are renounced by the cultivator.
Proponents of "gedoogbeleid" argue that such a policy practices more consistency in legal protection than without it. Opponents of the Dutch drug policy either call for full legalization, or argue that laws should penalize morally wrong or deviant behavior, whether enforceable or not. In the Dutch courts, however, it has long been determined that the institutionalized non-enforcement of statutes with well defined limits constitutes "de facto" decriminalization. The statutes are kept on the books mainly due to international pressure and in adherence with international treaties. A November 2008 poll showed that a 60% majority of the Dutch population support the legalisation of soft drugs. The same poll showed that 85% supported closing of all cannabis coffee shops within 250 meters walking distance from schools.
Drug law enforcement.
Importing and exporting of any classified drug is a serious offence. The penalty can run up to 12 to 16 years if it is hard drug trade, maximum 4 years for import or export of large quantities of cannabis. It is prohibited to operate a motor vehicle while under the influence of any drug that affects driving ability to such an extent that you are unable to drive properly. (Section 8 of the 1994 Road Traffic Act section 1). The Dutch police have the right to do a drug test if they suspect influenced driving. For example, anybody involved in a traffic accident may be tested. Causing an accident that inflicts bodily harm, while under influence of any drug, is seen as a crime that may be punished by up to 3 years in prison (9 years in case of a fatal accident). Suspension of driving license is also normal in such a case (maximum 5 years). Schiphol, a large international airport near Amsterdam, has long practiced a zero tolerance policy regarding airline passengers carrying drugs. In 2006 there were 20,769 drug crimes registered by public prosecutors and 4,392 persons received an unconditional prison sentence The rate of imprisonment for drug crimes is about the same as in Sweden, which has a zero tolerance policy for drug crimes.
Despite the high priority given by the Dutch government to fighting illegal drug trafficking, the Netherlands continue to be an important transit point for drugs entering Europe. The Netherlands is a major producer and leading distributor of cannabis, heroin, cocaine, amphetamines and other synthetic drugs, and a medium consumer of illicit drugs. Despite the crackdown by Interpol on traffic and illicit manufacture of temazepam, the country has also become a major exporter of illicit temazepam of the "jelly" variety, trafficking it to the United Kingdom and other European nations. The Netherlands' special synthetic drug unit, set up in 1997 to coordinate the fight against designer drugs, appears to be successful. The government has intensified cooperation with neighbouring countries and stepped up border controls. In recent years, it also introduced so-called 100% checks and bodyscans at Schiphol Airport on incoming flights from Dutch overseas territories Aruba and Netherlands Antilles to prevent importing cocaine by means of swallowing balloons by mules.
Although drug use, as opposed to trafficking, is seen primarily as a public health issue, responsibility for drug policy is shared by both the Ministry of Health, Welfare, and Sports, and the Ministry of Justice.
The Netherlands spends more than €130 million annually on facilities for addicts, of which about fifty percent goes to drug addicts. The Netherlands has extensive demand reduction programs, reaching about ninety percent of the country's 25,000 to 28,000 hard drug users. The number of hard drug addicts has stabilized in the past few years and their average age has risen to 38 years, which is generally seen as a positive trend. Notably, the number of drug-related deaths in the country remains amongst the lowest in Europe.
On 27 November 2003, the Dutch Justice Minister Piet Hein Donner announced that his government was considering rules under which coffeeshops would only be allowed to sell soft drugs to Dutch residents in order to satisfy both European neighbors' concerns about the influx of drugs from the Netherlands, as well as those of Netherlands border town residents unhappy with the influx of "drug tourists" from elsewhere in Europe. The European Court of Justice ruled in December 2010 that Dutch authorities can ban coffee shops from selling marijuana to foreigners. The EU court said the southern Dutch city of Maastricht was within its rights when it introduced a "weed passport" in 2005 to prevent foreigners from entering cafés that sell marijuana.
In 2010 the owner of Netherlands's largest cannabis selling coffee shop was fined 10 million euros for breaking drug laws by keeping more than the tolerated amount of cannabis in the shop. He was also sentenced to a 16-week prison term.
Results of the drug policy.
Criminal investigations into more serious forms of organized crime mainly involve drugs (72%). Most of these are investigations of hard drug crime (specifically cocaine and synthetic drugs) although the number of soft drug cases is rising and currently accounts for 69% of criminal investigations.
In a study of the levels of cannabis, cocaine, ecstasy, meth and other amphetamine in wastewater from 42 major cities in Europe Amsterdam came near the top of the list in every category but meth.
In the province of North-Brabant in the south of the Netherlands, the organized crime organizations form the main producer of MDMA, amphetamine en cannabis in Europe. Together with the proximity of the ports of Antwerp and especially Rotterdam where heroin and cocaine enter the European continent, this causes these substances to be readily available for a relative low price. Therefor there is a large quantity drugs of a relative high quality with few pollution available. This means that users will not have to rely on more polluted substances with greater health risks. Together with an approach that focusses on easily accessible health care, harm reduction and prevention, this causes the medical condition of the Dutch addicts to be less severe than that of many other countries.
Implications of international law.
The Netherlands is a party to the 1961 Single Convention on Narcotic Drugs, the 1971 Convention on Psychotropic Substances, and the 1988 United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. The 1961 convention prohibits cultivation and trade of naturally-occurring drugs such as cannabis; the 1971 treaty bans the manufacture and trafficking of synthetic drugs such as barbiturates and amphetamines; and the 1988 convention requires states to criminalize illicit drug possession:
Subject to its constitutional principles and the basic concepts of its legal system, each Party shall adopt such measures as may be necessary to establish as a criminal offence under its domestic law, when committed intentionally, the possession, purchase or cultivation of narcotic drugs or psychotropic substances for personal consumption contrary to the provisions of the 1961 Convention, the 1961 Convention as amended or the 1971 Convention.
The International Narcotics Control Board typically interprets this provision to mean that states must prosecute drug possession offenses. The conventions clearly state that controlled substances are to be restricted to scientific and medical uses. However, Cindy Fazey, former Chief of Demand Reduction for the United Nations Drug Control Programme, believes that the treaties have enough ambiguities and loopholes to allow some room to maneuver. In her report entitled "The Mechanics and Dynamics of the UN System for International Drug Control", she notes:
Many countries have now decided not to use the full weight of criminal sanctions against people who are in possession of drugs that are for their personal consumption. The Conventions say that there must be an offence under domestic criminal law, it does not say that the law has to be enforced, or that when it is what sanctions should apply. . . . Despite such grey areas latitude is by no means unlimited. The centrality of the principle of limiting narcotic and psychotropic drugs for medical and scientific purposes leaves no room for the legal possibility of recreational use. . . . Nations may currently be pushing the boundaries of the international system, but the pursuit of any action to formally legalize non-medical and non-scientific drug use would require either treaty revision or a complete or partial withdrawal from the current regime.
The Dutch policy of keeping anti-drug laws on the books while limiting enforcement of certain offenses is carefully designed to reduce harm while still complying with the letter of international drug control treaties. This is necessary in order to avoid criticism from the International Narcotics Board, which historically has taken a dim view of any moves to relax official drug policy. In their annual report, the Board has criticised many governments, including Canada, for permitting the medicinal use of cannabis, Australia for providing injecting rooms and the United Kingdom for proposing to downgrade the classification of cannabis, which it has since done (although this change was reversed by the Home Secretary on 7 May 2008 against the advice of its own commissioned report)
Recent developments.
The liberal drug policy of the authorities in the Netherlands especially led to problems in "border hot spots" that attracted "drug tourism" as well as trafficking and related law enforcement problems in towns like Enschede in the East and Terneuzen, Venlo, Maastricht and Heerlen in the South. In 2006, Gerd Leers, then mayor of the border city of Maastricht, on the Dutch-Belgian border, criticised the current policy as inconsistent, by recording a song with the Dutch punk rock band De Heideroosjes. By allowing possession and retail sales of cannabis, but not cultivation or wholesale, the government creates numerous problems of crime and public safety, he alleges, and therefore he would like to switch to either legalising and regulating production, or to the full repression that his party (CDA) officially advocates. The latter suggestion has widely been interpreted as rhetorical. Leers's comments have garnered support from other local authorities and put the cultivation issue back on the agenda.
In November 2008, Pieter van Geel, the leader of the CDA (Christian Democrats) in the Dutch parliament, called for a ban on the cafés where marijuana is sold. He said the practice of allowing so-called coffee shops to operate had failed. The CDA had the support of its smaller coalition partner, the CU (ChristenUnie), but the third party in government, PvdA (Labour), opposed. The coalition agreement worked out by the three coalition parties in 2007 stated that there would be no change in the policy of tolerance. Prominent CDA member Gerd Leers spoke out against him: cannabis users who now cause no trouble would be viewed as criminals if an outright ban was to be implemented. Van Geel later said that he respected the coalition agreement and would not press for a ban during the current government's tenure .
By 2009, 27 "coffee shops" selling cannabis in Rotterdam, all within 200 metres from schools, must close down. This is nearly half of the "coffee shops" that currently operate within its municipality. This is due to the new policy of city mayor Ivo Opstelten and the town council. The higher levels of the active ingredient in cannabis in Netherlands create a growing opposition to the traditional Dutch view of cannabis as a relatively innocent soft drug. Supporters of "coffee shops" state that such claims are often exaggerated and ignore the fact that higher content means a user needs to use less of the plant to get the desired effects, making it in effect safer. Dutch research has however shown that an increase of THC content also increase the occurrence of impaired psychomotor skills, particularly among younger or inexperienced cannabis smokers, who do not adapt their smoking-style to the higher THC content. Closing of "coffeeshops" is not unique for Rotterdam. Many other towns have done the same in the last 10 years.
In 2008, the municipality of Utrecht imposed a Zero Tolerance Policy to all events like the big dance party Trance Energy held in Jaarbeurs. However, such zero-tolerance policy at dance parties are now becoming common in the Netherlands and are even stricter in cities like Arnhem.
The two towns Roosendaal and Bergen op Zoom announced in October 2008 that they would start closing all "coffee shops", each week visited by up to 25000 French and Belgian drug tourists, with closures beginning in February 2009.
In May 2011 the Dutch government announced that tourist are to be banned from Dutch coffee shops, starting in the southern provinces and at the end of 2011 in the rest of the country. In a letter to the parliament, the Dutch health and justice ministers said that, "In order to tackle the nuisance and criminality associated with coffee shops and drug trafficking, the open-door policy of coffee shops will end".
A government committee delivered in June 2011 a report about Cannabis to the Dutch government. It includes a proposal that cannabis with more than 15 percent THC should be labeled as hard drugs. Higher concentrations of THC and drug tourism have challenged the current policy and led to a re-examination of the current approach; e.g. ban of all sales of cannabis to tourists in coffee shops from end of 2011 was proposed but currently only the border city of Maastricht has adopted the measure in order to test out its feasibility. According to the initial measure, starting in 2012, each coffee shop was to operate like a private club with some 1,000 to 1,500 members. In order to qualify for a membership card, applicants would have to be adult Dutch citizens, membership was only to be allowed in one club.
In Amsterdam 26 coffeeshops in the De Wallen area will have to close their doors between 1 September 2012 and 31 August 2015.
A Dutch judge has ruled that tourists can legally be banned from entering cannabis cafés, as part of new restrictions which come into force in 2012.
Law banning "magic mushrooms".
In October 2007, the prohibition of hallucinogenic or "magic mushrooms" was announced by the Dutch authorities.
On April 25, 2008, the Dutch government, backed by a majority of members of parliament, decided to ban cultivation and use of all magic mushrooms. Amsterdam mayor Job Cohen proposed a three-day cooling period in which clients would be informed three days before actually procuring the mushrooms and if they would still like to go through with it they could pick up their spores from the smart shop.
The ban has been considered a retreat from liberal drug policies. This followed a few deadly incidents mostly involving tourists. These deaths were not directly caused by the use of the drug "per se", but by deadly accidents occurring while under the influence of magic mushrooms.
As of December 1, 2008, all psychedelic mushrooms are banned. However, schlerotia (what are termed as "truffles"), mushroom spores, and active mycellium cultures remained legal and are readily available in the "smartshops", the stores in the Dutch cities that sell legal drugs, herbs and related gadgets.
Supply control.
The relatively recent increase in the cocaine trafficking business has been largely focused on the Caribbean area. Since early 2003, a special law court with prison facilities has been operational at Schiphol airport. Since the beginning of 2005, there has been 100% control of all flights from key countries in the Caribbean. In 2004, an average of 290 drug couriers per month were arrested, decreasing to 80 per month by early 2006.

</doc>
<doc id="21168" url="http://en.wikipedia.org/wiki?curid=21168" title="2001 in the Netherlands">
2001 in the Netherlands

This article lists some of the events that took place in the Netherlands in 2001.
Births.
15 April - Anna van Lippe-Biesterfeld van Vollenhoven, daughter of Prince Maurits and Princess Marilène

</doc>
<doc id="21170" url="http://en.wikipedia.org/wiki?curid=21170" title="Numeral system">
Numeral system

A numeral system (or system of numeration) is a writing system for expressing numbers, that is, a mathematical notation for representing numbers of a given set, using digits or other symbols in a consistent manner. It can be seen as the context that allows the symbols "11" to be interpreted as the binary symbol for "three", the decimal symbol for "eleven", or a symbol for other numbers in different bases.
The number the numeral represents is called its value.
Ideally, a numeral system will:
For example, the usual decimal representation of whole numbers gives every non zero whole number a unique representation as a finite sequence of digits, beginning by a non-zero digit. However, when decimal representation is used for the rational or real numbers, such numbers in general have an infinite number of representations, for example 2.31 can also be written as 2.310, 2.3100000, 2.309999999..., etc., all of which have the same meaning except for some scientific and other contexts where greater precision is implied by a larger number of figures shown.
Numeral systems are sometimes called "number systems", but that name is ambiguous, as it could refer to different systems of numbers, such as the system of real numbers, the system of complex numbers, the system of "p"-adic numbers, etc. Such systems are not the topic of this article.
Main numeral systems.
The most commonly used system of numerals is the Hindu–Arabic numeral system. Two Indian mathematicians are credited with developing it. Aryabhata of Kusumapura developed the place-value notation in the 5th century and a century later Brahmagupta introduced the symbol for zero. The numeral system and the zero concept, developed by the Hindus in India, slowly spread to other surrounding countries due to their commercial and military activities with India. The Arabs adopted and modified it. Even today, the Arabs call the numerals they use "Rakam Al-Hind" or the Hindu numeral system. The Arabs translated Hindu texts on numerology and spread them to the western world due to their trade links with them. The Western world modified them and called them the Arabic numerals, as they learned from them. Hence the current western numeral system is the modified version of the Hindu numeral system developed in India. It also exhibits a great similarity to the Sanskrit–Devanagari notation, which is still used in India.
The simplest numeral system is the unary numeral system, in which every natural number is represented by a corresponding number of symbols. If the symbol / is chosen, for example, then the number seven would be represented by ///////. Tally marks represent one such system still in common use. The unary system is only useful for small numbers, although it plays an important role in theoretical computer science. Elias gamma coding, which is commonly used in data compression, expresses arbitrary-sized numbers by using unary to indicate the length of a binary numeral.
The unary notation can be abbreviated by introducing different symbols for certain new values. Very commonly, these values are powers of 10; so for instance, if / stands for one, − for ten and + for 100, then the number 304 can be compactly represented as +++ //// and the number 123 as + − − /// without any need for zero. This is called sign-value notation. The ancient Egyptian numeral system was of this type, and the Roman numeral system was a modification of this idea.
More useful still are systems which employ special abbreviations for repetitions of symbols; for example, using the first nine letters of the alphabet for these abbreviations, with A standing for "one occurrence", B "two occurrences", and so on, one could then write C+ D/ for the number 304. This system is used when writing Chinese numerals and other East Asian numerals based on Chinese. The number system of the English language is of this type ("three hundred [and] four"), as are those of other spoken languages, regardless of what written systems they have adopted. However, many languages use mixtures of bases, and other features, for instance 79 in French is "soixante dix-neuf" (60 + 10 + 9) and in Welsh is "pedwar ar bymtheg a thrigain" (4 + (5 + 10) + (3 × 20)) or (somewhat archaic) "pedwar ugain namyn un" (4 × 20 − 1). In English, one could say "four score less one", as in the famous Gettysburg Address representing "87 years ago" as "four score and seven years ago".
More elegant is a "positional system", also known as place-value notation. Again working in base 10, ten different digits 0, ..., 9 are used and the position of a digit is used to signify the power of ten that the digit is to be multiplied with, as in or more precisely 3×102 + 0×101 + 4×100. Note that zero, which is not needed in the other systems, is of crucial importance here, in order to be able to "skip" a power. The Hindu–Arabic numeral system, which originated in India and is now used throughout the world, is a positional base-10 system.
Arithmetic is much easier in positional systems than in the earlier additive ones; furthermore, additive systems need a large number of different symbols for the different powers of 10; a positional system needs only ten different symbols (assuming that it uses base 10). 
Positional decimal system is presently universally used in human writing. The base 1000 is also used, by grouping the digits and considering a sequence of three decimal digits as a single digit. This is the meaning of the common notation 1,000,234,567 used for very large numbers.
In computers, the main numeral systems are based on the positional system in base 2 (binary numeral system), with two binary digits, 0 and 1. Positional systems obtained by grouping binary digits by three (octal numeral system) or four (hexadecimal numeral system) are commonly used. For very large integers, bases 232 or 264 (grouping binary digits by 32 or 64, the length of the machine word) are used, as, for example, in GMP.
The numerals used when writing numbers with digits or symbols can be divided into two types that might be called the arithmetic numerals 0,1,2,3,4,5,6,7,8,9 and the geometric numerals 1, 10, 100, 1000, 10000 ..., respectively. The sign-value systems use only the geometric numerals and the positional systems use only the arithmetic numerals. A sign-value system does not need arithmetic numerals because they are made by repetition (except for the Ionic system), and a positional system does not need geometric numerals because they are made by position. However, the spoken language uses "both" arithmetic and geometric numerals.
In certain areas of computer science, a modified base-"k" positional system is used, called bijective numeration, with digits 1, 2, ..., "k" ("k" ≥ 1), and zero being represented by an empty string. This establishes a bijection between the set of all such digit-strings and the set of non-negative integers, avoiding the non-uniqueness caused by leading zeros. Bijective base-"k" numeration is also called "k"-adic notation, not to be confused with "p"-adic numbers. Bijective base-1 is the same as unary.
Positional systems in detail.
In a positional base-"b" numeral system (with "b" a natural number greater than 1 known as the radix), "b" basic symbols (or digits) corresponding to the first "b" natural numbers including zero are used. To generate the rest of the numerals, the position of the symbol in the figure is used. The symbol in the last position has its own value, and as it moves to the left its value is multiplied by "b".
For example, in the decimal system (base 10), the numeral 4327 means (4×103) + (3×102) + (2×101) + (7×100), noting that .
In general, if "b" is the base, one writes a number in the numeral system of base "b" by expressing it in the form "a""n""b""n" + "a""n" − 1"b""n" − 1 + "a""n" − 2"b""n" − 2 + ... + "a"0"b"0 and writing the enumerated digits "a"n"a""n" − 1"a""n" − 2 ... "a"0 in descending order. The digits are natural numbers between 0 and "b" − 1, inclusive.
If a text (such as this one) discusses multiple bases, and if ambiguity exists, the base (itself represented in base 10) is added in subscript to the right of the number, like this: numberbase. Unless specified by context, numbers without subscript are considered to be decimal.
By using a dot to divide the digits into two groups, one can also write fractions in the positional system. For example, the base-2 numeral 10.11 denotes 1×21 + 0×20 + 1×2−1 + 1×2−2 = 2.75.
In general, numbers in the base "b" system are of the form:
The numbers "b""k" and "b"−"k" are the weights of the corresponding digits. The position "k" is the logarithm of the corresponding weight "w", that is formula_2. The highest used position is close to the order of magnitude of the number.
The number of tally marks required in the unary numeral system for "describing the weight" would have been w. In the positional system, the number of digits required to describe it is only formula_3formula_4formula_5, for "k" ≥ 0. For example, to describe the weight 1000 then four digits are needed because formula_6. The number of digits required to "describe the position" is formula_7 (in positions 1, 10, 100... only for simplicity in the decimal example).
Note that a number has a terminating or repeating expansion if and only if it is rational; this does not depend on the base. A number that terminates in one base may repeat in another (thus ). An irrational number stays aperiodic (with an infinite number of non-repeating digits) in all integral bases. Thus, for example in base 2, can be written as the aperiodic 11.001001000011111...2.
Putting overscores, "n", or dots, "ṅ", above the common digits is a convention used to represent repeating rational expansions. Thus:
If "b" = "p" is a prime number, one can define base-"p" numerals whose expansion to the left never stops; these are called the "p"-adic numbers.
Generalized variable-length integers.
More general is using a mixed radix notation (here written little-endian) like formula_8 for formula_9, etc.
This is used in punycode, one aspect of which is the representation of a sequence of non-negative integers of arbitrary size in the form of a sequence without delimiters, of "digits" from a collection of 36: a–z and 0–9, representing 0–25 and 26–35 respectively. A digit lower than a threshold value marks that it is the most-significant digit, hence the end of the number. The threshold value depends on the position in the number. For example, if the threshold value for the first digit is b (i.e. 1) then a (i.e. 0) marks the end of the number (it has just one digit), so in numbers of more than one digit the range is only b–9 (1–35), therefore the weight "b"1 is 35 instead of 36. Suppose the threshold values for the second and third digits are c (2), then the third digit has a weight 34 × 35 = 1190 and we have the following sequence:
a (0), ba (1), ca (2), .., 9a (35), bb (36), cb (37), .., 9b (70), bca (71), .., 99a (1260), bcb (1261), etc.
Unlike a regular based numeral system, there are numbers like 9b where 9 and b each represents 35; yet the representation is unique because ac and aca are not allowed – the a would terminate the number.
The flexibility in choosing threshold values allows optimization depending on the frequency of occurrence of numbers of various sizes.
The case with all threshold values equal to 1 corresponds to bijective numeration, where the zeros correspond to separators of numbers with digits which are non-zero.

</doc>
<doc id="21173" url="http://en.wikipedia.org/wiki?curid=21173" title="Natural language">
Natural language

In neuropsychology, linguistics and the philosophy of language, a natural language or ordinary language is any language which arises, unpremeditated, in the brains of human beings. Typically, therefore, these are the languages human beings use to communicate with each other, whether by speech, signing, touch or writing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.
Defining natural language.
Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature (such as bees' waggle dance). Definitions of "natural language" also usually state or imply that a "natural" language is one that any cognitively normal human infant is able to learn and whose development has been through use rather than by prescription. An unstandardized language such as African American Vernacular English, for example, is a natural language, whereas a standardized language such as Standard American English is, in part, prescribed.
Native language learning.
The learning of one's own native language, typically that of one's parents, normally occurs spontaneously in early human childhood and is biologically, socially and ecologically driven. A crucial role of this process is the ability of humans from an early age to engage in speech repetition and so quickly acquire a spoken vocabulary from the pronunciation of words spoken around them. This together with other aspects of speech involves the neural activity of parts of the human brain such as the Wernicke's and Broca's areas.
There are approximately 7,000 current human languages, and many, if not most seem to share certain properties, leading to the hypothesis of universal grammar, as argued by the generative grammar studies of Noam Chomsky and his followers. Recently, it has been demonstrated that a dedicated network in the human brain (crucially involving Broca's area, a portion of the left inferior frontal gyrus), is selectively activated by complex verbal structures (but not simple ones) of those languages that meet the universal grammar requirements.
Although it is clear that there are innate mechanisms that enable the learning of language and define the range of languages that can be learned, it is not clear that these mechanisms in any way resemble a human language or universal grammar. The study of language acquisition is the domain of psycholinguistics and Chomsky always declined to engage in questions of how his putative language organ, the Language Acquisition Device or universal grammar, might have evolved. During a period (the 1970s and 80s) when nativist Transformational Generative Grammar was becoming dominant in Linguistics, and called "Standard Theory", linguists who questioned these tenets were disenfranchised and Cognitive Linguistics and Computational Psycholinguistics were born and the more general term Emergentism developed for the anti-nativist view that language is emergent from more fundamental cognitive processes that are not specifically linguistic in nature.
Origins of natural language.
There is disagreement among anthropologists on when language was first used by humans. Estimates range from about two million (2,000,000) years ago, during the time of "Homo habilis", to as recently as forty thousand (40,000) years ago, during the time of Cro-Magnon man. However recent evidence suggests modern human language was invented or evolved in Africa prior to the dispersal of humans from Africa around 50,000 years ago. Because all people including the most isolated indigenous groups such as the Andamanese or the Tasmanian aboriginals possess language, then it was presumedly present in the ancestral populations in Africa before the human population split into various groups to inhabit the rest of the world.
Controlled languages.
Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.
Constructed languages and international auxiliary languages.
Constructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L.L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally "standardized" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in The Language Instinct), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino Sine Flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.
Modalities.
Natural language manifests itself in modalities other than speech. See also nonverbal communication.
Sign languages.
A sign language is a language which conveys meaning through visual rather than acoustic patterns—simultaneously combining hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts. Sign languages are natural languages which have developed in Deaf communities, which can include interpreters and friends and families of deaf people as well as people who are deaf or hard of hearing themselves.
In contrast, a manually coded language (or signed oral language) is a constructed sign system combining elements of a sign language and an oral language. For example, Signed Exact English (SEE) did not develop naturally in any population, but was "created by a committee of individuals".
Written languages.
In a sense, written language should be distinguished from natural language. Until recently in the developed world, it was common for many people to be fluent in spoken and yet remain illiterate; this is still the case in poor countries today. Furthermore, natural language acquisition during childhood is largely spontaneous, whereas literacy must usually be intentionally acquired.

</doc>
<doc id="21174" url="http://en.wikipedia.org/wiki?curid=21174" title="Nanook of the North">
Nanook of the North

Nanook of the North (also known as Nanook of the North: A Story Of Life and Love In the Actual Arctic) is a 1922 American silent documentary film by Robert J. Flaherty, with elements of docudrama, at a time when separating films into documentary and drama did not yet exist.
In the tradition of what would later be called salvage ethnography, Flaherty captured the struggles of the Inuk man named Nanook and his family in the Canadian Arctic. The film is considered the first feature-length documentary. Some have criticized Flaherty for staging several sequences, but the film is generally viewed as standing "alone in its stark regard for the courage and ingenuity of its heroes."
In 1989, this film was one of the first 25 films to be selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".
Plot.
The documentary follows the lives of an Inuk, Nanook, and his family as they travel, search for food, and trade in northern Quebec, Canada. Nanook, his wife, Nyla, and their baby, Cunayou, are introduced as fearless heroes who endure rigors "no other race" could survive.
Production.
Development.
In 1910 Flaherty was hired as an explorer and prospector along the Hudson Bay for the Canadian Pacific Railway. Learning about the lands and people there, Flaherty decided to bring a camera with him on his third expedition in 1913, but knowing nothing about film, Flaherty took a three-week course on cinematography in Rochester, New York.
Filming.
Using a Bell & Howell camera, a portable developing and printing machine, and some lighting equipment, Flaherty spent 1914 and 1915 shooting hours of film of Eskimo life. By 1916, Flaherty had enough footage that he began test screenings and was met with wide enthusiasm. However, in 1916, Flaherty dropped a cigarette onto the original camera negative (which was highly flammable nitrate stock) and lost 30,000 feet of film. With his first attempt ruined, Flaherty decided to not only return for new footage, but also to refocus the film on one Eskimo family as he felt his earlier footage was too much of travelogue. Spending four years raising money, Flaherty was eventually funded by French fur company Revillon Frères and returned to the North and shot from August 1920 to August 1921. As a main character, Flaherty chose the celebrated hunter of the Itivimuit tribe, Allakariallak. The full collaboration of the Eskimos was key to Flaherty's success as the Eskimos were his film crew and many of them knew his camera better than he did.
Controversies.
Flaherty has been criticized for deceptively portraying staged events as reality. "Nanook" was in fact named Allakariallak (]), while the "wife" shown in the film was not really his wife. According to Charles Nayoumealuk, who was interviewed in "Nanook Revisited" (1988), "the two women in "Nanook" - Nyla (Alice [?] Nuvalinga) and Cunayou (whose real name we do not know) were not Allakariallak's wives, but were in fact common-law wives of Flaherty." And although Allakariallak normally used a gun when hunting, Flaherty encouraged him to hunt after the fashion of his recent ancestors in order to capture the way the Inuit lived before European influence. On the other hand, while Flaherty made his Inuit actors use spears instead of guns during the walrus and seal hunts, the prey shown in the film were genuine, wild animals. Flaherty also exaggerated the peril to Inuit hunters with his claim, often repeated, that Allakariallak had died of starvation two years after the film was completed, whereas in fact he died at home, likely of tuberculosis.
Building of the igloo.
The building of the igloo is one of the most celebrated sequences in the film, but interior photography presented a problem. Building an igloo large enough for a camera to enter resulted in the dome collapsing, and when they finally succeeded in making the igloo it was too dark for photography. Instead, the images of the inside of the igloo in the film were actually shot in a special three-walled igloo for Flaherty's bulky camera so that there would be enough light for it to capture interior shots.
Visit to the trade post of the white man.
Another scene that is much discussed is the visit to the "Trade Post of the White Man." In this scene, Nanook and his family arrive in a kayak at the trading post and one family member after another emerge from a small kayak, akin to a clown car at the circus. Going to trade his hunt from the year, including foxes, seals and polar bears, Nanook comes in contact with the white man and there is a funny interaction as the two cultures meet. The trader plays music on a gramophone and tries to explain how a man 'cans' his voice. Bending forward and staring at the machine, Nanook puts his ear closer as the trader cranks the mechanism again. The trader removes the record and hands it to Nanook who at first peers at it and then puts it in his mouth and bites it. The scene is meant to be a comical one as the audience laughs at the naivete of Nanook and people isolated from Western culture. In truth, the scene was entirely scripted and Allakariallak knew what a gramophone was.
Hunting of the walrus.
It has been pointed out that in the 1920s when Nanook was filmed, the Inuit had already begun integrating the use of Western clothing and were utilizing rifles to hunt rather than the harpoon; but this does not negate that the Inuit knew how to make traditional clothing from animals found in their environment, and could still fashion traditional weapons; and were perfectly free to utilize them if found to be preferable for a given situation.
The film is not technically sophisticated; how could it be, with one camera, no lights, freezing cold, and everyone equally at the mercy of nature? But it has an authenticity that prevails over any complaints that some of the sequences were staged. If you stage a walrus hunt, it still involves hunting a walrus, and the walrus hasn't seen the script. What shines through is the humanity and optimism of the Inuit.—Roger Ebert
Flaherty defended his work by stating that a filmmaker must often distort a thing to catch its true spirit. Later filmmakers have pointed out that the only cameras available to Flaherty at the time were both large and immobile, making it impossible to effectively capture most interior shots or unstructured exterior scenes without significantly modifying the environment and subject action.
Reception.
As the first nonfiction work of its scale, "Nanook of the North" was ground-breaking cinema. It captured many authentic details of a culture little-known to outsiders, and was filmed in a remote location. Hailed almost unanimously by critics, the film was a box office success in the United States and abroad. In the following years, many others would try to follow in Flaherty's success with "primitive peoples" films. In 2005 film critic Roger Ebert described the film's central figure, Nanook, as "one of the most vital and unforgettable human beings ever recorded on film." In a 2014 "Sight and Sound" poll, film critics voted "Nanook of the North" the seventh best documentary film of all time.
Legacy.
At the time, few documentaries had been filmed and there was little precedent to guide Flaherty's work. Since Flaherty's time both staging action and attempting to steer documentary action have come to be considered unethical amongst cinéma vérité purists, because they believe such reenactments deceive the audience.
In its earliest years (approx. 1895–1902), film production was dominated by actualities, short pictures of real people in real places. Robert Flaherty’s great innovation was simply to combine the two forms of actuality, infusing the exotic journey with the details of indigenous work and play and life.
Home media.
In 1999, "Nanook of the North" was digitally remastered and released on DVD by The Criterion Collection. It includes an interview with Flaherty's widow (and "Nanook of the North" co-editor), Frances Flaherty, photos from Flaherty's trip to the arctic, and excerpts from a TV documentary, "Flaherty and Film." In 2013, Flicker Alley released a remastered Blu-ray version that includes six other arctic films.
References.
</dl>

</doc>
<doc id="21175" url="http://en.wikipedia.org/wiki?curid=21175" title="Nitrogen">
Nitrogen

Nitrogen is a chemical element with symbol N and atomic number 7. It is the lightest pnictogen and at room temperature, it is a transparent, odorless diatomic gas. Nitrogen is a common element in the universe, estimated at about seventh in total abundance in the Milky Way and the Solar System. On Earth, the element forms about 78% of Earth's atmosphere and as such is the most abundant uncombined element. The element nitrogen was discovered as a separable component of air, by Scottish physician Daniel Rutherford, in 1772.
Many industrially important compounds, such as ammonia, nitric acid, organic nitrates (propellants and explosives), and cyanides, contain nitrogen. The extremely strong triple bond in elemental nitrogen (N≡N) dominates nitrogen chemistry, causing difficulty for both organisms and industry in converting the N2 into useful compounds, but at the same time causing release of large amounts of often useful energy when the compounds burn, explode, or decay back into nitrogen gas. Synthetically-produced ammonia and nitrates are key industrial fertilizers and fertilizer nitrates are key pollutants in causing the eutrophication of water systems.
Outside the major uses of nitrogen compounds as fertilizers and energy-stores, nitrogen is a constituent of organic compounds as diverse as Kevlar fabric and cyanoacrylate "super" glue. Nitrogen is a constituent of molecules in every major pharmacological drug class, including antibiotics. Many drugs are mimics or prodrugs of natural nitrogen-containing signal molecules: for example, the organic nitrates nitroglycerin and nitroprusside control blood pressure by being metabolized to nitric oxide. Plant alkaloids (often defense chemicals) contain nitrogen by definition, and thus many notable nitrogen-containing drugs, such as caffeine and morphine are either alkaloids or synthetic mimics that act (as many plant alkaloids do) on receptors of animal neurotransmitters (for example, synthetic amphetamines).
Nitrogen occurs in all organisms, primarily in amino acids (and thus proteins) and also in the nucleic acids (DNA and RNA). The human body contains about 3% by mass of nitrogen, the fourth most abundant element in the body after oxygen, carbon, and hydrogen. The nitrogen cycle describes movement of the element from the air, into the biosphere and organic compounds, then back into the atmosphere.
History and etymology.
Nitrogen is formally considered to have been discovered by Scottish physician Daniel Rutherford in 1772, who called it "noxious air". Though he did not recognize it as an entirely different chemical substance, he clearly distinguished it from "fixed air". The fact that there was a component of air that does not support combustion was clear to Rutherford. Nitrogen was also studied at about the same time by Carl Wilhelm Scheele, Henry Cavendish, and Joseph Priestley, who referred to it as "burnt air" or "phlogisticated air". Nitrogen gas was inert enough that Antoine Lavoisier referred to it as "mephitic air" or "azote", from the Greek word ἄζωτος "azotos", "lifeless". In it, animals died and flames were extinguished. This "mephitic air" consisted mostly of N2, but might also have included more than 1% argon.
Lavoisier's name for nitrogen is used in many languages (French, Italian, Polish, Russian, Albanian, Turkish, etc.) and still remains in English in the common names of many compounds, such as hydrazine and compounds of the azide ion. The English word nitrogen (1794) entered the language from the French "nitrogène", coined in 1790 by French chemist Jean-Antoine Chaptal (1756–1832), from the Greek νίτρον "nitron", "sodium carbonate" and the French "-gène", "producing" from Greek -γενής "-genes", "producer, begetter". The gas had been found in nitric acid. Chaptal's meaning was that nitrogen gas is the essential part of nitric acid, in turn formed from saltpeter (potassium nitrate), then known as niter.
Nitrogen compounds were well known by the Middle Ages. Alchemists knew nitric acid as "aqua fortis" (strong water). The mixture of nitric and hydrochloric acids was known as "aqua regia" (royal water), celebrated for its ability to dissolve gold (the "king" of metals). The earliest military, industrial, and agricultural applications of nitrogen compounds used saltpeter (sodium nitrate or potassium nitrate), most notably in gunpowder, and later as fertilizer. In 1910, Lord Rayleigh discovered that an electrical discharge in nitrogen gas produced "active nitrogen", a monatomic allotrope of nitrogen. The "whirling cloud of brilliant yellow light" produced by his apparatus reacted with quicksilver to produce explosive mercury nitride.
For a long time sources of nitrogen compounds were limited. Natural sources originated either from biology or deposits of nitrates produced by atmospheric reactions. Nitrogen fixation by industrial processes like the Frank–Caro process (1895–1899) and Haber–Bosch process (1908–1913) eased this shortage of nitrogen compounds, to the extent that half of global food production (see applications) now relies on synthetic nitrogen fertilizers. At the same time, use of the Ostwald process (1902) to produce nitrates from industrial nitrogen fixation allowed the large-scale industrial production of nitrates which fueled explosives in the World Wars of the 20th century.
Production.
Nitrogen gas is an industrial gas produced by the fractional distillation of liquid air, or by mechanical means using gaseous air (i.e., pressurized reverse osmosis membrane or pressure swing adsorption). Commercial nitrogen is often a byproduct of air-processing for industrial concentration of oxygen for steelmaking and other purposes. When supplied compressed in cylinders it is often called OFN (oxygen-free nitrogen).
In a chemical laboratory it is prepared by treating an aqueous solution of ammonium chloride with sodium nitrite.
Small amounts of impurities NO and HNO3 are also formed in this reaction. The impurities can be removed by passing the gas through aqueous sulfuric acid containing potassium dichromate. Very pure nitrogen can be prepared by the thermal decomposition of barium azide or sodium azide.
Properties.
Nitrogen is a nonmetal, with an electronegativity of 3.04. It has five electrons in its outer shell and is, therefore, trivalent in most compounds. The triple bond in molecular nitrogen (N2) is one of the strongest. The resulting difficulty of converting N2 into other compounds, and the ease (and associated high energy release) of converting nitrogen compounds into elemental N2, have dominated the role of nitrogen in both nature and human economic activities.
At atmospheric pressure, molecular nitrogen condenses (liquefies) at 77 K (−195.79 °C) and freezes at 63 K (−210.01 °C) into the beta hexagonal close-packed crystal allotropic form. Below 35.4 K (−237.6 °C) nitrogen assumes the cubic crystal allotropic form (called the alpha phase). Liquid nitrogen, a fluid resembling water in appearance, but with 80.8% of the density (the density of liquid nitrogen at its boiling point is 0.808 g/mL), is a common cryogen.
Unstable allotropes of nitrogen consisting of more than two nitrogen atoms have been produced in the laboratory, like N3 and N4. Under extremely high pressures (1.1 million atm) and high temperatures (2000 K), as produced using a diamond anvil cell, nitrogen polymerizes into the single-bonded cubic gauche crystal structure. This structure is similar to that of diamond, and both have extremely strong covalent bonds. N4 is nicknamed "nitrogen diamond".
Other (as yet unsynthesized) allotropes include hexazine (N6, a benzene analog) and octaazacubane (N8, a cubane analog). The former is predicted to be highly unstable, while the latter is predicted to be kinetically stable, for reasons of orbital symmetry.
Isotopes.
There are two stable isotopes of nitrogen: 14N and 15N. By far the most common is 14N (99.634%), which is produced in the CNO cycle in stars. Of the ten isotopes produced synthetically, 13N has a half-life of ten minutes and the remaining isotopes have half-lives on the order of seconds or less.
Biologically mediated reactions (e.g., assimilation, nitrification, and denitrification) strongly control nitrogen dynamics in the soil. These reactions typically result in 15N enrichment of the substrate and depletion of the product.
A small part (0.73%) of the molecular nitrogen in Earth's atmosphere is the isotopologue 14N15N, and almost all the rest is 14N2.
The radioisotope 16N is the dominant radionuclide in the coolant of pressurized water reactors or boiling water reactors during normal operation. It is produced from 16O (in water) via (n,p) reaction. It has a short half-life of about 7.1 s, but during its decay back to 16O produces high-energy gamma radiation (5 to 7 MeV).
Because of this, the access to the primary coolant piping in a pressurized water reactor must be restricted during reactor power operation. 16N is one of the main means used to immediately detect even small leaks from the primary coolant to the secondary steam cycle.
In similar fashion, access to any of the steam cycle components in a boiling water reactor nuclear power plant must be restricted during operation. Condensate from the condenser is typically retained for 10 minutes to allow for decay of the 16N. This eliminates the need to shield and restrict access to any of the feed water piping or pumps.
Electromagnetic spectrum.
Molecular nitrogen (14N2) is largely transparent to infrared and visible radiation because it is a homonuclear molecule and, thus, has no dipole moment to couple to electromagnetic radiation at these wavelengths. Significant absorption occurs at extreme ultraviolet wavelengths, beginning around 100 nanometers. This is associated with electronic transitions in the molecule to states in which charge is not distributed evenly between nitrogen atoms. Nitrogen absorption leads to significant absorption of ultraviolet radiation in the Earth's upper atmosphere and the atmospheres of other planetary bodies. For similar reasons, pure molecular nitrogen lasers typically emit light in the ultraviolet range.
Nitrogen also makes a contribution to visible air glow from the Earth's upper atmosphere, through electron impact excitation followed by emission. This visible blue air glow (seen in the polar aurora and in the re-entry glow of returning spacecraft) typically results not from molecular nitrogen but rather from free nitrogen atoms combining with oxygen to form nitric oxide (NO).
Nitrogen gas also exhibits scintillation.
Reactions.
In general, nitrogen is unreactive at standard temperature and pressure. N2 reacts spontaneously with few reagents, being resilient to acids and bases as well as oxidants and most reductants. When nitrogen reacts spontaneously with a reagent, the net transformation is often called nitrogen fixation.
Nitrogen reacts with elemental lithium. Lithium burns in an atmosphere of N2 to give lithium nitride:
Magnesium also burns in nitrogen, forming magnesium nitride.
N2 forms a variety of adducts with transition metals. The first example of a dinitrogen complex is [Ru(NH3)5(N2)]2+ (see figure at right). However, it is interesting to note that the N2 ligand was obtained by the decomposition of hydrazine, and not coordination of free dinitrogen. Such compounds are now numerous, other examples include IrCl(N2)(PPh3)2, W(N2)2(Ph2PCH2CH2PPh2)2, and [(η5-C5Me4H)2Zr]2(μ2, η2,η2-N2). These complexes illustrate how N2 might bind to the metal(s) in nitrogenase and the catalyst for the Haber process. A catalytic process to reduce N2 to ammonia with the use of a molybdenum complex in the presence of a proton source was published in 2005.
The beginning point for industrial production of nitrogen compounds is the Haber process, in which nitrogen is fixed by reacting N2 and H2 over an iron(II,
III) oxide (Fe3O4) catalyst at about 500 °C and 200 atmospheres pressure. Biological nitrogen fixation in free-living cyanobacteria and in the root nodules of plants also produces ammonia from molecular nitrogen. The reaction, which is the source of the bulk of nitrogen in the biosphere, is catalyzed by the nitrogenase enzyme complex that contains Fe and Mo atoms, using energy derived from hydrolysis of adenosine triphosphate (ATP) into adenosine diphosphate and inorganic phosphate (−20.5 kJ/mol).
Occurrence.
See also: the categories and .
Nitrogen gas (N2) is the largest constituent of the Earth's atmosphere (78.082% by volume of dry air, 75.3% by weight in dry air). However, this high concentration does not reflect nitrogen's overall low abundance in the makeup of the Earth, from which most of the element escaped by solar evaporation, early in the planet's formation.
Nitrogen is a common element in the universe, and is estimated to be approximately the seventh most abundant chemical element by mass in the universe, the Milky Way, and the Solar System. In these places it was originally created by fusion processes from carbon and hydrogen in supernovas. Molecular nitrogen and nitrogen compounds have been detected in interstellar space by astronomers using the Far Ultraviolet Spectroscopic Explorer.
Due to the volatility of elemental nitrogen and also its common compounds with hydrogen and oxygen, nitrogen and its compounds were driven out of the planetesimals in the early Solar System by the heat of the Sun, and in the form of gases, were lost to the rocky planets of the inner Solar System. Nitrogen is therefore a relatively rare element on these inner planets, including Earth, as a whole. In this, nitrogen resembles neon, which has a similar high abundance in the universe, but is also rare in the inner Solar System. Nitrogen is estimated at 30th of the elements in crustal abundance. There exist some relatively uncommon nitrogen minerals, such as saltpeter (potassium nitrate), Chile saltpeter (sodium nitrate) and sal ammoniac (ammonium chloride). Even these are known mainly as concentrated from evaporative ocean beds, due to their ready solubility of most naturally-occurring nitrogen compounds in water. A similar pattern occurs with the water solubility of the uncommon light element boron.
However, nitrogen and its compounds occur far more commonly as gases in the atmospheres of planets and moons that are large enough to have atmospheres. For example, molecular nitrogen is a major constituent of not only Earth's atmosphere, but also the Saturnian moon Titan's thick atmosphere. Also, due to retention by gravity at colder temperatures, nitrogen and its compounds occur in trace to appreciable amounts in planetary atmospheres of the gas giant planets.
Nitrogen is present in all known living organisms, in proteins, nucleic acids, and other molecules. It typically makes up around 4% of the dry weight of plant matter, and around 3% of the weight of the human body. It is a large component of animal waste (for example, guano), usually in the form of urea, uric acid, ammonium compounds, and derivatives of these nitrogenous products, which are essential nutrients for all plants that cannot fix atmospheric nitrogen.
Compounds.
See also: .
The main neutral hydride of nitrogen is ammonia (NH3), although hydrazine (N2H4) is also commonly used. Ammonia is more basic than water by 6 orders of magnitude. In solution ammonia forms the ammonium ion (NH4+). Liquid ammonia (boiling point 240 K) is amphiprotic (displaying either Brønsted–Lowry acidic or basic character) and forms ammonium and the less common amide ions (NH2-); both amides and nitride (N3-) salts are known, but decompose in water. Singly, doubly, triply and quadruply substituted alkyl compounds of ammonia are called amines (four substitutions, to form commercially and biologically important quaternary amines, results in a positively charged nitrogen, and thus a water-soluble, or at least amphiphilic, compound). Larger chains, rings and structures of nitrogen hydrides are also known, but are generally unstable.
Other classes of nitrogen anions (negatively charged ions) are the poisonous azides (N3-), which are linear and isoelectronic to carbon dioxide, but which bind to important iron-containing enzymes in the body in a manner more resembling cyanide. Another molecule of the same structure is the colorless and relatively inert anesthetic gas Nitrous oxide (dinitrogen monoxide, N2O), also known as laughing gas. This is one of a variety of nitrogen oxides that form a family often abbreviated as NOx. Nitric oxide (nitrogen monoxide, NO), is a natural free radical used in signal transduction in both plants and animals, for example, in vasodilation by causing the smooth muscle of blood vessels to relax. The reddish and poisonous nitrogen dioxide NO2 contains an unpaired electron and is an important component of smog. Nitrogen molecules containing unpaired electrons show a tendency to dimerize (thus pairing the electrons), and are, in general, highly reactive. The corresponding acids are nitrous HNO2 and nitric acid HNO3, with the corresponding salts called nitrites and nitrates.
The higher oxides dinitrogen trioxide N2O3, dinitrogen tetroxide N2O4 and dinitrogen pentoxide N2O5, are unstable and explosive, a consequence of the chemical stability of N2. Nearly every hypergolic rocket engine uses N2O4 as the oxidizer; their fuels, various forms of hydrazine, are also nitrogen compounds. These engines are extensively used on spacecraft such as the space shuttle and those of the Apollo Program because their propellants are liquids at room temperature and ignition occurs on contact without an ignition system, allowing many precisely controlled burns. Some launch vehicles such as the Titan II and Ariane 1 through 4 also use hypergolic fuels, although the trend is away from such engines for cost and safety reasons. N2O4 is an intermediate in the manufacture of nitric acid HNO3, a strong acid and a fairly strong oxidizing agent.
Nitrogen is notable for the range of explosively unstable compounds that it can produce. Nitrogen triiodide NI3 is an extremely sensitive contact explosive. Nitrocellulose, produced by nitration of cellulose with nitric acid, is also known as guncotton. Nitroglycerin, made by nitration of glycerin, is the dangerously unstable explosive ingredient of dynamite. The comparatively stable, but less powerful explosive trinitrotoluene (TNT) is the standard explosive against which the power of nuclear explosions are measured.
Nitrogen can also be found in many organic compounds. Common nitrogen functional groups include: amines, amides, nitro groups, imines, and enamines. The amount of nitrogen in a chemical substance can be determined by the Kjeldahl method.
Applications.
Nitrogen gas.
Nitrogen gas has a variety of applications, including serving as an inert replacement for air where oxidation is undesirable;
Nitrogen is commonly used during sample preparation procedures for chemical analysis. It is used to concentrate and reduce the volume of liquid samples. Directing a pressurized stream of nitrogen gas perpendicular to the surface of the liquid allows the solvent to evaporate while leaving the solute(s) and un-evaporated solvent behind.
Nitrogen can be used as a replacement, or in combination with, carbon dioxide to pressurize kegs of some beers, particularly stouts and British ales, due to the smaller bubbles it produces, which makes the dispensed beer smoother and headier. A pressure-sensitive nitrogen capsule known commonly as a "widget" allows nitrogen-charged beers to be packaged in cans and bottles.
Nitrogen tanks are also replacing carbon dioxide as the main power source for paintball guns. Nitrogen must be kept at higher pressure than CO2, making N2 tanks heavier and more expensive.
Nitrogen gas has become the inert gas of choice for inert gas asphyxiation, and is under consideration as a replacement for lethal injection in Oklahoma. Nitrogen is promoted by euthanasia advocate Philip Nitschke to end life in a "peaceful, reliable [and] totally legal" manner.
Liquid nitrogen.
Liquid nitrogen is a cryogenic liquid. At atmospheric pressure, it boils at −195.8 °C. When insulated in proper containers such as Dewar flasks, it can be transported without much evaporative loss.
Like dry ice, the main use of liquid nitrogen is as a refrigerant. Among other things, it is used in the cryopreservation of blood, reproductive cells (sperm and egg), and other biological samples and materials. It is used in the clinical setting in cryotherapy to remove cysts and warts on the skin. It is used in cold traps for certain laboratory equipment and to cool infrared detectors or X-ray detectors. It has also been used to cool central processing units and other devices in computers that are overclocked, and that produce more heat than during normal operation.
Nitrogen compounds.
Molecular nitrogen (N2) in the atmosphere is relatively non-reactive due to its strong triple bond, N≡N, and molecular nitrogen plays an inert role in the human body, being neither produced nor destroyed. In nature, nitrogen is converted into biologically (and industrially) useful compounds by lightning, and by some living organisms, notably certain bacteria (i.e., nitrogen-fixing bacteria—see "Biological role" below). Molecular nitrogen is released into the atmosphere in the process of decay, in dead plant and animal tissues.
The ability to combine, or fix, molecular nitrogen is a key feature of modern industrial chemistry. Previously to the 20th century, access to nitrogen compounds for fertilizers and gunpowder had been through deposits of natural nitrates, such as Chilean saltpeter. However, first the Frank–Caro process for producing cyanamide, and then the Haber–Bosch process for producing ammonia from air and natural gas (developed just before the first world war) eased this shortage of nitrogen compounds, to the extent that half of global food production now relies on synthetic nitrogen fertilizers.
The Ostwald process, developed a few years before the Haber process, allowed large-scale production of nitric acid and nitrate from ammonia, thus freeing large-scale industrial production of nitrate explosives and weapons propellants from the need to mine nitrate salt deposits. The organic and inorganic salts of nitric acid have been important historically as convenient stores of chemical energy for warfare and rocket fuels. Historically, such compounds included important compounds such as potassium nitrate, used in gunpowder which was often produced by biological means (bacterial fermentation) before natural mineral sources were discovered. Later, all such sources were displaced by industrial production, in the early 1900s.
Ammonium nitrate has been used as both fertilizer and explosive (see ANFO). Various other nitrated organic compounds, such as nitroglycerin, trinitrotoluene, and nitrocellulose, are used as explosives and propellants for modern firearms. Nitric acid is used as an oxidizing agent in liquid fueled rockets. Hydrazine and hydrazine derivatives find use as rocket fuels and monopropellants. In most of these compounds, the basic instability and tendency to burn or explode is derived from the fact that nitrogen is present as an oxide, and not as the far more stable nitrogen molecule (N2), which is a product of the compounds' thermal decomposition. When nitrates burn or explode, the formation of the powerful triple bond in the N2 produces most of the energy of the reaction.
Nitrogen is a constituent of molecules in every major drug class in pharmacology and medicine. Nitrous oxide (N2O) was discovered early in the 19th century to be a partial anesthetic, though it was not used as a surgical anesthetic until later. Called "laughing gas", it was found capable of inducing a state of social disinhibition resembling drunkenness. Other notable nitrogen-containing drugs are drugs derived from plant alkaloids, such as morphine (there exist many alkaloids known to have pharmacological effects; in some cases, they appear as natural chemical defenses of plants against predation). Drugs that contain nitrogen include all major classes of antibiotics and organic nitrate drugs like nitroglycerin and nitroprusside that regulate blood pressure and heart action by mimicking the action of nitric oxide.
Biological role.
Nitrogen is an essential building block of amino and nucleic acids, essential to life on Earth.
Elemental nitrogen in the atmosphere cannot be used directly by either plants or animals, and must be converted to a reduced (or 'fixed') state to be useful for higher plants and animals. Precipitation often contains substantial quantities of ammonium and nitrate, thought to result from nitrogen fixation by lightning and other atmospheric electric phenomena. This was first proposed by Liebig in 1827 and later confirmed. However, because ammonium is preferentially retained by the forest canopy relative to atmospheric nitrate, most fixed nitrogen reaches the soil surface under trees as nitrate. Soil nitrate is preferentially assimilated by tree roots relative to soil ammonium.
Specific bacteria (e.g., "Rhizobium trifolium") possess nitrogenase enzymes that can fix atmospheric nitrogen (see nitrogen fixation) into a form (ammonium ion) that is chemically useful to higher organisms. This process requires a large amount of energy and anoxic conditions. Such bacteria may live freely in soil (e.g., "Azotobacter") but normally exist in a symbiotic relationship in the root nodules of leguminous plants (e.g. clover, "Trifolium", or soybean plant, "Glycine max") and fertilizer trees. Nitrogen-fixing bacteria are also symbiotic with a number of unrelated plant species such as alders ("Alnus") spp., lichens, "Casuarina", "Myrica", liverworts, and "Gunnera".
As part of the symbiotic relationship, the plant converts the 'fixed' ammonium ion to nitrogen oxides and amino acids to form proteins and other molecules, (e.g., alkaloids). In return for the 'fixed' nitrogen, the plant secretes sugars to the symbiotic bacteria. Legumes maintain an anaerobic (oxygen free) environment for their nitrogen-fixing bacteria.
Plants are able to assimilate nitrogen directly in the form of nitrates that may be present in soil from natural mineral deposits, artificial fertilizers, animal waste, or organic decay (as the product of bacteria, but not bacteria specifically associated with the plant). Nitrates absorbed in this fashion are converted to nitrites by the enzyme "nitrate" reductase, and then converted to ammonia by another enzyme called "nitrite" reductase.
Nitrogen compounds are basic building blocks in animal biology as well. Animals use nitrogen-containing amino acids from plant sources as starting materials for all nitrogen-compound animal biochemistry, including the manufacture of proteins and nucleic acids. Plant-feeding insects are dependent on nitrogen in their diet, such that varying the amount of nitrogen fertilizer applied to a plant can affect the reproduction rate of insects feeding on fertilized plants.
Soluble nitrate is an important limiting factor in the growth of certain bacteria in ocean waters. In many places in the world, artificial fertilizers applied to crop-lands to increase yields result in run-off delivery of soluble nitrogen to oceans at river mouths. This process can result in eutrophication of the water, as nitrogen-driven bacterial growth depletes water oxygen to the point that all higher organisms die. Well-known "dead zone" areas in the U.S. Gulf Coast and the Black Sea are due to this important polluting process.
Many saltwater fish manufacture large amounts of trimethylamine oxide to protect them from the high osmotic effects of their environment; conversion of this compound to dimethylamine is responsible for the early odor in unfresh saltwater fish. In animals, free radical nitric oxide (NO) (derived from an amino acid), serves as an important regulatory molecule for circulation.
Nitric oxide's rapid reaction with water in animals results in production of its metabolite nitrite. Animal metabolism of nitrogen in proteins, in general, results in excretion of urea, while animal metabolism of nucleic acids results in excretion of urea and uric acid. The characteristic odor of animal flesh decay is caused by the creation of long-chain, nitrogen-containing amines, such as putrescine and cadaverine, which are breakdown products of the amino acids ornithine and lysine, respectively, in decaying proteins.
Decay of organisms and their waste products may produce small amounts of nitrate, but most decay eventually returns nitrogen content to the atmosphere, as molecular nitrogen. The circulation of nitrogen from atmosphere, to organic compounds, then back to the atmosphere, is referred to as the nitrogen cycle.
Safety.
Rapid release of nitrogen gas into an enclosed space can displace oxygen, and therefore presents an asphyxiation hazard. This may happen with few warning symptoms, since the human carotid body is a relatively poor and slow low-oxygen (hypoxia) sensing system. An example occurred shortly before the launch of the first Space Shuttle mission in 1981, when two technicians lost consciousness (and one of them died) after they walked into a space located in the Shuttle's Mobile Launcher Platform that was pressurized with pure nitrogen as a precaution against fire. The technicians would have been able to exit the room if they had experienced any early symptoms from nitrogen-breathing.
When inhaled at high partial pressures (more than about 4 bar, encountered at depths below about 30 m in scuba diving), nitrogen begins to act as an anesthetic agent. It can cause nitrogen narcosis, a temporary semi-anesthetized state of mental impairment similar to that caused by nitrous oxide.
Nitrogen also dissolves in the bloodstream and body fats. Rapid decompression (in particular, in the case of divers ascending too quickly, or astronauts decompressing too quickly from cabin pressure to spacesuit pressure) can lead to a potentially fatal condition called decompression sickness (formerly known as caisson sickness or "the bends"), when nitrogen bubbles form in the bloodstream, nerves, joints, and other sensitive or vital areas. Bubbles from other "inert" gases (those gases other than carbon dioxide and oxygen) cause the same effects, so replacement of nitrogen in breathing gases may prevent nitrogen narcosis, but does not prevent decompression sickness.
Direct skin contact with liquid nitrogen will cause severe frostbite (cryogenic "burns"). This may happen almost instantly on contact, or after a second or more, depending on the form of liquid nitrogen. Bulk liquid nitrogen causes less rapid freezing than a spray of nitrogen mist (such as is used to freeze certain skin growths in the practice of dermatology). The extra surface area provided by nitrogen-soaked materials is also important, with soaked clothing or cotton causing far more rapid damage than a spill of direct liquid to skin. Full "contact" between naked skin and large collected-droplets or pools of liquid nitrogen may be prevented for a second or two, by a layer of insulating gas from the Leidenfrost effect. This may give the skin a second of protection from nitrogen bulk liquid. However, liquid nitrogen applied to skin in mists, and on fabrics, bypasses this effect, and causes local frostbite immediately.
Oxygen sensors are sometimes used as a safety precaution when working with liquid nitrogen to alert workers of gas spills into a confined space.

</doc>
<doc id="21176" url="http://en.wikipedia.org/wiki?curid=21176" title="Nominalism">
Nominalism

Nominalism is a metaphysical view in philosophy according to which general or abstract terms and predicates exist, while universals or abstract objects, which are sometimes thought to correspond to these terms, do not exist. There are at least two main versions of nominalism. One version denies the existence of universals – things that can be instantiated or exemplified by many particular things (e.g., strength, humanity). The other version specifically denies the existence of abstract objects – objects that do not exist in space and time.
Most nominalists have held that only physical particulars in space and time are real, and that universals exist only "post res", that is, subsequent to particular things. However, some versions of nominalism hold that some particulars are abstract entities (e.g., numbers), while others are concrete entities – entities that do exist in space and time (e.g., thrones, couches, bananas).
Nominalism is primarily a position on the problem of universals, which dates back at least to Plato, and is opposed to realism – the view that universals do exist over and above particulars. However, the name "nominalism" emerged from debates in medieval philosophy with Roscellinus.
The term 'nominalism' stems from the Latin "nomen", "name." For example, John Stuart Mill once wrote, that "there is nothing general except names". In the philosophy of law nominalism finds its application in what is called constitutional nominalism.
The problem of universals.
Nominalism arose in reaction to the problem of universals, specifically accounting for the fact that some things are of the same type. For example, Fluffy and Kitzler are both cats, or, the fact that certain properties are repeatable, such as: the grass, the shirt, and Kermit the Frog are green. One wants to know in virtue of "what" are Fluffy and Kitzler both cats, and "what" makes the grass, the shirt, and Kermit green.
The realist answer is that all the green things are green in virtue of the existence of a universal; a single abstract thing that, in this case, is a part of all the green things. With respect to the color of the grass, the shirt and Kermit, one of their parts is identical. In this respect, the three parts are literally one. Greenness is repeatable because there is one thing that manifests itself wherever there are green things.
Nominalism denies the existence of universals. The motivation for this flows from several concerns, the first one being where they might exist. Plato famously held, on one interpretation, that there is a realm of abstract forms or universals apart from the physical world (see theory of the forms). Particular physical objects merely exemplify or instantiate the universal. But this raises the question: Where is this universal realm? One possibility is that it is outside space and time. A view sympathetic with this possibility holds that, precisely because some form is immanent in several physical objects, it must also transcend each of those physical objects; in this way, the forms are "transcendent" only insofar as they are "immanent" in many physical objects. In other words, immanence implies transcendence; they are not opposed to one another. (Nor, on this view, would there be a separate "world" or "realm" of forms that is distinct from the physical world, thus shirking much of the worry about where to locate a "universal realm".) However, naturalists assert that nothing is outside of space and time. Some Neoplatonists, such as the pagan philosopher Plotinus and the Christian philosopher Augustine, imply (anticipating conceptualism) that universals are contained within the "mind" of God. To complicate things, what is the nature of the instantiation or exemplification relation?
Conceptualists hold a position intermediate between nominalism and realism, saying that universals exist only within the mind and have no external or substantial reality.
Moderate realists hold that there is no realm in which universals exist, but rather universals are located in space and time wherever they are manifest. Now, recall that a universal, like greenness, is supposed to be a single thing. Nominalists consider it unusual that there could be a single thing that exists in multiple places simultaneously. The realist maintains that all the instances of greenness are held together by the exemplification relation, but this relation cannot be explained.
Finally, many philosophers prefer simpler ontologies populated with only the bare minimum of types of entities, or as W. V. Quine said "They have a taste for 'desert landscapes.'" They try to express everything that they want to explain without using universals such as "catness" or "chairness."
Varieties of nominalism.
There are various forms of nominalism ranging from extreme to almost-realist. One extreme is "predicate nominalism", which states that Fluffy and Kitzler, for example, are both cats simply because the predicate 'is a cat' applies to both of them. And this is the case for all similarity of attribute among objects. The main criticism of this view is that it does not provide a sufficient solution to the problem of universals. It fails to provide an account of what makes it the case that a group of things warrant having the same predicate applied to them.
Resemblance nominalists believe that 'cat' applies to both cats because Fluffy and Kitzler resemble an exemplar cat closely enough to be classed together with it as members of its kind, or that they differ from each other (and other cats) quite less than they differ from other things, and this warrants classing them together. Some resemblance nominalists will concede that the resemblance relation is itself a universal, but is the only universal necessary. Others argue that each resemblance relation is a particular, and is a resemblance relation simply in virtue of its resemblance to other resemblance relations. This generates an infinite regress, but many argue that it is not vicious.
Class nominalism argues that class membership forms the metaphysical backing for property relationships: two particular red balls share a property in that they are both members of classes corresponding to their properties—that of being red and being balls. A version of class nominalism that sees some classes as "natural classes" is held by Anthony Quinton.
Conceptualism is a philosophical theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. The conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside of the mind's perception of them.
Another form of nominalism is "trope theory". A trope is a particular instance of a property, like the specific greenness of a shirt. One might argue that there is a primitive, objective resemblance relation that holds among like tropes. Another route is to argue that all apparent tropes are constructed out of more primitive tropes and that the most primitive tropes are the entities of complete physics. Primitive trope resemblance may thus be accounted for in terms of causal indiscernibility. Two tropes are exactly resembling if substituting one for the other would make no difference to the events in which they are taking part. Varying degrees of resemblance at the macro level can be explained by varying degrees of resemblance at the micro level, and micro-level resemblance is explained in terms of something no less robustly physical than causal power. David Armstrong, perhaps the most prominent contemporary realist, argues that such a trope-based variant of nominalism has promise, but holds that it is unable to account for the laws of nature in the way his theory of universals can.
Ian Hacking has also argued that much of what is called social constructionism of science in contemporary times is actually motivated by an unstated nominalist metaphysical view. For this reason, he claims, scientists and constructionists tend to "shout past each other".
Analytic philosophy and mathematics.
A notion that philosophy, especially ontology and the philosophy of mathematics should abstain from set theory owes much to the writings of Nelson Goodman (see especially Goodman 1977), who argued that concrete and abstract entities having no parts, called "individuals" exist. Collections of individuals likewise exist, but two collections having the same individuals are the same collection.
The principle of extensionality in set theory assures us that any matching pair of curly braces enclosing one or more instances of the same individuals denote the same set. Hence {"a", "b"}, {"b", "a"}, {"a", "b", "a", "b"} are all the same set. For Goodman and other nominalists, {"a", "b"} is also identical to {"a", {"b"} }, {"b", {"a", "b"} }, and any combination of matching curly braces and one or more instances of "a" and "b", as long as "a" and "b" are names of individuals and not of collections of individuals. Goodman, Richard Milton Martin, and Willard Quine all advocated reasoning about collectivities by means of a theory of "virtual sets" (see especially Quine 1969), one making possible all elementary operations on sets except that the universe of a quantified variable cannot contain any virtual sets.
In the foundation of mathematics, nominalism has come to mean doing mathematics without assuming that sets in the mathematical sense exist. In practice, this means that quantified variables may range over universes of numbers, points, primitive ordered pairs, and other abstract ontological primitives, but not over sets whose members are such individuals. To date, only a small fraction of the corpus of modern mathematics can be rederived in a nominalistic fashion.
History.
Plato was perhaps the first writer in Western philosophy to clearly state a non-Nominalist position, which he plainly endorsed:
 ...We customarily hypothesize a single form in connection with each of the many things to which we apply the same name. ... For example, there are many beds and tables. ... But there are only two forms of such furniture, one of the bed and one of the table. ("Republic" 596a-b, trans. Grube) 
What about someone who believes in beautiful things, but doesn't believe in the beautiful itself…? Don't you think he is living in a dream rather than a wakened state? ("Republic" 476c)
The Platonic universals corresponding to the names "bed" and "beautiful" were the Form of the Bed and the Form of the Beautiful, or the "Bed Itself" and the "Beautiful Itself". Platonic Forms were the first universals posited as such in philosophy.
Our term "universal" is due to the English translation of Aristotle's technical term "katholou" which he coined specially for the purpose of discussing the problem of universals. "Katholou" is a contraction of the phrase "kata holou", meaning "on the whole".
Aristotle famously rejected certain aspects of Plato's Theory of Forms, but he clearly rejected Nominalism as well:
...'Man', and indeed every general predicate, signifies not an individual, but some quality, or quantity or relation, or something of that sort. ("Sophistical Refutations" xxii, 178b37, trans. Pickard-Cambridge)
Applied Nominalism redirects the thesis of nominalism to the highest level of self actualism whereby all individuals are equal and collectively creates community actualism where everyone is without hierarchy. Nominalism is thus categorical but without hierarchy and leads us in directing thought towards the general idea of the community rather than the individual (Porter, 2006).
In "Alice in Wonderland", the problem of nominalism is presented in an anecdotal example:
"When "I" use a word," Humpty Dumpty said, in rather a scornful tone, "it means just what I choose it to mean – neither more nor less."
"The question is," said Alice, "whether you "can" make words mean so many different things."
"The question is," said Humpty Dumpty, "which is to be master – that's all."
Criticisms.
Critique of the historical origins of the term:
As a category of late medieval thought, the concept of 'nominalism' has been increasingly queried. Traditionally, the fourteenth century has been regarded as the heyday of nominalism, with figures such as John Buridan and William of Ockham viewed as founding figures. However, the concept of 'nominalism' as a movement (generally contrasted with 'realism'), first emerged only in the late fourteenth century, and only gradually became widespread during the fifteenth century. The notion of two distinct ways, a "via antiqua", associated with realism, and a "via moderna", associated with nominalism, became widespread only in the later fifteenth century – a dispute which eventually dried up in the sixteenth century.
Aware that explicit thinking in terms of a divide between 'nominalism' and 'realism' only emerged in the fifteenth century, scholars have increasingly questioned whether a fourteenth-century school of nominalism can really be said to have existed. While one might speak of family resemblances between Ockham, Buridan, Marsilius and others, there are also striking differences. More fundamentally, Robert Pasnau has questioned whether any kind of coherent body of thought that could be called 'nominalism' can be discerned in fourteenth century writing. This makes it difficult, it has been argued, to follow the twentieth century narrative which portrayed late scholastic philosophy as a dispute which emerged in the fourteenth century between the "via moderna", nominalism, and the "via antiqua", realism, with the nominalist ideas of William of Ockham foreshadowing the eventual rejection of scholasticism in the seventeenth century.
Critique of nominalist reconstructions in mathematics:
A critique of nominalist reconstructions in mathematics was undertaken by Burgess (1983) and Burgess and Rosen (1997). Burgess distinguished two types of nominalist reconstructions. Thus, "hermeneutic nominalism" is the hypothesis that science, properly interpreted, already dispenses with mathematical objects
(entities) such as numbers and sets. Meanwhile, "revolutionary nominalism" is the project of replacing current scientific theories by alternatives dispensing with mathematical objects, see (Burgess, 1983, p. 96). A recent study extends the Burgessian critique to three nominalistic reconstructions: the reconstruction of analysis by Georg Cantor, Richard Dedekind, and Karl Weierstrass that dispensed with infinitesimals; the constructivist re-reconstruction of Weiertrassian analysis by Errett Bishop that dispensed with the law of excluded middle; and the hermeneutic reconstruction, by Carl Boyer, Judith Grabiner, and others, of Cauchy's foundational contribution to analysis that dispensed with Cauchy's infinitesimals.

</doc>
<doc id="21178" url="http://en.wikipedia.org/wiki?curid=21178" title="Non-cognitivism">
Non-cognitivism

Non-cognitivism is the meta-ethical view that ethical sentences do not express propositions (i.e. statements) and thus cannot be true or false (they are not truth-apt). A noncognitivist denies the cognitivist claim that "moral judgments are capable of being objectively true, because they describe some feature of the world." If moral statements cannot be true, and if one cannot know something that is not true, noncognitivism implies that moral knowledge is impossible.
Non-cognitivism entails that non-cognitive attitudes underlie moral discourse and this discourse therefore consists of non-declarative speech acts, although accepting that its surface features may consistently and efficiently work as if moral discourse were cognitive. The point of interpreting moral claims as non-declarative speech acts is to explain what moral claims mean if they are neither true nor false (as philosophies such as logical positivism entail). Utterances like "Boo to killing!" and "Don't kill" are not candidates for truth or falsity, but have non-cognitive meaning.
Varieties of non-cognitivism.
Emotivism, associated with A. J. Ayer, the Vienna Circle and C. L. Stevenson, suggests that ethical sentences are primarily emotional expressions of one's own attitudes and are intended to influence the actions of the listener. Under this view, "Killing is wrong" is translated as "Killing, boo!" or "I disapprove of killing."
A close cousin of emotivism, developed by R. M. Hare, is called universal prescriptivism. Prescriptivists interpret ethical statements as being universal "imperatives", prescribing behavior for all to follow. According to prescriptivism, 
phrases like "Thou shalt not murder!" or "Do not steal!" are the clearest expressions of morality, while reformulations like "Killing is wrong" tend to obscure the meaning of moral sentences.
Other forms of non-cognitivism include Simon Blackburn's quasi-realism and Allan Gibbard's norm-expressivism.
Arguments in favour of non-cognitivism.
Arguments for prescriptivism focus on the "function" of normative statements.
Prescriptivists argue that factual statements and prescriptions are totally different, because of different expectations of change in cases of a clash between word and world.
In a descriptive sentence, if one premises that "red is a number" then according to the rules of English grammar said statement would be false. Since said premise describes the objects "red" and "number," anyone with an adequate understanding of English would notice the falseness of such description and the falseness of said statement. However, if the norm "thou shalt not kill!" is uttered, and this premise is negated (by the fact of a person being murdered), the speaker is not to change his sentence upon observation of this into "kill other people!", but is to reiterate the moral outrage of the act of killing. Adjusting statements based upon objective reality and adjusting reality based upon statements are contrary uses of language, so descriptive statement are a different kind of sentences than norms. If truth is understood according to correspondence theory, the question of the truth or falsity of sentences not contingent upon external phenomena cannot be tested (see tautologies ).
Some cognitivists argue that some expressions like "courageous" have both a factual as well as a normative component which cannot be distinguished by analysis. Prescriptivists argue that according to context, either the factual or the normative component of the meaning is dominant. The sentence "Hero A behaved courageously" is wrong, if A ran away in the face of danger. But the sentence "Be brave and fight for the glory of your country!" has no truth value and cannot be falsified by someone who doesn't join the army.
Prescriptivism is also supported by the actual way of speaking. Many moral statements are de facto uttered as recommendations or commands, e.g. when parents or teachers forbid children to do wrong actions. The most famous moral ideas are prescriptions: the Ten Commandments, the command of charity, the categorical imperative, and the Golden Rule command to do or not to do something rather than state that something is or is not the case.
Prescriptivism can fit the theist idea of morality as obedience towards god. It is however different from the cognitivist supernaturalism which interprets morality as subjective will of god, while prescriptivism claims that moral rules are universal and can be found by reason alone without reference to a god.
According to Hare, prescriptivists cannot argue that amoralists are logically wrong or contradictive. Everyone can choose to follow moral commands or not. This is the human condition according to the Christian reinterpretation of the Choice of Heracles. According to prescriptivism, morality is not about knowledge (of moral facts), but about character (to choose to do the right thing). Actors cannot externalize their responsibility and freedom of will towards some moral truth in the world, virtuous people don't need to wait for some cognition to choose what's right.
Prescriptivism is also supported by imperative logic, in which there are no truth values for imperatives, and by the idea of the naturalistic fallacy: even if someone could prove the existence of an ethical property and express it in a factual statement, he could never derive any command from this statement, so the search for ethical properties is pointless.
As with other anti-realist meta-ethical theories, non-cognitivism is largely supported by the argument from queerness: ethical properties, if they existed, would be different from any other thing in the universe, since they have no observable effect on the world. People generally have a negative attitude towards murder, which presumably keeps most of us from murdering. But does the actual "wrongness" of murder play an "independent" role? Is there any evidence that there is a property of wrongness that some types of acts have? Some people might think that the strong feelings we have when we see or consider a murder provide evidence of murder's wrongness. But it is not difficult to explain these feelings without saying that "wrongness" was their cause. Thus there is no way of discerning which, if any, ethical properties exist; by Occam's Razor, the simplest assumption is that none do. The non-cognitivist then asserts that, since a proposition about an ethical property would have no referent, ethical statements must be something else.
Arguments for emotivism focus on what normative statements "express" when uttered by a speaker. A person who says that killing is wrong certainly expresses her disapproval of killing. Emotivists claim that this is "all" she does, that the statement "killing is wrong" is not a truth-apt declaration, and that the burden of evidence is on the cognitivists who want to show that in addition to expressing disapproval, the claim "killing is wrong" is also true. Emotivists ask whether there really is evidence that killing is wrong. We have evidence that Jupiter has a magnetic field and that birds are oviparous, but as yet, we do not seem to have found evidence of moral properties, such as "goodness". Emotivists ask why, without such evidence, we should think there "is" such a property. Ethical intuitionists think the evidence comes not from science or reason but from our own feelings: good deeds make us feel a certain way and bad deeds make us feel very differently. But is this enough to show that there are genuinely good and bad deeds? Emotivists think not, claiming that we do not need to postulate the existence of moral "badness" or "wrongness" to explain why considering certain deeds makes us feel disapproval; that all we really observe when we introspect are feelings of disapproval. Thus the emotivist asks why not adopt the simple explanation and say that this is all there is, rather than insist that some intrinsic "badness" (of murder, for example) must be causing feelings when a simpler explanation is available.
Arguments against non-cognitivism.
One argument against non-cognitivism is that it ignores the external "causes" of emotional and prescriptive reactions. If someone says, "John is a good person," something about John must have inspired that reaction. If John gives to the poor, takes care of his sick grandmother, and is friendly to others, and these are what inspire the speaker to think well of him, it is plausible to say, "John is a good person because he gives to the poor, takes care of his sick grandmother, and is friendly to others." If, in turn, the speaker responds positively to the idea of giving to the poor, then some aspect of that idea must have inspired a positive response; one could argue that that aspect is also the basis of its goodness.
Another argument is the "embedding problem." Consider the following sentences:
Attempts to translate these sentences in an emotivist framework seem to fail (e.g. "She does not realize, 'Boo on eating meat!'"). Prescriptivist translations fare only slightly better ("She does not realize that she is not to eat meat"). Even the act of forming such a construction indicates some sort of cognition in the process.
According to some non-cognitivist points of view, these sentences simply assume the false premise that ethical statements are either true or false. They might be literally translated as:
These translations, however, seem divorced from the way people actually use language. A non-cognitivist would have to disagree with someone saying, "'Eating meat is wrong' is a false statement" (since "Eating meat is wrong" is not truth-apt at all), but may be tempted to agree with a person saying, "Eating meat is not wrong."
One might more constructively interpret these statements to describe the underlying emotional statement that they express, i.e.: I disapprove/do not disapprove of eating meat, I used to, he doesn't, I do and she doesn't, etc.; however, this interpretation is closer to ethical subjectivism than to non-cognitivism proper.
A similar argument against non-cognitivism is that of ethical argument. A common argument might be, "If killing an innocent human is always wrong, and all fetuses are innocent humans, then killing a fetus is always wrong." Most people would consider such an utterance to represent an analytic proposition which is true "a priori". However, if ethical statements do not represent cognitions, it seems odd to use them as premises in an argument, and even odder to assume they follow the same rules of syllogism as true propositions. However, R.M. Hare, proponent of universal prescriptivism, has argued that the rules of logic are independent of grammatical mood, and thus the same logical relations may hold between imperatives as hold between indicatives.
Many objections to non-cognitivism based on the linguistic characteristics of what purport to be moral judgments were originally raised by Peter Glassen in "The Cognitivity of Moral Judgments", published in "Mind" in January 1959, and in Glassen's follow-up article in the January 1963 issue of the same journal.

</doc>
<doc id="21179" url="http://en.wikipedia.org/wiki?curid=21179" title="North Sea">
North Sea

 
The North Sea is a marginal sea of the Atlantic Ocean located between Great Britain, Scandinavia, Germany, the Netherlands, Belgium, and France. An epeiric (or "shelf") sea on the European continental shelf, it connects to the ocean through the English Channel in the south and the Norwegian Sea in the north. It is more than 970 km long and 580 km wide, with an area of around 750000 km2.
The North Sea has long been the site of important European shipping lanes as well as a major fishery. The sea is a popular destination for recreation and tourism in bordering countries and more recently has developed into a rich source of energy resources including fossil fuels, wind, and early efforts in wave power.
Historically, the North Sea has featured prominently in geopolitical and military affairs, particularly in Northern Europe but also globally through the power northern Europeans projected worldwide during much of the Middle Ages and into the modern era. The North Sea was the centre of the Vikings' rise. Subsequently, the Hanseatic League, the Netherlands, and the British each sought to dominate the North Sea and thus the access to the markets and resources of the world. As Germany's only outlet to the ocean, the North Sea continued to be strategically important through both World Wars.
The coast of the North Sea presents a diversity of geological and geographical features. In the north, deep fjords and sheer cliffs mark the Norwegian and Scottish coastlines, whereas in the south it consists primarily of sandy beaches and wide mudflats. Due to the dense population, heavy industrialization, and intense use of the sea and area surrounding it, there have been a number of environmental issues affecting the sea's ecosystems. Environmental concerns — commonly including overfishing, industrial and agricultural runoff, dredging, and dumping among others — have led to a number of efforts to prevent degradation of the sea while still making use of its economic potential.
Geography.
NorthSea
NorwegianSea
Sk
Ka
Eng Ch
Sk=Skagerrak   Ka=KattegatEng Ch=English Channel 
The North Sea is bounded by the Orkney Islands and east coast of Great Britain to the west and the northern and central European mainland to the east and south, including Norway, Denmark, Germany, the Netherlands, Belgium, and France. In the southwest, beyond the Straits of Dover, the North Sea becomes the English Channel connecting to the Atlantic Ocean. In the east, it connects to the Baltic Sea via the Skagerrak and Kattegat, narrow straits that separate Denmark from Norway and Sweden respectively. In the north it is bordered by the Shetland Islands, and connects with the Norwegian Sea, which lies in the very north-eastern part of the Atlantic.
The North Sea is more than 970 km long and 580 km wide, with an area of 750000 km2 and a volume of 94000 km3. Around the edges of the North Sea are sizeable islands and archipelagos, including Shetland, Orkney, and the Frisian Islands. The North Sea receives freshwater from a number of European continental watersheds, as well as the British Isles. A large part of the European drainage basin empties into the North Sea including water from the Baltic Sea. The largest and most important rivers flowing into the North Sea are the Elbe and the Rhine – Meuse watershed. Around 185 million people live in the catchment area of the rivers discharging into the North Sea encompassing some highly industrialized areas.
Major features.
For the most part, the sea lies on the European continental shelf with a mean depth of 90 m. The only exception is the Norwegian trench, which extends parallel to the Norwegian shoreline from Oslo to an area north of Bergen. It is between 20 and wide and has a maximum depth of 725 m.
The Dogger Bank, a vast moraine, or accumulation of unconsolidated glacial debris, rises to a mere 15 to 30 metres (50–100 ft) below the surface. This feature has produced the finest fishing location of the North Sea. The Long Forties and the Broad Fourteens are large areas with roughly uniform depth in fathoms, (forty fathoms and fourteen fathoms or 73 and 26 m deep respectively). These great banks and others make the North Sea particularly hazardous to navigate, which has been alleviated by the implementation of satellite navigation systems. The Devil's Hole lies 200 mi east of Dundee, Scotland. The feature is a series of asymmetrical trenches between 20 and long, 1 and wide and up to 230 m deep.
Extent.
The International Hydrographic Organization defines the limits of the North Sea as follows:
"On the Southwest." A line joining the Walde Lighthouse (France, 1°55'E) and Leathercoat Point (England, 51°10'N).
"On the Northwest." From Dunnet Head (3°22'W) in Scotland to Tor Ness (58°47'N) in the Island of Hoy, thence through this island to the Kame of Hoy (58°55'N) on to Breck Ness on Mainland (58°58'N) through this island to Costa Head (3°14'W) and to Inga Ness (59'17'N) in Westray through Westray, to Bow Head, across to Mull Head (North point of Papa Westray) and on to Seal Skerry (North point of North Ronaldsay) and thence to Horse Island (South point of the Shetland Islands).
"On the North." From the North point (Fethaland Point) of the Mainland of the Shetland Islands, across to Graveland Ness (60°39'N) in the Island of Yell, through Yell to Gloup Ness (1°04'W) and across to Spoo Ness (60°45'N) in Unst island, through Unst to Herma Ness (60°51'N), on to the SW point of the Rumblings and to Muckle Flugga () all these being included in the North Sea area; thence up the meridian of 0°53' West to the parallel of 61°00' North and eastward along this parallel to the coast of Norway, the whole of Viking Bank being thus included in the North Sea.
"On the East." The Western limit of the Skagerrak [A line joining Hanstholm () and the Naze (Lindesnes, )].
Hydrology.
Temperature and salinity.
The average temperature in summer is 17 °C and 6 °C in the winter. The average temperatures have been trending higher since 1988, which has been attributed to climate change. Air temperatures in January range on average between 0 to and in July between 13 to. The winter months see frequent gales and storms.
The salinity averages between 34 to 35 grams of salt per litre of water. The salinity has the highest variability where there is fresh water inflow, such as at the Rhine and Elbe estuaries, the Baltic Sea exit and along the coast of Norway.
Water circulation and tides.
The main pattern to the flow of water in the North Sea is an anti-clockwise rotation along the edges.
The North Sea is an arm of the Atlantic Ocean receiving the majority of ocean current from the northwest opening, and a lesser portion of warm current from the smaller opening at the English Channel. These tidal currents leave along the Norwegian coast. Surface and deep water currents may move in different directions. Low salinity surface coastal waters move offshore, and deeper, denser high salinity waters move in shore.
The North Sea located on the continental shelf has different waves than those in deep ocean water. The wave speeds are diminished and the wave amplitudes are increased. In the North Sea there are two amphidromic systems and a third incomplete amphidromic system. In the North Sea the average tide difference in wave amplitude is between 0 to.
The Kelvin tide of the Atlantic ocean is a semidiurnal wave that travels northward. Some of the energy from this wave travels through the English Channel into the North Sea. The wave still travels northward in the Atlantic Ocean, and once past the northern tip of Great Britain, the Kelvin wave turns east and south and once again enters into the North Sea.
Coasts.
The eastern and western coasts of the North Sea are jagged, formed by glaciers during the ice ages. The coastlines along the southernmost part are covered with the remains of deposited glacial sediment. The Norwegian mountains plunge into the sea creating deep fjords and archipelagos. South of Stavanger, the coast softens, the islands become fewer. The eastern Scottish coast is similar, though less severe than Norway. From north east of England, the cliffs become lower and are composed of less resistant moraine, which erodes more easily, so that the coasts have more rounded contours. In Holland, Belgium and in the east of England (East Anglia) the littoral is low and marshy. The east coast and south-east of the North Sea (Wadden Sea) have coastlines that are mainly sandy and straight owing to longshore drift, particularly along Belgium and Denmark.
Coastal management.
The southern coastal areas were originally amphibious flood plains and swampy land. In areas especially vulnerable to storm tides, people settled behind elevated levees and on natural areas of high ground such as spits and geestland.:[302,303] As early as 500 BC, people were constructing artificial dwelling hills higher than the prevailing flood levels.:[306,308] It was only around the beginning of the High Middle Ages, in 1200 AD, that inhabitants began to connect single ring dikes into a dike line along the entire coast, thereby turning amphibious regions between the land and the sea into permanent solid ground.
The modern form of the dikes supplemented by overflow and lateral diversion channels, began to appear in the 17th and 18th centuries, built in the Netherlands. The North Sea Floods of 1953 and 1962 were impetus for further raising of the dikes as well as the shortening of the coast line so as to present as little surface area as possible to the punishment of the sea and the storms. Currently, 27% of the Netherlands is below sea level protected by dikes, dunes, and beach flats.
Coastal management today consists of several levels. The dike slope reduces the energy of the incoming sea, so that the dike itself does not receive the full impact. Dikes that lie directly on the sea are especially reinforced. The dikes have, over the years, been repeatedly raised, sometimes up to 9 m and have been made flatter to better reduce wave erosion. Where the dunes are sufficient to protect the land behind them from the sea, these dunes are planted with beach grass to protect them from erosion by wind, water, and foot traffic.
Storm tides.
Storm tides threaten, in particular, the coasts of the Netherlands, Belgium, Germany, and Denmark and low lying areas of eastern England particularly around The Wash and Fens.
Storm surges are caused by changes in barometric pressure combined with strong wind created wave action.
The first recorded storm tide flood was the "Julianenflut", on 17 February 1164. In its wake the Jadebusen, (a bay on the coast of Germany), began to form.
A storm tide in 1228 is recorded to have killed more than 100,000 people. In 1362, the Second Marcellus Flood, also known as the Grote Manndränke, hit the entire southern coast of the North Sea. Chronicles of the time again record more than 100,000 deaths as large parts of the coast were lost permanently to the sea, including the now legendary lost city of Rungholt.
In the 20th century, the North Sea flood of 1953 flooded several nations' coasts and cost more than 2,000 lives.
315 citizens of Hamburg died in the North Sea flood of 1962.:[79,86]
Tsunamis.
Though rare, the North Sea has been the site of a number of historically documented tsunamis. The Storegga Slides were a series of underwater landslides, in which a piece of the Norwegian continental shelf slid into the Norwegian Sea. The immense landslips occurred between 8150 BCE and 6000 BCE, and caused a tsunami up to 20 m high that swept through the North Sea, having the greatest effect on Scotland and the Faeroe Islands.
The Dover Straits earthquake of 1580 is among the first recorded earthquakes in the North Sea measuring between 5.6 and 5.9 on the Richter Scale. This event caused extensive damage in Calais both through its tremors and possibly triggered a tsunami, though this has never been confirmed. The theory is a vast underwater landslide in the English Channel was triggered by the earthquake, which in turn caused a tsunami. The tsunami triggered by the 1755 Lisbon Earthquake reached Holland, although the waves had lost their destructive power. The largest earthquake ever recorded in the United Kingdom was the 1931 Dogger Bank earthquake, which measured 6.1 on the Richter Scale and caused a small tsunami that flooded parts of the British coast.
Geology.
Shallow epicontinental seas like the current North Sea have since long existed on the European continental shelf. The rifting that formed the northern part of the Atlantic Ocean during the Jurassic and Cretaceous periods, from about million years ago, caused tectonic uplift in the British Isles. Since then, a shallow sea has almost continuously existed between the highs of the Fennoscandian Shield and the British Isles. This precursor of the current North Sea has grown and shrunk with the rise and fall of the eustatic sea level during geologic time. Sometimes it was connected with other shallow seas, such as the sea above the Paris Basin to the south-west, the Paratethys Sea to the south-east, or the Tethys Ocean to the south.
During the Late Cretaceous, about million years ago, all of modern mainland Europe except for Scandinavia was a scattering of islands. By the Early Oligocene, 28, the emergence of Western and Central Europe had almost completely separated the North Sea from the Tethys Ocean, which gradually shrank to become the Mediterranean as Southern Europe and South West Asia became dry land. The North Sea was cut off from the English Channel by a narrow land bridge until that was breached by at least two catastrophic floods between 450,000 and 180,000 years ago. Since the start of the Quaternary period about million years ago, the eustatic sea level has fallen during each glacial period and then risen again. Every time the ice sheet reached its greatest extent, the North Sea became almost completely dry. The present-day coastline formed after the Last Glacial Maximum when the sea began to flood the European continental shelf.
In 2006 a bone fragment was found while drilling for oil in the north sea. Analysis indicated that it was a Plateosaurus from 199 to 216 million years ago. This was the deepest dinosaur fossil ever found and the first find for Norway.
Natural history.
Fish and shellfish.
Copepods and other zooplankton are plentiful in the North Sea. These tiny organisms are crucial elements of the food chain supporting many species of fish. Over 230 species of fish live in the North Sea. Cod, haddock, whiting, saithe, plaice, sole, mackerel, herring, pouting, sprat, and sandeel are all very common and are fished commercially. Due to the various depths of the North Sea trenches and differences in salinity, temperature, and water movement, some fish such as blue-mouth redfish and rabbitfish reside only in small areas of the North Sea.
Crustaceans are also commonly found throughout the sea. Norway lobster, deep-water prawns, and brown shrimp are all commercially fished, but other species of lobster, shrimp, oyster, mussels and clams all live in the North Sea. Recently non-indigenous species have become established including the Pacific oyster and Atlantic jackknife clam.
Birds.
The coasts of the North Sea are home to nature reserves including the Ythan Estuary, Fowlsheugh Nature Preserve, and Farne Islands in the UK and the Wadden Sea National Parks in Denmark, Germany and the Netherlands. These locations provide breeding habitat for dozens of bird species. Tens of millions of birds make use of the North Sea for breeding, feeding, or migratory stopovers every year. Populations of black legged kittiwakes, Atlantic puffins, northern fulmars, and species of petrels, gannets, seaducks, loons (divers), cormorants, gulls, auks, and terns, and many other seabirds make these coasts popular for birdwatching.
Marine mammals.
The North Sea is also home to marine mammals. Common seals, and harbour porpoises can be found along the coasts, at marine installations, and on islands. The very northern North Sea islands such as the Shetland Islands are occasionally home to a larger variety of pinnipeds including bearded, harp, hooded and ringed seals, and even walrus. North Sea cetaceans include various porpoise, dolphin and whale species.
Flora.
Plant species in the North Sea include species of wrack, among them bladder wrack, knotted wrack, and serrated wrack. Algae, macroalgal, and kelp, such as oarweed and laminaria hyperboria, and species of maerl are found as well. Eelgrass, formerly common in the entirety of the Wadden Sea, was nearly wiped out in the 20th century by a disease. Similarly, sea grass used to coat huge tracts of ocean floor, but have been damaged by trawling and dredging have diminished its habitat and prevented its return. Invasive Japanese seaweed has spread along the shores of the sea clogging harbours and inlets and has become a nuisance.
Biodiversity and conservation.
Due to the heavy human populations and high level of industrialization along its shores, the wildlife of the North Sea has suffered from pollution, overhunting, and overfishing. Flamingos, pelicans, and great auk were once found along the southern shores of the North Sea, but went extinct over the 2nd millennium. Walruses frequented the Orkney Islands through the mid-16th century, as both Sable Island and Orkney Islands lay within its normal range. Gray whales also resided in the North Sea but were driven to extinction in the Atlantic in the 17th century Other species have dramatically declined in population, though they are still found. Right whales, sturgeon, shad, rays, skates, salmon, and other species were common in the North Sea until the 20th century, when numbers declined due to overfishing. Other factors like the introduction of non-indigenous species, industrial and agricultural pollution, trawling and dredging, human-induced eutrophication, construction on coastal breeding and feeding grounds, sand and gravel extraction, offshore construction, and heavy shipping traffic have also contributed to the decline.
The OSPAR commission manages the OSPAR convention to counteract the harmful effects of human activity on wildlife in the North Sea, preserve endangered species, and provide environmental protection. All North Sea border states are signatories of the MARPOL 73/78 Accords, which preserve the marine environment by preventing pollution from ships.
Germany, Denmark, and the Netherlands also have a trilateral agreement for the protection of the Wadden Sea, or mudflats, which run along the coasts of the three countries on the southern edge of the North Sea.
Whaling.
Whaling was an important economic activity from the 9th until the 13th century for Flemish whalers. The medieval Flemish, Basque and Norwegian whalers who were replaced in the 16th century by Dutch, English, Danes and Germans, took massive numbers of whales and dolphins and nearly depleted the right whales. This activity likely led to the extinction of the Atlantic population of the once common gray whale. By 1902 the whaling had ended. After being absent for 300 years a single gray whale returned, it probably was the first of many more to find its way through the now ice-free North-west Passage. Once 16 m "fish" were taken in large quantities at the mouth of the Seine River. Perhaps the gray whale will someday return to its former Seine estuary breeding grounds and to the feeding grounds of the Wadden Sea where it will again roil the sediments and release its benthic nutrients that will benefit the ecosystem.
History.
Name.
Through history various names have been used for the North Sea. One of the earliest recorded names was "Septentrionalis Oceanus", or "Northern Ocean," which was cited by Pliny. The name "North Sea" probably came into English, however, via the Dutch "Noordzee", who named it thus either in contrast with the Zuiderzee ("South Sea"), located south of Frisia, or simply because the sea is generally to the north of the Netherlands. Before the adoption of "North Sea," the names used in English were "German Sea" or "German Ocean", the latter referred to the Latin name "Oceanus Germanicus", and they persisted even into the 1830s.
Other common names in use for long periods were the Latin terms "Mare Frisicum", as well as their English equivalents, "Frisian Sea".
The modern names of the sea in local languages are: Danish: "Nordsøen", Dutch: "Noordzee", Dutch Low Saxon: "Noordzee", French: "Mer du Nord", Frisian: "Noardsee", German: "Nordsee", Low German: "Noordsee", Northern Frisian: "Weestsiie", Norwegian: "Nordsjøen", Nynorsk: "Nordsjøen", Scots: "German Ocean", Scottish Gaelic: "An Cuan a Tuath", West Flemish: "Nôordzêe" and Zeeuws: "Noôrdzeê".
Early history.
The North Sea has provided waterway access for commerce and conquest. Many areas have access to the North Sea with its long coastline and European rivers that empty into it. The British Isles had been protected from invasion by the North Sea waters until the Roman conquest of Britain in 43 CE. The Romans established organised ports, shipping increased and sustained trade began. When the Romans abandoned Britain in 410 the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea during the Migration Period invading England.
The Viking Age began in 793 with the attack on Lindisfarne and for the next quarter-millennium the Vikings ruled the North Sea. In their superior longships, they raided, traded, and established colonies and outposts on the sea's coasts. From the Middle Ages through the 15th century, the northern European coastal ports exported domestic goods, dyes, linen, salt, metal goods and wine. The Scandinavian and Baltic areas shipped grain, fish, naval necessities, and timber. In turn the North Sea countries imported high grade cloths, spices, and fruits from the Mediterranean region Commerce during this era was mainly undertaken by maritime trade due to underdeveloped roadways.
In the 13th century the Hanseatic League, though centred on the Baltic Sea, started to control most of the trade through important members and outposts on the North Sea. The League lost its dominance in the 16th century, as neighbouring states took control of former Hanseatic cities and outposts and internal conflict prevented effective cooperation and defence. Furthermore, as the League lost control of its maritime cities, new trade routes emerged that provided Europe with Asian, American, and African goods.
Age of sail.
The 17th century Dutch Golden Age during which Dutch herring, cod and whale fisheries reached an all time high saw Dutch power at its zenith. Important overseas colonies, a vast merchant marine, powerful navy and large profits made the Dutch the main challengers to an ambitious England. This rivalry led to the first three Anglo-Dutch Wars between 1652 and 1673, which ended with Dutch victories. After the Glorious Revolution the Dutch prince William ascended to the English throne. With both countries united, commercial, military, and political power shifted from Amsterdam to London.
The British did not face a challenge to their dominance of the North Sea until the 20th century.
Modern era.
Tensions in the North Sea were again heightened in 1904 by the Dogger Bank incident. During the Russo-Japanese War, several ships of the Russian Baltic Fleet, which was on its way to the Far East, mistook British fishing boats for Japanese ships and fired on them, and then upon each other, near the Dogger Bank, nearly causing Britain to enter the war on the side of Japan.
During the First World War, Great Britain's Grand Fleet and Germany's Kaiserliche Marine faced each other in the North Sea, which became the main theatre of the war for surface action. Britain's larger fleet and North Sea Mine Barrage were able to establish an effective blockade for most of the war, which restricted the Central Powers' access to many crucial resources. Major battles included the Battle of Heligoland Bight, the Battle of the Dogger Bank, and the Battle of Jutland.
World War I also brought the first extensive use of submarine warfare, and a number of submarine actions occurred in the North Sea.
The Second World War also saw action in the North Sea, though it was restricted more to aircraft reconnaissance, and action by fighter/bomber aircraft, submarines, and smaller vessels such as minesweepers and torpedo boats.
In the aftermath the war, hundreds of thousands of tons of chemical weapons were disposed of by being dumped in the North Sea.
After the war, the North Sea lost much of its military significance because it is bordered only by NATO member-states. However, it gained significant economic importance in the 1960s as the states around the North Sea began full-scale exploitation of its oil and gas resources. The North Sea continues to be an active trade route.
Economy.
Political status.
Countries that border the North Sea all claim the 12 nmi of territorial waters, within which they have exclusive fishing rights. The Common Fisheries Policy of the European Union (EU) exists to coordinate fishing rights and assist with disputes between EU states and the EU border state of Norway.
After the discovery of mineral resources in the North Sea, the Convention on the Continental Shelf established country rights largely divided along the median line. The median line is defined as the line "every point of which is equidistant from the nearest points of the baselines from which the breadth of the territorial sea of each State is measured."
The ocean floor border between Germany, the Netherlands, and Denmark was only reapportioned after protracted negotiations and a judgement of the International Court of Justice.
Oil and gas.
As early as 1859, oil was discovered in onshore areas around the North Sea and natural gas as early as 1910.
Test drilling began in 1966 and then, in 1969, Phillips Petroleum Company discovered the Ekofisk oil field distinguished by valuable, low-sulphur oil. Commercial exploitation began in 1971 with tankers and, after 1975, by a pipeline, first to Teesside, England and then, after 1977, also to Emden, Germany.
The exploitation of the North Sea oil reserves began just before the 1973 oil crisis, and the climb of international oil prices made the large investments needed for extraction much more attractive.
Although the production costs are relatively high, the quality of the oil, the political stability of the region, and the nearness of important markets in western Europe has made the North Sea an important oil producing region. The largest single humanitarian catastrophe in the North Sea oil industry was the destruction of the offshore oil platform Piper Alpha in 1988 in which 167 people lost their lives.
Besides the Ekofisk oil field, the Statfjord oil field is also notable as it was the cause of the first pipeline to span the Norwegian trench. The largest natural gas field in the North Sea, Troll gas field, lies in the Norwegian trench dropping over 300 m requiring the construction of the enormous Troll A platform to access it.
The price of Brent Crude, one of the first types of oil extracted from the North Sea, is used today as a standard price for comparison for crude oil from the rest of the world. The North Sea contains western Europe's largest oil and natural gas reserves and is one of the world's key non-OPEC producing regions.
In the UK sector of the North Sea, the oil industry invested £14.4 billion in 2013, and was on track to spend £13 billion in 2014. Industry body Oil & Gas UK put the decline down to rising costs, lower production, high tax rates, and less exploration.
Fishing.
The North Sea is Europe's main fishery accounting for over 5% of international commercial fish caught. Fishing in the North Sea is concentrated in the southern part of the coastal waters. The main method of fishing is trawling.
In 1995, the total volume of fish and shellfish caught in the North Sea was approximately 3.5 million tonnes. Besides fish, it is estimated that one million tonnes of unmarketable by-catch is caught and discarded each year.
In recent decades, overfishing has left many fisheries unproductive, disturbing marine food chain dynamics and costing jobs in the fishing industry. Herring, cod and plaice fisheries may soon face the same plight as mackerel fishing, which ceased in the 1970s due to overfishing.
The objective of the European Union Common Fisheries Policy is to minimize the environmental impact associated with resource use by reducing fish discards, increasing productivity of fisheries, stabilising markets of fisheries and fish processing, and supplying fish at reasonable prices for the consumer.
Mineral resources.
In addition to oil, gas, and fish, the states along the North Sea also take millions of cubic metres per year of sand and gravel from the ocean floor. These are used for beach nourishment, land reclamation
and construction.
Rolled pieces of amber may be picked up on the east coast of England.
Renewable energy.
Due to the strong prevailing winds, countries on the North Sea, particularly Germany and Denmark, have used the shore for wind power since the 1990s. The North Sea is the home of one of the first large-scale offshore wind farms in the world, Horns Rev 1, completed in 2002. Since then many other wind farms have been commissioned in the North Sea (and elsewhere), including the two largest windfarms in the world as of September 2010; Thanet in the UK and Horns Rev 2 in Denmark.
The expansion of offshore wind farms has met with some resistance. Concerns have included shipping collisions and environmental effects on ocean ecology and wildlife such as fish and migratory birds, however, these concerns were found to be negligible in a long-term study in Denmark released in 2006 and again in a UK government study in 2009.
There are also concerns about reliability, and the rising costs of constructing and maintaining offshore wind farms. Despite these, development of North Sea wind power is continuing, with plans for additional wind farms off the coasts of Germany, the Netherlands, and the UK. There have also been proposals for a transnational power grid in the North Sea to connect new offshore wind farms.
Energy production from tidal power is still in a pre-commercial stage. The European Marine Energy Centre has installed a wave testing system at Billia Croo on the Orkney mainland and a tidal power testing station on the nearby island of Eday. Since 2003, a prototype Wave Dragon energy converter has been in operation at Nissum Bredning fjord of northern Denmark.
Tourism.
The beaches and coastal waters of the North Sea are popular destinations for tourists. The Belgian, Dutch, German and Danish coasts are especially developed for tourism. The North Sea coast of the United Kingdom is also a well developed tourist destination with beach resorts and is particularly famous for its golfing resorts. The Kingdom of Fife in Scotland being particularly famous for its Golfing coastal links courses. The coastal City of St. Andrews being renowned as the "Home of Golf". The coast of North East England has several tourist towns such as Scarborough, Bridlington, Seahouses, Whitby, Robin Hood's Bay & Seaton Carew. The coast of North East England has long sandy beaches and famous links golfing locations such as Seaton Carew Golf Club and Goswick Golf Club.
The North Sea Trail is a long-distance trail linking seven countries around the North Sea. Windsurfing and sailing are popular sports because of the strong winds. Mudflat hiking, recreational fishing and birdwatching are among other popular activities.
The climatic conditions on the North Sea coast are often claimed to be especially healthful. As early as the 19th century, travellers used their stays on the North Sea coast as curative and restorative vacations. The sea air, temperature, wind, water, and sunshine are counted among the beneficial conditions that are said to activate the body's defences, improve circulation, strengthen the immune system, and have healing effects on the skin and the respiratory system.
Marine traffic.
The North Sea is important for marine transport and its shipping lanes are among the busiest in the world. Major ports are located along its coasts: Rotterdam, the busiest port in Europe and the fourth busiest port in the world by tonnage as of 2013[ [update]], Antwerp (was 16th) and Hamburg (was 27th), Bremen/Bremerhaven and Felixstowe, both in the top 30 busiest container seaports, as well as the Port of Bruges-Zeebrugge, Europe's leading RoRo port.
Fishing boats, service boats for offshore industries, sport and pleasure craft, and merchant ships to and from North Sea ports and Baltic ports must share routes on the North Sea. The Dover Strait alone sees more than 400 commercial vessels a day. Because of this volume, navigation in the North Sea can be difficult in high traffic zones, so ports have established elaborate vessel traffic services to monitor and direct ships into and out of port.
The North Sea coasts are home to numerous canals and canal systems to facilitate traffic between and among rivers, artificial harbours, and the sea. The Kiel Canal, connecting the North Sea with the Baltic Sea, is the most heavily used artificial seaway in the world reporting an average of 89 ships per day not including sporting boats and other small watercraft in 2009. It saves an average of 250 nmi, instead of the voyage around the Jutland Peninsula. The North Sea Canal connects Amsterdam with the North Sea.

</doc>
<doc id="21180" url="http://en.wikipedia.org/wiki?curid=21180" title="Natural Born Killers">
Natural Born Killers

 
Natural Born Killers is a 1994 American black comedy crime film directed by Oliver Stone and starring Woody Harrelson, Juliette Lewis, Robert Downey, Jr., Tom Sizemore, and Tommy Lee Jones. The film was released in the United States on August 26, 1994. The film tells the story of two victims of traumatic childhoods who became lovers and mass murderers, and are irresponsibly glorified by the mass media.
The film is based on an original screenplay by Quentin Tarantino that was heavily revised by writer David Veloz, associate producer Richard Rutowski, and director Stone. Tarantino received story credit. Jane Hamsher, Don Murphy, and Clayton Townsend produced the film, with Arnon Milchan, Thom Mount, and Stone as executive producers.
Notorious for its violent content and inspiring "copycat" crimes, the film was named the eighth most controversial film of all time by "Entertainment Weekly" in 2006.
Plot.
Prologue.
Mickey Knox and his wife Mallory stop at a roadside café in the New Mexico desert. A group of rednecks arrive and one begins sexually harassing Mallory. She briefly encourages him before beating him to a pulp. Mickey and Mallory then murder all but one of the diner's patrons, culminating in a morbid game of Eeny, meeny, miny, moe to decide who lives and dies. After executing the waitress Mabel, the couple ensures that the only survivor remembers their names before they embrace and declare their undying love.
Part I.
Mickey and Mallory camp out in the desert, and Mallory reminisces about when they first met. A flashback (done in the style of a TV sitcom, including a laughtrack) shows Mickey as a deliveryman who came to the house where Mallory lived with her sexually abusive father, her neglectful mother, and her younger brother, Kevin. Mickey and Mallory fall in love instantly and leave together, as Mickey steals a car that belongs to Mallory's father. Soon Mickey is arrested and imprisoned for grand theft auto, but he subsequently escapes from a prison work farm during a tornado and returns to Mallory's house. The two kill Mallory's parents, but spare Kevin, and go on the road together and get "married" on the side of a bridge, celebrating by taking a hostage. Furious with Mickey's notion that they have a threesome, Mallory drives to a nearby gas station, where she flirts with the mechanic. They begin to have sex on the hood of a car, but Mallory kills him when he recognizes her as a wanted killer. During this time, Mickey rapes the hostage.
Part II.
The pair continue their killing spree, ultimately claiming fifty-two victims in New Mexico, Arizona, and Nevada. Pursuing them is Detective Jack Scagnetti, who became obsessed with mass murderers after witnessing his mother being shot and killed by Charles Whitman when he was eight. Beneath his heroic facade, he is a violent psychopath, once strangling a prostitute to death. The killers are also followed by self-serving tabloid journalist Wayne Gale. Gale profiles Mickey and Mallory on his show, "American Maniacs", soon elevating them to cult hero status.
Mickey and Mallory become lost in the desert and encounter Warren Red Cloud, a Navajo Indian, and his young grandson. After the two fall asleep, the Navajo, hoping to expel the demon he perceives in Mickey, begins chanting beside the fire, invoking nightmares in Mickey about his abusive parents. Mickey wakes up in a rage and fatally shoots Red Cloud before he realizes what he is doing. It is the first time Mallory and Mickey feel guilty for a murder. Fleeing from the scene through the desert, they stray onto a field of rattlesnakes and are both bitten.
They drive to a drugstore to find snakebite antidote, but the pharmacist sets off the silent alarm before Mickey kills him. Soon police cars arrive and Mallory is captured and subsequently beaten by the police. A gunfight breaks out between Mickey and the others. Scagnetti arrives and tells Mickey that unless he surrenders, he will cut off Mallory's breasts. Mickey gives up his guns, but attacks Scagnetti with a knife. The police taser him and the scene ends with Mickey and Mallory being beaten by a group of vengeful policemen as a Japanese news crew fronted by a female reporter films the action.
Part III.
The story picks up one year later: the homicidal couple have been imprisoned, and are due to be moved to a mental hospital after being declared insane. Scagnetti arrives at the prison and encounters Warden Dwight McClusky, with whom he plans to murder the two criminals. McClusky will arrange for Scagnetti to be the driver for the Knoxes' transfer. Alone with the pair, Scagnetti will murder them, then claim that they tried to escape.
Meanwhile, Gale has persuaded Mickey to agree to a live interview that will air immediately after the Super Bowl. Mallory is held in solitary confinement elsewhere in the prison, awaiting her transport to the mental hospital. During the interview, Mickey gives a speech about how murder provides enlightenment and declares himself a "natural born killer". His words inspire the other inmates (who are watching the interview on TV in the recreation room) and incite them to riot.
McClusky, upon learning of the riot, orders the interview terminated despite Gale's vehement protests. Mickey is left alone with Gale, the film crew and several guards. Using a lengthy joke as a diversion, Mickey overpowers a guard and grabs his shotgun. He kills most of the guards with it and takes the survivors hostage, leading them through the prison riot. Gale follows, giving a live television report as people are beaten and killed around him.
Scagnetti enters Mallory's cell and attempts to seduce her. Mallory rebuffs his efforts, smashing his face against the wall and breaking his nose. The guards and Scagnetti subdue her. Still live on national television, Mickey engages in a Mexican standoff with Scagnetti, eventually feigning a concession. Mallory then approaches Scagnetti from behind and slashes his throat with a shank. To Scagnetti's horror, Mickey tells him that he was out of shotgun shells during the standoff. Mallory then picks up Scagnetti's gun and kills him.
Mickey and Mallory continue to escape through the riot torn prison, with Gale's entire TV crew getting killed. Gale himself snaps, succumbing to Stockholm syndrome, and begins to shoot at the guards with a pistol that he has taken from one of the dead guards. After being rescued by a mysterious prisoner named Owen Traft, the trio of Mickey, Mallory, and Gale run into McClusky and a heavily armed posse of guards. The trio takes cover in a blood-splattered shower room. McClusky threatens to storm the shower room; Mickey, in turn, threatens to kill both Gale and a guard on live TV, and the prisoners walk out the front door. McClusky and his guards are quickly massacred by hordes of inmates.
Mickey and Mallory steal a van and kill the last guard; Owen's fate is unknown. Escaping to a rural location, they give a final interview to Gale, whose penis they have seemingly removed (implied, but as a camera track reveals), before they tell him he must die also. He attempts various arguments to change their minds, finally appealing to their trademark practice of leaving one survivor; Mickey informs him they are leaving a witness to tell the tale, his camera. Gale accepts his fate and extends his arms as if on a cross as they shoot him dead while his unattended camera continues to roll. The couple is shown several years later, in an RV, with Mickey driving and a pregnant Mallory watching their two children play.
Production.
"Natural Born Killers" was based upon a screenplay written by Quentin Tarantino, in which a married couple suddenly decide to go on a killing spree. Tarantino had sold an option for his script to producers Jane Hamsher and Don Murphy for $10,000 after he had tried, and failed, to direct it himself for $500,000. Hamsher and Murphy subsequently sold the screenplay to Warner Bros. Around the same time, Oliver Stone was made aware of the script. He was keen to find something more straightforward than his previous production, "Heaven & Earth"; a difficult shoot which had left him exhausted, and he felt that "Natural Born Killers" could be what he was looking for.
David Veloz, associate producer Richard Rutowski, and Stone rewrote the script, keeping much of the dialogue but changing the focus of the film from journalist Wayne Gale to Mickey and Mallory. The script was changed so much that as per WGA rules, Tarantino was credited for the film's story only. In a 1993 interview, Tarantino stated that he did not hold any animosity towards Stone, and that he wished the film well; "It's not going to be my movie, it's going to be Oliver Stone's, and God bless him. I hope he does a good job with it. If I wasn't emotionally attached to it, I'm sure I would find it very interesting. If you like my stuff, you might not like this movie. But if you like his stuff, you're probably going to love it. It might be the best thing he's ever done, but not because of anything to do with me. [...] I actually can't wait to see it, to tell you the truth."
Initially, when producers Hamsher and Murphy had first brought the script to Stone's attention, he had seen it as an action film; "something Arnold Schwarzenegger would be proud of." As the project developed however, incidents such as the O.J. Simpson case, the Menendez brothers case, the Tonya Harding/Nancy Kerrigan incident, the Rodney King incident, and the Federal assault of the Branch Davidian sect all took place. Stone came to feel that the media was heavily involved in the outcome of all of these cases, and that the media had become an all-pervasive entity which marketed violence and suffering for the good of ratings. As such, he changed the tone of the movie from one of simple action to a satirical critique of the media in general. Also coloring Stone's approach, and contributing to the violent nature of the film, were the anger and sadness he felt at the breakdown of his second marriage. He also said in an interview that the film was influenced by the "vitality" of Indian cinema. Rodney Dangerfield wrote or rewrote all of his lines.
During pre-production, to prepare for the role of Wayne Gale, Downey spent time with Australian TV shock-king Steve Dunleavy, and later convinced Stone to allow him to portray Gale with an Australian accent. Also during pre-production, Stone tried to convince actress Juliette Lewis to bulk up for the role of Mallory so that she looked tougher, but she refused, saying she wanted the character to look like a pushover, not a bodybuilder.
Principal photography took only 56 days to shoot, but the editing process went on for eleven months, with the final film containing almost 3,000 cuts (most films have 600–700). Filming locations included the Rio Grande Gorge Bridge just west of Taos, New Mexico, where the wedding scene was filmed, and Stateville Correctional Center in Joliet, Illinois, where the prison riot was filmed. In Stateville, 80% of the prisoners are incarcerated for violent crimes. For the first two weeks on location at the prison, the extras were actual inmates with rubber weapons. For the subsequent two weeks, 200 extras were needed because the Stateville inmates were on lockdown. According to Tom Sizemore, during filming on the prison set, Stone would play African tribal music at full blast between takes to keep the frantic energy up. Whilst shooting the POV scene wherein Mallory runs into the wire mesh, director of photography Robert Richardson broke his finger and the replacement cameraman cut his eye. According to Oliver Stone, he wasn’t too popular with the camera department on set that day. For the scenes involving rear projection, the projected footage was shot prior to principal photography, then edited together, and projected onto the stage, behind the live actors. For example, when Mallory drives past a building and flames are projected onto the wall, this was shot live using footage projected onto the facade of a real building.
The famous Coca-Cola polar bear ad is seen twice during the film. According to Stone, Coca-Cola approved the use of the ad without having a full idea of what the film was about. When they saw the completed film, they were furious.
The film's soundtrack was produced by Stone and Trent Reznor of Nine Inch Nails, who reportedly watched the film over fifty times to "get in the mood". Reznor reportedly produced the soundtrack while on tour. On his approach to compiling the soundtrack, Reznor told MTV:
I suggested to Oliver [Stone] to try to turn the soundtrack into a collage-of-sound, kind of the way the movie used music: make edits, add dialog, and make it something interesting, rather than a bunch of previously released music.
Some songs were written especially for the film or soundtrack, such as "Burn" by Nine Inch Nails.
Style.
"Natural Born Killers" is shot and edited in a frenzied and psychedelic style consisting of black and white, animation, and other unusual color schemes, and employing a wide range of camera angles, filters, lenses and special effects. Much of the film is told via parodies of television shows, including a scene ("I Love Mallory") presented in the style of a sitcom about a dysfunctional family. Commercials which were commonly on the air at the time of the film's release make brief, intermittent appearances. In his DVD director's commentary, Stone goes into great detail about the look of the film, explaining scene by scene why a particular look was chosen for a particular scene.
Stone considered "Natural Born Killers" his road film, specifically naming "Bonnie and Clyde" as a source of inspiration. The famous death scene in "Bonnie and Clyde" used innovative editing techniques provided by multiple cameras shot from different angles at different speeds; this sporadic interchange between fast-paced and slow-motion editing that concludes Arthur Penn's film is used throughout the entirety of "Natural Born Killers".
Furthermore, both films fall under the road film genre through their constant challenges of the society in which the characters live. While Bonnie and Clyde attempt to disintegrate the weakened economic and social landscape of the 1930s, Mickey and Mallory try to free America from the overarching conventions which influence the common masses, primarily the media. However, whilst "Bonnie and Clyde" concludes with a pessimistic outlook regarding individual freedom within the American sphere of influence, Oliver Stone sees "Natural Born Killers" as having an optimistic finale. In "Bonnie and Clyde", the police's ambush of the couple exhibits the empirical control of law enforcement over the individual. "Natural Born Killers", however, ends with the couple symbolically destroying the mass media, as represented by Wayne Gale, and successfully fleeing together to live a relatively "normal" life. As Stone himself says, ""In its own way, "Natural Born Killers" is ultimately a very optimistic film about the future. It's about freedom, and the ability of every human being to get it"."
Release.
Box office.
In its opening weekend, "Natural Born Killers" grossed a total of $11,166,687 in 1,510 theaters. As of January 12, 2007, the film has grossed a total of $50,282,766 domestically, compared to its $34 million budget.
Critical response.
The film received a mixed to positive critical response. As of April 7, 2015, the review aggregator Rotten Tomatoes records an average response of 46%, based on 35 reviews. However, Metacritic records a score of 74 out of 100 based on 20 reviews. Roger Ebert of the "Chicago Sun-Times" gave the film four stars out of four and wrote, "Seeing this movie once is not enough. The first time is for the visceral experience, the second time is for the meaning." On his television show, his partner Gene Siskel agreed with him, adding extra praise to the scene featuring Rodney Dangerfield.
Other critics found the film unsuccessful in its aims. Hal Hinson of "The Washington Post" claimed that "Stone's sensibility is white-hot and personal. As much as he'd like us to believe that his camera is turned outward on the culture, it's vividly clear that he can't resist turning it inward on himself. This wouldn't be so troublesome if Stone didn't confuse the public and the private." Janet Maslin of "The New York Times" wrote, "for all its surface passions, "Natural Born Killers" never digs deep enough to touch the madness of such events, or even to send them up in any surprising way. Mr. Stone's vision is impassioned, alarming, visually inventive, characteristically overpowering. But it's no match for the awful truth."
James Berardinelli gave the film a negative review but his criticism was different from many other such pans, which generally said that Oliver Stone was a hypocrite for making an ultra-violent film in the guise of a critique of American attitudes. Berardinelli noted that the movie "hits the bullseye" as a satire of America's lust for bloodshed, but repeated Stone's main point so often and so loudly that it became unbearable.
Lionsgate Films released a director's cut on DVD. Stone himself retained ownership of his preferred cut. Distribution rights to the director's cut reverted from Lionsgate to Warner Bros. in 2009, giving Warner all distribution rights.
Controversies.
Censorship.
When the film was first handed in to the MPAA, they told Stone they would give it an NC-17 unless he cut it. As such, Stone toned down the violence by cutting approximately four minutes of footage, and the MPAA re-rated the film as an R. In 1996, a Director's Cut was released on home video by Vidmark Entertainment and Pioneer Entertainment, as Warner Bros. wanted nothing to do with that particular version. Warner Home Video later released this cut on Blu-ray.
The film was banned completely in Ireland, although it has since been unbanned.
In the UK, though the cinema release was delayed while the BBFC investigated reports that the film caused copycat murders in the USA and France, it was finally shown in cinemas in February 1995.
The original intended UK home video release in March 1996 was cancelled due to the Dunblane massacre in Scotland. In the meantime, Channel Five showed the film in November 1997. It was finally released on video in July 2001.
Stone has continually maintained that the film is a satire on how serial killers are adored by the media for their horrific actions and that those who claim that the violence in the film itself is a cause of societal violence miss the point of the film.
"Entertainment Weekly" ranked the film as the eighth most controversial film ever.
'Copycat' crimes.
From almost the moment of its release, the film has been accused of encouraging and inspiring numerous murderers in North America, including the Heath High School shooting and the Columbine High School massacre.
Soundtrack.
The soundtrack was released August 23, 1994 by Interscope Records.
Tracks 10, 13, 18, 20, 23, 25 are assembled from various recordings and dialogue from the film.
External links.
 
 

</doc>
<doc id="21181" url="http://en.wikipedia.org/wiki?curid=21181" title="Nancy Reagan">
Nancy Reagan

Nancy Davis Reagan (born Anne Frances Robbins; July 6, 1921) is the widow of the 40th President of the United States, Ronald Reagan, and was First Lady of the United States from 1981 to 1989.
Although she was born in New York City, her parents divorced and she grew up in Maryland, living with an aunt and uncle for some years. As Nancy Davis, she was an actress in Hollywood in the 1940s and 1950s, starring in films such as "Donovan's Brain", "Night Into Morning", and "Hellcats of the Navy" (the latter opposite Ronald Reagan). In 1952, she married Ronald, who was then president of the Screen Actors Guild, and they had two children. Reagan was the First Lady of California when her husband was Governor from 1967 to 1975, and she began to work with the Foster Grandparents Program.
Nancy Reagan became First Lady of the United States in January 1981 following her husband's election. She was criticized early in his first term largely due to her decision to replace the White House china, despite its being paid for by private donations. Nancy restored a Kennedy-esque glamour to the White House following years of lax formality, and her interest in high-end fashion garnered much attention, as well as criticism. She championed recreational drug prevention causes by founding the "Just Say No" drug awareness campaign, which was considered her major initiative as First Lady. More controversy ensued when it was revealed in 1988 that she, always protective of her husband, had consulted an astrologer to assist in planning the president's schedule after the 1981 assassination attempt on her husband. She had a strong influence on her husband and played a role in a few of his personnel and diplomatic decisions.
The Reagans retired to their home in Bel Air, Los Angeles, California in 1989. Nancy devoted most of her time to caring for her ailing husband, diagnosed in 1994 with Alzheimer's disease, until his death in 2004. Nancy Reagan has remained active within the Reagan Library and in politics, particularly in support of embryonic stem cell research.
Early life and education.
Anne Frances Robbins was born on July 6, 1921, at Manhattan's Sloane Hospital for Women in New York, as the only child of car salesman Kenneth Seymour Robbins (1894–1972) and his actress wife, Edith Luckett (1888–1987). Her godmother was silent-film-star Alla Nazimova. From birth, she was commonly called Nancy.
She lived her first two years in Flushing, Queens, in New York, in a two-story house on Roosevelt Avenue between 149th and 150th Streets. While her parents divorced soon after her birth, they had already been separated for some time. As her mother traveled the country to pursue acting jobs, Nancy was raised in Bethesda, Maryland, for the next six years by her aunt Virginia and uncle Audley Gailbraith. Nancy describes longing for her mother during those years: "My favorite times were when Mother had a job in New York, and Aunt Virgie would take me by train to stay with her."
In 1929, her mother married Loyal Davis (1896–1982), a prominent, politically conservative neurosurgeon who moved the family to Chicago. Nancy and her stepfather got along very well; Nancy would later write that he was "a man of great integrity who exemplified old-fashioned values." He formally adopted her in 1935, and she would always refer to him as her father. At the time of the adoption, her name was legally changed to Nancy Davis. She attended the Girls' Latin School of Chicago (describing herself as an average student), graduated in 1939, and later attended Smith College in Massachusetts, where she majored in English and Drama and graduated in 1943.
Acting career.
In 1940, a young Davis had appeared as a National Foundation for Infantile Paralysis volunteer in a memorable short subject shown in movie theaters to raise donations for the crusade against polio. "The Crippler" featured a sinister figure spreading over playgrounds and farms, laughing over its victims, until finally dispelled by the volunteer. It was very effective in raising contributions.
Following her graduation from college, Davis held jobs in Chicago as a sales clerk in Marshall Field's department store and as a nurse's aide. With the help of her mother's colleagues in theatre, including Zasu Pitts, Walter Huston, and Spencer Tracy, she pursued a career as a professional actress. She first gained a part in Pitts' 1945 road tour of "Ramshackle Inn", moving to New York City. She landed the role of Si-Tchun, a lady-in-waiting, in the 1946 Broadway musical about the Orient, "Lute Song", starring Mary Martin and a pre-stardom Yul Brynner. The show's producer told her, "You look like you could be Chinese."
After passing a screen test, she moved to California and signed a seven-year contract with Metro Goldwyn Mayer Studios (MGM) in 1949; she later remarked, "Joining Metro was like walking into a dream world." Her combination of attractive appearance – centered around her large eyes – and somewhat distant and understated manner made her hard at first for MGM to cast and publicize. Davis appeared in eleven feature films, usually typecast as a "loyal housewife," "responsible young mother," or "the steady woman." Jane Powell, Debbie Reynolds, Leslie Caron, and Janet Leigh were among those that she competed with for roles at MGM.
Davis' film career began with small supporting roles in two films of 1949, "The Doctor and the Girl" with Glenn Ford and "East Side, West Side" starring Barbara Stanwyck. She played a child psychiatrist in the film noir "Shadow on the Wall" (1950) with Ann Sothern and Zachary Scott; her performance was called "beautiful and convincing" by "New York Times" critic A. H. Weiler. She co-starred in 1950's "The Next Voice You Hear...", playing a pregnant housewife who hears the voice of God from her radio. Influential reviewer Bosley Crowther of "The New York Times" wrote that "Nancy Davis [is] delightful as [a] gentle, plain, and understanding wife." In 1951, Davis appeared in her favorite screen role, "Night Into Morning", a study of bereavement starring Ray Milland. Crowther said that Davis "does nicely as the fiancée who is widowed herself and knows the loneliness of grief," while another noted critic, "The Washington Post"'s Richard L. Coe, said Davis "is splendid as the understanding widow." MGM released Davis from her contract in 1952; she sought a broader range of parts, but also married Reagan, keeping her professional name as Davis, and had her first child that year. She soon starred in the 1953 science fiction film "Donovan's Brain"; Crowther said that Davis, playing the role of a possessed scientist's "sadly baffled wife," "walked through it all in stark confusion" in an "utterly silly" film. In her next-to-last movie, "Hellcats of the Navy" (1957), she played nurse Lieutenant Helen Blair and appeared in a film for the only time with her husband, playing what one critic called "a housewife who came along for the ride." Another reviewer, however, stated that Davis plays her part well, and "does well with what she has to work with."
Author Garry Wills believes that Davis was underrated as an actress overall because her constrained part in "Hellcats" was her most widely seen performance. In addition, Davis downplayed her Hollywood goals: MGM promotional material in 1949 said that her "greatest ambition" was to have a "successful happy marriage"; decades later, in 1975, she would say, "I was never really a career woman but [became one] only because I hadn't found the man I wanted to marry. I couldn't sit around and do nothing, so I became an actress." Ronald Reagan biographer Lou Cannon nevertheless characterized her as a "reliable" and "solid" performer who held her own in performances with better-known actors. After her final film, "Crash Landing" (1958), Davis appeared for a brief time as a guest star in television dramas such as "Wagon Train" and "The Tall Man" until 1962, when she retired as an actress. During her career, Davis served on the board of directors of the Screen Actors Guild for nearly ten years. Decades later, Albert Brooks attempted to coax her out of acting retirement by offering her the title role opposite himself in his 1996 film "Mother". She declined in order to care for her husband, and Debbie Reynolds played the part.
Marriage and family.
During her Hollywood career, Davis dated many actors, including Clark Gable, Robert Stack, and Peter Lawford; she later called Gable the nicest of the stars she had met. On November 15, 1949, she met Ronald Reagan, who was then president of the Screen Actors Guild. Nancy had noticed that her name had appeared on the Hollywood blacklist and sought Reagan's help to maintain her employment as a guild actress in Hollywood, and for assistance in having her name removed from the list. Reagan informed her that she had been confused with another actress of the same name. The two began dating and their relationship was the subject of many gossip columns; one Hollywood press account described their nightclub-free times together as "the romance of a couple who have no vices". Ronald Reagan was skeptical about marriage, however, following his painful 1948 divorce from Jane Wyman, and he still saw other women. After three years of dating, he eventually proposed to Davis in the couple's favorite booth at the Beverly Hills restaurant Chasen's. They married on March 4, 1952 in a simple ceremony designed to avoid the press at the Little Brown Church in the San Fernando Valley of Los Angeles. The only people in attendance were actor William Holden, the best man, and his wife, actress Brenda Marshall, the matron of honor. The couple's first child, Patricia Ann Reagan (better known by her professional name, Patti Davis), was born on October 21, 1952. Their son, Ronald Prescott Reagan, was born six years later on May 20, 1958. Nancy Reagan also became stepmother to Maureen Reagan (1941–2001) and Michael Reagan (born 1945), the children of her husband's first marriage to Jane Wyman.
Observers described Ronald and Nancy Reagan's relationship as intimate. As President and First Lady, the Reagans were reported to display their affection frequently, with one press secretary noting, "They never took each other for granted. They never stopped courting." Ronald often called Nancy "Mommy"; she called him "Ronnie". While the President was recuperating in the hospital after the 1981 assassination attempt, Nancy Reagan wrote in her diary, "Nothing can happen to my Ronnie. My life would be over." In a letter to Nancy, Ronald wrote, "whatever I treasure and enjoy ... all would be without meaning if I didn't have you." In 1998, while her husband was afflicted with Alzheimer's disease, Nancy told "Vanity Fair", "Our relationship is very special. We were very much in love and still are. When I say my life began with Ronnie, well, it's true. It did. I can't imagine life without him." Nancy was known for the focused and attentive look, termed "the Gaze", that she fastened upon her husband during his speeches and appearances. President Reagan's death in June 2004 ended what Charlton Heston called "the greatest love affair in the history of the American Presidency."
Nancy's relationship with her children was not always as close as that with her husband. She frequently quarreled with her biological children and her stepchildren. Her relationship with Patti was the most contentious; Patti flouted American conservatism and rebelled against her parents by joining the nuclear freeze movement and authoring many anti-Reagan books. The nearly 20 years of family feuding left her very much estranged from both her mother and father. Soon after her father was diagnosed with Alzheimer's disease, Patti and her mother reconciled and began to speak on a daily basis. Nancy's disagreements with Michael were also public matters; in 1984, she was quoted as saying that the two were in an "estrangement right now". Michael responded that Nancy was trying to cover up for the fact she had not met his daughter, Ashley, who had been born nearly a year earlier. They too eventually made peace. Nancy was thought to be closest to her stepdaughter Maureen during the White House years, but each of the Reagan children experienced periods of estrangement from their parents.
First Lady of California, 1967–1975.
Reagan was First Lady of California during her husband's two terms as governor. She disliked living in Sacramento, which lacked the excitement, social life, and mild climate to which she was accustomed in Los Angeles. She first attracted controversy early in 1967, when, after four months' residence in the California Governor's Mansion in Sacramento, she moved her family into a wealthy suburb because fire officials had labeled the mansion as a "firetrap". Though the Reagans leased the new house at their expense, the move was viewed as snobbish. Nancy defended her actions as being for the good of her family, a judgment with which her husband readily agreed. Friends of the family later helped support the cost of the leased house, while Nancy Reagan supervised construction of a new ranch-style governor's residence in nearby Carmichael. The new residence was finished just as Ronald Reagan left office in 1975, but his successor, Jerry Brown, refused to live there. It was sold in 1982, and California governors have been living in improvised arrangements ever since.
In 1967, Nancy Reagan was appointed by her husband to the California Arts Commission, and a year later was named "Los Angeles Times"' Woman of the Year; in its profile, the "Times" labeled her "A Model First Lady". Her glamour, style, and youthfulness made her a frequent subject for press photographers. As first lady, Reagan visited veterans, the elderly, and the handicapped, and worked with a number of charities. She became involved with the Foster Grandparents Program, helping to popularize it in the United States and Australia. She later expanded her work with the organization after arriving in Washington, and wrote about her experiences in her 1982 book "To Love a Child". The Reagans held dinners for former POWs and Vietnam War veterans while governor and first lady.
Role in 1976 and 1980 presidential campaigns.
Governor Reagan's term ended in 1975, and he did not run for a third; instead, he met with advisors to discuss a possible bid for the presidency in 1976, challenging incumbent President Gerald Ford. Reagan still needed to convince a reluctant Nancy before running, however. She feared for her husband's health and his career as a whole, though she felt that he was the right man for the job and eventually approved. Nancy took on a more traditional role in the campaign, holding coffees, luncheons, and talks with senior citizens. With that, she oversaw personnel, monitored her husband's schedule, and occasionally provided press conferences. The 1976 campaign included the so-called "battle of the queens", contrasting Nancy with First Lady Betty Ford. They both spoke out over the course of the campaign on similar issues, but with different approaches. Nancy was particularly upset by the warmonger image that the Ford campaign had drawn of her husband.
Though he lost the 1976 Republican nomination, Reagan ran again for the presidency in 1980 and succeeded in winning the nomination and election. During this second campaign, Nancy played a very prominent role and her management of staff became more apparent. She arranged a meeting among feuding campaign managers John Sears and Michael Deaver and her husband, which resulted in Deaver leaving the campaign and Sears being given full control. After the Reagan camp lost the Iowa caucus and fell behind in New Hampshire polls, Nancy organized a second meeting and decided it was time to fire Sears and his associates; she gave Sears a copy of the press release announcing his dismissal. Her influence on her husband became particularly notable; her presence at rallies, luncheons, and receptions increased his confidence.
First Lady of the United States, 1981–1989.
White House glamour.
Renovation.
Nancy Reagan became the First Lady of the United States when Ronald Reagan was inaugurated as president in January 1981. Early in her husband's presidency, Reagan stated her desire to create a more suitable "first home" in the White House, as the building had fallen into a state of disrepair following years of neglect. White House aide Michael Deaver described the second and third floor family residence as having "cracked plaster walls, chipped paint [and] beaten up floors"; rather than use government funds to renovate and redecorate, she sought private donations. In 1981, Nancy directed a major renovation of several White House rooms, including all of the second and third floors and rooms adjacent to the Oval Office, including the press briefing room. The renovation included repainting walls, refinishing floors, repairing fireplaces, and replacing antique pipes, windows, and wires. The closet in the master bedroom was converted into a beauty parlor and dressing room, and the West bedroom was made into a small gymnasium. 
The first lady secured the assistance of renowned interior designer Ted Graber, popular with affluent West Coast social figures, to redecorate the family living quarters. A Chinese-pattern, handpainted wallpaper was added to the master bedroom. Family furniture was placed in the president's private study. The first lady and her designer retrieved a number of White House antiques, which had been in storage, and placed them throughout the mansion. In addition many of Nancy Reagan's own collectibles were put out for display, including around twenty-five Limoges boxes as well as some porcelain eggs and a collection of plates. 
The extensive redecoration was paid for by private donations. Many significant and long-lasting changes occurred as a result of the renovation and refurbishment, of which Nancy Reagan said, "This house belongs to all Americans, and I want it to be something of which they can be proud." The renovations received some criticisms for being funded by tax-deductible donations, meaning some of it eventually did indirectly come from the tax-paying public.
Fashion.
Another of Nancy Reagan's trademarks was her interest in fashion. While her husband was still president-elect, press reports speculated about Nancy's social life and interest in fashion. In many press accounts, Nancy's sense of style was favorably compared to that of previous First Lady Jacqueline Kennedy. Friends and those close to her remarked that, while fashionable like Kennedy, she would be different from other first ladies; close friend Harriet Deutsch was quoted as saying, "Nancy has her own imprint."
Nancy Reagan's wardrobe consisted of dresses, gowns, and suits made by luxury designers, including James Galanos, Bill Blass and Oscar de la Renta. Her white, hand-beaded, one shoulder Galanos 1981 inaugural gown was estimated to cost $10,000, while the overall price of her inaugural wardrobe was said to cost $25,000. She favored the color red, calling it "a picker-upper", and wore it accordingly. Her wardrobe included red so often that the fire-engine shade became known as "Reagan red". She employed two private hairdressers who would style her hair on a regular basis in the White House.
Fashion designers were pleased with the emphasis Nancy Reagan placed on clothing. Adolfo said the first lady embodied an "elegant, affluent, well-bred, chic American look", while Bill Blass commented, "I don't think there's been anyone in the White House since Jacqueline Kennedy Onassis who has her flair." William Fine, president of cosmetic company Frances Denney, noted that she "stays in style, but she doesn't become trendy."
Though her elegant fashions and wardrobe were hailed as a "glamorous paragon of chic", they were also controversial subjects. In 1982, she revealed that she had accepted thousands of dollars in clothing, jewelry, and other gifts, but defended her actions by stating that she had borrowed the clothes and that they would either be returned or donated to museums, and that she was promoting the American fashion industry. Facing criticism, she soon said she would no longer accept such loans. While often buying her clothes, she continued to borrow and sometimes keep designer clothes throughout her time as first lady, which came to light in 1988. None of this had been included on financial disclosure forms; the non-reporting of loans under $10,000 in liability was in violation of a voluntary agreement the White House had made in 1982, while not reporting more valuable loans or clothes not returned was a possible violation of the Ethics in Government Act. Nancy expressed through her press secretary "regrets that she failed to heed counsel's advice" on disclosing them.
Despite the controversy, many designers who allowed her to borrow clothing noted that the arrangement was good for their businesses as well as for the American fashion industry overall. In 1989, Nancy was honored at the annual gala awards dinner of the Council of Fashion Designers of America, during which she received the council's lifetime achievement award. Barbara Walters said of her, "She has served every day for eight long years the word 'style.'"
Elegance and formality.
Approximately a year into her husband's first term, Nancy Reagan explored the idea of ordering new state china service for the White House. A full china service had not been purchased since the Truman administration in the 1940s, as only a partial service was ordered in the Johnson administration. She was quoted as saying, "The White House really badly, badly needs china." Working with Lenox, the primary porcelain manufacturer in America, the first lady chose a design scheme of a red with etched gold band, bordering the scarlet and cream colored ivory plates with a raised presidential seal etched in gold in the center. The full service comprised 4,370 pieces, with 19 pieces per individual set. The service totaled $209,508. Although it was paid for by private donations, some from the private Knapp Foundation, the purchase generated quite a controversy, for it was ordered at a time when the nation was undergoing an economic recession.
The new china, White House renovations, expensive clothing, and her attendance at the wedding of Charles and Diana, Prince and Princess of Wales, gave her an aura of being "out of touch" with the American people during an economic recession. This and her taste for splendor inspired the derogatory nickname "Queen Nancy". While Jacqueline Kennedy had also faced some press criticism for her spending habits, Reagan's treatment was much more consistent and negative. In an attempt to deflect the criticism, she self-deprecatingly donned a baglady costume at the 1982 Gridiron Dinner and sang "Second-Hand Clothes", mimicking the song "Second-Hand Rose". The skit helped to restore her reputation.
Nancy Reagan reflected on the criticisms in her 1989 autobiography, "My Turn". Reagan describes lunching with former Democratic National Committee chairman Robert Strauss, wherein Strauss said to her, "When you first came to town, Nancy, I didn't like you at all. But after I got to know you, I changed my mind and said, 'She's some broad!'" Nancy responded, "Bob, based on the press reports I read then, I wouldn't have liked me either!"
After the presidencies of Gerald Ford (who favored the Michigan fight song over "Hail to the Chief") and Jimmy Carter (who dramatically reduced the formality of presidential functions), Nancy brought a Kennedy-esque glamour back into the White House. She hosted 56 state dinners over eight years, compared to six by George and Laura Bush. She remarked that hosting the dinners is "the easiest thing in the world. You don't have to do anything. Just have a good time and do a little business. And that's the way Washington works." The White House residence staff found Nancy Reagan demanding to work for during the preparation for the state dinners, with the First Lady overlooking every aspect of meal presentations and sometimes requesting one dessert after another be prepared before finally settling on one she approved of.
In general, the First Lady's desire for everything to appear just right in the White House led the residence staff to consider her not easy to work for, with tirades following what she perceived as mistakes. One staffer later recalled, "I remember hearing her call for her personal maid one day and it scared the dickens out of me—just her tone. I never wanted to be on the wrong side of her." She did show loyalty and respect to a number of the staff. In particular, she came to the public defense of a maid who was indicted on charges of helping to smuggle ammunition to Paraguay, providing an affidavit to the maid's good character (even though it was politically inopportune to do so at the time of the Iran–Contra affair); charges were subsequently dropped and the maid returned to work at the White House. 
In 1987, Mikhail Gorbachev became the first Soviet leader to visit Washington, D.C. since Nikita Khrushchev in 1959, and Nancy Reagan was in charge of planning and hosting the important and highly anticipated state dinner, with the goal to impress both the Soviet leader and especially his wife Raisa Gorbachev. After the meal, Nancy recruited pianist Van Cliburn to sing a rendition of "Moscow Nights" for the Soviet delegation, to which Mikhail and Raisa broke out into song. Former Secretary of State George Shultz commented on the evening, saying "We felt the ice of the Cold War crumbling." Nancy concluded, "It was a perfect ending for one of the great evenings of my husband's presidency."
Just Say No.
Nancy Reagan launched the "Just Say No" drug awareness campaign in 1982, which was her primary project and major initiative as first lady. Nancy first became aware of the need to educate young people about drugs during a 1980 campaign stop in Daytop Village, New York. She remarked in 1981 that "Understanding what drugs can do to your children, understanding peer pressure and understanding why they turn to drugs is ... the first step in solving the problem." Her campaign focused on drug education and informing the youth of the danger of drug abuse.
In 1982, Nancy Reagan was asked by a schoolgirl what to do when offered drugs; Nancy responded "Just say no." The phrase proliferated in the popular culture of the 1980s and was eventually adopted as the name of club organizations and school anti-drug programs. Reagan became actively involved by traveling more than 250000 mi throughout the United States and several nations, visiting drug abuse prevention programs and drug rehabilitation centers. She also appeared on television talk shows, recorded public service announcements, and wrote guest articles. She appeared in an episode of the hit television drama "Dynasty" to underscore support for the anti-drug campaign. As she continued to promote "Just Say No", she appeared in an episode of the popular 1980s sitcom "Diff'rent Strokes" and in a 1985 rock music video, "Stop the Madness". When asked about her campaign, the first lady remarked, "If you can save just one child, it's worth it."
In 1985, Nancy expanded the campaign to an international level by inviting the First Ladies of various nations to the White House for a conference on drug abuse. On October 27, 1986, President Reagan signed a drug enforcement bill into law, which granted $1.7 billion in funding to fight the crisis and ensured a mandatory minimum penalty for drug offenses. Although the bill was criticized by some, Nancy Reagan considered it a personal victory. In 1988, she became the first First Lady invited to address the United Nations General Assembly, where she spoke on international drug interdiction and trafficking laws.
Critics of Reagan's efforts questioned their purpose and argued that the program did not go far enough in addressing many social issues, including unemployment, poverty, and family dissolution; Nancy's approach to promoting drug awareness was labeled as simplistic by liberal critics. Nonetheless, a number of "Just Say No" clubs and organizations remain in operation around the country, and they aim to educate children and teenagers about the effects of drugs.
Her husband's protector.
Nancy Reagan assumed the role of unofficial "protector" for her husband after the attempted assassination on his life in 1981. On March 30 of that year, President Reagan and three others were shot as they left the Washington Hilton Hotel. Nancy was alerted and arrived at George Washington University Hospital, where the President was hospitalized. She recalled having seen "emergency rooms before, but I had never seen one like this—with my husband in it." She was escorted into a waiting room, and when granted access to see her husband, he quipped to her, "Honey, I forgot to duck", borrowing the defeated boxer Jack Dempsey's jest to his wife.
An early example of her protective nature occurred when Senator Strom Thurmond entered the President's hospital room that day in March, passing the Secret Service detail by claiming he was the President's "close friend", presumably to acquire media attention. Nancy was outraged and demanded he leave. While the president recuperated in the hospital, the first lady slept with one of his shirts to be comforted by the scent. When Reagan was released from the hospital on April 12, she escorted him back to the White House.
Press accounts framed Nancy as her husband's "chief protector", an extension of their general initial framing of her as a helpmate and a Cold War domestic ideal. As it happened, the day after her husband was shot, Nancy fell off a chair while trying to take down a picture to bring to him in the hospital; she suffered several broken ribs, but was determined to not reveal it publicly. 
Influence in the White House.
Nancy stated in her memoirs, "I felt panicky every time [Ronald] left the White House" following the assassination attempt, and made it her concern to know her husband's schedule: the events he would be attending, and with whom. Eventually, this protectiveness led to her consulting an astrologer, Joan Quigley, who offered insight on which days were "good", "neutral", or should be avoided, which influenced her husband's White House schedule. Days were color-coded according to the astrologer's advice to discern precisely which days and times would be optimal for the president's safety and success.
The White House Chief of Staff, Donald Regan, grew frustrated with this regimen, which created friction between him and the First Lady. This escalated with the revelation of the Iran-Contra affair, an administration scandal, in which the First Lady felt Regan was damaging the president. She thought he should resign, and expressed this to her husband although he did not share her view. Regan wanted President Reagan to address the Iran-Contra matter in early 1987 by means of a press conference, though Nancy refused to allow Reagan to overexert himself due to a recent prostate surgery and astrological warnings. Regan became so angry with Nancy that he hung up on her during a 1987 telephone conversation. According to former ABC News correspondent Sam Donaldson, when the President heard of this treatment, he demanded—and eventually received—Regan's resignation. In his 1988 memoirs, Regan wrote about Nancy's consultations with the astrologer, the first public mention of them, which resulted in embarrassment for the First Lady. Nancy later wrote, "Astrology was simply one of the ways I coped with the fear I felt after my husband almost died... Was astrology one of the reasons [further attempts did not occur]? I don't "really" believe it was, but I don't "really" believe it wasn't."
Nancy Reagan wielded a powerful influence over President Reagan. Again stemming from the assassination attempt, she strictly controlled access to the president and even occasionally attempted to influence her husband's decision making.
Beginning in 1985, Nancy strongly encouraged her husband to hold "summit" conferences with Soviet General Secretary Gorbachev, and suggested they form a personal relationship beforehand. Both Ronald Reagan and Mikhail Gorbachev had developed a productive relationship through their summit negotiations. The relationship between Nancy Reagan and Raisa Gorbachev was anything but the friendly, diplomatic one between their husbands; Nancy found Raisa hard to converse with and their relationship was described as "frosty". The two women usually had tea, and discussed differences between the USSR and the United States. Visiting the U.S. for the first time in 1987, Raisa irked Reagan with lectures on subjects ranging from architecture to socialism, reportedly prompting the American President's wife to quip, "Who does that dame think she is?"
Press framing of Nancy changed from that of just helpmate and protector to someone with hidden power. As the image of her as a political interloper grew, she sought to explicitly deny that she was the power behind the throne. At the end of her time as First Lady, however, she said that her husband had not been well-served by his staff. She acknowledged her role in reaction in influencing him on personnel decisions, saying "In no way do I apologize for it." She wrote in her memoirs, "I don't think I was as bad, or as extreme in my power or my weakness, as I was depicted," but went on, "[H]owever the first lady fits in, she has a unique and important role to play in looking after her husband. And it's only natural that she'll let him know what she thinks. I always did that for Ronnie, and I always will."
Breast cancer.
In October 1987, a mammogram detected a lesion in Nancy Reagan's left breast and she was subsequently diagnosed with breast cancer. She chose to undergo a mastectomy rather than a lumpectomy and the breast was removed on October 17, 1987. Ten days after the operation, her mother, Edith Luckett Davis, died in Phoenix, Arizona, leading Nancy to dub the period "a terrible month".
After the surgery, more women across the country had mammograms, an example of the influence the First Lady possessed.
Subsequent life.
Though Nancy was a controversial First Lady, 56 percent of Americans had a favorable opinion of her when her husband left office on January 20, 1989, with 18 percent having an unfavorable opinion and the balance not giving an opinion. Compared to fellow First Ladies when their husbands left office, Reagan's approval was higher than those of Rosalynn Carter and Hillary Rodham Clinton, however she was less popular than Barbara Bush and her disapproval rating was double that of Carter's.
Upon leaving the White House, the couple returned to California, where they purchased a home in the wealthy East Gate Old Bel Air neighborhood of Bel Air, Los Angeles, dividing their time between Bel Air and the Reagan Ranch in Santa Barbara, California; Ronald and Nancy regularly attended Bel Air Presbyterian Church as well. After leaving Washington, Nancy made numerous public appearances, many on behalf of her husband. She continues to reside at the Bel Air home, where she lived with her husband until his death on June 5, 2004.
Early post-White House activities.
In late 1989, the former First Lady established the Nancy Reagan Foundation, which aimed to continue to educate people about the dangers of substance abuse. The Foundation teamed with the BEST Foundation For A Drug-Free Tomorrow in 1994, and developed the Nancy Reagan Afterschool Program. She continued to travel around the nation, speaking out against drug and alcohol abuse. After President Reagan revealed that he had been diagnosed with Alzheimer's disease in 1994, she made herself his primary caregiver and became actively involved with the National Alzheimer's Association and its affiliate, the Ronald and Nancy Reagan Research Institute in Chicago, Illinois.
Also in 1989 she published "My Turn: The Memoirs of Nancy Reagan", which gives an account of her life in the White House, speaking openly about her influence within the Reagan administration and discussing the myths and controversies that surrounded the couple. In 1991, the controversial author Kitty Kelley wrote an unauthorized and largely uncited biography about Nancy Reagan, repeating accounts of a poor relationship with her children and introducing rumors of alleged sexual relations with singer Frank Sinatra. A wide range of sources commented that Kelley's largely unsupported claims are most likely false.
In 1989 the Internal Revenue Service began investigating the Reagans for whether they owed additional tax on the gifts and loans of high-fashion clothes and jewelry to Nancy during their time in the White House (recipients benefiting from the display of such items recognize taxable income even if they are returned). In 1992 the IRS determined the Reagans had failed to include some $3 million worth of fashion items between 1983 and 1988 on their tax returns; they were billed for a large amount of back taxes and interest, which was subsequently paid.
Nancy Reagan was awarded the Presidential Medal of Freedom, the nation's highest civilian honor, by President George W. Bush on July 9, 2002. President Reagan received his own Presidential Medal of Freedom in January 1993. Nancy and her husband were jointly awarded the Congressional Gold Medal on May 16, 2002 at the Capitol, and were only the third President and First Lady to receive it; she accepted the medal on behalf of both of them.
Ronald Reagan's funeral.
Ronald Reagan died in their Bel Air home on June 5, 2004. During the seven-day state funeral, Nancy, accompanied by her children and military escort, led the nation in mourning by keeping a strong composure, traveling from her home to the Reagan Library for a memorial service, then to Washington, D.C., where her husband's body lay in state for 34 hours prior to a national funeral service in the Washington National Cathedral. She returned to the library in California for a sunset memorial service and interment, where, overcome with emotion, she lost her composure, crying in public for the first time during the week. After accepting the folded flag, she kissed the casket and mouthed "I love you" before leaving. Journalist Wolf Blitzer said of Reagan during the week, "She's a very, very strong woman, even though she looks frail."
She had directed the detailed planning of the funeral, including ordering all the major events and asking former President George H. W. Bush as well as former British Prime Minister Margaret Thatcher and former Canadian Prime Minister Brian Mulroney to speak during the National Cathedral Service. She paid very close attention to the details, something she had always done in her husband's life. Betsy Bloomingdale, one of Reagan's closest friends, stated, "She looks a little frail. But she is very strong inside. She is. She has the strength. She is doing her last thing for Ronnie. And she is going to get it right." The funeral marked her first major public appearance since delivering a speech to the 1996 Republican National Convention on her husband's behalf.
The funeral had a great impact on Reagan's public image. Following substantial criticism during her tenure as first lady, she was seen somewhat as a national heroine, praised by many for supporting and caring for her husband while he suffered from Alzheimer's disease. "U.S. News & World Report" opined, "after a decade in the shadows, a different, softer Nancy Reagan emerged."
Widowhood.
Reagan has remained active in politics, particularly relating to stem cell research. Beginning in 2004, she favored what many consider to be the Democratic Party's position, and urged President George W. Bush to support federally funded embryonic stem cell research in the hope that this science could lead to a cure for Alzheimer's disease. Although she failed to change the president's position, she did support his campaign for a second term.
In 2005, Reagan was honored at a gala dinner at the Ronald Reagan Building in Washington, D.C. where guests included Dick Cheney, Harry Reid and Condoleezza Rice. It was her first major public appearance since the funeral. Asked what her plans were, Reagan shook her head and responded, "I don't know. I'll know when I'll know. But the [Reagan] library is Ronnie, so that's where I spend my time."
In 2007, she attended the national funeral service for Gerald Ford in the Washington National Cathedral. Nancy Reagan hosted two 2008 Republican Presidential Candidates Debates at the Reagan Presidential Library, the first in May 2007 and the second in January 2008. While she did not participate in the discussions, she sat in the front row and listened as the men vying to become the nation's 44th president claimed to be a rightful successor to her husband. Though some speculation arose as to whether Reagan might support New York City Mayor Michael Bloomberg in a presidential bid, nothing came of it. She formally endorsed Senator John McCain, then the presumptive Republican party nominee, for president on March 25.
She attended the funeral of Lady Bird Johnson in Austin, Texas on July 14, 2007 and three days later accepted the highest Polish distinction, the Order of the White Eagle, on behalf of Ronald Reagan at the Reagan Library. The Reagan Library opened the temporary exhibit "Nancy Reagan: A First Lady's Style", which displayed over eighty designer dresses belonging to the first lady.
Nancy Reagan's health and well-being became a prominent concern in 2008. In February she suffered a fall at her Bel Air home and was taken to St. John's Health Center in Santa Monica, California. Doctors reported that she did not break her hip as feared and she was released from the hospital two days later. News commentators noted that Reagan's step had slowed significantly, as the following month she walked in very slow strides with John McCain. NBC's Brian Williams, who attended a dinner with Reagan in mid-2008, recalled, "Mrs. Reagan's vision isn't what it always was so she was taking very halting steps as a lot of folks her age do... [I]t is so important for folks in her age bracket and in her bracket of life to remain upright and captain of their own ship. She very much is captain of her own ship." As for her mental ability, Williams remarked, "She's as sharp as ever and enjoys a robust life with her friends in California, but [falling] is always a danger of course. She's a very stoic, hardy person full of joy and excitement for life... She is not without opinions on politics and political types these days... She is, as most of her friends described her, a pistol."
In October 2008, Reagan was admitted to Ronald Reagan UCLA Medical Center after falling at home. Doctors determined that the 87-year-old had fractured her pelvis and sacrum and could recuperate at home with a regimen of physical therapy. As a result of her mishap, medical articles were published containing information on how to prevent falls. In January 2009, Reagan was said to be "improving every day and starting to get out more and more."
In March 2009 she praised President Barack Obama for reversing the ban on federally funded embryonic stem cell research. She traveled to Washington, D.C. in June 2009 to unveil a statue of her late husband in the Capitol rotunda. She was also on hand as President Obama signed the Ronald Reagan Centennial Commission Act and lunched privately with Michelle Obama. Nancy revealed in an interview with "Vanity Fair" that Michelle Obama had telephoned her for advice on living and entertaining in the White House. Following the August 2009 death of Senator Edward M. Kennedy, she said she was "terribly saddened ... Given our political differences, people are sometimes surprised how close Ronnie and I have been to the Kennedy family. ... I will miss him." She attended the funeral of Betty Ford in Rancho Mirage, California, on July 12, 2011.
Reagan hosted a 2012 Republican presidential debate at the Reagan Presidential Library on September 7, 2011. Her representative said that she suffered a fall in March 2012. Two months later, she recovered from several broken ribs, which prevented her from attending a speech given by Paul Ryan in the Reagan Presidential Library in May 2012; her spokesperson said, "Mrs. Reagan has been recovering slowly and has been adding a few appointments back on to her schedule, but was advised by her doctor today not to try and attend large events too far from home just yet." She endorsed Republican presidential candidate Mitt Romney on May 31, 2012, explaining that her husband would have liked Romney's business background and what she called "strong principles". Following the April 2013 death of former British Prime Minister Margaret Thatcher, she said that, "The world has lost a true champion of freedom and democracy. ...Ronnie and I knew her as a dear and trusted friend, and I will miss her."
Filmography.
She also made a number of television appearances from 1953 through 1962, as a guest star in dramatic shows or installments of anthology series. These included "The Ford Television Theatre" (her first appearance with Ronald Reagan came during a 1953 episode titled "First Born"), "Schlitz Playhouse of Stars", "Wagon Train", "The Tall Man", and "General Electric Theater" (hosted by Ronald Reagan).

</doc>
<doc id="21182" url="http://en.wikipedia.org/wiki?curid=21182" title="New Brunswick">
New Brunswick

New Brunswick (French: "Nouveau-Brunswick"; ], ]) is one of Canada's three Maritime provinces and is constitutionally bilingual (English–French). It was created as a result of the partitioning of the British Colony of Nova Scotia in 1784. Fredericton is the capital and Saint John is the most populous city. In the 2011 nationwide census, Statistics Canada estimated the provincial population to have been 751,171. The majority of the population is English-speaking, but there is also a large Francophone minority (33%), chiefly of Acadian origin.
Etymology.
The province is named for the city of Braunschweig, formerly known in English as "Brunswick", located in modern day Lower Saxony in northern Germany (and also the former duchy of the same name). Braunschweig is the ancestral home of the British monarch George I and his successors.
Geography.
New Brunswick is bordered on the north by Quebec's Gaspé Peninsula and by Chaleur Bay. The eastern boundary is formed by the Gulf of Saint Lawrence and Northumberland Strait. The southeast corner of the province is connected to the Nova Scotia peninsula by the narrow Isthmus of Chignecto. The south of the province is bounded by the Bay of Fundy coast, (which with a rise of 16 m, has amongst the highest tides in the world). The US state of Maine forms the western boundary.
New Brunswick differs from the other Maritime provinces physiographically, climatologically, and ethnoculturally. Both Nova Scotia and Prince Edward Island are either surrounded by, or are almost completely surrounded by water. Oceanic effects therefore tend to define their climate, economy, and culture. On the other hand, New Brunswick, although having a significant seacoast, is sheltered from the Atlantic Ocean proper and has a large interior that is removed from oceanic influences. As a result, the climate tends to be more continental in character rather than maritime.
The major river systems of the province include the St. Croix River, Saint John River, Kennebecasis River, Petitcodiac River, Magaguadavic River, Miramichi River, Nepisiguit River, and the Restigouche River. Although smaller, the Bouctouche River, Richibucto River and Kouchibouguac River are also important. The settlement patterns and the economy of New Brunswick are based more on the province's river systems than its seacoasts. Because of this, New Brunswick's population centers tend to be less 'centralized' than in Prince Edward Island and Nova Scotia. Fredericton, Saint John, and Moncton all sit on rivers that have played a significant role in their economic history.
Northern New Brunswick is dominated by the Appalachian Mountains within the Eastern Canadian forests ecoregion, with the northwestern part of the province consisting of the remote and rugged Miramichi Highlands as well as the Chaleur Uplands and the Notre Dame Mountains, with a maximum elevation at Mount Carleton of 817 m. The New Brunswick Lowlands form the eastern and central portions of the province and are part of the Gulf of St. Lawrence lowland forests ecoregion. Finally the Caledonia Highlands and St. Croix Highlands extend along the Bay of Fundy coast reaching elevations of more than 400 m.
The total land and water area of the province is 72908 km2, over 80 percent of which is forested. Agricultural lands are found mostly in the upper Saint John River valley, with lesser amounts of farmland in the southeast of the province, especially in the Kennebecasis and Petitcodiac river valleys. The three major urban centres are all in the southern third of the province.
History.
The original First Nations inhabitants of New Brunswick were members of three distinct tribes. The largest tribe was the Mi'kmaq, and they occupied the eastern and coastal areas of the province. They were responsible for the Augustine Mound, a burial ground built about 800 B.C. near Metepnákiaq (Red Bank First Nation). The western portion of the province was the traditional home of the Wolastoqiyik (Maliseet) people. The smaller Passamaquoddy tribe occupied lands in the southwest of the province.
French colonial era.
Although it is possible that Vikings may have reached as far south as New Brunswick, the first known European exploration of New Brunswick was that of French explorer Jacques Cartier in 1534, who discovered and named the Bay of Chaleur. The next French contact was in 1604, when a party led by Pierre du Gua de Monts and Samuel de Champlain set up camp for the winter on St. Croix Island, between New Brunswick and Maine. The colony relocated the following year across the Bay of Fundy to Port Royal, Nova Scotia. Over the next 150 years, a number of other French settlements and seigneuries were founded in the area occupied by present-day New Brunswick, including along the Saint John River, the upper Bay of Fundy region, in the Tantramar Marshes at Beaubassin, and finally at St. Pierre (site of present day Bathurst). The whole maritime region (as well as parts of Maine) was at that time claimed by France and was designated as the colony of Acadia.
One of the provisions of the Treaty of Utrecht of 1713 was the surrender of peninsular Nova Scotia to the British. The bulk of the Acadian population thus found themselves residing in the new British colony of Nova Scotia. The remainder of Acadia (including the New Brunswick region) was only lightly populated and poorly defended. The Maliseet from their headquarters at Meductic on the Saint John River, participated in numerous raids and battles against New England during Father Rale's War and King William's War.
During Father Le Loutre's War, in 1750, in order to protect their territorial interests in what remained of Acadia, France built three forts (Fort Beauséjour, Fort Menagoueche and Fort Gaspareaux) along the frontier with Nova Scotia. (A major French fortification (Fortress of Louisbourg) was also built on Île Royale (now Cape Breton Island) after Queen Anne's War, but the function of this fort was mostly to defend the approaches to the colony of Canada, not Acadia.)
During the French and Indian War (1754–63), the British completed their conquest of Acadia and extended their control to include all of New Brunswick. Fort Beauséjour (near Sackville), Fort Menagoueche and Fort Gaspareaux were captured by a British force commanded by Lt. Col. Robert Monckton in 1755. Inside Fort Beauséjour, the British forces found not only French regular troops, but also Acadian irregulars. Governor Charles Lawrence of Nova Scotia used the discovery of Acadian civilians helping in the defence of the fort as a pretext to order the expulsion of the Acadian population from Nova Scotia. The Acadians of the recently captured Beaubassin and Petitcodiac regions were included in the expulsion order. Some of the Acadians in the Petitcodiac and Memramcook region escaped, and under the leadership of Joseph Broussard continued to conduct guerrilla action against the British forces for a couple of years. Other actions in the war included British expeditions up the Saint John River in the St. John River Campaign. Fort Anne (Fredericton) fell during the 1759 campaign, and following this, all of present-day New Brunswick came under British control.
British colonial era.
After the Seven Years' War, most of present day New Brunswick (and parts of Maine) were absorbed into the colony of Nova Scotia and designated as Sunbury County. New Brunswick's relatively isolated location on the Bay of Fundy, away from the Atlantic coastline proper tended to discourage settlement during the postwar period. There were exceptions however, such as the coming of New England Planters to the Sackville region and the arrival of Pennsylvania Dutch settlers in Moncton in 1766. In both these cases, many of the new settlers took up land that had originally belonged to displaced Acadians before the deportation.
There were several actions on New Brunswick soil during the American Revolutionary War: the Maugerville Rebellion (1776), the Battle of Fort Cumberland (1776), the Siege of Saint John (1777) and the Battle at Miramichi (1779). The Battle of Fort Cumberland was the largest and most significant of these conflicts. Following the war, significant population growth finally came to the area, when 14,000 refugee Loyalists, having lost the war, came from the newly created United States, arriving on the Saint John River in 1783. Influential Loyalists such as Harvard-educated Edward Winslow saw themselves as the natural leaders of their community and that they should be recognized for their rank and that their loyalty deserved special compensation. However they were not appreciated by the pre-loyalist population in Nova Scotia. As Colonel Thomas Dundas wrote from Saint John, "They [the loyalists] have experienced every possible injury from the old inhabitants of Nova Scotia." Therefore 55 prominent merchants and professionals petitioned for 5000 acre grants each. Winslow pressed for the creation of a "Loyalist colony" – an asylum that could become "the envy of the American states".
Nova Scotia was therefore partitioned. In 1784, Britain split the colony of Nova Scotia into three separate colonies: New Brunswick, Cape Breton Island, and present-day peninsular Nova Scotia, in addition to the adjacent colonies of St. John's Island (renamed Prince Edward Island in 1798) and Newfoundland. The Colony of New Brunswick was created on August 16, 1784; Sir Thomas Carleton was appointed as Lieutenant-Governor in 1784, and in 1785 a new assembly was established with the first elections. The new colony was almost called New Ireland after a failed attempt to establish a colony of that name in Maine during the war.
The province later gained control over its crown lands in 1837.
Even though the bulk of the Loyalist population was located in Parrtown (Saint John), the decision was made by the colonial authorities to place the new colonial capital at St. Anne's Point (Fredericton), about 150 km up the Saint John River as it was felt that by placing the capital inland, it would be less vulnerable to American attack. The University of New Brunswick was founded at Fredericton at the same time (1785), making it the oldest English-language university in Canada and the first public university in North America.
Initial Loyalist population growth in the new colony extended along the Fundy coastline from Saint Andrews to Saint Martins and up the Kennebecasis and lower Saint John River valleys.
During the late 18th and early 19th centuries, some of the deported Acadians from Nova Scotia found their way back to "Acadie," where they settled mostly along the eastern and northern shores of the new colony of New Brunswick. Here, they lived in relative (and in many ways, self-imposed) isolation.
Additional immigration to New Brunswick in the early part of the 19th century was from Scotland; western England; and Waterford, Ireland, often after first having come through (or having lived in) Newfoundland. A large influx of settlers arrived in New Brunswick after 1845 from Ireland as a result of the Potato Famine; many of these people settled in Saint John or Chatham. Both Saint John and the Miramichi region remain largely Irish today.
The northwestern border between Maine and New Brunswick had not been clearly defined by the Treaty of Paris (1783) that had ended the American Revolution. By the late 1830s, population growth and competing lumber interests in the upper Saint John River valley created the need for a definite boundary in the area. During the winter of 1838–39, the situation quickly deteriorated, with both Maine and New Brunswick calling out their respective militias. The "Aroostook War" was bloodless (but politically very tense), and the boundary was subsequently settled by the Webster-Ashburton Treaty of 1842.
Throughout the mid 19th century, shipbuilding on the Bay of Fundy shore and also on the Petitcodiac River and rivers on the east coast became a dominant industry in New Brunswick. The "Marco Polo", the fastest clipper ship ever built, was launched from Saint John in 1851. Resource-based industries such as logging and farming were also important components of the New Brunswick economy during this time and railways were constructed throughout the province to serve them and link the rural communities.
Canadian province.
New Brunswick, one of the four original provinces of Canada, entered the Canadian Confederation on 1 July 1867. The Charlottetown Conference of 1864, which ultimately led to the confederation movement, originally had been intended to discuss only a Maritime Union, but concerns over the American Civil War as well as Fenian activity along the border led to an interest in expanding the scope of the proposed union. This interest in an expanded union arose from the Province of Canada (formerly Upper and Lower Canada, later Ontario and Quebec), and a request was made by the Canadian political leaders to the organizers of the Maritime conference to have the meeting agenda altered.
Although the Maritime leaders were swayed by the arguments of the Canadians, many ordinary residents of the Maritimes wanted no part of this larger confederation for fear that their interests and concerns would be ignored in a wider national union. Many politicians who supported confederation, such as Sir Samuel Leonard Tilley (New Brunswick's best-known Father of Confederation), found themselves without a seat after the next election; nevertheless, backers of the wider confederation eventually prevailed.
Following confederation, the fears of the anti-confederates were proven correct as new national policies and trade barriers were soon adopted by the central government, thus disrupting the historic trading relationship between the Maritime Provinces and New England. The situation in New Brunswick was exacerbated by both the Great Fire of 1877 in Saint John and the decline of the wooden shipbuilding industry; skilled workers were thus forced to move to other parts of Canada or to the United States to seek employment.
As the 20th century dawned, however, the province's economy again began to expand. Manufacturing gained strength with the construction of textile mills such as the St. Croix Cotton Mill; and in the crucial forestry sector, the sawmills that had dotted inland sections of the province gave way to larger pulp and paper mills. The railway industry, meanwhile, provided for growth and prosperity in the Moncton region. Nevertheless, unemployment remained high throughout the province, and the Great Depression brought another setback. Two influential families, the Irvings and the McCains, emerged from the Depression to begin to modernise and vertically integrate the provincial economy—especially in the vital forestry, food processing, and energy sectors. In the mid-1960s, forestry practices changed from the controlled harvests of a commodity to the cultivation of the forests. New Brunswick changed from more than two-thirds rural before 1941 to predominantly urban by 1971. Education and health care were poorly funded, and in the 1940s and 1950s the rates of illiteracy and infant mortality were among the highest in Canada. During the period 1950-1980, 80% of New Brunswick's small farms disappeared, as the agroindustry took root.
The Acadians in northern New Brunswick had long been geographically and linguistically isolated from the more numerous English speakers, who lived in the south of the province. The population of French origin grew dramatically after Confederation, from about 16 per cent in 1871 to 24 per cent in 1901 and 34 per cent in 1931. Government services were often not available in French, and the infrastructure in predominantly Francophone areas was noticeably less developed than in the rest of the province; this changed with the election of Premier Louis Robichaud in 1960. He embarked on the ambitious Equal Opportunity Plan, in which education, rural road maintenance, and health care fell under the sole jurisdiction of a provincial government that insisted on equal coverage throughout the province. County councils were abolished, and the rural areas came under direct provincial jurisdiction. The 1969 Official Languages Act made French an official language.
Demography.
Ethnicity.
First Nations in New Brunswick include the Mi'kmaq and Maliseet (Wolastoqiyik). The first European settlers, the Acadians, are today descendants of survivors of the Great Expulsion (1755), which drove thousands of French residents into exile in North America, Britain, and France for refusing to take an oath of allegiance to King George III during the Seven Years' War (French and Indian War). Acadians who were deported to Louisiana are often referred to as Cajuns in English.
Much of the English Canadian population of New Brunswick is descended from Loyalists who fled the American Revolution, including a considerable number of Black Loyalists. This is commemorated in the province's motto, "Spem reduxit" ("hope restored"). There is also a significant population with Irish ancestry, especially in Saint John and the Miramichi Valley. People of Scottish descent are scattered throughout the province, with higher concentrations in the Miramichi and in Campbellton.
In the 2001 Canadian census, the most commonly reported ethnicities were 193,470 French (26.9%); 165,235 English (23.0%); 135,835 Irish (18.9%); 127,635 Scottish (17.7%); 27,490 German (3.8%); 26,220 Acadians (3.6%); 23,815 "North American Indian" (First Nations) (3.3%); 13,355 Dutch (Netherlands) (1.9%); and 7,620 Welsh (1.1%). It should be noted that 242,220 people (33.7%) identified themselves as simply "Canadian" or "Canadien," while 173,585 (24.1%) also selected another ethnicity—for a total of 415,810 (57.8%) calling themselves Canadian. (Each person could choose more than one ethnicity.)
Languages.
The 2011 Canadian census showed a population of 751,171. Of the 731,855 single responses to the census question concerning mother tongue, the most commonly reported languages were:
New Brunswick's official languages are shown in bold. Figures shown are for the number of single-language responses and the percentage of total single-language responses. During the 19th century Scottish Gaelic was also spoken in the Campbellton and Dalhousie area. The language died out as a natively-spoken language in the province in the early 20th century.
Religion.
The largest denominations by number of adherents according to the 2011 National Household Survey were the Roman Catholic Church, with 366,000 (52%); Baptists, with 70,990 (8%); the United Church of Canada, with 54,265 (7%); the Anglicans, with 51,365 (7%); the Pentecostals with 18,435 (3%).
Economy.
New Brunswick's urban areas have modern, service-based economies dominated by the health care, educational, retail, finance, and insurance sectors. These sectors are reasonably equitably distributed in all three principal urban centres. In addition, heavy industry and port facilities are found in Saint John; Fredericton is dominated by government services, universities, and the military; and Moncton has developed as a commercial, retail, transportation, and distribution centre with important rail and air terminal facilities.
The rural primary economy is best known for forestry, mining, mixed farming, and fishing.
The US is the province's largest export market, accounting for 92% of a foreign trade valued in 2014 at C$12,964bn. Refined petroleum accounts for 63% of the total, followed by seafood products, pulp, paper and sawmill products and non-metallic minerals (chiefly potash). 
Forestry is important in all areas of the province but especially in the heavily forested central regions. There are many sawmills in the smaller towns and large pulp and paper mills located in Saint John, Atholville, Miramichi, Nackawic, and Edmundston.
Heavy metals, including lead and zinc, are mined in the north around Bathurst. One of the world's largest potash deposits is located in Sussex; a second potash mine, costing over a billion dollars, is in development in the Sussex region. Oil and natural gas deposits are also being developed in the Sussex region.
Farming is concentrated in the upper Saint John River valley (in the northwest portion of the province), where the most valuable crop is potatoes. Mixed and dairy farms are found elsewhere, but especially in the southeast, concentrated in the Kennebecasis and Petitcodiac river valleys.
The most valuable fish catches are lobster, scallops and snow crab. The farming of Atlantic salmon in the Passamaquoddy Bay region is an important local industry.
The largest employers in the province are the Irving group of companies, several large multinational forest companies, the government of New Brunswick, and the McCain Foods group of companies.
In the 2014-15 fiscal year, provincial debt reached $12.2 billion or 37.7 per cent of nominal GDP. This represented a significant increase over the $10.1 billion recorded in 2011-12, when provincial debt was 32.2 per cent of provincial GDP. Although the province has a Fiscal Responsibility and Balanced Budget Act, the governments of Shawn Graham and David Alward both ran large deficits to place their constituents in a precarious position. The Auditor-General compared the public finances of the province unfavourably with both Manitoba and Saskatchewan in 2013.
Government of New Brunswick.
New Brunswick has a unicameral legislature with 49 seats. Elections are held at least every five years, but may be called at any time by the Lieutenant Governor (the viceregal representative) on consultation with the Premier. The Premier is the leader of the party that holds the most seats in the legislature.
There are two dominant political parties in New Brunswick, the Liberal Party and the Progressive Conservative Party. While consistently polling approximately 10% of the electoral vote since the early 1980s, the New Democratic Party has elected few members to the Legislative Assembly. From time to time, other parties, such as the Confederation of Regions Party, have held seats in the legislature, but only on the strength of a strong protest vote.
The dynamics of New Brunswick politics are different from those of other Canadian provinces. The lack of a dominant urban centre in the province means that the government has to be responsive to issues affecting all areas of the province. In addition, the presence of a large Francophone minority dictates that consensus politics is necessary, even when there is a majority government present. In this manner, the ebb and flow of New Brunswick provincial politics parallels the federal stage.
Since 1960, the province has tended to elect a succession of young bilingual leaders. Former Premier Bernard Lord (Progressive Conservative) once was touted as a potential leader of the Conservative Party of Canada. Frank McKenna (premier, 1987–97), had been considered the Liberal Party of Canada leadership material. Richard Hatfield (premier, 1970–87) played an active role in the patriation of the Canadian constitution and creation of the Canadian Charter of Rights and Freedoms. Louis Robichaud (premier, 1960–70) was responsible for a wide range of social reforms.
On September 21, 2014, the Liberal Party won the provincial election making 32-year-old Brian Gallant the new Premier. The Liberals won 27 seats (before recounts), the Conservatives won 21, and the Green Party won its first seat. In the prior election on September 27, 2010, the Progressive Conservatives won a large majority with 42 out of 55 seats by taking 16 formerly Liberal seats, making David Alward the new Premier of New Brunswick. "see New Brunswick general election, 2010"
A September 2010 report released by the Canadian Taxpayers Federation criticized the pensions made by members of the legislative assembly, which take 16 taxpayer dollars for every dollar contributed by the Member of the Legislative Assembly (MLA) and cost taxpayers $7.6 million annually. According to the organization, New Brunswick legislators have one of the richest pension plans in the country, after voting for an 85 percent increase two years ago.
Municipalities.
Saint John is a historic city and popular port of call. The Loyalist City, as it is often referred to, is the largest in the province and the oldest in the country. The city is one of the busiest shipping ports in Canada in terms of gross tonnage. Saint John has become a major energy hub for the East Coast. It is the home of Canada's biggest oil refinery and an LNG terminal has also been constructed in the city. In addition, the public owns large oil-fired and nuclear power plants, which are located in or near the city. Due to recent prosperity, the retail, commercial, and residential sectors are currently experiencing a resurgence. Saint John is arguably the most 'Maritime' city in New Brunswick, both in terms of its culture and traditional industries. Unlike Moncton and (to a lesser extent) Fredericton, Saint John does not have a significant French speaking population, making its cultural experience much more akin to cities in PEI and Nova Scotia.
Moncton is the fastest growing metropolitan area in the province and is among the top ten fastest growing urban areas in Canada. Its economy is principally based on the transportation, distribution, information technology, commercial, and retail sectors. Moncton has a sizable Francophone Acadian minority population (35%) and became officially bilingual in 2002. Since the city's transition to bilingualism, Moncton has experienced an upsurge in French in-migration from elsewhere in the province. The depopulation of the Acadian Peninsula and other areas in Northern New Brunswick are partially a result of French New Brunswickers seeking new opportunities in urban centers like Moncton and it's sister city, Dieppe.
Fredericton, the capital of the province, is home to the Beaverbrook Art Gallery, the University of New Brunswick, and St. Thomas University. One of Canada's largest military bases, CFB Gagetown, is located near suburban Oromocto; which is situated just east of Fredericton. The economy of Fredericton is intricately tied to the governmental, military, and university sectors. Fredericton is also one of the few urban centers in Atlantic Canada that sits significantly far inland (~100 km), making its city-scape unique to the region.
Education.
Public education in the province is administered by the Department of Education, a department of the Government of New Brunswick.
New Brunswick has a comprehensive parallel system of Anglophone and Francophone public schools providing education to both the primary and secondary levels. These schools are segregated by government decree. The English system developed out of a mixture of the British and American systems, reflecting the Loyalist background of so many early settlers. There are also secular and religious private schools in the province.
The New Brunswick Community College system has campuses in all regions of the province. This comprehensive trade school system offers roughly parallel programs in both official languages at either Francophone or Anglophone campuses. Each campus, however, tends to have areas of concentration to allow for specialization. There are also a number of private colleges for specialised training in the province, such as the Moncton Flight College, one of the top pilot-training academies in Canada.
There are four publicly funded secular universities and four private degree-granting institutions with religious affiliation in the province. The two comprehensive provincial universities are the University of New Brunswick and the Université de Moncton. These institutions have extensive postgraduate programs and Schools of Law. Medical education programs have also been established at both the Université de Moncton and at UNBSJ in Saint John (although affiliated with Universite de Sherbrooke and Dalhousie University respectively). Mount Allison University in Sackville is currently ranked as the best undergraduate liberal arts university in Canada and has produced 51 Rhodes Scholars, more than any other liberal arts university in the Commonwealth.
Publicly funded undergraduate liberal arts universities
Private Christian undergraduate liberal arts university
Private degree-granting religious training institutions
Culture.
Early New Brunswick culture was aboriginal in flavour, influenced by the native populations who made their home along the coast and riverbanks until the arrival of French-speaking in the early 17th century and English-speaking settlers beginning in the mid 18th century. Aboriginal culture in turn quickly came under European influence through trade and religion. Even writing was affected; see for example, Mi'kmaq hieroglyphic writing. Aboriginal societies were gradually marginalized under the reserve system, and it was not until the late nineteenth century, through the work of Silas Rand, that the tales of Glooscap began to emerge.
As described by the political historian Arthur Doyle, an invisible line separated the two founding European cultures, beginning on the eastern outskirts of Moncton and running diagonally across the province northwest towards Grand Falls. Franco-New Brunswick (Acadie) lay to the northeast of this divide, and Anglo-New Brunswick lay to the southwest.
Doyle's characterization was made not long after government reforms by former premier Louis J. Robichaud had significantly improved the status of French-speaking Acadians within the province and initiated their journey towards cultural recognition and equality with their English-speaking counterparts.
Early New Brunswick was influenced by its colonial ties to France, England, Scotland, and Ireland as well as by its geographical proximity to New England and the arrival of about 40,000 Loyalists in 1783.
As local society was founded in forestry and seaborne endeavours, a tradition of lumber camp songs and sea shanties prevailed. Acadian cloggers and Irish and Scots step dancers competed at festivals to expressive fiddle and accordion music. The art of storytelling, well-known to the native populations, passed on to the early settlers, and poetry—whether put to music or not—was a common form of commemorating shared events, as the voice of a masterful poet or soulful musician easily conquered the province's language barriers.
Other cultural expressions were found in family gatherings and the church; both French and English cultures saw a long and early influence of ecclesiastical architecture, with Western European and American influences dominating rather than a particular vernacular sense. Poets produced the first important literary contributions in the province. Cousins Bliss Carman and Sir Charles G.D. Roberts found inspiration in the landscape, as would later writers as well. In painting, individual artists such as Anthony Flower worked in obscurity, either through design or neglect, while others such as Edward Mitchell Bannister left the province before ever developing a local influence.
Few 19th-century artists emerged, but those who did often benefited from fine arts training at Mount Allison University in Sackville, which began offering classes in 1854. The program came into its own under John A. Hammond, who served from 1893 to 1916. Alex Colville and Lawren Harris later studied and taught art there and both Christopher Pratt and Mary Pratt were trained at Mount Allison. The University’s art gallery – which opened in 1895 and is named for its patron, John Owens of Saint John – is Canada’s oldest.
In French-speaking New Brunswick, it would not be until the 1960s that a comparable institution was founded, the Université de Moncton. Then, a cultural renaissance occurred under the influence of Acadian historians and such teachers as Claude Roussel and through coffeehouses, music, and protest. An outpouring of Acadian art, literature, and music has pressed on unabated since that time. Popular exponents of modern Acadian literature and music include Antonine Maillet, Édith Butler and France Daigle. A recent New Brunswick Lieutenant-Governor, Herménégilde Chiasson, was a poet. In northwest New Brunswick and neighbouring Quebec and northern Maine, a separate French-speaking group, the Brayon, have fostered such important artists as Roch Voisine and Lenny Breau. "(See also "Music of New Brunswick)"
Dr. John Clarence Webster and Max Aitken, 1st Baron Beaverbrook have made important endowments to provincial museums. Dr. Webster gave his art collection to the New Brunswick Museum in 1934, thereby endowing the museum with one of its greatest assets, James Barry's "Death of General Wolfe", which ranks as a Canadian national treasure. Courtesy of Lord Beaverbrook, the Beaverbrook Art Gallery in Fredericton has a collection of world-renowned art, including works by Salvador Dalí and J. M. W. Turner.
The performing arts have a long tradition in New Brunswick, dating back to travelling road shows and 19th-century opera in Saint John. The early recording star Henry Burr was discovered at the Imperial Theatre in Saint John. Based in Fredericton, the most important proponent of theatre today is Theatre New Brunswick, originally under the direction of Walter Learning, which tours plays around the province; Canadian playwright Norm Foster saw his early works premiere at TNB. Other live theatre troops include Théâtre l’Escaouette in Moncton, the Théatre populaire d'Acadie in Caraquet, and Live Bait Theatre in Sackville. All three major cities have significant performance spaces. The refurbished Imperial and Capitol Theatres are found in Saint John and Moncton, respectively; the more modern Playhouse is located in Fredericton.
In modern literature, writers Alfred Bailey and Alden Nowlan dominated the New Brunswick literary scene in the last third of the 20th century and world-renowned literary critic Northrop Frye was influenced by his upbringing in Moncton. The annual Frye Festival in that city celebrates his legacy. The expatriate British poet John Thompson, who settled outside Sackville, proved influential in his short-lived career. Douglas Lochhead and K. V. Johansen are other prominent writers living in the town of Sackville. David Adams Richards, born in the Miramichi, has become a well-respected Governor-General's Award-winning author. Canadian novelist, story-writer, biographer and poet, Raymond Fraser, grew up in Chatham and lives now in Fredericton.
The Atlantic Ballet Theatre of Canada, based in Moncton and featuring Russian and European trained dancers, has recently flourished and has started touring both nationally and internationally. Symphony New Brunswick, based in Saint John, also tours extensively in the province.
New Brunswick differs culturally from Prince Edward Island and Nova Scotia in a number of ways. Because of the provinces's sizable French speaking population, French-Canadian culture (specifically Acadian) permeates many parts of New Brunswick society. Likewise, New Brunswick's proximity to the United States effects the everyday life of people that live close to the 'line'. New Brunswick shares more border crossings with the U.S. State of Maine than any other Province/State share in North America. Furthermore, the well-known Maritime dialect so recognizable in Nova Scotia and PEI becomes 'watered down' the further west (and north) you move in the Province. While some areas, like Saint John, strongly share in the Maritime cultural experience, a number of population centers in New Brunswick have more in common with communities in Maine than they do with Halifax or Charlottetown. New Brunswick has been coined by many as Canada's 'Drive-thru Province.' While this title is used in jest, there is truth behind the fact that the sheer distances between major population centers in New Brunswick do a lot to transform culture from place to place. Despite being within the same provincial boundaries, Moncton, Saint John, and Fredericton differ culturally, economically, and geographically in significant ways.
Media.
New Brunswick has four daily newspapers (three of which are in English), the "Times & Transcript", based in Moncton and serving eastern New Brunswick. Also, there is the "Telegraph-Journal", based in Saint John and is distributed province-wide, and the provincial capital daily "The Daily Gleaner", based in Fredericton. The French-language daily is "L'Acadie Nouvelle", based in Caraquet. There are also several weekly newspapers that are local in scope and based in the province's smaller towns and communities.
The three English-language dailies and the majority of the weeklies are owned and operated by Brunswick News, privately owned by J.K. Irving. The other major media group in the province is Acadie Presse, which publishes "L'Acadie Nouvelle".
The Canadian Broadcasting Corporation has various news bureaus throughout the province, but its main Anglophone television and radio operations are centred in Fredericton. Télévision de Radio-Canada (CBC French) service is based in Moncton. Global TV is based in Halifax, with news bureaus in Fredericton, Moncton and Saint John. CTV Atlantic, the regional CTV station, is based in Halifax and has offices in Moncton, Fredericton, and Saint John. Western New Brunswick is served by WAGM-TV which broadcasts CBS and Fox stations into the province and covers New Brunswick news and weather on its "NewsSource 8" broadcasts.
There are many private radio stations in New Brunswick, with each of the three major cities having a dozen or more stations. Most smaller cities and towns also have one or two stations. Due to all this many Regional interests, were crested in New Brunswick.
Tourism.
New Brunswick is divided into five scenic drives: Fundy Coastal Drive, Acadian Coastal Drive, River Valley Scenic Drive, Miramichi River Route and Appalachian Range Route. Provincial and Municipal Visitor Information Centres are located throughout each drive.
Aside from Saint John's large tourism industry from cruise ships, some of the province's tourist attractions include the New Brunswick Museum, Minister's Island, the Beaverbrook Art Gallery, Grand Manan Island, Kings Landing Historical Settlement, Village Historique Acadien, Les Jardins de la République, Hopewell Rocks, La Dune de Bouctouche, Saint John Reversing Falls, Magnetic Hill and the Magnetic Hill Zoo, Crystal Palace, Magic Mountain Water Park, Casino New Brunswick, Cape Jourimain National Wildlife Preserve, Sackville Waterfowl Park, and the 41 km Fundy Hiking Trail.
Parks.
Provincial Parks: de la République, Herring Cove, Mactaquac, Mount Carleton, Murray Beach, New River Beach, Parlee Beach, Sugarloaf, The Anchorage
National Parks: Fundy National Park, Kouchibouguac National Park
International Parks: Roosevelt Campobello International Park
See also.
Lists:
Further reading.
</dl>

</doc>
<doc id="21184" url="http://en.wikipedia.org/wiki?curid=21184" title="Nova Scotia">
Nova Scotia

Nova Scotia (Latin for "New Scotland", pronounced in English as ) is one of Canada's three Maritime provinces and constitutes one of the four Atlantic Canada provinces. Located almost exactly halfway between the Equator and the North Pole (44º 39' N Latitude), its provincial capital is Halifax. Nova Scotia is the second-smallest province in Canada, with an area of 55284 km2, including Cape Breton Island and another 3,800 coastal islands. As of 2011, the population was 921,727, making Nova Scotia the second-most-densely populated province in Canada.
Etymology.
"Nova Scotia" means "New Scotland" in Latin and is the recognized English-language name for the province. In Scottish Gaelic, the province is called Alba Nuadh, which also simply means New Scotland. The province was first named in the 1621 Royal Charter granting the right to settle lands including modern Nova Scotia, Cape Breton Island, Prince Edward Island, New Brunswick, and the Gaspé peninsula to Sir William Alexander in 1632.
Geography.
Nova Scotia is Canada's second-smallest province in area after Prince Edward Island. The province's mainland is the Nova Scotia peninsula surrounded by the Atlantic Ocean, including numerous bays and estuaries. Nowhere in Nova Scotia is more than 67 km from the ocean. Cape Breton Island, a large island to the northeast of the Nova Scotia mainland, is also part of the province, as is Sable Island, a small island notorious for its shipwrecks, approximately 175 km from the province's southern coast.
Climate.
Nova Scotia lies in the mid-temperate zone. Since the province is almost entirely surrounded by the sea, the climate is closer to maritime than to continental climate. The winter and summer temperature extremes of the continental climate are moderated by the ocean.
Described on the provincial vehicle-licence plate as Canada's Ocean Playground, Nova Scotia is surrounded by four major bodies of water: the Gulf of Saint Lawrence to the north, the Bay of Fundy to the west, the Gulf of Maine to the southwest, and Atlantic Ocean to the east.
History.
Overview.
The province includes regions of the Mi'kmaq nation of Mi'kma'ki (mi'gama'gi). Nova Scotia was already home to the Mi'kmaq people when the first European colonists arrived. In 1605, French colonists established the first permanent European settlement in Canada and the first north of Florida at Port Royal, founding what would become known as Acadia.
The British conquest of Acadia took place in 1710. It was formally recognized in 1713 by the Treaty of Utrecht, when Cape Breton Island (Île Royale) was returned to the French. What is now New Brunswick was still a part of the French colony of Acadia. The name of the capital was changed from Port Royal to Annapolis Royal, Nova Scotia. In 1749, the capital of Nova Scotia was changed from Annapolis Royal to the newly established Halifax. In 1755, the vast majority of the French population (the Acadians) were expelled and replaced by New England Planters who arrived between 1759 and 1768.
In 1763, most of Acadia (Cape Breton Island, St. John's Island (now Prince Edward Island), and New Brunswick) became part of Nova Scotia. In 1769, St. John's Island became a separate colony. Nova Scotia included present-day New Brunswick until that province was established in 1784, after the arrival of United Empire Loyalists. In 1867, Nova Scotia was one of the four founding provinces of the Canadian Confederation.
17th and 18th centuries.
The history of Nova Scotia was significantly influenced by the warfare that took place on its soil during the 17th and 18th centuries. The Mi’kmaq had lived in Nova Scotia for centuries. The French arrived in 1604, and Catholic Mi’kmaq and Acadians were the predominant populations in the colony for the next 150 years. During the first 80 years the French and Acadians were in Nova Scotia, there were nine significant battles as the English and Scottish (later British), Dutch and French fought for possession of the colony. These battles happened at Port Royal, Saint John, Cap de Sable (present-day Port La Tour, Nova Scotia), Jemseg and Baleine. During the 17th Century there was the Acadian Civil War (1640–45).
Beginning with King William's War in 1688, there were six wars in Nova Scotia before the French were defeated and peace was made with the Acadians and Mi’kmaq:
The battles during these wars were primarily fought at Port Royal, Saint John, Canso, Chignecto, Dartmouth, Lunenburg and Grand-Pré. Despite the British Conquest of Acadia in 1710, Nova Scotia remained primarily occupied by Catholic Acadians and Mi'kmaq, who confined British forces to Annapolis and Canso. A generation later, Father Le Loutre's War began when Edward Cornwallis arrived to establish Halifax with 13 transports on June 21, 1749. A General Court, made up of the governor and the Council, was the highest court in the colony at the time. Jonathan Belcher was sworn in as chief justice of the Nova Scotia Supreme Court on 21 October 1754. The first legislative assembly in Halifax, under the Governorship of Edward Cornwallis, met on 2 October 1758. During the French and Indian War (North American theatre of the Seven Years' War), the British deported the Acadians and recruited New England Planters to resettle the colony. The seventy-five-year period of war ended with the Burial of the Hatchet Ceremony between the British and the Mi'kmaq (1761). After the war, some Acadians were allowed to return and the British made treaties with the Mi’kmaq.
The American Revolution (1775–1783) had a significant impact on shaping Nova Scotia. At the beginning, there was ambivalence in Nova Scotia, "the 14th American Colony" as some called it, over whether the colony should join the Americans in the war against Britain and rebellion flared at the Battle of Fort Cumberland and the Siege of Saint John (1777). Throughout the war, American privateers devastated the maritime economy by capturing ships and looting almost every community outside of Halifax. These American raids alienated many sympathetic or neutral Nova Scotians into supporting the British. By the end of the war a number of Nova Scotian privateers were outfitted to attack American shipping. British military forces based at Halifax were successful in preventing American support for rebels in Nova Scotia and deterred any invasion of Nova Scotia. However the British navy was unable to establish naval supremacy. While many American privateers were captured in battles such as the Naval battle off Halifax, many more continued attacks on shipping and settlements until the final months of the war. The Royal Navy struggled to maintain British supply lines, defending convoys from American and French attacks such as the fiercely fought convoy battle, the Naval battle off Cape Breton.
After the British were defeated in the Thirteen Colonies, its troops helped evacuate approximately 30,000 United Empire Loyalists (American Tories), who settled in Nova Scotia, with land grants by the Crown as some compensation for their losses. (Nova Scotia was divided and the present-day province of New Brunswick created). The Loyalist exodus created new communities across Nova Scotia, including Shelburne, which was briefly one of the larger British settlements in North America, and infused the province with additional capital and skills. However the migration also caused political tensions between Loyalist leaders and the leaders of the existing New England Planters settlement. The Loyalist influx also pushed Nova Scotia's Mi'kmaq People to the margins as Loyalist land grants encroached on ill-defined native lands. Approximately 3,000 members of the Loyalist migration were Black Loyalists who founded the largest free Black settlement in North America at Birchtown, near Shelburne. However unfair treatment and harsh conditions caused about one-third of the Black Loyalists to resettle in Sierra Leone in 1792 where they founded Freetown and became known in Africa as the Nova Scotian Settlers.
19th century.
During the War of 1812, Nova Scotia’s contribution to the war effort was communities either purchasing or building various privateer ships to lay siege to American vessels. Perhaps the most dramatic moment in the war for Nova Scotia was when HMS "Shannon" escorted the captured American frigate USS "Chesapeake" into Halifax Harbour (1813). Many of the prisoners were kept at Deadman's Island, Halifax.
During this century, Nova Scotia was the first colony in British North America and in the British Empire to achieve responsible government in January–February 1848 and become self-governing through the efforts of Joseph Howe. Nova Scotia had established representative government in 1758, an achievement that was later commemorated by erecting the Dingle Tower in 1908.
Nova Scotians fought in the Crimean War. The Welsford-Parker Monument in Halifax is the second oldest war monument in Canada (1860) and the only Crimean War monument in North America. It commemorates the Siege of Sevastopol (1854–1855).
Thousands of Nova Scotians fought in the American Civil War (1861–1865), primarily for the North. The British Empire (including Nova Scotia) was declared neutral in the struggle between the North and the South. As a result, Britain (and Nova Scotia) continued to trade with both the South and the North. Nova Scotia’s economy boomed during the Civil War.
Immediately after the Civil War, Pro-Confederation premier Charles Tupper led Nova Scotia into the Canadian Confederation on July 1, 1867, along with New Brunswick and the Province of Canada. The Anti-Confederation Party was led by Joseph Howe. Almost three months later, in the election of September 18, 1867, the Anti-Confederation Party won 18 out of 19 federal seats, and 36 out of 38 seats in the provincial legislature.
Nova Scotia became a world leader in both building and owning wooden sailing ships in the second half of the 19th century. Nova Scotia produced internationally recognized shipbuilders Donald McKay and William Dawson Lawrence. The fame Nova Scotia achieved from sailors was assured when Joshua Slocum became the first man to sail single-handedly around the world (1895). This international attention continued into the following century with the many racing victories of the "Bluenose" schooner. Nova Scotia was also the birthplace and home of Samuel Cunard, a British shipping magnate, born at Halifax, Nova Scotia, who founded the Cunard Line.
Throughout the nineteenth century, there were numerous businesses that were developed in Nova Scotia that became of national and international importance: The Starr Manufacturing Company (first skate manufacturer in Canada), the Bank of Nova Scotia, Cunard Line, Alexander Keith's Brewery, Morse's Tea Company (first tea company in Canada), among others. (Early in the 20th century Sobey's was established as was Maritime Life.)
Demography.
Ethnic origins.
According to the 2006 Canadian census the largest ethnic group in Nova Scotia is Scottish (28.3%), followed by English (28.1%), Irish (19.9%), French (17.7%), Aboriginal origin (10.2%), German (10.0%), Dutch (3.9%), Black (2.3%), Italian (1.3%), and Acadian (1.2%). Almost half of respondents (47.4%) identified their ethnicity as "Canadian".
Nova Scotia has a long history of social justice work to address issues such as racism and sexism within its borders. The Nova Scotia legislature was the third in the Canada to pass human rights legislation (1963). The Nova Scotia Human Rights Commission was established in 1967.
Language.
The 2011 Canadian census showed a population of 921,727.Of the 904,285 singular responses to the census question concerning mother tongue the most commonly reported languages were:
Figures shown are for the number of single language responses and the percentage of total single-language responses.
Religion.
In 1871, the largest religious denominations were Presbyterian with 103,500 (27%); Roman Catholic with 102,000 (26%); Baptist with 73,295 (19%); Episcopal with 55,124 (14%); Methodist with 40,748 (10%), Lutheran with 4,958 (1.3%); and Congregationalist with 2,538 (0.65%).
According to the 2001 census, the largest denominations by number of adherents were the Roman Catholic Church with 327,940 (37%); the United Church of Canada with 142,520 (17%); and the Anglican Church of Canada with 120,315 (13%).
Economy.
Per capita GDP in 2010 was $38,475, significantly lower than the national average per capita GDP of $47,605 and a little more than half that of Canada's richest province, Alberta. GDP growth has lagged behind the rest of the country for at least the past decade.
Nova Scotia's traditionally resource-based economy has diversified in recent decades. The rise of Nova Scotia as a viable jurisdiction in North America, historically, was driven by the ready availability of natural resources, especially the fish stocks off the Scotian Shelf. The fishery was pillar of the economy since its development as part of the economy of New France in the 17th century; however, the fishery suffered a sharp decline due to overfishing in the late 20th century. The collapse of the cod stocks and the closure of this sector resulted in a loss of approximately 20,000 jobs in 1992. Other sectors in the province were also hit hard, particularly during the last two decades: coal mining in Cape Breton and northern mainland Nova Scotia has virtually ceased production, and a large steel mill in Sydney closed during the 1990s. More recently, the high value of the Canadian dollar relative to the U.S. dollar has hurt the forestry industry, leading to the shut down of a long-running pulp and paper mill near Liverpool. Mining, especially of gypsum and salt and to a lesser extent silica, peat and barite, is also a significant sector. Since 1991, offshore oil and gas has become an increasingly important part of the economy, although production and revenue are now declining. Agriculture remains an important sector in the province, particularly in the Annapolis Valley.
Nova Scotia’s defence and aerospace sector generates approximately $500 million in revenues and contributes about $1.5 billion to the provincial economy annually. To date, 40% of Canada’s military assets reside in Nova Scotia. Nova Scotia has the fourth-largest film industry in Canada hosting over 100 productions yearly, more than half of which are the products of international film and television producers.
The Nova Scotia tourism industry includes more than 6,500 direct businesses, supporting nearly 40,000 jobs. 200,000 cruise ship passengers from around the world flow through the Port of Halifax, Nova Scotia each year. This industry contributes approximately $1.3 billion annually to the economy. The province also boasts a rapidly developing Information & Communication Technology (ICT) sector which consists of over 500 companies, and employs roughly 15,000 people. In 2006, the manufacturing sector brought in over $2.6 billion in chained GDP, the largest output of any industrial sector in Nova Scotia. Michelin remains by far the largest single employer in this sector, operating three production plants in the province.
As of 2012, the median family income in Nova Scotia was $67,910, below the national average of $74,540; in Halifax the figure rises to $80,490.
Nova Scotia has a number of incentive programs, including tax refunds and credits that work to encourage small business growth. The province is attracting major companies from all over the world that will help fuel the economy and provide jobs; companies like Blackberry (formerly Research in Motion (RIM)) and Lockheed Martin have seen the value of Nova Scotia and established branches in the province.
Though only the second smallest province in Canada, Nova Scotia is a recognized exporter. The province is the world’s largest exporter of Christmas trees, lobster, gypsum, and wild berries. Its export value of fish exceeds $1 billion, and fish products are received by 90 countries around the world.
Government, law and politics.
Nova Scotia is ordered by a parliamentary government within the construct of constitutional monarchy; the monarchy in Nova Scotia is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces, and the Canadian federal realm, and resides predominantly in the United Kingdom. As such, the Queen's representative, the Lieutenant Governor of Nova Scotia (presently John James Grant), carries out most of the royal duties in Nova Scotia. 
In 1937, Everett Farmer was the last person hanged (for murder) in Nova Scotia. 
The direct participation of the royal and viceroyal figures in any of these areas of governance is limited, though; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected House of Assembly and chosen and headed by the Premier of Nova Scotia (presently Stephen McNeil), the head of government. To ensure the stability of government, the lieutenant governor will usually appoint as premier the person who is the current leader of the political party that can obtain the confidence of a plurality in the House of Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition (presently Jamie Baillie) and is part of an adversarial parliamentary system intended to keep the government in check.
Each of the 51 Members of the Legislative Assembly in the House of Assembly is elected by single member plurality in an electoral district or riding. General elections must be called by the lieutenant governor on the advice of the premier, or may be triggered by the government losing a confidence vote in the House. There are three dominant political parties in Nova Scotia: the Liberal Party, the New Democratic Party, and the Progressive Conservative Party.
The province's revenue comes mainly from the taxation of personal and corporate income, although taxes on tobacco and alcohol, its stake in the Atlantic Lottery Corporation, and oil and gas royalties are also significant. In 2006–07, the province passed a budget of $6.9 billion, with a projected $72 million surplus. Federal equalization payments account for $1.385 billion, or 20.07% of the provincial revenue. The province participates in the HST, a blended sales tax collected by the federal government using the GST tax system.
Nova Scotia no longer has any incorporated cities; they were amalgamated into Regional Municipalities in 1996.
Culture.
Fine arts.
Nova Scotia has long been a centre for artistic and cultural excellence. The capital, Halifax, hosts institutions such as Nova Scotia College of Art and Design University, Art Gallery of Nova Scotia, Neptune Theatre, , Two Planks and a Passion Theatre, Ship's Company Theatre and the Symphony Nova Scotia. The province is home to avant-garde visual art and traditional crafting, writing and publishing and a film industry.
Much of the historic public art sculptures in the province were made by the renowned New York sculptor J. Massey Rhind as well as Canadian sculptors Hamilton MacCarthy, George Hill, Emanuel Hahn and Louis-Philippe Hébert. Some of this public art was also created by internationally renowned Nova Scotian John Wilson (sculptor). Nova Scotian George Lang was a stone sculptor who also built many landmark buildings in the province, perhaps most notably he created the Welsford-Parker Monument.
Some of the province's greatest painters were William Valentine, Maria Morris, Jack L. Gray, Mabel Killiam Day, Ernest Lawson, Frances Bannerman, Alex Colville, Tom Forrestall and ship portrait artist John O'Brien. Some of most renowned artists whose works have been acquired by Nova Scotia are British artist Joshua Reynolds (collection of Art Gallery of Nova Scotia); William Gush and William J. Weaver (both have works in Province House); Robert Field (Government House), as well as leading American artists Benjamin West (self portrait in The Halifax Club), John Singleton Copley, Robert Feke, and Robert Field (the latter three have works in the Uniacke Estate).
Two famous Nova Scotian photographers are Wallace R. MacAskill and Sherman Hines. Two of the most accomplished illustrators were Bob Chambers (cartoonist) and Donald A. Mackay.
Film and television.
Nova Scotia has produced numerous film actors. Academy Award nominee Ellen Page ("Juno", "Inception") lives in Nova Scotia; five time Academy Award nominee Arthur Kennedy (Lawrence of Arabia, High Sierra) called Nova Scotia his home; and two time Golden Globe winner Donald Sutherland ("MASH", "Ordinary People") spent most of his youth in the province. Other actors include John Paul Tremblay, Robb Wells and John Dunsworth of ("Trailer Park Boys").
Nova Scotia has also produced numerous film directors such as Thom Fitzgerald ("The Hanging Garden"), Daniel Petrie ("Resurrection"—Academy Award nominee) and Acadian film director Phil Comeau's multiple award-winning local story ("Le secret de Jérôme").
Nova Scotian stories are the subject of numerous feature films: "Margaret's Museum" (starring Helena Bonham Carter); "The Bay Boy" (directed by Daniel Petrie and starring Kiefer Sutherland); "New Waterford Girl"; "The Story of Adele H." (the story of unrequited love of Adele Hugo); and two films of "Evangeline" (one starring Miriam Cooper and another starring Dolores del Río).
There is a significant film industry in Nova Scotia. Feature filmmaking began in Canada with Evangeline (1913), made by Canadian Bioscope Company in Halifax, Nova Scotia, which released six films before it closed. The film has since been lost. Some of the award winning feature films that have been made in the province are: "Titanic" (starring Leonardo DiCaprio and Kate Winslet); "Bowling for Columbine" (starring Michael Moore); and "The Shipping News" (starring Kevin Spacey and Cate Blanchett). Other films include" " (starring Harrison Ford and Liam Neeson) and "Amelia" (starring Hilary Swank, Richard Gere and Ewan McGregor).
Nova Scotia has also produced numerous television series: "This Hour has 22 Minutes", "Don Messer's Jubilee", "Black Harbour", "Haven", "Trailer Park Boys", "Mr. D", and "Theodore Tugboat". The "Jesse Stone" film series on CBS starring Tom Selleck is also routinely produced in the province.
Literature.
There are numerous Nova Scotian authors who have achieved international fame: Thomas Chandler Haliburton ("The Clockmaker"); Alistair MacLeod ("No Great Mischief"); Margaret Marshall Saunders ("Beautiful Joe"), Laurence D. Dakin (Marco Polo), and Joshua Slocum ("Sailing Alone Around the World"). Other authors include Johanna Skibsrud ("The Sentimentalists"), Alden Nowlan ("Bread, Wine and Salt"), George Elliott Clarke ("Execution Poems"), Lesley Choyce ("Nova Scotia: Shaped by the Sea"), Thomas Raddall ("Halifax: Warden of the North"), Donna Morrissey ("Kit's Law"), Frank Parker Day ("Rockbound").
Nova Scotia has also been the subject of numerous literary books. Some of the international best-sellers are: "Last Man Out: The Story of the Springhill Mining Disaster" (by Melissa Fay Greene) ; "Curse of the Narrows: The Halifax Explosion 1917" (by Laura MacDonald); "In the Village" (short story by Pulitzer Prize–winning author Elizabeth Bishop); and National Book Critics Circle Award winner "Rough Crossings" (by Simon Schama). Other authors who have written novels about Nova Scotian stories include: Linden MacIntyre ("The Bishop's Man"); Hugh MacLennan ("Barometer Rising"); Ernest Buckler ("The Valley and the Mountain"); Archibald MacMechan ("Red Snow on Grand Pré"), Henry Wadsworth Longfellow (long poem "Evangeline"); Lawrence Hill ("The Book of Negroes") and John Mack Faragher ("Great and Nobel Scheme").
Music.
Nova Scotia has produced numerous musicians. The Grammy Award winners include Denny Doherty (from The Mamas & the Papas), Anne Murray, and Sarah McLachlan. Other musicians include country singer Hank Snow, country singer George Canyon, jazz singer Holly Cole, opera singers Portia White and Barbara Hannigan, multi-Juno Award nominated rapper Classified, Rita MacNeil, Matt Mays, Sloan, Feist, Todd Fancey, The Rankin Family, April Wine, Buck 65, Joel Plaskett, Grand Dérangement, and country music singer Drake Jensen.
There are numerous songs written about Nova Scotia: The Ballad of Springhill (written by Peggy Seeger and performed by Irish folk singer Luke Kelly a member of The Dubliners, U2); numerous songs by Stan Rogers including Bluenose, The Jeannie C (mentions Little Dover, NS), Barrett's Privateers, Giant, and The Rawdon Hills; Farewell to Nova Scotia (traditional); Blue Nose (Stompin' Tom Connors); She’s Called Nova Scotia (by Rita MacNeil); Cape Breton (by David Myles); Acadian Driftwood (by Robbie Robertson); Acadie (by Daniel Lanois); and My Nova Scotia Home (by Hank Snow).
Nova Scotia has also produced some significant song writers such as Grammy Award winning Gordie Sampson. Sampson has written songs for Carrie Underwood ("Jesus, Take the Wheel", "Just a Dream", "Get Out of This Town"), Martina McBride ("If I Had Your Name", You're Not Leavin Me"), LeAnn Rimes ("Long Night", "Save Myself"), and George Canyon ("My Name"). Another successful Nova Scotia song writer was Hank Snow whose songs have been recorded by The Rolling Stones, Elvis Presley, and Johnny Cash.
Music producer Brian Ahern is a Nova Scotian. He got his start by being music director for CBC television's Singalong Jubilee. He later produced 12 albums for Anne Murray (“Snowbird,” Danny’s Song” and “You Won’t See Me”); 11 albums for Emmylou Harris (whom he married at his home in Halifax on January 9, 1977). He also produced discs for Johnny Cash, George Jones, Roy Orbison, Glen Campbell, Don Williams, Jesse Winchester and Linda Ronstadt. Another noted writer is Cape Bretoner Leon Dubinsky, who wrote the anthem, "Rise Again", among many other songs performed by various Canadian artists.
Sports.
Sport is an important part of Nova Scotia culture. There are numerous professional sports teams. The Halifax Rainmen of the National Basketball League of Canada are one team that calls Nova Scotia home. The Province has also produced numerous athletes such as Sidney Crosby (hockey), Nathan Mackinnon (hockey), Brad Marchand (hockey), Colleen Jones (curling), Al MacInnis (hockey), TJ Grant (mixed martial artist), Rocky Johnson (wrestling, and father of Dwayne "The Rock" Johnson) and George Dixon (boxer). The achievements of Nova Scotian athletes are presented at the Nova Scotia Sport Hall of Fame.
Endemic cuisine.
The cuisine of Nova Scotia is typically Canadian with an emphasis on local seafood. The only truly endemic dish (in the sense "peculiar to" and "originating from") is the "donair", a distant variant of the doner kebab, that is made from thinly sliced beef meatloaf and uses a sweet condensed milk sauce.
Tourism.
Nova Scotia's tourism industry showcases Nova Scotia's culture, scenery and coastline.
Nova Scotia has many museums reflecting its ethnic heritage, including the Glooscap Heritage Centre, Grand-Pré National Historic Site, Hector Heritage Quay and the Black Cultural Centre for Nova Scotia. Others museums tell the story of its working history, such as the Cape Breton Miners' Museum, and the Maritime Museum of the Atlantic.
Nova Scotia is home to several internationally renowned musicians and there are visitor centres in the home towns of Hank Snow, Rita MacNeil, and Anne Murray Centre. There are also numerous music and cultural festivals such as the Stan Rogers Folk Festival, Celtic Colours, the Nova Scotia Gaelic Mod, Royal Nova Scotia International Tattoo, the Atlantic Film Festival and the Atlantic Fringe Festival.
The province has 87 National Historic Sites of Canada, including the Habitation at Port-Royal, the Fortress of Louisbourg and Citadel Hill (Fort George) in Halifax.
Nova Scotia has two national parks, Kejimkujik and Cape Breton Highlands, and many other protected areas. The Bay of Fundy has the highest tidal range in the world, and the iconic Peggys Cove is internationally recognized and receives 600,000 plus visitors a year.
Cruise ships pay regular visits to the province. In 2010, Halifax received 261,000 passengers and Sydney 69,000.
A 2008 Nova Scotia tourism campaign included advertising a fictional mobile phone called Pomegranate and establishing website, which after reading about "new phone" redirected to tourism info about region.
Education.
The Minister of Education is responsible for the administration and delivery of education, as defined by the Education Act and other acts relating to colleges, universities and private schools. The powers of the Minister and the Department of Education are defined by the Ministerial regulations and constrained by the Governor-In-Council regulations.
Nova Scotia has more than 450 public schools for children. The public system offers primary to Grade 12. There are also private schools in the province. Public education is administered by seven regional school boards, responsible primarily for English instruction and French immersion, and also province-wide by the Conseil Scolaire Acadien Provincial, which administers French instruction to students for whom the primary language is French.
The Nova Scotia Community College system has 13 campuses around the province. The community college, with its focus on training and education, was established in 1988 by amalgamating the province's former vocational schools.
In addition to its community college system the province has 10 universities, including Dalhousie University, University of King's College, Saint Mary's University (Halifax), Mount Saint Vincent University, NSCAD University, Acadia University, Université Sainte-Anne, Saint Francis Xavier University, Cape Breton University and the Atlantic School of Theology.
There are also more than 90 registered private commercial colleges in Nova Scotia.
Bibliography.
</dl>

</doc>
<doc id="21186" url="http://en.wikipedia.org/wiki?curid=21186" title="Northwest Territories">
Northwest Territories

The Northwest Territories (NWT; French: "les Territoires du Nord-Ouest", "TNO") is a territory of Canada. With a population of 41,462 in 2011 and an estimated population of 43,537 in 2013, the Northwest Territories is the most populous territory in Northern Canada. Yellowknife became the territorial capital in 1967, following recommendations by the Carrothers Commission.
The Northwest Territories, a portion of the old North-West Territory, entered the Canadian Confederation July 15, 1870, but the current borders were formed April 1, 1999, when the territory was subdivided to create Nunavut to the east, via the "Nunavut Act" and the "Nunavut Land Claims Agreement Act". While Nunavut is mostly Arctic tundra, the Northwest Territories has a slightly warmer climate and is mostly boreal forest (taiga), although portions of the territory lie north of the tree line, and its most northern regions form part of the Canadian Arctic Archipelago.
The Northwest Territories are bordered by Canada's two other territories, Nunavut to the east and Yukon to the west, and by the provinces of British Columbia, Alberta, and Saskatchewan to the south.
Etymology.
The name is descriptive, adopted by the British government during the colonial era to indicate where it lay in relation to Rupert's Land. It is shortened from North-Western Territory ("see" History). In Inuktitut, the Northwest Territories are referred to as ᓄᓇᑦᓯᐊᖅ ("Nunatsiaq"), "beautiful land."
There was some discussion of changing the name of the Northwest Territories after the splitting off of Nunavut, possibly to a term from an Aboriginal language. One proposal was "Denendeh" (an Athabaskan language word meaning "our land"), as advocated by the former premier Stephen Kakfwi, among others. One of the most popular proposals for a new name – one to name the territory "Bob" – began as a prank, but for a while it was at or near the top in the public-opinion polls.
In the end a poll conducted prior to division showed that strong support remained to keep the name "Northwest Territories". This name arguably became more appropriate following division than it had been when the territories actually extended far into Canada's north-central and northeastern areas.
Geography.
Located in northern Canada, the territory borders Canada's two other territories, Yukon to the west and Nunavut to the east, and four provinces: British Columbia to the southwest, and Alberta and Saskatchewan to the south and Manitoba to the extreme southeast. It has a land area of 1183085 km2.
Geographical features include Great Bear Lake, the largest lake entirely within Canada, and Great Slave Lake, the deepest body of water in North America at 614 m, as well as the Mackenzie River and the canyons of the Nahanni National Park Reserve, a national park and UNESCO World Heritage Site. Territorial islands in the Canadian Arctic Archipelago include Banks Island, Borden Island, Prince Patrick Island, and parts of Victoria Island and Melville Island. Its highest point is Mount Nirvana near the border with Yukon at an elevation of 2773 m.
Climate.
The Northwest Territories reaches for over 1300000 km2 so there is a large climate variant from south to north. The southern part of the territory (most of the mainland portion) has a subarctic climate while the islands and northern coast have a polar climate.
Summers in the north are short and cool, with daytime highs in the mid teens, and lows in the single degrees. Winters are long and harsh, daytime highs in the mid -20 C and lows around -40 C. Extremes are common with summer highs in the south reaching 36 C and lows reaching into the negatives. In winter in the south it is not uncommon for the temperatures to reach -40 C, but they can also reach the low teens during the day. In the north, temperatures can reach highs of 30 C, and lows can reach into the low negatives.
In winter in the north it is not uncommon for the temperatures to reach -50 C but they can also reach the single digits during the day. Thunderstorms are not rare in the south. In the north they are very rare, but do occur. Tornadoes are extremely rare but have happened with the most notable one happening just outside of Yellowknife that destroyed a communications tower. The Territory has a fairly dry climate due to the mountains in the west.
About half of the territory is above the tree line. There are not many trees in most of the eastern areas of the territory, or the north islands.
History.
The present-day territory came under government authority in July 1870, after the Hudson's Bay Company transferred Rupert's Land and North-Western Territory to the British Crown, which subsequently transferred them to the government of Canada, giving it the name the North-West Territories. This immense region comprised all of today's Canada except that which was encompassed within the early signors of Canadian Confederation, that is, British Columbia, early forms of present-day Ontario and Quebec (which encompassed the coast of the Great Lakes, the Saint Lawrence River valley and the southern third of Quebec), the Maritimes (PEI, NS and NB), Newfoundland, the Labrador coast, and the Arctic Islands, except the southern half of Baffin Island (the Arctic Islands remained under direct British claim until 1880).
After the 1870 transfer, some of the North-West Territories was whittled away. The province of Manitoba was created on July 15, 1870, at first a tiny square area around Winnipeg, and then enlarged in 1881 to a rectangular region composing the modern province's south. By the time British Columbia joined Confederation on July 20, 1871, it had already (1866) been granted the portion of North-Western Territory south of 60 degrees north and west of 120 degrees west, an area that comprised most of the Stickeen Territories.
In 1882, Regina in the District of Assiniboia became the territorial capital. Alberta and Saskatchewan were separated from the NWT to become provinces in 1905 (Regina became the provincial capital of Saskatchewan).
In 1876, the District of Keewatin, at the centre of the territory, was separated from the NWT. In 1882 and again in 1896, the remaining portion was divided into the following districts (corresponding to the following modern-day areas):
Keewatin was returned to the Northwest Territories in 1905.
In the meantime, the Province of Ontario was enlarged northwestward in 1882. Quebec was also extended northwards in 1898, and the Yukon was made a separate territory in that same year to deal with the Klondike Gold Rush, and also to remove the NWT's government from the burden of administering the sudden boom of population, economic activity, and the influx of non-Canadians.
The provinces of Alberta and Saskatchewan were created in 1905, and Manitoba, Ontario, and Quebec acquired the last addition to their modern landmass from the NWT in 1912. This left only the districts of Mackenzie, Franklin (which absorbed the remnants of Ungava in 1920), and Keewatin within what was then given the name Northwest Territories. In 1925, the boundaries of the NWT were extended all the way to the North Pole on the sector principle, vastly expanding its territory onto the northern ice cap.
The reduced Northwest Territories was not represented in the Canadian House of Commons from 1907 until 1947, when the electoral district of Yukon—Mackenzie River was created. This riding only included the District of Mackenzie. The rest of the Northwest Territories had no representation in the House of Commons until 1962, when the Northwest Territories electoral district was created in recognition of the Inuit having been given the right to vote in 1953.
In 1912, the Parliament of Canada made the official name of these territories the Northwest Territories, dropping all hyphenated forms of it. Between 1925 and 1999, the Northwest Territories covered a land area of 3439296 km2 – larger than that of India.
On April 1, 1999, the eastern three-fifths of the Northwest Territories (including all of the District of Keewatin and much of that of Mackenzie and Franklin) became a separate Canadian territory named "Nunavut".
Demography.
The NWT is one of two jurisdictions in Canada – Nunavut being the other – where Aboriginal peoples are in the majority, constituting 50.3% of the population.
According to the 2006 Canadian census, the 10 major ethnic groups were:
Population of the Northwest Territories since 1871
Religion.
The largest denominations by number of adherents according to the 2001 census were Roman Catholic with 16,940 (46.7%); the Anglican Church of Canada with 5,510 (14.9%); and the United Church of Canada with 2,230 (6.0%), while a total of 6,465 (17.4%) people stated no religion.
Language.
French was made an official language in 1877 by the territorial government. After a lengthy and bitter debate resulting from a speech from the throne in 1888 by Lieutenant Governor Joseph Royal the members of the day voted on more than one occasion to nullify and make English the only language used in the assembly. After some conflict with Ottawa and a decisive vote on January 19, 1892, the assembly members voted for an English-only territory.
In the early 1980s, the federal government pressured the government of the Northwest Territories to reintroduce French as an official language. Some Native members walked out of the assembly, protesting that they were not permitted to speak their own language. The executive council appointed a special committee to study the matter, which decided that if French was to be an official language, then the other languages in the territories must also be allowed.
The Northwest Territories' Official Languages Act recognizes the following eleven official languages, which are more than in any other political division in the Americas:
NWT residents have a right to use any of the above languages in a territorial court and in debates and proceedings of the legislature. However, laws are legally binding only in their French and English versions, and the NWT government only publishes laws and other documents in the territory's other official languages when the legislature asks it to. Furthermore, access to services in any language is limited to institutions and circumstances where there is significant demand for that language or where it is reasonable to expect it given the nature of the services requested. In practical terms, English language services are universally available, and there is no guarantee that other languages, including French, will be used by any particular government service except for the courts.
The 2006 census returns showed a population of 41,464. Of the 40,680 singular responses to the census question regarding each inhabitant's "mother tongue", the most reported languages were the following:
There were also 320 responses of both English and a "non-official language"; 15 of both French and a "non-official language"; 45 of both English and French, and about 400 people who either did not respond to the question, or reported multiple non-official languages, or else gave some other un-enumeratable response. The Northwest Territories' official languages are shown in bold.
Communities.
As of 2014 there are 33 official communities in the NWT. These range in size from Yellowknife with a population of 19,234 to Kakisa with 45 people. Governance of each community differs, some are run under various types of First Nations control, while others are designated as a city, town, village or hamlet, but most communities are municipal corporations. Yellowknife is the largest community and has the largest number of Aboriginal peoples, 4,105 (22.2%) people. However, Behchoko, with a population of 1,926, is the largest First Nations community, 1,730 (91.5%), and Inuvik with 3,463 people is the largest Inuvialuit community, 1,335 (38.9%). There is one Indian reserve in the NWT, Hay River Reserve, located on the south shore of the Hay River.
Economy.
The NWT's geological resources include gold, diamonds, natural gas and petroleum. BP is the only oil company currently producing oil in the Territory. NWT diamonds are promoted as an alternative to purchasing blood diamonds. Two of the biggest mineral resource companies in the world, BHP Billiton and Rio Tinto mine many of their diamonds from the NWT. In 2010, NWT accounted for 28.5% of Rio Tinto's total diamond production (3.9 million carats, 17% more than in 2009, from the Diavik Diamond Mine) and 100% of BHP's (3.05 million carats from the EKATI mine).
The Northwest Territories has the highest per capita GDP of all provinces or territories in Canada, C$76,000 in 2009. However, as production at the current mines started to wind down, no new mines opened and the public service shrank, the territory lost 1,200 jobs between November 2013 and November 2014.
Government.
As a territory, the NWT has fewer rights than the provinces. During his term, Premier Kakfwi pushed to have the federal government accord more rights to the territory, including having a greater share of the returns from the territory's natural resources go to the territory. Devolution of powers to the territory was an issue in the 20th general election in 2003, and has been ever since the territory began electing members in 1881.
The Commissioner of the NWT is the chief executive and is appointed by the Governor-in-Council of Canada on the recommendation of the federal Minister of Aboriginal Affairs and Northern Development. The position used to be more administrative and governmental, but with the devolution of more powers to the elected assembly since 1967, the position has become symbolic. The Commissioner had full governmental powers until 1980 when the territories were given greater self-government. The Legislative Assembly then began electing a cabinet and "Government Leader", later known as the Premier. Since 1985 the Commissioner no longer chairs meetings of the Executive Council (or cabinet), and the federal government has instructed commissioners to behave like a provincial Lieutenant Governor. Unlike Lieutenant Governors, the Commissioner of the Northwest Territories is not a formal representative of the Queen of Canada.
Unlike provincial governments and the government of Yukon, the government of the Northwest Territories does not have political parties, except for the period between 1898 and 1905. It is a consensus government called the Legislative Assembly. This group is composed of one member elected from each of the nineteen constituencies. After each general election, the new Assembly elects the Premier and the Speaker by secret ballot. Seven MLAs are also chosen as cabinet ministers, with the remainder forming the opposition.
The current Legislative Assembly is the 17th and the most recent election was held October 3, 2011. The Premier is Bob McLeod. The member of Parliament for the Northwest Territories is Dennis Bevington (New Democratic Party). The Commissioner of the Northwest Territories is George Tuccaro and the Deputy Commissioner is Margaret Thom.
In the Parliament of Canada, the NWT comprises a single Senate division and a single House of Commons electoral district, titled Northwest Territories ("Western Arctic" until 2014).
Departments.
The Government of the NWT comprises the following departments:
Culture.
Aboriginal issues in the Northwest Territories include the fate of the Dene who, in the 1940s, were employed to carry radioactive uranium ore from the mines on Great Bear Lake. Of the thirty plus miners who worked at the Port Radium site, at least fourteen have died due to various forms of cancer. A study was done in the community of Deline, called "A Village of Widows" by Cindy Kenny-Gilday, which indicated that the number of people involved were too small to be able to confirm or deny a link.
There has been racial tension based on a history of violent conflict between the Dene and the Inuit, who have now taken recent steps towards reconciliation.
Land claims in the NWT began with the Inuvialuit Final Agreement, signed on June 5, 1984. It was the first Land Claim signed in the Territory, and the second in Canada. It culminated with the creation of the Inuit homeland of Nunavut, the result of the Nunavut Land Claims Agreement, the largest land claim in Canadian history.
Another land claims agreement with the Tłı̨chǫ people created a region within the NWT called Tli Cho, between Great Bear and Great Slave Lakes, which gives the Tłı̨chǫ their own legislative bodies, taxes, resource royalties, and other affairs, though the NWT still maintains control over such areas as health and education. This area includes two of Canada's three diamond mines at Ekati and Diavik.
Further reading.
</dl>

</doc>
<doc id="21187" url="http://en.wikipedia.org/wiki?curid=21187" title="Nez Perce people">
Nez Perce people

The Nez Perce (autonym: Niimíipu) are an American Indian tribe who live in the Pacific Northwest region (Columbia River Plateau) of the United States. An anthropological interpretation says they descended from the Old Cordilleran Culture, which moved south from the Rocky Mountains and west into lands where the tribe coalesced. The federally recognized Nez Perce Nation currently governs and lives within its reservation in Idaho. Their name for themselves is Nimíipuu (pronounced ]), meaning, "The People," in their language, part of the Sahaptin family.
They speak the Nez Perce language or Niimiipuutímt, a Sahaptian language related to the several dialects of Sahaptin. The Sahaptian sub-family is one of the branches of the Plateau Penutian family (which in turn may be related to a larger Penutian grouping).
Name.
"Nez Percé" is an exonym given by French Canadian fur traders who visited the area regularly in the late 18th century, meaning literally "pierced nose". Today the Nez Perce identify most often as Niimíipu in Sahaptin. The tribe also uses the term "Nez Perce," as do the United States Government in its official dealings with them, and contemporary historians. Older historical ethnological works use the French spelling of "Nez Percé," with the diacritic. The original French pronunciation is , with three syllables.
William Clark in his journals referred to the people as the Chopunnish . This term is an adaptation of the term "cú·pŉitpeľu" (the Nez Perce people) which is formed from "cú·pŉit" (piercing with a pointed object) and "peľu" (people). When analyzed through the "Nez Perce Language Dictionary," the term "cúpnitpelu" contains no reference to "Piercing with a pointed object," as described by D.E. Walker. The prefix "cú"- means "in single file." This prefix, combined with the verb "-piní", "to come out (e.g. of forest, bushes, ice)". Finally, with the suffix of "-pelú", meaning "people or inhabitants of". Put all three parts of the Nez Perce word together now to get "cú"- + -"piní" + "pelú" = "cúpnitpelu", or "the People Walking Single File Out of the Forest." Nez Perce oral tradition indicates the name "Cuupn'itpel'uu" meant "we walked out of the woods or walked out of the mountains" and referred to the time before the Nez Perce had horses. 
Nez Perce is a misnomer given by the interpreter of the Lewis and Clark Expedition at the time they first encountered the Nez Perce in 1805. It was a French term meaning "pierced nose." This is an inaccurate description of the tribe. They did not practice nose piercing or wearing ornaments. The "pierced nose" tribe lived on and around the lower Columbia River in the Pacific Northwest and are commonly called the Chinook tribe by historians and anthropologists. The Chinook relied heavily upon salmon, as did the Nez Perce. The peoples shared fishing and trading sites but the Chinook were much more hierarchical in their social arrangements.
Traditional lands and culture.
The Nez Perce territory at the time of Lewis and Clark (1804-1806) was approximately 17000000 acre. It covered parts of present-day Washington, Oregon, Montana, and Idaho, in an area surrounding the Snake, Salmon and the Clearwater rivers. The tribal area extended from the Bitterroots in the east to the Blue Mountains in the west between latitudes 45°N and 47°N.
In 1800, the Nez Perce had more than 100 permanent villages, ranging from 50 to 600 individuals, depending on the season and social grouping. Archeologists have identified a total of about 300 related sites, mostly in the Salmon River Canyon, including both camps and villages. In 1805 the Nez Perce were the largest tribe on the Columbia River Plateau, with a population of about 12,000. By the beginning of the 20th century, the Nez Perce had declined to about 8500 because of epidemics, conflicts with non-Indians, and other factors. A total of 3499 Nez Perce were counted in the 2010 Census.
Similar to many western Native American tribes, the Nez Perce were migratory and would travel in seasonal rounds, according to where the abundant food was to be found at a given time of year. This migration followed a predictable pattern from permanent winter villages through several temporary camps, nearly always returning to the same locations each year. They were known to go as far east as the Great Plains of Montana to hunt buffalo, and as far west as the west coast. Before construction of The Dalles Dam in 1957, which flooded this area, Celilo Falls was a respected and favored location to fish for salmon on the Columbia River. They relied heavily on "q'emes" or camas root as a food source; it was gathered in the region between the Salmon and Clearwater river drainages. 
The Nez Perce believed in spirits called "weyekins" (Wie-a-kins) which would, they thought, offer "a link to the invisible world of spiritual power". The weyekin would protect one from harm and become a personal guardian spirit. To receive a weyekin, both girls and boys, aged 12 to 15, would go to the mountains alone on a vision quest. The person on quest would take a brew of dry peyote or other sacred plants, which the people believed held spiritual power. They did not eat and drank very little water. While on the quest, the individual would receive a vision of a spirit, which would take the form of a mammal or bird. This vision could appear physically or in a dream or trance. The weyekin was to bestow the animal's powers on its bearer—for example; a deer might give its bearer swiftness. A person's weyekin was very personal. It was rarely shared with anyone and was contemplated in private. The weyekin stayed with the person until death.
The Nez Perce National Historical Park, with headquarters in Spalding, Idaho, is managed overall by the National Park Service. The museum in Spalding includes a research center holding the park's historical archives and library collection. It is available for on-site use in the study and interpretation of Nez Perce history and culture. The park includes 38 sites associated with the Nez Perce in the states of Idaho, Montana, Oregon, and Washington, many of which are managed by local and state agencies.
History.
European contact.
In 1805 William Clark was the first known Euro-American to meet any of the tribe, excluding the aforementioned French Canadian traders. While he, Meriwether Lewis and their men were crossing the Bitterroot Mountains, they ran low of food, and Clark took six hunters and hurried ahead to hunt. On September 20, 1805, near the western end of the Lolo Trail, he found a small camp at the edge of the camas-digging ground, which is now called Weippe Prairie. The explorers were favorably impressed by the Nez Perce whom they met. Preparing to make the remainder of their journey to the Pacific by boats on rivers, they entrusted the keeping of their horses until they returned to "2 brothers and one son of one of the Chiefs." One of these Indians was "Walammottinin" (meaning "Hair Bunched and tied," but more commonly known as Twisted Hair). He was the father of Chief Lawyer, who by 1877 was a prominent member of the "Treaty" faction of the tribe. The Nez Perce were, generally, faithful to the trust; and the party recovered their horses without serious difficulty when they returned.
Fight of the Nez Perce.
Under pressure from the European Americans, in the late 19th century the Nez Perce split into two groups: one side accepted the coerced relocation to a reservation and the other refused to give up their fertile land in Idaho and Oregon. Those willing to go to a reservation made a treaty in 1877. The flight of the non-treaty Nez Perce began on June 15, 1877, with Chief Joseph, Looking Glass, White Bird, Ollokot, Lean Elk (Poker Joe) and Toohoolhoolzote leading 2,900 men, women and children in an attempt to reach a peaceful sanctuary. They intended to seek shelter with their allies the Crow but, upon the Crow's refusal to offer help, the Nez Perce tried to reach the camp in Canada of Lakota Chief Sitting Bull. He had migrated there after decisively defeating United States forces in the Battle of the Little Bighorn. 
The Nez Perce were pursued by over 2,000 soldiers of the U.S. Army on an epic flight to freedom of more than 1170 mi across four states and multiple mountain ranges. The 800 Nez Perce warriors defeated or held off the pursuing troops in 18 battles, skirmishes, and engagements. More than 300 US soldiers and 1,000 Nez Perce (including women and children) were killed in these conflicts. 
A majority of the surviving Nez Perce were finally forced to surrender on October 5, 1877, after the Battle of the Bear Paw Mountains in Montana, 40 mi from the Canadian border. Chief Joseph surrendered to General Oliver O. Howard of the U.S. Cavalry. During the surrender negotiations, Chief Joseph sent a message, usually described as a speech, to the US soldiers. It has become renowned as one of the greatest American speeches: "...Hear me, my chiefs, I am tired. My heart is sick and sad. From where the sun now stands, I will fight no more forever."
The route of the Nez Perce flight is preserved by the Nez Perce National Historic Trail. The annual Cypress Hills ride in June commemorates the Nez Perce people's attempt to escape to Canada.
Nez Perce horse breeding program.
In 1994 the Nez Perce tribe began a breeding program, based on crossbreeding the Appaloosa and a Central Asian breed called Akhal-Teke, to produce what they called the Nez Perce Horse. They wanted to restore part of their traditional horse culture, where they had conducted selective breeding of their horses, long considered a marker of wealth and status, and trained their members in a high quality of horsemanship. Social disruption due to reservation life and assimilationist pressures by Americans and the government resulted in the destruction of their horse culture in the 19th century. The 20th-century breeding program was financed by the United States Department of Health and Human Services, the Nez Perce tribe, and a nonprofit group called the First Nations Development Institute (based in Washington D.C.) It has promoted businesses in Native American country that reflect values and traditions of the peoples. The Nez Perce Horse breed is noted for its speed.
Fishing.
Fishing is an important ceremonial, subsistence, and commercial activity for the Nez Perce tribe. Nez Perce fishers participate in tribal fisheries in the mainstream Columbia River between Bonneville and McNary dams. The Nez Perce also fish for spring and summer Chinook salmon and steelhead in the Snake River and its tributaries. The Nez Perce tribe runs the Nez Perce Tribal Hatchery on the Clearwater River, as well as several satellite hatchery programs.
Nez Perce people historically depended on fish for their food. Chinook salmon were eaten the most, but other species such as lampreys, whitefish, and chiselmouth. Prior to contact with Europeans, the Nez Perce's traditional hunting and fishing areas spanned from the Cascade Range in the west to the Bitterroot Mountains in the east.
Nez Perce Indian Reservation.
The current tribal lands consist of a reservation in north central Idaho at , primarily in the Camas Prairie region south of the Clearwater River, in parts of four counties. In descending order of surface area, the counties are Nez Perce, Lewis, Idaho, and Clearwater. The total land area is about 1195 sqmi, and the reservation's population at the 2000 census was 17,959. Its largest community is the city of Orofino, near its northeast corner. Lapwai, the seat of tribal government, has the highest percentage of Nez Perce people, at about 81.4 percent.
Similar to the opening of lands in Oklahoma, the U.S. government opened the reservation for white settlement on November 18, 1895. The proclamation had been signed less than two weeks earlier by President Cleveland.
Communities.
In addition, the Colville Indian Reservation in eastern Washington contains the Joseph band of Nez Perce.
Further reading.
 #if: 
 #if:The Indian Dispossessed
 }}""{{
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if: The Indian Dispossessed
 #if: Humphrey
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2="The Indian Dispossessed{{
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Humphrey
 |{{
 #if: 1906
 |, 1906{{
 #if:
}}{{
 #if: 
 #ifeq: | 1906
 |{{
 #if: 
 #if: Humphrey
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if: The Indian Dispossessed
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |book{{
 #if: 
 }}{{
 #if: Humphrey |&rft.aulast={{urlencode:Humphrey}}{{
 }}{{
 #if: Humphrey |&rft.au={{urlencode:Humphrey}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: 
 #if: 
 #if: The Nez Perces
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle =  
 |IncludedWorkURL = 
 |Other = 
 |Edition = Revised
 |Place = Boston
 |PublicationPlace = Boston
 |Publisher = Little, Brown and Company . Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=68571148
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = .
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="21189" url="http://en.wikipedia.org/wiki?curid=21189" title="Neolithic">
Neolithic

The Neolithic Era, or Period, from νέος (néos, "new") and λίθος (líthos, "stone"), or New Stone Age, was a period in the development of human technology, beginning about 10,200 BC, according to the ASPRO chronology, in some parts of the Middle East, and later in other parts of the world and ending between 4,500 and 2,000 BC.
Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene "Epipaleolithic" period and commenced with the beginning of farming, which produced the "Neolithic Revolution". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.
The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200–8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called "proto-neolithic" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming. By 10,200–8,800 BC, farming communities arose in the Levant and spread to Asia Minor, North Africa and North Mesopotamia. Early Neolithic farming was limited to a narrow range of plants, both wild and domesticated, which included einkorn wheat, millet and spelt, and the keeping of dogs, sheep and goats. By about 6,900–6,400 BC, it included domesticated cattle and pigs, the establishment of permanently or seasonally inhabited settlements, and the use of pottery.
Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures that arose completely independent of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery "before" developing agriculture.
Unlike the Paleolithic, when more than one human species existed, only one human species (Homo sapiens sapiens) reached the Neolithic. Homo floresiensis may have survived right up to the very dawn of the Neolithic, about 12,200 years ago.
The term "Neolithic" derives from the Greek "νεολιθικός", "neolithikos", from "νέος" "neos", "new" + "λίθος" "lithos", "stone", literally meaning "New Stone Age". The term was invented by Sir John Lubbock in 1865 as a refinement of the three-age system.
Periods by pottery phase.
In the Middle East, cultures identified as Neolithic began appearing by in the 10th millennium BC. Early development occurred in the Levant (e.g. Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. Neolithic cultures are also attested in southeastern Anatolia and northern Mesopotamia by c. 8,000 BC.
The prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 5,000–6,000 BC, neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than 1,200 square yards and the collection of neolithic findings at the site consists of two phases.
Neolithic 1 – Pre-Pottery Neolithic A (PPNA).
The Neolithic 1 (PPNA) period began roughly 10,000 years ago in the Levant. A temple area in southeastern Turkey at Göbekli Tepe dated around 9,500 BC may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, evidenced by the lack of permanent housing in the vicinity and may be the oldest known human-made place of worship. At least seven stone circles, covering 25 acre, contain limestone pillars carved with animals, insects and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which may have supported roofs. Other early PPNA sites dating to around 9,500 to 9,000 BC have been found in Jericho, Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.
The major advance of Neolithic 1 was true farming. In the proto-Neolithic Natufian cultures, wild cereals were harvested, and perhaps early seed selection and re-seeding occurred. The grain was ground into flour. Emmer wheat was domesticated, and animals were herded and domesticated (animal husbandry and selective breeding).
In the 21st century, remains of figs were discovered in a house in Jericho dated to 9,400 BC. The figs are of a mutant variety that cannot be pollinated by insects, and therefore the trees can only reproduce from cuttings. This evidence suggests that figs were the first cultivated crop and mark the invention of the technology of farming. This occurred centuries before the first cultivation of grains.
Settlements became more permanent with circular houses, much like those of the Natufians, with single rooms. However, these houses were for the first time made of mudbrick. The settlement had a surrounding stone wall and perhaps a stone tower (as in Jericho). The wall served as protection from nearby groups, as protection from floods, or to keep animals penned. There are also some enclosures that suggest grain and meat storage.
Neolithic 2 – Pre-Pottery Neolithic B (PPNB).
The Neolithic 2 (PPNB) began around 8,800 BC according to the ASPRO chronology in the Levant (Jericho, Israel). As with the PPNA dates there are two versions from the same laboratories noted above. But this terminological structure is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. This era was before the Mesolithic era.
Settlements have rectangular mud-brick houses where the family lived together in single or multiple rooms. Burial findings suggest an ancestor cult where people preserved skulls of the dead, which were plastered with mud to make facial features. The rest of the corpse may have been left outside the settlement to decay until only the bones were left, then the bones were buried inside the settlement underneath the floor or between houses.
Neolithic 3 – Pottery Neolithic (PN).
The Neolithic 3 (PN) began around 6,400 BC in the Fertile Crescent. By then distinctive cultures emerged, with pottery like the Halafian (Turkey, Syria, Northern Mesopotamia) and Ubaid (Southern Mesopotamia). This period has been further divided into PNA (Pottery Neolithic A) and PNB (Pottery Neolithic B) at some sites.
The Chalcolithic period began about 4500 BC, then the Bronze Age began about 3500 BC, replacing the Neolithic cultures.
Periods by region.
Fertile Crescent.
Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent. Around 10,700 to 9,400 BC a settlement was established in Tell Qaramel, 10 miles north of Aleppo. The settlement included 2 temples dating back to 9,650. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone and marble wall and contained a population of 2000–3000 people and a massive stone tower. Around 6,400 BC the Halaf culture appeared in Lebanon, Israel and Palestine, Syria, Anatolia, and Northern Mesopotamia and subsisted on dryland agriculture.
In 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. Natufian (1) between 12,000 and 10,200 BC, Khiamian (2) between 10,200-8,800 BC, PPNA: Sultanian (Jericho), Mureybetian, early PPNB ("PPNB ancien") (3) between 8,800-7,600 BC, middle PPNB ("PPNB moyen") 7,600-6,900 BC, late PPNB ("PPNB récent") (4) between 7,500 and 7,000 BC and a PPNB (sometimes called PPNC) transitional stage ("PPNB final") (5) where Halaf and dark faced burnished ware begin to emerge between 6,900-6,400 BC. They also advanced the idea of a transitional stage between the PPNA and PPNB between 8,800 and 8,600 BC at sites like Jerf el Ahmar and Tell Aswad.
Southern Mesopotamia.
Alluvial plains (Sumer/Elam). Little rainfall makes irrigation systems necessary. Ubaid culture from 6,900 BC.
North Africa.
Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6,000 BC. Graeme Barker states "The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium bc in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East.
Europe.
In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC. Anthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by c. 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing. The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to c. 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated c. 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and showing a degree of artistry in stone sculpture unique in prehistory to the Maltese islands.
After 2500 BC, the Maltese Islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population certainly different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found in the largest island of the Mediterranean sea.
South and East Asia.
The earliest Neolithic site in South Asia is Mehrgarh, dated to 7500 BC, in the Kachi plain of Baluchistan, Pakistan; the site has evidence of farming (wheat and barley) and herding (cattle, sheep and goats).
In South India, the Neolithic began by 3000 BC and lasted until around 1400 BC when the Megalithic transition period began. South Indian Neolithic is characterized by Ashmounds since 2500 BC in Karnataka region, expanded later to Tamil Nadu.
In East Asia, the earliest sites include Nanzhuangtou culture around 9500 BC to 9000 BC, Pengtoushan culture around 7500 BC to 6100 BC, and Peiligang culture around 7000 BC to 5000 BC.
The 'Neolithic' (defined in this paragraph as using polished stone implements) remains a living tradition in small and extremely remote and inaccessible pockets of West Papua (Indonesian New Guinea). Polished stone adze and axes are used in the present day (as of 2008[ [update]]) in areas where the availability of metal implements is limited. This is likely to cease altogether in the next few years as the older generation die off and steel blades and chainsaws prevail.
In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. "No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula". The farm was dated between 3600 and 3000 B.C. Pottery, stone projectile points, and possible houses were also found. "In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site.
America.
In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic and Paleo-Indian for the preceding period. The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the Southwestern United States it occurred from 500 to 1200 C.E. when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced.
Social organization.
During most of the Neolithic age, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Neolithic societies formed complex stratified chiefdoms similar to Polynesian societies such as the Ancient Hawaiians, most Neolithic societies were relatively simple and egalitarian. However, Neolithic societies were noticeably more hierarchical than the Paleolithic cultures that preceded them and hunter-gatherer cultures in general.
The domestication of animals (c. 8000 BC) resulted in a dramatic increase in social inequality. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others.
Families and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures ("Linearbandkeramik") were building large arrangements of circular ditches between 4800 BC and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities.
There is a large body of evidence for fortified settlements at "Linearbandkeramik" sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones have been discovered, such as at the Talheim Death Pit demonstrates "...systematic violence between groups" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a "peaceful, unfortified lifestyle".
Control of labour and inter-group conflict is characteristic of corporate-level or 'tribal' groups, headed by a charismatic individual; whether a 'big man' or a proto-chief, functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism.
Shelter.
The shelter of the early people changed dramatically from the paleolithic to the neolithic era. In the paleolithic, people did not normally live in permanent constructions. In the neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example.
Farming.
A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands.
The profound differences in human interactions and subsistence methods associated with the onset of early agricultural practices in the Neolithic have been called the "Neolithic Revolution", a term coined in the 1920s by the Australian archaeologist Vere Gordon Childe.
One potential benefit of the development and increasing sophistication of farming technology was the possibility of producing surplus crop yields, in other words, food supplies in excess of the immediate needs of the community. Surpluses could be stored for later use, or possibly traded for other necessities or luxuries. Agricultural life afforded securities that pastoral life could not, and sedentary farming populations grew faster than nomadic.
However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued.
Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development is still debated.
In addition, increased population density, decreased population mobility, increased continuous proximity to domesticated animals, and continuous occupation of comparatively population-dense sites would have altered sanitation needs and patterns of disease.
Technology.
The identifying characteristic of Neolithic technology is the use of polished or ground stone tools, in contrast to the flaked stone tools used during the Paleolithic era.
Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland.
Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatal höyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives.
The peoples of the Americas and the Pacific mostly retained the Neolithic level of tool technology until the time of European contact. Exceptions include copper hatchets and spearheads in the Great Lakes region.
Clothing.
Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins which are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones which (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper age).
Early settlements.
Neolithic human settlements include:
The world's oldest known engineered roadway, the Sweet Track in England, dates from 3800 BC and the world's oldest free-standing structure is the neolithic temple of Ggantija in Gozo, Malta.
List of cultures and sites.
"Note: Dates are very approximate, and are only given for a rough estimate; consult each culture for specific time periods."
Early Neolithic 
"Periodization: The Levant: 10,000 to 8500 BC; Europe: 5000 to 4000 BC; Elsewhere: varies greatly, depending on region."
Middle Neolithic 
"Periodization: The Levant: 8500 to 6500 BC; Europe: 4000 to 3500 BC; Elsewhere: varies greatly, depending on region."
Later Neolithic 
"Periodization: 6500 to 4500 BC; Europe: 3500 to 3000 BC; Elsewhere: varies greatly, depending on region."
"Periodization: Middle East: 4500 to 3300 BC; Europe: 3000 to 1700 BC; Elsewhere: varies greatly, depending on region. In the Americas, the Eneolithic ended as late as the 1800s for some people."
Bibliography.
</dl>

</doc>
<doc id="21190" url="http://en.wikipedia.org/wiki?curid=21190" title="Nomic">
Nomic

Nomic is a game created in 1982 by philosopher Peter Suber in which the rules of the game include mechanisms for the players to change those rules, usually beginning through a system of democratic voting. Nomic is a game in which changing the rules is a move. In that respect it differs from almost every other game. The primary activity of Nomic is proposing changes in the rules, debating the wisdom of changing them in that way, voting on the changes, deciding what can and cannot be done afterwards, and doing it. Even this core of the game, of course, can be changed.—Peter Suber, The Paradox of Self-Amendment
"Nomic" actually refers to a large number of games based on the initial ruleset laid out by Peter Suber in his book "The Paradox of Self-Amendment". (The ruleset was actually first published in Douglas Hofstadter's column "Metamagical Themas" in "Scientific American" in June 1982. The column discussed Suber's then-upcoming book, which was published some years later.) The game is in some ways modeled on modern government systems. It demonstrates that in any system where rule changes are possible, a situation may arise in which the resulting laws are contradictory or insufficient to determine what is in fact legal. Because the game models (and exposes conceptual questions about) a legal system and the problems of legal interpretation, it is named after νόμος ("nomos"), Greek for "law".
While the victory condition in Suber's initial ruleset is the accumulation of 100 points by the roll of dice, he once said that "this rule is deliberately boring so that players will quickly amend it to please themselves." Players can change the rules to such a degree that points can become irrelevant in favor of a true currency, or make victory an unimportant concern. Any rule in the game, including the rules specifying the criteria for winning and even the rule that rules must be obeyed, can be changed. Any loophole in the ruleset, however, may allow the first player to discover it with the chance to pull a "scam" and modify the rules to win the game. Complicating this process is the fact that Suber's initial ruleset allows for the appointment of judges to preside over issues of rule interpretation.
Gameplay.
The game can be played face-to-face with as many written notes as are required, or through any of a number of Internet media (usually an archived mailing list or Internet forum).
Initially, gameplay occurs in clockwise order, with each player taking a turn. In that turn, they propose a change in rules that all the other players vote on, and then roll a die to determine the number of points they add to their score. If this rule change is passed, it comes into effect at the end of their round. Any rule can be changed with varying degrees of difficulty, including the core rules of the game itself. As such, the gameplay may quickly change.
Under Suber's initial ruleset, rules are divided into two types: mutable and immutable. The main difference between these is that immutable rules must be changed into mutable rules (called "transmuting") before they can be modified or removed. Immutable rules also take precedence over mutable ones. A rule change may be:
Alternative starting rulesets exist for Internet and mail games, wherein gameplay occurs in alphabetical order by surname, and points added to the score are based on the success of a proposed rule change rather than random dice rolls.
Variants.
Not only can every aspect of the rules be altered in some way over the course of a game of Nomic, but myriad variants also exist: some that have themes, begin with a single rule, or begin with a dictator instead of a democratic process to validate rules. Others combine Nomic with an existing game (such as Monopoly, chess, or in one humorously paradoxical attempt, Mornington Crescent). There is even a version in which the players are games of Nomic themselves. Even more unusual variants include a ruleset in which the rules are hidden from players' view, and a game which, instead of allowing voting on rules, splits into two sub-games, one with the rule, and one without it.
Online versions often have initial rulesets where play is not turn-based; typically, players in such games may propose rule changes at any time, rather than having to wait for their turn.
One spin-off of a now-defunct Nomic (Nomic World) is called the Fantasy Rules Committee; it adds every legal rule submitted by a player to the ruleset until the players run out of ideas, after which all the "fantasy rules" are repealed and the game begins again.
Online play.
The game of Nomic is particularly suited to being played online, where all proposals and rules can be shared in web pages or email archives for ease of reference. Such games of Nomic sometimes last for a very long time – has been running since 1993. The longevity of nomic games can pose a serious problem, in that the rulesets can grow so complex that current players do not fully understand them and prospective players are deterred from joining. One currently active game, , gets around this problem by dividing the game into "dynasties"; every time someone wins, a new dynasty begins, and all the rules except a privileged few are repealed. This keeps the game relatively simple and accessible. Nomicron (now defunct) was similar in that it had rounds — when a player won a round, a convention was started to plan for the next round. A young game of Nomic on reddit, , uses a similar mechanism modeled on Nomicron's system.
Another facet of Nomic is the way in which the implementation of the rules affects the way the game of Nomic itself works. ThermodyNomic, for example, had a ruleset in which rule changes were carefully considered before implementation, and rules were rarely introduced which provide loopholes for the players to exploit. , by contrast, was once described by one of its players as "the equivalent of throwing logical hand grenades."
This is essentially part of the differentiation between "procedural" games, where the aim (acknowledged or otherwise) is to tie the entire ruleset into a paradoxical condition during each turn (a player who has no legal move available wins), and "substantive" games, which try to avoid paradox and reward winning by achieving certain goals, such as attaining a given number of points.
While "Nomic" is traditionally capitalized as the proper name of the game it describes, it has also sometimes been used in a more informal way as a lowercased generic term, "nomic", referring to anything with Nomic-like characteristics, including games where the rules may be changed during play as well as non-gaming situations where it can be alleged that "rules lawyers" are tinkering with the process used to amend rules and policies (in an organization or community) in a manner akin to a game of Nomic.
Computerized version.
The idea of a computerized Nomic is that the rules should be interpreted by a computer, rather than by humans. This implies that the rules should be written in a language that a computer can understand, typically some sort of programming language. is such an implementation.

</doc>
<doc id="21197" url="http://en.wikipedia.org/wiki?curid=21197" title="Nintendo">
Nintendo

Nintendo Co., Ltd. (Japanese: 任天堂株式会社, Hepburn: Nintendō Kabushiki gaisha) is a Japanese multinational consumer electronics company headquartered in Kyoto, Japan. Nintendo is the world's largest video game company by revenue. Founded on September 23, 1889, by Fusajiro Yamauchi, it originally produced handmade hanafuda playing cards. By 1963, the company had tried several small niche businesses, such as cab services and love hotels.
Abandoning previous ventures in favor of toys in the 1960s, Nintendo then developed into a video game company in the 1970s, ultimately becoming one of the most influential in the industry and Japan's third most valuable listed company with a market value of over US$85 billion. Nintendo of America is also the majority owner of the Seattle Mariners Major League Baseball team.
The word "Nintendo" can be roughly translated from Japanese to English as "leave luck to heaven." s of 31, 2014[ [update]], Nintendo reports historically cumulative sales of over 670.43 million hardware units and 4.23 billion software units.
History.
1889–1956: As a card company.
Nintendo was founded as a card company in late 1889, later (1951) named "Nintendo Koppai" (Nintendo Playing Card Co. Ltd.), by Fusajiro Yamauchi Based in Kyoto, Japan, the business produced and marketed a playing card game called Hanafuda. The handmade cards soon became popular, and Yamauchi hired assistants to mass-produce cards to satisfy demand. Nintendo now continues to manufacture playing cards in Japan and organizes its own contract bridge tournament called the "Nintendo Cup".
1956–1974: New ventures.
In 1956, Hiroshi Yamauchi, grandson of Fusajiro Yamauchi, visited the U.S. to talk with the United States Playing Card Company, the dominant playing card manufacturer there. He found that the biggest playing card company in the world was using only a small office. Yamauchi's realization that the playing card business had limited potential was a turning point. He then acquired the license to use Disney characters on playing cards to drive sales.
In 1963, Yamauchi renamed Nintendo Playing Card Co. Ltd. to Nintendo Co., Ltd. The company then began to experiment in other areas of business using newly injected capital during the period of time between 1963 and 1968. Nintendo set up a taxi company called "Daiya". This business was earlier successful however Nintendo was forced to sell it because problems with the labour unions were making it too expensive to run the service. It also set up a love hotel chain, a TV network, a food company (selling instant rice) and several other ventures. All of these ventures eventually failed, and after the 1964 Tokyo Olympics, playing card sales dropped, and Nintendo's stock price plummeted to its lowest recorded level of ¥60.
In 1966, Nintendo moved into the Japanese toy industry with the Ultra Hand, an extendable arm developed by its maintenance engineer Gunpei Yokoi in his free time. Yokoi was moved from maintenance to the new "Nintendo Games" department as a product developer. Nintendo continued to produce popular toys, including the Ultra Machine, Love Tester and the "Kousenjuu" series of light gun games. Despite some successful products, Nintendo struggled to meet the fast development and manufacturing turnaround required in the toy market, and fell behind the well-established companies such as Bandai and Tomy.
In 1973, its focus shifted to family entertainment venues with the Laser Clay Shooting System, using the same light gun technology used in Nintendo's "Kousenjuu" series of toys, and set up in abandoned bowling alleys. Following some success, Nintendo developed several more light gun machines (such as the light gun shooter game "Wild Gunman") for the emerging arcade scene. While the Laser Clay Shooting System ranges had to be shut down following excessive costs, Nintendo had found a new market.
1974–1978: Early electronic era.
Nintendo's first venture into the video gaming industry was securing rights to distribute the Magnavox Odyssey video game console in Japan in 1974. Nintendo began to produce its own hardware in 1977, with the Color TV-Game home video game consoles. Four versions of these consoles were produced, each including variations of a single game (for example, Color TV Game 6 featured six versions of "Light Tennis").
A student product developer named Shigeru Miyamoto was hired by Nintendo at this time. He worked for Yokoi, and one of his first tasks was to design the casing for several of the Color TV Game consoles. Miyamoto went on to create, direct and produce some of Nintendo's most famous video games and become one of the most recognizable figures in the video game industry.
In 1975, Nintendo moved into the video arcade game industry with "EVR Race", designed by their first game designer, Genyo Takeda, and several more titles followed. Nintendo had some small success with this venture, but the release of "Donkey Kong" in 1981, designed by Miyamoto, changed Nintendo's fortunes dramatically. The success of the game and many licensing opportunities (such as ports on the Atari 2600, Intellivision and ColecoVision) gave Nintendo a huge boost in profit and in addition, the game also introduced an early iteration of Mario, known then as Jumpman, the eventual mascot of the company.
1979–2003: Success with video games.
In 1979, Gunpei Yokoi conceived the idea of a handheld video game, while observing a fellow bullet train commuter who passed the time by interacting idly with a portable LCD calculator, which gave birth to "Game & Watch". In 1980, Nintendo launched "Game & Watch"—a handheld video game series developed by Yokoi. These systems do not contain interchangeable cartridges and thus the hardware was tied to the game. The first Game & Watch game released, titled "Ball", was distributed worldwide. The modern "cross" D-pad design was developed in 1982, by Yokoi for a "Donkey Kong" version. Proven to be popular, the design was patented by Nintendo. It later earned a Technology & Engineering Emmy Award.
In 1983, Nintendo launched the Family Computer (colloquialized as "Famicom") home video game console in Japan, alongside ports of its most popular arcade titles. In 1985, a cosmetically reworked version of the system known outside of Japan as the Nintendo Entertainment System or NES, launched in North America. The practice of bundling the system along with select games helped to make "Super Mario Bros." one of the best-selling video games in history.
In 1988, Gunpei Yokoi and his team at Nintendo R&D1 conceived the new Game Boy handheld system, with the purpose of merging the two very successful ideas of the Game & Watch's portability along with the NES's cartridge interchangeability. Nintendo released the Game Boy in Japan on April 21, 1989, and in North America on July 31, 1989. Nintendo of America president Minoru Arakawa managed a deal to bundle the popular third party game "Tetris" along with the Game Boy, and the pair launched as an instant success.
In 1989, Nintendo announced plans to release the successor to the Famicom, the Super Famicom. Based on a 16-bit processor, Nintendo boasted significantly superior hardware specifications of graphics, sound, and game speed over the original 8-bit Famicom. The system was also said to have backwards compatibility with Famicom games, though this feature was ultimately cut upon release. The Super Famicom was finally released relatively late to the market in Japan on November 21, 1990, and released as the Super Nintendo Entertainment System (abbreviated to SNES or Super Nintendo) in North America on August 23, 1991 and in Europe in 1992. Its main rival was the 16-bit Sega Mega Drive, known in North America as Sega Genesis, which had been advertised aggressively against the nascent 8-bit NES. A console war between Sega and Nintendo ensued during the early 1990s. From 1990 to 1992, Nintendo opened "World of Nintendo" shops in the United States where consumers could test and buy Nintendo products.
In August 1993, Nintendo announced the SNES's successor, code-named "Project Reality". Featuring 64-bit graphics, the new system was developed as a joint venture between Nintendo and North-American-based technology company Silicon Graphics. The system was announced to be released by the end of 1995, but was subsequently delayed. Meanwhile, Nintendo continued the Nintendo Entertainment System family with the release of the NES-101, a smaller redesign of the original NES. Nintendo also announced a CD drive peripheral called the SNES-CD, which was co-developed first by Sony with the name "Play Station" and then by Philips. Bearing prototypes and joint announcements at the Consumer Electronics Show, it was on track for a 1994 release, but was controversially cancelled.
During the same year, Nintendo announced that it had sold one billion game cartridges worldwide, one tenth of it being from the Mario franchise. This prompted Nintendo to deem 1994 the "Year of the Cartridge". To further their support for cartridges, Nintendo announced that Project Reality, which had now been renamed the Ultra 64, would not use a CD format as expected, but would rather use cartridges as its primary media format. Nintendo IRD general manager Genyo Takeda was impressed by video game development company Rare Ltd.'s progress with real-time 3D graphics technology, using state of the art Silicon Graphics workstations. As a result, Nintendo bought a 25% stake in the company, eventually expanding to 49%, and offered their catalogue of characters to create a CGI game around, making Rare a Nintendo's first western-based second-party developer. Their first game as partners with Nintendo was "Donkey Kong Country". The game was a critical success and sold over eight million copies worldwide, making it the second best-selling game in the SNES library. In September 1994, Nintendo, along with six other video game giants including Sega, Electronic Arts, Atari, Acclaim, Philips, and 3DO approached the United States Senate and demanded a ratings system for video games to be enforced, with prompted the decision to create the Entertainment Software Rating Board.
Aiming to produce an affordable virtual reality console, Nintendo released the Virtual Boy in 1995, designed by Gunpei Yokoi. The console consists of a head-mounted semi-portable system with one red-colored screen for each of the user's eyes, featuring stereoscopic graphics. Games are viewed through a binocular eyepiece and controlled using an affixed gamepad. Critics were generally disappointed with the quality of the games and the red-colored graphics, and complained of gameplay-induced headaches. The system sold poorly and was quietly discontinued. Amid the system's failure, Yokoi retired from Nintendo. During the same year, Nintendo launched the Satellaview in Japan, a peripheral for the Super Famicom. The accessory allowed users to play video games via broadcast for a set period of time. Various games were made exclusively for the platform, as well as various remakes.
In 1996, Nintendo released the Ultra 64 as the Nintendo 64 in Japan and North America. The console was later released in Europe and Australia in 1997. Despite the limitations set by using cartridges, the technical specifications of the Nintendo 64 surpassed its competitors. With its market shares slipping to the Sega Saturn and partner-turned-rival Sony PlayStation, Nintendo revitalized its brand by launching a $185 million marketing campaign centered around the "Play it Loud" slogan. During the same year, Nintendo also released the Game Boy Pocket in Japan, a smaller version of the Game Boy that generated more sales for the platform. On October 4, 1996, famed Nintendo developer Gunpei Yokoi died in a car crash. In 1997, Nintendo released the SNS-101 (called Super Famicom Jr. in Japan), a smaller redesigned version of the Super Nintendo Entertainment System.
In 1998, the successor to the Game Boy, the Game Boy Color, was released. The system had improved technical specifications allowing it to run games made specifically for the system as well as games released for the Game Boy, albeit with added color. The Game Boy Camera and Printer were also released as accessories. In October 1998, Retro Studios was founded as an alliance between Nintendo and former Iguana Entertainment founder Jeff Spangenberg. Nintendo saw an opportunity for the new studio to create games for the upcoming GameCube targeting an older demographic, in the same vein as Iguana Entertainment's successful "" series for the Nintendo 64.
In 2001, just three years later, Nintendo introduced the totally redesigned Game Boy Advance. The same year, Nintendo also released the GameCube to lukewarm sales, and it ultimately failed to regain the market share lost by the Nintendo 64.
In 2003, Nintendo released the Game Boy Advance SP, its fourth handheld system.
2004–2011: Nintendo DS and Wii.
In 2004, Nintendo released the Nintendo DS, its fourth major handheld system. The DS is a dual screened handheld featuring touch screen capabilities, which respond to either a stylus or the touch of a finger. Former Nintendo president and now chairman Hiroshi Yamauchi was translated by GameScience as explaining, "If we can increase the scope of the industry, we can re-energise the global market and lift Japan out of depression - that is Nintendo's mission." Regarding lukewarm GameCube sales which had yielded the company's first reported operating loss in over 100 years, Yamauchi continued: "The DS represents a critical moment for Nintendo's success over the next two years. If it succeeds, we rise to the heavens, if it fails, we sink into hell." Thanks to titles such as Nintendogs and Mario Kart DS, the DS became a success. In 2005, Nintendo released the Game Boy Micro in North America, a redesign of the Game Boy Advance. The last system in the Game Boy line, it was also the smallest Game Boy, and the least successful. In the middle of 2005, Nintendo opened the Nintendo World Store in New York City, which would sell Nintendo games, present a museum of Nintendo history, and host public parties such as for product launches.
In the first half of 2006, Nintendo released the Nintendo DS Lite, a version of the original Nintendo DS with lighter weight, brighter screen, and better battery life. In addition to this streamlined design, its prolific subset of casual games appealed to the masses, such as the Brain Age series. Meanwhile, New Super Mario Bros. provided a substantial addition to the "Mario" series when it was launched to the top of sales charts. The successful direction of the Nintendo DS had a big influence on Nintendo's next home console, which had been code named "Revolution" and was now renamed to "Wii".
In the latter half of 2006, Nintendo released the Wii as the backwards-compatible successor to the GameCube. Based upon intricate Wii Remote motion controls and a balance board, the Wii inspired several new game franchises, some targeted at entirely new market segments of casual and fitness gaming. At over 100 million units, the Wii is the best selling console of the seventh generation, regaining the market share lost during the tenures of the Nintendo 64 and the GameCube.
On May 1, 2007, Nintendo acquired an 80% stake on video game development company Monolith Soft, previously owned by Bandai Namco. Monolith Soft is best known for developing role-playing games such as the Xenosaga and Baten Kaitos series.
During the holiday season of 2008, Nintendo followed up the success of the DS Lite with the release of the Nintendo DSi in Japan. The system features two cameras, one facing towards the player and one facing outwards, and had an online distribution store called DSiWare. The DSi was later released worldwide during 2009. In the latter half of 2009, Nintendo released the Nintendo DSi XL in Japan, a larger version of the DSi. This updated system was later released worldwide in 2010.
2011–present: Nintendo 3DS and Wii U.
In 2011, Nintendo greatly expanded the DS legacy by releasing the Nintendo 3DS, based upon a glasses-free 3D display.
In February 2012, Nintendo acquired Mobiclip, a France-based research and development company specialized in highly optimized software technologies such as video compression. The company's name was later changed to Nintendo European Research & Development. During the fourth quarter of 2012, Nintendo released the Wii U. It sold slower than expected, although being the first eighth generation console. By September 2013, however, sales had rebounded. Intending to broaden the 3DS market, Nintendo released 2013's cost-reduced Nintendo 2DS. The 2DS is completely compatible but lacks the 3DS's more expensive but cosmetic autostereoscopic 3D feature. Nintendo also released the Wii Mini, a cheaper and non-networked redesign of the Wii.
2013 onward: Mobile, QOL, and the NX.
On September 25, 2013, Nintendo announced it had purchased a 28% stake in a Panasonic spin-off company called PUX Corporation. The company specializes in face and voice recognition technology, with which Nintendo intends to improve the usability of future game systems. Nintendo has also worked with this company in the past to create character recognition software for a Nintendo DS touchscreen. After announcing a 30% dive in profits for the April to December 2013 period, President Satoru Iwata announced he would take a 50% pay-cut, with other executives seeing reductions by 20%-30%. During a May 7, 2014, investors' meeting, Nintendo confirmed that it had spent over $150 million on an acquisition of an unspecified, non-Japanese, non-gaming, technology company. In January 2015, Nintendo announced its exit from the Brazilian market after four years of distributing products in the country. Nintendo cited high import duties and lack of local manufacturing operation as reasons for leaving. Nintendo continues its partnership with Juegos de Video Latinoamérica to distribute products to rest of Latin America.
In March 2015, Nintendo announced that they would team up with Japanese mobile company DeNA to produce games for smart devices.
On the same day Nintendo also announced a new "dedicated games platform with a brand new concept" under the codename "NX" that would be further revealed in 2016. In May 2015, Universal Parks and Resorts announced that they were teaming up with Nintendo to create attractions at Universal Parks based upon Nintendo properties.
Products.
Home consoles.
Nintendo Entertainment System/Famicom Disk System.
The Nintendo Entertainment System (abbreviated as NES or Nintendo) is an 8-bit video game console, which released in North America in 1985, and in Europe throughout 1986 and 1987. The console was initially released in Japan as the Family Computer (abbreviated as Famicom) in 1983. The best-selling gaming console of its time,e[›] the NES helped revitalize the US video game industry following the video game crash of 1983. With the NES, Nintendo introduced a now-standard business model of licensing third-party developers, authorizing them to produce and distribute titles for Nintendo's platform. The NES was bundled with "Super Mario Bros.", one of the best-selling video games of all time, and received ports of Nintendo's most popular arcade titles. s of 31, 2014[ [update]], Nintendo reports sales of 61.91 million NES hardware units and 500.01 million NES software units worldwide.
Super Nintendo Entertainment System.
The Super Nintendo Entertainment System (abbreviated as SNES, Super NES or Super Nintendo) is a 16-bit video game console, which was released in North America in 1991, and in Europe in 1992. The console was initially released in Japan in 1990 as the Super Famicom, officially adopting the colloquially abbreviated name of its predecessor. The console introduced advanced graphics and sound capabilities compared with other consoles at the time. Soon, the development of a variety of enhancement chips which were integrated onto each new game cartridge's circuit boards, progressed the SNES's competitive edge. While even crude three-dimensional graphics had previously rarely been seen on home consoles, the SNES's enhancement chips suddenly enabled a new caliber of games containing increasingly sophisticated faux 3D effects as seen in 1991's "Pilotwings" and 1992's "Super Mario Kart", and then fundamentally three-dimensional worlds beginning with 1993's SuperFX-powered "Star Fox" game. This platform-enhancing development in cartridge technology sparked the industry's increasingly widespread interest in polygon graphics, helping to usher in a fundamental shift to 3D graphics as seen in the next generation. The SNES is the best-selling console of the 16-bit era although having experienced a relatively late start and fierce competition from Sega's Genesis/Mega Drive console. s of 31, 2014[ [update]], Nintendo reports sales of 49.10 million SNES hardware units and 379.06 million SNES software units worldwide.
Nintendo 64.
The Nintendo 64 was released in 1996, featuring 3D polygon model rendering capabilities and built-in multiplayer for up to four players. The system's controller introduced the analog stick and later introduced the Rumble Pak, an accessory for the controller that produces force feedback with compatible games. Both are the first such features to have come to market for home console gaming and eventually became the "de facto" industry standard. Announced in 1995, prior to the console's 1996 launch, the 64DD ("DD" standing for "Disk Drive") was designed to enable the development of new genre of video games by way of 64 MB writable magnetic disks, video editing, and Internet connectivity. Eventually released only in Japan in 1999, the 64DD peripheral's commercial failure there resulted in only nine games being released and precluded further worldwide release.
GameCube.
The GameCube (officially called Nintendo GameCube, abbreviated NGC in Japan and GCN in North America) was released in 2001, in Japan and North America, and in 2002 worldwide. The sixth-generation console is the successor to the Nintendo 64 and competed with Sony's PlayStation 2, Microsoft's Xbox, and Sega's Dreamcast. The GameCube is the first Nintendo console to use optical discs as its primary storage medium. The discs are similar to the miniDVD format, but the system was not designed to play standard DVDs or audio CDs. Nintendo introduced a variety of connectivity options for the GameCube. The GameCube's game library has sparse support for Internet gaming, a feature that requires the use of the aftermarket Nintendo GameCube Broadband Adapter and Modem Adapter. The GameCube supports connectivity to the Game Boy Advance, allowing players to access exclusive in-game features using the handheld as a second screen and controller. s of 31, 2014[ [update]], Nintendo reports sales of 21.74 million GameCube hardware units and 208.57 million GameCube software units worldwide.
Wii.
The Wii was released during the holiday season of 2006 worldwide. The system the Wii Remote controller, which can be used as a handheld pointing device and which detects movement in three dimensions. Another notable feature of the console is WiiConnect24, which enables it to receive messages and updates over the Internet while in standby mode. It also features a game download service, called "Virtual Console", which features emulated games from past systems. Since its release, the Wii has spawned many peripheral devices, including the Wii Balance Board and Motion Plus, and has had several hardware revisions. The "Wii Family Edition" variant is identical to the original model, but is designed to sit horizontally and removes the GameCube compatibility. The "Wii Mini" is a smaller, redesigned Wii which lacks GameCube compatibility, online connectivity, the SD card slot and Wi-Fi support, and has only one USB port unlike the previous models' two. s of 31, 2014[ [update]], Nintendo reports sales of 101.06 million Wii hardware units and 895.22 million Wii software units worldwide, making it Nintendo's best-selling home video game console.
Wii U.
The Wii U, the successor to the Wii, was released during the holiday season of 2012 worldwide. The Wii U is the first Nintendo console to support high-definition graphics. The Wii U's primary controller is the Wii U GamePad, which features an embedded touchscreen. Each software title may be designed to utilize this touchscreen as being supplemental to the main TV, or as the only screen for Off-TV Play. The system supports most Wii controllers and accessories, and the more classically shaped Wii U Pro Controller. The system is backward compatible with Wii software and accessories; this mode also utilizes Wii-based controllers, and it optionally offers the GamePad as its primary Wii display and motion sensor bar. The console has various online services powered by Nintendo Network, including: the Nintendo eShop for online distribution of software and content; and Miiverse, a social network which can be variously integrated with games and applications. As of December 2014, worldwide Wii U sales had totaled 9.20 million hardware units and 52.87 million software units.
Handheld consoles.
Game & Watch.
Game and Watch is a handheld line produced from 1980 to 1991 by Gunpei Yokoi. It features a single game and a clock and/or alarm.
Game Boy.
After the success of the "Game & Watch" series, Yokoi developed the Game Boy handheld console, which was released in 1989. Eventually becoming the best-selling handheld of all time, the Game Boy remained dominant for more than a decade, seeing critically and commercially popular games such as "Pokémon Yellow" released as late as 1998 in Japan and 2000 in Europe. Incremental updates of the Game Boy, including "Game Boy Pocket", "Game Boy Light" and "Game Boy Color", did little to change the original formula, though the latter introduced color graphics to the Game Boy line.
The first major update to its handheld line since 1989, Game Boy Advance features improved technical specifications similar to those of the SNES. The "Game Boy Advance SP" was the first revision to the GBA line and introduced screen lighting and a clam shell design, while later iteration, the "Game Boy Micro", brought a smaller form factor.
Nintendo DS.
Although originally advertised as an alternative to the Game Boy Advance, the Nintendo DS replaced the Game Boy line after its initial release in 2004. It was distinctive for its dual screens and a microphone, as well as a touch-sensitive lower screen. The "Nintendo DS Lite" brought a smaller form factor while the "Nintendo DSi" features larger screens and two cameras, and was followed by an even larger model, the "Nintendo DSi XL", with a 90% bigger screen.
Nintendo 3DS.
Further expanding the Nintendo DS line, the Nintendo 3DS uses the process of autostereoscopy to produce a stereoscopic three-dimensional effect without glasses. Released to major markets during 2011, the 3DS got off to a slow start, initially missing many key features that were promised before the system launched. Partially as a result of slow sales, Nintendo stock declined in value. Subsequent price cuts and game releases helped to boost 3DS and 3DS software sales and to renew investor confidence in the company. As of August 2013, the 3DS was the best selling console in the United States for four consecutive months. The "Nintendo 3DS XL" was introduced in August 2012 and includes a 90% larger screen, a 4GB SD card and extended battery life. In August 2013, Nintendo announced the cost-reduced "Nintendo 2DS", a version of the 3DS without an autostereoscopic 3D screen. It has a slate-like design as opposed to the hinged, clamshell design of its DS-line predecessors. The 2DS was released on October 12, 2013 in North America, Europe, Australia, and New Zealand, although no Japanese release has been announced.
In August 2014, it was announced that Japan would receive a new 3DS called "New 3DS" with extra shoulder buttons, a right analogue stick, faster processor, compatibility with Amiibo and other changes. It was released in October 2014.
Organization.
Marketing.
Nintendo of America has engaged in several high-profile marketing campaigns to define and position its brand. One of its earliest and most enduring slogans was "Now you're playing with power!", used first to promote its Nintendo Entertainment System. It modified the slogan to include "SUPER power" for the Super Nintendo Entertainment System, and "PORTABLE power" for the Game Boy. Its 1994 "Play It Loud!" campaign played upon teenage rebellion and fostered an edgy reputation. During the Nintendo 64 era, the slogan was "Get N or get out." During the GameCube era, the "Who Are You?" suggested a link between the games and the players' identities. The company promoted its Nintendo DS handheld with the tagline "Touching is Good." For the Wii, they used the "Wii would like to play" slogan to promote the console with the people who tried the games including "Super Mario Galaxy" and "Super Paper Mario". The Nintendo DS's successor, the Nintendo 3DS, used the slogan "Take a look inside". The Wii's successor, the Wii U, used the slogan "How U will play next."
Board of directors.
Other key executives:
International divisions.
Nintendo Co., Ltd. (NCL).
Headquartered in Kyoto, Japan since the beginning, Nintendo Co., Ltd. oversees the organization's global operations and manages Japanese operations specifically. The company's two major subsidiaries, Nintendo of America and Nintendo of Europe, manage operations in North America and Europe respectively. Nintendo Co., Ltd. moved from its original Kyoto location to a new office in Higashiyama-ku, Kyoto,; in 2000, this became the research and development building when the head office relocated to its present[ [update]] location in Minami-ku, Kyoto.
Nintendo of America (NOA).
Nintendo's North American subsidiary is based in Redmond, Washington. Originally the NOA headquarters handled sales, marketing, and advertising. However, the office in Redwood City, California now directs those functions. The company maintains distribution centers in Atlanta (Nintendo Atlanta) and North Bend, Washington (Nintendo North Bend). The 380000 sqft Nintendo North Bend facility processes more than 20,000 orders a day to Nintendo customers, which include retail stores that sell Nintendo products in addition to consumers who shop Nintendo's web site. Nintendo of America's Canadian branch, Nintendo of Canada, Ltd. (NOCL), is based in Vancouver, BC, with a distribution center in Toronto, Ontario.
Nintendo of Europe (NOE).
Nintendo's European subsidiary was established in June 1990, based in Großostheim, close to Frankfurt, Germany. The company handles operations in Europe and South Africa. Nintendo of Europe's United Kingdom branch handles operations in that country and in Ireland from its headquarters in Windsor, Berkshire. In June 2014, NOE initiated a reduction and consolidation process, yielding a combined 130 layoffs: the closing of its office and warehouse, and termination of all employment, in Großostheim; and the consolidation of all of those operations into, and terminating some employment at, its Frankfurt location.
Nintendo Australia (NAL).
Nintendo's Australian subsidiary is based in Melbourne, Victoria. It handles the publishing, distribution, sales and marketing of Nintendo products in Australia, New Zealand, and Oceania (Cook Islands, Fiji, New Caledonia, Papua New Guinea, Samoa, and Vanuatu). It also manufactures some Wii games locally. Nintendo Australia is also a third-party distributor of some titles from Rising Star Games, Namco Bandai Games Europe, Atlus, The Tetris Company, Sega, Tecmo Koei Games Europe and Capcom Europe.
iQue, Ltd..
A Chinese joint venture between its founder, Wei Yen, and Nintendo, manufactures and distributes official Nintendo consoles and games for the mainland Chinese market, under the iQue brand. The product lineup for the Chinese market is considerably different from that for other markets. For example, Nintendo's only console in China is the iQue Player, a modified version of the Nintendo 64. The company has not released its more modern GameCube or Wii to the market, although a version of the Nintendo 3DS XL was released in 2012. As of 2013, it is a 100% Nintendo-owned subsidiary.
Nintendo of Korea (NOK).
Nintendo's South Korean subsidiary was established on July 7, 2006.
Research & Development.
Divisions.
Nintendo's internal Research & Development operations are divided into four main division: the "Nintendo Entertainment Analysis & Development" (or "EAD"), the main software development division of Nintendo, which focuses on internal-only video game development; the "Nintendo Software Planning & Development" (or "SPD"), which main focus is overseeing second and third-party licensing and development activity; the "Nintendo Integrated Research & Development" (or "IRD"), the main hardware development division of Nintendo, which focuses on home and handheld video game console development; and the "Nintendo System Development" (or "SDD"), which focuses on developing Nintendo Network services and Software Development Kits (SDK's) for Nintendo consoles and other experimental technology.
Subsidiaries.
Although most of the Research & Development is being done in Japan, there are some R&D facilities in the United States and Europe that are focused on developing software and hardware technologies used in Nintendo products. Although they all are subsidiaries of Nintendo (and therefore "first party"), they are often referred to as external resources when being involved in joint development processes with Nintendo's "internal" developers by the Japanese personal involved. This can be seen in a variety of "Iwata asks..." interviews. "Nintendo Software Technology" ("NST") and "Nintendo Technology Development" ("NTD") are located in Redmond, Washington, USA, while "Nintendo European Research & Development" ("NERD") is located in Paris, France, and "Nintendo Network Service Database" ("NSD") is located in Kyoto, Japan.
Most "external first-party" software development is being done in Japan, since the only overseas subsidiary is "Retro Studios" in the United States. Although these studios are all subsidiaries of Nintendo (and therefore "first party"), they are often referred to as external resources when being involved in joint development processes with Nintendo's "internal" developers by the "Nintendo Software Planning & Development" (or "SPD") division. 1-UP Studio, Creatures Inc. and Nd Cube are located in Tokyo, Japan, while Monolith Soft has one studio located in Tokyo and another in Kyoto, Japan. Finally, Retro Studios is located in Austin, Texas, USA.
Partners.
Since the release of the Famicom/Nintendo Entertainment System, Nintendo has built up a large group of second-party development partners, through publishing agreements and development collaboration. Most of these "external" Nintendo project are overseen by the "Nintendo Software Planning & Development" (or "SPD") division.
Policy.
Content guidelines.
For many years, Nintendo had a policy of strict content guidelines for video games published on its consoles. Although Nintendo of Japan allowed graphic violence in its video games, nudity and sexuality were strictly prohibited. Former Nintendo president Hiroshi Yamauchi believed that if the company allowed the licensing of pornographic games, the company's image would be forever tarnished. Nintendo of America went further in that games released for Nintendo consoles could not feature nudity, sexuality, profanity (including racism, sexism or slurs), blood, graphic or domestic violence, drugs, political messages or religious symbols (with the exception of widely unpracticed religions, such as the Greek Pantheon). The Japanese parent company was concerned that it may be viewed as a "Japanese Invasion" by forcing Japanese community standards on North American and European children. Despite the strict guidelines, some exceptions have occurred: "Bionic Commando" (though swastikas were eliminated in the US version), "Smash TV" and ' contained human violence, the latter also containing implied sexuality and tobacco use; "River City Ransom" and ' contained nudity, and the latter also contained religious images, as did ' and "".
A known side effect of this policy was the Sega Genesis version of "Mortal Kombat" selling over double the number of the Super NES version, mainly because Nintendo had forced publisher Acclaim to recolor the red blood to look like white sweat and replace some of the more gory graphics in its release of the game, making it less violent. By contrast, Sega allowed blood and gore to remain in the Genesis version (though a code was required to unlock the gore). Nintendo allowed the Super NES version of "Mortal Kombat II" to ship uncensored the following year with a content warning on the packaging.
In 1994 and 2003, when the ESRB and PEGI (respectively) video game ratings systems were introduced, Nintendo chose to abolish most of these policies in favor of consumers making their own choices about the content of the games they played. Today, changes to the content of games are done primarily by the game's developer or, occasionally, at the request of Nintendo. The only clear-set rule is that ESRB AO-rated games will not be licensed on Nintendo consoles in North America, a practice which is also enforced by Sony and Microsoft, its two greatest competitors in the present market. Nintendo has since allowed several mature-content games to be published on its consoles, including: "Perfect Dark", "Conker's Bad Fur Day", "Doom" and "Doom 64", "BMX XXX", the "Resident Evil" series, "Killer7", the "Mortal Kombat" series, ', "BloodRayne", "Geist" and '. Certain games have continued to be modified, however. For example, Konami was forced to remove all references to cigarettes in the 2000 Game Boy Color game "Metal Gear Solid" (although the previous NES version of "Metal Gear" and the subsequent GameCube game ' both included such references, as did Wii title "MadWorld"), and maiming and blood were removed from the Nintendo 64 port of "Cruis'n USA". Another example is in the Game Boy Advance game "Mega Man Zero 3", in which one of the bosses, called Hellbat Schilt in the Japanese and European releases, was renamed Devilbat Schilt in the North American localization. In North America releases of the "Mega Man Zero" games, enemies and bosses killed with a saber attack would not gush blood as they did in the Japanese versions. However, the release of the Wii has been accompanied by a number of even more controversial mature titles, such as "Manhunt 2", "No More Heroes", ' and "MadWorld", the latter three of which are published exclusively for the console. The Nintendo DS also has violent games, such as ', ' and its sequel, "Ultimate Mortal Kombat", and "".
License guidelines.
Nintendo of America also had guidelines before 1993 that had to be followed by its licensees to make games for the Nintendo Entertainment System, in addition to the above content guidelines. Guidelines were enforced through the 10NES lockout chip.
The last rule was circumvented in a number of ways; for example, Konami, wanting to produce more games for Nintendo's consoles, formed Ultra Games and later Palcom to produce more games as a technically different publisher. This disadvantaged smaller or emerging companies, as they could not afford to start additional companies. In another side effect, Square Co. (now Square Enix) executives have suggested that the price of publishing games on the Nintendo 64 along with the degree of censorship and control that Nintendo enforced over its games, most notably "Final Fantasy VI", were factors in switching its focus towards Sony's PlayStation console.
In 1993, a class action suit was taken against Nintendo under allegations that their lockout chip enabled unfair business practices. The case was settled, with the condition that California consumers were entitled to a $3 discount coupon for a game of Nintendo's choice.
Emulation.
Nintendo is opposed to any third-party emulation of its video games and consoles, stating that it is the single largest threat to the intellectual property rights of video game developers. However, emulators have been used by Nintendo and licensed third party companies as a means to re-release older games (through the Virtual Console). Nintendo remains the only modern console manufacturer that has not sued an emulator developer.
Seal of Quality.
The gold sunburst seal was first used by Nintendo of America, and later Nintendo of Europe. It is displayed on any game, system, or accessory licensed for use on one of its video game consoles, denoting the game has been properly approved by Nintendo. The seal is also displayed on any Nintendo-licensed merchandise, such as trading cards, game guides, or apparel, albeit with the words "Official Nintendo Licensed Product".
Sid Meier in 2008 cited the Seal of Quality as one of the three most important innovations in videogame history, as it helped set a standard for game quality that protected consumers from shovelware.
NTSC regions.
In NTSC regions, this seal is an elliptical starburst titled "Official Nintendo Seal." Originally, for NTSC countries, the seal was a large, black and gold circular starburst. The seal read as follows: "This seal is your assurance that NINTENDO has approved and guaranteed the quality of this product." This seal was later altered in 1988: "approved and guaranteed" was changed to "evaluated and approved." In 1989, the seal became gold and white, as it currently appears, with a shortened phrase, "Official Nintendo Seal of Quality." It was changed in 2003 to read "Official Nintendo Seal."
The seal currently reads:
The official seal is your assurance that this product is licensed or manufactured by Nintendo. Always look for this seal when buying video game systems, accessories, games and related products.
PAL regions.
In PAL regions, the seal is a circular starburst titled, "Original Nintendo Seal of Quality." Text near the seal in the Australian Wii manual states:
This seal is your assurance that Nintendo has reviewed this product and that it has met our standards for excellence in workmanship, reliability and entertainment value. Always look for this seal when buying games and accessories to ensure complete compatibility with your Nintendo product.
Environmental record.
Nintendo has consistently been ranked last in Greenpeace's "Guide to Greener Electronics" due to Nintendo's failure to publish information. Similarly, they are ranked last in the Enough Project's "Conflict Minerals Company Rankings" due to Nintendo's refusal to respond to multiple requests for information.
Like many other electronics companies, Nintendo does offer a take-back recycling program which allows customers to mail in old products they no longer use; Nintendo of America claimed that it took in 548 tons of returned products in 2011, 98% of which was either reused or recycled.

</doc>
<doc id="21201" url="http://en.wikipedia.org/wiki?curid=21201" title="Nobel Prize">
Nobel Prize

The Nobel Prize (], Swedish definite form, singular: "Nobelpriset"; Norwegian: "Nobelprisen") is a set of annual international awards bestowed in a number of categories by Swedish and Norwegian committees in recognition of academic, cultural and/or scientific advances. The will of the Swedish inventor Alfred Nobel established the prizes in 1895. The prizes in Chemistry, Literature, Peace, Physics, and Physiology or Medicine were first awarded in 1901. The related Nobel Memorial Prize in Economic Sciences was established by Sweden's central bank in 1968. The Nobel prize was made of the mixture of gold and silver with 24 carat or 4.8 g gold coating. Between 1901 and 2012, the Nobel Prizes and the Prize in Economic Sciences were awarded 555 times to 856 people and organizations. With some receiving the Nobel Prize more than once, this makes a total of 835 individuals (791 men and 44 women) and 21 organizations.
The prizes are awarded in Stockholm, Sweden, except for the peace prize which is awarded in Oslo, Norway. The Nobel Prize is widely regarded as the most prestigious award available in the fields of literature, medicine, physics, chemistry, peace, and economics.
The Royal Swedish Academy of Sciences awards the Nobel Prize in Physics, the Nobel Prize in Chemistry, and the Nobel Memorial Prize in Economic Sciences; the Nobel Assembly at Karolinska Institutet awards the Nobel Prize in Physiology or Medicine; the Swedish Academy grants the Nobel Prize in Literature; and the Nobel Peace Prize is awarded not by a Swedish organisation but by the Norwegian Nobel Committee.
The various prizes are awarded yearly. Each recipient, or laureate, receives a gold medal, a diploma and a sum of money, which is decided by the Nobel Foundation. s of 2012[ [update]], each prize was worth 8 million SEK (c. US$, €0.93 million). The prize is not awarded posthumously; however, if a person is awarded a prize and dies before receiving it, the prize may still be presented. Though the average number of laureates per prize increased substantially during the 20th century, a prize may not be shared among more than three people.
History.
Alfred Nobel (  ) was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers. He was a chemist, engineer, and inventor. In 1894, Nobel purchased the Bofors iron and steel mill, which he made into a major armaments manufacturer. Nobel also invented ballistite. This invention was a precursor to many smokeless military explosives, especially the British smokeless powder cordite. As a consequence of his patent claims, Nobel was eventually involved in a patent infringement lawsuit over cordite. Nobel amassed a fortune during his lifetime, with most of his wealth from his 355 inventions, of which dynamite is the most famous.
In 1888, Nobel was astonished to read his own obituary, titled "The merchant of death is dead", in a French newspaper. As it was Alfred's brother Ludvig who had died, the obituary was eight years premature. The article disconcerted Nobel and made him apprehensive about how he would be remembered. This inspired him to change his will. On 10 December 1896, Alfred Nobel died in his villa in San Remo, Italy, from a cerebral haemorrhage. He was 63 years old.
Nobel wrote several wills during his lifetime. He composed the last over a year before he died, signing it at the Swedish–Norwegian Club in Paris on 27 November 1895. To widespread astonishment, Nobel's last will specified that his fortune be used to create a series of prizes for those who confer the "greatest benefit on mankind" in physics, chemistry, peace, physiology or medicine, and literature. Nobel bequeathed 94% of his total assets, 31 million SEK (c. US$186 million, €150 million in 2008), to establish the five Nobel Prizes.
Because of scepticism surrounding the will, it was not until 26 April 1897 that it was approved by the Storting in Norway. The executors of Nobel's will, Ragnar Sohlman and Rudolf Lilljequist, formed the Nobel Foundation to take care of Nobel's fortune and organise the award of prizes.
Nobel's instructions named a Norwegian Nobel Committee to award the Peace Prize, the members of whom were appointed shortly after the will was approved in April 1897. Soon thereafter, the other prize-awarding organisations were designated or established. These were Karolinska Institutet on 7 June, the Swedish Academy on 9 June, and the Royal Swedish Academy of Sciences on 11 June. The Nobel Foundation reached an agreement on guidelines for how the prizes should be awarded; and, in 1900, the Nobel Foundation's newly created statutes were promulgated by King Oscar II. In 1905, the personal union between Sweden and Norway was dissolved.
Nobel Foundation.
The Nobel Foundation was founded as a private organisation on 29 June 1900. Its function is to manage the finances and administration of the Nobel Prizes. In accordance with Nobel's will, the primary task of the Foundation is to manage the fortune Nobel left. Robert and Ludwig Nobel were involved in the oil business in Azerbaijan and, according to Swedish historian E. Bargengren, who accessed the Nobel family archives, it was this "decision to allow withdrawal of Alfred's money from Baku that became the decisive factor that enabled the Nobel Prizes to be established". Another important task of the Nobel Foundation is to market the prizes internationally and to oversee informal administration related to the prizes. The Foundation is not involved in the process of selecting the Nobel laureates. In many ways, the Nobel Foundation is similar to an investment company, in that it invests Nobel's money to create a solid funding base for the prizes and the administrative activities. The Nobel Foundation is exempt from all taxes in Sweden (since 1946) and from investment taxes in the United States (since 1953). Since the 1980s, the Foundation's investments have become more profitable and as of 31 December 2007, the assets controlled by the Nobel Foundation amounted to 3.628 billion Swedish "kronor" (c. US$560 million).
According to the statutes, the Foundation consists of a board of five Swedish or Norwegian citizens, with its seat in Stockholm. The Chairman of the Board is appointed by the Swedish King in Council, with the other four members appointed by the trustees of the prize-awarding institutions. An Executive Director is chosen from among the board members, a Deputy Director is appointed by the King in Council, and two deputies are appointed by the trustees. However, since 1995, all the members of the board have been chosen by the trustees, and the Executive Director and the Deputy Director appointed by the board itself. As well as the board, the Nobel Foundation is made up of the prize-awarding institutions (the Royal Swedish Academy of Sciences, the Nobel Assembly at Karolinska Institute, the Swedish Academy, and the Norwegian Nobel Committee), the trustees of these institutions, and auditors.
First prizes.
Once the Nobel Foundation and its guidelines were in place, the Nobel Committees began collecting nominations for the inaugural prizes. Subsequently they sent a list of preliminary candidates to the prize-awarding institutions.
The Nobel Committee's Physics Prize shortlist cited Wilhelm Conrad Röntgen's discovery of X-rays and Philipp Lenard's work on cathode rays. The Academy of Sciences selected Röntgen for the prize. In the last decades of the 19th century, many chemists had made significant contributions. Thus, with the Chemistry Prize, the Academy "was chiefly faced with merely deciding the order in which these scientists should be awarded the prize." The Academy received 20 nominations, eleven of them for Jacobus van't Hoff. Van't Hoff was awarded the prize for his contributions in chemical thermodynamics.
The Swedish Academy chose the poet Sully Prudhomme for the first Nobel Prize in Literature. A group including 42 Swedish writers, artists and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they consider Prudhomme a mediocre poet. Feldman's explanation is that most of the Academy members preferred Victorian literature and thus selected a Victorian poet. The first Physiology or Medicine Prize went to the German physiologist and microbiologist Emil von Behring. During the 1890s, von Behring developed an antitoxin to treat diphtheria, which until then was causing thousands of deaths each year.
Second World War.
In 1938 and 1939, Adolf Hitler's Third Reich forbade three laureates from Germany (Richard Kuhn, Adolf Friedrich Johann Butenandt, and Gerhard Domagk) from accepting their prizes. Each man was later able to receive the diploma and medal. Even though Sweden was officially neutral during the Second World War, the prizes were awarded irregularly. In 1939, the Peace Prize was not awarded. No prize was awarded in any category from 1940–42, due to the occupation of Norway by Germany. In the subsequent year, all prizes were awarded except those for literature and peace.
During the occupation of Norway, three members of the Norwegian Nobel Committee fled into exile. The remaining members escaped persecution from the Germans when the Nobel Foundation stated that the Committee building in Oslo was Swedish property. Thus it was a safe haven from the German military, which was not at war with Sweden. These members kept the work of the Committee going, but did not award any prizes. In 1944, the Nobel Foundation, together with the three members in exile, made sure that nominations were submitted for the Peace Prize and that the prize could be awarded once again.
Prize in Economic Sciences.
In 1968, Sveriges Riksbank celebrated its 300th anniversary by donating a large sum of money to the Nobel Foundation to be used to set up a prize in honor of Nobel. The following year, the Nobel Memorial Prize in Economic Sciences was awarded for the first time. The Royal Swedish Academy of Sciences became responsible for selecting laureates. The first laureates for the Economics Prize were Jan Tinbergen and Ragnar Frisch "for having developed and applied dynamic models for the analysis of economic processes." Although not a Nobel Prize, it is intimately identified with the other awards; the laureates are announced with the Nobel Prize recipients, and the Prize in Economic Sciences is presented at the Swedish Nobel Prize Award Ceremony. The Board of the Nobel Foundation decided that after this addition, it would allow no further new prizes.
Award process.
The award process is similar for all of the Nobel Prizes; the main difference is in who can make nominations for each of them.
Nominations.
Nomination forms are sent by the Nobel Committee to about 3,000 individuals, usually in September the year before the prizes are awarded. These individuals are generally prominent academics working in a relevant area. Regarding the Peace Prize, inquiries are also sent to governments, former Peace Prize laureates and current or former members of the Norwegian Nobel Committee. The deadline for the return of the nomination forms is 31 January of the year of the award. The Nobel Committee nominates about 300 potential laureates from these forms and additional names. The nominees are not publicly named, nor are they told that they are being considered for the prize. All nomination records for a prize are sealed for 50 years from the awarding of the prize.
Selection.
The Nobel Committee then prepares a report reflecting the advice of experts in the relevant fields. This, along with the list of preliminary candidates, is submitted to the prize-awarding institutions. The institutions meet to choose the laureate or laureates in each field by a majority vote. Their decision, which cannot be appealed, is announced immediately after the vote. A maximum of three laureates and two different works may be selected per award. Except for the Peace Prize, which can be awarded to institutions, the awards can only be given to individuals. If the Peace Prize is not awarded, the money is split among the scientific prizes. This has happened 19 times so far.
Posthumous nominations.
Although posthumous nominations are not permitted, individuals who die in the months between their nomination and the decision of the prize committee were originally eligible to receive the prize. This has occurred twice: the 1931 Literature Prize awarded to Erik Axel Karlfeldt, and the 1961 Peace Prize awarded to UN Secretary General Dag Hammarskjöld. Since 1974, laureates must be thought alive at the time of the October announcement. There has been one laureate, William Vickrey, who in 1996 died after the prize (in Economics) was announced but before it could be presented. On 3 October 2011, the laureates for the Nobel Prize in Physiology or Medicine were announced; however, the committee was not aware that one of the laureates, Ralph M. Steinman, had died three days earlier. The committee was debating about Steinman's prize, since the rule is that the prize is not awarded posthumously. The committee later decided that as the decision to award Steinman the prize "was made in good faith", it would remain unchanged.
Recognition time lag.
Nobel's will provided for prizes to be awarded in recognition of discoveries made "during the preceding year". Early on, the awards usually recognised recent discoveries. However, some of these early discoveries were later discredited. For example, Johannes Fibiger was awarded the 1926 Prize for Physiology or Medicine for his purported discovery of a parasite that caused cancer. To avoid repeating this embarrassment, the awards increasingly recognised scientific discoveries that had withstood the test of time. According to Ralf Pettersson, former chairman of the Nobel Prize Committee for Physiology or Medicine, "the criterion ‘the previous year’ is interpreted by the Nobel Assembly as the year when the full impact of the discovery has become evident."
The interval between the award and the accomplishment it recognises varies from discipline to discipline. The Literature Prize is typically awarded to recognise a cumulative lifetime body of work rather than a single achievement. The Peace Prize can also be awarded for a lifetime body of work. For example 2008 laureate Martti Ahtisaari was awarded for his work to resolve international conflicts. However, they can also be awarded for specific recent events. For instance, Kofi Annan was awarded the 2001 Peace Prize just four years after becoming the Secretary-General of the United Nations. Similarly Yasser Arafat, Yitzhak Rabin, and Shimon Peres received the 1994 award, about a year after they successfully concluded the Oslo Accords.
Although Nobel's will stated that prizes should be awarded for contributions made "during the preceding year", awards for physics, chemistry, and medicine are typically awarded once the achievement has been widely accepted. Sometimes, this takes decades – for example, Subrahmanyan Chandrasekhar shared the 1983 Physics Prize for his 1930s work on stellar structure and evolution. Not all scientists live long enough for their work to be recognised. Some discoveries can never be considered for a prize if their impact is realised after the discoverers have died.
Award ceremonies.
Except for the Peace Prize, the Nobel Prizes are presented in Stockholm, Sweden, at the annual Prize Award Ceremony on 10 December, the anniversary of Nobel's death. The recipients' lectures are normally held in the days prior to the award ceremony. The Peace Prize and its recipients' lectures are presented at the annual Prize Award Ceremony in Oslo, Norway, usually on 10 December. The award ceremonies and the associated banquets are typically major international events. The Prizes awarded in Sweden's ceremonies' are held at the Stockholm Concert Hall, with the Nobel banquet following immediately at Stockholm City Hall. The Nobel Peace Prize ceremony has been held at the Norwegian Nobel Institute (1905–1946), at the auditorium of the University of Oslo (1947–1989) and at Oslo City Hall (1990–present).
The highlight of the Nobel Prize Award Ceremony in Stockholm occurs when each Nobel laureate steps forward to receive the prize from the hands of the King of Sweden. In Oslo, the Chairman of the Norwegian Nobel Committee presents the Nobel Peace Prize in the presence of the King of Norway. At first King Oscar II did not approve of awarding grand prizes to foreigners. It is said that his mind changed once his attention had been drawn to the publicity value of the prizes for Sweden.
Nobel Banquet.
After the award ceremony in Sweden, a banquet is held in the Blue Hall at the Stockholm City Hall, which is attended by the Swedish Royal Family and around 1,300 guests.
The Nobel Peace Prize banquet is held in Norway at the Oslo Grand Hotel after the award ceremony. Apart from the laureate, guests include the President of the Storting, the Prime Minister, and, since 2006, the King and Queen of Norway. In total, about 250 guests attend.
Nobel lecture.
According to the statutes of the Nobel Foundation, each laureate is required to give a public lecture on a subject related to the topic of their prize. The Nobel lecture as a rhetorical genre took decades to reach its current format. These lectures normally occur during Nobel Week (the week leading up to the award ceremony and banquet, which begins with the laureates arriving in Stockholm and normally ends with the Nobel banquet), but this is not mandatory. The laureate is only obliged to give the lecture within six months of receiving the prize. Some have happened even later. For example, U.S. President Theodore Roosevelt received the Peace Prize in 1906 but gave his lecture in 1910, after his term in office. The lectures are organized by the same association which selected the laureates.
Prizes.
Medals.
It was announced on 30 May 2012 that the Nobel Foundation had awarded the contract for the production of the five (Swedish) Nobel Prize medals to Svenska Medalj AB. Formerly, the Nobel Prize medals were minted by Myntverket (the Swedish Mint) in between 1902–2010. Myntverket, Sweden's oldest company, ceased operations in 2011 after 1,017 years. In 2011 the Mint of Norway, located in Kongsberg, made the medals. The Nobel Prize medals are registered trademarks of the Nobel Foundation. Each medal features an image of Alfred Nobel in left profile on the obverse. The medals for physics, chemistry, physiology or medicine, and literature have identical obverses, showing the image of Alfred Nobel and the years of his birth and death. Nobel's portrait also appears on the obverse of the Peace Prize medal and the medal for the Economics Prize, but with a slightly different design. For instance, the laureate's name is engraved on the rim of the Economics medal. The image on the reverse of a medal varies according to the institution awarding the prize. The reverse sides of the medals for chemistry and physics share the same design.
All medals made before 1980 were struck in 23 carat gold. Since then they have been struck in 18 carat green gold plated with 24 carat gold. The weight of each medal varies with the value of gold, but averages about 175 g for each medal. The diameter is 66 mm and the thickness varies between 5.2 mm and 2.4 mm. Because of the high value of their gold content and tendency to be on public display, Nobel medals are subject to medal theft. During World War II, the medals of German scientists Max von Laue and James Franck were sent to Copenhagen for safekeeping. When Germany invaded Denmark, chemist George de Hevesy dissolved them in aqua regia (nitro-hydrochloric acid), to prevent confiscation by Nazi Germany and to prevent legal problems for the holders. After the war, the gold was recovered from solution, and the medals re-cast.
Diplomas.
Nobel laureates receive a diploma directly from the hands of the King of Sweden or, in the case of the peace prize, the Chairman of the Norwegian Nobel Committee. Each diploma is uniquely designed by the prize-awarding institutions for the laureates that receive them. The diploma contains a picture and text which states the name of the laureate and normally a citation of why they received the prize. None of the Nobel Peace Prize laureates has ever had a citation on their diplomas.
Award money.
The laureates are given a sum of money when they receive their prizes, in the form of a document confirming the amount awarded. The amount of prize money depends upon how much money the Nobel Foundation can award each year. The purse has increased since the 1980s, when the prize money was 880 000 SEK (c. 2.6 million SEK, US$350 000 or €295,000 today) per prize. In 2009, the monetary award was 10 million SEK (US$1.4 million, €950,000). In June 2012, it was lowered to 8 million SEK. If there are two laureates in a particular category, the award grant is divided equally between the recipients. If there are three, the awarding committee has the option of dividing the grant equally, or awarding one-half to one recipient and one-quarter to each of the others. It is common for recipients to donate prize money to benefit scientific, cultural, or humanitarian causes.
Controversies and criticisms.
Controversial recipients.
Among other criticisms, the Nobel Committees have been accused of having a political agenda, and of omitting more deserving candidates. They have also been accused of Eurocentrism, especially for the Literature Prize.
Among the most criticised Nobel Peace Prizes was the one awarded to Henry Kissinger and Lê Đức Thọ. This led to the resignation of two Norwegian Nobel Committee members. Lê Đức Thọ declined the prize. Kissinger and Thọ were awarded the prize for negotiating a ceasefire between North Vietnam and the United States in January 1973. However, when the award was announced, both sides were still engaging in hostilities. Many critics were of the opinion that Kissinger was not a peace-maker but the opposite, responsible for widening the war.
Yasser Arafat, Shimon Peres, and Yitzhak Rabin received the Peace Prize in 1994 for their efforts in making peace between Israel and Palestine. Immediately after the award was announced, one of the five Norwegian Nobel Committee members denounced Arafat as a terrorist and resigned. Additional misgivings about Arafat were widely expressed in various newspapers.
Another controversial Peace Prize was that awarded to Barack Obama in 2009. Nominations had closed only eleven days after Obama took office as President, but the actual evaluation occurred over the next eight months. Obama himself stated that he did not feel deserving of the award, or worthy of the company it would place him in. Past Peace Prize laureates were divided, some saying that Obama deserved the award, and others saying he had not secured the achievements to yet merit such an accolade. Obama's award, along with the previous Peace Prizes for Jimmy Carter and Al Gore, also prompted accusations of a left-wing bias.
The award of the 2004 Literature Prize to Elfriede Jelinek drew a protest from a member of the Swedish Academy, Knut Ahnlund. Ahnlund resigned, alleging that the selection of Jelinek had caused "irreparable damage to all progressive forces, it has also confused the general view of literature as an art." He alleged that Jelinek's works were "a mass of text shovelled together without artistic structure." The 2009 Literature Prize to Herta Müller also generated criticism. According to "The Washington Post" many US literary critics and professors were ignorant of her work. This made those critics feel the prizes were too Eurocentric.
In 1949, the neurologist António Egas Moniz received the Physiology or Medicine Prize for his development of the prefrontal leucotomy. The previous year Dr. Walter Freeman had developed a version of the procedure which was faster and easier to carry out. Due in part to the publicity surrounding the original procedure, Freeman's procedure was prescribed without due consideration or regard for modern medical ethics. Endorsed by such influential publications as "The New England Journal of Medicine", leucotomy or "lobotomy" became so popular that about 5,000 lobotomies were performed in the United States in the three years immediately following Moniz's receipt of the Prize.
Overlooked achievements.
The Norwegian Nobel Committee confirmed that Mahatma Gandhi was nominated for the Peace Prize in 1937–39, 1947 and a few days before he was assassinated in January 1948. Later members of the Norwegian Nobel Committee expressed regret that he was not given the prize. Geir Lundestad, Secretary of Norwegian Nobel Committee in 2006 said, "The greatest omission in our 106 year history is undoubtedly that Mahatma Gandhi never received the Nobel Peace prize. Gandhi could do without the Nobel Peace prize. Whether Nobel committee can do without Gandhi is the question". In 1948, the year of Gandhi's death, the Nobel Committee declined to award a prize on the grounds that "there was no suitable living candidate" that year. Later, when the Dalai Lama was awarded the Peace Prize in 1989, the chairman of the committee said that this was "in part a tribute to the memory of Mahatma Gandhi." Other high profile individuals with widely recognised contributions to peace have been missed out. "Foreign Policy" lists Eleanor Roosevelt, Václav Havel, Ken Saro-Wiwa, Sari Nusseibeh and Corazon Aquino as people who "never won the prize, but should have.". The physicist Arnold Sommerfeld was nominated 81 times but an award was never made.
In 1965, UN Secretary General U Thant was informed by the Norwegian Permananent Representative to the UN that he would be awarded that year's prize and asked whether or not he would accept. He consulted staff and later replied that he would. At the same time, Chairman Gunnar Jahn of the Nobel Peace prize committee, lobbied heavily against giving U Thant the prize and the prize was at the last minute awarded to UNICEF. The rest of the committee all wanted the prize to go to U Thant, for his work in defusing the Cuban Missile Crisis, ending the war in the Congo, and his ongoing work to mediate an end to the Vietnam War. The disagreement lasted three years and in 1966 and 1967 no prize was given, with Gunnar Jahn effectively vetoing an award to U Thant.
The Literature Prize also has controversial omissions. Adam Kirsch has suggested that many notable writers have missed out on the award for political or extra-literary reasons. The heavy focus on European and Swedish authors has been a subject of criticism. The Eurocentric nature of the award was acknowledged by Peter Englund, the 2009 Permanent Secretary of the Swedish Academy, as a problem with the award and was attributed to the tendency for the academy to relate more to European authors. This tendency towards European authors still leaves a number of European writers on a list of notable writers that have been overlooked for the Literature Prize, including Europe's Leo Tolstoy, Anton Chekhov, J. R. R. Tolkien, Émile Zola, Marcel Proust, Vladimir Nabokov, James Joyce, August Strindberg, Simon Vestdijk, the New World's Jorge Luis Borges, Ezra Pound, John Updike, Arthur Miller, Mark Twain, and Africa's Chinua Achebe.
The strict rule against awarding a prize to more than three people is also controversial. When a prize is awarded to recognize an achievement by a team of more than three collaborators, one or more will miss out. For example, in 2002, the prize was awarded to Koichi Tanaka and John Fenn for the development of mass spectrometry in protein chemistry, an award that did not recognize the achievements of Franz Hillenkamp and Michael Karas of the Institute for Physical and Theoretical Chemistry at the University of Frankfurt. According to one of the nominees for the prize in physics, the three person limit deprived him and two other members of his team of the honor in 2013: the team of Carl Hagen, Gerald Guralnik, and Tom Kibble published a paper in 1964 that gave answers to how the Cosmos began, but did not share the 2013 Physics Prize awarded to Peter Higgs and François Englert, who had also published papers in 1964 concerning the subject. All five physicists arrived at the same conclusion, albeit from different angles. Hagen contends that an equitable solution is to either abandon the three limit restriction, or expand the time period of recognition for a given achievement to two years.
Similarly, the prohibition of posthumous awards fails to recognise achievements by an individual or collaborator who dies before the prize is awarded. In 1962, Francis Crick, James D. Watson, and Maurice Wilkins were awarded the Physiology or Medicine Prize for discovering the structure of DNA. Rosalind Franklin, a key contributor in that discovery, died of ovarian cancer four years earlier. The Economics Prize was not awarded to Fischer Black, who died in 1995, when his co-author Myron Scholes received the honor in 1997 for their landmark work on option pricing along with Robert C. Merton, another pioneer in the development of valuation of stock options. In the announcement of the award that year, the Nobel committee prominently mentioned Black's key role.
Political subterfuge may also deny proper recognition. Lise Meitner and Fritz Strassmann, who co-discovered nuclear fission along with Otto Hahn, may have been denied a share of Hahn's 1944 Nobel Chemistry Award due to having fled Germany when the Nazis came to power. The Meitner and Strassmann roles in the research was not fully recognized until years later, when they joined Hahn in receiving the 1966 Enrico Fermi Award.
Emphasis on discoveries over inventions.
Alfred Nobel left his fortune to finance annual prizes to be awarded "to those who, during the preceding year, shall have conferred the greatest benefit on mankind." He stated that the Nobel Prizes in Physics should be given "to the person who shall have made the most important 'discovery' or 'invention' within the field of physics." Nobel did not emphasise discoveries, but they have historically been held in higher respect by the Nobel Prize Committee than inventions: 77% of the Physics Prizes have been given to discoveries, compared with only 23% to inventions. Christoph Bartneck and Matthias Rauterberg, in papers published in "Nature" and "Technoetic Arts", have argued this emphasis on discoveries has moved the Nobel Prize away from its original intention of rewarding the greatest contribution to society.
Specially distinguished laureates.
Multiple laureates.
Four people have received two Nobel Prizes. Marie Skłodowska-Curie received the Physics Prize in 1903 for the discovery of radioactivity and the Chemistry Prize in 1911 for the isolation of pure radium, making her the only person to win a Nobel Prize in two different sciences. Linus Pauling won the 1954 Chemistry Prize for his research into the chemical bond and its application to the structure of complex substances. Pauling also won the Peace Prize in 1962 for his anti-nuclear activism, making him the only laureate of two unshared prizes. John Bardeen received the Physics Prize twice: in 1956 for the invention of the transistor and in 1972 for the theory of superconductivity. Frederick Sanger received the prize twice in Chemistry: in 1958 for determining the structure of the insulin molecule and in 1980 for inventing a method of determining base sequences in DNA.
Two organisations have received the Peace Prize multiple times. The International Committee of the Red Cross received it three times: in 1917 and 1944 for its work during the world wars; and in 1963 during the year of its centenary. The United Nations High Commissioner for Refugees has won the Peace Prize twice for assisting refugees: in 1954 and 1981.
Family laureates.
The Curie family has received the most prizes, with five. Marie Skłodowska-Curie received the prizes in Physics (in 1903) and Chemistry (in 1911). Her husband, Pierre Curie, shared the 1903 Physics prize with her. Their daughter, Irène Joliot-Curie, received the Chemistry Prize in 1935 together with her husband Frédéric Joliot-Curie. In addition, the husband of Marie Curie's second daughter, Henry Labouisse, was the director of UNICEF when it won the Nobel Peace Prize in 1965.
Although no family matches the Curie family's record, there have been several with two laureates.
The husband-and-wife team of Gerty Radnitz Cori and Carl Ferdinand Cori
shared the 1947 Prize in Physiology or Medicine as did the husband-and-wife team of May-Britt Moser and Edvard Moser in 2014 (along with John O'Keefe). J. J. Thomson was awarded the Physics Prize in 1906 for showing that electrons are particles. His son, George Paget Thomson, received the same prize in 1937 for showing that they also have the properties of waves. William Henry Bragg and his son, William Lawrence Bragg, shared the Physics Prize in 1915 for inventing the X-ray spectrometer. Niels Bohr won the Physics prize in 1922, as did his son, Aage Bohr, in 1975. Manne Siegbahn, who received the Physics Prize in 1924, was the father of Kai Siegbahn, who received the Physics Prize in 1981. Hans von Euler-Chelpin, who received the Chemistry Prize in 1929, was the father of Ulf von Euler, who was awarded the Physiology or Medicine Prize in 1970. C.V. Raman won the Physics Prize in 1930 and was the uncle of Subrahmanyan Chandrasekhar, who won the same prize in 1983. Arthur Kornberg received the Physiology or Medicine Prize in 1959. Kornberg's son, Roger later received the Chemistry Prize in 2006. Jan Tinbergen, who won the first Economics Prize in 1969, was the brother of Nikolaas Tinbergen, who received the 1973 Physiology or Medicine Prize. Alva Myrdal, Peace Prize laureate in 1982, was the wife of Gunnar Myrdal who was awarded the Economics Prize in 1974. Economics laureates Paul Samuelson and Kenneth Arrow were brothers-in-law.
Fictional laureates.
Due to its well-known nature, the Nobel Prize has been fictionally awarded or offered to many fake characters within popular media, including:
Refusals and constraints.
Two laureates have voluntarily declined the Nobel Prize. In 1964 Jean-Paul Sartre was awarded the Literature Prize but refused, stating, "A writer must refuse to allow himself to be transformed into an institution, even if it takes place in the most honourable form." The other is Lê Đức Thọ, chosen for the 1973 Peace Prize for his role in the Paris Peace Accords. He declined, stating that there was no actual peace in Vietnam.
During the Third Reich, Adolf Hitler hindered Richard Kuhn, Adolf Butenandt, and Gerhard Domagk from accepting their prizes. All of them were awarded their diplomas and gold medals after World War II. In 1958, Boris Pasternak declined his prize for literature due to fear of what the Soviet Union government might do if he travelled to Stockholm to accept his prize. In return, the Swedish Academy refused his refusal, saying "this refusal, of course, in no way alters the validity of the award." The Academy announced with regret that the presentation of the Literature Prize could not take place that year, holding it until 1989 when Pasternak's son accepted the prize on his behalf. Aung San Suu Kyi was awarded the Nobel Peace Prize in 1991, but her children accepted the prize because she had been placed under house arrest in Burma; Suu Kyi delivered her speech two decades later, in 2012. Liu Xiaobo was awarded the Nobel Peace Prize in 2010 while he and his wife were under house arrest in China as political prisoners.
Legacy.
The memorial symbol "Planet of Alfred Nobel" was opened in Dnipropetrovsk University of Economics and Law at in 2008. On the globe there are 802 Nobel laureates' reliefs made of a composite alloy obtained when disposing of military strategic missiles.
References.
Bibliography.
</dl>

</doc>
<doc id="21210" url="http://en.wikipedia.org/wiki?curid=21210" title="Niels Bohr">
Niels Bohr

Niels Henrik David Bohr (]; 7 October 1885 – 18 November 1962) was a Danish physicist who made foundational contributions to understanding atomic structure and quantum theory, for which he received the Nobel Prize in Physics in 1922. Bohr was also a philosopher and a promoter of scientific research.
Bohr developed the Bohr model of the atom, in which he proposed that energy levels of electrons are discrete and that the electrons revolve in stable orbits around the atomic nucleus but can jump from one energy level (or orbit) to another. Although the Bohr model has been supplanted by other models, its underlying principles remain valid. He conceived the principle of complementarity: that items could be separately analysed in terms of contradictory properties, like behaving as a wave or a stream of particles. The notion of complementarity dominated Bohr's thinking in both science and philosophy.
Bohr founded the Institute of Theoretical Physics at the University of Copenhagen, now known as the Niels Bohr Institute, which opened in 1920. Bohr mentored and collaborated with physicists including Hans Kramers, Oskar Klein, George de Hevesy and Werner Heisenberg. He predicted the existence of a new zirconium-like element, which was named hafnium, after the Latin name for Copenhagen, where it was discovered. Later, the element bohrium was named after him.
During the 1930s, Bohr helped refugees from Nazism. After Denmark was occupied by the Germans, he had a famous meeting with Heisenberg, who had become the head of the German nuclear energy project. In September 1943, word reached Bohr that he was about to be arrested by the Germans, and he fled to Sweden. From there, he was flown to Britain, where he joined the British Tube Alloys nuclear weapons project, and was part of the British mission to the Manhattan Project. After the war, Bohr called for international cooperation on nuclear energy. He was involved with the establishment of CERN and the Research Establishment Risø of the Danish Atomic Energy Commission, and became the first chairman of the Nordic Institute for Theoretical Physics in 1957.
Early years.
Niels Bohr was born in Copenhagen, Denmark, on 7 October 1885, the second of three children of Christian Bohr, a professor of physiology at the University of Copenhagen, and Ellen Adler Bohr, who came from a wealthy Danish Jewish family prominent in banking and parliamentary circles. He had an elder sister, Jenny, and a younger brother Harald. Jenny became a teacher, while Harald became a mathematician and Olympic footballer who played for the Danish national team at the 1908 Summer Olympics in London. Niels was a passionate footballer as well, and the two brothers played several matches for the Copenhagen-based Akademisk Boldklub (Academic Football Club), with Niels as goalkeeper.
Bohr was educated at Gammelholm Latin School, starting when he was seven. In 1903, Bohr enrolled as an undergraduate at Copenhagen University. His major was physics, which he studied under Professor Christian Christiansen, the university's only professor of physics at that time. He also studied astronomy and mathematics under Professor Thorvald Thiele, and philosophy under Professor Harald Høffding, a friend of his father.
In 1905, a gold medal competition was sponsored by the Royal Danish Academy of Sciences and Letters to investigate a method for measuring the surface tension of liquids that had been proposed by Lord Rayleigh in 1879. This involved measuring the frequency of oscillation of the radius of a water jet. Bohr conducted a series of experiments using his father's laboratory in the university; the university itself had no physics laboratory. To complete his experiments, he had to make his own glassware, creating test tubes with the required elliptical cross-sections. He went beyond the original task, incorporating improvements into both Rayleigh's theory and his method, by taking into account the viscosity of the water, and by working with finite amplitudes instead of just infinitesimal ones. His essay, which he submitted at the last minute, won the prize. He later submitted an improved version of the paper to the Royal Society in London for publication in the "Philosophical Transactions of the Royal Society".
Harald became the first of the two Bohr brothers to earn a master's degree, which he earned for mathematics in April 1909. Niels took another nine months to earn his. Students had to submit a thesis on a subject assigned by their supervisor. Bohr's supervisor was Christiansen, and the topic he chose was the electron theory of metals. Bohr subsequently elaborated his master's thesis into his much-larger Doctor of Philosophy (dr. phil.) thesis. He surveyed the literature on the subject, settling on a model postulated by Paul Drude and elaborated by Hendrik Lorentz, in which the electrons in a metal are considered to behave like a gas. Bohr extended Lorentz's model, but was still unable to account for phenomena like the Hall effect, and concluded that electron theory could not fully explain the magnetic properties of metals. The thesis was accepted in April 1911, and Bohr conducted his formal defence on 13 May. Harald had received his doctorate the previous year. Bohr's thesis was groundbreaking, but attracted little interest outside Scandinavia because it was written in Danish, a Copenhagen University requirement at the time. In 1921, the Dutch physicist Hendrika Johanna van Leeuwen would independently derive a theorem from Bohr's thesis that is today known as the Bohr–van Leeuwen theorem.
In 1910, Bohr met Margrethe Nørlund, the sister of the mathematician Niels Erik Nørlund. Bohr resigned his membership in the Church of Denmark on 16 April 1912, and he and Margrethe were married in a civil ceremony at the town hall in Slagelse on 1 August. Years later, his brother Harald similarly left the church before getting married. Niels and Margrethe had six sons. The oldest, Christian, died in a boating accident in 1934, and another, Harald, died from childhood meningitis. Aage Bohr became a successful physicist, and in 1975 was awarded the Nobel Prize in physics, like his father. Hans became a physician; Erik, a chemical engineer; and Ernest, a lawyer. Like his uncle Harald, Ernest Bohr became an Olympic athlete, playing field hockey for Denmark at the 1948 Summer Olympics in London.
Physics.
Bohr model.
In 1911, Bohr travelled to England. At the time, it was where most of the theoretical work on the structure of atoms and molecules was being done. He met J. J. Thomson of the Cavendish Laboratory and Trinity College, Cambridge. He attended lectures on electromagnetism given by James Jeans and Joseph Larmor, and did some research on cathode rays, but failed to impress Thomson. He had more success with younger physicists like the Australian William Lawrence Bragg, and New Zealand's Ernest Rutherford, whose 1911 Rutherford model of the atom had challenged Thomson's 1904 plum pudding model. Bohr received an invitation from Rutherford to conduct post-doctoral work at Victoria University of Manchester, where Bohr met George de Hevesy and Charles Galton Darwin (whom Bohr referred to as "the grandson of the real Darwin").
Bohr returned to Denmark in July 1912 for his wedding, and travelled around England and Scotland on his honeymoon. On his return, he became a "privatdocent" at the University of Copenhagen, giving lectures on thermodynamics. Martin Knudsen put Bohr's name forward for a "docent", which was approved in July 1913, and Bohr then began teaching medical students. His three papers, which later became famous as "the trilogy", were published in "Philosophical Magazine" in July, September and November of that year. He adapted Rutherford's nuclear structure to Max Planck's quantum theory and so created his Bohr model of the atom.
Planetary models of atoms were not new, but Bohr's treatment was. Taking the 1912 paper by Darwin on the role of electrons in the interaction of alpha particles with a nucleus as his starting point, he advanced the theory of electrons travelling in orbits around the atom's nucleus, with the chemical properties of each element being largely determined by the number of electrons in the outer orbits of its atoms. He introduced the idea that an electron could drop from a higher-energy orbit to a lower one, in the process emitting a quantum of discrete energy. This became a basis for what is now known as the old quantum theory.
In 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atoms:
where λ is the wavelength of the absorbed or emitted light and "R"H is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model:
where "m"e is the electron's mass, "e" is its charge, "h" is Planck's constant and "Z" is the atom's atomic number (1 for hydrogen).
The model's first hurdle was the Pickering series, lines which did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Max Born and Arnold Sommerfeld saw it as a breakthrough. The trilogy's acceptance was entirely due to its ability to explain phenomena which stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts.
Bohr did not enjoy teaching medical students. He decided to return to Manchester, where Rutherford had offered him a job as a reader in place of Darwin, whose tenure had expired. Bohr accepted. He took a leave of absence from the University of Copenhagen, which he started by taking a holiday in Tyrol with his brother Harald and aunt Hanna Adler. There, he visited the University of Göttingen and the Ludwig Maximilian University of Munich, where he met Sommerfeld and conducted seminars on the trilogy. The First World War broke out while they were in Tyrol, greatly complicating the trip back to Denmark and Bohr's subsequent voyage with Margrethe to England, where he arrived in October 1914. They stayed until July 1916, by which time he had been appointed to the Chair of Theoretical Physics at the University of Copenhagen, a position created especially for him. His docentship was abolished at the same time, so he still had to teach physics to medical students. New professors were formally introduced to King Christian X, who expressed his delight at meeting such a famous football player.
Institute of Physics.
In April 1917, Bohr began a campaign to establish an Institute of Theoretical Physics. He gained the support of the Danish government and the Carlsberg Foundation, and sizeable contributions were also made by industry and private donors, many of them Jewish. Legislation establishing the Institute was passed in November 1918. Now known as the Niels Bohr Institute, it opened its doors on 3 March 1921 with Bohr as its director. His family moved into an apartment on the first floor. Bohr's institute served as a focal point for researchers into quantum mechanics and related subjects in the 1920s and 1930s, when most of the world's best known theoretical physicists spent some time in his company. Early arrivals included Hans Kramers from the Netherlands, Oskar Klein from Sweden, George de Hevesy from Hungary, Wojciech Rubinowicz from Poland and Svein Rosseland from Norway. Bohr became widely appreciated as their congenial host and eminent colleague. Klein and Rosseland produced the Institute's first paper even before it opened.
The Bohr model worked well for hydrogen, but could not explain more complex elements. By 1919, Bohr was moving away from the idea that electrons orbited the nucleus, and he developed heuristics to describe them. The rare earth elements posed a particular classification problem for chemists, because they were so chemically similar. An important development came in 1924 with Wolfgang Pauli's discovery of the Pauli exclusion principle, which put Bohr's models on a firm theoretical footing. Bohr was then able to declare that the as-yet-undiscovered element 72 was not a rare earth element, but an element with chemical properties similar to those of zirconium. He was immediately challenged by the French chemist Georges Urbain, who claimed to have discovered a rare earth element 72, which he called "celtium". At the Institute in Copenhagen, Dirk Coster and George de Hevesy took up the challenge of proving Bohr right and Urbain wrong. Starting with a clear idea of the chemical properties of the unknown element greatly simplified the search process. They went through samples from Copenhagen's Museum of Mineralogy looking for a zirconium-like element, and soon found it. The element, which they named hafnium, "Hafnia" being the Latin name for Copenhagen, turned out to be more common than gold.
In 1922, Bohr was awarded the Nobel Prize in Physics "for his services in the investigation of the structure of atoms and of the radiation emanating from them". The award thus recognised both the Trilogy and his early leading work in the emerging field of quantum mechanics. For his Nobel lecture, Bohr gave his audience a comprehensive survey of what was then known about the structure of the atom, including the correspondence principle, which he had formulated. This states that the behaviour of systems described by quantum theory reproduces classical physics in the limit of large quantum numbers.
The discovery of Compton scattering by Arthur Holly Compton in 1923 convinced most physicists that light was composed of photons, and that energy and momentum were conserved in collisions between electrons and photons. In 1924, Bohr, Kramers and John C. Slater, an American physicist working at the Institute in Copenhagen, proposed the Bohr–Kramers–Slater theory (BKS). It was more a programme than a full physical theory, as the ideas it developed were not worked out quantitatively. BKS theory became the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the old quantum theory, in which quantum phenomena were treated by imposing quantum restrictions on a classical wave description of the electromagnetic field.
Modelling atomic behaviour under incident electromagnetic radiation using "virtual oscillators" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, led Max Born, Werner Heisenberg and Kramers to explore different mathematical models. They led to the development of matrix mechanics, the first form of modern quantum mechanics. The BKS theory also generated discussion of, and renewed attention to, difficulties in the foundations of the old quantum theory. The most provocative element of BKS – that momentum and energy would not necessarily be conserved in each interaction, but only statistically – was soon shown to be in conflict with experiments conducted by Walther Bothe and Hans Geiger. In light of these results, Bohr informed Darwin, "there is nothing else to do than to give our revolutionary efforts as honourable a funeral as possible."
Quantum mechanics.
The introduction of spin by George Uhlenbeck and Samuel Goudsmit in November 1925 was a milestone. The next month, Bohr travelled to Leiden to attend celebrations of the 50th anniversary of Hendrick Lorentz receiving his doctorate. When his train stopped in Hamburg, he was met by Wolfgang Pauli and Otto Stern, who asked for his opinion of the spin theory. Bohr pointed out that he had concerns about the interaction between electrons and magnetic fields. When he arrived in Leiden, Paul Ehrenfest and Albert Einstein informed Bohr that Einstein had resolved this problem using relativity. Bohr then had Uhlenbeck and Goudsmit incorporate this into their paper. Thus, when he met Werner Heisenberg and Pascual Jordan in Göttingen on the way back, he had become, in his own words, "a prophet of the electron magnet gospel".
Heisenberg first came to Copenhagen in 1924, then returned to Göttingen in June 1925, shortly thereafter developing the mathematical foundations of quantum mechanics. When he showed his results to Max Born in Göttingen, Born realised that they could best be expressed using matrices. This work attracted the attention of the British physicist Paul Dirac, who came to Copenhagen for six months in September 1926. Austrian physicist Erwin Schrödinger also visited in 1926. His attempt at explaining quantum physics in classical terms using wave mechanics impressed Bohr, who believed it contributed "so much to mathematical clarity and simplicity that it represents a gigantic advance over all previous forms of quantum mechanics".
When Kramers left the Institute in 1926 to take up a chair as professor of theoretical physics at the Utrecht University, Bohr arranged for Heisenberg to return and take Kramers's place as a "lektor" at the University of Copenhagen. Heisenberg worked in Copenhagen as a university lecturer and assistant to Bohr from 1926 to 1927,
Bohr became convinced that light behaved like both waves and particles, and in 1927, experiments confirmed the de Broglie hypothesis that matter (like electrons) also behaved like waves. He conceived the philosophical principle of complementarity: that items could have apparently mutually exclusive properties, such as being a wave or a stream of particles, depending on the experimental framework. He felt that that it was not fully understood by professional philosophers.
In Copenhagen in 1927 Heisenberg developed his uncertainty principle, which Bohr embraced. In a paper he presented at the Volta Conference at Como in September 1927, he demonstrated that the uncertainty principle could be derived from classical arguments, without quantum terminology or matrices. Einstein preferred the determinism of classical physics over the probabilistic new quantum physics to which he himself had contributed. Philosophical issues that arose from the novel aspects of quantum mechanics became widely celebrated subjects of discussion. Einstein and Bohr had good-natured arguments over such issues throughout their lives.
In 1914, Carl Jacobsen, the heir to Carlsberg breweries, bequeathed his mansion to be used for life by the Dane who had made the most prominent contribution to science, literature or the arts, as an honorary residence (Danish: "Æresbolig"). Harald Høffding had been the first occupant, and upon his death in July 1931, the Royal Danish Academy of Sciences and Letters gave Bohr occupancy. He and his family moved there in 1932. He was elected president of the Academy on 17 March 1939.
By 1929, the phenomenon of beta decay prompted Bohr to again suggest that the law of conservation of energy be abandoned, but Enrico Fermi's hypothetical neutrino and the subsequent 1932 discovery of the neutron provided another explanation. This prompted Bohr to create a new theory of the compound nucleus in 1936, which explained how neutrons could be captured by the nucleus. In this model, the nucleus could be deformed like a drop of liquid. He worked on this with a new collaborator, the Danish physicist Fritz Kalckar, who died suddenly in 1938.
The discovery of nuclear fission by Otto Hahn in December 1938 (and its theoretical explanation by Lise Meitner) generated intense interest among physicists. Bohr brought the news to the United States where he opened the Fifth Washington Conference on Theoretical Physics with Fermi on 26 January 1939. When Bohr told George Placzek that this resolved all the mysteries of transuranic elements, Placzek told him that one remained: the neutron capture energies of uranium did not match those of its decay. Bohr thought about it for a few minutes and then announced to Placzek, Léon Rosenfeld and John Wheeler that "I have understood everything." Based on his liquid drop model of the nucleus, Bohr concluded that it was the uranium-235 isotope and not the more abundant uranium-238 that was primarily responsible for fission. In April 1940, John R. Dunning demonstrated that Bohr was correct. In the meantime, Bohr and Wheeler developed a theoretical treatment which they published in a September 1939 paper on "The Mechanism of Nuclear Fission".
Philosophy.
Bohr read the 19th-century Danish Christian existentialist philosopher, Søren Kierkegaard. Richard Rhodes argued in "The Making of the Atomic Bomb" that Bohr was influenced by Kierkegaard through Høffding. In 1909, Bohr sent his brother Kierkegaard's "Stages on Life's Way" as a birthday gift. In the enclosed letter, Bohr wrote, "It is the only thing I have to send home; but I do not believe that it would be very easy to find anything better ... I even think it is one of the most delightful things I have ever read." Bohr enjoyed Kierkegaard's language and literary style, but mentioned that he had some disagreement with Kierkegaard's philosophy. Some of Bohr's biographers suggested that this disagreement stemmed from Kierkegaard's advocacy of Christianity, while Bohr was an atheist.
There has been some dispute over the extent to which Kierkegaard influenced Bohr's philosophy and science. David Favrholdt argued that Kierkegaard had minimal influence over Bohr's work, taking Bohr's statement about disagreeing with Kierkegaard at face value, while Jan Faye argued that one can disagree with the content of a theory while accepting its general premises and structure.
Nazism and Second World War.
The rise of Nazism in Germany prompted many scholars to flee their countries, either because they were Jewish or because they were political opponents of the Nazi regime. In 1933, the Rockefeller Foundation created a fund to help support refugee academics, and Bohr discussed this programme with the President of the Rockefeller Foundation, Max Mason, in May 1933 during a visit to the United States. Bohr offered the refugees temporary jobs at the Institute, provided them with financial support, arranged for them to be awarded fellowships from the Rockefeller Foundation, and ultimately found them places at institutions around the world. Those that he helped included Guido Beck, Felix Bloch, James Franck, George de Hevesy, Otto Frisch, Hilde Levi, Lise Meitner, George Placzek, Eugene Rabinowitch, Stefan Rozental, Erich Ernst Schneider, Edward Teller, Arthur von Hippel and Victor Weisskopf.
In April 1940, early in the Second World War, Nazi Germany invaded and occupied Denmark. To prevent the Germans from discovering Max von Laue's and James Franck's gold Nobel medals, Bohr had de Hevesy dissolve them in aqua regia. In this form, they were stored on a shelf at the Institute until after the war, when the gold was precipitated and the medals re-struck by the Nobel Foundation. Bohr kept the Institute running, but all the foreign scholars departed.
Meeting with Heisenberg.
Bohr was aware of the possibility of using uranium-235 to construct an atomic bomb, referring to it in lectures in Britain and Denmark shortly before and after the war started, but he did not believe that it was technically feasible to extract a sufficient quantity of uranium-235. In September 1941, Heisenberg, who had become head of the German nuclear energy project, visited Bohr in Copenhagen. During this meeting the two men took a private moment outside, the content of which has caused much speculation, as both gave differing accounts.
According to Heisenberg, he began to address nuclear energy, morality and the war, to which Bohr seems to have reacted by terminating the conversation abruptly while not giving Heisenberg hints about his own opinions. Ivan Supek, one of Heisenberg's students and friends, claimed that the main subject of the meeting was Carl Friedrich von Weizsäcker, who had proposed trying to persuade Bohr to mediate peace between Britain and Germany.
In 1957, Heisenberg wrote to Robert Jungk, who was then working on the book "". Heisenberg explained that he had visited Copenhagen to communicate to Bohr the views of several German scientists, that production of a nuclear weapon was possible with great efforts, and this raised enormous responsibilities on the world's scientists on both sides. When Bohr saw Jungk's depiction in the Danish translation of the book, he drafted (but never sent) a letter to Heisenberg, stating that he never understood the purpose of Heisenberg's visit, was shocked by Heisenberg's opinion that Germany would win the war, and that atomic weapons could be decisive.
Michael Frayn's 1998 play "Copenhagen" explores what might have happened at the 1941 meeting between Heisenberg and Bohr. A BBC television film version of the play was first screened on 26 September 2002, with Stephen Rea as Bohr, and Daniel Craig as Heisenberg. The same meeting had previously been dramatised by the BBC's "Horizon" science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg.
Manhattan Project.
In September 1943, word reached Bohr and his brother Harald that the Nazis considered their family to be Jewish, since their mother, Ellen Adler Bohr, had been a Jew, and that they were therefore in danger of being arrested. The Danish resistance helped Bohr and his wife escape by sea to Sweden on 29 September. The next day, Bohr persuaded King Gustaf V of Sweden to make public Sweden's willingness to provide asylum to Jewish refugees. On 2 October 1943, Swedish radio broadcast that Sweden was ready to offer asylum, and the mass rescue of the Danish Jews by their countrymen followed swiftly thereafter. Some historians claim that Bohr's actions led directly to the mass rescue, while others say that, though Bohr did all that he could for his countrymen, his actions were not a decisive influence on the wider events. Eventually, over 7,000 Danish Jews escaped to Sweden.
When the news of Bohr's escape reached Britain, Lord Cherwell sent a telegram to Bohr asking him to come to Britain. Bohr arrived in Scotland on 6 October in a de Havilland Mosquito operated by British Overseas Airways Corporation. The Mosquitos were unarmed high-speed bomber aircraft that had been converted to carry small, valuable cargoes or important passengers. By flying at high speed and high altitude, they could cross German-occupied Norway, and yet avoid German fighters. Bohr, equipped with parachute, flying suit and oxygen mask, spent the three-hour flight lying on a mattress in the aircraft's bomb bay. During the flight, Bohr did not wear his flying helmet as it was too small, and consequently did not hear the pilot's intercom instruction to turn on his oxygen supply when the aircraft climbed to high altitude to overfly Norway. He passed out from oxygen starvation and only revived when the aircraft descended to lower altitude over the North Sea. Bohr's son Aage followed his father to Britain on another flight a week later, and became his personal assistant.
Bohr was warmly received by James Chadwick and Sir John Anderson, but for security reasons Bohr was kept out of sight. He was given an apartment at St James's Palace and an office with the British Tube Alloys nuclear weapons development team. Bohr was astonished at the amount of progress that had been made. Chadwick arranged for Bohr to visit the United States as a Tube Alloys consultant, with Aage as his assistant. On 8 December 1943, Bohr arrived in Washington, D.C., where he met with the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr. He visited Einstein and Pauli at the Institute for Advanced Study in Princeton, New Jersey, and went to Los Alamos in New Mexico, where the nuclear weapons were being designed. For security reasons, he went under the name of "Nicholas Baker" in the United States, while Aage became "James Baker". In May 1944 the Danish resistance newspaper De frie Danske reported that they had learned that 'the famous son of Denmark Professor Niels Bohr' in October the previous year had fled his country via Sweden to London and from there travelled to Moscow from where he could be assumed to support the war effort.
Bohr did not remain at Los Alamos, but paid a series of extended visits over the course of the next two years. Robert Oppenheimer credited Bohr with acting "as a scientific father figure to the younger men", most notably Richard Feynman. Bohr is quoted as saying, "They didn't need my help in making the atom bomb." Oppenheimer gave Bohr credit for an important contribution to the work on modulated neutron initiators. "This device remained a stubborn puzzle," Oppenheimer noted, "but in early February 1945 Niels Bohr clarified what had to be done."
Bohr recognised early that nuclear weapons would change international relations. In April 1944, he received a letter from Peter Kapitza, written some months before when Bohr was in Sweden, inviting him to come to the Soviet Union. The letter convinced Bohr that the Soviets were aware of the Anglo-American project, and would strive to catch up. He sent Kapitza a non-committal response, which he showed to the authorities in Britain before posting. Bohr met Churchill on 16 May 1944, but found that "we did not speak the same language". Churchill disagreed with the idea of openness towards the Russians to the point that he wrote in a letter: "It seems to me Bohr ought to be confined or at any rate made to see that he is very near the edge of mortal crimes."
Oppenheimer suggested that Bohr visit President Franklin D. Roosevelt to convince him that the Manhattan Project should be shared with the Soviets in the hope of speeding up its results. Bohr's friend, Supreme Court Justice Felix Frankfurter, informed President Roosevelt about Bohr's opinions, and a meeting between them took place on 26 August 1944. Roosevelt suggested that Bohr return to the United Kingdom to try to win British approval. When Churchill and Roosevelt met at Hyde Park on 19 September 1944, they rejected the idea of informing the world about the project, and the aide-mémoire of their conversation contained a rider that "enquiries should be made regarding the activities of Professor Bohr and steps taken to ensure that he is responsible for no leakage of information, particularly to the Russians".
In June 1950, Bohr addressed an "Open Letter" to the United Nations calling for international cooperation on nuclear energy. In the 1950s, after the Soviet Union's first nuclear weapon test, the International Atomic Energy Agency was created along the lines of Bohr's suggestion. In 1957 he received the first ever Atoms for Peace Award.
Later years.
With the war now ended, Bohr returned to Copenhagen on 25 August 1945, and was re-elected President of the Royal Danish Academy of Arts and Sciences on 21 September. At a memorial meeting of the Academy on 17 October 1947 for King Christian X, who had died in April, the new king, Frederick IX, announced that he was conferring the Order of the Elephant on Bohr. This award was normally awarded only to royalty and heads of state, but the king said that it honoured not just Bohr personally, but Danish science. Bohr designed his own coat of arms which featured a taijitu (symbol of yin and yang) and a motto in Latin: "contraria sunt complementa", "opposites are complementary".
The Second World War demonstrated that science, and physics in particular, now required considerable financial and material resources. To avoid a brain drain to the United States, twelve European countries banded together to create CERN, a research organisation along the lines of the national laboratories in the United States, designed to undertake Big Science projects beyond the resources of any one of them alone. Questions soon arose regarding the best location for the facilities. Bohr and Kramers felt that the Institute in Copenhagen would be the ideal site. Pierre Auger, who organised the preliminary discussions, disagreed; he felt that both Bohr and his Institute were past their prime, and that Bohr's presence would overshadow others. After a long debate, Bohr pledged his support to CERN in February 1952, and Geneva was chosen as the site in October. The CERN Theory Group was based in Copenhagen until their new accommodation in Geneva was ready in 1957. Victor Weisskopf, who later became the Director General of CERN, summed up Bohr's role, saying that "there were other personalities who started and conceived the idea of CERN. The enthusiasm and ideas of the other people would not have been enough, however, if a man of his stature had not supported it."
Meanwhile, Scandinavian countries formed the Nordic Institute for Theoretical Physics in 1957, with Bohr as its chairman. He was also involved with the founding of the Research Establishment Risø of the Danish Atomic Energy Commission, and served as its first chairman from February 1956.
Bohr died of heart failure at his home in Carlsberg on 18 November 1962. He was cremated, and his ashes were buried in the family plot in the Assistens Cemetery in the Nørrebro section of Copenhagen, along with those of his parents, his brother Harald, and his son Christian. Years later, his wife's ashes were also interred there. On 7 October 1965, on what would have been his 80th birthday, the Institute was officially renamed to what it had been called unofficially for many years: the Niels Bohr Institute.
Accolades.
Bohr received numerous honours and accolades. In addition to the Nobel Prize, he received the Hughes Medal in 1921, the Matteucci Medal in 1923, the Franklin Medal in 1926, the Copley Medal in 1938, the Order of the Elephant in 1947, the Atoms for Peace Award in 1957 and the Sonning Prize in 1961. The Bohr model's semicentennial was commemorated in Denmark on 21 November 1963 with a postage stamp depicting Bohr, the hydrogen atom and the formula for the difference of any two hydrogen energy levels: formula_3. Several other countries have also issued postage stamps depicting Bohr. In 1997, the Danish National Bank began circulating the 500-krone banknote with the portrait of Bohr smoking a pipe. An asteroid, 3948 Bohr, was named after him, as was a lunar crater (Bohr (crater)), and bohrium, the chemical element with atomic number 107.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="21211" url="http://en.wikipedia.org/wiki?curid=21211" title="National Football League">
National Football League

The National Football League (NFL) is a professional American football league consisting of 32 teams, divided equally between the National Football Conference (NFC) and the American Football Conference (AFC). The NFL is one of the four major professional sports leagues in North America, and the highest professional level of American football in the world. The NFL's 17-week regular season runs from the week after Labor Day to the week after Christmas, with each team playing sixteen games and having one bye week. Following the conclusion of the regular season, six teams from each conference (four division winners and two wild card teams) advance to the playoffs, a single-elimination tournament culminating in the Super Bowl, played between the champions of the NFC and AFC.
The NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1923 season. The NFL agreed to merge with the American Football League (AFL) in 1966, and the first Super Bowl was held at the end of that season; the merger was completed in 1970. Today, the NFL has the highest average attendance (67,591) of any professional sports league in the world and is the most popular sports league in the United States. The Super Bowl is among the biggest club sporting events in the world and individual Super Bowl games account for many of the most watched television programs in American history, all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015. At the corporate level, the NFL is a nonprofit 501(c)(6) association. The NFL's executive officer is the commissioner, who has broad authority in governing the league.
The team with the most NFL championships is the Green Bay Packers with thirteen; the team with the most Super Bowl championships is the Pittsburgh Steelers with six. The current NFL champions are the New England Patriots, who defeated the Seattle Seahawks 28–24 in Super Bowl XLIX.
History.
Founding and history.
On August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio. This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the "Canton Evening Repository", intended to "raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules". Another meeting held on September 17, 1920 resulted in the renaming of the league to the American Professional Football Association (APFA). The league hired Jim Thorpe as their first president, and consisted of 14 teams. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain.
Although the league did not maintain official standings for their 1920 inaugural season and teams played schedules that included non-league opponents, the APFA awarded the Akron Pros the championship by virtue of their 8–0–3 (8 wins, 0 losses, and 3 ties) record. The following season resulted in the Chicago Staleys controversially winning the title over the Buffalo All-Americans. In 1922, the APFA changed its name to the National Football League (NFL).
In 1932, the season ended with the Chicago Bears (6-1-6) and the Portsmouth Spartans (6-1-4) tied for first in the league standings. At the time, teams were ranked on a single table and the team with the highest winning percentage (not including ties, which were not counted towards the standings) at the end of the season was declared the champion. This method had been used since the league's creation in 1920, but no situation had been encountered where two teams were tied for first. The league quickly determined that a playoff game between Chicago and Portsmouth was needed to decide the league's champion. The teams were originally scheduled to play the playoff game, officially a regular season game that would count towards the regular season standings, at Wrigley Field in Chicago, but a combination of heavy snow and extreme cold forced the game to be moved indoors to Chicago Stadium, which did not have a regulation-size football field. Playing with altered rules to accommodate the smaller playing field, the Bears won the game 9-0 and thus won the championship. Fan interest in the "de facto" championship game led the NFL, beginning in 1933, to split into two divisions with a championship game to be played between the division champions. The 1933 season also marked the first of 13 seasons in which African Americans were prohibited from playing in the league. The ban was rescinded in 1947, following public pressure and the removal of a similar ban in Major League Baseball.
Up until the 1960s, the NFL was the dominant professional football league and faced little competition. Rival leagues included three separate American Football Leagues and the All-America Football Conference (AAFC), none of which lasted for more than four seasons, although several teams from the AAFC joined the NFL after the league dissolved in 1949.
A new professional league, the fourth American Football League (AFL), began play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV. After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.
Today, the NFL is considered the most popular sports league in North America; much of its growth is attributed to former Commissioner Pete Rozelle, who led the league from 1960 to 1989. Overall annual attendance increased from three million at the beginning of his tenure to seventeen million by the end of his tenure, and 400 million viewers watched 1989's Super Bowl XXIII. The NFL established NFL Properties in 1963. The league's licensing wing, NFL Properties earns the league billions of dollars annually; Rozelle's tenure also marked the creation of NFL Charities and a national partnership with United Way. Paul Tagliabue was elected as commissioner to succeed Rozelle; his seventeen-year tenure, which ended in 2006, was marked by large increases in television contracts and the addition of four expansion teams, as well as the introduction of league initiatives to increase the number of minorities in league and team management roles. The league's current Commissioner, Roger Goodell, has focused on reducing the number of illegal hits and making the sport safer, mainly through fining or suspending players who break rules. These actions are among many the NFL is taking to reduce concussions and improve player safety.
Season and playoff development.
From 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a 12-game regular season for each team beginning in 1935, later shortening this to 11 games in 1937 and 10 games in 1943, mainly due to World War II. After the war ended, the number of games returned to 11 games in 1946 and to 12 in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to the current 16-game schedule in 1978. Proposals to increase the regular season to 18 games have been made, but have been rejected in labor negotiations with the National Football League Players Association (NFLPA).
The NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the NFL Championship Game. If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four conference champions would meet in the NFL playoffs, a two-round playoff. The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.
Following the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams, would also feature an expanded eight-team eight playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two.
Corporate structure.
The National Football League is an unincorporated nonprofit 501(c)(6) association, meaning its league office is not subject to income tax because it does not make a profit. In contrast, each individual team (except the non-profit Green Bay Packers) is subject to tax because they make a profit. The NFL considers itself a trade association made up of and financed by its 32 member teams.<ref name="http://www.nytimes.com/2008/08/12/sports/football/12nfltax.html"></ref>
The league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one officer, the president. The commissioner is elected by affirmative vote of two-thirds or 18 (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or ten of the conference members. The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the "principal executive officer" of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league bylaws or committed "conduct detrimental to the welfare of the League or professional football". The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.
In extremely egregious cases, the commissioner can offer recommendations to the NFL's Executive Committee up to and including the "cancellation or forfeiture" of a club's franchise or any other action he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games. The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.
Season format.
The NFL season format consists of a four-week preseason, a seventeen-week regular season, and a twelve-team single-elimination playoff culminating in the Super Bowl, the league's championship game.
Preseason.
The NFL preseason begins with the Pro Football Hall of Fame Game, played at Fawcett Stadium in Canton.<ref name="NFL/Hall of Fame Game"></ref> Each NFL team is required to schedule four preseason games, two of which must be at their home stadium, but the teams involved in the Hall of Fame game, as well as any teams playing in an American Bowl game, play five preseason games. Preseason games are exhibition matches and do not count towards regular-season totals. Because the preseason does not count towards standings, teams do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut. The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games, as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.
Regular season.
This chart displays the application of NFL scheduling formula for the New England Patriots (green) after the 2014 season. The Patriots finished first in the AFC East in the 2014 season, and will play the following teams in the 2015 regular season:
The National Football League runs a seventeen-week, 256-game regular season. Since 2001, the season has begun the week after Labor Day and concluded the week after Christmas. The opening game of the season is normally a primetime home game for the league's defending champion.
Most NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well. NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the third Friday in December. NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used twice since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard, and in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention.
NFL regular season matchups are determined according to a scheduling formula. Within a division, all four teams play fourteen out of their sixteen games against common opponents - two games (home and away) are played against the other three teams in the division, while one game is held against all the members of a division from the NFC and a division from the AFC division as determined by a rotating cycle (three years for the conference the team is in, and four years in the conference they are not in). The other two games are intraconference games, determined by the standings of the previous year - for example, if a team finishes first in their division, they will play two other first-place teams in their conference, while a team that finishes last would play two other last-place teams in the conference. In total, each team plays sixteen games and has one bye week, where they do not play any games.
Although the teams any given club will play are known by the end of the previous year's regular season, the exact dates, times, and home/away status for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered "playable schedules" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try and ensure that the chosen schedule would be the best possible one.
Postseason.
Following the conclusion of the regular season, a twelve-team single elimination tournament, the NFL Playoffs, is held. Six teams are selected from each conference: the winners of each of the four divisions as well as two wild card teams (the two remaining teams with the best overall record). These teams are seeded according to overall record, with the division champions always ranking higher than either of the wild card teams. The top two teams (seeded one and two) from each conference are awarded a bye week, while the remaining four teams (seeded 3-6) from each conference compete in the first round of the playoffs, the Wild Card round, with the third seed competing against the sixth seed and the fourth seed competing against the fifth seed. The winners of the Wild Card round advance to the Divisional Round, which matches the lower seeded team against the first seed and the higher seeded team against the second seed. The winners of those games then compete in the Conference Championships, with the higher remaining seed hosting the lower remaining seed. The AFC and NFC champions then compete in the Super Bowl to determine the league champion.
The only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. The Pro Bowl is held the week before the Super Bowl.
Trophies and awards.
Team trophies.
The National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a "silver loving cup", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.
A second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1969. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, like that of its predecessor, is unknown. The predominant theory is that the Minnesota Vikings, the last team to be awarded the trophy, somehow misplaced it after the 1969 season.
The current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from $25,000 to $300,000. Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring. The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.
The conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy, named after Chicago Bears founder George Halas, who is also considered as one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy, named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.
Player and coach awards.
See also: .
The NFL recognizes a number of awards for their players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award. Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards. Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance. The NFL Coach of the Year award is the highest coaching award. The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week and the Pepsi MAX NFL Rookie of the Week awards.
Media coverage.
In the United States, the National Football League has television contracts with four networks: CBS, ESPN, Fox, and NBC. CBS televises Sunday afternoon AFC intraconference games, while Fox does likewise for the NFC. If a game is interconference, the conference affiliation of the visiting team determines which network will host the game. Since 2011, the league has reserved the right to give games that, under the contract, would normally air on one network to the other network. CBS also carries a package of eight games on Thursday nights during the 2014 season. NBC carries the primetime Sunday Night Football package, the NFL Kickoff game, and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games. The NFL's own network, NFL Network, carries all Thursday Night Football games, including those on CBS but not the ones on NBC.
The Super Bowl television rights are rotated on a three-year basis between CBS, Fox, and NBC. In 2011, all four stations signed new nine-year contracts with the NFL, each running until 2022; CBS, Fox, and NBC are estimated by "Forbes" to pay a combined total of $3 billion a year, while ESPN will pay $1.9 billion a year. The league also has deals with Spanish-language broadcasters Telemundo and ESPN Deportes, which air Sunday and Monday Night Football, respectively.
Through the 2014 season, the NFL had a blackout policy in which games were 'blacked out' on local television in the home team's area if the home stadium was not sold out. Clubs could elect to set this requirement at only 85%, but they would have to give more ticket revenue to the visiting team; teams could also request a specific exemption from the NFL for the game. The vast majority of NFL games were not blacked out; only 6% of games were blacked out during the 2011 season, and only two games were blacked out in 2013 and none in 2014. The NFL announced in March 2015 that it would suspend its blackout policy for at least the 2015 season. According to Nielsen, the NFL regular season since 2012 was watched by at least 200 million individuals, accounting for 80% of all television households in the United States and 69% of all potential viewers in the United States. NFL regular season games accounted for 31 out of the top 32 most-watched programs in the fall season and an NFL game ranked as the most-watched television show in all 17 weeks of the regular season. At the local level, NFL games were the highest-ranked shows in NFL markets 92% of the time. Super Bowls account for the 22 most-watched programs (based on total audience) in US history, including a record 167 million people that watched Super Bowl XLVIII, the conclusion to the 2013 season.
In addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and will run through 2017.
On March 23, 2015, "The Wall Street Journal" reported that the NFL would distribute the 2015 International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars exclusively via Internet streaming, except for over-the-air broadcasts in the Buffalo and Jacksonville local markets. 
Clubs.
Bills 
Dolphins 
Patriots 
Jets 
Ravens 
Bengals 
Browns 
Steelers 
Texans 
Colts 
Titans 
Broncos 
Chiefs 
Raiders 
Chargers 
Cowboys 
Giants 
Eagles 
Redskins 
Bears 
Lions 
Packers 
Vikings 
Falcons 
Panthers 
Saints 
Buccaneers 
Jaguars 
Cardinals 
Rams 
49ers 
Seahawks 
 
The NFL consists of thirty-two clubs divided into two conferences of sixteen teams each. Each conference is divided into four divisions of four clubs each. During the regular season, each team is allowed a maximum of fifty-three players on their roster; only forty-six of these may be active (eligible to play) on game days. Teams can also have an eight-player practice squad separate from their main roster, but the practice squad may only be composed of players who were not active for at least nine games in any of their seasons in the league. A player can only be on a practice squad for a maximum of three seasons.
Each NFL club is granted a franchise, the league's authorization for the team to operate in their city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates their training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside their Home Territory and the exclusive right to advertise, promote, and host events in their Home Marketing Area. There are several exceptions to this rule, mostly relating to teams with close proximity to each other: the San Francisco 49ers and Oakland Raiders only have exclusive rights in their cities and share rights outside of it; and teams that operate in the same city (e.g. New York Giants and New York Jets) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively. The Los Angeles home territory has no team, but is "owned and controlled" by the league.
Every NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Buffalo Bills played one home game every season at the Rogers Centre in Toronto, Canada as part of the Bills Toronto Series from 2008 until 2013, and the Jacksonville Jaguars will play one home game a year from 2013 to 2016 at Wembley Stadium in London, England as part of the NFL International Series. Mexico also has hosted an NFL regular-season game, a 2005 game between San Francisco 49ers and Arizona dubbed "Fútbol Americano", and 39 international pre-season games were played from 1986 to 2005 as part of the American Bowl series.
The Dallas Cowboys, at approximately $3.2 billion, are the most valuable NFL franchise according to Forbes. Three other franchises are worth $2 billion or greater, the New England Patriots ($2.6 billion), the Washington Redskins ($2.4 billion), and the New York Giants ($2.1 billion). The Cowboys are the most valuable sports team in the United States, and tied with soccer club FC Barcelona for the second-most in the world; only Real Madrid ($3.4b) is valued higher than the Cowboys. All 32 NFL teams rank among the top 50 most valuable sports teams in the world. Fourteen of the NFL's owners are listed on the Forbes 400, the most of any sports league or organization.
Draft.
Each April (except in 2014 when it was moved to May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round. The draft order for non-playoff teams is determined by regular season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the runner-up always drafts second-to-last. All potential draftees must be at least three years removed from high school in order to be eligible for the draft. Underclassmen that have met that criteria to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility. Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.
Aside from the 32 picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given. Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs. Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for next year's draft. Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with. Players who were draft eligible but not picked in the draft are free to sign with any club.
The NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to their pick in the equivalent round of the next NFL draft. Players are only eligible for the supplemental draft after being granted a petition for special eligibility. The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team. Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.
Like the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being cancelled, a restocking draft would be held. Neither of these protocols have ever had to be implemented.
Free agency.
Free agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as "six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists". Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.
Clubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of their previous contract or no less than the average of the five highest-paid players at their position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.
Clubs also have the option to use a transition tag, which is similar to the non-exclusive franchise tag but offers no compensation if the former club refuses to match the offer. Due to that stipulation, the transition tag is rarely used, even with the removal of the "poison pill" strategy (offering a contract with stipulations that the former club would be unable to match) that essentially ended the usage of the tag league-wide. Each club is subject to a salary cap, which is set at $123 million for the 2013 season.
Fantasy football.
The National Football League supports fantasy football.
References.
Explanatory notes
Citations
Bibliography
</dl>

</doc>
<doc id="21212" url="http://en.wikipedia.org/wiki?curid=21212" title="Nazi Germany">
Nazi Germany

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1933–1934
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1939 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1939 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
Nazi Germany or the Third Reich was the period in the history of Germany from 1933 to 1945, when it was a dictatorship under the control of Adolf Hitler and the Nazi Party (NSDAP). Under Hitler's rule, Germany was transformed into a fascist totalitarian state which controlled nearly all aspects of life. Nazi Germany ceased to exist after the Allied Forces defeated Germany in May 1945, ending World War II in Europe.
Hitler was appointed Chancellor of Germany by the President of the Weimar Republic Paul von Hindenburg on 30 January 1933. The Nazi Party then began to eliminate all political opposition and consolidate its power. Hindenburg died on 2 August 1934, and Hitler became dictator of Germany by merging the powers and offices of the Chancellery and Presidency. A national referendum held 19 August 1934 confirmed Hitler as sole Führer (leader) of Germany. All power was centralised in Hitler's hands, and his word became above all laws. The government was not a coordinated, co-operating body, but a collection of factions struggling for power and Hitler's favour. In the midst of the Great Depression, the Nazis restored economic stability and ended mass unemployment using heavy military spending and a mixed economy. Extensive public works were undertaken, including the construction of "Autobahns" (high speed highways). The return to economic stability boosted the regime's popularity.
Racism, especially antisemitism, was a central feature of the regime. The Germanic peoples (the Nordic race) were considered the purest of the Aryan race, and were therefore the master race. Millions of Jews and others deemed undesirable were persecuted and murdered in the Holocaust. Opposition to Hitler's rule was ruthlessly suppressed. Members of the liberal, socialist, and communist opposition were killed, imprisoned, or exiled. The Christian churches were also oppressed, with many leaders imprisoned. Education focused on racial biology, population policy, and fitness for military service. Career and educational opportunities for women were curtailed. Recreation and tourism were organised via the Strength Through Joy program, and the 1936 Summer Olympics showcased the Third Reich on the international stage. Propaganda minister Joseph Goebbels made effective use of film, mass rallies, and Hitler's hypnotising oratory to control public opinion. The government controlled artistic expression, promoting specific art forms and banning or discouraging others.
Nazi Germany made increasingly aggressive territorial demands, threatening war if they were not met. It seized Austria and Czechoslovakia in 1938 and 1939. Hitler made a pact with Joseph Stalin and invaded Poland in September 1939, launching World War II in Europe. In alliance with Italy and smaller Axis powers, Germany conquered most of Europe by 1940 and threatened Great Britain. "Reichskommissariats" took control of conquered areas, and a German administration was established in what was left of Poland. Jews and others deemed undesirable were imprisoned and murdered in Nazi concentration camps and extermination camps. The implementation of the regime's racial policies culminated in the mass murder of Jews and other minorities in the Holocaust. Following the German invasion of the Soviet Union in 1941, the tide turned against the Third Reich, and it suffered major military defeats in 1943. Large-scale bombing of Germany escalated in 1944, and the Nazis retreated from Eastern and Southern Europe. Following the Allied invasion of France, Germany was conquered by the Soviets from the east and the other Allied powers from the west and surrendered within a year. Hitler's refusal to admit defeat led to massive destruction of German infrastructure and additional war-related deaths in the closing months of the war. The victorious Allies initiated a policy of denazification and put many of the surviving Nazi leadership on trial for war crimes at the Nuremberg trials.
Name.
The official name of the state was "Deutsches Reich" (German Reich) from 1933 to 1943, and "Großdeutsches Reich" (Greater German Reich) from 1943 to 1945. The name "Deutsches Reich" is usually translated into English as "German Empire" or "German Reich". Modern Germans refer to the period as "Zeit des Nationalsozialismus" (National Socialist period), "Nationalsozialistische Gewaltherrschaft" (National Socialist tyranny), or simply as "das Dritte Reich" (the Third Reich).
Common English terms are "Nazi Germany" and "Third Reich". The latter, adopted by Nazi propaganda, was first used in a 1923 book by Arthur Moeller van den Bruck. The book counted the Holy Roman Empire (962–1806) as the first Reich and the German Empire (1871–1918) as the second. The Nazis used it to legitimize their regime as a successor state. After they seized power, Nazi propaganda retroactively referred to the Weimar Republic as the "Zwischenreich" ("Interim Reich").
Beginning in the 1980s, German linguistic critics have questioned the uncritical adoption of the expression "Third Reich". In 1984, German jurist Walter Mallman wrote that in the "conceptual history of political, constitutional, and legal thought", the term is "indefensible". In 1989, Dieter Gunst further noted that referring to the Hitler regime as the Third Reich is not only a "positive revaluation of National Socialism" but also a misrepresentation of history, adding that Hitler did not found a state or any "particular Reich".
History.
Background.
The German economy suffered severe setbacks after the end of World War I, partly because of reparations payments required under the 1919 Treaty of Versailles. The government printed money to make the payments and to repay the country's war debt; the resulting hyperinflation led to inflated prices for consumer goods, economic chaos, and food riots. When the government failed to make the reparations payments in January 1923, French troops occupied German industrial areas along the Ruhr. Widespread civil unrest followed.
The National Socialist German Workers' Party (NSDAP; Nazi Party) was the renamed successor of the German Workers' Party founded in 1919, one of several far-right political parties active in Germany at the time. The party platform included removal of the Weimar Republic, rejection of the terms of the Treaty of Versailles, radical antisemitism, and anti-Bolshevism. They promised a strong central government, increased "Lebensraum" (living space) for Germanic peoples, formation of a national community based on race, and racial cleansing via the active suppression of Jews, who would be stripped of their citizenship and civil rights. The Nazis proposed national and cultural renewal based upon the "Völkisch" movement.
When the stock market in the United States crashed on 24 October 1929, the impact in Germany was dire. Millions were thrown out of work, and several major banks collapsed. Hitler and the NSDAP prepared to take advantage of the emergency to gain support for their party. They promised to strengthen the economy and provide jobs. Many voters decided the NSDAP was capable of restoring order, quelling civil unrest, and improving Germany's international reputation. After the federal election of 1932, the Nazis were the largest party in the Reichstag, holding 230 seats with 37.4 per cent of the popular vote.
Nazi seizure of power.
Although the Nazis won the greatest share of the popular vote in the two Reichstag general elections of 1932, they did not have a majority, so Hitler led a short-lived coalition government formed by the NSDAP and the German National People's Party. Under pressure from politicians, industrialists, and the business community, President Paul von Hindenburg appointed Hitler as Chancellor of Germany on 30 January 1933. This event is known as the "Machtergreifung" (seizure of power). In the following months, the NSDAP used a process termed "Gleichschaltung" (co-ordination) to rapidly bring all aspects of life under control of the party. All civilian organisations, including agricultural groups, volunteer organisations, and sports clubs, had their leadership replaced with Nazi sympathisers or party members. By June 1933, virtually the only organisations not in the control of the NSDAP were the army and the churches.
On the night of 27 February 1933, the Reichstag building was set afire; Marinus van der Lubbe, a Dutch communist, was found guilty of starting the blaze. Hitler proclaimed that the arson marked the start of a communist uprising. Violent suppression of communists by the "Sturmabteilung" (SA) was undertaken all over the country, and four thousand members of the Communist Party of Germany were arrested. The Reichstag Fire Decree, imposed on 28 February 1933, rescinded most German civil liberties, including rights of assembly and freedom of the press. The decree also allowed the police to detain people indefinitely without charges or a court order. The legislation was accompanied by a propaganda blitz that led to public support for the measure.
In March 1933, the Enabling Act, an amendment to the Weimar Constitution, passed in the Reichstag by a vote of 444 to 94. This amendment allowed Hitler and his cabinet to pass laws—even laws that violated the constitution—without the consent of the president or the Reichstag. As the bill required a two-thirds majority to pass, the Nazis used the provisions of the Reichstag Fire Decree to keep several Social Democratic deputies from attending; the Communists had already been banned. On 10 May the government seized the assets of the Social Democrats; they were banned in June. The remaining political parties were dissolved, and on 14 July 1933, Germany became a de facto single-party state when the founding of new parties was made illegal. Further elections in November 1933, 1936, and 1938 were entirely Nazi-controlled and saw only the Nazis and a small number of independents elected. The regional state parliaments and the "Reichsrat" (federal upper house) were abolished in January 1934.
The Nazi regime abolished the symbols of the Weimar Republic, including the black, red, and gold tricolor flag, and adopted reworked imperial symbolism. The previous imperial black, white, and red tricolor was restored as one of Germany's two official flags; the second was the swastika flag of the NSDAP, which became the sole national flag in 1935. The NSDAP anthem "Horst-Wessel-Lied" ("Horst Wessel Song") became a second national anthem.
In this period, Germany was still in a dire economic situation; millions were unemployed and the balance of trade deficit was daunting. Hitler knew that reviving the economy was vital. In 1934, using deficit spending, public works projects were undertaken. A total of 1.7 million Germans were put to work on the projects in 1934 alone. Average wages both per hour and per week began to rise.
The demands of the SA for more political and military power caused anxiety among military, industrial, and political leaders. In response, Hitler purged the entire SA leadership in the Night of the Long Knives, which took place from 30 June to 2 July 1934. Hitler targeted Ernst Röhm and other SA leaders who, along with a number of Hitler's political adversaries (such as Gregor Strasser and former chancellor Kurt von Schleicher), were rounded up, arrested, and shot.
On 2 August 1934, President von Hindenburg died. The previous day, the cabinet had enacted the "Law Concerning the Highest State Office of the Reich", which stated that upon Hindenburg's death, the office of president would be abolished and its powers merged with those of the chancellor. Hitler thus became head of state as well as head of government. He was formally named as "Führer und Reichskanzler" (leader and chancellor). Germany was now a totalitarian state with Hitler at its head. As head of state, Hitler became Supreme Commander of the armed forces. The new law altered the traditional loyalty oath of servicemen so that they affirmed loyalty to Hitler personally rather than the office of supreme commander or the state. On 19 August, the merger of the presidency with the chancellorship was approved by 90 per cent of the electorate in a plebiscite.
Most Germans were relieved that the conflicts and street fighting of the Weimar era had ended. They were deluged with propaganda orchestrated by Joseph Goebbels, who promised peace and plenty for all in a united, Marxist-free country without the constraints of the Versailles Treaty. The first Nazi concentration camp, initially for political prisoners, was opened at Dachau in 1933. Hundreds of camps of varying size and function were created by the end of the war. Upon seizing power, the Nazis took repressive measures against their political opposition and rapidly began the comprehensive marginalisation of persons they considered socially undesirable. Under the guise of combating the Communist threat, the National Socialists secured immense power. Above all, their campaign against Jews living in Germany gained momentum.
Beginning in April 1933, scores of measures defining the status of Jews and their rights were instituted at the regional and national level. Initiatives and legal mandates against the Jews reached their culmination with the establishment of the Nuremberg Laws of 1935, stripping them of their basic rights. The Nazis would take from the Jews their wealth, their right to intermarry with non-Jews, and their right to occupy many fields of labour (such as practising law, medicine, or working as educators). They eventually declared them undesirable to remain among German citizens and society, which over time dehumanised the Jews; arguably, these actions desensitised Germans to the extent that it resulted in the Holocaust. Ethnic Germans who refused to ostracise Jews or who showed any signs of resistance to Nazi propaganda were placed under surveillance by the Gestapo, had their rights removed, or were sent to concentration camps. Everyone and everything was monitored in Nazi Germany. Inaugurating and legitimising power for the Nazis was thus accomplished by their initial revolutionary activities, then through the improvisation and manipulation of the legal mechanisms available, through the use of police powers by the Nazi Party (which allowed them to include and exclude from society whomever they chose), and finally by the expansion of authority for all state and federal institutions.
Militaristic foreign policy.
As early as February 1933, Hitler announced that rearmament must be undertaken, albeit clandestinely at first, as to do so was in violation of the Versailles Treaty. A year later he told his military leaders that 1942 was the target date for going to war in the east. He pulled Germany out of the League of Nations in 1933, claiming its disarmament clauses were unfair, as they applied only to Germany. The Saarland, which had been placed under League of Nations supervision for 15 years at the end of World War I, voted in January 1935 to become part of Germany. In March 1935 Hitler announced that the "Reichswehr" would be increased to 550,000 men and that he was creating an air force. Britain agreed that the Germans would be allowed to build a naval fleet with the signing of the Anglo-German Naval Agreement on 18 June 1935.
When the Italian invasion of Ethiopia led to only mild protests by the British and French governments, on 7 March 1936 Hitler ordered the "Reichswehr" to march 3,000 troops into the demilitarised zone in the Rhineland in violation of the Versailles Treaty; an additional 30,000 troops were on standby. As the territory was part of Germany, the British and French governments did not feel that attempting to enforce the treaty was worth the risk of war. In the single-party election held on 29 March, the NSDAP received 98.9 per cent support. In 1936 Hitler signed an Anti-Comintern Pact with Japan and a non-aggression agreement with the Fascist Italy of Benito Mussolini, who was soon referring to a "Rome-Berlin Axis".
Hitler sent air and armoured units to assist General Francisco Franco and his Nationalist forces in the Spanish Civil War, which broke out in July 1936. The Soviet Union sent a smaller force to assist the Republican government. Franco's Nationalists were victorious in 1939 and became an informal ally of Nazi Germany.
Austria and Czechoslovakia.
In February 1938, Hitler emphasised to Austrian Chancellor Kurt Schuschnigg the need for Germany to secure its frontiers. Schuschnigg scheduled a plebiscite regarding Austrian independence for 13 March, but Hitler demanded that it be cancelled. On 11 March, Hitler sent an ultimatum to Schuschnigg demanding that he hand over all power to the Austrian NSDAP or face an invasion. The "Wehrmacht" entered Austria the next day, to be greeted with enthusiasm by the populace.
The Republic of Czechoslovakia was home to a substantial minority of Germans, who lived mostly in the Sudetenland. Under pressure from separatist groups within the Sudeten German Party, the Czechoslovak government offered economic concessions to the region. Hitler decided to incorporate not just the Sudetenland but the whole of Czechoslovakia into the Reich. The Nazis undertook a propaganda campaign to try to drum up support for an invasion. Top leaders of the armed forces were not in favour of the plan, as Germany was not yet ready for war. The crisis led to war preparations by the British, the Czechoslovaks, and France (Czechoslovakia's ally). Attempting to avoid war, British Prime Minister Neville Chamberlain arranged a series of meetings, the result of which was the Munich Agreement, signed on 29 September 1938. The Czechoslovak government was forced to accept the Sudetenland's annexation into Germany. Chamberlain was greeted with cheers when he landed in London bringing, he said, "peace for our time." The agreement lasted six months before Hitler seized the rest of Czech territory in March 1939. A puppet state was created in Slovakia.
Austrian and Czech foreign exchange reserves were soon seized by the Nazis, as were stockpiles of raw materials such as metals and completed goods such as weaponry and aircraft, which were shipped back to Germany. The "Reichswerke Hermann Göring" industrial conglomerate took control of steel and coal production facilities in both countries.
Poland.
In March 1939, Hitler demanded the return of the Free City of Danzig and the Polish Corridor, a strip of land that separated East Prussia from the rest of Germany. The British announced they would come to the aid of Poland if it was attacked. Hitler, believing the British would not actually take action, ordered an invasion plan should be readied for a target date of September 1939. On 23 May he described to his generals his overall plan of not only seizing the Polish Corridor but greatly expanding German territory eastward at the expense of Poland. He expected this time they would be met by force.
The Germans reaffirmed their alliance with Italy and signed non-aggression pacts with Denmark, Estonia, and Latvia. Trade links were formalised with Romania, Norway, and Sweden. Hitler's foreign minister, Joachim von Ribbentrop, arranged in negotiations with the Soviet Union a non-aggression pact, the Molotov–Ribbentrop Pact, which was signed in August 1939. The treaty also contained secret protocols dividing Poland and the Baltic states into German and Soviet spheres of influence.
World War II.
Foreign policy.
Germany's foreign policy during the war involved the creation of allied governments under direct or indirect control from Berlin. A main goal was obtaining soldiers from the senior allies, such as Italy and Hungary, and millions of workers and ample food supplies from subservient allies such as Vichy France. By the fall of 1942, there were 24 divisions from Romania on the Eastern Front, 10 from Italy, and 10 from Hungary. When a country was no longer dependable, Germany assumed full control, as it did with France in 1942, Italy in 1943, and Hungary in 1944. Although Japan was an official powerful ally, the relationship was distant and there was little co-ordination or co-operation. For example, Germany refused to share their formula for synthetic oil from coal until late in the war.
Outbreak of war.
Germany invaded Poland on 1 September 1939. Britain and France declared war on Germany two days later. World War II was under way. Poland fell quickly, as the Soviets attacked from the east on 17 September. Reinhard Heydrich, then head of the Gestapo, ordered on 21 September that Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport the Jews to points further east, or possibly to Madagascar. Using lists prepared ahead of time, some 65,000 Polish intelligentsia, noblemen, clergy, and teachers were killed by the end of 1939 in an attempt to destroy Poland's identity as a nation. The Soviets continued to attack, advancing into Finland in the Winter War, and German forces were involved in action at sea. But little other activity occurred until May, so the period became known as the "Phoney War".
From the start of the war, a British blockade on shipments to Germany had an impact on the Reich economy. The Germans were particularly dependent on foreign supplies of oil, coal, and grain. To safeguard Swedish iron ore shipments to Germany, Hitler ordered an attack on Norway, which took place on 9 April 1940. Much of the country was occupied by German troops by the end of April. Also on 9 April, the Germans invaded and occupied Denmark.
Conquest of Europe.
Against the judgement of many of his senior military officers, Hitler ordered an attack on France and the Low Countries, which began in May 1940. They quickly conquered Luxembourg, the Netherlands, and Belgium, and France surrendered on 22 June. The unexpectedly swift defeat of France resulted in an upswing in Hitler's popularity and a strong upsurge in war fever.
In spite of the provisions of the Hague Convention, industrial firms in the Netherlands, France, and Belgium were put to work producing war materiel for the occupying German military. Officials viewed this option as being preferable to their citizens being deported to the Reich as forced labour.
The Nazis seized from the French thousands of locomotives and rolling stock, stockpiles of weapons, and raw materials such as copper, tin, oil, and nickel. Financial demands were levied on the governments of the occupied countries as well; payments for occupation costs were received from France, Belgium, and Norway. Barriers to trade led to hoarding, black markets, and uncertainty about the future. Food supplies were precarious; production dropped in most areas of Europe, but not as much as during World War I. Greece experienced famine in the first year of occupation and the Netherlands in the last year of the war.
Hitler made peace overtures to the new British leader, Winston Churchill, and upon their rejection he ordered a series of aerial attacks on Royal Air Force airbases and radar stations. However, the German Luftwaffe failed to defeat the Royal Air Force in what became known as the Battle of Britain. By the end of October, Hitler realised the necessary air superiority for his planned invasion of Britain could not be achieved, and he ordered nightly air raids on British cities, including London, Plymouth, and Coventry.
In February 1941, the German "Afrika Korps" arrived in Libya to aid the Italians in the North African Campaign and attempt to contain Commonwealth forces stationed in Egypt. On 6 April, Germany launched the invasion of Yugoslavia and the battle of Greece. German efforts to secure oil included negotiating a supply from their new ally, Romania, who signed the Tripartite Pact in November 1940.
On 22 June 1941, contravening the Molotov–Ribbentrop Pact, 5.5 million Axis troops attacked the Soviet Union. In addition to Hitler's stated purpose of acquiring "Lebensraum", this large-scale offensive (codenamed Operation Barbarossa) was intended to destroy the Soviet Union and seize its natural resources for subsequent aggression against the Western powers. The reaction among Germans was one of surprise and trepidation. Many were concerned about how much longer the war would drag on or suspected that Germany could not win a war fought on two fronts.
The invasion conquered a huge area, including the Baltic republics, Belarus, and West Ukraine. After the successful Battle of Smolensk, Hitler ordered Army Group Centre to halt its advance to Moscow and temporarily divert its Panzer groups to aid in the encirclement of Leningrad and Kiev. This pause provided the Red Army with an opportunity to mobilise fresh reserves. The Moscow offensive, which resumed in October 1941, ended disastrously in December. On 7 December 1941, Japan attacked Pearl Harbor, Hawaii. Four days later, Germany declared war on the United States.
Food was in short supply in the conquered areas of the Soviet Union and Poland, with rations inadequate to meet nutritional needs. The retreating armies had burned the crops, and much of the remainder was sent back to the Reich. In Germany itself, food rations had to be cut in 1942. In his role as Plenipotentiary of the Four Year Plan, Hermann Göring demanded increased shipments of grain from France and fish from Norway. The 1942 harvest was a good one, and food supplies remained adequate in Western Europe.
Reichsleiter Rosenberg Taskforce was an organisation set up to loot artwork and cultural material from Jewish collections, libraries, and museums throughout Europe. Some 26,000 railroad cars full of art treasures, furniture, and other looted items were sent back to Germany from France alone. In addition, soldiers looted or purchased goods such as produce and clothing—items which were becoming harder to obtain in Germany—for shipment back home.
Turning point and collapse.
Germany, and Europe as a whole, was almost totally dependent on foreign oil imports. In an attempt to resolve the persistent shortage, Germany launched "Fall Blau" (Case Blue), an offensive against the Caucasian oilfields, in June 1942. The Soviets launched a counter-offensive on 19 November and encircled the German armies, who were trapped in Stalingrad on 23 November. Göring assured Hitler that the 6th Army could be supplied by air, but this turned out to be infeasible. Hitler's refusal to allow a retreat led to the deaths of 200,000 German and Romanian soldiers; of the 91,000 men who surrendered in the city on 31 January 1943, only 6,000 survivors returned to Germany after the war. Soviet forces continued to push the invaders westward after the failed German offensive at the Battle of Kursk, and by the end of 1943 the Germans had lost most of their territorial gains in the east.
In Egypt, Field Marshal Erwin Rommel's "Afrika Korps" were defeated by British forces under Field Marshal Bernard Montgomery in October 1942. Allied forces landed in Sicily in July 1943 and in Italy in September. Meanwhile, American and British bomber fleets, based in Britain, began operations against Germany. In an effort to destroy German morale, many sorties were intentionally given civilian targets. Soon German aircraft production could not keep pace with losses, and without air cover, the Allied bombing campaign became even more devastating. By targeting oil refineries and factories, they crippled the German war effort by late 1944.
On 6 June 1944, American, British, and Canadian forces established a western front with the D-Day landings in Normandy. On 20 July 1944, Hitler narrowly survived a bomb attack. He ordered savage reprisals, resulting in 7,000 arrests and the execution of more than 4,900 people. The failed Ardennes Offensive (16 December 1944 – 25 January 1945) was the last major German campaign of the war. Soviet forces entered Germany on 27 January. Hitler's refusal to admit defeat and his repeated insistence that the war be fought to the last man led to unnecessary death and destruction in the closing months of the war. Through his Justice Minister, Otto Georg Thierack, he ordered that anyone who was not prepared to fight should be summarily court-martialed. Thousands of people were put to death. In many areas, people looked for ways to surrender to the approaching Allies, in spite of exhortations of local leaders to continue the struggle. Hitler also ordered the intentional destruction of transport, bridges, industries, and other infrastructure—a scorched earth decree—but Armaments Minister Albert Speer was able to keep this order from being fully carried out.
During the Battle of Berlin (16 April 1945 – 2 May 1945), Hitler and his staff lived in the underground "Führerbunker", while the Red Army approached. On 30 April, when Soviet troops were one or two blocks away from the Reich Chancellery, Hitler and Eva Braun committed suicide in the "Führerbunker". On 2 May General Helmuth Weidling unconditionally surrendered Berlin to Soviet General Vasily Chuikov. Hitler was succeeded by Grand Admiral Karl Dönitz as Reich President and Goebbels as Reich Chancellor. Goebbels and his wife Magda committed suicide the next day, after murdering their six children. On 4–8 May 1945 most of the remaining German armed forces surrendered unconditionally. The German Instrument of Surrender was signed 7 May, marking the end of World War II in Europe.
Suicide rates in Germany increased as the war drew to a close, particularly in areas where the Red Army was advancing. More than a thousand people (out of a population of around 16,000) committed suicide in Demmin on and around 1 May 1945 as the 65th Army of 2nd Belorussian Front first broke into a distillery and then rampaged through the town, committing mass rapes, arbitrarily executing civilians, and setting fire to buildings. High numbers of suicides took place in many other locations, including Neubrandenburg (600 dead), Stolp in Pommern (1,000 dead), and Berlin, where at least 7,057 people committed suicide in 1945.
German casualties.
Estimates of the total German war dead range from 5.5 to 6.9 million persons. A study by German historian puts the number of German military dead and missing at 5.3 million, including 900,000 men conscripted from outside of Germany's 1937 borders, in Austria, and in east-central Europe. Overy estimated in 2014 that in all about 353,000 civilians were killed by British and American bombing of German cities. An additional 20,000 died in the land campaign. Some 22,000 citizens died during the Battle of Berlin. Other civilian deaths include 300,000 Germans (including Jews) who were victims of Nazi political, racial, and religious persecution, and 200,000 who were murdered in the Nazi euthanasia program. Political courts called "Sondergerichte" sentenced some 12,000 members of the German resistance to death, and civil courts sentenced an additional 40,000 Germans. Mass rapes of German women also took place.
At the end of the war, Europe had more than 40 million refugees, its economy had collapsed, and 70 per cent of its industrial infrastructure was destroyed. Between twelve and fourteen million ethnic Germans fled or were expelled from east-central Europe to Germany. During the Cold War, the West German government estimated a death toll of 2.2 million civilians due to the flight and expulsion of Germans and through forced labour in the Soviet Union. This figure remained unchallenged until the 1990s, when some historians put the death toll at 500,000–600,000 confirmed deaths. In 2006 the German government reaffirmed its position that 2.0–2.5 million deaths occurred.
Geography.
Territorial changes.
As a result of their defeat in World War I and the resulting Treaty of Versailles, Germany lost Alsace-Lorraine, Northern Schleswig, and Memel. The Saarland temporarily became a protectorate of France, under the condition that its residents would later decide by referendum which country to join. Poland became a separate nation and was given access to the sea by the creation of the Polish Corridor, which separated Prussia from the rest of Germany. Danzig was made a free city.
Germany regained control of the Saarland via a referendum held in 1935 and annexed Austria in the Anschluss of 1938. The Munich Agreement of 1938 gave Germany control of the Sudetenland, and they seized the remainder of Czechoslovakia six months later. Under threat of invasion by sea, Lithuania surrendered the Memel district to the Nazis in March 1939.
Between 1939 and 1941 the Third Reich invaded Poland, France, Luxembourg, the Netherlands, Belgium, and the Soviet Union. Trieste, South Tyrol, and Istria were ceded to Germany by Mussolini in 1943. Two puppet districts were set up in the area, the Operational Zone of the Adriatic Littoral and the Operational Zone of the Alpine Foothills.
Occupied territories.
Some of the conquered territories were immediately incorporated into Germany as part of Hitler's long-term goal of creating a Greater Germanic Reich. Several areas, such as Alsace-Lorraine, were placed under the authority of an adjacent "Gau" (regional district). Beyond the territories incorporated into Germany were the "Reichskommissariate" (Reich Commissariats), quasi-colonial regimes established in a number of occupied countries. Areas placed under German administration included the Protectorate of Bohemia and Moravia, "Reichskommissariat Ostland" (encompassing the Baltic states and Belarus), and "Reichskommissariat Ukraine". Conquered areas of Belgium and France were placed under control of the Military Administration in Belgium and Northern France. Part of Poland was immediately incorporated into the Reich, and the General Government was established in occupied central Poland. Hitler intended to eventually incorporate many of these areas into the Reich.
The governments of Denmark, Norway ("Reichskommissariat Norwegen"), and the Netherlands ("Reichskommissariat Niederlande") were placed under civilian administrations staffed largely by natives.
Post-war changes.
With the issuance of the Berlin Declaration on 5 June 1945 and later creation of the Allied Control Council, the four Allied powers temporarily assumed governance of Germany. At the Potsdam Conference in August 1945, the Allies arranged for the Allied occupation and denazification of the country. Germany was split into four zones, each occupied by one of the Allied powers, who drew reparations from their zone. Since most of the industrial areas were in the western zones, the Soviet Union was transferred additional reparations. The Allied Control Council disestablished Prussia on 20 May 1947. Aid to Germany began arriving from the United States under the Marshall Plan in 1948. The occupation lasted until 1949, when the countries of East Germany and West Germany were created. Germany finalised her border with Poland by signing the Treaty of Warsaw (1970). Germany remained divided until 1990, when the Allies renounced all claims to German territory with the Treaty on the Final Settlement with Respect to Germany, under which Germany also renounced claims to territories lost during World War II.
Politics.
Ideology.
The NSDAP was a far-right political party which came into its own during the social and financial upheavals that occurred with the onset of the Great Depression in 1929. While in prison after the failed Beer Hall Putsch of 1923, Hitler wrote "Mein Kampf", which laid out his plan for transforming German society into one based on race. The ideology of Nazism brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more "Lebensraum" for the Germanic people. The regime attempted to obtain this new territory by attacking Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race and part of a Jewish Bolshevik conspiracy. Others deemed unworthy of life by the Nazis included the mentally and physically disabled, Romani people, homosexuals, Jehovah's Witnesses, and social misfits.
Influenced by the "Völkisch" movement, the regime was against cultural modernism and supported the development of an extensive military at the expense of intellectualism. Creativity and art were stifled, except where they could serve as propaganda media. The party used symbols such as the Blood Flag and rituals such as the Nazi party rallies to foster unity and bolster the regime's popularity.
Government.
A law promulgated 30 January 1934 abolished the existing "Länder" (constituent states) of Germany and replaced them with new administrative divisions of Nazi Germany, the "Gaue", headed by NSDAP leaders ("Gauleiters"), who effectively became the governor of their region. The change was never fully implemented, as the Länder were still used as administrative divisions for some government departments such as education. This led to a bureaucratic tangle of overlapping jurisdictions and responsibilities typical of the administrative style of the Nazi regime.
Jewish civil servants lost their jobs in 1933, except for those who had seen military service in World War I. Members of the NSDAP or party supporters were appointed in their place. As part of the process of "Gleichschaltung", the Reich Local Government Law of 1935 abolished local elections. From that point forward, mayors were appointed by the Ministry of the Interior.
Hitler ruled Germany autocratically by asserting the "Führerprinzip" (leader principle), which called for absolute obedience of all subordinates. He viewed the government structure as a pyramid, with himself—the infallible leader—at the apex. Rank in the party was not determined by elections; positions were filled through appointment by those of higher rank. The party used propaganda to develop a cult of personality around Hitler. Historians such as Kershaw emphasise the psychological impact of Hitler's skill as an orator. Kressel writes, "Overwhelmingly ... Germans speak with mystification of Hitler's 'hypnotic' appeal".
Top officials reported to Hitler and followed his policies, but they had considerable autonomy. Officials were expected to "work towards the Führer" – to take the initiative in promoting policies and actions in line with his wishes and the goals of the NSDAP, without Hitler having to be involved in the day-to-day running of the country. The government was not a coordinated, co-operating body, but rather a disorganised collection of factions led by members of the party elite who struggled to amass power and gain the Führer's favour. Hitler's leadership style was to give contradictory orders to his subordinates and to place them in positions where their duties and responsibilities overlapped. In this way he fostered distrust, competition, and infighting among his subordinates to consolidate and maximise his own power.
Law.
On 20 August 1934, civil servants were required to swear an oath of unconditional obedience to Hitler; a similar oath had been required of members of the military several weeks prior. This law became the basis of the "Führerprinzip", the concept that Hitler's word overrode all existing laws. Any acts that were sanctioned by Hitler—even murder—thus became legal. All legislation proposed by cabinet ministers had to be approved by the office of Deputy Führer Rudolf Hess, who also had a veto over top civil service appointments.
Most of the judicial system and legal codes of the Weimar Republic remained in use during and after the Third Reich to deal with non-political crimes. The courts issued and carried out far more death sentences than before the Nazis took power. People who were convicted of three or more offences—even petty ones—could be deemed habitual offenders and jailed indefinitely. People such as prostitutes and pickpockets were judged to be inherently criminal and a threat to the racial community. Thousands were arrested and confined indefinitely without trial.
Although the regular courts handled political cases and even issued death sentences for these cases, a new type of court, the "Volksgerichtshof" (People's Court), was established in 1934 to deal with politically important matters. This court handed out over 5,000 death sentences until its dissolution in 1945. The death penalty could be issued for offences such as being a communist, printing seditious leaflets, or even making jokes about Hitler or other top party officials. Nazi Germany employed three types of capital punishment; hanging, decapitation, and death by shooting. The Gestapo was in charge of investigative policing to enforce National Socialist ideology. They located and confined political offenders, Jews, and others deemed undesirable. Political offenders who were released from prison were often immediately re-arrested by the Gestapo and confined in a concentration camp.
In September 1935 the Nuremberg Laws were enacted. These laws initially prohibited sexual relations and marriages between Aryans and Jews and were later extended to include "Gypsies, Negroes or their bastard offspring". The law also forbade the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of "German or related blood" were eligible for citizenship. At the same time the Nazis used propaganda to promulgate the concept of "Rassenschande" (race defilement) to justify the need for a restrictive law. Thus Jews and other non-Aryans were stripped of their German citizenship. The wording of the law also potentially allowed the Nazis to deny citizenship to anyone who was not supportive enough of the regime. A supplementary decree issued in November defined as Jewish anyone with three Jewish grandparents, or two grandparents if the Jewish faith was followed.
Military and paramilitary.
Wehrmacht.
The unified armed forces of Germany from 1935 to 1945 were called the Wehrmacht. This included the "Heer" (army), "Kriegsmarine" (navy), and the "Luftwaffe" (air force). From 2 August 1934, members of the armed forces were required to pledge an oath of unconditional obedience to Hitler personally. In contrast to the previous oath, which required allegiance to the constitution of the country and its lawful establishments, this new oath required members of the military to obey Hitler even if they were being ordered to do something illegal. Hitler decreed that the army would have to tolerate and even offer logistical support to the "Einsatzgruppen"—the mobile death squads responsible for millions of deaths in Eastern Europe—when it was tactically possible to do so. Members of the "Wehrmacht" also participated directly in the Holocaust by shooting civilians or undertaking genocide under the guise of anti-partisan operations. The party line was that the Jews were the instigators of the partisan struggle, and therefore needed to be eliminated. On 8 July 1941, Heydrich announced that all Jews were to be regarded as partisans, and gave the order for all male Jews between the ages of 15 and 45 to be shot.
In spite of efforts to prepare the country militarily, the economy could not sustain a lengthy war of attrition such as had occurred in World War I. A strategy was developed based on the tactic of "Blitzkrieg" (lightning war), which involved using quick coordinated assaults that avoided enemy strong points. Attacks began with artillery bombardment, followed by bombing and strafing runs. Next the tanks would attack and finally the infantry would move in to secure any ground that had been taken. Victories continued through mid-1940, but the failure to defeat Britain was the first major turning point in the war. The decision to attack the Soviet Union and the decisive defeat at Stalingrad led to the retreat of the German armies and the eventual loss of the war. The total number of soldiers who served in the "Wehrmacht" from 1935 to 1945 was around 18.2 million, of whom 5.3 million died.
The SA and SS.
The "Sturmabteilung" (SA; Storm Detachment; Brownshirts), founded in 1921, was the first paramilitary wing of the Nazi Party. Their initial assignment was to protect Nazi leaders at rallies and assemblies. They also took part in street battles against the forces of rival political parties and violent actions against Jews and others. By 1934, under Ernst Röhm's leadership, the SA had grown to over half a million members—4.5 million including reserves—at a time when the regular army was still limited to 100,000 men by the Versailles Treaty.
Röhm hoped to assume command of the army and absorb it into the ranks of the SA. Hindenburg and Defence Minister Werner von Blomberg threatened to impose martial law if the alarming activities of the SA were not curtailed. Hitler also suspected that Röhm was plotting to depose him, so he ordered the deaths of Röhm and other political enemies. Up to 200 people were killed from 30 June to 2 July 1934 in an event that became known as the Night of the Long Knives. After this purge the SA was no longer a major force.
Initially a force of a dozen men under the auspices of the SA, the "Schutzstaffel" (SS) grew to become one of the largest and most powerful groups in Nazi Germany. Led by "Reichsführer-SS" Heinrich Himmler from 1929, the SS had over a quarter million members by 1938 and continued to grow. Himmler envisioned the SS as being an elite group of guards, Hitler's last line of defence. The "Waffen-SS", the military branch of the SS, became a de facto fourth branch of the Wehrmacht.
In 1931 Himmler organised an SS intelligence service which became known as the "Sicherheitsdienst" (SD; Security Service) under his deputy, SS-"Obergruppenführer" Reinhard Heydrich. This organisation was tasked with locating and arresting communists and other political opponents. Himmler hoped it would eventually totally replace the existing police system. Himmler also established the beginnings of a parallel economy under the auspices of the SS Economy and Administration Head Office. This holding company owned housing corporations, factories, and publishing houses.
From 1935 forward the SS was heavily involved in the persecution of Jews, who were rounded up into ghettos and concentration camps. With the outbreak of World War II, SS units called "Einsatzgruppen" followed the army into Poland and the Soviet Union, where from 1941 and 1945 they killed more than two million people, including 1.3 million Jews. The "SS-Totenkopfverbände" (death's head units) were in charge of the concentration camps and extermination camps, where millions more were killed.
Economy.
Reich economics.
The most pressing economic matter the Nazis initially faced was the 30 per cent national unemployment rate. Economist Dr. Hjalmar Schacht, President of the Reichsbank and Minister of Economics, created in May 1933 a scheme for deficit financing. Capital projects were paid for with the issuance of promissory notes called Mefo bills. When the notes were presented for payment, the Reichsbank printed money to do so. While the national debt soared, Hitler and his economic team expected that the upcoming territorial expansion would provide the means of repaying the debt. Schacht's administration achieved a rapid decline in the unemployment rate, the largest of any country during the Great Depression.
On 17 October 1933, aviation pioneer Hugo Junkers, owner of the Junkers Aircraft Works, was arrested. Within a few days his company was expropriated by the regime. In concert with other aircraft manufacturers and under the direction of Aviation Minister Göring, production was immediately ramped up industry-wide. From a workforce of 3,200 people producing 100 units per year in 1932, the industry grew to employ a quarter of a million workers manufacturing over 10,000 technically advanced aircraft per year less than ten years later.
An elaborate bureaucracy was created to regulate German imports of raw materials and finished goods with the intention of eliminating foreign competition in the German marketplace and improving the nation's balance of payments. The Nazis encouraged the development of synthetic replacements for materials such as oil and textiles. As the market was experiencing a glut and prices for petroleum were low, in 1933 the Nazi government made a profit-sharing agreement with IG Farben, guaranteeing them a 5 per cent return on capital invested in their synthetic oil plant at Leuna. Any profits in excess of that amount would be turned over to the Reich. By 1936, Farben regretted making the deal, as the excess profits by then being generated had to be given to the government.
Major public works projects financed with deficit spending included the construction of a network of "Autobahns" and providing funding for programmes initiated by the previous government for housing and agricultural improvements. To stimulate the construction industry, credit was offered to private businesses and subsidies were made available for home purchases and repairs. On the condition that the wife would leave the workforce, a loan of up to 1,000 Reichsmarks could be accessed by young couples of Aryan descent who intended to marry. The amount that had to be repaid was reduced by 25 per cent for each child born. The caveat that the woman had to remain unemployed was dropped by 1937 due to a shortage of skilled labourers.
Hitler envisioned widespread car ownership as part of the new Germany. He arranged for designer Ferdinand Porsche to draw up plans for the "KdF-wagen" (Strength Through Joy car), intended to be an automobile that every German citizen could afford. A prototype was displayed at the International Motor Show in Berlin on 17 February 1939. With the outbreak of World War II, the factory was converted to produce military vehicles. No production models were sold until after the war, when the vehicle was renamed the Volkswagen (people's car).
Six million people were unemployed when the Nazis took power in 1933, and by 1937 there were fewer than a million. This was in part due to the removal of women from the workforce. Real wages dropped by 25 per cent between 1933 and 1938. Trade unions were abolished in May 1933 with the seizure of the funds and arrest of the leadership of the Social Democratic trade unions. A new organisation, the German Labour Front, was created and placed under NSDAP functionary Robert Ley. The average German worked 43 hours a week in 1933, and by 1939 this increased to 47 hours a week.
By early 1934 the focus shifted away from funding work creation schemes and toward rearmament. By 1935, military expenditures accounted for 73 per cent of the government's purchases of goods and services. On 18 October 1936 Hitler named Göring as Plenipotentiary of the Four Year Plan, intended to speed up the rearmament programme. In addition to calling for the rapid construction of steel mills, synthetic rubber plants, and other factories, Göring instituted wage and price controls and restricted the issuance of stock dividends. Large expenditures were made on rearmament, in spite of growing deficits. With the introduction of compulsory military service in 1935, the "Reichswehr", which had been limited to 100,000 by the terms of the Versailles Treaty, expanded to 750,000 on active service at the start of World War II, with a million more in the reserve. By January 1939, unemployment was down to 301,800, and it dropped to only 77,500 by September.
Wartime economy and forced labour.
The Nazi war economy was a mixed economy that combined a free market with central planning; historian Richard Overy described it as being somewhere in between the command economy of the Soviet Union and the capitalist system of the United States.
In 1942, after the death of Armaments Minister Fritz Todt, Hitler appointed Albert Speer as his replacement. Speer improved production via streamlined organisation, the use of single-purpose machines operated by unskilled workers, rationalisation of production methods, and better co-ordination between the many different firms that made tens of thousands of components. Factories were relocated away from rail yards, which were bombing targets. By 1944, the war was consuming 75 per cent of Germany's gross domestic product, compared to 60 per cent in the Soviet Union and 55 per cent in Britain.
The wartime economy relied heavily upon the large-scale employment of forced labourers. Germany imported and enslaved some 12 million people from 20 European countries to work in factories and on farms; approximately 75 per cent were Eastern European. Many were casualties of Allied bombing, as they received poor air raid protection. Poor living conditions led to high rates of sickness, injury, and death, as well as sabotage and criminal activity.
Foreign workers brought into Germany were put into four different classifications; guest workers, military internees, civilian workers, and Eastern workers. Different regulations were placed upon the worker depending on their classification. To separate Germans and foreign workers, the Nazis issued a ban on sexual relations between Germans and foreign workers.
Women played an increasingly large role. By 1944 over a half million served as auxiliaries in the German armed forces, especially in anti-aircraft units of the Luftwaffe; a half million worked in civil aerial defence; and 400,000 were volunteer nurses. They also replaced men in the wartime economy, especially on farms and in small family-owned shops.
Very heavy strategic bombing by the Allies targeted refineries producing synthetic oil and gasoline as well as the German transportation system, especially rail yards and canals. The armaments industry began to break down by September 1944. By November fuel coal was no longer reaching its destinations, and the production of new armaments was no longer possible. Overy argues that the bombing strained the German war economy and forced it to divert up to one-fourth of its manpower and industry into anti-aircraft resources, which very likely shortened the war.
Racial policy.
Racism and antisemitism were basic tenets of the NSDAP and the Nazi regime. Nazi Germany's racial policy was based on their belief in the existence of a superior master race. The Nazis postulated the existence of a racial conflict between the Aryan master race and inferior races, particularly Jews, who were viewed as a mixed race that had infiltrated society and were responsible for the exploitation and repression of the Aryan race.
Persecution of Jews.
Discrimination against Jews began immediately after the seizure of power; following a month-long series of attacks by members of the SA on Jewish businesses, synagogues, and members of the legal profession, on 1 April 1933 Hitler declared a national boycott of Jewish businesses. The Law for the Restoration of the Professional Civil Service, passed on 7 April, forced all non-Aryan civil servants to retire from the legal profession and civil service. Similar legislation soon deprived Jewish members of other professions of their right to practise. On 11 April a decree was promulgated that stated anyone who had even one Jewish parent or grandparent was considered non-Aryan. As part of the drive to remove Jewish influence from cultural life, members of the National Socialist Student League removed from libraries any books considered un-German, and a nationwide book burning was held on 10 May.
Violence and economic pressure were used by the regime to encourage Jews to voluntarily leave the country. Jewish businesses were denied access to markets, forbidden to advertise in newspapers, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks. Many towns posted signs forbidding entry to Jews.
In November 1938, a young Jewish man requested an interview with the German ambassador in Paris. He met with a legation secretary, whom he shot and killed to protest his family's treatment in Germany. This incident provided the pretext for a pogrom the NSDAP incited against the Jews on 9 November 1938. Members of the SA damaged or destroyed synagogues and Jewish property throughout Germany. At least 91 German Jews were killed during this pogrom, later called "Kristallnacht", the Night of Broken Glass. Further restrictions were imposed on Jews in the coming months – they were forbidden to own businesses or work in retail shops, drive cars, go to the cinema, visit the library, or own weapons. Jewish pupils were removed from schools. The Jewish community was fined one billion marks to pay for the damage caused by "Kristallnacht" and told that any money received via insurance claims would be confiscated. By 1939 around 250,000 of Germany's 437,000 Jews emigrated to the United States, Argentina, Great Britain, Palestine, and other countries. Many chose to stay in continental Europe. Emigrants to Palestine were allowed to transfer property there under the terms of the Haavara Agreement, but those moving to other countries had to leave virtually all their property behind, and it was seized by the government.
Persecution of Roma and other groups.
Like the Jews, the Romani people were subjected to persecution from the early days of the regime. As a non-Aryan race, they were forbidden to marry people of German extraction. Romani were shipped to concentration camps starting in 1935 and were killed in large numbers. Action T4 was a programme of systematic murder of the physically and mentally handicapped and patients in psychiatric hospitals that mainly took place from 1939 to 1941 but continued until the end of the war. Initially the victims were shot by the "Einsatzgruppen" and others, but gas chambers were used by the end of 1941. Under the provisions of a law promulgated 14 July 1933, the Nazi regime carried out the compulsory sterilisation of over 400,000 individuals labelled as having hereditary defects. More than half the people sterilised were those considered mentally deficient, which included not only people who scored poorly on intelligence tests, but those who deviated from expected standards of behaviour regarding thrift, sexual behaviour, and cleanliness. Mentally and physically ill people were also targeted. The majority of the victims came from disadvantaged groups such as prostitutes, the poor, the homeless, and criminals. Other groups persecuted and killed included Jehovah's Witnesses, homosexuals, social misfits, and members of the political and religious opposition.
The Holocaust.
Germany's war in the East was based on Hitler's long-standing view that Jews were the great enemy of the German people and that "Lebensraum" was needed for Germany's expansion. Hitler focused his attention on Eastern Europe, aiming to defeat Poland, the Soviet Union and remove or kill the resident Jews and Slavs in the process. After the occupation of Poland, all Jews living in the General Government were confined to ghettos, and those who were physically fit were required to perform compulsory labour. In 1941 Hitler decided to destroy the Polish nation completely. He planned that within 10 to 20 years the section of Poland under German occupation would be cleared of ethnic Poles and resettled by German colonists. About 3.8 to 4 million Poles would remain as slaves, part of a slave labour force of 14 million the Nazis intended to create using citizens of conquered nations in the East.
The "Generalplan Ost" (General Plan for the East) called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. To determine who should be killed, Himmler created the "Volksliste", a system of classification of people deemed to be of German blood. He ordered that those of Germanic descent who refused to be classified as ethnic Germans should be deported to concentration camps, have their children taken away, or be assigned to forced labour. The plan also included the kidnapping of children deemed to have Aryan-Nordic traits, who were presumed to be of German descent. The goal was to implement "Generalplan Ost" after the conquest of the Soviet Union, but when the invasion failed, Hitler had to consider other options. One suggestion was a mass forced deportation of Jews to Poland, Palestine, or Madagascar.
Somewhere around the time of the failed offensive against Moscow in December 1941, Hitler resolved that the Jews of Europe were to be exterminated immediately. Plans for the total eradication of the Jewish population of Europe—eleven million people—were formalised at the Wannsee Conference on 20 January 1942. Some would be worked to death and the rest would be killed in the implementation of "Die Endlösung der Judenfrage" (the Final Solution of the Jewish question). Initially the victims were killed with gas vans or by "Einsatzgruppen" firing squads, but these methods proved impracticable for an operation of this scale. By 1941, killing centres at Auschwitz concentration camp, Sobibor, Treblinka, and other Nazi extermination camps replaced "Einsatzgruppen" as the primary method of mass killing. The total number of Jews murdered during the war is estimated at 5.5 to six million people, including over a million children. Twelve million people were put into forced labour.
German citizens (despite much of the later denial) had access to information about what was happening, as soldiers returning from the occupied territories would report on what they had seen and done. Evans states that most German citizens disapproved of the genocide. Some Polish citizens tried to rescue or hide the remaining Jews, and members of the Polish underground got word to their government in exile in London as to what was happening.
In addition to eliminating Jews, the Nazis also planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists. Together, the Hunger Plan and "Generalplan Ost" would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the democidal deaths of an estimated 19.3 million civilians and prisoners of war.
Oppression of ethnic Poles.
During the German occupation of Poland, 2.7 million ethnic Poles were killed by the Nazis. Polish civilians were subject to forced labour in German industry, internment, wholesale expulsions to make way for German colonists and mass executions. The German authorities engaged in a systematic effort to destroy Polish culture and national identity. During operation AB-Aktion, many university professors and members of the Polish intelligentsia were arrested and executed, or transported to concentration camps. During the war, Poland lost 39 to 45 per cent of its physicians and dentists, 26 to 57 per cent of its lawyers, 15 to 30 per cent of its teachers, 30 to 40 per cent of its scientists and university professors, and 18 to 28 per cent of its clergy. Further, 43 per cent of Poland's educational and research institutions and 14 per cent of its museums had been destroyed.
Mistreatment of Soviet POWs.
During the war between June 1941 and January 1942, the Nazis killed an estimated 2.8 million Soviet prisoners of war. Many starved to death while being held in open-air pens at Auschwitz and elsewhere. The Soviet Union lost 27 million people during the war; less than nine million of these were combat deaths. One in four Soviets were killed or wounded.
Society.
Education.
Antisemitic legislation passed in 1933 led to the removal all of Jewish teachers, professors, and officials from the education system. Most teachers were required to belong to the "Nationalsozialistischer Lehrerbund" (National Socialist Teachers League; NSLB), and university professors were required to join the National Socialist German Lecturers. Teachers had to take an oath of loyalty and obedience to Hitler, and those who failed to show sufficient conformity to party ideals were often reported by students or fellow teachers and dismissed. Lack of funding for salaries led to many teachers leaving the profession. The average class size increased from 37 in 1927 to 43 in 1938 due to the resulting teacher shortage.
Frequent and often contradictory directives were issued by Reich Minister of the Interior Wilhelm Frick, Bernhard Rust of the "Reichserziehungsministerium" (Ministry of Education), and various other agencies regarding content of lessons and acceptable textbooks for use in primary and secondary schools. Books deemed unacceptable to the regime were removed from school libraries. Indoctrination in National Socialist thought was made compulsory in January 1934. Students selected as future members of the party elite were indoctrinated from the age of 12 at Adolf Hitler Schools for primary education and National Political Institutes of Education for secondary education. Detailed National Socialist indoctrination of future holders of elite military rank was undertaken at Order Castles.
Primary and secondary education focused on racial biology, population policy, culture, geography, and especially physical fitness. The curriculum in most subjects, including biology, geography, and even arithmetic, was altered to change the focus to race. Military education became the central component of physical education, and education in physics was oriented toward subjects with military applications, such as ballistics and aerodynamics. Students were required to watch all films prepared by the school division of the Ministry of Public Enlightenment and Propaganda.
At universities, appointments to top posts were the subject of power struggles between the education ministry, the university boards, and the National Socialist German Students' League. In spite of pressure from the League and various government ministries, most university professors did not make changes to their lectures or syllabus during the Nazi period. This was especially true of universities located in predominately Catholic regions. Enrolment at German universities declined from 104,000 students in 1931 to 41,000 in 1939. But enrolment in medical schools rose sharply; Jewish doctors had been forced to leave the profession, so medical graduates had good job prospects. From 1934, university students were required to attend frequent and time-consuming military training sessions run by the SA. First-year students also had to serve six months in a labour camp for the "Reichsarbeitsdienst" (National Labour Service); an additional ten weeks service were required of second-year students.
Oppression of churches.
About 65 per cent of the population of Germany was Protestant when the Nazis seized power in 1933. Under the "Gleichschaltung" process, Hitler attempted to create a unified Protestant Reich Church from Germany's 28 existing Protestant churches, with the ultimate goal of eradication of the churches in Germany. Ludwig Müller, a pro-Nazi, was installed as Reich Bishop, and the German Christians, a pro-Nazi pressure group, gained control of the new church. They objected to the Old Testament because of its Jewish origins, and demanded that converted Jews be barred from their church. Pastor Martin Niemöller responded with the formation of the Confessing Church, from which some clergymen opposed the Nazi regime. When in 1935 the Confessing Church synod protested the Nazi policy on religion, 700 of their pastors were arrested. Müller resigned and Hitler appointed Hanns Kerrl as Minister for Church Affairs, to continue efforts to control Protestantism. In 1936, a Confessing Church envoy protested to Hitler against the religious persecutions and human rights abuses. Hundreds more pastors were arrested. The church continued to resist, and by early 1937 Hitler abandoned his hope of uniting the Protestant churches. The Confessing Church was banned on 1 July 1937. Neimoller was arrested and confined, first in Sachsenhausen concentration camp and then at Dachau. Theological universities were closed and more pastors and theologians were arrested.
Persecution of the Catholic Church in Germany followed the Nazi takeover. Hitler moved quickly to eliminate political catholicism, rounding up functionaries of the Catholic-aligned Bavarian People's Party and Catholic Centre Party, which, along with all other non-Nazi political parties, ceased to exist by July. The "Reichskonkordat" (Reich Concordat) treaty with the Vatican was signed in 1933, amid continuing harassment of the church in Germany. The treaty required the regime to honour the independence of Catholic institutions and prohibited clergy from involvement in politics. Hitler routinely disregarded the Concordat, closing all Catholic institutions whose functions were not strictly religious. Clergy, nuns, and lay leaders were targeted, with thousands of arrests over the ensuing years, often on trumped-up charges of currency smuggling or immorality. Several high profile Catholic lay leaders were targeted in the 1934 Night of the Long Knives assassinations. Most Catholic youth groups refused to dissolve themselves and Hitler Youth leader Baldur von Schirach encouraged members to attack Catholic boys in the streets. Propaganda campaigns claimed the church was corrupt, restrictions were placed on public meetings, and Catholic publications faced censorship. Catholic schools were required to reduce religious instruction and crucifixes were removed from state buildings.
Pope Pius XI had the "Mit brennender Sorge" ("With Burning Concern") Encyclical smuggled into Germany for Passion Sunday 1937 and read from every pulpit. It denounced the systematic hostility of the regime toward the church. In response, Goebbels renewed the regime's crackdown and propaganda against Catholics. Enrolment in denominational schools dropped sharply, and by 1939 all such schools were disbanded or converted to public facilities. Later Catholic protests included the 22 March 1942 pastoral letter by the German bishops on "The Struggle against Christianity and the Church". About 30 per cent of Catholic priests were disciplined by police during the Nazi era. A vast security network spied on the activities of clergy, and priests were frequently denounced, arrested, or sent to concentration camps – many to the dedicated clergy barracks at Dachau. In the areas of Poland annexed in 1940, the Nazis instigated a brutal suppression and systematic dismantling of the Catholic Church.
Health.
Nazi Germany had a strong anti-tobacco movement. Pioneering research by Franz H. Müller in 1939 demonstrated a causal link between tobacco smoking and lung cancer. The Reich Health Office took measures to try to limit smoking, including producing lectures and pamphlets. Smoking was banned in many workplaces, on trains, and among on-duty members of the military. Government agencies also worked to control other carcinogenic substances such as asbestos and pesticides. As part of a general public health campaign, water supplies were cleaned up, lead and mercury were removed from consumer products, and women were urged to undergo regular screenings for breast cancer.
Government-run health care insurance plans were available, but Jews were denied coverage starting in 1933. That same year, Jewish doctors were forbidden to treat government-insured patients. In 1937 Jewish doctors were forbidden to treat non-Jewish patients, and in 1938 their right to practice medicine was removed entirely.
Medical experiments, many of them unscientific, were performed on concentration camp inmates beginning in 1941. The most notorious doctor to perform medical experiments was SS-"Hauptsturmführer" Dr Josef Mengele, camp doctor at Auschwitz. Many of his victims died or were intentionally killed. Concentration camp inmates were made available for purchase by pharmaceutical companies for drug testing and other experiments.
Role of women and family.
Women were a cornerstone of Nazi social policy. The Nazis opposed the feminist movement, claiming that it was the creation of Jewish intellectuals, and instead advocated a patriarchal society in which the German woman would recognise that her "world is her husband, her family, her children, and her home." Soon after the seizure of power, feminist groups were shut down or incorporated into the National Socialist Women's League. This organisation coordinated groups throughout the country to promote motherhood and household activities. Courses were offered on childrearing, sewing, and cooking. The League published the "NS-Frauen-Warte", the only NSDAP-approved women's magazine in Nazi Germany. Despite some propaganda aspects, it was predominantly an ordinary woman's magazine.
Women were encouraged to leave the workforce, and the creation of large families by racially suitable women was promoted through a propaganda campaign. Women received a bronze award—known as the "Ehrenkreuz der Deutschen Mutter" (Cross of Honour of the German Mother)—for giving birth to four children, silver for six, and gold for eight or more. Large families received subsidies to help with their utilities, school fees, and household expenses. Though the measures led to increases in the birth rate, the number of families having four or more children declined by five per cent between 1935 and 1940. Removing women from the workforce did not have the intended effect of freeing up jobs for men. Women were for the most part employed as domestic servants, weavers, or in the food and drink industries—jobs that were not of interest to men. Nazi philosophy prevented large numbers of women from being hired to work in munitions factories in the build-up to the war, so foreign labourers were brought in. After the war started, slave labourers were extensively used. In January 1943 Hitler signed a decree requiring all women under the age of fifty to report for work assignments to help the war effort. Thereafter, women were funnelled into agricultural and industrial jobs. By September 1944, 14.9 million women were working in munitions production.
The Nazi regime discouraged women from seeking higher education. Nazi leaders held conservative views about women and endorsed the idea that rational and theoretical work was alien to a woman's nature since they were considered inherently emotional and instinctive – as such, engaging in academics and careerism would only "divert them from motherhood." The number of women allowed to enrol in universities dropped drastically, as a law passed in April 1933 limited the number of females admitted to university to ten per cent of the number of male attendees. Female enrolment in secondary schools dropped from 437,000 in 1926 to 205,000 in 1937. The number of women enrolled in post-secondary schools dropped from 128,000 in 1933 to 51,000 in 1938. However, with the requirement that men be enlisted into the armed forces during the war, women comprised half of the enrolment in the post-secondary system by 1944.
Women were expected to be strong, healthy, and vital. The sturdy peasant woman who worked the land and bore strong children was considered ideal, and athletic women were praised for being tanned from working outdoors. Organisations were created for the indoctrination of Nazi values. From 25 March 1939, membership in the Hitler Youth became compulsory for all children over the age of ten. The "Jungmädelbund" (Young Girls League) section of the Hitler Youth was for girls age 10 to 14, and the "Bund Deutscher Mädel" (BDM; League of German Girls) was for young women age 14 to 18. The BDM's activities focused on physical education, with activities such as running, long jumping, somersaulting, tightrope walking, marching, and swimming.
The Nazi regime promoted a liberal code of conduct regarding sexual matters, and was sympathetic to women who bore children out of wedlock. Promiscuity increased as the war progressed, with unmarried soldiers often intimately involved with several women simultaneously. The same was the case for married women, who liaised with soldiers, civilians, or slave labourers. Sex was sometimes used as a commodity to obtain, for example, better work from a foreign labourer. Pamphlets enjoined German women to avoid sexual relations with foreign workers as a danger to their blood.
With Hitler's approval, Himmler intended that the new society of the Nazi regime should de-stigmatise illegitimate births, particularly of children fathered by members of the SS, who were vetted for racial purity. His hope was that each SS family would have between four and six children. The "Lebensborn" (Fountain of Life) association, founded by Himmler in 1935, created a series of maternity homes where single mothers could be accommodated during their pregnancies. Both parents were examined for racial suitability before acceptance. The resulting children were often adopted into SS families. The homes were also made available to the wives of SS and NSDAP members, who quickly filled over half the available spots.
Existing laws banning abortion except for medical reasons were strictly enforced by the Nazi regime. The number of abortions declined from 35,000 per year at the start of the 1930s to fewer than 2,000 per year at the end of the decade. In 1935 a law was passed allowing abortions for eugenics reasons.
Environmentalism.
Nazi society had elements supportive of animal rights, and many people were fond of zoos and wildlife. The government took several measures to ensure the protection of animals and the environment. In 1933, the Nazis enacted a stringent animal-protection law that had an impact on what was allowed for medical research. But the law was only loosely enforced. In spite of a ban on vivisection, the Ministry of the Interior readily handed out permits for experiments on animals.
The Reich Forestry Office, under Göring, enforced regulations that required foresters to plant a wide variety of trees to ensure suitable habitat for wildlife. A new Reich Animal Protection Act became law in 1933. The regime enacted the Reich Nature Protection Act in 1935 to protect the natural landscape from excessive economic development. The act allowed for the expropriation of privately owned land to create nature preserves and aided in long-range planning. Perfunctory efforts were made to curb air pollution, but little enforcement of existing legislation was undertaken once the war began.
Culture.
The regime promoted the concept of "Volksgemeinschaft", a national German ethnic community. The goal was to build a classless society based on racial purity and the perceived need to prepare for warfare, conquest, and a struggle against Marxism. The German Labour Front founded the "Kraft durch Freude" (KdF; Strength Through Joy) organisation in 1933. In addition to taking control of tens of thousands of previously privately run recreational clubs, it offered highly regimented holidays and entertainment experiences such as cruises, vacation destinations, and concerts.
The "Reichskulturkammer" (Reich Chamber of Culture) was organised under the control of the Propaganda Ministry in September 1933. Sub-chambers were set up to control various aspects of cultural life, such as films, radio, newspapers, fine arts, music, theatre, and literature. All members of these professions were required to join their respective organisation. Jews and people considered politically unreliable were prevented from working in the arts, and many emigrated. Books and scripts had to be approved by the Propaganda Ministry prior to publication. Standards deteriorated as the regime sought to use cultural outlets exclusively as propaganda media.
Radio became very popular in Germany during the 1930s, with over 70 per cent of households owning a receiver by 1939, more than any other country. Radio station staffs were purged of leftists and others deemed undesirable by July 1933. Propaganda and speeches were typical radio fare immediately after the seizure of power, but as time went on Goebbels insisted that more music be played so that people would not turn to foreign broadcasters for entertainment.
As with other media, newspapers were controlled by the state, with the Reich Press Chamber shutting down or buying newspapers and publishing houses. By 1939 over two-thirds of the newspapers and magazines were directly owned by the Propaganda Ministry. The NSDAP daily newspaper, the "Völkischer Beobachter" (Ethnic Observer), was edited by Alfred Rosenberg, author of "The Myth of the Twentieth Century", a book of racial theories espousing Nordic superiority. Although Goebbels insisted that all newspapers in Germany should publish content uniformly favourable to the regime, publishers still managed to include veiled criticism, for example by editorialising about dictatorships in ancient Rome or Greece. Newspaper readership plummeted, partly because of the decreased quality of the content, and partly because of the surge in popularity of radio. Authors of books left the country in droves, and some wrote material highly critical of the regime while in exile. Goebbels recommended that the remaining authors should concentrate on books themed on Germanic myths and the concept of blood and soil. By the end of 1933 over a thousand books, most of them by Jewish authors or featuring Jewish characters, had been banned by the Nazi regime.
Hitler took a personal interest in architecture, and worked closely with state architects Paul Troost and Albert Speer to create public buildings in a neoclassical style based on Roman architecture. Speer constructed imposing structures such as the Nazi party rally grounds in Nuremberg and a new Reich Chancellery building in Berlin. Hitler's plans for rebuilding Berlin included a gigantic dome based on the Pantheon in Rome and a triumphal arch more than double the height of the Arc de Triomphe in Paris. Neither of these structures were ever built.
Hitler felt that abstract, Dadaist, expressionist, and modern art were decadent, an opinion that became the basis for policy. Many art museum directors lost their posts in 1933 and were replaced by party members. Some 6,500 modern works of art were removed from museums and replaced with works chosen by a Nazi jury. Exhibitions of the rejected pieces, under titles such as "Decadence in Art", were launched in sixteen different cities by 1935. The Degenerate Art Exhibition, organised by Goebbels, ran in Munich from July to November 1937. The exhibition proved wildly popular, attracting over two million visitors.
Composer Richard Strauss was appointed president of the "Reichsmusikkammer" (Reich Music Chamber) on its founding in November 1933. As was the case with other art forms, the Nazis ostracised musicians who were not deemed racially acceptable, and for the most part did not approve of music that was too modern or atonal. Jazz music was singled out as being especially inappropriate, and foreign musicians of this genre left the country or were expelled. Hitler favoured the music of Richard Wagner, especially pieces based on Germanic myths and heroic stories, and attended the Bayreuth Festival each year from 1933.
Movies were popular in Germany in the 1930s and 1940s, with admissions of over a billion people in 1942, 1943, and 1944. By 1934 German regulations restricting currency exports made it impossible for American film makers to take their profits back to America, so the major film studios closed their German branches. Exports of German films plummeted, as their heavily antisemitic content made them impossible to show in other countries. The two largest film companies, Universum Film AG and Tobis, were purchased by the Propaganda Ministry, which by 1939 was producing most German films. The productions were not always overtly propagandistic, but generally had a political subtext and followed party lines regarding themes and content. Scripts were pre-censored.
Leni Riefenstahl's "Triumph of the Will" (1935), documenting the 1934 Nuremberg Rally, and "Olympia" (1938), covering the 1936 Summer Olympics, pioneered techniques of camera movement and editing that influenced later films. New techniques such as telephoto lenses and cameras mounted on tracks were employed. Both films remain controversial, as their aesthetic merit is inseparable from their propagandising of national socialist ideals.
Legacy.
The Allied powers organised war crimes trials, beginning with the Nuremberg trials, held from November 1945 to October 1946, of 23 top Nazi officials. They were charged with four counts—conspiracy to commit crimes, crimes against peace, war crimes, and crimes against humanity—in violation of international laws governing warfare. All but three of the defendants were found guilty; twelve were sentenced to death. The victorious Allies outlawed the NSDAP and its subsidiary organisations. The display or use of Nazi symbolism such as flags, swastikas, or greetings, is illegal in Germany and Austria.
Nazi ideology and the actions taken by the regime are almost universally regarded as gravely immoral. Hitler, Nazism, and the Holocaust have become symbols of evil in the modern world. Interest in Nazi Germany continues in the media and the academic world. Historian Sir Richard J. Evans remarks that the era "exerts an almost universal appeal because its murderous racism stands as a warning to the whole of humanity."
The Nazi era continues to inform how Germans view themselves and their country. Virtually every family suffered losses during the war or has a story to tell. For many years Germans kept quiet about their experiences and felt a sense of communal guilt, even if they were not directly involved in war crimes. Once study of Nazi Germany was introduced into the school curriculum starting in the 1970s, people began researching the experiences of their family members. Study of the era and a willingness to critically examine its mistakes has led to the development of a strong democracy in today's Germany, but with lingering undercurrents of antisemitism and neo-Nazi thought.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="21214" url="http://en.wikipedia.org/wiki?curid=21214" title="Naraoiidae">
Naraoiidae

Naraoiidae is a family, of extinct, soft-shelled trilobite-like arthropods, that belongs to the order Nectaspida. Species included in the Naraoiidae are known from the second half of the Lower Cambrian to the end of the Upper Silurian. The total number of collection sites is limited and distributed over a vast period of time: Maotianshan Shale and Balang Formation (China), Burgess Shale and Bertie Formation (Canada), the Šárka Formation (Czech Republic), Emu Bay Shale (Australia), Idaho and Utah (USA). This is probably due to the rare occurrence of the right circumstances for soft tissue preservation, needed for these non-calcified exoskeletons.
Ecology.
Naraoiids probably were deposit feeders ("Naraoia" and "Pseudonaraoia"), predators or scavengers ("Misszhouia"), living on the sea floor.
Description.
The species of the family "Naraoiidae" are almost flat (dorso-ventrally). The upper (or dorsal) side of the body consists of a non-calcified transversely oval or semi-circular headshield (cephalon), and a circular to long oval tailshield (pygidium) equal to or longer than the cephalon, without any body segments in between. The body is narrowed at the articulation between cephalon and pygidium. The antennas are long and many-segmented. There are no eyes. The 17 to 25 pairs of legs have two branches on a common basis, like trilobites. The outer (dorsal) branches of the limbs (exopods) have flattened side branches (setae) on the shaft (probably acting as gills). The inner branches (or endopods are composed of 6 or 7 segments (or podomeres).
Differences with other Nectaspida.
Naraoiidae lack thoracic segments (or tergites), while the species of the sister family Liwiidae have between 3 and 6 tergites.
Taxonomic history.
The taxonomic placement of the Naraoiidae has long been debated until detailed appendages were uncovered, that showed that "N. compacta" shares biramous legs of very comparable anatomy with trilobites. Some debate is still going on if the parent taxon "Nektaspida" should be included in the Trilobita, or is better placed as a sister group.

</doc>
<doc id="21215" url="http://en.wikipedia.org/wiki?curid=21215" title="Northwest Passage">
Northwest Passage

The Northwest Passage is a sea route through the Arctic Ocean, along the northern coast of North America via waterways through the Canadian Arctic Archipelago, connecting the Atlantic and Pacific Oceans. The various islands of the archipelago are separated from one another and the Canadian mainland by a series of Arctic waterways collectively known as the Northwest Passages or Northwestern Passages. The Parliament of Canada renamed these waterways the "Canadian Northwest Passage" in motion M-387 passed unanimously 2 December 2009.
Sought by explorers for centuries as a possible trade route, it was first navigated by Norwegian explorer Roald Amundsen with a small expedition in 1903–1906. Until 2009, the Arctic pack ice prevented regular marine shipping throughout most of the year. Changes in the pack ice (Arctic shrinkage) caused by climate change have rendered the waterways more navigable. The contested sovereignty claims over the waters may complicate future shipping through the region: the Canadian government considers the Northwestern Passages part of Canadian Internal Waters, but the United States and various European countries maintain they are an international strait and transit passage, allowing free and unencumbered passage. If, as has been claimed, parts of the eastern end of the Passage are barely 15 m deep, the route's viability as a Euro-Asian shipping route is reduced.
Overview.
Before the Little Ice Age, Norwegian Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with the Inuit and people of the Dorset culture who already inhabited the region. Between the end of the 15th century and the 20th century, colonial powers from Europe dispatched explorers in an attempt to discover a commercial sea route north and west around North America. The Northwest Passage represented a new route to the established trading nations of Asia.
England called the hypothetical northern route the "Northwest Passage". The desire to establish such a route motivated much of the European exploration of both coasts of North America. When it became apparent that there was no route through the heart of the continent, attention turned to the possibility of a passage through northern waters. There was a lack of scientific knowledge about conditions; for instance, some people believed that seawater was incapable of freezing. (As late as the mid-18th century, Captain James Cook had reported that Antarctic icebergs had yielded fresh water, seemingly confirming the hypothesis.) Explorers thought that an open water route close to the North Pole must exist. The belief that a route lay to the far north persisted for several centuries and led to numerous expeditions into the Arctic. Many ended in disaster, including that by Sir John Franklin in 1845. In 1906, the Norwegian explorer Roald Amundsen first successfully completed a passage from Greenland to Alaska in the sloop "Gjøa". Since that date, several fortified ships have made the journey.
From east to west, the direction of most early exploration attempts, expeditions entered the passage from the Atlantic Ocean via the Davis Strait and through Baffin Bay. Five to seven routes have been taken through the Canadian Arctic Archipelago, via the McClure Strait, Dease Strait, and the Prince of Wales Strait, but not all of them are suitable for larger ships. From there ships passed through waterways through the Beaufort Sea, Chukchi Sea, and Bering Strait (separating Russia and Alaska), into the Pacific Ocean.
In the 21st century, major changes to the ice pack due to climate change have stirred speculation that the passage may become clear enough of ice to permit safe commercial shipping for at least part of the year. On August 21, 2007, the Northwest Passage became open to ships without the need of an icebreaker. According to Nalan Koc of the Norwegian Polar Institute, this is the first time the Passage has been clear since they began keeping records in 1972. The Northwest Passage opened again on August 25, 2008.
Thawing ocean or melting ice simultaneously opened up the Northwest Passage and the Northeast Passage (and within it, the Northern Sea Route), making it possible to sail around the Arctic ice cap. Compared to 1979, the "Daily Mail" published "Blocked: The Arctic ice, showing as a pink mass in the 1979 picture, links up with northern Canada and Russia." Awaited by shipping companies, this 'historic event' will cut thousands of miles off their routes. Warning, however, that the NASA satellite images indicated the Arctic may have entered a "death spiral" caused by climate change, Professor Mark Serreze, a sea ice specialist at National Snow and Ice Data Center (NSIDC), USA, said: "The passages are open. It's a historic event. We are going to see this more and more as the years go by."
Due to Arctic shrinkage, the Beluga group of Bremen, Germany, sent the first Western commercial vessels through the Northern Sea Route (Northeast Passage) in 2009. However, Canada's Prime Minister Stephen Harper announced that "ships entering the North-West passage should first report to his government."
The first commercial cargo ship to have sailed through the Northwest Passage was the SS "Manhattan" in August 1969.
Routes.
The Northwest Passage has three parts:
Many attempts were made to find a salt water exit west from Hudson Bay, but the Fury and Hecla Strait in the far north is blocked by ice. The eastern entrance and main axis of the northwest passage, the Parry Channel, was found in 1819. The approach from the west through Bering Strait is impractical because of the need to sail around ice near Point Barrow. East of Point Barrow the coast is fairly clear in summer. This area was mapped in pieces from overland in 1821-1839. This leaves the large rectangle north of the coast, south of Parry Channel and east of Baffin Island. This area was mostly mapped in 1848-1854 by ships looking for Franklin's lost expedition. The first crossing was made by Amundsen in 1903-1905. He used a small ship and hugged the coast.
Extent.
The International Hydrographic Organization defines the limits of the Northwestern Passages as follows:
Historical expeditions.
As a result of their westward explorations and their settlement of Greenland, the Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with Inuit groups. The subsequent arrival of the Little Ice Age is thought to have been one of the reasons that European seafaring into the Northwest Passage ceased until the late 15th century.
Strait of Anián.
In 1539, Hernán Cortés commissioned Francisco de Ulloa to sail along the peninsula of Baja California on the western coast of North America. Ulloa concluded that the Gulf of California was the southernmost section of a strait supposedly linking the Pacific with the Gulf of Saint Lawrence. His voyage perpetuated the notion of the Island of California and saw the beginning of a search for the Strait of Anián.
The strait probably took its name from Ania, a Chinese province mentioned in a 1559 edition of Marco Polo's book; it first appears on a map issued by Italian cartographer Giacomo Gastaldi about 1562. Five years later Bolognini Zaltieri issued a map showing a narrow and crooked Strait of Anian separating Asia from the Americas. The strait grew in European imagination as an easy sea lane linking Europe with the residence of Khagan (the Great Khan) in Cathay (northern China).
Cartographers and seamen tried to demonstrate its reality. Sir Francis Drake sought the western entrance in 1579. The Greek pilot Juan de Fuca, sailing from Acapulco (in Mexico) under the flag of the Spanish crown, claimed he had sailed the strait from the Pacific to the North Sea and back in 1592. The Spaniard Bartholomew de Fonte claimed to have sailed from Hudson Bay to the Pacific via the strait in 1640.
Northern Atlantic.
The first recorded attempt to discover the Northwest Passage was the east-west voyage of John Cabot in 1497, sent by Henry VII in search of a direct route to the Orient. In 1524, Charles V sent Estêvão Gomes to find a northern Atlantic passage to the Spice Islands. An English expedition was launched in 1576 by Martin Frobisher, who took three trips west to what is now the Canadian Arctic in order to find the passage. Frobisher Bay, which he first charted, is named after him.
As part of another expedition, in July 1583 Sir Humphrey Gilbert, who had written a treatise on the discovery of the passage and was a backer of Frobisher, claimed the territory of Newfoundland for the English crown. On August 8, 1585, the English explorer John Davis entered Cumberland Sound, Baffin Island.
The major rivers on the east coast were also explored in case they could lead to a transcontinental passage. Jacques Cartier's explorations of the Saint Lawrence River were initiated in hope of finding a way through the continent. Cartier became persuaded that the St. Lawrence was the Passage; when he found the way blocked by rapids at what is now Montreal, he was so certain that these rapids were all that was keeping him from China (in French, "la Chine"), that he named the rapids for China. To this day, they are known as the Lachine Rapids.
In 1609 Henry Hudson sailed up what is now called the Hudson River in search of the Passage; encouraged by the saltiness of the water in the estuary, he reached present-day Albany, New York, before giving up. He later explored the Arctic and Hudson Bay. In 1611, while in James Bay, Hudson's crew mutinied. They set Hudson and his teenage son John, along with seven sick, infirm, or loyal crewmen, adrift in a small open boat. He was never seen again. Cree oral legend reports that the survivors lived and traveled with the Cree for more than a year.
On May 9, 1619, under the auspices of King Christian IV of Denmark-Norway, Jens Munk set out with 65 men and the king's two ships, the "Einhörningen" (Unicorn), a small frigate, and "Lamprenen" (Lamprey), a sloop, which were outfitted under his own supervision. His mission was to discover the Northwest Passage to the Indies and China. Munk penetrated Davis Strait as far north as 69°, found Frobisher Bay, and then spent almost a month fighting his way through Hudson Strait. In September 1619 he found the entrance to Hudson Bay and spent the winter near the mouth of the Churchill River. Cold, famine, and scurvy destroyed so many of his men that only he and two other men survived. With these men, he sailed for home with the "Lamprey" on July 16, 1620, reaching Bergen, Norway, on September 20, 1620.
René-Robert Cavelier, Sieur de La Salle built the sailing ship, "Le Griffon", in his quest to find the Northwest Passage via the upper Great Lakes. "Le Griffon" disappeared in 1679 on the return trip of her maiden voyage. In the spring of 1682, La Salle made his famous voyage down the Mississippi River to the Gulf of Mexico. La Salle led an expedition from France in 1684 to establish a French colony on the Gulf of Mexico. He was murdered by his followers in 1687.
In 1772 Samuel Hearne travelled overland northwest from Hudson Bay to the Arctic Ocean, thereby proving that there was no strait connecting Hudson Bay to the Pacific Ocean.
Northern Pacific.
 Most Northwest Passage expeditions originated in Europe or on the east coast of North America, seeking to traverse the Passage in the westbound direction. Some progress was made in exploring the western reaches of the imagined passage.
In 1728 Vitus Bering, a Danish Navy officer in Russian service, used the strait first discovered by Semyon Dezhnyov in 1648 but later accredited to and named after Bering (the Bering Strait). He concluded by this sailing that North America and Russia were separate land masses. In 1741 with Lieutenant Aleksei Chirikov, he explored seeking further lands beyond Siberia. While they were separated, Chirikov discovered several of the Aleutian Islands while Bering charted the Alaskan region. His ship was wrecked off the Kamchatka Peninsula, as many of his crew were disabled by scurvy.
In 1762, the English trading ship "Octavius" reportedly hazarded the passage from the west but became trapped in sea ice. In 1775, the whaler "Herald" found the "Octavius" adrift near Greenland with the bodies of her crew frozen below decks. Thus the "Octavius" may have earned the distinction of being the first Western sailing ship to make the passage, although the fact that it took 13 years and occurred after the crew was dead somewhat tarnishes this achievement. (The veracity of the "Octavius" story is questionable.)
The Spanish made several voyages to the northwest coast of North America during the late 18th century. Determining whether a Northwest Passage existed was one of the motives for their efforts. Among the voyages that involved careful searches for a Passage included the 1775 and 1779 voyages of Juan Francisco de la Bodega y Quadra. The journal of Francisco Antonio Mourelle, who served as Quadra's second in command in 1775, fell into English hands. It was translated and published in London, stimulating exploration.
Captain James Cook made use of the journal during his explorations of the region. In 1791 Alessandro Malaspina sailed to Yakutat Bay, Alaska, which was rumoured to be a Passage. In 1790 and 1791 Francisco de Eliza led several exploring voyages into the Strait of Juan de Fuca, searching for a possible Northwest Passage and finding the Strait of Georgia. To fully explore this new inland sea, an expedition under Dionisio Alcalá Galiano was sent in 1792. He was explicitly ordered to explore all channels that might turn out to be a Northwest Passage.
Cook and Vancouver.
In 1776 Captain James Cook was dispatched by the Admiralty in Great Britain on an expedition to explore the Passage. A 1745 act, when extended in 1775, promised a £20,000 prize for whoever discovered the passage. Initially the Admiralty had wanted Charles Clerke to lead the expedition, with Cook (in retirement following his exploits in the Pacific) acting as a consultant. However Cook had researched Bering's expeditions, and the Admiralty ultimately placed their faith in the veteran explorer to lead, with Clerke accompanying him.
After journeying through the Pacific, to make an attempt from the west, Cook began at Nootka Sound in April 1778. He headed north along the coastline, charting the lands and searching for the regions sailed by the Russians 40 years previously. The Admiralty's orders had commanded the expedition to ignore all inlets and rivers until they reached a latitude of 65°N. Cook, however, failed to make any progress in sighting a Northwestern Passage.
Various officers on the expedition, including William Bligh, George Vancouver, and John Gore, thought the existence of a route was 'improbable'. Before reaching 65°N they found the coastline pushing them further south, but Gore convinced Cook to sail on into the Cook Inlet in the hope of finding the route. They continued to the limits of the Alaskan peninsula and the start of the 1200 mi chain of Aleutian Islands. Despite reaching 70°N, they encountered nothing but icebergs.
From 1792 to 1794, the Vancouver Expedition (led by George Vancouver who had previously accompanied Cook ) surveyed in detail all the passages from the Northwest Coast. He confirmed that there was no such passage south of the Bering Strait. This conclusion was supported by the evidence of Alexander MacKenzie, who explored the Arctic and Pacific oceans in 1793.
19th century.
In the first half of the 19th century, some parts of the Northwest Passage (north of the Bering Strait) were explored separately by many expeditions, including those by John Ross, Elisha Kent Kane, William Edward Parry, and James Clark Ross; overland expeditions were also led by John Franklin, George Back, Peter Warren Dease, Thomas Simpson, and John Rae. In 1826 Frederick William Beechey explored the north coast of Alaska, discovering Point Barrow.
Sir Robert McClure was credited with the discovery of the Northwest Passage in 1851 when he looked across McClure Strait from Banks Island and viewed Melville Island. However, this strait was not navigable to ships at that time. The only usable route linking the entrances of Lancaster Sound and Dolphin and Union Strait was discovered by John Rae in 1854.
Franklin expedition.
In 1845 a lavishly equipped two-ship expedition led by Sir John Franklin sailed to the Canadian Arctic to chart the last unknown swaths of the Northwest Passage. Confidence was high, as they estimated there was less than 500 km remaining of unexplored Arctic mainland coast. When the ships failed to return, relief expeditions and search parties explored the Canadian Arctic, which resulted in a thorough charting of the region, along with a possible passage. Many artifacts from the expedition were found over the next century and a half, including notes that the ships were ice-locked in 1846 near King William Island, about half way through the passage, and unable to break free. Records showed Franklin died in 1847 and Captain Francis Rawdon Moira Crozier took over command. In 1848 the expedition abandoned the two ships and its members tried to escape south across the tundra by sledge. Although some of the crew may have survived into the early 1850s, no evidence has ever been found of any survivors. In 1853 explorer John Rae was told by local Inuit about the disastrous fate of Franklin's expedition, but his reports were not welcomed in Britain.
Starvation, exposure and scurvy all contributed to the men's deaths. In 1981 Owen Beattie, an anthropologist from the University of Alberta, examined remains from sites associated with the expedition. This led to further investigations and the examination of tissue and bone from the frozen bodies of three seamen, John Torrington, William Braine and John Hartnell, exhumed from the permafrost of Beechey Island. Laboratory tests revealed high concentrations of lead in all three (the expedition carried 8,000 tins of food sealed with a lead-based solder). Another researcher has suggested botulism caused deaths among crew members. New evidence, confirming reports first made by John Rae in 1854 based on Inuit accounts, has shown that the last of the crew resorted to cannibalism of deceased members in an effort to survive.
McClure expedition.
During the search for Franklin, Commander Robert McClure and his crew in HMS "Investigator" traversed the Northwest Passage from west to east in the years 1850 to 1854, partly by ship and partly by sledge. McClure started out from England in December 1849, sailed the Atlantic Ocean south to Cape Horn and entered the Pacific Ocean. He sailed the Pacific north and passed through the Bering Strait, turning east at that point and reaching Banks Island.
McClure's ship was trapped in the ice for three winters near Banks Island, at the western end of Viscount Melville Sound. Finally McClure and his crew—who were by that time dying of starvation—were found by searchers who had travelled by sledge over the ice from a ship of Sir Edward Belcher's expedition. They rescued McClure and his crew, returning with them to Belcher's ships, which had entered the Sound from the east. McClure and his crew returned to England in 1854 on one of Belcher's ships. They were the first people known to circumnavigate the Americas and to discover and transit the Northwest Passage, albeit by ship and by sledge over the ice. (Both McClure and his ship were found by a party from HMS "Resolute", one of Belcher's ships, so his sledge journey was relatively short.)
This was an astonishing feat for that day and age, and McClure was knighted and promoted in rank. (He was made rear-admiral in 1867.) Both he and his crew also shared £10,000 awarded them by the British Parliament. In July 2010 Canadian archaeologists found his ship, HMS "Investigator," fairly intact but sunk about 8 m below the surface.
John Rae.
The expeditions by Franklin and McClure were in the tradition of British exploration: well-funded ship expeditions using modern technology, and usually including British Naval personnel. By contrast, John Rae was an employee of the Hudson's Bay Company, which operated a far-flung trade network and drove exploration of the Canadian North. They adopted a pragmatic approach and tended to be land-based. While Franklin and McClure tried to explore the passage by sea, Rae explored by land. He used dog sleds and techniques of surviving in the environment which he had learned from the native Inuit. The Franklin and McClure expeditions each employed hundreds of personnel and multiple ships. John Rae's expeditions included fewer than ten people and succeeded. Rae was also the explorer with the best safety record, having lost only one man in years of traversing Arctic lands. In 1854, Rae returned to the cities with information from the Inuit about the disastrous fate of the Franklin expedition.
Amundsen expedition.
The first explorer to conquer the Northwest Passage solely by ship was the Norwegian explorer Roald Amundsen. In a three-year journey between 1903 and 1906, Amundsen explored the passage with a crew of six. Amundsen, who had sailed to escape creditors seeking to stop the expedition, completed the voyage in the converted 45 net register tonnage (4500 cuft) herring boat "Gjøa". Gjøa was much smaller than vessels used by other Arctic expeditions and had a shallow draft. Amundsen intended to hug the shore, live off the limited resources of the land and sea through which he was to travel, and had determined that he needed to have a tiny crew to make this work. (Trying to support much larger crews had contributed to the catastrophic failure of John Franklin's expedition fifty years previously). The ship's shallow draft was intended to help her traverse the shoals of the Arctic straits.
Amundsen set out from Kristiania (Oslo) in June 1903 and was west of the Boothia Peninsula by late September. The Gjøa was put into a natural harbour on the south shore of King William Island; by October 3 she was iced in. There the expedition remained for nearly two years, with the expedition members learning from the local Inuit people and undertaking measurements to determine the location of the North Magnetic Pole. The harbour, now known as Gjoa Haven, later developed as the only permanent settlement on the island.
After completing the Northwest Passage portion of this trip and having anchored near Herschel Island, Amundsen skied 800 kilometres to the city of Eagle, Alaska. He sent a telegram announcing his success and skied the return 800 km to rejoin his companions. Although his chosen east–west route, via the Rae Strait, contained young ice and thus was navigable, some of the waterways were extremely shallow (3 ft deep), making the route commercially impractical.
Later expeditions.
The first traversal of the Northwest Passage via dog sled was accomplished by Greenlander Knud Rasmussen while on the Fifth Thule Expedition (1921–1924). Rasmussen and two Greenland Inuit travelled from the Atlantic to the Pacific over the course of 16 months via dog sled.
Canadian RCMP officer Henry Larsen was the second to sail the passage, crossing west to east, leaving Vancouver 23 June 1940 and arriving at Halifax on 11 October 1942. More than once on this trip, he was uncertain whether the "St. Roch," a Royal Canadian Mounted Police "ice-fortified" schooner, would survive the pressures of the sea ice. At one point, Larsen wondered "if we had come this far only to be crushed like a nut on a shoal and then buried by the ice." The ship and all but one of her crew survived the winter on Boothia Peninsula. Each of the men on the trip was awarded a medal by Canada's sovereign, King George VI, in recognition of this notable feat of Arctic navigation.
Later in 1944, Larsen's return trip was far more swift than his first. He made the trip in 86 days to sail back from Halifax, Nova Scotia to Vancouver, British Columbia. He set a record for traversing the route in a single season. The ship, after extensive upgrades, followed a more northerly, partially uncharted route.
On July 1, 1957, the United States Coast Guard Cutter "Storis" departed in company with USCGC "Bramble" and USCGC "Spar" to search for a deep-draft channel through the Arctic Ocean and to collect hydrographic information. Upon her return to Greenland waters, the "Storis" became the first U.S.-registered vessel to have circumnavigated North America. Shortly after her return in late 1957, she was reassigned to her new home port of Kodiak, Alaska.
In 1969, the SS "Manhattan" made the passage, accompanied by the Canadian icebreakers CCGS "John A. Macdonald" and CCGS "Louis S. St-Laurent". The U.S. Coast Guard icebreakers "Northwind" and "Staten Island" also sailed in support of the expedition.
The "Manhattan" was a specially reinforced supertanker sent to test the viability of the passage for the transport of oil. While the "Manhattan" succeeded, the route was deemed not to be cost effective. The United States built the Alaska Pipeline instead.
In June 1977, sailor Willy de Roos left Belgium to attempt the Northwest Passage in his 13.8 m steel yacht "Williwaw". He reached the Bering Strait in September and after a stopover in Victoria, British Columbia, went on to round Cape Horn and sail back to Belgium, thus being the first sailor to circumnavigate the Americas entirely by ship.
In 1981 as part of the Transglobe Expedition, Ranulph Fiennes and Charles R. Burton completed the Northwest Passage. They left Tuktoyaktuk on July 26, 1981, in the 18 ft open Boston Whaler and reached Tanquary Fiord on August 31, 1981. Their journey was the first open boat transit from west to east and covered around 3000 mi, taking a route through Dolphin and Union Strait following the south coast of Victoria and King William islands, north to Resolute Bay via Franklin Strait and Peel Sound, around the south and east coasts of Devon Island, through Hell Gate and across Norwegian Bay to Eureka, Greely Bay and the head of Tanquary Fiord. Once they reached Tanquary Fiord, they had to trek 150 mi via Lake Hazen to Alert before setting up their winter base camp.
In 1984, the commercial passenger vessel MS "Explorer" (which sank in the Antarctic Ocean in 2007) became the first cruise ship to navigate the Northwest Passage.
In July 1986, Jeff MacInnis and Mike Beedell set out on an 18 ft catamaran called "Perception" on a 100-day sail, west to east, through the Northwest Passage. This pair was the first to sail the passage, although they had the benefit of doing so over a couple of summers.
In July 1986, David Scott Cowper set out from England in a 12.8 m lifeboat, the "Mabel El Holland", and survived three Arctic winters in the Northwest Passage before reaching the Bering Strait in August 1989. He continued around the world via the Cape of Good Hope to return to England on September 24, 1990. His was the first vessel to circumnavigate the world via the Northwest Passage.
On July 1, 2000, the Royal Canadian Mounted Police patrol vessel "Nadon", having assumed the name "St Roch II", departed Vancouver on a "Voyage of Rediscovery". Nadon's mission was to circumnavigate North America via the Northwest Passage and the Panama Canal, recreating the epic voyage of her predecessor, "St. Roch." The 22000 mi Voyage of Rediscovery was intended to raise awareness concerning "St. Roch" and kick off the fund-raising efforts necessary to ensure the continued preservation of "St. Roch". The voyage was organized by the Vancouver Maritime Museum and supported by a variety of corporate sponsors and agencies of the Canadian government.
"Nadon" is an aluminum, catamaran-hulled, high-speed patrol vessel. To make the voyage possible, she was escorted and supported by the Canadian Coast Guard icebreaker "Simon Fraser". The Coast Guard vessel was chartered by the Voyage of Rediscovery and crewed by volunteers. Throughout the voyage, she provided a variety of necessary services, including provisions and spares, fuel and water, helicopter facilities, and ice escort; she also conducted oceanographic research during the voyage. The Voyage of Rediscovery was completed in five and a half months, with "Nadon" reaching Vancouver on December 16, 2000.
On September 1, 2001, "Northabout", an 14.3 m aluminium sailboat with diesel engine, built and captained by Jarlath Cunnane, completed the Northwest Passage east-to-west from Ireland to the Bering Strait. The voyage from the Atlantic to the Pacific was completed in 24 days. Cunnane cruised in the "Northabout" in Canada for two years before returning to Ireland in 2005 via the Northeast Passage; he completed the first east-to-west circumnavigation of the pole by a single sailboat. The Northeast Passage return along the coast of Russia was slower, starting in 2004, requiring an ice stop and winter over in Khatanga, Siberia. He returned to Ireland via the Norwegian coast in October 2005. On January 18, 2006, the Cruising Club of America awarded Jarlath Cunnane their Blue Water Medal, an award for "meritorious seamanship and adventure upon the sea displayed by amateur sailors of all nationalities."
On July 18, 2003, a father-and-son team, Richard and Andrew Wood, with Zoe Birchenough, sailed the yacht "Norwegian Blue" into the Bering Strait. Two months later she sailed into the Davis Strait to become the first British yacht to transit the Northwest Passage from west to east. She also became the only British vessel to complete the Northwest Passage in one season, as well as the only British sailing yacht to return from there to British waters.
In 2006 a scheduled cruise liner (the ) successfully ran the Northwest Passage, helped by satellite images telling the location of sea ice.
On May 19, 2007, a French sailor, Sébastien Roubinet, and one other crew member left Anchorage, Alaska, in "Babouche", a 7.5 m ice catamaran designed to sail on water and slide over ice. The goal was to navigate west to east through the Northwest Passage by sail only. Following a journey of more than 7200 km, Roubinet reached Greenland on September 9, 2007, thereby completing the first Northwest Passage voyage made in one season without engine.
In April 2009, planetary scientist Pascal Lee and a team of four on the "Northwest Passage Drive Expedition" drove the "Moon-1" Humvee Rover a record-setting 494 km on sea-ice from Kugluktuk to Cambridge Bay, Nunavut, the longest distance driven on sea-ice in a road vehicle. The "Moon-1" was being ferried to the Haughton-Mars Project Research Station on Devon Island, where it now serves as a simulator of future pressurized rovers to be used by astronauts on the Moon and Mars. The "Moon-1" was flown from Cambridge Bay to Resolute Bay in May 2009, and then driven again on sea-ice by Lee and a team of five from Resolute to Devon Island in May 2010.
In 2009 sea ice conditions were such that at least nine small vessels and two cruise ships completed the transit of the Northwest Passage. These trips included one by Eric Forsyth on board the 42 ft Westsail sailboat "Fiona", a boat he built in the 1980s. Self-financed, Forsyth, a retired engineer from the Brookhaven National Laboratory, and winner of the Cruising Club of America's Blue Water Medal, sailed the Canadian Archipelago with sailor Joey Waits, airline captain Russ Roberts and carpenter David Wilson. After successfully sailing the Passage, the 77-year-old Forsyth completed the circumnavigation of North America, returning to his home port on Long Island, New York.
On August 28, 2010, Bear Grylls and a team of five were the first rigid inflatable boat (RIB) crew to complete a point-to-point navigation between Pond Inlet and Tuktoyaktuk. Note: A Northwest Passage requires crossing the Arctic Circle twice, once each in the Atlantic and the Pacific oceans.
On August 29, 2012 the Swedish yacht "Belzebub," a 31-foot fiberglass cutter captained by Canadian Nicolas Peissel and Swede Edvin Buregren, became the first sailboat in history to sail through McClure Strait, part of a journey of achieving the most northerly Northwest Passage in recorded history. "Belzebub" departed Newfoundland following the coast of Greenland to Qaanaaq before tracking the sea ice to Grise Fiord, Canada's most northern community. From there the team continued through Parry Channel into McClure Strait and the Beaufort Sea, tracking the highest latitudes of 2012's record sea ice depletion before completing their Northwest Passage September 14, 2012. The expedition received extensive media coverage, including recognition by former Vice President Al Gore. The accomplishment is recorded in the Polar Scott Institutes record of Northwest Passage Transits and recognized by the Explorers Club and the Royal Canadian Geographic Society.
At 18:45 GMT September 18, 2012, "Best Explorer", a steel cutter 15.17 m, skipper Nanni Acquarone, passing between the two Diomedes, was the first Italian sailboat to complete the Northwest Passage along the classical Amundsen route. Twenty-two Italian amateur sailors took part of the trip, in eight legs from Tromsø, Norway, to King Cove, Alaska, totalling 8200 nmi.
Setting sail from Nome, Alaska, on August 18, 2012, and reaching Nuuk, Greenland, on September 12, 2012, "The World" became the largest passenger vessel to transit the Northwest Passage. The ship, carrying 481 passengers, for 26 days and 4800 nmi at sea, followed in the path of Captain Roald Amundsen. "The World"‍‍ '​‍s transit of the Northwest Passage was documented by "National Geographic" photographer Raul Touzon.
In September 2013 the became the first commercial bulk carrier to transit the Northwest Passage. She was carrying a cargo of 15,000 tons of coking coal from Port Metro Vancouver, Canada, to the Finnish Port of Pori. The Northwest Passage shortened the distance by 1,000 nautical miles compared to traditional route via the Panama Canal.
International waters dispute.
The Canadian government claims that some of the waters of the Northwest Passage, particularly those in the Canadian Arctic Archipelago, are internal to Canada, giving the nation the right to bar transit through these waters. Most maritime nations, including the United States and those of the European Union, classify these waters as an international strait, where foreign vessels have the right of "transit passage". In such a regime, Canada would have the right to enact fishing and environmental regulation, and fiscal and smuggling laws, as well as laws intended for the safety of shipping, but not the right to close the passage. If the passage’s deep waters become completely ice-free in summer months, they would be particularly enticing for massive supertankers that are too big to pass through the Panama Canal and must otherwise navigate around the tip of South America.
In 1985, the U.S. Coast Guard icebreaker "Polar Sea" passed through from Greenland to Alaska; the ship submitted to inspection by the Canadian Coast Guard before passing through, but the event infuriated the Canadian public and resulted in a diplomatic incident. The United States government, when asked by a Canadian reporter, indicated that they did not ask for permission as they were not legally required to. The Canadian government issued a declaration in 1986 reaffirming Canadian rights to the waters. But, the United States refused to recognize the Canadian claim. In 1988 the governments of Canada and the U.S. signed an agreement, "Arctic Cooperation", that resolved the practical issue without solving the sovereignty questions. Under the law of the sea, ships engaged in transit passage are not permitted to engage in research. The agreement states that all US Coast Guard vessels are engaged in research, and so would require permission from the Government of Canada to pass through.
In late 2005, it was reported that U.S. nuclear submarines had travelled unannounced through Canadian Arctic waters, sparking outrage in Canada. In his first news conference after the 2006 federal election, Prime Minister-designate Stephen Harper contested an earlier statement made by the U.S. ambassador that Arctic waters were international, stating the Canadian government's intention to enforce its sovereignty there. The allegations arose after the U.S. Navy released photographs of the USS "Charlotte" surfaced at the North Pole.
On April 9, 2006, Canada's Joint Task Force (North) declared that the Canadian military will no longer refer to the region as the Northwest Passage, but as the Canadian Internal Waters. The declaration came after the successful completion of Operation Nunalivut (Inuktitut for "the land is ours"), which was an expedition into the region by five military patrols.
In 2006 a report prepared by the staff of the Parliamentary Information and Research Service of Canada suggested that because of the September 11 attacks, the United States might be less interested in pursuing the international waterways claim in the interests of having a more secure North American perimeter. This report was based on an earlier paper, "The Northwest Passage Shipping Channel: Is Canada’s Sovereignty Really Floating Away?" by Andrea Charron, given to the 2004 Canadian Defence and Foreign Affairs Institute Symposium. Later in 2006 former United States Ambassador to Canada, Paul Cellucci agreed with this position; however, the succeeding ambassador, David Wilkins, stated that the Northwest Passage was in international waters.
On July 9, 2007, Prime Minister Harper announced the establishment of a deep-water port in the far North. In the government press release the Prime Minister is quoted as saying, “Canada has a choice when it comes to defending our sovereignty over the Arctic. We either use it or lose it. And make no mistake, this Government intends to use it. Because Canada’s Arctic is central to our national identity as a northern nation. It is part of our history. And it represents the tremendous potential of our future."
On July 10, 2007, Rear Admiral Timothy McGee of the United States Navy, and Rear Admiral Brian Salerno of the United States Coast Guard announced that the United States would be increasing its ability to patrol the Arctic.
Thinning ice cover and the Northwest Passage.
In the summer of 2000, two Canadian ships took advantage of thinning summer ice cover on the Arctic Ocean to make the crossing. It is thought that climate change is likely to open the passage for increasing periods of time, making it attractive as a major shipping route. However the passage through the Arctic Ocean would require significant investment in escort vessels and staging ports. Therefore the Canadian commercial marine transport industry does not anticipate the route as a viable alternative to the Panama Canal even within the next 10 to 20 years.
On September 14, 2007, the European Space Agency stated that ice loss that year had opened up the historically impassable passage, setting a new low of ice cover as seen in satellite measurements which went back to 1978. According to the Arctic Climate Impact Assessment, the latter part of the 20th century and the start of the 21st had seen marked shrinkage of ice cover. The extreme loss in 2007 rendered the passage "fully navigable". However, the ESA study was based only on analysis of satellite images and could in practice not confirm anything about the actual navigation of the waters of the passage. The ESA suggested the passage would be navigable "during reduced ice cover by multi-year ice pack" (namely sea ice surviving one or more summers) where previously any traverse of the route had to be undertaken during favourable seasonable climatic conditions or by specialist vessels or expeditions. The agency's report speculated that the conditions prevalent in 2007 had shown the passage may "open" sooner than expected. An expedition in May 2008 reported that the passage was not yet continuously navigable even by an icebreaker and not yet ice-free.
Scientists at a meeting of the American Geophysical Union on December 13, 2007, revealed that NASA satellites observing the western Arctic showed a 16% decrease in cloud coverage during the summer of 2007 compared to 2006. This would have the effect of allowing more sunlight to penetrate Earth's atmosphere and warm the Arctic Ocean waters, thus melting sea ice and contributing to the opening the Northwest Passage.
In 2006 the cruise liner MS "Bremen" successfully ran the Northwest Passage, helped by satellite images telling where sea ice was.
On November 28, 2008, the Canadian Broadcasting Corporation reported that the Canadian Coast Guard confirmed the first commercial ship sailed through the Northwest Passage. In September 2008, the MV "Camilla Desgagnés", owned by Desgagnés Transarctik Inc. and, along with the Arctic Cooperative, is part of Nunavut Sealift and Supply Incorporated (NSSI), transported cargo from Montreal to the hamlets of Cambridge Bay, Kugluktuk, Gjoa Haven, and Taloyoak. A member of the crew is reported to have claimed that "there was no ice whatsoever". Shipping from the east was to resume in the fall of 2009. Although sealift is an annual feature of the Canadian Arctic this is the first time that the western communities have been serviced from the east. The western portion of the Canadian Arctic is normally supplied by Northern Transportation Company Limited (NTCL) from Hay River. The eastern portion by NNSI and NTCL from Churchill and Montreal.
In January 2010, the ongoing reduction in the Arctic sea ice led telecoms cable specialist Kodiak-Kenai Cable to propose the laying of a fiberoptic cable connecting London and Tokyo, by way of the Northwest Passage, saying the proposed system would nearly cut in half the time it takes to send messages from the United Kingdom to Japan.
In September 2013 the first large sea freighter was able to use the passage.
Transfer of Pacific species to North Atlantic.
Scientists believe that reduced sea ice in the Northwest Passage has permitted some new species to migrate across the Arctic Ocean. The gray whale "Eschrichtius robustus" has not been seen in the Atlantic since it was hunted to extinction there in the 18th century, but in May 2010, one such whale turned up in the Mediterranean. Scientists speculated the whale had followed its food sources through the Northwest Passage and simply kept on going.
The plankton species " seminae" had not been seen in the Atlantic for 800,000 years. Over the past few years, however, it has become increasingly prevalent there. Again, scientists believe that it got there through the reopened Northwest Passage.
In August 2010, two bowhead whales from West Greenland and Alaska, respectively, entered the Northwest Passage from opposite directions and spent approximately 10 days in the same area.
References.
</dl>

</doc>
<doc id="21216" url="http://en.wikipedia.org/wiki?curid=21216" title="Nevada">
Nevada

Nevada is a state in the Western, Mountain West, and Southwestern regions of the United States. Nevada is the 7th most extensive, the 35th most populous, and the 9th least densely populated of the 50 United States. Nearly three-quarters of Nevada's people live in Clark County, which contains the Las Vegas–Paradise metropolitan area where the state's three largest incorporated cities are located. Nevada's capital is Carson City. Nevada is officially known as the "Silver State" due to the importance of silver to its history and economy. It is also known as the "Battle Born State", because it achieved statehood during the Civil War; as the "Sagebrush State", for the native plant of the same name; and as "Sage hen State".
Nevada is largely desert and semiarid, much of it located within the Great Basin. Areas south of the Great Basin are located within the Mojave Desert, while Lake Tahoe and the Sierra Nevada lie on the western edge. About 86% of the state's land is managed by various jurisdictions of the U.S. federal government, both civilian and military.
Before European contact, Native Americans of the Paiute, Shoshone, and Washoe tribes inhabited the land that is now Nevada. The first Europeans to explore the region were Spanish. They called the region "Nevada" (snowy) due to the snow which covered the mountains in winter. The area formed part of the Viceroyalty of New Spain, and became part of Mexico when it gained independence in 1821. The United States annexed the area in 1848 after its victory in the Mexican-American War, and it was incorporated as part of Utah Territory in 1850. The discovery of silver at the Comstock Lode in 1859 led to a population boom that became an impetus to the creation of Nevada Territory out of western Utah Territory in 1861. Nevada became the 36th state on October 31, 1864, as the second of two states added to the Union during the Civil War (the first being West Virginia).
Nevada has a reputation for its libertarian laws. With a population of just over 40,000 people, Nevada was by far the least populated state in 1900, with less than half the population of the next least-populated state. However, legalized gambling and lenient marriage and divorce laws transformed Nevada into a major tourist destination in the 20th century. Nevada is the only U.S. state where prostitution is legal, though it is illegal in Las Vegas (Clark County) and Reno (Washoe County). The tourism industry remains Nevada's largest employer, with mining continuing as a substantial sector of the economy: Nevada is the fourth-largest producer of gold in the world.
Etymology and pronunciation.
The name "Nevada" comes from the Spanish "nevada" ], meaning "snow-covered", after the Sierra Nevada ("snow-covered mountain range").
Nevadans usually pronounce the second syllable of their state name using the /æ/ vowel of "bad". Many from outside the Western United States pronounce it with the /ɑː/ vowel of "father" . Although the latter pronunciation is closer to the Spanish pronunciation, it is not the pronunciation preferred by Nevadans. State Assemblyman Harry Mortenson proposed a bill to recognize the alternate (quasi-Spanish) pronunciation of Nevada, though the bill was not supported by most legislators and never received a vote. The native pronunciation is the de facto official one, since it is the one used by the state legislature. Notably, the state's official tourism organization, stylizes the name of the state as Nevăda, with a breve accent over the "a" indicating the locally preferred pronunciation which is also available as a licence plate design.
Geography.
Nevada is almost entirely within the Basin and Range Province, and is broken up by many north-south mountain ranges. Most of these ranges have endorheic valleys between them, which belies the image portrayed by the term Great Basin.
Much of the northern part of the state is within the Great Basin, a mild desert that experiences hot temperatures in the summer and cold temperatures in the winter. Occasionally, moisture from the Arizona Monsoon will cause summer thunderstorms; Pacific storms may blanket the area with snow. The state's highest recorded temperature was 125 °F in Laughlin (elevation of 605 ft) on June 29, 1994. The coldest recorded temperature was -52 °F set in San Jacinto in 1972, in the northeastern portion of the state.
The Humboldt River crosses the state from east to west across the northern part of the state, draining into the Humboldt Sink near Lovelock. Several rivers drain from the Sierra Nevada eastward, including the Walker, Truckee, and Carson rivers. All of these rivers are endorheic basins, ending in Walker Lake, Pyramid Lake, and the Carson Sink, respectively. However, not all of Nevada is within the Great Basin. Tributaries of the Snake River drain the far north, while the Colorado River, which also forms much of the boundary with Arizona, drains much of southern Nevada.
The mountain ranges, some of which have peaks above 13000 ft, harbor lush forests high above desert plains, creating sky islands for endemic species. The valleys are often no lower in elevation than 3000 ft, while some in central Nevada are above 6000 ft.
The southern third of the state, where the Las Vegas area is situated, is within the Mojave Desert. The area receives less rain in the winter but is closer to the Arizona Monsoon in the summer. The terrain is also lower, mostly below 4000 ft, creating conditions for hot summer days and cool to chilly winter nights (due to temperature inversion).
Nevada and California have by far the longest diagonal line (in respect to the cardinal directions) as a state boundary at just over 400 mi. This line begins in Lake Tahoe nearly 4 mi offshore (in the direction of the boundary), and continues to the Colorado River where the Nevada, California, and Arizona boundaries merge 12 mi southwest of the Laughlin Bridge.
The largest mountain range in the southern portion of the state is the Spring Mountain Range, just west of Las Vegas. The state's lowest point is along the Colorado River, south of Laughlin.
Nevada has 172 mountain summits with 2000 ft of prominence. Nevada ranks second in the US, behind Alaska, and ahead of California, Montana, and Washington. This makes Nevada the "Most Mountainous" state in the country, at least by this measure. 
Climate.
Nevada is the driest state in the United States. It is made up of mostly desert and semiarid climate regions, daytime summer temperatures sometimes may rise as high as 125 °F and nighttime winter temperatures may reach as low as -50 °F. While winters in northern Nevada are long and fairly cold, the winter season in the southern part of the state tends to be of short duration and mild. Most parts of Nevada receive scarce precipitation during the year. Most rain that falls in the state falls on the lee side (east and northeast slopes) of the Sierra Nevada.
The average annual rainfall per year is about 7 in; the wettest parts get around 40 in. Nevada's highest recorded temperature is 125 F at Laughlin on June 29, 1994 and the lowest recorded temperature is -50 F at San Jacinto on January 8, 1937. Nevada's 125 F reading is the third highest temperature recorded in the U.S. just behind Arizona's 128 F reading and California's 134 F reading.
Vegetation.
The vegetation of Nevada is diverse and differs by state area. Nevada contains six biotic zones: alpine, sub-alpine, "Ponderosa Pine", "pinion-juniper", "sagebrush" and "creosotebush".
Counties.
Nevada is divided into political jurisdictions designated as "counties". Carson City is officially a consolidated municipality; however, for many purposes under state law it is considered to be a county. As of 1919 there were 17 counties in the state, ranging from 146 to.
Lake County, one of the original nine counties formed in 1861, was renamed Roop County in 1862. Part of the county became Lassen County, California in 1864. The portion that remained in Nevada was annexed in 1883 by Washoe County.
In 1970 Ormsby County was dissolved and the Consolidated Municipality of Carson City was created by the Legislature in its place co-terminous with the old boundaries of Ormsby County.
Bullfrog County, was formed in 1987 from part of Nye County. After the creation was declared unconstitutional the county was abolished in 1989.
Humboldt county was designated as a county in 1856 by Utah Territorial Legislature and again in 1861 by the new Nevada Legislature.
Clark County is the most populous county in Nevada, accounting for nearly three-quarters of its residents. Las Vegas, Nevada's most populous city, has been the county seat since the county was created. Clark County attracts numerous tourists. An estimated 40 million people have visited Clark County in 2009.
Washoe County is the second most populous county of Nevada. Its county seat is Reno. Washoe County includes the Reno-Sparks metropolitan area.
Lyon County is the third most populous county. It was one of the nine original counties created in 1861. It was named after Nathaniel Lyon, the first Union General to be killed in the Civil War. Its current county seat is Yerington. Its first county seat was established at Dayton on November 29, 1861.
History.
Before 1861.
Francisco Garcés was the first European in the area, Nevada was annexed as a part of the Spanish Empire in the northwestern territory of New Spain. Administratively, the area of Nevada was part of the Commandancy General of the Provincias Internas in the Viceroyalty of New Spain. Nevada became a part of Alta California ("Upper California") province in 1804 when the Californias were split. 
With the Mexican War of Independence won in 1821, the province of Alta California became a territory - not a state - of Mexico, due to the small population. 
Jedediah Smith entered the Las Vegas Valley in 1827, and Peter Skene Ogden traveled the Humboldt River in 1828. As a result of the Mexican–American War and the Treaty of Guadalupe-Hidalgo, Mexico permanently lost Alta California in 1848. The new areas acquired by the United States continued to be administered as territories. As part of the Mexican Cession (1848) and the subsequent California Gold Rush that used Emigrant Trails through the area, the state's area evolved first as part of the Utah Territory, then the Nevada Territory (March 2, 1861; named for the Sierra Nevada).
See History of Utah, History of Las Vegas, and the discovery of the first major U.S. deposit of silver ore in Comstock Lode under Virginia City, Nevada in 1859.
Separation from Utah Territory.
On March 2, 1861, the Nevada Territory separated from the Utah Territory and adopted its current name, shortened from "Sierra Nevada" (Spanish for "snow-covered mountain range").
The 1861 southern boundary is commemorated by Nevada Historical Markers 57 and 58 in Lincoln and Nye counties.
Statehood (1864).
Eight days prior to the presidential election of 1864, Nevada became the 36th state in the union. Statehood was rushed to the date of October 31 to help ensure Abraham Lincoln's reelection on November 8 and post-Civil War Republican dominance in Congress, as Nevada's mining-based economy tied it to the more industrialized Union. As it turned out, however, Lincoln and the Republicans won the election handily, and did not need Nevada's help.
Nevada is one of only two states to significantly expand its borders after admission to the Union. (The other is Missouri, which acquired additional territory in 1837 due to the Platte Purchase.)
In 1866 another part of the western Utah Territory was added to Nevada in the eastern part of the state, setting the current eastern boundary.
Nevada achieved its current southern boundaries on January 18, 1867, when it absorbed the portion of Pah-Ute County in the Arizona Territory west of the Colorado River, essentially all of present day Nevada south of the 37th parallel. The transfer was prompted by the discovery of gold in the area, and it was thought by officials that Nevada would be better able to oversee the expected population boom. This area includes most of what is now Clark County.
Mining shaped Nevada's economy for many years (see "Silver mining in Nevada"). When Mark Twain lived in Nevada during the period described in "Roughing It", mining had led to an industry of speculation and immense wealth. However, both mining and population declined in the late 19th century. However, the rich silver strike at Tonopah in 1900, followed by strikes in Goldfield and Rhyolite, again put Nevada's population on an upward trend.
Gambling and labor.
Unregulated gambling was commonplace in the early Nevada mining towns but was outlawed in 1909 as part of a nationwide anti-gambling crusade. Because of subsequent declines in mining output and the decline of the agricultural sector during the Great Depression, Nevada again legalized gambling on March 19, 1931, with approval from the legislature. Governor Fred B. Balzar's signature enacted the most liberal divorce laws in the country and open gambling. The reforms came just eight days after the federal government presented the $49 million construction contract for Boulder Dam (now Hoover Dam).
Nuclear testing.
The Nevada Test Site, 65 mi northwest of the city of Las Vegas, was founded on January 11, 1951, for the testing of nuclear weapons. The site consists of about 1350 sqmi of desert and mountainous terrain. Nuclear testing at the Nevada Test Site began with a 1 ktonTNT bomb dropped on Frenchman Flat on January 27, 1951. The last atmospheric test was conducted on July 17, 1962, and the underground testing of weapons continued until September 23, 1992. The location is known for having the highest concentration of nuclear-detonated weapons in the U.S.
Over 80% of the state's area is owned by the federal government. The primary reason for this is that homesteads were not permitted in large enough sizes to be viable in the arid conditions that prevail throughout desert Nevada. Instead, early settlers would homestead land surrounding a water source, and then graze livestock on the adjacent public land, which is useless for agriculture without access to water (this pattern of ranching still prevails).
Demographics.
Population.
The United States Census Bureau estimates that the population of Nevada was 2,839,099 on July 1, 2014, a 5.13% increase since the 2010 United States Census.
According to the Census Bureau's 2014 estimate, Nevada had an estimated population of 2,839,099 which is an increase of 48,963, from the prior year and an increase of 138,548, or 5.13%, since the year 2010. This includes a natural increase since the last census of 81,661 people (that is 170,451 births minus 88,790 deaths) and an increase due to net migration of 337,043 people into the state. Immigration resulted in a net increase of 66,098 people, and migration within the country produced a net increase of 270,945 people. According to the 2006 census estimate, Nevada is the eighth fastest growing state in the nation.
The center of population of Nevada is located in southern Nye County. In this county, the unincorporated town of Pahrump, located 60 mi west of Las Vegas on the California state line, has grown very rapidly from 1980 to 2010. At the 2010 census, the town had 36,441 residents. Las Vegas was America's fastest-growing city and metropolitan area from 1960 to 2000, but has grown from a gulch of 100 people in 1900 to 10,000 by 1950 to 100,000 by 1970.
From about the 1940s until 2003, Nevada was the fastest-growing state in the US percentage-wise. Between 1990 and 2000, Nevada's population increased 66%, while the USA's population increased 13%. Over two thirds of the population of the state lives in the Clark County Las Vegas metropolitan area.
Henderson and North Las Vegas are among the USA's top 20 fastest-growing cities of over 100,000.
The rural community of Mesquite located 65 mi northeast of Las Vegas was an example of micropolitan growth in the 1990s and 2000s. Other desert towns like Indian Springs and Searchlight on the outskirts of Las Vegas have seen some growth as well.
Large numbers of new residents in the state originate from California, which led some locals to feel that their state is being "Californicated".
Rural areas.
A small percentage of Nevada's population lives in rural areas. The culture of these places differs significantly from that of the major metropolitan areas. People in these rural counties tend to be native Nevada residents, unlike in the Las Vegas and Reno areas, where the vast majority of the population was born in another state. The rural population is also less diverse in terms of race and ethnicity. Mining plays an important role in the economies of the rural counties, with tourism being less prominent. Ranching also has a long tradition in rural Nevada.
Races.
According to the 2010 census estimates, racial distribution was as follows:
Hispanics or Latinos of any race made 26.5% of the population. In 1970, non-Hispanic whites made up 88% of the state's population.
The principal ancestries of Nevada's residents in 2009 have been surveyed to be the following:
Nevada is home to many cultures and nationalities. As of 2011, 63.6% of Nevada's population younger than age 1 were minorities. Las Vegas is minority majority city . Nevada also has a sizable Basque ancestry population. In Douglas, Mineral and Pershing counties, a plurality of residents are of Mexican ancestry, with Clark County (Las Vegas) alone being home to over 200,000 Mexican Americans. Nye County and Humboldt County have a plurality of Germans; and Washoe County has many Irish Americans. Americans of English descent form pluralities in Lincoln County, Churchill County, Lyon County, White Pine County and Eureka County. Las Vegas is home to rapid-growing ethnic communities, including Scandinavians, Italians, Poles, Greeks, Spaniards and Armenians.
Largely African American sections of Las Vegas ("the Meadows") and Reno can be found. Many current African-American Nevadans are newly transplanted residents from California.
Asian Americans lived in the state since the California Gold Rush of the 1850s brought thousands of Chinese miners to Washoe county. They were followed by a few hundred Japanese farm workers in the late 19th century. By the late 20th century, many immigrants from China, Japan, Korea, the Philippines and recently from India and Vietnam came to the Las Vegas metropolitan area. The city now has one of America's most prolific Asian American communities, with a mostly Chinese and Taiwanese area known as "Chinatown" west of I-15 on Spring Mountain Boulevard, and an "Asiatown" shopping mall for Asian customers located at Charleston Avenue and Paradise Boulevard. Filipino Americans form the largest Asian American group in the state, with a population of more than 113,000. They comprise 56.5% of the Asian American population in Nevada and constitute about 4.3% of the entire state's population.
According to the 2000 US Census, 16.19% of Nevada's population aged 5 and older speak Spanish at home, while 1.59% speak Filipino, and 1% speak Chinese languages.
At the 2010 census, 6.9% of the state's population were reported as under 5, 24.6% were under 18, and 12.0% were 65 or older. Females made up about 49.5% of the population.
Las Vegas was a major destination for immigrants from South Asia and Latin America seeking employment in the gaming and hospitality industries during the 1990s and first decade of the 21st century, but farming and construction are the biggest employers of immigrant labor.
Senior citizens (over age 65) and young children or teenagers (under age 18) form large sections of the Nevada population. The religious makeup of Nevadans includes large communities of Mormons, Roman Catholics and Evangelicals; each is known for higher birth rates and a younger than national average age. American Jews represent a large proportion of the active adult retirement community.
Data from 2000 and 2005 suggests the following figures:
Religion.
Church attendance in Nevada is among the lowest of all US states. In a 2009 Gallup poll only 30% of Nevadans said they attended church weekly or almost weekly, compared to 42% of all Americans (only four states were found to have a lower attendance rate than Nevada).
Major religious affiliations of the people of Nevada are: Roman Catholic 27%, Protestant 26%, Latter-day Saint 11%, Muslim 2%, Jewish 1%, Hindu 1%, and Buddhist 0.5%. The unaffiliated are at 20%. Parts of Nevada (in the eastern parts of the state) are situated in the Mormon Corridor.
The largest denominations by number of adherents in 2010 were the Roman Catholic Church with 451,070; The Church of Jesus Christ of Latter-day Saints with 175,149; and the Southern Baptist Convention with 45,535; Buddhist congregations 14,727; Bahá'í 1,723; and Muslim 1,700.
Economy.
The economy of Nevada is tied to tourism (especially entertainment and gambling related), mining, and cattle ranching. Nevada's industrial outputs are tourism, mining, machinery, printing and publishing, food processing, and electric equipment. The Bureau of Economic Analysis estimates that Nevada's total state product in 2010 was $126 billion. The state's per capita personal income in 2009 was $38,578, ranking nineteenth in the nation. Nevada's state debt in 2012 was calculated to be $7.5 billion, or $3,100 per taxpayer. As of December 2014, the state's unemployment rate was 6.8%.
Entertainment and tourism.
The economy of Nevada has long been tied to vice industries. "[Nevada was] founded on mining and refounded on sin—-beginning with prizefighting and easy divorce a century ago and later extending to gaming and prostitution", said the August 21, 2010 issue of "The Economist".
Resort areas like Las Vegas, Reno, Lake Tahoe, and Laughlin attract visitors from around the nation and world. In FY08 the total of 266 casinos with gaming revenue over $1m for the year, brought in revenue of $12 billion in gaming revenue, and $13 billion in non-gaming revenue. A review of gaming statistics can be found at Nevada gaming area.
Nevada has by far the most hotel rooms per capita in the United States. According to the American Hotel and Lodging Association, there were 187,301 rooms in 584 hotels (of 15 or more rooms). The state is ranked just below California, Texas, Florida, and New York in total number of rooms, but those states have much larger populations. Nevada has one hotel room for every 14 residents, far above the national average of one hotel room per 67 residents.
Prostitution is legal in parts of Nevada in licensed brothels, but only counties with populations under 400,000 residents have the option to legalize it. Although prostitution employs roughly 300 women as independent contractors, and not a major part of the Nevada economy, it is a very visible endeavor. Of the 14 counties that are permitted to legalize prostitution under state law, 8 have chosen to legalize brothels. State law prohibits prostitution in Clark County (which contains Las Vegas), and Washoe County (which contains Reno). However, prostitution is legal in Storey County, which is part of the Reno–Sparks metropolitan area.
Mining.
In portions of the state outside of the Las Vegas and Reno metropolitan areas mining plays a major economic role. By value, gold is by far the most important mineral mined. In 2004, 6800000 oz of gold worth $2.84 billion were mined in Nevada, and the state accounted for 8.7% of world gold production (see "Gold mining in Nevada"). Silver is a distant second, with 10300000 oz worth $69 million mined in 2004 (see "Silver mining in Nevada"). Other minerals mined in Nevada include construction aggregates, copper, gypsum, diatomite and lithium. Despite its rich deposits, the cost of mining in Nevada is generally high, and output is very sensitive to world commodity prices.
Cattle ranching.
Cattle ranching is a major economic activity in rural Nevada. Nevada's agricultural outputs are cattle, hay, alfalfa, dairy products, onions, and potatoes. As of January 1, 2006, there were an estimated 500,000 head of cattle and 70,000 head of sheep in Nevada. Most of these animals forage on rangeland in the summer, with supplemental feed in the winter. Calves are generally shipped to out-of-state feedlots in the fall to be fattened for market. Over 90% of Nevada's 484000 acre of cropland is used to grow hay, mostly alfalfa, for livestock feed.
Taxation.
Nevada does not have a state income tax.
The state sales tax(similar to VAT or GST) in Nevada is variable depending upon the county. The minimum statewide tax rate is 6.85%, with five counties (Elko, Esmeralda, Eureka, Humboldt, and Mineral) charging this minimum amount. All other counties assess various option taxes, making the combined state/county sales taxes rate in one county as high as 8.1%, which is the amount charged in Clark County. Sales tax in the other major counties: Carson at 7.745%, Washoe at 7.725%. The minimum Nevada sales tax rate changed on July 1, 2009.
Largest employers.
The largest employers in the state, as of the first fiscal quarter of 2011, are the following, according to the Nevada Department of Employment, Training and Rehabilitation:
Transportation.
Amtrak's "California Zephyr" train uses the Union Pacific's original transcontinental railroad line in daily service from Chicago to Emeryville, California, serving Elko, Winnemucca, and Reno. Amtrak Thruway Motorcoaches also provide connecting service from Las Vegas to trains at Needles, California, Los Angeles, and Bakersfield, California; and from Stateline, Nevada, to Sacramento, California. Las Vegas has had no passenger train service since Amtrak's Desert Wind was discontinued in 1997, although there have been a number of proposals to re-introduce service to either Los Angeles or Southern California.
The Union Pacific Railroad has some railroads in the north and in the south. Greyhound Lines provides some bus service.
Interstate 15 passes through the southern tip of the state, serving Las Vegas and other communities. I-215 and spur route I-515 also serve the Las Vegas metropolitan area. Interstate 80 crosses through the northern part of Nevada, roughly following the path of the Humboldt River from Utah in the east and passing westward through Reno and into California. It has a spur route, I-580. Nevada also is served by several federal highways: US 6, US 50, US 93, US 95 and US 395. There are also 189 Nevada state highways. Many of Nevada's counties have a system of county routes as well, though many are not signed or paved in rural areas. Nevada is one of a few states in the U.S. that does not have a continuous interstate highway linking its two major population centers. Even the non-interstate federal highways aren't contiguous between the Las Vegas and Reno areas.
The state is one of just a few in the country to allow semi-trailer trucks with three trailers—what might be called a "road train" in Australia. But American versions are usually smaller, in part because they must ascend and descend some fairly steep mountain passes.
RTC Transit is the public transit system in the Las Vegas metropolitan area. The agency is the largest transit agency in the state and operates a network of bus service across the Las Vegas Valley, including the use of The Deuce, double-decker buses, on the Las Vegas Strip and several outlying routes. RTC RIDE operates a system of local transit bus service throughout the Reno-Sparks metropolitan area. Other transit systems in the state include Carson City's JAC. Most other counties in the state do not have public transportation at all.
Additionally, a 4 mi monorail system provides public transportation in the Las Vegas area. The Las Vegas Monorail line services several casino properties and the Las Vegas Convention Center on the east side of the Las Vegas Strip, running near Paradise Road, with a possible future extension to McCarran International Airport. Several hotels also run their own monorail lines between each other, which are typically several blocks in length.
McCarran International Airport in Las Vegas is the busiest airport serving Nevada. The Reno-Tahoe International Airport (formerly known as the Reno Cannon International Airport) is the other major airport in the state.
Law and government.
Government.
The government of Nevada is defined under the Constitution of Nevada as a democratic republic with three branches of government: the executive branch consisting of the Governor of Nevada and their cabinet along with the other elected constitutional officers; the legislative branch consisting of the Nevada Legislature which includes the Assembly and the Senate; and the judicial branch consisting of the Supreme Court of Nevada and lower courts.
The Governor of Nevada is the chief magistrate of Nevada, the head of the executive department of the state's government, and the commander-in-chief of the state's military forces. The current Governor of Nevada is Brian Sandoval, a Republican.
The Nevada Legislature is a bicameral body divided into an Assembly and Senate. Members of the Assembly serve for 2 years, and members of the Senate serve for 4 years. Both houses of the Nevada Legislature will be impacted by term limits starting in 2010, as Senators and Assemblymen/women will be limited to a maximum of 12 years service in each house (by appointment or election which is a lifetime limit)—a provision of the constitution which was recently upheld by the Supreme Court of Nevada in a unanimous decision. Each session of the Legislature meets for a constitutionally mandated 120 days in every odd-numbered year, or longer if the Governor calls a special session.
The Supreme Court of Nevada is the state supreme court. Original jurisdiction is divided between the District Courts (with general jurisdiction), and Justice Courts and Municipal Courts (both of limited jurisdiction).
Incorporated towns in Nevada, known as cities, are given the authority to legislate anything not prohibited by law. A recent movement has begun to permit home rule in incorporated Nevada cities to give them more flexibility and fewer restrictions from the Legislature. Town Boards for unincorporated towns are limited local governments created by either the local county commission, or by referendum, and form a purely advisory role and in no way diminish the responsibilities of the county commission that creates them.
State agencies.
State departments and agencies:
Law.
In 1900, Nevada's population was the smallest of all states and was shrinking, as the difficulties of living in a "barren desert" began to outweigh the lure of silver for many early settlers. Historian Lawrence Friedman has explained what happened next:
With the advent of air conditioning for summertime use and Southern Nevada's mild winters, the fortunes of the state began to turn around, as it did for Arizona, making these two states the fastest growing in the Union.
Prostitution.
Nevada is the only state where prostitution is legal (under the form of licensed brothels).
Prostitution is specifically illegal by state law in the state's larger jurisdictions, which include Clark County (which contains Las Vegas), Washoe County (which contains Reno), and the independent city of Carson City. Otherwise, it is legal in those counties which specifically vote to permit it.
Divorce.
Nevada's early reputation as a "divorce haven" arose from the fact that, prior to the no-fault divorce revolution in the 1970s, divorces were quite difficult to obtain in the United States. Already having legalized gaming and prostitution, Nevada continued the trend of boosting its profile by adopting one of the most liberal divorce statutes in the nation. This resulted in "Williams v. North Carolina (1942)", 317 U.S. (1942), in which the U.S. Supreme Court ruled that North Carolina had to give "full faith and credit" to a Nevada divorce.
Nevada's divorce rate tops the national average.
Taxes.
Nevada's tax laws are intended to draw new residents and businesses to the state. Nevada has no personal income tax or corporate income tax. Since Nevada does not collect income data it cannot share such information with the federal government, the IRS.
Nevada's state sales tax rate is 6.85 percent. Counties may impose additional rates via voter approval or through approval of the Legislature; therefore, the applicable sales tax will vary by county from 6.85 percent to 8.1 percent in Clark County.
Clark County, which includes Las Vegas, imposes four separate county option taxes in addition to the statewide rate – 0.25 percent for flood control, 0.50 percent for mass transit, 0.25 percent for infrastructure, and 0.25 percent for more cops. In Washoe County, which includes Reno, the sales tax rate is 7.725 percent, due to county option rates for flood control, the ReTRAC train trench project, mass transit, and an additional county rate approved under the Local Government Tax Act of 1991.
The lodging tax rate in unincorporated Clark County, which includes the Las Vegas Strip, is 12%. Within the boundaries of the cities of Las Vegas and Henderson, the lodging tax rate is 13%.
Corporations such as Apple Inc. allegedly have set up investment companies and funds in Nevada to avoid paying taxes.
Gay rights.
In 2009, the Nevada Legislature passed a bill creating a domestic partnership registry that enables gay couples to enjoy the same rights as married couples.
Incorporation.
Nevada provides friendly environment for the formation of corporations, and many (especially California) businesses have incorporated in Nevada to take advantage of the benefits of the Nevada statute. Nevada corporations offer great flexibility to the Board of Directors and simplify or avoid many of the rules that are cumbersome to business managers in some other states. In addition, Nevada has no franchise tax, although it does require businesses to have a license for which the business has to pay the state.
Financial institutions.
Similarly, many U.S. states have usury laws limiting the amount of interest a lender can charge, but federal law allows corporations to 'import' these laws from their home state.
Alcohol and other drugs.
Non-alcohol drug laws are a notable exception to Nevada's otherwise libertarian principles. It is notable for having the harshest penalties for drug offenders in the country. Nevada remains the only state to still use mandatory minimum sentencing guidelines for marijuana possession. However, it is now a misdemeanor for possession of less than one ounce but only for persons age 21 and older. In 2006, voters in Nevada defeated attempts to allow possession of 1 ounce of marijuana (for personal use) without being criminally prosecuted, (55% against legalization, 45% in favor of legalization). However, Nevada is one of the states that allows for use of marijuana for medical reasons (though this remains illegal under federal law).
Nevada has very liberal alcohol laws. Bars are permitted to remain open 24 hours, with no "last call". Liquor stores, convenience stores and supermarkets may also sell alcohol 24 hours per day, and may sell beer, wine and spirits.
Smoking.
Nevada voters enacted a smoking ban ("The Nevada Clean Indoor Air Act") in November 2006 that became effective on December 8, 2006. It outlaws smoking in most workplaces and public places. Smoking is permitted in bars, but only if the bar serves no food, or the bar is inside a larger casino. Smoking is also permitted in casinos, certain hotel rooms, tobacco shops, and brothels. However, some businesses do not obey this law and the government tends not to enforce it. In 2011, smoking restrictions in Nevada were loosened for certain places which allow only people age 21 or older inside.
Crime.
In 2006, the crime rate in Nevada was about 24% higher than the national average rate, though crime has since decreased. Property crimes accounted for about 85% of the total crime rate in Nevada, which was 21% higher than the national rate. The remaining 20.3% were violent crimes. A complete listing of crime data in the state for 2013 can be found here:
Politics.
State politics.
Due to heavy growth in the southern portion of the state, there is a noticeable divide between politics of northern and southern Nevada. The north has long maintained control of key positions in state government, even while the population of southern Nevada is larger than the rest of the state combined. The north sees the high population south becoming more influential and perhaps commanding majority rule. The south sees the north as the "old guard" trying to rule as an oligarchy. This has fostered some resentment, however, due to a term limit amendment passed by Nevada voters in 1994, and again in 1996, some of the north's hold over key positions will soon be forfeited to the south, leaving Northern Nevada with less power.
Historically, northern Nevada has been very Republican. The more rural counties of the north are among the most conservative regions of the country. Washoe County, home to Reno, has historically been strongly Republican, but has become more of a swing county at least at the federal level. Clark County, home to Las Vegas, has become increasingly Democratic.
Clark and Washoe counties have long dominated the state's politics. Between them, they cast 87 percent of Nevada's vote, and elect a substantial majority of the state legislature. The great majority of the state's elected officials are either from Las Vegas or Reno.
An August 2012 Public Policy Polling survey found that 47% of Nevada voters supported legalizing same-sex marriage, with 42% thinking it should be illegal, and 11% were not sure. In a separate question, 80% of Nevada voters supported legal recognition of same-sex couples, while 17% opposed all legal recognition and 3% were not sure.
National politics.
Nevada has voted for the winner in every presidential election since 1912, except in 1976 when it voted for Gerald Ford over Jimmy Carter. This includes Nevada supporting Democrat Bill Clinton in 1992 and 1996, Republican George W. Bush in 2000 and 2004, and Democrat Barack Obama winning the state in both 2008 and 2012. This gives the state status as a political bellwether. Since 1912, Nevada has been carried by the presidential victor the most out of any state (25 of 26 elections). Nevada was one of only three states won by John F. Kennedy in the American West in the election of 1960, albeit narrowly.
The state's U.S. Senators are Democrat Harry Reid, the Senate Minority Leader, and Republican Dean Heller. The Governorship is held by Brian Sandoval, a Republican from Las Vegas.
Education.
Education in Nevada is achieved through public and private elementary, middle, and high schools, as well as colleges and universities.
Parks and recreation areas.
Wilderness.
There are 68 designated wilderness areas in Nevada, protecting some 6579014 acre under the jurisdiction of the National Park Service, U.S. Forest Service, and Bureau of Land Management.
State parks.
The Nevada state parks comprise protected areas managed by the state of Nevada, including state parks, state historic sites, and state recreation areas. There are currently 24 state park units, including Van Sickle Bi-State Park which opened in July 2011 and is operated in partnership with the state of California.
Sports.
Nevada is not well known for its professional sports teams, but the state takes pride in college sports, most notably its college football. Especially the Nevada Wolf Pack (representing the University of Nevada, Reno) of the Mountain West Conference (MW)—and the UNLV Rebels (representing the University of Nevada, Las Vegas), also of the MW. In 2012, The University of Nevada, Reno joined its cross-state rival in the MW.
UNLV is most remembered for its men's basketball program, which experienced its height of supremacy in the late 1980s and early 1990s. Coached by Jerry Tarkanian, the Runnin' Rebels became one of the most elite programs in the country. In 1990, UNLV won the Men's Division I Championship by defeating Duke 103–73, which set tournament records for most points scored by a team and largest margin of victory in the national title game.
In 1991, UNLV finished the regular season undefeated, a feat that would not be matched in Division I men's basketball for more than 20 years. Forward Larry Johnson won several awards, including the Naismith Award. UNLV reached the Final Four yet again, but lost their national semifinal against Duke 79–77. The Runnin' Rebels were the Associated Press pre-season No. 1 back to back (1989–90, 1990–91). North Carolina is the only other team to accomplish that (2007–08, 2008–09).
Las Vegas has hosted several professional boxing matches, most recently at the MGM Grand Garden Arena with bouts such as Mike Tyson vs. Evander Holyfield, Evander Holyfield vs. Mike Tyson II, Oscar De La Hoya vs. Floyd Mayweather and Oscar De La Hoya vs. Manny Pacquiao.
Along with significant rises in popularity in mixed martial arts (MMA), a number of fight leagues such as the UFC have taken interest in Las Vegas as a primary event location due to the number of suitable host venues. The Mandalay Bay Events Center and MGM Grand Garden Arena are among some of the more popular venues for fighting events such as MMA and have hosted several UFC and other MMA title fights. The city has held the most UFC events with 86 events.
The state is also home to the Las Vegas Motor Speedway, which hosts the Kobalt Tools 400. The Thomas & Mack Center, home to UNLV men's basketball, also hosts two major rodeo events—the National Finals Rodeo and the PBR World Finals, the latter operated by the bull riding-only Professional Bull Riders. Finally, Sam Boyd Stadium, home to the UNLV football team, also hosts the country's biggest rugby event, the USA Sevens tournament in the IRB Sevens World Series and the AMA Supercross Championship.
The state is also home to one of the most famous tennis players of all time, Andre Agassi.
Nevada sports teams.
Professional
College
The Nevada Aerospace Hall of Fame provides educational resources and promotes the aerospace and aviation history of the state.
Military.
Several United States Navy ships have been named USS "Nevada" in honor of the state. They include:
Area 51 is located near Groom Lake, a dry salt lake bed. The much smaller Creech Air Force Base is located in Indian Springs, Nevada; Hawthorne Army Depot in Hawthorne; the Tonopah Test Range near Tonopah; and Nellis AFB in the northeast part of the Las Vegas Valley. Naval Air Station Fallon in Fallon; NSAWC, (pronounced "EN-SOCK") in western Nevada. NSAWC consolidated three Command Centers into a single Command Structure under a flag officer on July 11, 1996. The Naval Strike Warfare Center (STRIKE "U") based at NAS Fallon since 1984, was joined with the Navy Fighter Weapons School (TOPGUN) and the Carrier Airborne Early Warning Weapons School (TOPDOME) which both moved from NAS Miramar as a result of a Base Realignment and Closure (BRAC) decision in 1993 which transferred that installation back to the Marine Corps as MCAS Miramar. The Seahawk Weapon School was added in 1998 to provide tactical training for Navy helicopters.
These bases host a number of activities including the Joint Unmanned Aerial Systems Center of Excellence, the Naval Strike and Air Warfare Center, Nevada Test and Training Range, Red Flag, the U.S. Air Force Thunderbirds, the United States Air Force Warfare Center, the United States Air Force Weapons School, and the United States Navy Fighter Weapons School.
Songs about Nevada.
"Goldfield" by Rocky Votolato off of the album "Makers" (2006)
Future issues.
Nevada enjoys many economic advantages, and the southern portion of the state enjoys mild winter weather, but rapid growth has led to some overcrowded roads and schools. Nevada has the nation's 5th largest school district in the Clark County School District (projected fall 2007 enrollment is 314,000 students grades K-12).
Coyote Springs is a proposed community for 240,000 inhabitants in Clark and Lincoln counties. It would be Nevada's largest planned city. The town is being developed by Harvey Whittemore and has generated some controversy because of environmental concerns and allegations of political favoritism.

</doc>
<doc id="21217" url="http://en.wikipedia.org/wiki?curid=21217" title="Native Americans in the United States">
Native Americans in the United States

Native Americans are indigenous within the boundaries of the present-day United States (including the indigenous peoples of Alaska and Hawaii) and are composed of numerous, distinct tribes, bands and ethnic groups, many of which survive as intact, sovereign nations. The terms Natives use to refer to themselves vary regionally and generationally, with many older Natives self-identifying as "Indians," while younger Natives often identify as "Indigenous." Which terms should be used by non-Natives has at times been controversial. The term "Native American" has been adopted by major newspapers and some academic groups, but does not traditionally include Native Hawaiians or certain Alaskan Natives, such as Aleut, Yup'ik, or Inuit peoples. Native peoples from Canada are known as First Nations.
Since the end of the 15th century, the migration of Europeans to the Americas has led to centuries of conflict and adjustment between Old and New World societies. Many Native Americans have historically lived as hunter-gatherer societies and preserved their histories by oral traditions and artwork. This resulted in the first written sources on the conflict being authored by Europeans.
At the time of first contact, the indigenous cultures were quite different from those of the proto-industrial and mostly Christian immigrants. Some of the Northeastern and Southwestern cultures in particular were matrilineal and operated on a more collective basis than the Europeans were familiar with. The majority of Indigenous American tribes maintained their hunting grounds and agricultural lands for use of the entire tribe. Europeans at that time had patriarchal cultures and had developed concepts of individual property rights with respect to land that were extremely different. The differences in cultures between the established Native Americans and immigrant Europeans, as well as shifting alliances among different nations in times of war, caused extensive political tension, ethnic violence, and social disruption. Native Americans suffered high fatalities from contact with European diseases to which they had yet not acquired immunity. Smallpox epidemics are thought to have caused the greatest loss of life for indigenous populations, although estimates of the pre-Columbian population of what today constitutes the U.S. vary significantly, from 1 million to 18 million.
After the colonies revolted against Great Britain and established the United States of America, President George Washington and Henry Knox conceived of the idea of "civilizing" Native Americans in preparation for assimilation as U.S. citizens. Assimilation (whether voluntary, as with the Choctaw, or forced) became a consistent policy through American administrations. During the 19th century, the ideology of manifest destiny became integral to the American nationalist movement. Expansion of European-American populations to the west after the American Revolution resulted in increasing pressure on Native American lands, warfare between the groups, and rising tensions. In 1830, the U.S. Congress passed the Indian Removal Act, authorizing the government to relocate Native Americans from their homelands within established states to lands west of the Mississippi River, accommodating European-American expansion. This resulted in the ethnic cleansing of many tribes, with the brutal, forced marches coming to be known as The Trail of Tears.
As American expansion reached into the West, settler and miner migrants came into increasing conflict with the Great Basin, Great Plains, and other Western tribes. These were complex nomadic cultures based on (introduced) horse culture and seasonal bison hunting. They carried out resistance against United States incursion in the decades after the completion of the Civil War and the Transcontinental Railroad in a series of Indian Wars, which were frequent up until the 1890s but continued into the 20th century. Over time, the United States forced a series of treaties and land cessions by the tribes and established reservations for them in many western states. U.S. agents encouraged Native Americans to adopt European-style farming and similar pursuits, but European-American agricultural technology of the time was inadequate for often dry reservation lands, leading to mass starvation. In 1924, Native Americans who were not already U.S. citizens were granted citizenship by Congress.
Contemporary Native Americans have a unique relationship with the United States because they may be members of nations, tribes, or bands with sovereignty and treaty rights. Cultural activism since the late 1960s has increased political participation and led to an expansion of efforts to teach and preserve Indigenous languages for younger generations and to establish a greater cultural infrastructure: Native Americans have founded independent newspapers and online media, recently including FNX, the first Native American television channel; established Native American studies programs, tribal schools and universities, and museums and language programs; and have increasingly been published as authors.
History.
Pre-Columbian.
"Neolithic" is not generally used to describe indigenous cultures in the Americas, see Archaeology of the Americas.
The usual theory of the settlement of the Americas is that earliest ancestors of the peoples of the Americas came from Eurasia over a land bridge which connected the two continents across what is now the Bering Strait during a period of glaciation, when the sea water level was lower. The number and nature of these migrations is uncertain but the land bridge is believed to have existed only until about 12,000 years ago, when the land bridge was flooded.
Three major migrations occurred, as traced by linguistic and genetic data; the early Paleoamericans soon spread throughout the Americas, diversifying into many hundreds of culturally distinct nations and tribes. By 8000 BCE the North American climate was very similar to today's.
The Clovis culture, a megafauna-hunting culture of about 11,000 B.P. that ranged over much of North America and also appeared in South America, has been identified by the distinctive Clovis point. Dating of Clovis materials has been by association with animal bones and by the use of carbon dating methods.
Numerous Paleoindian cultures occupied North America. According to later oral histories, Native Americans have been living on this continent since their genesis, described by a wide range of traditional creation stories. However, genetic and linguistic data connect the indigenous people of this continent with ancient northeast Asians.
The Folsom Tradition was characterized by use of Folsom points as projectile tips, and data from kill sites, where slaughter and butchering of bison took place. Folsom tools were left behind between 9000 BCE and 8000 BCE.
Na-Dené-speaking peoples entered North America starting around 8000 BCE, reaching the Pacific Northwest by 5000 BCE, and from there migrating along the Pacific Coast and into the interior. It is believed that their ancestors comprised a separate migration into North America, later than the first Paleo-Indians. They migrated into Alaska and northern Canada, south along the Pacific Coast, into the interior of Canada, and south to the Great Plains and the American Southwest. They were the earliest ancestors of the Athabascan-speaking peoples, including the present-day and historical Navajo and Apache.
Since the 1990s, archeologists have explored and dated eleven Middle Archaic sites in present-day Louisiana and Florida at which early cultures built complexes with multiple earthwork mounds; they were societies of hunter-gatherers rather than the settled agriculturalists believed necessary according to the theory of Neolithic Revolution to sustain such large villages over long periods. The prime example is Watson Brake in northern Louisiana, whose 11-mound complex is dated to 3500 BCE, making it the oldest, dated site in the Americas for such complex construction. Construction of the mounds went on for 500 years until abandoned about 2800 BCE, probably due to changing environmental conditions. Poverty Point culture is a Late Archaic archaeological culture that inhabited the area of the lower Mississippi Valley and surrounding Gulf Coast. The culture thrived from 2200 BCE to 700 BCE, during the Late Archaic period. Artifacts show the people traded with other Native Americans located from Georgia to the Great Lakes region. This is one among numerous mound sites of complex indigenous cultures throughout the Mississippi and Ohio valleys. They were one of several succeeding cultures often referred to as mound builders.
The Woodland period of North American pre-Columbian cultures refers to the time period from roughly 1000 BCE to 1,000 CE in the eastern part of North America. The term "Woodland" was coined in the 1930s and refers to prehistoric sites dated between the Archaic period and the Mississippian cultures. The Hopewell tradition is the term for the common aspects of the Native American culture that flourished along rivers in the northeastern and midwestern United States from 200 BCE to 500 CE, stretching as far south as Crystal River.
Hohokam is one of the four major prehistoric archaeological traditions of the present-day American Southwest. Living as simple farmers, they raised corn and beans. The early Hohokam founded a series of small villages along the middle Gila River. The communities were located near good arable land, with dry farming common in the earlier years of this period.
The Mississippian culture, which extended throughout the Ohio and Mississippi valleys and built sites throughout the Southeast, created the largest earthworks in North America north of Mexico, most notably at Cahokia, on a tributary of the Mississippi River in present-day Illinois. The society began building at this site about 950 CE, and reached its peak population in 1,250 CE of 20,000–30,000 people, which was not equalled by any city in the present-day United States until after 1800.
Sophisticated pre-Columbian sedentary societies evolved in North America. The rise of the complex cultures was based on the people's adoption of maize agriculture, development of greater population densities, and chiefdom-level complex social organization from 1200 CE to 1650 CE. The introduction of maize from Mesoamerica allowed the accumulation of crop surpluses to support a higher density of population and led to development of specialized skills.
The Iroquois League of Nations or "People of the Long House", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.
Inter-tribal warfare was endemic resulting in displacement and migration of numerous tribes.
European exploration and colonization.
After 1492 European exploration and colonization of the Americas revolutionized how the Old and New Worlds perceived themselves. Many of the first major contacts were in Florida and the Gulf coast by Spanish explorers.
Impact on native populations.
From the 16th through the 19th centuries, the population of Indians sharply declined. Most mainstream scholars believe that, among the various contributing factors, Epidemic disease was the overwhelming cause of the population decline of the American natives because of their lack of immunity to new diseases brought from Europe. It is difficult to estimate the number of Native Americans living in what is today the United States of America. Estimates range from a low of 2.1 million to a high of 18 million (Dobyns 1983). By 1800, the Native population of the present-day United States had declined to approximately 600,000, and only 250,000 Native Americans remained in the 1890s. Chicken pox and measles, endemic but rarely fatal among Europeans (long after being introduced from Asia), often proved deadly to Native Americans. In the 100 years following the arrival of the Spanish to the Americas, large disease epidemics depopulated large parts of the eastern United States in the 16th century.
The extent to which the Native American population was intentionally infected with disease through biological warfare, as opposed to accidental infection, is unknown although there are few known deliberate attempts. The most well known example occurred In 1763, when Sir Jeffrey Amherst, Commander-in-Chief of the Forces of the British Army, wrote praising the use of smallpox infected blankets to "extirpate" the Indian race. Blankets infected with smallpox were later given to natives besieging Fort Pitt. It is unclear whether the blanket attempt succeeded.
In 1634, Fr. Andrew White of the Society of Jesus established a mission in what is now the state of Maryland, and the purpose of the mission, stated through an interpreter to the chief of an Indian tribe there, was "to extend civilization and instruction to his ignorant race, and show them the way to heaven." Fr. Andrew's diaries report that by 1640, a community had been founded which they named St. Mary's, and the Indians were sending their children there "to be educated among the English." This included the daughter of the Piscataway Indian chief Tayac, which exemplifies not only a school for Indians, but either a school for girls, or an early co-ed school. The same records report that in 1677, "a school for humanities was opened by our Society in the centre of [Maryland], directed by two of the Fathers; and the native youth, applying themselves assiduously to study, made good progress. Maryland and the recently established school sent two boys to St. Omer who yielded in abilities to few Europeans, when competing for the honour of being first in their class. So that not gold, nor silver, nor the other products of the earth alone, but men also are gathered from thence to bring those regions, which foreigners have unjustly called ferocious, to a higher state of virtue and cultivation."
In 1727, the Sisters of the Order of Saint Ursula founded Ursuline Academy in New Orleans, which is currently the oldest, continuously-operating school for girls and the oldest Catholic school in the United States. From the time of its foundation it offered the first classes for Native American girls, and would later offer classes for female African-American slaves and free women of color.
Between 1754 and 1763, many Native American tribes were involved in the French and Indian War/Seven Years' War. Those involved in the fur trade tended to ally with French forces against British colonial militias. The British had made fewer allies, but it was joined by some tribes that wanted to prove assimilation and loyalty in support of treaties to preserve their territories. They were often disappointed when such treaties were later overturned. The tribes had their own purposes, using their alliances with the European powers to battle traditional Native enemies. Some Iroquois who were loyal to the British, and helped them fight in the American Revolution, fled north into Canada.
After European explorers reached the West Coast in the 1770s, smallpox rapidly killed at least 30% of Northwest Coast Native Americans. For the next 80 to 100 years, smallpox and other diseases devastated native populations in the region. Puget Sound area populations, once estimated as high as 37,000 people, were reduced to only 9,000 survivors by the time settlers arrived en masse in the mid-19th century.
Smallpox epidemics in 1780–1782 and 1837–1838 brought devastation and drastic depopulation among the Plains Indians. By 1832, the federal government established a smallpox vaccination program for Native Americans ("The Indian Vaccination Act of 1832"). It was the first federal program created to address a health problem of Native Americans.
Animal introductions.
With the meeting of two worlds, animals, insects, and plants were carried from one to the other, both deliberately and by chance, in what is called the Columbian Exchange. In the 16th century, Spaniards and other Europeans brought horses to Mexico. Some of the horses escaped and began to breed and increase their numbers in the wild. As Native Americans adopted use of the animals, they began to change their cultures in substantial ways, especially by extending their nomadic ranges for hunting. The reintroduction of the horse to North America had a profound impact on Native American culture of the Great Plains.
King Philip's War.
King Philip's War, also called Metacom's War or Metacom's Rebellion, was an armed conflict between Native American inhabitants of present-day southern New England and English colonists and their Native American allies from 1675 to 1676. It continued in northern New England (primarily on the Maine frontier) even after King Philip was killed, until a treaty was signed at Casco Bay in April 1678.
Foundations for Freedom.
Some Europeans considered Native American societies to be representative of a golden age known to them only in folk history. In the 20th century, some writers have credited the Iroquois nations' political confederacy and democratic government as being influences for the development of the Articles of Confederation and the United States Constitution.
American Revolution.
During the American Revolution, the newly proclaimed United States competed with the British for the allegiance of Native American nations east of the Mississippi River. Most Native Americans who joined the struggle sided with the British, based both on their trading relationships and hopes that colonial defeat would result in a halt to further colonial expansion onto Native American land. Many native communities were divided over which side to support in the war and others wanted to remain neutral. The first native community to sign a treaty with the new United States Government was the Lenape. For the Iroquois Confederacy, based in New York, the American Revolution resulted in civil war. The British made peace with the Americans in the Treaty of Paris (1783), through which they ceded vast Native American territories to the United States without informing or consulting with the Native Americans.
18th-century United States.
The United States was eager to expand, to develop farming and settlements in new areas, and to satisfy land hunger of settlers from New England and new immigrants. The national government initially sought to purchase Native American land by treaties. The states and settlers were frequently at odds with this policy.
United States policy toward Native Americans continued to evolve after the American Revolution. George Washington and Henry Knox believed that Native Americans were equals but that their society was inferior. Washington formulated a policy to encourage the "civilizing" process. Washington had a six-point plan for civilization which included:
In the late 18th century, reformers starting with Washington and Knox, supported educating native children and adults, in efforts to "civilize" or otherwise assimilate Native Americans to the larger society (as opposed to relegating them to reservations). The Civilization Fund Act of 1819 promoted this civilization policy by providing funding to societies (mostly religious) who worked on Native American improvement.
19th century.
As American expansion continued, Native Americans resisted settlers' encroachment in several regions of the new nation (and in unorganized territories), from the Northwest to the Southeast, and then in the West, as settlers encountered the tribes of the Great Plains. East of the Mississippi River, an intertribal army led by Tecumseh, a Shawnee chief, fought a number of engagements in the Northwest during the period 1811–12, known as Tecumseh's War. Conflicts in the Southeast include the Creek War and Seminole Wars, both before and after the Indian Removals of most members of the Five Civilized Tribes. Native American nations on the plains in the west continued armed conflicts with the United States throughout the 19th century, through what were called generally "Indian Wars".
In the 1830s President Jackson signed the Indian Removal Act of 1830, a policy of relocating Indians from their homelands to Indian Territory and reservations in surrounding areas to open their lands for non-native settlements. This resulted in the Trail of Tears.
In July 1845, the New York newspaper editor John L. O'Sullivan coined the phrase, "Manifest Destiny", as the "design of Providence" supporting the territorial expansion of the United States. Manifest Destiny had serious consequences for Native Americans, since continental expansion for the United States took place at the cost of their occupied land.
The Indian Appropriations Act of 1851 set the precedent for modern-day Native American reservations through allocating funds to move western tribes onto reservations since there were no more lands available for relocation.
Civil War.
Native Americans served in both the Union and Confederate military during the American Civil War. At the outbreak of the war, for example, the minority party of the Cherokees gave its allegiance to the Confederacy, while originally the majority party went for the North. Native Americans fought knowing they might jeopardize their freedom, unique cultures, and ancestral lands if they ended up on the losing side of the Civil War. 28,693 Native Americans served in the Union and Confederate armies during the Civil War, participating in battles such as Pea Ridge, Second Manassas, Antietam, Spotsylvania, Cold Harbor, and in Federal assaults on Petersburg. A few Native American tribes, such as the Creek and the Choctaw, were slaveholders and found a political and economic commonality with the Confederacy. The Choctaw owned over 2,000 slaves.
Removals and reservations.
In the 19th century, the incessant westward expansion of the United States incrementally compelled large numbers of Native Americans to resettle further west, often by force, almost always reluctantly. Native Americans believed this forced relocation illegal, given the Hopewell Treaty of 1785. Under President Andrew Jackson, United States Congress passed the Indian Removal Act of 1830, which authorized the President to conduct treaties to exchange Native American land east of the Mississippi River for lands west of the river.
As many as 100,000 Native Americans relocated to the West as a result of this Indian Removal policy. In theory, relocation was supposed to be voluntary and many Native Americans did remain in the East. In practice, great pressure was put on Native American leaders to sign removal treaties. The most egregious violation, the Trail of Tears, was removal of the Cherokee by President Jackson to Indian Territory.
Native Americans and U.S. Citizenship.
In 1817, the Cherokee became the first Native Americans recognized as U.S. citizens. Under Article 8 of the 1817 Cherokee treaty, "Upwards of 300 Cherokees (Heads of Families) in the honest simplicity of their souls, made an election to become American citizens".
Factors establishing citizenship included:
After the American Civil War, the Civil Rights Act of 1866 states, "that all persons born in the United States, and not subject to any foreign power, excluding Indians not taxed, are hereby declared to be citizens of the United States".
Indian Appropriations Act of 1871.
In 1871 Congress added a rider to the Indian Appropriations Act ending United States recognition of additional Native American tribes or independent nations, and prohibiting additional treaties.
Education.
After the Indian wars in the late 19th century, the government established Native American boarding schools, initially run primarily by or affiliated with Christian missionaries. At this time American society thought that Native American children needed to be acculturated to the general society. The boarding school experience Was a total immersion in modern American society, but it could prove traumatic to children, who were forbidden to speak their native languages. They were taught Christianity and not allowed to practice their native religions, and in numerous other ways forced to abandon their Native American identities.
Before the 1930s, schools on the reservations provided no schooling beyond the sixth grade. To obtain more, boarding school was usually necessary. Small reservations with a few hundred people usually send their children to nearby public schools. The "Indian New Deal" of the 1930s closed many of the boarding schools, and downplayed the assimilationist goals. The Indian Division of the Civilian Conservation Corps operated large-scale construction projects on the reservations, building thousands of new schools and community buildings. Under the leadership of John Collier the BIA brought in progressive educators to reshape Indian education. The Bureau of Indian Affairs (BIA) by 1938 taught 30,000 students in 377 boarding and day schools, or 40% of all Indian children in school. The Navajo largely opposed schooling of any sort, but the other tribes accepted the system. There were now high schools on larger reservations, make educated not only teenagers but also an adult audience. There were no Indian facilities for higher education. They deemphasized textbooks, emphasized self-esteem, and started teaching Indian history. They promoted traditional arts and crafts of the sort that could be conducted on the reservations, such as making jewelry. The New Deal reformers met significant resistance from parents and teachers, and had mixed results. World War II brought younger Indians in contact with the broader society through military service and work in the munitions industries. The role of schooling was changed to focus on vocational education for jobs in urban America. 
Since the rise of self-determination for Native Americans, they have generally emphasized education of their children at schools near where they live. In addition, many federally recognized tribes have taken over operations of such schools and added programs of language retention and revival to strengthen their cultures. Beginning in the 1970s, tribes have also founded colleges at their reservations, controlled, and operated by Native Americans, to educate their young for jobs as well as to pass on their cultures.
20th century.
On August 29, 1911, Ishi, generally considered to have been the last Native American to live most of his life without contact with European-American culture, was discovered near Oroville, California.
On June 2, 1924, U.S. Republican President Calvin Coolidge signed the Indian Citizenship Act, which made all Native Americans born in the United States and its territories American citizens. Prior to passage of the act, nearly two-thirds of Native Americans were already U.S. citizens.
American Indians today in the United States have all the rights guaranteed in the U.S. Constitution, can vote in elections, and run for political office. Controversies remain over how much the federal government has jurisdiction over tribal affairs, sovereignty, and cultural practices.
Mid-century, the Indian termination policy and the Indian Relocation Act of 1956 marked a new direction for assimilating Native Americans into urban life. 
The census counted 332,000 Indians in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940. 
World War II.
Some 44,000 Native Americans served in the United States military during World War II: at the time, one-third of all able-bodied Indian men from 18 to 50 years of age. Described as the first large-scale exodus of indigenous peoples from the reservations since the removals of the 19th century, the men's service with the U.S. military in the international conflict was a turning point in Native American history. The overwhelming majority of Native Americans welcomed the opportunity to serve; they had a voluntary enlistment rate that was 40% higher than those drafted.
Their fellow soldiers often held them in high esteem, in part since the legend of the tough Native American warrior had become a part of the fabric of American historical legend. White servicemen sometimes showed a lighthearted respect toward Native American comrades by calling them "chief". The resulting increase in contact with the world outside of the reservation system brought profound changes to Native American culture. "The war", said the U.S. Indian Commissioner in 1945, "caused the greatest disruption of Native life since the beginning of the reservation era", affecting the habits, views, and economic well-being of tribal members. The most significant of these changes was the opportunity—as a result of wartime labor shortages—to find well-paying work in cities, and many people relocated to urban areas, particularly on the West Coast with the buildup of the defense industry.
There were also losses as a result of the war. For instance, a total of 1,200 Pueblo men served in World War II; only about half came home alive. In addition many more Navajo served as code talkers for the military in the Pacific. The code they made, although cryptologically very simple, was never cracked by the Japanese.
Self-determination.
Military service and urban residency contributed to the rise of American Indian activism, particularly after the 1960s and the occupation of Alcatraz Island (1969–1971) by a student Indian group from San Francisco. In the same period, the American Indian Movement (AIM) was founded in Minneapolis, and chapters were established throughout the country, where American Indians combined spiritual and political activism. Political protests gained national media attention and the sympathy of the American public.
Through the mid-1970s, conflicts between governments and Native Americans occasionally erupted into violence. A notable late 20th-century event was the Wounded Knee incident on the Pine Ridge Indian Reservation. Upset with tribal government and the failures of the federal government to enforce treaty rights, about 300 Oglala Lakota and AIM activists took control of Wounded Knee on February 27, 1973.
Indian activists from around the country joined them at Pine Ridge, and the occupation became a symbol of rising American Indian identity and power. Federal law enforcement officials and the national guard cordoned off the town, and the two sides had a standoff for 71 days. During much gunfire, one United States Marshal was wounded and paralyzed. In late April a Cherokee and local Lakota man were killed by gunfire; the Lakota elders ended the occupation to ensure no more lives were lost.
In June 1975, two FBI agents seeking to make an armed robbery arrest at Pine Ridge Reservation were wounded in a firefight, and killed at close range. The AIM activist Leonard Peltier was sentenced in 1976 to two consecutive terms of life in prison in the FBI deaths.
In 1968 the government enacted the Indian Civil Rights Act. This gave tribal members most of the protections against abuses by tribal governments that the Bill of Rights accords to all U.S. citizens with respect to the federal government. In 1975 the U.S. government passed the Indian Self-Determination and Education Assistance Act, marking the culmination of 15 years of policy changes. It resulted from American Indian activism, the Civil Rights Movement, and community development aspects of President Lyndon Johnson's social programs of the 1960s. The Act recognized the right and need of Native Americans for self-determination. It marked the U.S. government's turn away from the 1950s policy of termination of the relationship between tribes and the government. The U.S. government encouraged Native Americans' efforts at self-government and determining their futures. Tribes have developed organizations to administer their own social, welfare and housing programs, for instance. Tribal self-determination has created tension with respect to the federal government's historic trust obligation to care for Indians, however, the Bureau of Indian Affairs has never lived up to that responsibility.
Tribal colleges.
Navajo Community College, now called Diné College, the first tribal college, was founded in Tsaile, Arizona, in 1968 and accredited in 1979. Tensions immediately arose between two philosophies: one that the tribal colleges should have the same criteria, curriculum and procedures for educational quality as mainstream colleges, the other that The faculty and curriculum should be closely adapted to the particular historical culture of the tribe. There was a great deal of turnover, exacerbated by very tight budgets. In 1994 the U.S. Congress passed legislation recognizing the tribal colleges as land-grant colleges, which provided opportunities for large-scale funding. Thirty-two tribal colleges in the United States belong to the American Indian Higher Education Consortium. By the early 21st century, tribal nations had also established numerous language revival programs in their schools.
In addition, Native American activism has led major universities across the country to establish Native American studies programs and departments, increasing awareness of the strengths of Indian cultures, providing opportunities for academics, and deepening research on history and cultures in the United States. Native Americans have entered academia; journalism and media; politics at local, state and federal levels; and public service, for instance, influencing medical research and policy to identify issues related to American Indians.
21st century.
In 2009 an "apology to Native Peoples of the United States" was included in the defense appropriations act. It states that the U.S. "apologizes on behalf of the people of the United States to all Native Peoples for the many instances of violence, maltreatment, and neglect inflicted on Native Peoples by citizens of the United States.
In 2013 jurisdiction over persons who were not tribal members under the Violence Against Women Act was extended to Indian Country. This closed a gap which prevented arrest or prosecution by tribal police or courts of abusive partners of tribal members who were not native or from another tribe.
Migration to urban areas continued to grow with 70% of Native Americans living in urban areas in 2012, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Albuquerque, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, and Rapid City. Many lived in poverty. Racism, unemployment, drugs and gangs were common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempted to address.
Demographics.
Historical population.
The census counted 248,000 Indians in 1890, 332,000 in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940. 
Population and distribution.
The 2010 census permitted respondents to self-identify as being of one or more races. Self-identification dates from the census of 1960; prior to that the race of the respondent was determined by opinion of the census taker. The option to select more than one race was introduced in 2000. If American Indian or Alaska Native was selected, the form requested the individual provide the name of the "enrolled or principal tribe". The 2010 Census showed that the U.S. population on April 1, 2010, was 308.7 million.
Out of the total U.S. population, 2.9 million people, or 0.9 percent, reported American Indian or Alaska Native alone. In addition, 2.3 million people, or another 0.7 percent, reported American Indian or Alaska Native in combination with one or more other races. Together, these two groups totaled 5.2 million people. Thus, 1.7 percent of all people in the United States identified as American Indian or Alaska Native, either alone or in combination with one or more other races.
The definition of American Indian or Alaska Native used in the 2010 census:According to Office of Management and Budget, "American Indian or Alaska Native" refers to a person having origins in any of the original peoples of North and South America (including Central America) and who maintains tribal affiliation or community attachment.
78% of Native Americans live outside a reservation. Full-blood individuals are more likely to live on a reservation than mixed-blood individuals. The Navajo, with 286,000 full-blood individuals, is the largest tribe if only full-blood individuals are counted; the Navajo are the tribe with the highest proportion of full-blood individuals, 86.3%. The Cherokee have a different history; it is the largest tribe with 819,000 individuals, and it has 284,000 full-blood individuals.
Urban migration.
As of 2012 70% of American Indians live in urban areas, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, and Rapid City. Many live in poverty. Racism, unemployment, drugs and gangs are common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempt to address.
Distribution by U.S. state.
According to 2003 United States Census Bureau estimates, a little over one third of the 2,786,652 Native Americans in the United States live in three states: California at 413,382, Arizona at 294,137 and Oklahoma at 279,559.
In 2010, the U.S. Census Bureau estimated that about 0.8% of the U.S. population was of American Indian or Alaska Native descent. This population is unevenly distributed across the country. Below, all 50 states, as well as the District of Columbia and Puerto Rico, are listed by the proportion of residents citing American Indian or Alaska Native ancestry, based on the 2010 U.S. Census.
In 2006, the U.S. Census Bureau estimated that about less than 1.0% of the U.S. population was of Native Hawaiian or Pacific Islander descent. This population is unevenly distributed across 26 states. Below, are the 26 states that had at least 0.1%. They are listed by the proportion of residents citing Native Hawaiian or Pacific Islander ancestry, based on 2006 estimates:
Population by tribal grouping.
Below are numbers for U.S. citizens self-identifying to selected tribal grouping, according to the 2000 U.S. census.
Current legal status.
There are 562 federally recognized tribal governments in the United States. These tribes possess the right to form their own governments, to enforce laws (both civil and criminal) within their lands, to tax, to establish requirements for membership, to license and regulate activities, to zone and to exclude persons from tribal territories. Limitations on tribal powers of self-government include the same limitations applicable to states; for example, neither tribes nor states have the power to make war, engage in foreign relations, or coin money (this includes paper currency).
Many Native Americans and advocates of Native American rights point out that the U.S. federal government's claim to recognize the "sovereignty" of Native American peoples falls short, given that the United States wishes to govern Native American peoples and treat them as subject to U.S. law. Such advocates contend that full respect for Native American sovereignty would require the U.S. government to deal with Native American peoples in the same manner as any other sovereign nation, handling matters related to relations with Native Americans through the Secretary of State, rather than the Bureau of Indian Affairs. The Bureau of Indian Affairs reports on its website that its "responsibility is the administration and management of 55700000 acre of land held in trust by the United States for American Indians, Indian tribes, and Alaska Natives". Many Native Americans and advocates of Native American rights believe that it is condescending for such lands to be considered "held in trust" and regulated in any fashion by other than their own tribes, whether the U.S. or Canadian governments, or any other non-Native American authority.
As of year 2000, the largest groups in the United States by population were Navajo, Cherokee, Choctaw, Sioux, Chippewa, Apache, Blackfeet, Iroquois, and Pueblo. In 2000, eight of ten Americans with Native American ancestry were of mixed ancestry. It is estimated that by 2100 that figure will rise to nine out of ten.
In addition, there are a number of tribes that are recognized by individual states, but not by the federal government. The rights and benefits associated with state recognition vary from state to state.
Some tribal groups have been unable to document the cultural continuity required for federal recognition. The Muwekma Ohlone of the San Francisco bay area are pursuing litigation in the federal court system to establish recognition. Many of the smaller eastern tribes, long considered remnants of extinct peoples, have been trying to gain official recognition of their tribal status. Several in Virginia and North Carolina have gained state recognition. Federal recognition confers some benefits, including the right to label arts and crafts as Native American and permission to apply for grants that are specifically reserved for Native Americans. But gaining federal recognition as a tribe is extremely difficult; to be established as a tribal group, members have to submit extensive genealogical proof of tribal descent and continuity of the tribe as a culture.
In July 2000 the Washington State Republican Party adopted a resolution recommending that the federal and legislative branches of the U.S. government terminate tribal governments. In 2007 a group of Democratic Party congressmen and congresswomen introduced a bill in the U.S. House of Representatives to "terminate" the Cherokee Nation. This was related to their voting to exclude Cherokee Freedmen as members of the tribe unless they had a Cherokee ancestor on the Dawes Rolls, although all Cherokee Freedmen and their descendants had been members since 1866.
As of 2004, various Native Americans are wary of attempts by others to gain control of their reservation lands for natural resources, such as coal and uranium in the West.
In the state of Virginia, Native Americans face a unique problem. Virginia has no federally recognized tribes but the state has recognized eight. This is related historically to the greater impact of disease and warfare on the Virginia Indian populations, as well as their intermarriage with Europeans and Africans. Some people confused the ancestry with culture, but groups of Virginia Indians maintained their cultural continuity. Most of their early reservations were ended under the pressure of early European settlement.
Some historians also note the problems of Virginia Indians in establishing documented continuity of identity, due to the work of Walter Ashby Plecker (1912–1946). As registrar of the state's Bureau of Vital Statistics, he applied his own interpretation of the one-drop rule, enacted in law in 1924 as the state's Racial Integrity Act. It recognized only two races: "white" and "colored".
Plecker, a segregationist, believed that the state's Native Americans had been "mongrelized" by intermarriage with African Americans; to him, ancestry determined identity, rather than culture. He thought that some people of partial black ancestry were trying to "pass" as Native Americans. Plecker thought that anyone with any African heritage had to be classified as colored, regardless of appearance, amount of European or Native American ancestry, and cultural/community identification. Plecker pressured local governments into reclassifying all Native Americans in the state as "colored", and gave them lists of family surnames to examine for reclassification based on his interpretation of data and the law. This led to the state's destruction of accurate records related to families and communities who identified as Native American (as in church records and daily life). By his actions, sometimes different members of the same family were split by being classified as "white" or "colored". He did not allow people to enter their primary identification as Native American in state records. In 2009, the Senate Indian Affairs Committee endorsed a bill that would grant federal recognition to tribes in Virginia.
To achieve federal recognition and its benefits, tribes must prove continuous existence since 1900. The federal government has maintained this requirement, in part because through participation on councils and committees, federally recognized tribes have been adamant about groups' satisfying the same requirements as they did.
Contemporary issues.
Native American struggles amid poverty to maintain life on the reservation or in larger society have resulted in a variety of health issues, some related to nutrition and health practices. The community suffers a vulnerability to and disproportionately high rate of alcoholism.
It has long been recognized that Native Americans are dying of diabetes, alcoholism, tuberculosis, suicide, and other health conditions at shocking rates. Beyond disturbingly high mortality rates, Native Americans also suffer a significantly lower health status and disproportionate rates of disease compared with all other Americans. — U.S. Commission on Civil Rights (September 2004)
Societal discrimination and racism.
Most non-Native Americans admitted they rarely encountered Native Americans in their daily lives. While sympathetic toward Native Americans and expressing regret over the past, most people had only a vague understanding of the problems facing Native Americans today. For their part, Native Americans told researchers that they believed they continued to face prejudice, mistreatment, and inequality in the broader society.
Affirmative action issues.
Federal contractors and subcontractors, such as businesses and educational institutions, are legally required to adopt equal opportunity employment and affirmative action measures intended to prevent discrimination against employees or applicants for employment on the basis of "color, religion, sex, or national origin". For this purpose, a Native American is defined as "A person having origins in any of the original peoples of North and South America (including Central America), and who maintains a tribal affiliation or community attachment". However, self-reporting is permitted: "Educational institutions and other recipients should allow students and staff to self-identify their race and ethnicity unless self-identification is not practicable or feasible."
Self-reporting opens the door to "box checking" by people who, despite not having a substantial relationship to Native American culture, innocently or fraudulently check the box for Native American.
Native American mascots in sports.
American Indian activists in the United States and Canada have criticized the use of Native American mascots in sports, as perpetuating stereotypes.
Many universities and professional sports teams no longer use such images. Examples include Stanford University, which changed from Indians to Cardinal in 1972; Miami University, which switched from Redskins to RedHawks in 1997; and the NBA's Golden State Warriors, who originally used Native American-themed logos but have not since 1971.
However, controversy has remained regarding teams such as the NFL's Washington Redskins, whose name is considered to be a racial slur, and MLB's Cleveland Indians, whose usage of a caricature called Chief Wahoo has diminished but not ended altogether.
Some tribal team names have been approved by the tribe in question, such as the Seminole Tribe of Florida's approving use of their name for the teams of Florida State University.
Historical depictions in art.
Native Americans have been depicted by American artists in various ways at different periods. A number of 19th and 20th-century United States and Canadian painters, often motivated by a desire to document and preserve Native culture, specialized in Native American subjects. Among the most prominent of these were Elbridge Ayer Burbank, George Catlin, Seth Eastman, Paul Kane, W. Langdon Kihn, Charles Bird King, Joseph Henry Sharp, and John Mix Stanley.
In the 20th century, early portrayals of Native Americans in movies and television roles were first performed by European Americans dressed in mock traditional attire. Examples included "The Last of the Mohicans" (1920), "Hawkeye and the Last of the Mohicans" (1957), and "F Troop" (1965–67). In later decades, Native American actors such as Jay Silverheels in "The Lone Ranger" television series (1949–57) came to prominence. Roles of Native Americans were limited and not reflective of Native American culture. By the 1970s some Native American film roles began to show more complexity, such as those in "Little Big Man" (1970), "Billy Jack" (1971), and "The Outlaw Josey Wales" (1976), which depicted Native Americans in minor supporting roles.
For years, Native people on U.S. television were relegated to secondary, subordinate roles. During the years of the series "Bonanza" (1959–1973), no major or secondary Native characters appeared on a consistent basis. The series "The Lone Ranger" (1949–1957), "Cheyenne" (1955–1963), and "Law of the Plainsman" (1959–1963) had Native characters who were essentially aides to the central white characters. This continued in such series as "How the West Was Won". These programs resembled the "sympathetic" yet contradictory film "Dances With Wolves" of 1990, in which, according to Ella Shohat and Robert Stam, the narrative choice was to relate the Lakota story as told through a Euro-American voice, for wider impact among a general audience.
Like the 1992 remake of "The Last of the Mohicans" and "" (1993), "Dances with Wolves" employed a number of Native American actors, and made an effort to portray Indigenous languages.
In 2009 "We Shall Remain" (2009), a television documentary by Ric Burns and part of the American Experience series, presented a five-episode series "from a Native American perspective". It represented "an unprecedented collaboration between Native and non-Native filmmakers and involves Native advisors and scholars at all levels of the project". The five episodes explore the impact of King Philip's War on the northeastern tribes, the "Native American confederacy" of Tecumseh's War, the U.S.-forced relocation of Southeastern tribes known as the Trail of Tears, the pursuit and capture of Geronimo and the Apache Wars, and concludes with the Wounded Knee incident, participation by the American Indian Movement, and the increasing resurgence of modern Native cultures since.
Terminology differences.
Common usage in the United States.
Native Americans are more commonly known as Indians or American Indians. The term "Native American" was introduced in the United States in preference to the older term "Indian" to distinguish the indigenous peoples of the Americas from the people of India, and to avoid negative stereotypes associated with the term "Indian". Some academics believe that the term "Indian" should be considered outdated or offensive. Many indigenous Americans, however, prefer the term "American Indian".
Criticism of the neologism "Native American" comes from diverse sources. Russell Means, an American Indian activist, opposed the term "Native American" because he believed it was imposed by the government without the consent of American Indians. He has also argued that the use of the word "Indian" derives not from a confusion with India but from a Spanish expression "En Dio", meaning "in God".
A 1995 U.S. Census Bureau survey found that more Native Americans in the United States preferred "American Indian" to "Native American". Most American Indians are comfortable with "Indian", "American Indian", and "Native American", and the terms are often used interchangeably. The traditional term is reflected in the name chosen for the National Museum of the American Indian, which opened in 2004 on the Mall in Washington, D.C.
Gambling industry.
Gambling has become a leading industry. Casinos operated by many Native American governments in the United States are creating a stream of gambling revenue that some communities are beginning to use as leverage to build diversified economies. Although many Native American tribes have casinos, the impact of Native American gaming is widely debated. Some tribes, such as the Winnemem Wintu of Redding, California, feel that casinos and their proceeds destroy culture from the inside out. These tribes refuse to participate in the gambling industry.
Crime on reservations.
Prosecution of serious crime, historically endemic on reservations, was required by the 1885 Major Crimes Act, 18 U.S.C. §§1153, 3242, and court decisions to be investigated by the federal government, usually the Federal Bureau of Investigation, and prosecuted by United States Attorneys of the United States federal judicial district in which the reservation lies.
A December 13, 2009" New York Times" article about growing gang violence on the Pine Ridge Indian Reservation estimated that there were 39 gangs with 5,000 members on that reservation alone. Navajo country recently reported 225 gangs in its territory.
As of 2012, a high incidence of rape continued to impact Native American women and Alaskan native women. According to the Department of Justice, 1 in 3 Native women have suffered rape or attempted rape, more than twice the national rate. About 46 percent of Native American women have been raped, beaten, or stalked by an intimate partner, according to a 2010 study by the Centers for Disease Control. According to Professor N. Bruce Duthu, "More than 80 percent of Indian victims identify their attacker as non-Indian".
Society, language, and culture.
Though cultural features, language, clothing, and customs vary enormously from one tribe to another, there are certain elements which are encountered frequently and shared by many tribes. Early European American scholars described the Native Americans as having a society dominated by clans.
Early hunter-gatherer tribes made stone weapons from around 10,000 years ago; as the age of metallurgy dawned, newer technologies were used and more efficient weapons produced. Prior to contact with Europeans, most tribes used similar weaponry. The most common implements were the bow and arrow, the war club, and the spear. Quality, material, and design varied widely. Native American use of fire both helped provide and prepare for food and altered the landscape of the continent to help the human population flourish.
Large mammals like mammoths and mastodons were largely extinct by around 8000 BCE. Native Americans switched to hunting other large game, such as bison. The Great Plains tribes were still hunting the bison when they first encountered the Europeans. The Spanish reintroduction of the horse to North America in the 17th century and Native Americans' learning to use them greatly altered the natives' culture, including changing the way in which they hunted large game. Horses became such a valuable, central element of Native lives that they were counted as a measure of wealth.
Ethno-linguistic classification.
Native Americans were divided into several hundred ethno-linguistic groups.
A number of English words have been derived from Native American languages.
Society and art.
The Iroquois, living around the Great Lakes and extending east and north, used strings or belts called "wampum" that served a dual function: the knots and beaded designs mnemonically chronicled tribal stories and legends, and further served as a medium of exchange and a unit of measure. The keepers of the articles were seen as tribal dignitaries.
Pueblo peoples crafted impressive items associated with their religious ceremonies. "Kachina" dancers wore elaborately painted and decorated masks as they ritually impersonated various ancestral spirits. Sculpture was not highly developed, but carved stone and wood fetishes were made for religious use. Superior weaving, embroidered decorations, and rich dyes characterized the textile arts. Both turquoise and shell jewelry were created, as were high-quality pottery and formalized pictorial arts.
Navajo spirituality focused on the maintenance of a harmonious relationship with the spirit world, often achieved by ceremonial acts, usually incorporating sandpainting. The colors—made from sand, charcoal, cornmeal, and pollen—depicted specific spirits. These vivid, intricate, and colorful sand creations were erased at the end of the ceremony.
Agriculture.
An early crop the Native Americans grew was squash. Other early crops included cotton, sunflower, pumpkins, tobacco, goosefoot, knotgrass, and sump weed.
Agriculture in the southwest started around 4,000 years ago when traders brought cultigens from Mexico. Due to the varying climate, some ingenuity was needed for agriculture to be successful. The climate in the southwest ranged from cool, moist mountains regions, to dry, sandy soil in the desert. Some innovations of the time included irrigation to bring water into the dry regions and the selection of seed based on the traits of the growing plants that bore them.
In the southwest, they grew beans that were self-supported, much like the way they are grown today. In the east, however, they were planted right by corn in order for the vines to be able to "climb" the cornstalks.
The most important crop the Native Americans raised was maize. It was first started in Mesoamerica and spread north. About 2,000 years ago it reached eastern America. This crop was important to the Native Americans because it was part of their everyday diet; it could be stored in underground pits during the winter, and no part of it was wasted. The husk was made into art crafts, and the cob was used as fuel for fires.
By 800 CE the Native Americans had established three main crops — beans, squash, and corn — called the three sisters.
The agriculture gender roles of the Native Americans varied from region to region. In the southwest area, men prepared the soil with hoes. The women were in charge of planting, weeding, and harvesting the crops. In most other regions, the women were in charge of doing everything, including clearing the land. Clearing the land was an immense chore since the Native Americans rotated fields frequently. There is a tradition that Squanto showed the Pilgrims in New England how to put fish in fields to act like a fertilizer, but the truth of this story is debated.
Native Americans did plant beans next to corn; the beans would replace the nitrogen which the corn took from the ground, as well as using corn stalks for support for climbing. Native Americans used controlled fires to burn weeds and clear fields; this would put nutrients back into the ground. If this did not work, they would simply abandon the field to let it be fallow, and find a new spot for cultivation.
Europeans in the eastern part of the continent observed that Natives cleared large areas for cropland. Their fields in New England sometimes covered hundreds of acres. Colonists in Virginia noted thousands of acres under cultivation by Native Americans.
Native Americans commonly used tools such as the hoe, maul, and dibber. The hoe was the main tool used to till the land and prepare it for planting; then it was used for weeding. The first versions were made out of wood and stone. When the settlers brought iron, Native Americans switched to iron hoes and hatchets. The dibber was a digging stick, used to plant the seed. Once the plants were harvested, women prepared the produce for eating. They used the maul to grind the corn into mash. It was cooked and eaten that way or baked as corn bread.
Religion.
Traditional Native American ceremonies are still practiced by many tribes and bands, and the older theological belief systems are still held by many of the native people. These spiritualities may accompany adherence to another faith, or can represent a person's primary religious identity. While much Native American spiritualism exists in a tribal-cultural continuum, and as such cannot be easily separated from tribal identity itself, certain other more clearly defined movements have arisen among "traditional" Native American practitioners, these being identifiable as "religions" in the prototypical sense familiar in the industrialized Western world.
Traditional practices of some tribes include the use of sacred herbs such as tobacco, sweetgrass or sage. Many Plains tribes have sweatlodge ceremonies, though the specifics of the ceremony vary among tribes. Fasting, singing and prayer in the ancient languages of their people, and sometimes drumming are also common.
The Midewiwin Lodge is a traditional medicine society inspired by the oral traditions and prophesies of the Ojibwa (Chippewa) and related tribes.
Another significant religious body among Native peoples is known as the Native American Church. It is a syncretistic church incorporating elements of Native spiritual practice from a number of different tribes as well as symbolic elements from Christianity. Its main rite is the peyote ceremony. Prior to 1890, traditional religious beliefs included Wakan Tanka. In the American Southwest, especially New Mexico, a syncretism between the Catholicism brought by Spanish missionaries and the native religion is common; the religious drums, chants, and dances of the Pueblo people are regularly part of Masses at Santa Fe's Saint Francis Cathedral. Native American-Catholic syncretism is also found elsewhere in the United States. (e.g., the National Kateri Tekakwitha Shrine in Fonda, New York, and the National Shrine of the North American Martyrs in Auriesville, New York).
The eagle feather law (Title 50 Part 22 of the Code of Federal Regulations) stipulates that only individuals of certifiable Native American ancestry enrolled in a federally recognized tribe are legally authorized to obtain eagle feathers for religious or spiritual use. The law does not allow Native Americans to give eagle feathers to non-Native Americans.
Gender roles.
Gender roles are differentiated in many Native American tribes. Whether a particular tribe is predominantly matrilineal or patrilineal, usually both sexes have some degree of decision-making power within the tribe. Many Nations, such as the Haudenosaunee Five Nations and the Southeast Muskogean tribes, have matrilineal or Clan Mother systems, in which property and hereditary leadership are controlled by and passed through the maternal lines. The children are considered to belong to the mother's clan. In Cherokee culture, women own the family property. When traditional young women marry, their husbands may join them in their mother's household.
Matrilineal structures enable young women to have assistance in childbirth and rearing, and protect them in case of conflicts between the couple. If a couple separates or the man dies, the woman has her family to assist her. In matrilineal cultures the mother's brothers are usually the leading male figures in her children's lives; fathers have no standing in their wife and children's clan, as they still belong to their own mother's clan. Hereditary clan chief positions pass through the mother's line and chiefs have historically been selected on recommendation of women elders, who also could disapprove of a chief.
In the patrilineal tribes, such as the Omaha, Osage and Ponca, hereditary leadership passes through the male line, and children are considered to belong to the father and his clan. In patrilineal tribes, if a woman marries a non-Native, she is no longer considered part of the tribe, and her children are considered to share the ethnicity and culture of their father.
In some tribes, men have historically hunted, traded and made war while, as life-givers, women have primary responsibility for the survival and welfare of the families (and future of the tribe). In many tribes women gather and cultivate plants, use plants and herbs to treat illnesses, care for the young and the elderly, make all the clothing and instruments, and process and cure meat and skins from the game. Some mothers use cradleboards to carry an infant while working or traveling. In other tribes, the gender roles are not so clear-cut, and are even less so in the modern era.
At least several dozen tribes allowed polygyny to sisters, with procedural and economic limits.
Apart from making homes, women have many additional tasks that are also essential for the survival of the tribes. Historically they have made weapons and tools, they take care of the roofs of their homes and often help the men hunt and fish. In many tribes, medicine women gather herbs and cure the ill, while in others men may also be healers.
Lakota, Dakota, and Nakota girls are encouraged to learn to ride, hunt and fight. Though fighting in war has mostly been left to the boys and men, occasionally women fought as well - both in battles and in defense of the home - especially if the tribe was severely threatened.
Sports.
Native American leisure time led to competitive individual and team sports. Jim Thorpe, Joe Hipp, Notah Begay III, Chris Wondolowski, Jacoby Ellsbury, Joba Chamberlain, Kyle Lohse, Sam Bradford, Jack Brisco, Tommy Morrison, Billy Mills, and Shoni Schimmel are well known professional athletes.
Team based.
Native American ball sports, sometimes referred to as lacrosse, stickball, or baggataway, was often used to settle disputes, rather than going to war, as a civil way to settle potential conflict. The Choctaw called it "isitoboli" ("Little Brother of War"); the Onondaga name was "dehuntshigwa'es" ("men hit a rounded object"). There are three basic versions, classified as Great Lakes, Iroquoian, and Southern.
The game is played with one or two rackets/sticks and one ball. The object of the game is to land the ball on the opposing team's goal (either a single post or net) to score and to prevent the opposing team from scoring on your goal. The game involves as few as 20 or as many as 300 players with no height or weight restrictions and no protective gear. The goals could be from around 200 ft apart to about 2 mi; in Lacrosse the field is 110 yd. A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject.
Individual based.
Chunkey was a game that consisted of a stone shaped disk that was about 1–2 inches in diameter. The disk was thrown down a 200 ft corridor so that it could roll past the players at great speed. The disk would roll down the corridor, and players would throw wooden shafts at the moving disk. The object of the game was to strike the disk or prevent your opponents from hitting it.
U.S. Olympics.
Jim Thorpe, a Sauk and Fox Native American, was an all-round athlete playing football and baseball in the early 20th century. Future President Dwight Eisenhower injured his knee while trying to tackle the young Thorpe. In a 1961 speech, Eisenhower recalled Thorpe: "Here and there, there are some people who are supremely endowed. My memory goes back to Jim Thorpe. He never practiced in his life, and he could do anything better than any other football player I ever saw."
In the 1912 Olympics, Thorpe could run the 100-yard dash in 10 seconds flat, the 220 in 21.8 seconds, the 440 in 51.8 seconds, the 880 in 1:57, the mile in 4:35, the 120-yard high hurdles in 15 seconds, and the 220-yard low hurdles in 24 seconds. He could long jump 23 ft 6 in and high-jump 6 ft 5 in. He could pole vault 11 ft, put the shot 47 ft, throw the javelin 163 ft, and throw the discus 136 ft. Thorpe entered the U.S. Olympic trials for the pentathlon and the decathlon.
Ellison Brown, of the Narragansett people from Rhode Island, better known as "Tarzan" Brown, won two Boston Marathons (1936, 1939) and competed on the United States Olympic team in the 1936 Olympic Games in Berlin, Germany, but did not finish due to injury. He qualified for the 1940 Olympic Games in Helsinki, Finland, but the games were canceled due to the outbreak of World War II.
Billy Mills, a Lakota and USMC officer, won the gold medal in the 10,000 meter run at the 1964 Tokyo Olympics. He was the only American ever to win the Olympic gold in this event. An unknown before the Olympics, Mills finished second in the U.S. Olympic trials.
Billy Kidd, part Abenaki from Vermont, became the first American male to medal in alpine skiing in the Olympics, taking silver at age 20 in the slalom in the 1964 Winter Olympics at Innsbruck, Austria. Six years later at the 1970 World Championships, Kidd won the gold medal in the combined event and took the bronze medal in the slalom.
Music and art.
Traditional Native American music is almost entirely monophonic, but there are notable exceptions. Native American music often includes drumming and/or the playing of rattles or other percussion instruments but little other instrumentation. Flutes and whistles made of wood, cane, or bone are also played, generally by individuals, but in former times also by large ensembles (as noted by Spanish conquistador de Soto). The tuning of modern flutes is typically pentatonic.
Performers with Native American parentage have occasionally appeared in American popular music such as Rita Coolidge, Wayne Newton, Gene Clark, Buffy Sainte-Marie, Blackfoot, Tori Amos, Redbone (members are also of Mexican descent), and CocoRosie. Some, such as John Trudell, have used music to comment on life in Native America. Other musicians such as R. Carlos Nakai, Joanne Shenandoah and Robert "Tree" Cody integrate traditional sounds with modern sounds in instrumental recordings, whereas the music by artist Charles Littleleaf is derived from ancestral heritage as well as nature. A variety of small and medium-sized recording companies offer an abundance of recent music by Native American performers young and old, ranging from pow-wow drum music to hard-driving rock-and-roll and rap. In the International world of ballet dancing Maria Tallchief was considered America's first major prima ballerina, and was the first person of Native American descent to hold the rank. along with her sister Marjorie Tallchief both became star ballerinas.
The most widely practiced public musical form among Native Americans in the United States is that of the pow-wow. At pow-wows, such as the annual Gathering of Nations in Albuquerque, New Mexico, members of drum groups sit in a circle around a large drum. Drum groups play in unison while they sing in a native language and dancers in colorful regalia dance clockwise around the drum groups in the center. Familiar pow-wow songs include honor songs, intertribal songs, crow-hops, sneak-up songs, grass-dances, two-steps, welcome songs, going-home songs, and war songs. Most indigenous communities in the United States also maintain traditional songs and ceremonies, some of which are shared and practiced exclusively within the community.
Native American art comprises a major category in the world art collection. Native American contributions include pottery, paintings, jewellery, weavings, sculpture, basketry, and carvings. Franklin Gritts was a Cherokee artist who taught students from many tribes at Haskell Institute (now Haskell Indian Nations University) in the 1940s, the "Golden Age" of Native American painters. The integrity of certain Native American artworks is protected by the Indian Arts and Crafts Act of 1990, that prohibits representation of art as Native American when it is not the product of an enrolled Native American artist. Attorney Gail Sheffield and others claim that this law has had "the unintended consequence of sanctioning discrimination against Native Americans whose tribal affiliation was not officially recognized." Native artists such as Jeanne Rorex Bridges (Cherokee) who are not enrolled run the risk of fines or imprisonment if they continue to sell their art while affirming their Indian heritage.
Traditional economy.
The Inuit, or Eskimo, prepared and buried large amounts of dried meat and fish. Pacific Northwest tribes crafted seafaring dugouts 40 – long for fishing. Farmers in the Eastern Woodlands tended fields of maize with hoes and digging sticks, while their neighbors in the Southeast grew tobacco as well as food crops. On the Plains, some tribes engaged in agriculture but also planned buffalo hunts in which herds were driven over bluffs.
Dwellers of the Southwest deserts hunted small animals and gathered acorns to grind into flour with which they baked wafer-thin bread on top of heated stones. Some groups on the region's mesas developed irrigation techniques, and filled storehouses with grain as protection against the area's frequent droughts.
In the early years, as these native peoples encountered European explorers and settlers and engaged in trade, they exchanged food, crafts, and furs for blankets, iron and steel implements, horses, trinkets, firearms, and alcoholic beverages.
Contemporary barriers to economic development.
Today, other than tribes successfully running casinos, many tribes struggle, as they are often located on reservations isolated from the main economic centers of the country. The estimated 2.1 million Native Americans are the most impoverished of all ethnic groups. According to the 2000 Census, an estimated 400,000 Native Americans reside on reservation land. While some tribes have had success with gaming, only 40% of the 562 federally recognized tribes operate casinos. According to a 2007 survey by the U.S. Small Business Administration, only 1% of Native Americans own and operate a business.
Native Americans rank at the bottom of nearly every social statistic: highest teen suicide rate of all minorities at 18.5 per 100,000, highest rate of teen pregnancy, highest high school drop-out rate at 54%, lowest per capita income, and unemployment rates between 50% and 90%. Many Native Americans have become urbanized to survive, moving to urban centers in the states where their reservations are, or out of state. Others have entered academic and political fields that take them away from the reservations.
The barriers to economic development on Native American reservations have been identified by Joseph Kalt and Stephen Cornell of the Harvard Project on American Indian Economic Development at Harvard University, in their report: "What Can Tribes Do? Strategies and Institutions in American Indian Economic Development" (2008), are summarized as follows:
A major barrier to development is the lack of entrepreneurial knowledge and experience within Indian reservations. "A general lack of education and experience about business is a significant challenge to prospective entrepreneurs", was the report on Native American entrepreneurship by the Northwest Area Foundation in 2004. "Native American communities that lack entrepreneurial traditions and recent experiences typically do not provide the support that entrepreneurs need to thrive. Consequently, experiential entrepreneurship education needs to be embedded into school curricula and after-school and other community activities. This would allow students to learn the essential elements of entrepreneurship from a young age and encourage them to apply these elements throughout life". "Rez Biz" magazine addresses these issues.
Native Americans, Europeans, and Africans.
Interracial relations between Native Americans, Europeans, and Africans is a complex issue that has been mostly neglected with "few in-depth studies on interracial relationships". Some of the first documented cases of European/Native American intermarriage and contact were recorded in Post-Columbian Mexico. One case is that of Gonzalo Guerrero, a European from Spain, who was shipwrecked along the Yucatan Peninsula, and fathered three Mestizo children with a Mayan noblewoman. Another is the case of Hernán Cortés and his mistress La Malinche, who gave birth to another of the first multi-racial people in the Americas.
Assimilation.
European impact was immediate, widespread, and profound—more than any other race that had contact with Native Americans during the early years of colonization and nationhood. Europeans living among Native Americans were often called "white indians". They "lived in native communities for years, learned native languages fluently, attended native councils, and often fought alongside their native companions".
Early contact was often charged with tension and emotion, but also had moments of friendship, cooperation, and intimacy. Marriages took place in English, Spanish, and French colonies between Native Americans and Europeans. Given the preponderance of men among the colonists in the early years, generally European men married American Indian women.
In 1528, Isabel de Moctezuma, an heir of Moctezuma II, was married to Alonso de Grado, a Spanish Conquistador. After his death, the widow married Juan Cano de Saavedra. Together they had five children. Many heirs of Emperor Moctezuma II were acknowledged by the Spanish crown, who granted them titles including Duke of Moctezuma de Tultengo.
On April 5, 1614, Pocahontas married the Englishman John Rolfe. They had a child called Thomas Rolfe. Intimate relations among Native American and Europeans were widespread, beginning with the French and Spanish explorers and trappers. For instance, in the early 19th century, the Native American woman Sacagawea, who would help translate for the Lewis and Clark Expedition, was married to the French-Canadian trapper Toussaint Charbonneau. They had a son named Jean Baptiste Charbonneau. This was the most typical pattern among the traders and trappers.
There was fear on both sides, as the different peoples realized how different their societies were. The whites regarded the Indians as "savage" because they were not Christian. They were suspicious of cultures which they did not understand. The Native American author, Andrew J. Blackbird, wrote in his "History of the Ottawa and Chippewa Indians of Michigan," (1897), that white settlers introduced some immoralities into Native American tribes. Many Indians suffered because the Europeans introduced alcohol and the whiskey trade resulted in alcoholism among the people, who were alcohol-intolerant.
Blackbird wrote:
The Ottawas and Chippewas were quite virtuous in their primitive state, as there were no illegitimate children reported in our old traditions. But very lately this evil came to exist among the Ottawas-so lately that the second case among the Ottawas of 'Arbor Croche' is yet living in 1897. And from that time this evil came to be quite frequent, for immorality has been introduced among these people by evil white persons who bring their vices into the tribes.
The U.S. government had two purposes when making land agreements with Native Americans: to open it up more land for white settlement, and to ease tensions between whites and Native Americans by forcing Natives to use the land in the same way as did the whites—for subsistence farms. The government used a variety of strategies to achieve these goals; many treaties required Native Americans to become farmers in order to keep their land. Government officials often did not translate the documents which Native Americans were forced to sign, and native chiefs often had little or no idea what they were signing.
For a Native American man to marry a white woman, he had to get consent of her parents, as long as "he can prove to support her as a white woman in a good home". In the early 19th century, the Shawnee Tecumseh and blonde hair, blue-eyed Rebbecca Galloway had an interracial affair. In the late 19th century, three European-American middle-class women teachers at Hampton Institute married Native American men whom they had met as students.
As European-American women started working independently at missions and Indian schools in the western states, there were more opportunities for their meeting and developing relationships with Native American men. For instance, Charles Eastman, a man of European and Lakota descent whose father sent both his sons to Dartmouth College, got his medical degree at Boston University and returned to the West to practice. He married Elaine Goodale, whom he met in South Dakota. He was the grandson of Seth Eastman, a military officer from Maine, and a chief's daughter. Goodale was a young European-American teacher from Massachusetts and a reformer, who was appointed as the U.S. superintendent of Native American education for the reservations in the Dakota Territory. They had six children together.
European enslavement.
When Europeans arrived as colonists in North America, Native Americans changed their practice of slavery dramatically. Native Americans began selling war captives to whites rather than integrating them into their own societies as they had done before. As the demand for labor in the West Indies grew with the cultivation of sugar cane, Europeans enslaved Native Americans for the Thirteen Colonies, and some were exported to the "sugar islands". The British settlers, especially those in the southern colonies, purchased or captured Native Americans to use as forced labor in cultivating tobacco, rice, and indigo. Accurate records of the numbers enslaved do not exist. Scholars estimate tens of thousands of Native Americans may have been enslaved by the Europeans, being sold by Native Americans themselves.
Slaves became a caste of people who were foreign to the English (Native Americans, Africans and their descendants) and non-Christians. The Virginia General Assembly defined some terms of slavery in 1705:
All servants imported and brought into the Country ... who were not Christians in their native Country ... shall be accounted and be slaves. All Negro, mulatto and Indian slaves within this dominion ... shall be held to be real estate. If any slave resists his master ... correcting such slave, and shall happen to be killed in such correction ... the master shall be free of all punishment ... as if such accident never happened.—Virginia General Assembly declaration, 1705
The slave trade of Native Americans lasted only until around 1730. It gave rise to a series of devastating wars among the tribes, including the Yamasee War. The Indian Wars of the early 18th century, combined with the increasing importation of African slaves, effectively ended the Native American slave trade by 1750. Colonists found that Native American slaves could easily escape, as they knew the country. The wars cost the lives of numerous colonial slave traders and disrupted their early societies. The remaining Native American groups banded together to face the Europeans from a position of strength. Many surviving Native American peoples of the southeast strengthened their loose coalitions of language groups and joined confederacies such as the Choctaw, the Creek, and the Catawba for protection.
Native American women were at risk for rape whether they were enslaved or not; during the early colonial years, settlers were disproportionately male. They turned to Native women for sexual relationships. Both Native American and African enslaved women suffered rape and sexual harassment by male slaveholders and other white men.
Traditions of Native American slavery.
The majority of Native American tribes did practice some form of slavery before the European introduction of African slavery into North America, but none exploited slave labor on a large scale. In addition, Native Americans did not buy and sell captives in the pre-colonial era, although they sometimes exchanged enslaved individuals with other tribes in peace gestures or in exchange for their own members.
The conditions of enslaved Native Americans varied among the tribes. In many cases, young enslaved captives were adopted into the tribes to replace warriors killed during warfare or by disease. Other tribes practiced debt slavery or imposed slavery on tribal members who had committed crimes; but, this status was only temporary as the enslaved worked off their obligations to the tribal society.
Native American and African relations.
African and Native Americans have interacted for centuries. The earliest record of Native American and African contact occurred in April 1502, when Spanish colonists transported the first Africans to Hispaniola to serve as slaves.
Sometimes Native Americans resented the presence of African Americans. The "Catawaba tribe in 1752 showed great anger and bitter resentment when an African American came among them as a trader". To gain favor with Europeans, the Cherokee exhibited the strongest color prejudice of all Native Americans. He contends that because of European fears of a unified revolt of Native Americans and African Americans, the colonists encouraged hostility between the ethnic groups: "Whites sought to convince Native Americans that African Americans worked against their best interests." In 1751, South Carolina law stated:
The carrying of Negroes among the Indians has all along been thought detrimental, as an intimacy ought to be avoided.
In addition, in 1758 the governor of South Carolina James Glen wrote:
it has always been the policy of this government to create an aversion in them Indians to Negroes.
Europeans considered both races inferior and made efforts to make both Native Americans and Africans enemies. Native Americans were rewarded if they returned escaped slaves, and African Americans were rewarded for fighting in the late 19th-century Indian Wars.
"Native Americans, during the transitional period of Africans becoming the primary race enslaved, were enslaved at the same time and shared a common experience of enslavement. They worked together, lived together in communal quarters, produced collective recipes for food, shared herbal remedies, myths and legends, and in the end they intermarried." Because of a shortage of men due to warfare, many tribes encouraged marriage between the two groups, to create stronger, healthier children from the unions.
In the 18th century, many Native American women married freed or runaway African men due to a decrease in the population of men in Native American villages. Records show that many Native American women bought African men but, unknown to the European sellers, the women freed and married the men into their tribe. When African men married or had children by a Native American woman, their children were born free, because the mother was free (according to the principle of "partus sequitur ventrem", which the colonists incorporated into law).
European colonists often required the return of runaway slaves to be included as a provision in treaties with American Indians. In 1726, the British Governor of New York exacted a promise from the Iroquois to return all runaway slaves who had joined up with them. In the mid-1760s, the government requested the Huron and Delaware to return runaway slaves, but there was no record of slaves having been returned. Colonists placed ads about runaway slaves.
While numerous tribes used captive enemies as servants and slaves, they also often adopted younger captives into their tribes to replace members who had died. In the Southeast, a few Native American tribes began to adopt a slavery system similar to that of the American colonists, buying African American slaves, especially the Cherokee, Choctaw, and Creek. Though less than 3% of Native Americans owned slaves, divisions grew among the Native Americans over slavery. Among the Cherokee, records show that slave holders in the tribe were largely the children of European men that had shown their children the economics of slavery. As European colonists took slaves into frontier areas, there were more opportunities for relationships between African and Native American peoples.
Based on the work of geneticists, a PBS series on African Americans explained that while most African Americans are racially mixed, it is relatively rare that they have Native American ancestry. According to the PBS series, the most common "non-black" mix is English and Scots-Irish. However, the Y-chromosome and mtDNA (mitochondrial DNA) testing processes for direct-line male and female ancestors can fail to pick up the heritage of many ancestors. (Some critics thought the PBS series did not sufficiently explain the limitations of DNA testing for assessment of heritage.)
Another study suggests that relatively few Native Americans have African-American heritage. A study reported in "The American Journal of Human Genetics" stated, "We analyzed the European genetic contribution to 10 populations of African descent in the United States (Maywood, Illinois; Detroit; New York; Philadelphia; Pittsburgh; Baltimore; Charleston, South Carolina; New Orleans; and Houston) ... mtDNA haplogroups analysis shows no evidence of a significant maternal Amerindian contribution to any of the 10 populations." A few writers persist in the myth that most African Americans have Native American heritage.
DNA testing has limitations and should not be depended on by individuals to answer all their questions about heritage. So far, such testing cannot distinguish among the many distinct Native American tribes. No tribes accept DNA testing to satisfy their differing qualifications for membership, usually based on documented blood quantum or descent from ancestor(s) listed on the Dawes Rolls.
Native American adoption of African slavery.
Native Americans interacted with enslaved Africans and African Americans on many levels. Over time all the cultures interacted. Native Americans began slowly to adopt white culture. Native Americans in the South shared some experiences with Africans, especially during the period, primarily in the 17th century, when both were enslaved. The colonists along the Atlantic Coast had begun enslaving Native Americans to ensure a source of labor. At one time the slave trade was so extensive that it caused increasing tensions with the various Algonquian tribes, as well as the Iroquois. Based in New York and Pennsylvania, they had threatened to attack colonists on behalf of the related Iroquoian Tuscarora before they migrated out of the South in the early 1700s.
In the 1790s, Benjamin Hawkins was assigned as the U.S. agent to the southeastern tribes, who became known as the Five Civilized Tribes for their adoption of numerous Anglo-European practices. He advised the tribes to take up slaveholding to aid them in European-style farming and plantations. He thought their traditional form of slavery, which had looser conditions, was less efficient than chattel slavery. In the 19th century, some members of these tribes who were more closely associated with settlers, began to purchase African-American slaves for workers. They adopted some European-American ways to benefit their people.
From the late 1700s to the 1860s, the Five Civilized Tribes were involved in the institution of African slavery as planters. For example, Cherokee leader Joseph Vann owned more than 100 slaves. The proportion of Cherokee families who owned slaves did not exceed ten percent, and was comparable to the percentage among white families across the South, where a slaveholding elite owned most of the laborers.
The writer William Loren Katz contends that Native Americans treated their slaves better than did the typical white American in the Deep South. Though less than 3% of Native Americans owned slaves, bondage created destructive cleavages among those who were slaveholders. Among the Five Civilized Tribes, mixed-race slaveholders were generally part of an elite hierarchy, often based on their mothers' clan status, as the societies had matrilineal systems. As did Benjamin Hawkins, European fur traders and colonial officials tended to marry high-status women, in strategic alliances seen to benefit both sides. The Choctaw, Creek and Cherokee believed they benefited from stronger alliances with the traders and their societies. The women's sons gained their status from their mother's families; they were part of hereditary leadership lines who exercised power and accumulated personal wealth in their changing Native American societies. The historian Greg O'Brien calls them the Creole generation to show that they were part of a changing society. The chiefs of the tribes believed that some of the new generation of mixed-race, bilingual chiefs would lead their people into the future and be better able to adapt to new conditions influenced by European Americans.
Proposals for Indian Removal heightened the tensions of cultural changes, due to the increase in the number of mixed-race Native Americans in the South. Full bloods, who tended to live in areas less affected by colonial encroachment, generally worked to maintain traditional ways, including control of communal lands. While the traditional members often resented the sale of tribal lands to Anglo-Americans, by the 1830s they agreed it was not possible to go to war with the colonists on this issue.
Who are Native Americans?
Admixture and genetics.
Intertribal mixing was common among many Native American tribes prior to European contact, as they would adopt captives taken in warfare. Individuals often had ancestry from more than one tribe, particularly after tribes lost so many members from disease in the colonial era and after. Bands or entire tribes occasionally split or merged to form more viable groups in reaction to the pressures of climate, disease and warfare.
A number of tribes traditionally adopted captives into their group to replace members who had been captured or killed in battle. Such captives were from rival tribes and later were taken from raids on European settlements. Some tribes also sheltered or adopted white traders and runaway slaves, and others owned slaves of their own. Tribes with long trading histories with Europeans show a higher rate of European admixture, reflecting years of intermarriage between Native American women and European men, often seen as advantageous to both sides. A number of paths to genetic and ethnic diversity among Native Americans have occurred.
In recent years, genetic genealogists have been able to determine the proportion of Native American ancestry carried by the African-American population. The literary and history scholar Henry Louis Gates, Jr., had experts on his TV programs who discussed African-American ancestry. They stated that 5% of African Americans have at least 12.5% Native American ancestry, or the equivalent to one great-grandparent, which may represent more than one distant ancestor. A greater percentage could have a smaller proportion of Indian ancestry, but their conclusions show that popular estimates of Native American admixture may have been too high.
DNA testing is not sufficient to qualify a person for specific tribal membership, as it cannot distinguish among Native American tribes.
Native American identity has historically been based on culture, not just biology, as many American Indian peoples adopted captives from their enemies and assimilated them into their tribes. The Indigenous Peoples Council on Biocolonialism (IPCB) notes that:
"Native American markers" are not found solely among Native Americans. While they occur more frequently among Native Americans, they are also found in people in other parts of the world.
Geneticists state:Not all Native Americans have been tested; especially with the large number of deaths due to disease such as smallpox, it is unlikely that Native Americans only have the genetic markers they have identified [so far], even when their maternal or paternal bloodline does not include a [known] non-Native American.
Tribal membership.
To receive tribal services, a Native American must be a certified (or enrolled) member of a federally recognized tribal organization. Each tribal government makes its own rules for eligibility of citizens or tribal members. Among tribes, qualification for enrollment may be based upon a required percentage of Native American "blood" (or the "blood quantum") of an individual seeking recognition, or documented descent from an ancestor on the Dawes Rolls or other registers. But, the federal government has its own standards related to who qualifies for services available to certified Native Americans. For instance, federal scholarships for Native Americans require the student both to be enrolled in a federally recognized tribe "and" to be of at least one-quarter Native American descent (equivalent to one grandparent), attested to by a Certificate of Degree of Indian Blood (CDIB) card issued by the federal government.
Some tribes have begun requiring genealogical DNA testing of individuals' applying for membership, but this is usually related to an individual's proving parentage or direct descent from a certified member. Requirements for tribal membership vary widely by tribe. The Cherokee require documented direct genealogical descent from a Native American listed on the early 1906 Dawes Rolls. Tribal rules regarding recognition of members who have heritage from multiple tribes are equally diverse and complex.
Tribal membership conflicts have led to a number of legal disputes, court cases, and the formation of activist groups. One example of this are the Cherokee Freedmen. Today, they include descendants of African Americans once enslaved by the Cherokees, who were granted, by federal treaty, citizenship in the historic Cherokee Nation as freedmen after the Civil War. The modern Cherokee Nation, in the early 1980s, passed a law to require that all members must prove descent from a Cherokee Native American (not Cherokee Freedmen) listed on the Dawes Rolls, resulting in the exclusion of some individuals and families who had been active in Cherokee culture for years.
Increased self-identification.
Since the census of 2000, people may identify as being of more than one race. Since the 1960s, the number of people claiming Native American ancestry has grown significantly and by the 2000 census, the number had more than doubled. Sociologists attribute this dramatic change to "ethnic shifting" or "ethnic shopping"; they believe that it reflects a willingness of people to question their birth identities and adopt new ethnicities which they find more compatible.
The author Jack Hitt writes:
The reaction from lifelong Indians runs the gamut. It is easy to find Native Americans who denounce many of these new Indians as members of the wannabe tribe. But it is also easy to find Indians like Clem Iron Wing, an elder among the Lakota, who sees this flood of new ethnic claims as magnificent, a surge of Indians 'trying to come home.' Those Indians who ridicule Iron Wing's lax sense of tribal membership have retrofitted the old genocidal system of blood quantum—measuring racial purity by blood—into the new standard for real Indianness, a choice rich with paradox.
The journalist Mary Annette Pember notes that identifying with Native American culture may be a result of a person's increased interest in genealogy, the romanticization of the lifestyle, and a family tradition of Native American ancestors in the distant past. There are different issues if a person wants to pursue enrollment as a member of a tribe. Different tribes have different requirements for tribal membership; in some cases persons are reluctant to enroll, seeing it as a method of control initiated by the federal government; and there are individuals who are 100% Native American but, because of their mixed tribal heritage, do not qualify to belong to any individual tribe. Pember concludes:
The subjects of genuine American Indian blood, cultural connection and recognition by the community are extremely contentious issues, hotly debated throughout Indian country and beyond. The whole situation, some say, is ripe for misinterpretation, confusion and, ultimately, exploitation.
Genetics.
The genetic history of indigenous peoples of the Americas primarily focuses on human Y-chromosome DNA haplogroups and human mitochondrial DNA haplogroups. "Y-DNA" is passed solely along the patrilineal line, from father to son, while "mtDNA" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal "atDNA" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. Autosomal DNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations.
The genetic pattern indicates Indigenous Americans experienced two very distinctive genetic episodes; first with the initial-peopling of the Americas, and secondly with European colonization of the Americas. The former is the determinant factor for the number of gene lineages, zygosity mutations and founding haplotypes present in today's Indigenous Amerindian populations.
Human settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain Amerindian populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q-M242 (Y-DNA) mutations, however, that are distinct from other indigenous Amerindians, and that have various mtDNA and atDNA mutations. This suggests that the paleo-Indian migrants into the northern extremes of North America and Greenland were descended from a later, independent migrant population.
Further reading.
</dl>

</doc>
<doc id="21218" url="http://en.wikipedia.org/wiki?curid=21218" title="Nights into Dreams...">
Nights into Dreams...

Nights into Dreams... (ナイツ, Naitsu, written as NiGHTS into Dreams...) is an action game released by Sega in 1996 for the Sega Saturn video game system. The game's story follows two children entering a dream world, where they are aided by the titular main character, Nights. "Nights" was developed by Sonic Team, with Yuji Naka as producer and lead programmer, and Naoto Ōshima as the character designer and director. The developers' intention was to make the sensation of flight the central gameplay element, achieved through 3D graphics and a combination of both 2D and 3D gameplay.
The game was re-released for the Sony PlayStation 2 on February 21, 2008 exclusively in Japan. A high-definition version was released worldwide for PlayStation Network and Xbox Live Arcade in October 2012, with a Microsoft Windows version later released via Steam on December 17, 2012. A direct sequel, "", was released for the Wii in 2007.
Story.
Every night, all human dreams are played out in Nightopia and Nightmare, the two parts of the dream world. In Nightopia, distinct aspects of dreamers' personalities are represented by luminous colored spheres known as "Ideya". However, the evil ruler of Nightmare, Wizeman the Wicked, is stealing this dream energy from sleeping visitors to gather power to take control of Nightopia and eventually the real world. To achieve this, he creates numerous beings called "Nightmaren", including two "Level One" Nightmaren, acrobatic jester-like, flight-capable beings called Nights and Reala. However, Nights rebels against Wizeman's plans, and is punished by being imprisoned inside an Ideya palace, a gazebo-like container for dreamers' Ideya.
One day, Elliot Edwards and Claris Sinclair, two children from the city of Twin Seeds, go through failures. Elliot likes to play basketball, but is challenged by kids from another grade and loses. Claris wants to sing in a play but is overcome by stage-fright in front of the judges. That night, they both suffer nightmares that replay the events. They escape into Nightopia and find that they both possess the rare Red Ideya of Courage, the only type Wizeman cannot steal. They release Nights, who tells them about dreams, and Wizeman and his plans, and the three begin a journey to stop Wizeman and restore peace to Nightopia.
Gameplay.
"NiGHTS into Dreams..." is split into seven levels, referred to as "Dreams". The levels are distributed equally between the two child characters; three are unique to Claris, three to Elliot, and each play through an identical final seventh level, "Twin Seeds". Initially, only Claris' Spring Valley and Elliot's Splash Garden are available, and successful completion of one of these unlocks the next level in that child's path. Previously completed stages may be revisited to improve the player's high scores; a "C" grade in all the selected child's levels must be achieved to unlock the relevant Twin Seeds stage for that character.
Each level is split up into four "Mares" set in Nightopia and a boss fight which takes place in Nightmare. In each level, players initially control Claris or Elliot, who immediately have their Ideyas of hope, growth, intelligence and purity stolen from them by Wizeman's minions, leaving behind only their Ideya of courage. The goal of each Mare is to recover one of the stolen Ideya by collecting 20 blue chips and delivering them to the cage holding the Ideya, which will overload and release the orb it holds. It is possible to complete some of the levels' goals by wandering around the landscape of Nightopia as Claris or Elliot (pursued by an egg-shaped alarm clock which will wake up the child and end the level if it catches the player), but the majority of the gameplay centers on Nights' flying sequences, triggered by walking into the Ideya Palace near the start of each level so that the child merges with the imprisoned Nights.
In the flying sections, the player controls Nights' flight along a particular predetermined route through each Mare. Players can only fly in the 2D plane of the screen, with their actual motion through the level determined by the automatic camera angle at that point in the Mare. The player has only a limited period available before Nights falls to the ground and turns back into Claris or Elliot, and each collision with an enemy subtracts five seconds from the time remaining. After retrieving the Ideya in a Mare, the player can either return to the Ideya Palace to progress to the next Mare, or continue flying more laps of the Mare (with replenished items) to greatly increase the points score. Scoring points is the main source of the games' replay value. The ability to fly more laps was removed from the sequel, "Journey of Dreams".
Whilst flying, Nights can use a boost to travel faster, as well as defeat certain enemies littered across the field. Grabbing onto certain enemies will cause Nights to spin around, launching Nights and the enemy in the direction the boost is pressed. Various acrobatic maneuvers can be performed, including the "Paraloop", whereby flying around in a complete circle and connecting the trail of stars left in Nights' wake will cause any items within the loop to be attracted towards Nights. The game features a combo system known as "Linking", whereby actions such as collecting items and flying through rings are worth more points when performed in quick succession than they are individually. Flying through certain special rings activates a short period of time in which extra points can be earned through the performance of various aerobatic stunts.
At the end of each Mare, players are given a rank (between A and F) based on their score, and after all four Mares are cleared an overall rank for the level is displayed. Nights is then transported to Nightmare for a boss fight against one of Wizeman's Nightmarens. Each boss fight has a time limit, and the game will end if the player runs out of time during the battle. Upon winning the boss fight, the player is awarded a score multiplier based on how quickly the boss was defeated, which is then applied to the score earned in the Nightopia section in order to produce the player's final score for that Dream.
A-Life.
Aside from the immediate game mission, the game also contains an artificial life ("A-Life") system, a precursor to the Chao featured in Sonic Team's later "Sonic Adventure" titles. The system involves entities called Nightopians. The game keeps track of the moods of the Nightopians (harming them will displease them, for example), and the game features an evolving music engine, allowing tempo, pitch, and melody to alter depending on the state of Nightopians within the level. It is also possible to merge Nightopians with the small Nightmaren enemies, creating a hybrid being called a Mepian. It is even possible through extensive controlled breeding to produce a "King" Pian, or Superpian. An expanded version of the A-life system was included in "".
3D controller.
"Nights into Dreams..." was introduced alongside an optional game controller, included with most copies of the game, called the Saturn 3D controller. This gamepad features an analog stick (launching just after the release of the Nintendo 64 in Japan, which features a control stick on its standard controller) and analog triggers.
Music.
The game's music has proven to be quite popular and has been remixed several times, both professionally and by fans. It also makes an appearance in Sega's Phantasy Star Online games, as bonus music for completing a quest themed after the original Nights game. The music from the levels was generated in-game using the Saturn version of Invision's Cybersound, which was also used in other games such as "Panzer Dragoon II Zwei" and "Panzer Dragoon Saga".
Several versions of the "Nights" theme song "Dreams Dreams" appear throughout the games in the series. The song is a duet between a man and a woman, which features a call and answer chorus. The vocal versions of the song featured in this game are the adults' version, sung by Curtis King, Jr., & Dana Calitri; and the children's version, sung by Cameron Earl Strother and Jasmine Ann Allen, which segues into the adults' version after the bridge. "Christmas Nights" features an a cappella version sung by Marlon Saunders, Gabriel Morris and Issa Clemon.
Development.
The concept for the game originated during the development of "Sonic the Hedgehog 2". Yuji Naka recounted, "[I] headed back to Japan so that I could work with Mr. Oshima and while I was waiting for the plane to take off, I thought, 'Let's make a game where we can fly!' So I guess that's where it all started." Naoto Ohshima added, "We had a lot of different ideas about how to portray the whole idea of flying. One idea was that a little bird who couldn't fly grows into a big bird and then has the freedom of being able to fly. But we decided against the whole animal related idea as this would be a direct comparison to Sonic. So we decided a more human like character was a better idea."
Ports.
Remake.
Sega released a remake of "Nights into Dreams..." for the Sony PlayStation 2 in Japan on February 21, 2008. It includes 16:9 wide screen support, both a classic Sega Saturn graphics mode and a Sony PS2 remake mode, an illustration gallery and a movie viewer mode. The game is available for a budget price, or the Nightopia Dream Pack, which includes a reprint of a picture book that was released in Japan alongside the original Saturn game. The Christmas Nights levels are included as an unlockable bonus, playable after the main game is completed. In keeping with the time-oriented surprises of the Saturn original, the remake features Nights, Elliot and Claris wearing special seasonal clothing during certain holidays, such as party costumes during Halloween in Mystic Forest or swim-wear during two special days in summer in Splash Garden.
The Sony PlayStation 2 version of the game was released only in Japan.
HD version.
A high definition port of the PS2 version was released for PlayStation Network on October 2, 2012 and for Xbox Live Arcade on October 5, 2012. A Microsoft Windows version was released via Steam on December 17, 2012. This version of the game introduces online high score leaderboards, and, like the PS2 version, includes the option to play either with enhanced graphics or with the original Saturn version's graphics. The HD version also includes "Christmas Nights"; but like before, the original game's two player versus mode, and "Christmas Nights"‍ '​ Sonic the Hedgehog level were removed.
Reception.
The game was previewed in the May 1996 issue of "Sega Saturn Magazine", which compared it to "Super Mario 64" and described "Nights" as "one of the best games you've ever seen in your life." It was also previewed in the June 1996 issue of "Computer and Video Games", which described it as "one of the most impressive and innovative 3D games we've ever seen" and noted that an analogue controller "similar to the N64 pad" was "designed for it."
At the time of its release, "Nights into Dreams" was the top-selling game for the Sega Saturn, and was the 21st highest selling game in Japan for the year 1996.
"Nights" has appeared in several "greatest games" lists. In a January 2000 poll of readers of "Computer and Video Games" magazine, it appeared in 15th place in a list of the "100 greatest games" (directly behind "Super Mario 64"). "Edge" gave it a score of 8/10 in its original 1996 review of the game, and in its October 2003 issue the magazine's staff placed "Nights" in its list of the ten greatest platform games. "Electronic Gaming Monthly", in its "The Greatest 200 Videogames of Their Time" list, ranked the game 160th. "Next Generation Magazine" ranked the game 25th in its list of the "100 Greatest Games of All-Time" in the September 1996 issue. 1UP.com ranked the Game 3rd in its "Top Ten Cult Classics." IGN's 2007 "Top 100 Games of All Time!" list ranked the game at 94/100, and in 2008, IGN staff writer Levi Buchanan ranked it 4th in his list of the top 10 Sega Saturn games.
Eurogamer gave the HD re-release a 9/10, opining that "perhaps Nights is Sonic Team's masterpiece," while acknowledging that the game "is still destined to be misunderstood by many" due to its "utterly unique" design.
Related games.
Christmas Nights.
"Christmas Nights" is a Christmas-themed two-level game of "Nights into Dreams..." that was released in December 1996. In Japan, it was part of a Christmas Sega Saturn bundle. Elsewhere it was given away with the purchase of select Sega Saturn games such as "Daytona USA Championship Circuit Edition", and was bundled in with issues of "Sega Saturn Magazine", "Game Players" and "Next Generation Magazine". The CD-ROMs given away on the front cover of "Sega Saturn Magazine" were missing the card slip case in which to keep the disc, and were instead supplied with a transparent plastic one. The game was also available for rent at Blockbuster Video locations in America. In the UK, "Christmas Nights" was not included with the official "Sega Saturn Magazine" until December 1997.
The "Christmas Nights" disc contains the full version of Claris' Spring Valley dream level from "Nights into Dreams", which allows both Claris and Elliot to play through the area, something Elliot could not do previously as it was not his dream. Elliot's version of the level contains a different item layout than Claris'.
The story of "Christmas Nights" follows Elliot and Claris during the holiday season following their adventures with Nights. Though they both enjoy the Christmas season, they feel as if something is missing. Finding that the Christmas Star that usually sits at the top of the Twin Seeds Christmas tree is missing, the pair head off to Nightopia to find it. There, they meet up with Nights again and re-explore Spring Valley, which has now been Christmas-ified due to the kids' dreams of the holiday season. The trio of heroes must now take down a revived Gillwing and retrieve the Christmas Star from his lair.
The game uses the Saturn's internal clock to change elements of the game according to the date and time. In November and January, the title screen label "Nights: Limited Edition" is changed to "Winter Nights", with the lush greenery of the environment being replaced by white snow. During December, "Christmas Nights" mode is activated, resulting in further Christmas-themed alterations, such as item boxes becoming Christmas presents, Nightopians dressing in elf costumes, and Christmas trees replacing Ideya captures. The background music is replaced by an instrumental version of "Jingle Bells", as well as there being a "Christmas-like" rendition of the boss theme. During the "Winter Nights" period, the weather in Spring Valley will change according to what hour it is. Other cosmetic changes are visible on New Year's Day and Halloween, and loading the game on April Fool's Day results in Reala replacing Nights as the playable character.
The disc features a number of unlockable bonuses such as character artwork. Further extra modes allow players to observe the status of the A-life system, experiment with the game's music mixer, time attack one Mare, or play the demo stage as Sega's mascot Sonic the Hedgehog. In the "Sonic the Hedgehog: Into Dreams" minigame, Sonic is only able to traverse the Spring Valley stage on foot, and the original game's Puffy boss is re-skinned as a "bouncy ball" version of Doctor Robotnik. The music is a slightly remixed version of "Final Fever", the final boss battle music from the Japanese and European version of "Sonic CD". The "Christmas Nights" content is playable in the HD version after the game has been cleared once.
Sequel.
Demand for a sequel to "Nights into Dreams..." has been strong for many years. A game with the working title "Air Nights" was intended to use a tilt sensor in the Saturn analog pad, and development later moved to the Dreamcast for a time, but eventually the project was discontinued. Aside from a handheld electronic game released by Tiger Electronics (which was also ported to Tiger's R-Zone console) and small minigames featured in several Sega titles, no full "Nights" sequel was released for a Sega console. Yuji Naka expressed his reluctance to develop a sequel, but also noted that he was interested in using "Nights" as a license "to reinforce Sega's identity".
On April 1, 2007, a sequel called "" was officially announced for the Wii. The official announcement followed items on the game published in several magazines and websites. The sequel is a Wii exclusive, making use of the system's motion-sensing controller, as was initially planned for "Air Nights". The gameplay involves the use of various masks, and features a multiplayer mode for two players in addition to WiFi online functions. The game was developed by Sega Studio USA, with Takashi Iizuka, one of the designers of the original game, as producer. It was released in Japan and the United States in December 2007, and in Europe and Australia on January 18, 2008.
In 2010, Takashi Iizuka commented that he would be very interested in making a third "Nights" game, should Sega's management decide to develop one.
Further appearances.
Claris and Eliot make a cameo appearance in Sonic Team's Burning Rangers, with Eliot sending the Rangers a message thanking them for saving him, and telling them about his friend, NiGHTS. A "Nights" handheld electronic game was released by Tiger Electronics, and a port of it was later released for Tiger's short lived R-Zone console. "Nights"-themed pinball areas feature in "Sonic Adventure" and "Sonic Pinball Party". The Sony PlayStation 2 EyeToy title, "Sega SuperStars", features a minigame based on "Nights" in which Nights is controlled using the player's body. Nights is an unlockable character in "Sonic Riders" and "". Nights and Reala also makes a cameo appearance in "Sonic Shuffle" when the Sega Dreamcast system's date is set to Christmas Eve or April Fools' Day, respectively. A minigame version of "Nights into Dreams..." is playable utilizing the Nintendo GameCube-to-Game Boy Advance connectivity with "Phantasy Star Online Episode I & II" and "Billy Hatcher and the Giant Egg". Following a successful fan campaign by , Nights was integrated into "Sonic & Sega All-Stars Racing" as the flagman, and Nights and Reala appear as playable characters in "Sega Superstars Tennis" and "Sonic & All-Stars Racing Transformed", the latter of which also features a "Nights into Dreams..."-themed racetrack. The limited Deadly Six edition of SEGA's "Sonic Lost World" features a NIGHTS-inspired stage, named ``Nightmare Zone``, as DLC. In this stage, Sonic must face off against the aforementioned Deadly Six in boss fights, each of the fights being from the game, albeit reskinned with boss characters from "Nights Into Dreams...". It also contains other features from "NIGHTS", such as one part of the stage being based on the flying sections.
Comics.
Archie Comics adapted "Nights into Dreams..." into a three-issue comic book miniseries to test whether or not a Nights comic would sell well in North America. The first mini-series was loosely based on the game, with Nights being specifically identified as a male despite the character's androgynous design. The company later released a second three-issue miniseries, continuing the story of the first, but the series didn't gain enough sales to warrant an ongoing series. The series would later be added to a list of guest franchises featured in Archie Comics' "Worlds Unite" crossover between its "Sonic the Hedgehog" and "Mega Man" titles. 

</doc>
<doc id="21220" url="http://en.wikipedia.org/wiki?curid=21220" title="Negligence per se">
Negligence per se

Negligence "per se" is the legal doctrine whereby an act is considered negligent because it violates a statute (or regulation). 
Elements.
In order to prove negligence "per se", the plaintiff usually must show that:
In some jurisdictions, negligence "per se" creates merely a rebuttable presumption of negligence.
A typical example is one in which a contractor violates a building code when constructing a house. The house then collapses, injuring somebody. The violation of the building code establishes negligence "per se" and the contractor will be found liable, so long as the contractor's breach of the code was the cause (proximate cause and actual cause) of the injury.
History.
A famous early case in negligence "per se" is "Gorris v. Scott", an 1874 Court of Exchequer case that established that the harm in question must be of the kind that the statute was intended to prevent. "Gorris" involved a shipment of sheep that was washed overboard but would not have been washed overboard had the shipowner complied with the regulations established pursuant to the Contagious Diseases (Animals) Act 1869, which required that livestock be transported in pens to segregate potentially-infected animal populations from uninfected ones. Chief Baron Fitzroy Kelly held that as the statute was intended to prevent the spread of disease, rather than the loss of livestock in transit, the plaintiff could not claim negligence "per se".
A subsequent New York Court of Appeals case, "Martin v. Herzog", penned by Judge Benjamin N. Cardozo, first presented the notion that negligence "per se" could be absolute evidence of negligence in certain cases.

</doc>
<doc id="21221" url="http://en.wikipedia.org/wiki?curid=21221" title="Neuromyotonia">
Neuromyotonia

Neuromyotonia (NMT), also known as Isaacs Syndrome and Isaacs-Merton syndrome, is a form of peripheral nerve hyperexcitability that causes spontaneous muscular activity resulting from repetitive motor unit action potentials of peripheral origin.
Causes.
The three causes of NMT are:
1) Acquired 
2) Paraneoplastic
3) Hereditary
The acquired form is the most common accounting for up to 80 percent of all cases and is suspected to be autoimmune-mediated, which is usually caused by antibodies against the neuromuscular junction.
The exact cause is unknown. However, autoreactive antibodies can be detected in a variety of peripheral (e.g. myasthenia gravis, Lambert-Eaton myasthenic syndrome) and central nervous system (e.g. paraneoplastic cerebellar degeneration, paraneoplastic limbic encephalitis) disorders. Their causative role has been established in some of these diseases but not all. Neuromyotonia is considered to be one of these with accumulating evidence for autoimmune origin over the last few years. Autoimmune neuromyotonia is typically caused by antibodies that bind to potassium channels on the motor nerve resulting in continuous/hyper-excitability. Onset is typically seen between the ages of 15-60, with most experiencing symptoms before the age of 40. Some neuromyotonia cases do not only improve after plasma exchange but they may also have antibodies in their serum samples against voltage-gated potassium channels. Moreover, these antibodies have been demonstrated to reduce potassium channel function in neuronal cell lines.
Presentation/Symptoms.
NMT is a diverse disorder. As a result of muscular hyperactivity patients may present with muscle cramps, stiffness, myotonia-like symptoms (slow relaxation), associated walking difficulties, hyperhidrosis (excessive sweating), myokymia (quivering of a muscle), fasciculations (muscle twitching), fatigue, exercise intolerance, myoclonic jerks and other related symptoms. The symptoms (especially the stiffness and fasciculations) are most prominent in the calves, legs, trunk, and sometimes the face and neck, but can also affect other body parts. NMT symptoms may fluctuate in severity and frequency. Symptoms range from mere inconvenience to debilitating. At least a third of people also experience sensory symptoms.
Types.
There are three main types of NMT:
Diagnosis.
Diagnosis is clinical and initially consists of ruling out more common conditions, disorders, and diseases, and usually begins at the general practitioner level. A doctor may conduct a basic neurological exam, including coordination, strength, reflexes, sensation, etc. A doctor may also run a series of tests that include blood work and MRIs.
From there, a patient is likely to be referred to a neurologist or a neuromuscular specialist. The neurologist or specialist may run a series of more specialized tests, including needle electromyography EMG/ and nerve conduction studies (NCS) (these are the most important tests), chest CT (to rule out paraneoplastic) and specific blood work looking for voltage-gated potassium channel antibodies, acetylcholine receptor antibody, and serum immunofixation, TSH, ANA ESR, EEG etc. Neuromyotonia is characterized electromyographically by doublet, triplet or multiplet single unit discharges that have a high, irregular intraburst frequency. Fibrillation potentials and fasciculations are often also present with electromyography.
Because the condition is so rare, it can often be years before a correct diagnosis is made.
NMT is not fatal and many of the symptoms can be controlled. However, because NMT mimics some symptoms of motor neuron disease (ALS) and other more severe diseases, which may be fatal, there can often be significant anxiety until a diagnosis is made. In some rare cases, acquired neuromyotonia has been misdiagnosed as amyotrophic lateral sclerosis (ALS) particularly if fasciculations may be evident in the absence of other clinical features of ALS. However, fasciculations are rarely the first sign of ALS as the hallmark sign is weakness. Similarly, Multiple Sclerosis has been the initial misdiagnosis in some NMT patients. In order to get an accurate diagnosis see a trained neuromuscular specialist.
Peripheral Nerve Hyperexcitability.
Neuromyotonia is a type of peripheral nerve hyperexcitability. Peripheral Nerve Hyperexcitability is an umbrella diagnosis that includes (in order of severity of symptoms from least severe to most severe) Benign Fasciculation Syndrome, Cramp Fasciculation Syndrome, and Neuromyotonia. Some doctors will only give the diagnosis of Peripheral Nerve Hyperexcitability as the differences between the three are largely a matter of the severity of the symptoms and can be subjective. However, some objective EMG criteria have been established to help distinguish between the three.
Moreover, the generic use of the term "Peripheral Nerve Hyperexcitability syndromes" to describe the aforementioned conditions is recommended and endorsed by several prominent researchers and practitioners in the field. (http://brain.oxfordjournals.org/cgi/reprint/125/8/1887.pdf)
Treatments.
There is no known cure for Neuromyotonia, but the condition is treatable. Anticonvulsants, including phenytoin and carbamazepine, usually provide significant relief from the stiffness, muscle spasms, and pain associated with Neuromyotonia. Plasma exchange and IVIg treatment may provide short-term relief for patients with some forms of the acquired disorder. It is speculated that the plasma exchange causes an interference with the function of the voltage-dependent potassium channels, one of the underlying issues of hyper-excitability in autoimmune Neuromyotonia. Botox injections also provide short-term relief. Immunosuppressants, such as Prednisone may provide long term relief for patients with some forms of the acquired disorder.
Prognosis.
The long-term prognosis for individuals with the disorder is uncertain, and this has mostly to do with the etiology (underlying cause; i.e. autoimmune, paraneoplastic, etc.) and lack of research for this disorder. However, in recent years our increased understanding of the basic mechanisms of NMT and autoimmunity has led to the development of novel treatment strategies. NMT disorders are now amenable to treatment and their prognoses are good. Many patients respond well to treatment, which usually provide significant relief of symptoms. Some cases of spontaneous remission have been noted, including Isaac's original two patients when followed up 14 years later.
While NMT symptoms may fluctuate, they generally don't deteriorate into anything more serious and with the correct treatment the symptoms are manageable.
A very small proportion of cases with NMT may develop central nervous system findings in their clinical course, causing a disorder called Morvan's syndrome and they may also have antibodies against potassium channels in their serum samples. Sleep disorder is only one of a variety of clinical conditions observed in Morvan's syndrome cases ranging from confusion and memory loss to hallucinations and delusions. However, this is a separate disorder.
Some studies have linked NMT with certain types of cancers, mostly lung and thymus, suggesting that NMT may be paraneoplastic in some cases. In these cases, the underlying cancer will determine prognosis. However, most examples of NMT are autoimmune and not associated with cancer.

</doc>
<doc id="21224" url="http://en.wikipedia.org/wiki?curid=21224" title="Napoleon (disambiguation)">
Napoleon (disambiguation)

Napoleon (1769–1821) also known as Napoleon Bonaparte or Napoleon I, was a French military leader and emperor.
Napoleon (English) or Napoléon (French) or Napoleón (Spanish) or Napoleone (Italian) may also refer to:

</doc>
<doc id="21226" url="http://en.wikipedia.org/wiki?curid=21226" title="Neurology">
Neurology

Neurology (from Greek: νεῦρον, "neuron", and the suffix
-λογία -logia "study of") is a branch of medicine dealing with disorders of the nervous system. Neurology deals with the diagnosis and treatment of all categories of conditions and disease involving the central and peripheral nervous system (and its subdivisions, the autonomic nervous system and the somatic nervous system); including their coverings, blood vessels, and all effector tissue, such as muscle. Neurological practice relies heavily on the field of neuroscience, which is the scientific study of the nervous system.
A neurologist is a physician specializing in neurology and trained to investigate, or diagnose and treat neurological disorders. Neurologists may also be involved in clinical research, clinical trials, and basic or translational research. While neurology is a non-surgical specialty, its corresponding surgical specialty is neurosurgery.
Scope.
A large number of neurological disorders have been described. These can affect the central nervous system (brain and spinal cord), the peripheral nervous system, the autonomic nervous system and the muscular system.
Training.
In the United States and Canada, neurologists are physicians having completed postgraduate training in neurology after graduation from medical school. Neurologists complete, on average, at least 10–13 years of college education and clinical training. This training includes obtaining a four-year undergraduate degree, a medical degree (D.O. or M.D.), which comprises an additional four years of study, and then completing a one-year internship and a three- or four-year residency in neurology. The four-year residency consists of one year of internal medicine training followed by three years of training in neurology.
Some neurologists receive additional subspecialty training focusing on a particular area of neurology. These training programs are called fellowships, and are one to two years in duration. Sub-specialties include: 
brain injury medicine, clinical neurophysiology, epilepsy, hospice and palliative medicine, neurodevelopmental disabilities, neuromuscular medicine, pain medicine and sleep medicine, vascular neurology (stroke), behavioral neurology, child neurology, headache, multiple sclerosis, neuroimaging, neurorehabilitation, and interventional neurology.
In Germany, a compulsory year of psychiatry must be done to complete a residency of neurology.
In the United Kingdom and Ireland, neurology is a subspecialty of general (internal) medicine. After five to nine years of medical school and a year as a pre-registration house officer (or two years on the Foundation Programme), a neurologist must pass the examination for Membership of the Royal College of Physicians (or the Irish equivalent) before completing two years of core medical training and then entering specialist training in neurology. A generation ago, some neurologists would have also spent a couple of years working in psychiatric units and obtain a Diploma in Psychological Medicine. However, this requirement has become uncommon, and, now that a basic psychiatric qualification takes three years to obtain, the requirement is no longer practical. A period of research is essential, and obtaining a higher degree aids career progression: Many found it was eased after an attachment to the Institute of Neurology at Queen Square in London. Some neurologists enter the field of rehabilitation medicine (known as physiatry in the US) to specialise in neurological rehabilitation, which may include stroke medicine as well as brain injuries.
Physical examination.
During a neurological examination, the neurologist reviews the patient's health history with special attention to the current condition. The patient then takes a neurological exam. Typically, the exam tests mental status, function of the cranial nerves (including vision), strength, coordination, reflexes, and sensation. This information helps the neurologist determine whether the problem exists in the nervous system and the clinical localization.mm Localization of the pathology is the key process by which neurologists develop their differential diagnosis. Further tests may be needed to confirm a diagnosis and ultimately guide therapy and appropriate management.
Clinical tasks.
Neurologists examine patients who have been referred to them by other physicians in both the inpatient and outpatient settings. A neurologist will begin their interaction with a patient by taking a comprehensive medical history, and then perform a physical examination focusing on evaluating the nervous system. Components of the neurological examination include assessment of the patient's cognitive function, cranial nerves, motor strength, sensation, reflexes, coordination, and gait.
In some instances, neurologists may order additional diagnostic tests as part of the evaluation. Commonly employed tests in neurology include imaging studies such as computed axial tomography (CAT) scans, magnetic resonance imaging (MRI), and ultrasound of major blood vessels of the head and neck. Neurophysiologic studies, including electroencephalography (EEG), needle electromyography (EMG), nerve conduction studies (NCSs) and evoked potentials are also commonly ordered. Neurologists frequently perform lumbar punctures in order to assess characteristics of a patient's cerebrospinal fluid. Advances in genetic testing has made genetic testing and important tool in the classification of inherited neuromuscular disease. The role of genetic influences on the development of acquired neuromuscular diseases is an active area of research.
Some of the commonly encountered conditions treated by neurologists include headaches, radiculopathy, neuropathy, stroke, dementia, seizures and epilepsy, Alzheimer's Disease, Attention deficit/hyperactivity disorder, Parkinson's Disease, Tourette's syndrome, multiple sclerosis, head trauma, sleep disorders, neuromuscular diseases, and various infections and tumors of the nervous system. Neurologists are also asked to evaluate unresponsive patients on life support in order to confirm brain death.
Treatment options vary depending on the neurological problem. They can include everything from referring the patient to a physiotherapist, to prescribing medications, to recommending a surgical procedure.
Some neurologists specialize in certain parts of the nervous system or in specific procedures. For example, clinical neurophysiologists specialize in the use of EEG and intraoperative monitoring in order to diagnose certain neurological disorders. Other neurologists specialize in the use of electrodiagnostic medicine studies - needle EMG and NCSs. In the US, physicians do not typically specialize in all the aspects of clinical neurophysiology - i.e. sleep, EEG, EMG, and NCSs. The American Board of Clinical Neurophysiology certifies US physicians in general clinical neurophysiology, epilepsy, and intraoperative monitoring. The American Board of Electrodiagnostic Medicine certifies US physicians in electrodiagnostic medicine and certifies technologists in nerve conduction studies. Sleep medicine is a subspecialty field in the US under several medical specialties including anesthesiology, internal medicine, family medicine, and neurology. Neurosurgery is a distinct specialty that involves a different training path, and emphasizes the surgical treatment of neurological disorders.
There are also many non-medical doctors, those with PhD degrees in subjects such as biology and chemistry, who study and research the nervous system. Working in labs in universities, hospitals, and private companies, these neuroscientists perform clinical and laboratory experiments and tests in order to learn more about the nervous system and find cures or new treatments for diseases and disorders.
There is a great deal of overlap between neuroscience and neurology. A large number of neurologists work in academic training hospitals, where they conduct research as neuroscientists in addition to treating patients and teaching neurology to medical students.
General caseload.
Neurologists are responsible for the diagnosis, treatment, and management of all the conditions mentioned above. When surgical intervention is required, the neurologist may refer the patient to a neurosurgeon. In some countries, additional legal responsibilities of a neurologist may include making a finding of brain death when it is suspected that a patient has died. Neurologists frequently care for people with hereditary (genetic) diseases when the major manifestations are neurological, as is frequently the case. Lumbar punctures are frequently performed by neurologists. Some neurologists may develop an interest in particular subfields, such as stroke, dementia, movement disorders, neurointensive care, headaches, epilepsy, sleep disorders, chronic pain management, multiple sclerosis, or neuromuscular diseases.
Overlapping areas.
There is some overlap with other specialties, varying from country to country and even within a local geographic area. Acute head trauma is most often treated by neurosurgeons, whereas sequelae of head trauma may be treated by neurologists or specialists in rehabilitation medicine. Although stroke cases have been traditionally managed by internal medicine or hospitalists, the emergence of vascular neurology and interventional neurologists has created a demand for stroke specialists. The establishment of JCAHO certified stroke centers has increased the role of neurologists in stroke care in many primary as well as tertiary hospitals. Some cases of nervous system infectious diseases are treated by infectious disease specialists. Most cases of headache are diagnosed and treated primarily by general practitioners, at least the less severe cases. Likewise, most cases of sciatica and other mechanical radiculopathies are treated by general practitioners, though they may be referred to neurologists or a surgeon (neurosurgeons or orthopedic surgeons). Sleep disorders are also treated by pulmonologists and psychiatrists. Cerebral palsy is initially treated by pediatricians, but care may be transferred to an adult neurologist after the patient reaches a certain age. Physical medicine and rehabilitation physicians also in the US diagnosis and treat patients with neuromuscular diseases through the use of electrodiagnostic studies (needle EMG and nerve conduction studies) and other diagnostic tools. In the United Kingdom and other countries, many of the conditions encountered by older patients such as movement disorders including Parkinson's Disease, stroke, dementia or gait disorders are managed predominantly by specialists in geriatric medicine.
Clinical neuropsychologists are often called upon to evaluate brain-behavior relationships for the purpose of assisting with differential diagnosis, planning rehabilitation strategies, documenting cognitive strengths and weaknesses, and measuring change over time (e.g., for identifying abnormal aging or tracking the progression of a dementia).
Relationship to clinical neurophysiology.
In some countries, e.g. USA and Germany, neurologists may subspecialize in clinical neurophysiology, the field responsible for EEG and intraoperative monitoring, or in electrodiagnostic medicine nerve conduction studies, EMG and evoked potentials. In other countries, this is an autonomous specialty (e.g., United Kingdom, Sweden, Spain).
Overlap with Psychiatry.
Although mental illnesses are believed by many to be neurological disorders affecting the central nervous system, traditionally they are classified separately, and treated by psychiatrists. In a 2002 review article in the American Journal of Psychiatry, Professor Joseph B. Martin, Dean of Harvard Medical School and a neurologist by training, wrote that "the separation of the two categories is arbitrary, often influenced by beliefs rather than proven scientific observations. And the fact that the brain and mind are one makes the separation artificial anyway".
Neurological diseases often have psychiatric manifestations, such as post-stroke depression, depression and dementia associated with Parkinson's disease, mood and cognitive dysfunctions in Alzheimer's disease and Huntington disease, to name a few. Hence, there is not always a sharp distinction between neurology and psychiatry on a biological basis. The dominance of psychoanalytic theory in the first three quarters of the 20th century has since then been largely replaced by a focus on pharmacology. Despite the shift to a medical model, brain science has not advanced to the point where scientists or clinicians can point to readily discernible pathologic lesions or genetic abnormalities that in and of themselves serve as reliable or predictive biomarkers of a given mental disorder.
Neurological enhancement.
The emerging field of neurological enhancement highlights the potential of therapies to improve such things as workplace efficacy, attention in school, and overall happiness in personal lives. However, this field has also given rise to questions about neuroethics and the psychopharmacology of lifestyle drugs.

</doc>
<doc id="21227" url="http://en.wikipedia.org/wiki?curid=21227" title="Nu">
Nu

Nu or NU may refer to:
Burmese people.
"Nu" is a common Burmese name and may refer to:

</doc>
