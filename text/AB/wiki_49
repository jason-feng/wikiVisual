<doc id="19577" url="http://en.wikipedia.org/wiki?curid=19577" title="Moses">
Moses

Moses (; Hebrew: מֹשֶׁה‎, "Moshe" "Mōšéh" "Moše"; Syriac: ܡܘܫܐ "Moushe"; Arabic: موسى‎ "Mūsā"; Greek: Mωϋσῆς "Mōÿsēs" in both the Septuagint and the New Testament) was, according to the Hebrew Bible, a former Egyptian prince later turned prophet, religious leader and lawgiver, to whom the authorship of the Torah is traditionally attributed. Also called "Moshe Rabbenu" in Hebrew (מֹשֶׁה רַבֵּנוּ, "Lit." "Moses our Teacher/Rabbi"), he is the most important prophet in Judaism. He is also an important prophet in Christianity and Islam, as well as a number of other faiths.
The existence of Moses as well as the veracity of the Exodus story are disputed among archaeologists and Egyptologists, with experts in the field of biblical criticism citing logical inconsistencies, new archaeological evidence, historical evidence, and related origin myths in Canaanite culture. Other historians maintain that the biographical details and Egyptian background attributed to Moses imply the existence of a historical political and religious leader who was involved in the consolidation of the Hebrew tribes in Canaan towards the end of the Bronze Age.
According to the Book of Exodus, Moses was born in a time when his people, the Israelites, were increasing in numbers and the Egyptian Pharaoh was worried that they might ally with Egypt's enemies. Moses' Hebrew mother, Jochebed, secretly hid him when the Pharaoh ordered all newborn Hebrew boys to be killed in order to reduce the population of the Israelites. Through the Pharaoh's daughter (identified as Queen Bithia in the Midrash), the child was adopted as a foundling from the Nile river and grew up with the Egyptian royal family. After killing an Egyptian slavemaster (because the slavemaster was smiting a Hebrew), Moses fled across the Red Sea to Midian, where he encountered the God of Israel speaking to him from within a "burning bush which was not consumed by the fire" on Mount Horeb (which he regarded as the Mountain of God).
God sent Moses back to Egypt to demand the release of the Israelites from slavery. Moses said that he could not speak with assurance or eloquence, so God allowed Aaron, his brother, to become his spokesperson. After the Ten Plagues, Moses led the Exodus of the Israelites out of Egypt and across the Red Sea, after which they based themselves at Mount Sinai, where Moses received the Ten Commandments. After 40 years of wandering in the desert, Moses died within sight of the Promised Land.
Rabbinical Judaism calculated a lifespan of Moses corresponding to 1391–1271 BCE; Jerome gives 1592 BCE, and Ussher 1571 BCE as his birth year.
In a metaphorical sense in the Christian tradition, a "Moses" is the leader who delivers the people from a terrible situation: when Abraham Lincoln was assassinated in 1865 after freeing the slaves, black Americans said they had lost "their Moses" and Harriet Tubman, who rescued approximately seventy enslaved family and friends, was described as the "Moses" of her people.
Name.
Moses' name is given to him by Pharaoh's daughter: "He became her son, and she named him Moshe (Moses)." This name may be either Egyptian or Hebrew. If connected to an Egyptian root, via "msy" "to be born" and "ms", "a son", it forms a wordplay: "he became her son, and she named him Son." There should, however, be a divine element to the name Moses (bearers of the Egyptian name are the "son of" a god, as in Thutmose, "son of Thut"), and his full name may therefore have included the name of one of the Egyptian gods. Most scholars agree that the name is Egyptian, and that the Hebrew etymology is a later interpretation, but if the name is from a Hebrew root then it is connected to the verb "to draw out": "I drew him ("masha") out of the water," states Pharaoh's daughter, possibly looking forward to Moses at the well in Midian, or to his role in saving Israel at the Red Sea.
Biblical narrative.
Prophet and deliverer of Israel.
The Israelites had settled in the Land of Goshen in the time of Joseph and Jacob, but a new pharaoh arose who oppressed the children of Israel. At this time Moses was born to his father Amram, son of Kohath the Levite, who entered Egypt with Jacob's household; his mother was Jochebed (also Yocheved), who was kin to Kohath. Moses had one older (by seven years) sister, Miriam, and one older (by three years) brother, Aaron.
Pharaoh had commanded that all male Hebrew children born be drowned in the river Nile, but Moses' mother placed him in an ark and concealed the ark in the bulrushes by the riverbank, where the baby was discovered and adopted by Pharaoh's daughter. One day after Moses had reached adulthood he killed an Egyptian who was beating a Hebrew. Moses, in order to escape Pharaoh's death penalty, fled to Midian (a desert country south of Judah). 
There, on Mount Horeb, God revealed to Moses his name YHWH (probably pronounced Yahweh) and commanded him to return to Egypt and bring his Chosen People (Israel) out of bondage and into the Promised Land (Canaan). Moses returned to carry out God's command, but God caused Pharaoh to refuse, and only after God had subjected Egypt to ten plagues did Pharaoh relent. Moses led the Israelites to the border of Egypt, but there God hardened Pharaoh's heart once more, so that he could destroy Pharaoh and his army at the Red Sea Crossing as a sign of his power to Israel and the nations.
From Egypt, Moses led the Israelites to Mount Sinai, where he was given ten commandments from God, written on stone tablets. However, since Moses remained a long time on the mountain, some of the people feared that he might be dead, so they made a golden statue of a calf and worshipped it, thus disobeying and angering God and Moses, the latter, out of anger, broke the tablets. Moses later ordered the elimination of those who had worshipped the golden statue, which was melted down and fed to the idolaters. He also wrote the ten commandments on a new set of tablets. Later at Mount Sinai, Moses and the elders entered into a covenant, by which Israel would become the people of YHWH, obeying his laws, and YHWH would be their god. Moses delivered laws of God to Israel, instituted the priesthood under the sons of Moses' brother Aaron, and destroyed those Israelites who fell away from his worship. In his final act at Sinai, God gave Moses instructions for the Tabernacle, the mobile shrine by which he would travel with Israel to the Promised Land.
From Sinai, Moses led the Israelites to Paran on the border of Canaan. There he sent twelve spies into the land. The spies returned with samples of the land's fertility, but warned that its inhabitants were giants. The people were afraid and wanted to return to Egypt, and some rebelled against Moses and against God. Moses told the Israelites that they were not worthy to inherit the land, and would wander the wilderness for forty years until the generation who had refused to enter Canaan had died, so that it would be their children who would possess the land.
When the forty years had passed, Moses led the Israelites east around the Dead Sea to the territories of Edom and Moab. There they escaped the temptation of idolatry, received God's blessing through Balaam the prophet, and massacred the Midianites, who were God's enemies. On the banks of the Jordan, in sight of the land, Moses assembled the tribes. After recalling their wanderings he delivered God's laws by which they must live in the land, sang a song of praise and pronounced a blessing on the people, and passed his authority to Joshua, under whom they would possess the land. Moses then went up Mount Nebo to the top of Pisgah, looked over the promised land of Israel spread out before him, and died, at the age of one hundred and twenty. More humble than any other man (Num. 12:3), "there hath not arisen a prophet since in Israel like unto Moses, whom YHWH knew face to face" (Deuteronomy. 34:10).
Lawgiver of Israel.
Moses is honoured among Jews today as the "lawgiver of Israel", and he delivers several sets of laws in the course of the four books. The first is the Covenant code, Exodus 19-24, the terms of the covenant which God offers to Israel at the foot of Sinai. Embedded in the covenant are the Decalogue (the Ten Commandments, Exodus 20:1-17) and the Book of the Covenant (Exodus 20:22-23:19). The entire Book of Leviticus constitutes a second body of law, the Book of Numbers begins with yet another set, and the Book of Deuteronomy another.
Moses has traditionally been regarded as the author of those four books and the Book of Genesis, which together comprise the Torah, the first and most revered section of the Jewish Bible.
Sources.
Apart from a few scattered references elsewhere in the Jewish scriptures, all that is known about Moses comes from the books of Exodus, Leviticus, Numbers and Deuteronomy. The majority of scholars date these four books to the Persian period, 538-332 BCE.
No Egyptian sources mention Moses or the events of Exodus-Deuteronomy, nor has any archeological evidence been discovered in Egypt or the Sinai wilderness to support the story in which he is the central figure.
Moses in Hellenistic literature.
Non-biblical writings about Jews, with references to the role of Moses, first appear at the beginning of the Hellenistic period, from 323 BCE to about 146 BCE. Shmuel notes that "a characteristic of this literature is the high honour in which it holds the peoples of the East in general and some specific groups among these peoples.":1102
In addition to the Judeo-Roman or Judeo-Hellenic historians Artapanus, Eupolemus, Josephus, and Philo, a few non-Jewish historians including Hecataeus of Abdera (quoted by Diodorus Siculus), Alexander Polyhistor, Manetho, Apion, Chaeremon of Alexandria, Tacitus and Porphyry also make reference to him. The extent to which any of these accounts rely on earlier sources is unknown.:1103 Moses also appears in other religious texts such as the Mishnah (c. 200 AD), Midrash (AD 200–1200), and the Qur'an (c. 610—653).
The figure of Osarseph in Hellenistic historiography is a renegade Egyptian priest who leads an army of lepers against the pharaoh and is finally expelled from Egypt, changing his name to Moses.
The earliest existing reference to Moses in Greek literature occurs in the Egyptian history of Hecataeus of Abdera (4th century BC). All that remains of his description of Moses are two references made by Diodorus Siculus, wherein, writes historian Arthur Droge, "he describes Moses as a wise and courageous leader who left Egypt and colonized Judaea.":18 Among the many accomplishments described by Hecataeus, Moses had founded cities, established a temple and religious cult, and issued laws:
Droge also points out that this statement by Hecataeus was similar to statements made subsequently by Eupolemus.:18
The Jewish historian Artapanus of Alexandria (2nd century BCE), portrayed Moses as a cultural hero, alien to the Pharaonic court. According to theologian John Barclay, the Moses of Artapanus "clearly bears the destiny of the Jews, and in his personal, cultural and military splendor, brings credit to the whole Jewish people."
Artapanus goes on to relate how Moses returns to Egypt with Aaron, and is imprisoned, but miraculously escapes through the name of YHWH in order to lead the Exodus. This account further testifies that all Egyptian temples of Isis thereafter contained a rod, in remembrance of that used for Moses' miracles. He describes Moses as 80 years old, "tall and ruddy, with long white hair, and dignified."
Some historians, however, point out the "apologetic nature of much of Artapanus' work,":40 with his addition extra-biblical details, as with references to Jethro: The non-Jewish Jethro expresses admiration for Moses' gallantry in helping his daughters, and chooses to adopt Moses as his son.:133
Strabo, a Greek historian, geographer and philosopher, in his "Geography" (c. AD 24), wrote in detail about Moses, whom he considered to be an Egyptian who deplored the situation in his homeland, and thereby attracted many followers who respected the deity. He writes, for example, that Moses opposed the picturing of the deity in the form of man or animal, and was convinced that the deity was an entity which encompassed everything – land and sea::1132
In Strabo’s writings of the history of Judaism as he understood it, he describes various stages in its development: from the first stage, including Moses and his direct heirs; to the final stage where "the Temple of Jerusalem continued to be surrounded by an aura of sanctity." Strabo’s "positive and unequivocal appreciation of Moses’ personality is among the most sympathetic in all ancient literature." :1133 His portrayal of Moses is said to be similar to the writing of Hecataeus who "described Moses as a man who excelled in wisdom and courage.":1133
Egyptologist Jan Assmann concludes that Strabo was the historian "who came closest to a construction of Moses' religion as monotheism and as a pronounced counter-religion." It recognized "only one divine being whom no image can represent. . . [and] the only way to approach this god is to live in virtue and in justice.":38
The Roman historian Tacitus (ca. 56—120 AD) refers to Moses by noting that the Jewish religion was monotheistic and without a clear image. His primary work, wherein he describes Jewish philosophy, is his "Histories" (ca. 100), where, according to Murphy, as a result of the Jewish worship of one God, "pagan mythology fell into contempt." Tacitus states that, despite various opinions current in his day regarding the Jews' ethnicity, most of his sources are in agreement that there was an Exodus from Egypt. By his account, the Pharaoh Bocchoris, suffering from a plague, banished the Jews in response to an oracle of the god Zeus-Amun.
In this version, Moses and the Jews wander through the desert for only six days, capturing the Holy Land on the seventh.
The Septuagint, the Greek version of the Hebrew Bible, influenced Longinus, who may have been the author of the great book of literary criticism, "On the Sublime", although the true author is still unknown for certain. However, most scholars agree that the author lived in the time of Augustus or Tiberius, the first and second Roman Emperors.
The writer quotes Genesis in a "style which presents the nature of the deity in a manner suitable to his pure and great being," however he does not mention Moses by name, but instead calls him "the Lawgiver of the Jews." Besides its mention of Cicero, Moses is the only non-Greek writer quoted in the work, and he is described "with far more admiration than even Greek writers who treated Moses with respect, such as Hecataeus and Strabo.:1140
In Josephus' (37 – c. 100 AD) "Antiquities of the Jews", Moses is mentioned throughout. For example Book VIII Ch. IV, describes Solomon's Temple, also known as the First Temple, at the time the Ark of the Covenant was first moved into the newly built temple:
When King Solomon had finished these works, these large and beautiful buildings, and had laid up his donations in the temple, and all this in the interval of seven years, and had given a demonstration of his riches and alacrity therein; ... he also wrote to the rulers and elders of the Hebrews, and ordered all the people to gather themselves together to Jerusalem, both to see the temple which he had built, and to remove the ark of God into it; and when this invitation of the whole body of the people to come to Jerusalem was everywhere carried abroad, ... The Feast of Tabernacles happened to fall at the same time, which was kept by the Hebrews as a most holy and most eminent feast. So they carried the ark and the tabernacle which Moses had pitched, and all the vessels that were for ministration to the sacrifices of God, and removed them to the temple... Now the ark contained nothing else but those two tables of stone that preserved the ten commandments, which God spake to Moses in Mount Sinai, and which were engraved upon them...
According to Feldman, Josephus also attaches particular significance to Moses' possession of the "cardinal virtues of wisdom, courage, temperance, and justice." He also includes piety as an added fifth virtue. In addition, he "stresses Moses' willingness to undergo toil and his careful avoidance of bribery. Like Plato's philosopher-king, Moses excels as an educator.":130
Numenius, a Greek philosopher who was a native of Apamea, in Syria, wrote during the latter half of the 2nd century AD. Historian Kennieth Guthrie writes that "Numenius is perhaps the only recognized Greek philosopher who explicitly studied Moses, the prophets, and the life of Jesus . . . ":194 He describes his background:
Numenius was a man of the world; he was not limited to Greek and Egyptian mysteries, but talked familiarly of the myths of Brahmins and Magi. It is however his knowledge and use of the Hebrew scriptures which distinguished him from other Greek philosophers. He refers to Moses simply as "the prophet", exactly as for him Homer is the poet. Plato is described as a Greek Moses.:101
The Christian saint and religious philosopher Justin Martyr (103–165 AD) drew the same conclusion as Numenius, according to other experts. Theologian Paul Blackham notes that Justin considered Moses to be "more trustworthy, profound and truthful because he is "older" than the Greek philosophers." He quotes him:
I will begin, then, with our first prophet and lawgiver, Moses . . . that you may know that, of all your teachers, whether sages, poets, historians, philosophers, or lawgivers, by far the oldest, as the Greek histories show us, was Moses, who was our first religious teacher.
Historicity.
The tradition of Moses as a lawgiver and culture hero of the Israelites can be traced to the Deuteronomist source, corresponding to the 7th-century Kingdom of Judah. Moses is a central figure in the Deuteronomist account of the origins of the Israelites, cast in a literary style of elegant flashbacks told by Moses. The mainstream view is that the Deuteronomist relies on earlier material that may date to the United Monarchy, so that the biblical narrative would be based on traditions that can be traced roughly to the 10th century, or about four centuries after the supposed lifetime of Moses. By contrast, Biblical minimalists such as Philip Davies and Niels Peter Lemche regard the Exodus as a fiction composed in the Persian period or even later to give hope of return to Canaan for a Diaspora community, without even the memory of a historical Moses. Given this possible late composition it would seem that the figure of Moses may be a composite drawn from a number of different sources.
The question of the historicity of the Exodus (specifically, the Pharaoh of the Exodus, identification of whom would connect the biblical narrative to Egyptological chronology) has long been debated, without conclusive result. There were at least two periods in Egyptian history in which Asiatic Semites were expelled from Egypt. One was associated with the expulsion of the Semitic Hyksos at the beginning of the Late Bronze Age. The second was following the commencement of the reign of Setnakhte at the end of the 19th Dynasty. Manetho seems to confuse the two, for instance, in a distorted account reported in Josephus, he supposedly states that Moses was originally Osarseph, a renegade priest, who led a band of lepers out of Avaris (referred to as Raamses in the Bible).() Osarseph, may be a memory of a shadowy visier, originally from Syria (Hurru), known as Yursu (self-made), who came to prominence as Chancellor Bay just prior to the second event. Pi Ramesses may be the "store city" Raamses mentioned in Exodus, which was the capital of the Egyptian Empire in the 19th to end of the 20th Dynasty of Egypt, giving quite a specific date to the Egyptian part of Exodus.
Some scholars, like Kenneth Kitchen and Frank Yurko suggest that there may be a historical core beneath the Exodus and Sinai traditions, even if the biblical narrative dramatizes by portraying as a single event what was more likely a gradual process of migration and conquest. Thus, the motif of "slavery in Egypt" may reflect the historical situation of imperialist control of the Egyptian Empire over Canaan over the period of the Thutmosides down to the revolt against Merenptah and Rameses III, after which it declined gradually during the 12th century under the pressure from the Sea Peoples and the general Bronze Age collapse:
Israel Finkelstein points to the appearance of settlements in the central hill country around 1200 as the earliest of the known settlements of the Israelites.
A cyclical pattern to these highland settlements, corresponding to the state of the surrounding cultures, suggests that the local Canaanites combined an agricultural and nomadic lifestyles, particularly under Aramaean and Neo-Hittite influence. When Egyptian rule collapsed after the invasion of the Sea Peoples, the central hill country could no longer sustain a large nomadic population, so they went from nomadism to sedentism. Canaanite refugees from the lowlands seem to have fused with Shasu, nomadic Aramaean elements, using pithoi cisterns for the capture of water, hillside terracing and other elements from the Aegean and Western Anatolian "Peoples of the Sea", living in scattered hamlets and avoding the husbandry of pigs, suggesting a new type of culture in the region.
However, Finkelstein states in the same book that at the earlier time proposed by most scientists for the Exodus, based upon the Biblical chronology 400 years prior to the reign of King David, Egypt was at the peak of its glory, with a series of fortresses guarding the borders and checkpoints watching the roads to Canaan. That means an exodus of the scale of over 600,000 soldiers described in the Torah would have been impossible. This implies a total civilian population, with women and children, of over a million, which would have numbered between a third and a half of the total Egyptian population at the time.
While the general narrative of the Exodus and the conquest of the Promised Land may be remotely rooted in historical events, the figure of Moses as a leader of the Israelites in these events cannot be substantiated. William Dever agrees with the Canaanite origin of the Israelites but allows for the possibility of some immigrants from Egypt among the early hilltop settlers, leaving open the possibility of a Moses-like figure in Transjordan ca 1250-1200.
Martin Noth holds that two different groups experienced the Exodus and Sinai events, and each group transmitted its own stories independently of the other one, writing that "The biblical story tracing the Hebrews from Egypt to Canaan resulted from an editor's weaving separate themes and traditions around a main character Moses, actually an obscure person from Moab." Given the existence of a Moabite king Mesha, etymologically identical to the Hebrew Moshe, it is possible that there was a memory of a culture hero who was associated with the end of Egyptian influence at Timna during the late Bronze Age.
The "Kenite hypothesis", originally suggested by Cornelius Tiele in 1872, supposes that the figure of Moses is a reflection of a historical Midianite priest of "Yahweh", whose cult was introduced to Israel from southern Canaan (Edom, Moab, Midian) by the Kenites. This idea is based on an old tradition (recorded in Judges 1:16, 4:11) that Moses' father-in-law was a Midianite priest of Yahweh, as it were preserving a memory of the Midianite origin of the deity. While the role of the Kenites in the transmission of the cult is widely accepted, Tiele's view on the historical role of Moses finds less support in modern scholarship.
William Albright held a more favorable view towards the traditional views regarding Moses, and accepted the essence of the biblical story, as narrated between Exodus 1:8 and Deuteronomy 34:12, but recognized the impact that centuries of oral and written transmission have had on the account, causing it to acquire layers of accretions.
Recently Aidan Dodson, M. Georg and R. Krauss all suggest that the story of Moses as a Prince of Egypt may contain a distorted memory of Pharaoh Amenmesses. In texts written after his disappearance Amenmesses "was explicitly denied any royal status - being simply ´Mose´ and perhaps also ´enemy´... Indeed it has been suggested that Amenmesses´ memory has survived in a far more universal way, in that his career was transmogrified into the Old Testament story of Jewish law-giver, Moses." Dodson concludes "... this connection is beyond proof and such a survival of Amenmesses into world consciousness remains but an intriguing possibility".
Moses in religious traditions.
Judaism.
There is a wealth of stories and additional information about Moses in the Jewish apocrypha and in the genre of rabbinical exegesis known as Midrash, as well as in the primary works of the Jewish oral law, the Mishnah and the Talmud.
Moses is also given a number of bynames in Jewish tradition.
The Midrash identifies Moses as one of seven biblical personalities who were called by various names. Moses' other names were: Jekuthiel (by his mother), Heber (by his father), Jered (by Miriam), Avi Zanoah (by Aaron), Avi Gedor (by Kohath), Avi Soco (by his wet-nurse), Shemaiah ben Nethanel (by people of Israel). Moses is also attributed the names Toviah (as a first name), and Levi (as a family name) (Vayikra Rabbah 1:3), Heman, Mechoqeiq (lawgiver) and Ehl Gav Ish (Numbers 12:3).
Jewish historians who lived at Alexandria, such as Eupolemus, attributed to Moses the feat of having taught the Phoenicians their alphabet, similar to legends of Thoth. Artapanus of Alexandria explicitly identified Moses not only with Thoth / Hermes, but also with the Greek figure Musaeus (whom he calls "the teacher of Orpheus"), and ascribed to him the division of Egypt into 36 districts, each with its own liturgy. He names the princess who adopted Moses as Merris, wife of Pharaoh Chenephres.
Ancient sources mention an Assumption of Moses and a Testimony of Moses. A Latin text was found in Milan in the 19th century by Antonio Ceriani who called it the Assumption of Moses, even though it does not refer to an assumption of Moses or contain portions of the Assumption which are cited by ancient authors, and it is apparently actually the Testimony. The incident which the ancient authors cite is also mentioned in the Epistle of Jude.
To Orthodox Jews, Moses is called "Moshe Rabbenu, `Eved HaShem, Avi haNeviim zya"a". He is defined "Our Leader Moshe", "Servant of God", and "Father of all the Prophets". In their view, Moses received not only the Torah, but also the revealed (written and oral) and the hidden (the "`hokhmat nistar" teachings, which gave Judaism the Zohar of the Rashbi, the Torah of the Ari haQadosh and all that is discussed in the Heavenly Yeshiva between the Ramhal and his masters). He is also considered the greatest prophet.
Arising in part from his age, but also because 120 is elsewhere stated as the maximum age for Noah's descendants (one interpretation of ), "may you live to 120" has become a common blessing among Jews.
Christianity.
For Christians, Moses — mentioned more often in the New Testament than any other Old Testament figure — is often a symbol of God's law, as reinforced and expounded on in the teachings of Jesus. New Testament writers often compared Jesus' words and deeds with Moses' to explain Jesus' mission. In Acts 7:39–43, 51–53, for example, the rejection of Moses by the Jews who worshiped the golden calf is likened to the rejection of Jesus by the Jews that continued in traditional Judaism.
Moses also figures in several of Jesus' messages. When he met the Pharisee Nicodemus at night in the third chapter of the Gospel of John, he compared Moses' lifting up of the bronze serpent in the wilderness, which any Israelite could look at and be healed, to his own lifting up (by his death and resurrection) for the people to look at and be healed. In the sixth chapter, Jesus responded to the people's claim that Moses provided them "manna" in the wilderness by saying that it was not Moses, but God, who provided. Calling himself the "bread of life", Jesus stated that He was provided to feed God's people.
Moses, along with Elijah, is presented as meeting with Jesus in all three Gospel accounts of the Transfiguration of Jesus in Matthew 17, Mark 9, and Luke 9, respectively. Later Christians found numerous other parallels between the life of Moses and Jesus to the extent that Jesus was likened to a "second Moses." For instance, Jesus' escape from the slaughter by Herod in Bethlehem is compared to Moses' escape from Pharaoh's designs to kill Hebrew infants. Such parallels, unlike those mentioned above, are not pointed out in Scripture. See the article on typology.
His relevance to modern Christianity has not diminished. Moses is considered to be a saint by several churches; and is commemorated as a prophet in the respective Calendars of Saints of the Eastern Orthodox Church, Roman Catholic Church, and Lutheran churches on September 4. He is commemorated as one of the Holy Forefathers in the Calendar of Saints of the Armenian Apostolic Church on July 30.
Mormonism.
Members of The Church of Jesus Christ of Latter-day Saints (colloquially called Mormons) generally view Moses in the same way that other Christians do. However, in addition to accepting the biblical account of Moses, Mormons include Selections from the Book of Moses as part of their scriptural canon. This book is believed to be the translated writings of Moses, and is included in the Pearl of Great Price.
Latter-day Saints are also unique in believing that Moses was taken to heaven without having tasted death (translated). In addition, Joseph Smith, Jr. and Oliver Cowdery stated that on April 3, 1836, Moses appeared to them in the Kirtland Temple in a glorified, immortal, physical form and bestowed upon them the "keys of the gathering of Israel from the four parts of the earth, and the leading of the ten tribes from the land of the north."
Islam.
Moses is mentioned more in the Quran than any other individual and his life is narrated and recounted more than that of any other prophet.
In general, Moses is described in ways which parallel the Islamic prophet Muhammad, and "his character exhibits some of the main themes of Islamic theology," including the "moral injunction that we are to submit ourselves to God."
Moses is defined in the Qur'an as both prophet ("nabi") and messenger ("rasul"), the latter term indicating that he was one of those prophets who brought a scripture and law to his people.
Huston Smith (1991) describes an account in the Qur'an of meetings in heaven between Moses and Muhammad, which Huston states were "one of the crucial events in Muhammad's life," and resulted in Muslims observing 5 daily prayers.
Moses is mentioned 502 times in the Qur'an; passages mentioning Moses include 2.49-61, 7.103-160, 10.75-93, 17.101-104, 20.9-97, 26.10-66, 27.7-14, 28.3-46, 40.23-30, 43.46-55, 44.17-31, and 79.15-25. and many others.
Most of the key events in Moses' life which are narrated in the Bible are to be found dispersed through the different Surahs of Qur'an, with a story about meeting Khidr which is not found in the Bible.
In the Moses story related by the Qur'an, Jochebed is commanded by God to place Moses in an ark and cast him on the waters of the Nile, thus abandoning him completely to God's protection. Pharaoh's wife Asiya, not his daughter, found Moses floating in the waters of the Nile. She convinced Pharaoh to keep him as their son because they were not blessed with any children.
The Qur'an's account has emphasized Moses' mission to invite the Pharaoh to accept God's divine message as well as give salvation to the Israelites. According to the Qur'an, Moses encourages the Israelites to enter Canaan, but they are unwilling to fight the Canaanites, fearing certain defeat. Moses responds by pleading to Allah that he and his brother Aaron be separated from the rebellious Israelites. After which the Israelites are made to wander for 40 years.
According to Islamic tradition, Moses is buried at Maqam El-Nabi Musa, Jericho.
Baha'i Faith.
In the Baha'i Faith, Moses is considered a messenger from God who is considered equally authentic as those sent in other eras. An epithet of Moses in Baha'i scriptures is "Interlocutor of God". Moses is further described as paving the way for Baha'ullah and his ultimate revelation, and a teacher of truth, whose teachings were in line with the customs of his time.
Modern reception.
Literature.
Thomas Mann's novella "The Tables of the Law" is a retelling of the story of the exodus from Egypt, with Moses as its main character.
In Freud.
Sigmund Freud, in his last book, "Moses and Monotheism" in 1939, postulated that Moses was an Egyptian nobleman who adhered to the monotheism of Akhenaten. Following a theory proposed by a contemporary biblical critic, Freud believed that Moses was murdered in the wilderness, producing a collective sense of patricidal guilt that has been at the heart of Judaism ever since. "Judaism had been a religion of the father, Christianity became a religion of the son", he wrote. The possible Egyptian origin of Moses and of his message has received significant scholarly attention.
Opponents of this view observe that the religion of the Torah seems different from Atenism in everything except the central feature of devotion to a single god, although this has been countered by a variety of arguments, e.g. pointing out the similarities between the Hymn to Aten and Psalm 104. Freud's interpretation of the historical Moses is not well accepted among historians, and is considered pseudohistory by many.
Criticism.
In the late 18th century the deist Thomas Paine commented at length on Moses' Laws in "The Age of Reason", and gave his view that "the character of Moses, as stated in the Bible, is the most horrid that can be imagined", giving the story at as an example. In the 19th century the agnostic Robert G. Ingersoll wrote "...that all the ignorant, infamous, heartless, hideous things recorded in the 'inspired' Pentateuch are not the words of God, but simply 'Some Mistakes of Moses'". In the 2000s, the atheist Richard Dawkins referring, like Paine, to the incident at , concluded, "No, Moses was not a great role model for modern moralists."
Figurative art.
Moses is depicted in several U.S. government buildings because of his legacy as a lawgiver. In the Library of Congress stands a large statue of Moses alongside a statue of the Apostle Paul. Moses is one of the 23 lawgivers depicted in marble bas-reliefs in the chamber of the U.S. House of Representatives in the United States Capitol. The other twenty-two figures have their profiles turned to Moses, which is the only forward-facing bas-relief.
Moses appears eight times in carvings that ring the Supreme Court Great Hall ceiling. His face is presented along with other ancient figures such as Solomon, the Greek god Zeus and the Roman goddess of wisdom, Minerva. The Supreme Court building's east pediment depicts Moses holding two tablets. Tablets representing the Ten Commandments can be found carved in the oak courtroom doors, on the support frame of the courtroom's bronze gates and in the library woodwork. A controversial image is one that sits directly above the chief justice's head. In the center of the 40-foot-long Spanish marble carving is a tablet displaying Roman numerals I through X, with some numbers partially hidden.
Michelangelo's statue.
Michelangelo's statue of Moses in the Church of San Pietro in Vincoli, Rome, is one of the most familiar masterpieces in the world. The horns the sculptor included on Moses' head are the result of a mistranslation of the Hebrew Bible into the Latin Vulgate Bible with which he was familiar. The Hebrew word taken from "Exodus" means either a "horn" or an "irradiation." Experts at the Archaeological Institute of America show that the term was used when Moses "returned to his people after seeing as much of the Glory of the Lord as human eye could stand," and his face "reflected radiance." In early Jewish art, moreover, Moses is often "shown with rays coming out of his head."
Another author explains, "When Saint Jerome translated the Old Testament into Latin, he thought no one but Christ should glow with rays of light — so he advanced the secondary translation. However, writer J. Stephen Lang points out that Jerome's version actually described Moses as "giving off hornlike rays," and he "rather clumsily translated it to mean 'having horns.'" It has also been noted that he had Moses seated on a throne, yet Moses was neither a King nor ever sat on such thrones.
Film and television.
Moses was portrayed by Theodore Roberts in DeMille's 1923 silent film "The Ten Commandments".
Moses appears as the central character in the 1956 Cecil B. DeMille movie, also called "The Ten Commandments", in which he is portrayed by Charlton Heston. A television remake was produced in 2006.
Burt Lancaster played "Moses" in the 1975 television miniseries "Moses the Lawgiver".
In the 1981 film "History of the World, Part I", Moses is portrayed by Mel Brooks. Sir Ben Kingsley is the narrator of the 2007 animated film, The Ten Commandments.
Moses appears as the central character in the 1998 DreamWorks Pictures animated movie, "The Prince of Egypt". He is voiced by Val Kilmer.
In 2014, Ridley Scott directed the film "", in which Christian Bale portrays the central character "Moses". It portrays Moses and Rameses II as being raised by Seti I as cousins.

</doc>
<doc id="19579" url="http://en.wikipedia.org/wiki?curid=19579" title="Mississippi River">
Mississippi River

The Mississippi River is the chief river of the largest drainage system on the North American continent. Flowing entirely in the United States (though its drainage basin reaches into Canada), it rises in northern Minnesota and meanders slowly southwards for 2320 mi to the Mississippi River Delta at the Gulf of Mexico. With its many tributaries, the Mississippi's watershed drains all or parts of 31 U.S. states and 2 Canadian provinces between the Rocky and Appalachian Mountains. The Mississippi ranks as the fourth longest and tenth largest river in the world. The river either borders or passes through the states of Minnesota, Wisconsin, Iowa, Illinois, Missouri, Kentucky, Tennessee, Arkansas, Mississippi, and Louisiana.
Native Americans long lived along the Mississippi River and its tributaries. Most were hunter-gatherers or herders, but some, such as the Mound builders, formed prolific agricultural societies. The arrival of Europeans in the 1500s changed the native way of life as first explorers, then settlers, ventured into the basin in increasing numbers. The river served first as a barrier – forming borders for New Spain, New France, and the early United States – then as a vital transportation artery and communications link. In the 19th century, during the height of Manifest Destiny, the Mississippi and several western tributaries, most notably the Missouri, formed pathways for the western expansion of the United States.
Formed from thick layers of this river's silt deposits, the Mississippi River Valley is one of the most fertile agricultural regions of the country, which resulted in the river's storied steamboat era. During the American Civil War, the Mississippi's capture by Union forces marked a turning point towards victory because of the river's importance as a route of trade and travel, not least to the Confederacy. Because of substantial growth of cities and the larger ships and barges that supplanted riverboats, the decades following the 1900s saw the construction of massive engineering works such as levees, locks and dams, often built in combination.
Since modern development of the basin began, the Mississippi has also seen its share of pollution and environmental problems – most notably large volumes of agricultural runoff, which has led to the Gulf of Mexico dead zone off the Delta. In recent years, the river has shown a steady shift towards the Atchafalaya River channel in the Delta; a course change would be an economic disaster for the port city of New Orleans.
Name.
The word itself comes from "Messipi", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, "Misi-ziibi" (Great River). See below in History section for additional information.
In addition to historical traditions shown by names, there are at least two other measures of a river's identity, one being the largest branch (by water volume), and the other being the longest branch. Using the largest-branch criterion, the Ohio (not the Middle and Upper Mississippi) would be the main branch of the Lower Mississippi. Using the longest-branch criterion, the Middle Mississippi-Missouri-Jefferson-Beaverhead-Red Rock-Hellroaring Creek River would be the main branch. According to either school of thought, the Upper Mississippi from Lake Itasca, Minnesota to St. Louis, despite its name, would only be a secondary tributary of the final river flowing from Cairo, Illinois to the Gulf of Mexico.
While the Missouri River, flowing from the confluence of the Jefferson, Madison and Gallatin Rivers to the Mississippi, is the longest continuously named river in the United States, the serially named river known sequentially as Hellroaring Creek, Red Rock, Beaverhead, Jefferson, Missouri, Middle Mississippi, and Lower Mississippi, as one continuous waterway, is the longest river in North America and the fourth longest river in the world. Its length of at least 3745 mi is exceeded only by the Nile, the Amazon, and perhaps the Yangtze River among the longest rivers in the world. The source of this waterway is at Brower's Spring, 8800 ft above sea level in southwestern Montana, along the Continental Divide outside Yellowstone National Park. 
In the 18th century, the river was the primary western boundary of the young United States, and since the country's expansion westward, the Mississippi River has been widely considered a convenient if approximate dividing line between the Eastern, Southern, and Midwestern United States, and the Western United States. This is exemplified by the Gateway Arch in St. Louis, and the phrase "Trans-Mississippi" as used in the name of the Trans-Mississippi Exposition. It is common to qualify a regionally superlative landmark in relation to it, such as "the highest peak east of the Mississippi" or "the oldest city west of the Mississippi".
Physical geography.
The geographical setting of the Mississippi River includes considerations of the course of the river itself, its watershed, its outflow, its prehistoric and historic course changes, and possibilities of future course changes. The New Madrid Seismic Zone along the river is also noteworthy. These various basic geographical aspects of the river in turn underlie its human history and present uses of the waterway and its adjacent lands.
Divisions.
The Mississippi River can be divided into three sections: the Upper Mississippi, the river from its headwaters to the confluence with the Missouri River; the Middle Mississippi, which is downriver from the Missouri to the Ohio River; and the Lower Mississippi, which flows from the Ohio to the Gulf of Mexico.
Upper Mississippi.
The Upper Mississippi runs from its headwaters to its confluence with the Missouri River at St. Louis, Missouri. The Upper Mississippi is divided into two sections:
The source of the Upper Mississippi branch is traditionally accepted as Lake Itasca, 1475 ft above sea level in Itasca State Park in Clearwater County, Minnesota. The name "Itasca" was chosen to designate the "true head" of the Mississippi River as a combination of the last four letters of the Latin word for truth ("veritas") and the first two letters of the Latin word for head ("caput"). However, the lake is in turn fed by a number of smaller streams.
From its origin at Lake Itasca to St. Louis, Missouri, the waterway's flow is moderated by 43 dams. Fourteen of these dams are located above Minneapolis in the headwaters region and serve multiple purposes, including power generation and recreation. The remaining 29 dams, beginning in downtown Minneapolis, all contain locks and were constructed to improve commercial navigation of the upper river. Taken as a whole, these 43 dams significantly shape the geography and influence the ecology of the upper river. Beginning just below Saint Paul, Minnesota, and continuing throughout the upper and lower river, the Mississippi is further controlled by thousands of wing dikes that moderate the river's flow in order to maintain an open navigation channel and prevent the river from eroding its banks.
The head of navigation on the Mississippi is the Coon Rapids Dam in Coon Rapids, Minnesota. Before it was built in 1913, steamboats could occasionally go upstream as far as Saint Cloud, Minnesota, depending on river conditions.
The uppermost lock and dam on the Upper Mississippi River is the Upper St. Anthony Falls Lock and Dam in Minneapolis. Above the dam, the river's elevation is 799 ft. Below the dam, the river's elevation is 750 ft. This 49 ft drop is the largest of all the Mississippi River locks and dams. The origin of the dramatic drop is a waterfall preserved adjacent to the lock under an apron of concrete. Saint Anthony Falls is the only true waterfall on the entire Mississippi River. The water elevation continues to drop steeply as it passes through the gorge carved by the waterfall.
The Upper Mississippi features various natural and artificial lakes, with its widest point being Lake Winnibigoshish, near Grand Rapids, Minnesota, over 7 mi across. Also of note is Lake Onalaska (created by Lock and Dam No. 7), near La Crosse, Wisconsin, over 4 mi wide. On the other hand, Lake Pepin is natural, formed due to the delta formed by the Chippewa River of Wisconsin as it enters the Upper Mississippi; it is more than 2 mi wide.
By the time the Upper Mississippi reaches Saint Paul, Minnesota, below Lock and Dam No. 1, it has dropped more than half its original elevation and is 687 ft above sea level. From St. Paul to St. Louis, Missouri, the river elevation falls much more slowly, and is controlled and managed as a series of pools created by 26 locks and dams.
The Upper Mississippi River is joined by the Minnesota River at Fort Snelling in the Twin Cities; the St. Croix River near Prescott, Wisconsin; the Cannon River near Red Wing, Minnesota; the Zumbro River at Wabasha, Minnesota; the Black, La Crosse, and Root rivers in La Crosse, Wisconsin; the Wisconsin River at Prairie du Chien, Wisconsin; the Rock River at the Quad Cities; the Iowa River near Wapello, Iowa; the Skunk River south of Burlington, Iowa; and the Des Moines River at Keokuk, Iowa. Other major tributaries of the Upper Mississippi include the Crow River in Minnesota, the Chippewa River in Wisconsin, the Maquoketa River and the Wapsipinicon River in Iowa, and the Big Muddy River and Illinois River.
The Upper Mississippi is largely a multi-thread stream with many bars and islands. From its confluence with the St. Croix River downstream to Dubuque, Iowa, the river is entrenched, with high bedrock bluffs lying on either side. The height of these bluffs decreases to the south of Dubuque, though they are still significant through Savanna, Illinois. This topography contrasts strongly with the Lower Mississippi, which is a meandering river in a broad, flat area, only rarely flowing alongside a bluff (as at Vicksburg, Mississippi).
The Upper Mississippi River is home to over 119 species of fish. Some fish include; walleye, sauger, large mouth bass, small mouth bass, and white bass. Northern pike, bluegill and crappie also reside in the Upper Mississippi River. Other fish like channel catfish, flathead catfish, carp, the common shiner, freshwater drum, paddlefish and shovelnose sturgeon also live in these upper Mississippi waters. The Minnesota Department of Natural Resources has designated much of the Mississippi River in the state as infested waters by the exotic species zebra mussels and Eurasian watermilfoil.
Middle Mississippi.
The Mississippi River is known as the Middle Mississippi from the Upper Mississippi River's confluence with the Missouri River at St. Louis, Missouri, for 190 mi to its confluence with the Ohio River at Cairo, Illinois.
The Middle Mississippi is relatively free-flowing. From St. Louis to the Ohio River confluence, the Middle Mississippi falls 220 ft over 180 mi for an average rate of 1.2 ft/mi. At its confluence with the Ohio River, the Middle Mississippi is 315 ft above sea level. Apart from the Missouri and Meramec rivers of Missouri and the Kaskaskia River of Illinois, no major tributaries enter the Middle Mississippi River.
Lower Mississippi.
The Mississippi River is called the Lower Mississippi River from its confluence with the Ohio River to its mouth at the Gulf of Mexico. Measured by water volume, the Lower Mississippi's primary branch is the Ohio River. At the confluence of the Ohio and the Middle Mississippi, the Ohio is the bigger river, with its long-term mean discharge at Cairo, Illinois being 281500 cuft/s, while the long-term mean discharge of the Mississippi at Thebes, Illinois (just upriver from Cairo) is 208200 cuft/s. Thus, by volume, the main branch of the Mississippi River system at Cairo can be considered to be the Ohio River (and the Allegheny River further upstream), rather than the Middle Mississippi.
In addition to the Ohio River, the major tributaries of the Lower Mississippi River are the White River, flowing in at the White River National Wildlife Refuge in east central Arkansas; the Arkansas River, joining the Mississippi at Arkansas Post; the Big Black River in Mississippi; the Yazoo River, meeting the Mississippi at Vicksburg, Mississippi; and the Red River in Louisiana. The widest point of the Mississippi River is in the Lower Mississippi portion where it exceeds 1 mi in width in several places.
Deliberate water diversion at the Old River Control Structure in Louisiana allows the Atchafalaya River in Louisiana to be a major distributary of the Mississippi River, with 30% of the Mississippi flowing to the Gulf of Mexico by this route, rather than continuing down the Mississippi's current channel past Baton Rouge and New Orleans on a longer route to the Gulf.
Watershed.
The Mississippi River has the world's fourth largest drainage basin ("watershed" or "catchment"). The basin covers more than 1245000 sqmi, including all or parts of 31 U.S. states and two Canadian provinces. The drainage basin empties into the Gulf of Mexico, part of the Atlantic Ocean. The total catchment of the Mississippi River covers nearly 40% of the landmass of the continental United States. The highest point within the watershed is also the highest point of the Rocky Mountains, Mount Elbert at 14440 ft.
In the United States, the Mississippi River drains the majority of the area between crest of the Rocky Mountains and the crest of the Appalachian Mountains, except for various regions drained to Hudson Bay by the Red River of the North; to the Atlantic Ocean by the Great Lakes and the Saint Lawrence River; and to the Gulf of Mexico by the Rio Grande, the Alabama and Tombigbee rivers, the Chattahoochee and Appalachicola rivers, and various smaller coastal waterways along the Gulf.
The Mississippi River empties into the Gulf of Mexico about 100 mi downstream from New Orleans. Measurements of the length of the Mississippi from Lake Itasca to the Gulf of Mexico vary somewhat, but the United States Geological Survey's number is 2320 mi. The retention time from Lake Itasca to the Gulf is typically about 90 days.
Outflow.
The Mississippi River discharges at an annual average rate of between 200 and 700 thousand cubic feet per second (7,000–20,000 m3/s). Although it is the 5th largest river in the world by volume, this flow is a mere fraction of the output of the Amazon, which moves nearly 7 million cubic feet per second (200,000 m3/s) during wet seasons. On average, the Mississippi has only 8% the flow of the Amazon River.
Fresh river water flowing from the Mississippi into the Gulf of Mexico does not mix into the salt water immediately. The images from NASA's MODIS (to the right) show a large plume of fresh water, which appears as a dark ribbon against the lighter-blue surrounding waters. These images demonstrate that the plume did not mix with the surrounding sea water immediately. Instead, it stayed intact as it flowed through the Gulf of Mexico, into the Straits of Florida, and entered the Gulf Stream. The Mississippi River water rounded the tip of Florida and traveled up the southeast coast to the latitude of Georgia before finally mixing in so thoroughly with the ocean that it could no longer be detected by MODIS.
Before 1900, the Mississippi River transported an estimated 400 million metric tons of sediment per year from the interior of the United States to coastal Louisiana and the Gulf of Mexico. During the last two decades, this number was only 145 million metric tons per year. The reduction in sediment transported down the Mississippi River is the result of engineering modification of the Mississippi, Missouri, and Ohio rivers and their tributaries by dams, meander cutoffs, river-training structures, and bank revetments and soil erosion control programs in the areas drained by them.
Course changes.
Over geologic time, the Mississippi River has experienced numerous large and small changes to its main course, as well as additions, deletions, and other changes among its numerous tributaries, and the lower Mississippi River has used different pathways as its main channel to the Gulf of Mexico across the delta region.
Through a natural process known as avulsion or delta switching, the lower Mississippi River has shifted its final course to the mouth of the Gulf of Mexico every thousand years or so. This occurs because the deposits of silt and sediment begin to clog its channel, raising the river's level and causing it to eventually find a steeper, more direct route to the Gulf of Mexico. The abandoned distributaries diminish in volume and form what are known as bayous. This process has, over the past 5,000 years, caused the coastline of south Louisiana to advance toward the Gulf from 15 to. The currently active delta lobe is called the Birdfoot Delta, after its shape, or the Balize Delta, after La Balize, Louisiana, the first French settlement at the mouth of the Mississippi.
Prehistoric courses.
The current form of the Mississippi River basin was largely shaped by the Laurentide Ice Sheet of the most recent Ice Age. The southernmost extent of this enormous glaciation extended well into the present-day United States and Mississippi basin. When the ice sheet began to recede, hundreds of feet of rich sediment were deposited, creating the flat and fertile landscape of the Mississippi Valley. During the melt, giant glacial rivers found drainage paths into the Mississippi watershed, creating such features as the Minnesota River, James River, and Milk River valleys. When the ice sheet completely retreated, many of these "temporary" rivers found paths to Hudson Bay or the Arctic Ocean, leaving the Mississippi Basin with many features "oversized" for the existing rivers to have carved in the same time period.
Ice sheets during the Illinoian Stage about 300,000 to 132,000 years before present, blocked the Mississippi near Rock Island, Illinois, diverting it to its present channel farther to the west, the current western border of Illinois. The Hennepin Canal roughly follows the ancient channel of the Mississippi downstream from Rock Island to Hennepin, Illinois. South of Hennepin, to Alton, Illinois, the current Illinois River follows the ancient channel used by the Mississippi River before the Illinoian Stage.
Timeline of outflow course changes
Historic course changes.
In March 1876, the Mississippi suddenly changed course near the settlement of Reverie, Tennessee, leaving a small part of Tipton County, Tennessee, attached to Arkansas and separated from the rest of Tennessee by the new river channel. Since this event was an avulsion, rather than the effect of incremental erosion and deposition, the state line still follows the old channel.
New Madrid Seismic Zone.
The New Madrid Seismic Zone, along the Mississippi River near New Madrid, Missouri, between Memphis and St. Louis, is related to an aulacogen (failed rift) that formed at the same time as the Gulf of Mexico. This area is still quite active seismically. Four great earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, had tremendous local effects in the then sparsely settled area, and were felt in many other places in the midwestern and eastern U.S. These earthquakes created Reelfoot Lake in Tennessee from the altered landscape near the river.
Cultural geography.
State boundaries.
The Mississippi River runs through or along 10 states, from Minnesota to Louisiana, and was used to define portions of these states' borders, with Wisconsin, Illinois, Kentucky, Tennessee, and Mississippi along the east side of the river, and Iowa, Missouri, and Arkansas along its west side. Substantial parts of both Minnesota and Louisiana are on either side of the river, although the Mississippi defines part of the boundary of each of these states.
In all of these cases, the middle of the riverbed at the time the borders were established was used as the line to define the borders between adjacent states. In various areas, the river has since shifted, but the state borders have not changed, still following the former bed of the Mississippi River as of their establishment, leaving several small isolated areas of one state across the new river channel, contiguous with the adjacent state. Also, due to a meander in the river, a small part of western Kentucky is contiguous with Tennessee, but isolated from the rest of its state.
Lake Pepin, the widest naturally occurring part of the Mississippi, is part of the Minnesota–Wisconsin border.
Communities along the river.
Many of the communities along the Mississippi River are listed below; most have either historic significance or cultural lore connecting them to the river. They are sequenced from the source of the river to its end.
Bridge crossings.
The first bridge across the Mississippi River was built in 1855. It spanned the river in Minneapolis, Minnesota where the current Hennepin Avenue Bridge is located. No highway or railroad tunnels cross under the Mississippi River.
The first railroad bridge across the Mississippi was built in 1856. It spanned the river between the Rock Island Arsenal in Illinois and Davenport, Iowa. Steamboat captains of the day, fearful of competition from the railroads, considered the new bridge a hazard to navigation. Two weeks after the bridge opened, the steamboat "Effie Afton" rammed part of the bridge, catching it on fire. Legal proceedings ensued, with Abraham Lincoln defending the railroad. The lawsuit went to the Supreme Court of the United States and was eventually ruled in favor of the railroad.
Below is a general overview of selected Mississippi bridges which have notable engineering or landmark significance, with their cities or locations. They are sequenced from the Upper Mississippi's source to the Lower Mississippi's mouth.
Navigation and flood control.
A clear channel is needed for the barges and other vessels that make the main stem Mississippi one of the great commercial waterways of the world. The task of maintaining a navigation channel is the responsibility of the United States Army Corps of Engineers, which was established in 1802. Earlier projects began as early as 1829 to remove snags, close off secondary channels and excavate rocks and sandbars.
Steamboats entered trade in the 1820s, so the period 1830 – 1850 became the golden age of steamboats. As there were few roads or rails in the lands of the Louisiana Purchase, river traffic was an ideal solution. Cotton, timber and food came down the river, as did Appalachian coal. The port of New Orleans boomed as it was the trans-shipment point to deep sea ocean vessels. As a result, the image of the twin stacked, wedding cake Mississippi steamer entered into American mythology. Steamers worked the entire route from the trickles of Montana, to the Ohio river; down the Missouri and Tennessee, to the main channel of the Mississippi. Only with the arrival of the railroads in the 1880s did steamboat traffic diminish. Steamboats remained a feature until the 1920s. Most have been superseded by pusher tugs. A few survive as icons—the Delta Queen and the River Queen for instance.
A series of 29 locks and dams on the upper Mississippi, most of which were built in the 1930s, is designed primarily to maintain a 9 ft deep channel for commercial barge traffic. The lakes formed are also used for recreational boating and fishing. The dams make the river deeper and wider but do not stop it. No flood control is intended. During periods of high flow, the gates, some of which are submersible, are completely opened and the dams simply cease to function. Below St. Louis, the Mississippi is relatively free-flowing, although it is constrained by numerous levees and directed by numerous wing dams.
 Barges on the Mississippi River near Ste. Genevieve, Missouri.
19th century.
In 1829, there were surveys of the two major obstacles on the upper Mississippi, the Des Moines Rapids and the Rock Island Rapids, where the river was shallow and the riverbed was rock. The Des Moines Rapids were about 11 mi (18 km) long and just above the mouth of the Des Moines River at Keokuk, Iowa. The Rock Island Rapids were between Rock Island and Moline, Illinois. Both rapids were considered virtually impassable.
In 1848, the Illinois and Michigan Canal was built to connect the Mississippi River to Lake Michigan via the Illinois River near Peru, Illinois. The canal allowed shipping between these important waterways. In 1900, the canal was replaced by the Chicago Sanitary and Ship Canal. The second canal, in addition to shipping, also allowed Chicago to address specific health issues (typhoid fever, cholera and other waterborne diseases) by sending its waste down the Illinois and Mississippi river systems rather than polluting its water source of Lake Michigan.
The Corps of Engineers recommended the excavation of a 5 ft-deep channel at the Des Moines Rapids, but work did not begin until after Lieutenant Robert E. Lee endorsed the project in 1837. The Corps later also began excavating the Rock Island Rapids. By 1866, it had become evident that excavation was impractical, and it was decided to build a canal around the Des Moines Rapids. The canal opened in 1877, but the Rock Island Rapids remained an obstacle. In 1878, Congress authorized the Corps to establish a 4.5 ft-deep channel to be obtained by building wing dams which direct the river to a narrow channel causing it to cut a deeper channel, by closing secondary channels and by dredging. The channel project was complete when the Moline Lock, which bypassed the Rock Island Rapids, opened in 1907.
To improve navigation between St. Paul, Minnesota, and Prairie du Chien, Wisconsin, the Corps constructed several dams on lakes in the headwaters area, including Lake Winnibigoshish and Lake Pokegama. The dams, which were built beginning in the 1880s, stored spring run-off which was released during low water to help maintain channel depth.
20th century.
In 1907, Congress authorized a 6 ft deep channel project on the Mississippi, which was not complete when it was abandoned in the late 1920s in favor of the 9 ft deep channel project.
In 1913, construction was complete on Lock and Dam No. 19 at Keokuk, Iowa, the first dam below St. Anthony Falls. Built by a private power company (Union Electric Company of St. Louis) to generate electricity (originally for Streetcars in St. Louis), the Keokuk dam was one of the largest hydro-electric plants in the world at the time. The dam also eliminated the Des Moines Rapids. Lock and Dam No. 1 was completed in Minneapolis, Minnesota in 1917. Lock and Dam No. 2, near Hastings, Minnesota was completed in 1930.
Before the Great Mississippi Flood of 1927, the Corps's primary strategy was to close off as many side channels as possible to increase the flow in the main river. It was thought that the river's velocity would scour off bottom sediments, deepening the river and decreasing the possibility of flooding. The 1927 flood proved this to be so wrong that communities threatened by the flood began to create their own levee breaks to relieve the force of the rising river.
The Rivers and Harbors Act of 1930 authorized the 9 ft channel project, which called for a navigation channel 9 feet deep and 400 ft wide to accommodate multiple-barge tows. This was achieved by a series of locks and dams, and by dredging. Twenty-three new locks and dams were built on the upper Mississippi in the 1930s in addition to the three already in existence.
Until the 1950s, there was no dam below Lock and Dam 26 at Alton, Illinois. Chain of Rocks Lock (Lock and Dam No. 27), which consists of a low-water dam and an 8.4 mi long canal, was added in 1953, just below the confluence with the Missouri River, primarily to bypass a series of rock ledges at St. Louis. It also serves to protect the St. Louis city water intakes during times of low water.
U.S. government scientists determined in the 1950s that the Mississippi River was starting to switch to the Atchafalaya River channel because of its much steeper path to the Gulf of Mexico. Eventually the Atchafalaya River would capture the Mississippi River and become its main channel to the Gulf of Mexico, leaving New Orleans on a side channel. As a result, the U.S. Congress authorized a project called the Old River Control Structure, which has prevented the Mississippi River from leaving its current channel that drains into the Gulf via New Orleans.
Because the large scale of high-energy water flow threatened to damage the structure, an auxiliary flow control station was built adjacent to the standing control station. This US$300 million project was completed in 1986 by the U.S. Army Corps Of Engineers. Beginning in the 1970s, the Corps applied hydrological transport models to analyze flood flow and water quality of the Mississippi. Dam 26 at Alton, Illinois, which had structural problems, was replaced by the Mel Price Lock and Dam in 1990. The original Lock and Dam 26 was demolished.
21st century.
The Corps now actively creates and maintains spillways and floodways to divert periodic water surges into backwater channels and lakes, as well as route part of the Mississippi's flow into the Atchafalaya Basin and from there to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. The main structures are the Birds Point-New Madrid Floodway in Missouri; the Old River Control Structure and the Morganza Spillway in Louisiana, which direct excess water down the west and east sides (respectively) of the Atchafalaya River; and the Bonnet Carré Spillway, also in Louisiana, which directs floodwaters to Lake Pontchartrain (see diagram).
Some of the pre-1927 strategy is still in use today, with the Corps actively cutting the necks of horseshoe bends, allowing the water to move faster and reducing flood heights.
History.
Native Americans.
The area of the Mississippi River basin was first settled by hunting and gathering Native American peoples and is considered one of the few independent centers of plant domestication in human history. Evidence of early cultivation of sunflower, a goosefoot, a marsh elder and an indigenous squash dates to the 4th millennium BCE. The lifestyle gradually became more settled after around 1000 BCE during what is now called the Woodland period, with increasing evidence of shelter construction, pottery, weaving and other practices. A network of trade routes referred to as the Hopewell interaction sphere was active along the waterways between about 200 and 500 CE, spreading common cultural practices over the entire area between the Gulf of Mexico and the Great Lakes. A period of more isolated communities followed, and agriculture introduced from Mesoamerica based on the Three Sisters (maize, beans and squash) gradually came to dominate. After around 800 CE there arose an advanced agricultural society today referred to as the Mississippian culture, with evidence of highly stratified complex chiefdoms and large population centers. The most prominent of these, now called Cahokia, was occupied between about 600 and 1400 CE and at its peak numbered between 8,000 and 40,000 inhabitants, larger than London, England of that time. At the time of first contact with Europeans, Cahokia and many other Mississippian cities had dispersed, and archaeological finds attest to increased social stress.
Modern American Indian nations inhabiting the Mississippi basin include Cheyenne, Sioux, Ojibwe, Potawatomi, Ho-Chunk, Fox, Kickapoo, Tamaroa, Moingwena, Quapaw and Chickasaw.
The word "Mississippi" itself comes from "Messipi", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, "Misi-ziibi" (Great River). The Ojibwe called Lake Itasca "Omashkoozo-zaaga'igan" (Elk Lake) and the river flowing out of it "Omashkoozo-ziibi" (Elk River). After flowing into Lake Bemidji, the Ojibwe called the river "Bemijigamaag-ziibi" (River from the Traversing Lake). After flowing into Cass Lake, the name of the river changes to "Gaa-miskwaawaakokaag-ziibi" (Red Cedar River) and then out of Lake Winnibigoshish as "Wiinibiigoonzhish-ziibi" (Miserable Wretched Dirty Water River), "Gichi-ziibi" (Big River) after the confluence with the Leech Lake River, then finally as "Misi-ziibi" (Great River) after the confluence with the Crow Wing River. After the expeditions by Giacomo Beltrami and Henry Schoolcraft, the longest stream above the juncture of the Crow Wing River and "Gichi-ziibi" was named "Mississippi River". The Mississippi River Band of Chippewa Indians, known as the "Gichi-ziibiwininiwag", are named after the stretch of the Mississippi River known as the "Gichi-ziibi". The Cheyenne, one of the earliest inhabitants of the upper Mississippi River, called it the "Máʼxe-éʼometaaʼe" (Big Greasy River) in the Cheyenne language. The Arapaho name for the river is Beesniicíe. The Pawnee name is "Kickaátit".
European exploration.
On May 8, 1541, Spanish explorer Hernando de Soto became the first recorded European to reach the Mississippi River, which he called "Río del Espíritu Santo" ("River of the Holy Spirit"), in the area of what is now Mississippi. In Spanish, the river is called "Río Mississippi".
French explorers Louis Jolliet and Jacques Marquette began exploring the Mississippi in the 17th century. Marquette traveled with a Sioux Indian who named it "Ne Tongo" ("Big river" in Sioux language) in 1673. Marquette proposed calling it the "River of the Immaculate Conception".
When Louis Jolliet explored the Mississippi Valley in the 17th century, natives guided him to a quicker way to return to French Canada via the Illinois River. When he found the Chicago Portage, he remarked that a canal of "only half a league" (less than 2 miles (3.2 km), 3 km) would join the Mississippi and the Great Lakes. In 1848, the continental divide separating the waters of the Great Lakes and the Mississippi Valley was breached by the Illinois and Michigan canal via the Chicago River. This both accelerated the development, and forever changed the ecology of the Mississippi Valley and the Great Lakes.
In 1682, René-Robert Cavelier, Sieur de La Salle and Henri de Tonti claimed the entire Mississippi River Valley for France, calling the river "Colbert River" after Jean-Baptiste Colbert and the region "La Louisiane", for King Louis XIV. On March 2, 1699, Pierre Le Moyne d'Iberville rediscovered the mouth of the Mississippi, following the death of La Salle. The French built the small fort of La Balise there to control passage.
In 1718, about 100 mi upriver, New Orleans was established along the river crescent by Jean-Baptiste Le Moyne, Sieur de Bienville, with construction patterned after the 1711 resettlement on Mobile Bay of Mobile, the capital of French Louisiana at the time.
In 1762 the entire region is part of the Spanish Louisiana from southern Canada to the Gulf of Mexico until 1802.
Colonization.
Following Britain's victory in the Seven Years War the Mississippi became the border between the British and Spanish Empires. The Treaty of Paris (1763) gave Great Britain rights to all land east of the Mississippi and Spain rights to land west of the Mississippi. Spain also ceded Florida to Britain to regain Cuba, which the British occupied during the war. Britain then divided the territory into East and West Florida.
Article 8 of the Treaty of Paris (1783) states, "The navigation of the river Mississippi, from its source to the ocean, shall forever remain free and open to the subjects of Great Britain and the citizens of the United States". With this treaty, which ended the American Revolutionary War, Britain also ceded West Florida back to Spain to regain the Bahamas, which Spain had occupied during the war. In 1800, under duress from Napoleon of France, Spain ceded an undefined portion of West Florida to France. When France then sold the Louisiana Territory to the U.S. in 1803, a dispute arose again between Spain and the U.S. on which parts of West Florida exactly had Spain ceded to France, which would in turn decide which parts of West Florida were now U.S. property versus Spanish property. These aspirations ended when Spain was pressured into signing Pinckney's Treaty in 1795.
France reacquired 'Louisiana' from Spain in the secret Treaty of San Ildefonso in 1800. The United States then bought the territory from France in the Louisiana Purchase of 1803. In 1815, the U.S. defeated Britain at the Battle of New Orleans, part of the War of 1812, securing American control of the river. So many settlers traveled westward through the Mississippi river basin, as well as settled in it, that Zadok Cramer wrote a guide book called "The Navigator", detailing the features and dangers and navigable waterways of the area. It was so popular that he updated and expanded it through 12 editions over a period of 25 years.
The colonization of the area was barely slowed by the three earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, that were centered near New Madrid, Missouri.
Steamboat era.
Mark Twain's book, "Life on the Mississippi", covered the steamboat commerce which took place from 1830 to 1870 on the river before more modern ships replaced the steamer. The book was published first in serial form in "Harper's Weekly" in seven parts in 1875. The full version, including a passage from the then unfinished "Adventures of Huckleberry Finn" and works from other authors, was published by James R. Osgood & Company in 1885.
The first steamboat to travel the full length of the Lower Mississippi from the Ohio River to New Orleans was the "New Orleans" in December 1811. Its maiden voyage occurred during the series of New Madrid earthquakes in 1811–12. Steamboat transport remained a viable industry, both in terms of passengers and freight until the end of the first decade of the 20th century. Among the several Mississippi River system steamboat companies was the noted Anchor Line, which, from 1859 to 1898, operated a luxurious fleet of steamers between St. Louis and New Orleans.
Civil War.
Control of the river was a strategic objective of both sides in the American Civil War. In 1862 Union forces coming down the river successfully cleared Confederate defenses at Island Number 10 and Memphis, Tennessee, while Naval forces coming upriver from the Gulf of Mexico captured New Orleans, Louisiana. The remaining major Confederate stronghold was on the heights overlooking the river at Vicksburg, Mississippi, and the Union's Vicksburg Campaign (December 1862 to July 1863), and the fall of Port Hudson, completed control of the lower Mississippi River. The Union victory ending the Siege of Vicksburg on July 4, 1863, was pivotal to the Union's final victory of the Civil War.
20th and 21st centuries.
The "Big Freeze" of 1918–19 blocked river traffic north of Memphis, Tennessee, preventing transportation of coal from southern Illinois. This resulted in widespread shortages, high prices, and rationing of coal in January and February.
In the spring of 1927, the river broke out of its banks in 145 places, during the Great Mississippi Flood of 1927 and inundated 27000 sqmi to a depth of up to 30 ft.
In 1962 and 1963, industrial accidents spilled 3.5 e6gal of soybean oil into the Mississippi and Minnesota rivers. The oil covered the Mississippi River from St. Paul to Lake Pepin, creating an ecological disaster and a demand to control water pollution.
On October 20, 1976, the automobile ferry, "MV George Prince", was struck by a ship traveling upstream as the ferry attempted to cross from Destrehan, Louisiana, to Luling, Louisiana. Seventy-eight passengers and crew died; only eighteen survived the accident.
In 1988, record low water levels provided an opportunity and obligation to examine the climax of the wooden-hulled age. The Mississippi fell to 10 ft below zero on the Memphis gauge. Water craft remains were exposed in an area of 4.5 acre on the bottom of the Mississippi River at West Memphis, Arkansas. They dated to the late 19th to early 20th centuries. The State of Arkansas, the Arkansas Archeological Survey, and the Arkansas Archeological Society responded with a two-month data recovery effort. The fieldwork received national media attention as good news in the middle of a drought.
The Great Flood of 1993 was another significant flood, primarily affecting the Mississippi above its confluence with the Ohio River at Cairo, Illinois.
Two portions of the Mississippi were designated as American Heritage Rivers in 1997: the lower portion around Louisiana and Tennessee, and the upper portion around Iowa, Illinois, Minnesota and Missouri.
In 2002, Slovenian long-distance swimmer Martin Strel swam the entire length of the river, from Minnesota to Louisiana, over the course of 68 days. In 2005, the Source to Sea Expedition paddled the Mississippi and Atchafalaya Rivers to benefit the Audubon Society's Upper Mississippi River Campaign.
Future.
Geologists believe that the lower Mississippi could take a new course to the Gulf. Either of two new routes – through the Atchafalaya Basin or through Lake Pontchartrain — might become the Mississippi's main channel if flood-control structures are overtopped or heavily damaged during a severe flood.
Failure of the Old River Control Structure, the Morganza Spillway, or nearby levees would likely re-route the main channel of the Mississippi through Louisiana's Atchafalaya Basin and down the Atchafalaya River to reach the Gulf of Mexico south of Morgan City in southern Louisiana. This route provides a more direct path to the Gulf of Mexico than the present Mississippi River channel through Baton Rouge and New Orleans. While the risk of such a diversion is present during any major flood event, such a change has so far been prevented by active human intervention involving the construction, maintenance, and operation of various levees, spillways, and other control structures by the U.S. Army Corps of Engineers.
The Old River Control Structure, between the present Mississippi River channel and the Atchafalaya Basin, sits at the normal water elevation and is ordinarily used to divert 30% of the Mississippi's flow to the Atchafalaya River. There is a steep drop here away from the Mississippi's main channel into the Atchafalaya Basin. If this facility were to fail during a major flood, there is a strong concern the water would scour and erode the river bottom enough to capture the Mississippi's main channel. The structure was nearly lost during the 1973 flood, but repairs and improvements were made after engineers studied the forces at play. In particular, the Corps of Engineers made many improvements and constructed additional facilities for routing water through the vicinity. These additional facilities give the Corps much more flexibility and potential flow capacity than they had in 1973, which further reduces the risk of a catastrophic failure in this area during other major floods, such as that of 2011.
Because the Morganza Spillway is located at slightly higher elevation well back from the river, it is normally dry on both sides. Even if this structure were to fail at the crest during a severe flood, the flood waters would have to cause a significant amount of erosion, down to normal water levels, before the Mississippi could permanently jump channel at this location. During the 2011 floods, the Corps of Engineers decided to open the Morganza Spillway to 1/4 of its capacity to allow 150,000 ft3/sec of water to flood the Morganza and Atchafalaya floodways and continue directly to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. In addition to reducing the Mississippi River crest downstream, this diversion reduced the chances of a channel change by reducing stress on the other elements of the control system.
Some geologists have noted that the possibility for course change into the Atchafalaya also exists in the area immediately north of the Old River Control Structure. Army Corps of Engineers geologist Fred Smith once stated, "The Mississippi wants to go west. 1973 was a forty-year flood. The big one lies out there somewhere—when the structures can't release all the floodwaters and the levee is going to have to give way. That is when the river's going to jump its banks and try to break through."
Another possible course change for the Mississippi River is a diversion into Lake Pontchartrain near New Orleans. This route is controlled by the Bonnet Carré Spillway, built to reduce flooding in New Orleans. This spillway and an imperfect natural levee about 4–6 meters (12 to 20 feet) high are all that prevents the Mississippi from taking a new, shorter course through Lake Pontchartrain to the Gulf of Mexico. Diversion of the Mississippi's main channel through Lake Pontchartrain would have consequences similar to an Atchafalaya diversion, but to a lesser extent, since the present river channel would remain in use past Baton Rouge and into the New Orleans area.
Recreation.
The sport of water skiing was invented on the river in a wide region between Minnesota and Wisconsin known as Lake Pepin. Ralph Samuelson of Lake City, Minnesota, created and refined his skiing technique in late June and early July 1922. He later performed the first water ski jump in 1925 and was pulled along at 80 mph by a Curtiss flying boat later that year.
There are seven National Park Service sites along the Mississippi River. The Mississippi National River and Recreation Area is the National Park Service site dedicated to protecting and interpreting the Mississippi River itself. The other six National Park Service sites along the river are (listed from north to south):

</doc>
<doc id="19581" url="http://en.wikipedia.org/wiki?curid=19581" title="Men in Black">
Men in Black

In popular culture and UFO conspiracy theories, Men in Black (MIB) are men dressed in black suits who claim to be government agents who harass or threaten UFO witnesses to keep them quiet about what they have seen. It is sometimes implied that they may be aliens themselves. The term is also frequently used to describe mysterious men working for unknown organizations, as well as various branches of government allegedly designed to protect secrets or perform other strange activities. The term is generic, used for any unusual, threatening or strangely behaved individual whose appearance on the scene can be linked in some fashion with a UFO sighting.
Folklore.
Folklorist Peter Rojcewicz compared Men in Black accounts to tales of people encountering the devil and speculated they could be considered a kind of "psychological drama".
UFOlogists.
Men in Black figure prominently in UFOlogy and UFO folklore. In 1947, Harold Dahl claimed to have been warned not to talk about his alleged UFO sighting on Maury Island by a man in a dark suit. In the mid 1950s, UFOlogist Albert K. Bender claimed he was visited by men in dark suits who threatened and warned him not to continue investigating UFOs. Bender believed Men in Black were secret government agents tasked with suppressing evidence of UFOs. The UFOlogist John Keel claimed to have encounters with Men in Black, and referred to them as "demonic supernaturals" with "dark skin and/or “exotic” facial features". According to UFOlogist Jerome Clark, reports of Men in Black represent "experiences" that “don’t seem to have occurred in the world of consensus reality.”
Hoax.
In his article, "Gray Barker: My Friend, the Myth-Maker," John C. Sherwood claims that, in the late 1960s, at the age of 18, he cooperated when Gray Barker urged him to develop a hoax – which Barker subsequently published – about what Barker called "blackmen", three mysterious UFO inhabitants who silenced Sherwood's pseudonymous identity, "Dr. Richard H. Pratt".
In popular culture.
British punk rockers The Stranglers released The Gospel According to the Meninblack in 1981. Previous songs like Meninblack and Who Wants The World also explored the band's fascination with the legend.
The first film appearance of Men in Black was in Hangar 18 (1980), that had four credits for MIBs, who chase the films protagonists and try to prevent them from finding the truth.
Later they appeared in John Sayles' 1984 film "The Brother from Another Planet". In this film, John Sayles himself and David Strathairn, both credited as "Man In Black", are aliens in search of an escaped alien slave (the titular "Brother").
Blue Öyster Cult directly mention the Men In Black in the lyrics to two of their songs. In the opening verse of 1976's "E.T.I (Extra Terrestrial Intelligence)" we are told: "I hear the music, daylight disc, Three men in black said, "Don't report this"". Then in 1983's "Take me away": "Don't ask if they are real, The men in black, their lips are sealed".
In the 1988 comical video game Zak McKracken and the Alien Mindbenders, that took place in the year 1997. The design of the villanious Caponian aliens was based on the Men in Black and the mafia (Al Capone). Their leader was nicknamed "The King", that resembled Elvis Presley or even was subtly implied to be Elvis Presley himself.
In the role-playing game, Mage: The Ascension; the Men In Black are a methodology of the New World Order, a convention of technology focused mages that use information control and espionage to enforce the scientific paradigm. 
In the alternate history short story "Dukakis and the Aliens" by Robert Sheckley contained in the anthology "Alternate Presidents", Michael Dukakis is elected president in 1988. However, he is reveled to be an alien attempting to infiltrate Dulce Base. This results in the Men in Black (along with friendly aliens) to rewrite history in order to let George H. W. Bush to win the election, instead. 
Frank Black, the singer for The Pixies also known by the pseudonym Black Francis, released a single entitled "Men in Black" in 1995 which subsequently appeared on his album The Cult of Ray. He described the song in 1996 by stating that "it's about the Men in Black who are the psychological intimidators sent by the alien or maybe the government or maybe both."
"Men in Black" (1997), starring Tommy Lee Jones and Will Smith as Agent K and Agent J respectively, was based on Lowell Cunningham's comic book about a secret organization that monitors and regulates alien activity on Earth – "The Men in Black" from Aircel Comics. The film was followed by "" and its 2002 sequel "Men in Black II". "Men in Black 3" was released on May 25, 2012. Scott Mitchell Rosenberg, who published the comic book, took the property to Sony to become a billion-dollar film franchise. Will Smith made a song called "Men in Black", for the movie "Men in Black" in 1997, and "Black Suits Comin' (Nod Ya Head)" for its sequel in 2002.

</doc>
<doc id="19582" url="http://en.wikipedia.org/wiki?curid=19582" title="May 7">
May 7

May 7 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19583" url="http://en.wikipedia.org/wiki?curid=19583" title="Monomer">
Monomer

A monomer ( ) ("mono-", "one" + "-mer", "part") is a molecule that may bind chemically to other molecules to form a polymer. The term "monomeric protein" may also be used to describe one of the proteins making up a multiprotein complex. The most common natural monomer is glucose, which is linked by glycosidic bonds into polymers such as cellulose and starch, and is over 77% of the mass of all plant matter. Most often the term "monomer" refers to the organic molecules which form synthetic polymers, such as, for example, vinyl chloride, which is used to produce the polymer polyvinyl chloride (PVC).The process by which monomers combine end to end to form a polymer is called polymerization. Molecules made of a small number of monomer units, up to a few dozen, are called oligomers.
IUPAC definition
 Monomer: A substance composed of "monomer molecules".
Monomer molecule: A molecule which can undergo polymerization, therebycontributing constitutional units to the essential structure of a macromolecule. 
Natural monomers.
Amino acids are natural monomers that polymerize at ribosomes to form proteins. Nucleotides, monomers found in the cell nucleus, polymerize to form nucleic acids – DNA and RNA. Glucose monomers can polymerize to form starches, glycogen or cellulose; xylose monomers can polymerise to form xylan. In all these cases and is thus not pliable, a hydrogen atom and a hydroxyl (-OH) group are lost to form H2O, and an oxygen atom links each monomer unit. Due to the formation of water as one of the products, these reactions are known as dehydration.
Isoprene is a natural monomer and polymerizes to form natural rubber, most often "cis-"1,4-polyisoprene, but also "trans-"1,4-polymer
Molecular weight.
The lower molecular weight compounds built from monomers are also referred to as dimers, trimers, tetramers, pentamers, hexamers, heptamers, octamers, nonamers, decamers, dodecamers, eicosamers, etc. if they have 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, or 20 monomer units, respectively. Any number of these monomer units may be indicated by the appropriate Greek prefix. Larger numbers are often stated in English or numbers instead of Greek; e.g., a "20-mer" is formed from 20 monomers. Molecules made of a small number of monomer units, up to a few dozen, are called oligomers.
Industrial use.
Considering the current tight monomers market, particularly in propylene, and the benefits of membrane-based recovery processes, major polyolefin producers around the world already employ such recovery processes in new state-of-the-art plants. In order to enhance the competitiveness of older plants, the use of a recovery solution is becoming mandatory.

</doc>
<doc id="19588" url="http://en.wikipedia.org/wiki?curid=19588" title="Mitochondrion">
Mitochondrion

The mitochondrion (plural mitochondria) is a membrane-bound organelle found in most eukaryotic cells. The word mitochondrion comes from the Greek "μίτος", "mitos", i.e. "thread", and "χονδρίον", "chondrion", i.e. "granule" or "grain-like".
Mitochondria range from 0.5 to 1.0 μm in diameter. These structures are sometimes described as "the powerhouse of the cell" because they generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy. In addition to supplying cellular energy, mitochondria are involved in other tasks such as signaling, cellular differentiation, cell death, as well as maintaining the control of the cell cycle and cell growth. Mitochondria have been implicated in several human diseases, including mitochondrial disorders, cardiac dysfunction, and heart failure. A recent University of California study including ten children diagnosed with severe autism suggests that autism may be correlated with mitochondrial defects as well.
Several characteristics make mitochondria unique. The number of mitochondria in a cell can vary widely by organism, tissue, and cell type. For instance, red blood cells have no mitochondria, whereas liver cells can have more than 2000. The organelle is composed of compartments that carry out specialized functions. These compartments or regions include the outer membrane, the intermembrane space, the inner membrane, and the cristae and matrix. Mitochondrial proteins vary depending on the tissue and the species. In humans, 615 distinct types of protein have been identified from cardiac mitochondria, whereas in rats, 940 proteins have been reported. The mitochondrial proteome is thought to be dynamically regulated. Although most of a cell's DNA is contained in the cell nucleus, the mitochondrion has its own independent genome. Further, its DNA shows substantial similarity to bacterial genomes.
History.
The first observations of intracellular structures that probably represent mitochondria were published in the 1840s. Richard Altmann, in 1894, established them as cell organelles and called them "bioblasts". The term "mitochondria" itself was coined by Carl Benda in 1898. Leonor Michaelis discovered that Janus green can be used as a supravital stain for mitochondria in 1900. Friedrich Meves, in 1904, made the first recorded observation of mitochondria in plants ("Nymphaea alba") and in 1908, along with Claudius Regaud, suggested that they contain proteins and lipids. Benjamin F. Kingsbury, in 1912, first related them with cell respiration, but almost exclusively based on morphological observations. In 1913 particles from extracts of guinea-pig liver were linked to respiration by Otto Heinrich Warburg, which he called "grana". Warburg and Heinrich Otto Wieland, who had also postulated a similar particle mechanism, disagreed on the chemical nature of the respiration. It was not until 1925 when David Keilin discovered cytochromes that the respiratory chain was described.
In 1939, experiments using minced muscle cells demonstrated that cellular respiration using one oxygen atom can form two adenosine triphosphate (ATP) molecules, and, in 1941, the concept of the phosphate bonds of ATP being a form of energy in cellular metabolism was developed by Fritz Albert Lipmann. In the following years, the mechanism behind cellular respiration was further elaborated, although its link to the mitochondria was not known. The introduction of tissue fractionation by Albert Claude allowed mitochondria to be isolated from other cell fractions and biochemical analysis to be conducted on them alone. In 1946, he concluded that cytochrome oxidase and other enzymes responsible for the respiratory chain were isolated to the mitchondria. Over time, the fractionation method was tweaked, improving the quality of the mitochondria isolated, and other elements of cell respiration were determined to occur in the mitochondria.
The first high-resolution micrographs appeared in 1952, replacing the Janus Green stains as the preferred way of visualising the mitochondria. This led to a more detailed analysis of the structure of the mitochondria, including confirmation that they were surrounded by a membrane. It also showed a second membrane inside the mitochondria that folded up in ridges dividing up the inner chamber and that the size and shape of the mitochondria varied from cell to cell.
The popular term "powerhouse of the cell" was coined by Philip Siekevitz in 1957.
In 1967, it was discovered that mitochondria contained ribosomes. In 1968, methods were developed for mapping the mitochondrial genes, with the genetic and physical map of yeast mitochondria being completed in 1976.
Structure.
A mitochondrion contains outer and inner membranes composed of phospholipid bilayers and proteins. The two membranes have different properties. Because of this double-membraned organization, there are five distinct parts to a mitochondrion. They are:
Mitochondria stripped of their outer membrane are called mitoplasts.
Outer membrane.
The outer mitochondrial membrane, which encloses the entire organelle, is 60 to 75 angstroms (Å) thick. It has a protein-to-phospholipid ratio similar to that of the eukaryotic plasma membrane (about 1:1 by weight). It contains large numbers of integral membrane proteins called porins. These porins form channels that allow molecules of 5000 daltons or less in molecular weight to freely diffuse from one side of the membrane to the other.Larger proteins can enter the mitochondrion if a signaling sequence at their N-terminus binds to a large multisubunit protein called translocase of the outer membrane, which then actively moves them across the membrane.Mitochondrial pro-proteins are imported through specialised translocate complexes. The outer membrane also contains enzymes involved in such diverse activities as the elongation of fatty acids, oxidation of epinephrine, and the degradation of tryptophan. These enzymes include monoamine oxidase, rotenone-insensitive NADH-cytochrome c-reductase, kynurenine hydroxylase and fatty acid Co-A ligase. Disruption of the outer membrane permits proteins in the intermembrane space to leak into the cytosol, leading to certain cell death. The mitochondrial outer membrane can associate with the endoplasmic reticulum (ER) membrane, in a structure called MAM (mitochondria-associated ER-membrane). This is important in the ER-mitochondria calcium signaling and is involved in the transfer of lipids between the ER and mitochondria.
Outside the outer membrane there are small (diameter: 60Å) particles named sub-units of Parson.
Intermembrane space.
The intermembrane space is the space between the outer membrane and the inner membrane. It is also known as perimitochondrial space. Because the outer membrane is freely permeable to small molecules, the concentrations of small molecules such as ions and sugars in the intermembrane space is the same as the cytosol. However, large proteins must have a specific signaling sequence to be transported across the outer membrane, so the protein composition of this space is different from the protein composition of the cytosol. One protein that is localized to the intermembrane space in this way is cytochrome c.
Inner membrane.
The inner mitochondrial membrane contains proteins with five types of functions:
It contains more than 151 different polypeptides, and has a very high protein-to-phospholipid ratio (more than 3:1 by weight, which is about 1 protein for 15 phospholipids). The inner membrane is home to around 1/5 of the total protein in a mitochondrion. In addition, the inner membrane is rich in an unusual phospholipid, cardiolipin. This phospholipid was originally discovered in cow hearts in 1942, and is usually characteristic of mitochondrial and bacterial plasma membranes. Cardiolipin contains four fatty acids rather than two, and may help to make the inner membrane impermeable. Unlike the outer membrane, the inner membrane doesn't contain porins, and is highly impermeable to all molecules. Almost all ions and molecules require special membrane transporters to enter or exit the matrix. Proteins are ferried into the matrix via the translocase of the inner membrane (TIM) complex or via Oxa1. In addition, there is a membrane potential across the inner membrane, formed by the action of the enzymes of the electron transport chain.
Cristae.
The inner mitochondrial membrane is compartmentalized into numerous cristae, which expand the surface area of the inner mitochondrial membrane, enhancing its ability to produce ATP. For typical liver mitochondria, the area of the inner membrane is about five times as large as the outer membrane. This ratio is variable and mitochondria from cells that have a greater demand for ATP, such as muscle cells, contain even more cristae. These folds are studded with small round bodies known as F1 particles or oxysomes. These are not simple random folds but rather invaginations of the inner membrane, which can affect overall chemiosmotic function.
One recent mathematical modeling study has suggested that the optical properties of the cristae in filamentous mitochondria may affect the generation and propagation of light within the tissue.
Matrix.
The matrix is the space enclosed by the inner membrane. It contains about 2/3 of the total protein in a mitochondrion. The matrix is important in the production of ATP with the aid of the ATP synthase contained in the inner membrane. The matrix contains a highly concentrated mixture of hundreds of enzymes, special mitochondrial ribosomes, tRNA, and several copies of the mitochondrial DNA genome. Of the enzymes, the major functions include oxidation of pyruvate and fatty acids, and the citric acid cycle.
Mitochondria have their own genetic material, and the machinery to manufacture their own RNAs and proteins ("see: protein biosynthesis"). A published human mitochondrial DNA sequence revealed 16,569 base pairs encoding 37 total genes: 22 tRNA, 2 rRNA, and 13 peptide genes. The 13 mitochondrial peptides in humans are integrated into the inner mitochondrial membrane, along with proteins encoded by genes that reside in the host cell's nucleus.
Mitochondria-associated ER membrane (MAM).
The mitochondria-associated ER membrane (MAM) is another structural element that is increasingly recognized for its critical role in cellular physiology and homeostasis. Once considered a technical snag in cell fractionation techniques, the alleged ER vesicle contaminants that invariably appeared in the mitochondrial fraction have been re-identified as membranous structures derived from the MAM—the interface between mitochondria and the ER. Physical coupling between these two organelles had previously been observed in electron micrographs and has more recently been probed with fluorescence microscopy. Such studies estimate that at the MAM, which may comprise up to 20% of the mitochondrial outer membrane, the ER and mitochondria are separated by a mere 10–25 nm and held together by protein tethering complexes.
Purified MAM from subcellular fractionation has shown to be enriched in enzymes involved in phospholipid exchange, in addition to channels associated with Ca2+ signaling. These hints of a prominent role for the MAM in the regulation of cellular lipid stores and signal transduction have been borne out, with significant implications for mitochondrial-associated cellular phenomena, as discussed below. Not only has the MAM provided insight into the mechanistic basis underlying such physiological processes as intrinsic apoptosis and the propagation of calcium signaling, but it also favors a more refined view of the mitochondria. Though often seen as static, isolated 'powerhouses' hijacked for cellular metabolism through an ancient endosymbiotic event, the evolution of the MAM underscores the extent to which mitochondria have been integrated into overall cellular physiology, with intimate physical and functional coupling to the endomembrane system.
Phospholipid transfer.
The MAM is enriched in enzymes involved in lipid biosynthesis, such as phosphatidylserine synthase on the ER face and phosphatidylserine decarboxylase on the mitochondrial face. Because mitochondria are dynamic organelles constantly undergoing fission and fusion events, they require a constant and well-regulated supply of phospholipids for membrane integrity. But mitochondria are not only a destination for the phospholipids they finish synthesis of; rather, this organelle also plays a role in inter-organelle trafficking of the intermediates and products of phospholipid biosynthetic pathways, ceramide and cholesterol metabolism, and glycosphingolipid anabolism.
Such trafficking capacity depends on the MAM, which has been shown to facilitate transfer of lipid intermediates between organelles. In contrast to the standard vesicular mechanism of lipid transfer, evidence indicates that the physical proximity of the ER and mitochondrial membranes at the MAM allows for lipid flipping between opposed bilayers. Despite this unusual and seemingly energetically unfavorable mechanism, such transport does not require ATP. Instead, in yeast, it has been shown to be dependent on a multiprotein tethering structure termed the ER-mitochondria encounter structure, or ERMES, although it remains unclear whether this structure directly mediates lipid transfer or is required to keep the membranes in sufficiently close proximity to lower the energy barrier for lipid flipping.
The MAM may also be part of the secretory pathway, in addition to its role in intracellular lipid trafficking. In particular, the MAM appears to be an intermediate destination between the rough ER and the Golgi in the pathway that leads to very-low-density lipoprotein, or VLDL, assembly and secretion. The MAM thus serves as a critical metabolic and trafficking hub in lipid metabolism.
Calcium signaling.
A critical role for the ER in calcium signaling was acknowledged before such a role for the mitochondria was widely accepted, in part because the low affinity of Ca2+ channels localized to the outer mitochondrial membrane seemed to fly in the face of this organelle's purported responsiveness to changes in intracellular Ca2+ flux. But the presence of the MAM resolves this apparent contradiction: the close physical association between the two organelles results in Ca2+ microdomains at contact points that facilitate efficient Ca2+ transmission from the ER to the mitochondria. Transmission occurs in response to so-called "Ca2+ puffs" generated by spontaneous clustering and activation of IP3R, a canonical ER membrane Ca2+ channel.
The fate of these puffs—in particular, whether they remain restricted to isolated locales or integrated into Ca2+ waves for propagation throughout the cell—is determined in large part by MAM dynamics. Although reuptake of Ca2+ by the ER (concomitant with its release) modulates the intensity of the puffs, thus insulating mitochondria to a certain degree from high Ca2+ exposure, the MAM often serves as a firewall that essentially buffers Ca2+ puffs by acting as a sink into which free ions released into the cytosol can be funneled. This Ca2+ tunneling occurs through the low-affinity Ca2+ receptor VDAC1, which recently has been shown to be physically tethered to the IP3R clusters on the ER membrane and enriched at the MAM. The ability of mitochondria to serve as a Ca2+ sink is a result of the electrochemical gradient generated during oxidative phosphorylation, which makes tunneling of the cation an exergonic process.
But transmission of Ca2+ is not unidirectional; rather, it is a two-way street. The properties of the Ca2+ pump SERCA and the channel IP3R present on the ER membrane facilitate feedback regulation coordinated by MAM function. In particular, clearance of Ca2+ by the MAM allows for spatio-temporal patterning of Ca2+ signaling because Ca2+ alters IP3R activity in a biphasic manner. SERCA is likewise affected by mitochondrial feedback: uptake of Ca2+ by the MAM stimulates ATP production, thus providing energy that enables SERCA to reload the ER with Ca2+ for continued Ca2+ efflux at the MAM. Thus, the MAM is not a passive buffer for Ca2+ puffs; rather it helps modulate further Ca2+ signaling through feedback loops that affect ER dynamics.
Regulating ER release of Ca2+ at the MAM is especially critical because only a certain window of Ca2+ uptake sustains the mitochondria, and consequently the cell, at homeostasis. Sufficient intraorganelle Ca2+ signaling is required to stimulate metabolism by activating dehydrogenase enzymes critical to flux through the citric acid cycle. However, once Ca2+ signaling in the mitochondria passes a certain threshold, it stimulates the intrinsic pathway of apoptosis in part by collapsing the mitochondrial membrane potential required for metabolism. Studies examining the role of pro- and anti-apoptotic factors support this model; for example, the anti-apoptotic factor Bcl-2 has been shown to interact with IP3Rs to reduce Ca2+ filling of the ER, leading to reduced efflux at the MAM and preventing collapse of the mitochondrial membrane potential post-apoptotic stimuli. Given the need for such fine regulation of Ca2+ signaling, it is perhaps unsurprising that dysregulated mitochondrial Ca2+ has been implicated in several neurodegenerative diseases, while the catalogue of tumor suppressors includes a few that are enriched at the MAM.
Molecular basis for tethering.
Recent advances in the identification of the tethers between the mitochondrial and ER membranes suggest that the scaffolding function of the molecular elements involved is secondary to other, non-structural functions. In yeast, ERMES, a multiprotein complex of interacting ER- and mitochondrial-resident membrane proteins, is required for lipid transfer at the MAM and exemplifies this principle. One of its components, for example, is also a constituent of the protein complex required for insertion of transmembrane beta-barrel proteins into the lipid bilayer. However, a homologue of the ERMES complex has not been identified yet in mammalian cells. Other proteins implicated in scaffolding likewise have functions independent of structural tethering at the MAM; for example, ER-resident and mitochondrial-resident mitofusins form heterocomplexes that regulate the number of inter-organelle contact sites, although mitofusins were first identified for their role in fission and fusion events between individual mitochondria. Glucose-related protein 75 (grp75) is another dual-function protein. In addition to the matrix pool of grp75, a portion serves as a chaperone that physically links the mitochondrial and ER Ca2+ channels VDAC and IP3R for efficient Ca2+ transmission at the MAM. Another potential tether is Sigma-1R, a non-opioid receptor whose stabilization of ER-resident IP3R may preserve communication at the MAM during the metabolic stress response.
Perspective.
The MAM is a critical signaling, metabolic, and trafficking hub in the cell that allows for the integration of ER and mitochondrial physiology. Coupling between these organelles is not simply structural but functional as well and critical for overall cellular physiology and homeostasis. The MAM thus offers a perspective on mitochondria that diverges from the traditional view of this organelle as a static, isolated unit appropriated for its metabolic capacity by the cell. Instead, this mitochondrial-ER interface emphasizes the integration of the mitochondria, the product of an endosymbiotic event, into diverse cellular processes.
Organization and distribution.
Mitochondria are found in nearly all eukaryotes. They vary in number and location according to cell type. A single mitochondrion is often found in unicellular organisms. Conversely, numerous mitochondria are found in human liver cells, with about 1000–2000 mitochondria per cell, making up 1/5 of the cell volume. The mitochondrial content of otherwise similar cells can vary substantially in size and membrane potential, with differences arising from sources including uneven partitioning at cell divisions, leading to extrinsic differences in ATP levels and downstream cellular processes. The mitochondria can be found nestled between myofibrils of muscle or wrapped around the sperm flagellum. Often they form a complex 3D branching network inside the cell with the cytoskeleton. The association with the cytoskeleton determines mitochondrial shape, which can affect the function as well. Mitochondria in cells are always distributed along microtubules and the distribution of these organelles is also correlated with the endoplasmic reticulum. Recent evidence suggests that vimentin, one of the components of the cytoskeleton, is also critical to the association with the cytoskeleton.
Function.
The most prominent roles of mitochondria are to produce the energy currency of the cell, ATP (i.e., phosphorylation of ADP), through respiration, and to regulate cellular metabolism. The central set of reactions involved in ATP production are collectively known as the citric acid cycle, or the Krebs Cycle. However, the mitochondrion has many other functions in addition to the production of ATP.
Energy conversion.
A dominant role for the mitochondria is the production of ATP, as reflected by the large number of proteins in the inner membrane for this task. This is done by oxidizing the major products of glucose: pyruvate, and NADH, which are produced in the cytosol. This process of cellular respiration, also known as aerobic respiration, is dependent on the presence of oxygen. When oxygen is limited, the glycolytic products will be metabolized by anaerobic fermentation, a process that is independent of the mitochondria. The production of ATP from glucose has an approximately 13-times higher yield during aerobic respiration compared to fermentation. Recently it has been shown that plant mitochondria can produce a limited amount of ATP without oxygen by using the alternate substrate nitrite.
Pyruvate and the citric acid cycle.
Each pyruvate molecule produced by glycolysis is actively transported across the inner mitochondrial membrane, and into the matrix where it is oxidized and combined with coenzyme A to form CO2, acetyl-CoA, and NADH.
The acetyl-CoA is the primary substrate to enter the "citric acid cycle", also known as the "tricarboxylic acid (TCA) cycle" or "Krebs cycle". The enzymes of the citric acid cycle are located in the mitochondrial matrix, with the exception of succinate dehydrogenase, which is bound to the inner mitochondrial membrane as part of Complex II. The citric acid cycle oxidizes the acetyl-CoA to carbon dioxide, and, in the process, produces reduced cofactors (three molecules of NADH and one molecule of FADH2) that are a source of electrons for the "electron transport chain", and a molecule of GTP (that is readily converted to an ATP).
NADH and FADH2: the electron transport chain.
The redox energy from NADH and FADH2 is transferred to oxygen (O2) in several steps via the electron transport chain. These energy-rich molecules are produced within the matrix via the citric acid cycle but are also produced in the cytoplasm by glycolysis. Reducing equivalents from the cytoplasm can be imported via the malate-aspartate shuttle system of antiporter proteins or feed into the electron transport chain using a glycerol phosphate shuttle. Protein complexes in the inner membrane (NADH dehydrogenase (ubiquinone), cytochrome c reductase, and cytochrome c oxidase) perform the transfer and the incremental release of energy is used to pump protons (H+) into the intermembrane space. This process is efficient, but a small percentage of electrons may prematurely reduce oxygen, forming reactive oxygen species such as superoxide. This can cause oxidative stress in the mitochondria and may contribute to the decline in mitochondrial function associated with the aging process.
As the proton concentration increases in the intermembrane space, a strong electrochemical gradient is established across the inner membrane. The protons can return to the matrix through the ATP synthase complex, and their potential energy is used to synthesize ATP from ADP and inorganic phosphate (Pi). This process is called chemiosmosis, and was first described by Peter Mitchell who was awarded the 1978 Nobel Prize in Chemistry for his work. Later, part of the 1997 Nobel Prize in Chemistry was awarded to Paul D. Boyer and John E. Walker for their clarification of the working mechanism of ATP synthase.
Heat production.
Under certain conditions, protons can re-enter the mitochondrial matrix without contributing to ATP synthesis. This process is known as "proton leak" or "mitochondrial uncoupling" and is due to the facilitated diffusion of protons into the matrix. The process results in the unharnessed potential energy of the proton electrochemical gradient being released as heat. The process is mediated by a proton channel called thermogenin, or UCP1. Thermogenin is a 33 kDa protein first discovered in 1973. Thermogenin is primarily found in brown adipose tissue, or brown fat, and is responsible for non-shivering thermogenesis. Brown adipose tissue is found in mammals, and is at its highest levels in early life and in hibernating animals. In humans, brown adipose tissue is present at birth and decreases with age.
Storage of calcium ions.
The concentrations of free calcium in the cell can regulate an array of reactions and is important for signal transduction in the cell. Mitochondria can transiently store calcium, a contributing process for the cell's homeostasis of calcium. In fact, their ability to rapidly take in calcium for later release makes them very good "cytosolic buffers" for calcium. The endoplasmic reticulum (ER) is the most significant storage site of calcium, and there is a significant interplay between the mitochondrion and ER with regard to calcium. The calcium is taken up into the matrix by a calcium uniporter on the inner mitochondrial membrane. It is primarily driven by the mitochondrial membrane potential. Release of this calcium back into the cell's interior can occur via a sodium-calcium exchange protein or via "calcium-induced-calcium-release" pathways. This can initiate calcium spikes or calcium waves with large changes in the membrane potential. These can activate a series of second messenger system proteins that can coordinate processes such as neurotransmitter release in nerve cells and release of hormones in endocrine cells.
Ca2+ influx to the mitochondrial matrix has recently been implicated as a mechanism to regulate respiratory bioenergetics by allowing the electrochemical potential across the membrane to transiently "pulse" from ΔΨ-dominated to pH-dominated, facilitating a reduction of oxidative stress. In neurons, concominant increases in cytosolic and mitochondrial calcium act to synchronize neuronal activity with mitochondrial energy metabolism. Mitochondrial matrix calcium levels can reach the tens of micromolar levels, which is necessary for the activation of isocitrate dehydrogenase, one of the key regulatory enzymes of the Kreb's cycle.
Additional functions.
Mitochondria play a central role in many other metabolic tasks, such as:
Some mitochondrial functions are performed only in specific types of cells. For example, mitochondria in liver cells contain enzymes that allow them to detoxify ammonia, a waste product of protein metabolism. A mutation in the genes regulating any of these functions can result in mitochondrial diseases.
Cellular proliferation regulation.
The relationship between cellular proliferation and mitochondria has been investigated using cervical cancer HeLa cells. Tumor cells require an ample amount of ATP (Adenosine triphosphate) in order to synthesize bioactive compounds such as lipids, proteins, and nucleotides for rapid cell proliferation. The majority of ATP in tumor cells is generated via the oxidative phosphorylation pathway (OxPhos). Interference with OxPhos have shown to cause cell cycle arrest suggesting that mitochondria play a role in cell proliferation. Mitochondrial ATP production is also vital for cell division in addition to other basic functions in the cell including the regulation of cell volume, solute concentration, and cellular architecture. ATP levels differ at various stages of the cell cycle suggesting that there is a relationship between the abundance of ATP and the cell's ability to enter a new cell cycle. ATP's role in the basic functions of the cell make the cell cycle sensitive to changes in the availability of mitochondrial derived ATP. The variation in ATP levels at different stages of the cell cycle support the hypothesis that mitochondria play an important role in cell cycle regulation. Although the specific mechanisms between mitochondria and the cell cycle regulation is not well understood, studies have shown that low energy cell cycle checkpoints monitor the energy capability before committing to another round of cell division.
Origin.
There are two hypotheses about the origin of mitochondria: endosymbiotic and autogenous. The endosymbiotic hypothesis suggests mitochondria were originally prokaryotic cells, capable of implementing oxidative mechanisms that were not possible for eukaryotic cells; they became endosymbionts living inside the eukaryote. In the autogenous hypothesis, mitochondria were born by splitting off a portion of DNA from the nucleus of the eukaryotic cell at the time of divergence with the prokaryotes; this DNA portion would have been enclosed by membranes, which could not be crossed by proteins. Since mitochondria have many features in common with bacteria, the most accredited theory at present is endosymbiosis.
A mitochondrion contains DNA, which is organized as several copies of a single, circular chromosome. This mitochondrial chromosome contains genes for redox proteins such as those of the respiratory chain. The CoRR hypothesis proposes that this co-location is required for redox regulation. The mitochondrial genome codes for some RNAs of ribosomes, and the twenty-two tRNAs necessary for the translation of messenger RNAs into protein. The circular structure is also found in prokaryotes. The proto-mitochondrion was probably closely related to the "Rickettsia". However, the exact relationship of the ancestor of mitochondria to the alphaproteobacteria and whether the mitochondrion was formed at the same time or after the nucleus, remains controversial.
A recent study by researchers of the University of Hawaii at Manoa and the Oregon State University indicates that the SAR11 clade of bacteria shares a relatively recent common ancestor with the mitochondria existing in most eukaryotic cells.
The ribosomes coded for by the mitochondrial DNA are similar to those from bacteria in size and structure. They closely resemble the bacterial 70S ribosome and not the 80S cytoplasmic ribosomes, which are coded for by nuclear DNA.
The endosymbiotic relationship of mitochondria with their host cells was popularized by Lynn Margulis. The endosymbiotic hypothesis suggests that mitochondria descended from bacteria that somehow survived endocytosis by another cell, and became incorporated into the cytoplasm. The ability of these bacteria to conduct respiration in host cells that had relied on glycolysis and fermentation would have provided a considerable evolutionary advantage. This symbiotic relationship probably developed 1.7 to 2 billion years ago.
A few groups of unicellular eukaryotes lack mitochondria: the microsporidians, metamonads, and archamoebae. These groups appear as the most primitive eukaryotes on phylogenetic trees constructed using rRNA information, which once suggested that they appeared before the origin of mitochondria. However, this is now known to be an artifact of long-branch attraction—they are derived groups and retain genes or organelles derived from mitochondria (e.g., mitosomes and hydrogenosomes).
Genome.
The human mitochondrial genome is a circular DNA molecule of about 16 kilobases. It encodes 37 genes: 13 for subunits of respiratory complexes I, III, IV and V, 22 for mitochondrial tRNA (for the 20 standard amino acids, plus an extra gene for leucine and serine), and 2 for rRNA. One mitochondrion can contain two to ten copies of its DNA.
As in prokaryotes, there is a very high proportion of coding DNA and an absence of repeats. Mitochondrial genes are transcribed as multigenic transcripts, which are cleaved and polyadenylated to yield mature mRNAs. Not all proteins necessary for mitochondrial function are encoded by the mitochondrial genome; most are coded by genes in the cell nucleus and the corresponding proteins are imported into the mitochondrion. The exact number of genes encoded by the nucleus and the mitochondrial genome differs between species. Most mitochondrial genomes are circular, although exceptions have been reported. In general, mitochondrial DNA lacks introns, as is the case in the human mitochondrial genome; however, introns have been observed in some eukaryotic mitochondrial DNA, such as that of yeast and protists, including "Dictyostelium discoideum". Between protein-coding regions, tRNAs are present. During transcription, the tRNAs acquire their characteristic L-shape that gets recognized and cleaved by specific enzymes. Mitochondrial tRNA genes have different sequences from the nuclear tRNAs but lookalikes of mitochondrial tRNAs have been found in the nuclear chromosomes with high sequence similarity.
In animals the mitochondrial genome is typically a single circular chromosome that is approximately 16 kb long and has 37 genes. The genes, while highly conserved, may vary in location. Curiously, this pattern is not found in the human body louse ("Pediculus humanus"). Instead this mitochondrial genome is arranged in 18 minicircular chromosomes, each of which is 3–4 kb long and has one to three genes. This pattern is also found in other sucking lice, but not in chewing lice. Recombination has been shown to occur between the minichromosomes. The reason for this difference is not known.
While slight variations on the standard code had been predicted earlier, none was discovered until 1979, when researchers studying human mitochondrial genes determined that they used an alternative code. Although, the mitochondria of many other eukaryotes, including most plants, use the standard code. Many slight variants have been discovered since, including various alternative mitochondrial codes. Further, the AUA, AUC, and AUU codons are all allowable start codons.
Some of these differences should be regarded as pseudo-changes in the genetic code due to the phenomenon of RNA editing, which is common in mitochondria. In higher plants, it was thought that CGG encoded for tryptophan and not arginine; however, the codon in the processed RNA was discovered to be the UGG codon, consistent with the standard genetic code for tryptophan. Of note, the arthropod mitochondrial genetic code has undergone parallel evolution within a phylum, with some organisms uniquely translating AGG to lysine.
Mitochondrial genomes have far fewer genes than the bacteria from which they are thought to be descended. Although some have been lost altogether, many have been transferred to the nucleus, such as the respiratory complex II protein subunits. This is thought to be relatively common over evolutionary time. A few organisms, such as the "Cryptosporidium", actually have mitochondria that lack any DNA, presumably because all their genes have been lost or transferred. In "Cryptosporidium", the mitochondria have an altered ATP generation system that renders the parasite resistant to many classical mitochondrial inhibitors such as cyanide, azide, and atovaquone.
Replication and inheritance.
Mitochondria divide by binary fission, similar to bacterial cell division. The regulation of this division differs between eukaryotes. In many single-celled eukaryotes, their growth and division is linked to the cell cycle. For example, a single mitochondrion may divide synchronously with the nucleus. This division and segregation process must be tightly controlled so that each daughter cell receives at least one mitochondrion. In other eukaryotes (in mammals for example), mitochondria may replicate their DNA and divide mainly in response to the energy needs of the cell, rather than in phase with the cell cycle. When the energy needs of a cell are high, mitochondria grow and divide. When the energy use is low, mitochondria are destroyed or become inactive. In such examples, and in contrast to the situation in many single celled eukaryotes, mitochondria are apparently randomly distributed to the daughter cells during the division of the cytoplasm. Understanding of mitochondrial dynamics, which is described as the balance between mitochondrial fusion and fission, has revealed that functional and structural alterations in mitochondrial morphology are important factors in pathologies associated with several disease conditions.
It is worthwhile to point out that the hypothesis of mitochondrial binary fission has relied on the visualization by fluorescence microscopy and conventional transmission electron microscopy (TEM). As is well known, the resolution of fluorescence microscopy(~200 nm) is insufficient to distinguish structural details such as double mitochondrial membrane in mitochondrial division. It is even not sufficient to distinguish individual mitochondrion when multiple mitochondria are close to each other. Conventional TEM has also some technical limitations in verifying mitochondrial division. A cutting edge technique termed Cryo-electron tomography was recently utilized to visualize mitochondrial division in frozen hydrated intact cells. It revealed that mitochondria divide by budding.
An individual's mitochondrial genes are not inherited by the same mechanism as nuclear genes. Typically, the mitochondria are inherited from one parent only. In humans, when an egg cell is fertilized by a sperm, the egg nucleus and sperm nucleus each contribute equally to the genetic makeup of the zygote nucleus. In contrast, the mitochondria, and therefore the mitochondrial DNA, usually come from the egg only. The sperm's mitochondria enter the egg but do not contribute genetic information to the embryo. Instead, paternal mitochondria are marked with ubiquitin to select them for later destruction inside the embryo. The egg cell contains relatively few mitochondria, but it is these mitochondria that survive and divide to populate the cells of the adult organism. Mitochondria are, therefore, in most cases inherited only from mothers, a pattern known as maternal inheritance. This mode is seen in most organisms including the majority of animals. However, mitochondria in some species can sometimes be inherited paternally. This is the norm among certain coniferous plants, although not in pine trees and yew trees. For Mytilidae mussels paternal inheritance only occurs within males of the species. It has been suggested that it occurs at a very low level in humans. There is a recent suggestion mitochondria that shorten male lifespan stay in the system because mitochondria are inherited only through the mother. By contrast natural selection weeds out mitochondria that reduce female survival as such mitochondria are less likely to be passed on to the next generation. Therefore it is suggested human females and female animals tend to live longer than males. The authors claim this is a partial explanation.
Uniparental inheritance leads to little opportunity for genetic recombination between different lineages of mitochondria, although a single mitochondrion can contain 2–10 copies of its DNA. For this reason, mitochondrial DNA usually is thought to reproduce by binary fission. What recombination does take place maintains genetic integrity rather than maintaining diversity. However, there are studies showing evidence of recombination in mitochondrial DNA. It is clear that the enzymes necessary for recombination are present in mammalian cells. Further, evidence suggests that animal mitochondria can undergo recombination. The data are a bit more controversial in humans, although indirect evidence of recombination exists. If recombination does not occur, the whole mitochondrial DNA sequence represents a single haplotype, which makes it useful for studying the evolutionary history of populations.
Population genetic studies.
The near-absence of genetic recombination in mitochondrial DNA makes it a useful source of information for scientists involved in population genetics and evolutionary biology. Because all the mitochondrial DNA is inherited as a single unit, or haplotype, the relationships between mitochondrial DNA from different individuals can be represented as a gene tree. Patterns in these gene trees can be used to infer the evolutionary history of populations. The classic example of this is in human evolutionary genetics, where the molecular clock can be used to provide a recent date for mitochondrial Eve. This is often interpreted as strong support for a recent modern human expansion out of Africa. Another human example is the sequencing of mitochondrial DNA from Neanderthal bones. The relatively large evolutionary distance between the mitochondrial DNA sequences of Neanderthals and living humans has been interpreted as evidence for lack of interbreeding between Neanderthals and anatomically modern humans.
However, mitochondrial DNA reflects the history of only females in a population and so may not represent the history of the population as a whole. This can be partially overcome by the use of paternal genetic sequences, such as the non-recombining region of the Y-chromosome. In a broader sense, only studies that also include nuclear DNA can provide a comprehensive evolutionary history of a population.
Recent measurements of the molecular clock for mitochondrial DNA reported a value of 1 mutation every 7884 years dating back to the most recent common ancestor of humans and apes, which is consistent with estimates of mutation rates of autosomal DNA (10−8 per base per generation).
Dysfunction and disease.
Mitochondrial diseases.
Damage and subsequent dysfunction in mitochondria is an important factor in a range of human diseases due to their influence in cell metabolism. Mitochondrial disorders often present themselves as neurological disorders, including autism. They can also manifest as myopathy, diabetes, multiple endocrinopathy, and a variety of other systemic disorders. Diseases caused by mutation in the mtDNA include Kearns-Sayre syndrome, MELAS syndrome and Leber's hereditary optic neuropathy. In the vast majority of cases, these diseases are transmitted by a female to her children, as the zygote derives its mitochondria and hence its mtDNA from the ovum. Diseases such as Kearns-Sayre syndrome, Pearson's syndrome, and progressive external ophthalmoplegia are thought to be due to large-scale mtDNA rearrangements, whereas other diseases such as MELAS syndrome, Leber's hereditary optic neuropathy, myoclonic epilepsy with ragged red fibers (MERRF), and others are due to point mutations in mtDNA.
In other diseases, defects in nuclear genes lead to dysfunction of mitochondrial proteins. This is the case in Friedreich's ataxia, hereditary spastic paraplegia, and Wilson's disease. These diseases are inherited in a dominance relationship, as applies to most other genetic diseases. A variety of disorders can be caused by nuclear mutations of oxidative phosphorylation enzymes, such as coenzyme Q10 deficiency and Barth syndrome. Environmental influences may interact with hereditary predispositions and cause mitochondrial disease. For example, there may be a link between pesticide exposure and the later onset of Parkinson's disease. Other pathologies with etiology involving mitochondrial dysfunction include schizophrenia, bipolar disorder, dementia, Alzheimer's disease, Parkinson's disease, epilepsy, stroke, cardiovascular disease, retinitis pigmentosa, and diabetes mellitus.
Mitochondria-mediated oxidative stress plays a role in cardiomyopathy in Type 2 diabetics. Increased fatty acid delivery to the heart increases fatty acid uptake by cardiomyocytes, resulting in increased fatty acid oxidation in these cells. This process increases the reducing equivalents available to the electron transport chain of the mitochondria, ultimately increasing reactive oxygen species (ROS) production. ROS increases uncoupling proteins (UCPs) and potentiate proton leakage through the adenine nucleotide translocator (ANT), the combination of which uncouples the mitochondria. Uncoupling then increases oxygen consumption by the mitochondria, compounding the increase in fatty acid oxiation. This creates a vicious cycle of uncoupling; furthermore, even though oxygen consumption increases, ATP synthesis does not increase proportionally because the mitochondria is uncoupled. Less ATP availability ultimately results in an energy deficit presenting as reduced cardiac efficiency and contractile dysfunction. To compound the problem, impaired sarcoplasmic reticulum calcium release and reduced mitochondrial reuptake limits peak cytosolic levels of the important signaling ion during muscle contraction. The decreased intra-mitochondrial calcium concentration increases dehydrogenase activation and ATP synthesis. So in addition to lower ATP synthesis due to fatty acid oxidation, ATP synthesis is impaired by poor calcium signaling as well, causing cardiac problems for diabetics.
Possible relationships to aging.
Given the role of mitochondria as the cell's powerhouse, there may be some leakage of the high-energy electrons in the respiratory chain to form reactive oxygen species. This was thought to result in significant oxidative stress in the mitochondria with high mutation rates of mitochondrial DNA (mtDNA). Hypothesized links between aging and oxidative stress are not new and were proposed in 1956, which was later refined into the mitochondrial free radical theory of aging. A vicious cycle was thought to occur, as oxidative stress leads to mitochondrial DNA mutations, which can lead to enzymatic abnormalities and further oxidative stress.
A number of changes can occur to mitochondria during the aging process. Tissues from elderly patients show a decrease in enzymatic activity of the proteins of the respiratory chain. However, mutated mtDNA can only be found in about 0.2% of very old cells. Large deletions in the mitochondrial genome have been hypothesized to lead to high levels of oxidative stress and neuronal death in Parkinson's disease.
References.
General

</doc>
<doc id="19589" url="http://en.wikipedia.org/wiki?curid=19589" title="Minimax">
Minimax

Minimax (sometimes MinMax or MM) is a decision rule used in decision theory, game theory, statistics and philosophy for "mini"mizing the possible loss for a worst case ("max"imum loss) scenario. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision making in the presence of uncertainty.
Game theory.
In the theory of simultaneous games, a minimax strategy is a mixed strategy that is part of the solution to a zero-sum game. In zero-sum games, the minimax solution is the same as the Nash equilibrium.
Minimax theorem.
The minimax theorem states:
For every two-person, zero-sum game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that
Equivalently, Player 1's strategy guarantees him a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee himself a payoff of −V. The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, he also minimizes his own maximum loss (i.e. maximize his minimum payoff).
This theorem was first published in 1928 by John von Neumann, who is quoted as saying "As far as I can see, there could be no theory of games … without that theorem … I thought there was nothing worth publishing until the "Minimax Theorem" was proved".
See Sion's minimax theorem and Parthasarathy's theorem for generalizations; see also example of a game without a value.
Example.
The following example of a zero-sum game, where A and B make simultaneous moves, illustrates "minimax" solutions. Suppose each player has three choices and consider the payoff matrix for A displayed at right. Assume the payoff matrix for B is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then B pays 3 to A). Then, the minimax choice for A is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for B is B2 since the worst possible result is then no payment. However, this solution is not stable, since if B believes A will choose A2 then B will choose B1 to gain 1; then if A believes B will choose B1 then A will choose A1 to gain 3; and then B will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.
Some choices are "dominated" by others and can be eliminated: A will not choose A3 since either A1 or A2 will produce a better result, no matter what B chooses; B will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what A chooses.
A can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for A would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case B chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1∕3 in case B chose B2. Similarly, B can ensure an expected gain of at least 1/3, no matter what A chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These mixed minimax strategies are now stable and cannot be improved.
Maximin.
Frequently, in game theory, maximin is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a zero-sum game, this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.
"Maximin" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy.
Combinatorial game theory.
In combinatorial game theory, there is a minimax algorithm for game solutions.
A simple version of the minimax "algorithm", stated below, deals with games such as tic-tac-toe, where each player can win, lose, or draw.
If player A "can" win in one move, his best move is that winning move.
If player B knows that one move will lead to the situation where player A "can" win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.
Late in the game, it's easy to see what the "best" move is.
The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).
Minimax algorithm with alternate moves.
A minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is A's turn to move, A gives a value to each of his legal moves.
A possible allocation method consists in assigning a certain win for A as +1 and for B as −1. This leads to combinatorial game theory as developed by John Horton Conway. An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and, if it is an immediate win for B, negative infinity. The value to A of any other move is the minimum of the values resulting from each of B's possible replies. For this reason, A is called the "maximizing player" and B is called the "minimizing player", hence the name "minimax algorithm". The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position. Often this is generally only possible at the very end of complicated games such as chess or go, since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.
This can be extended if we can supply a heuristic evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the "look-ahead", measured in "plies". For example, the chess computer Deep Blue (that beat Garry Kasparov) looked ahead at least 12 plies, then applied a heuristic evaluation function.
The algorithm can be thought of as exploring the nodes of a "game tree". The "effective branching factor" of the tree is the average number of children of each node (i.e., the average number of legal moves in a position). The number of nodes to be explored usually increases exponentially with the number of plies (it is less than exponential if evaluating forced moves or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore impractical to completely analyze games such as chess using the minimax algorithm.
The performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of alpha-beta pruning.
Other heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.
A naïve minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.
Pseudocode.
The pseudocode for the depth limited minimax algorithm is given below.
 function minimax(node, depth, maximizingPlayer)
 if depth = 0 or node is a terminal node
 return the heuristic value of node
 if maximizingPlayer
 bestValue := -∞
 for each child of node
 val := minimax(child, depth - 1, FALSE)
 bestValue := max(bestValue, val)
 return bestValue
 else
 bestValue := +∞
 for each child of node
 val := minimax(child, depth - 1, TRUE)
 bestValue := min(bestValue, val)
 return bestValue
 "(* Initial call for maximizing player *)"
 minimax(origin, depth, TRUE)
The minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth).
Non leaf nodes inherit their value, "bestValue", from a descendant leaf node.
The heuristic value is a score measuring the favor-ability of the node for the maximizing player.
Hence nodes resulting in a favorable outcome (such as a win) for the maximizing player have higher scores than nodes more favorable for the minimizing player.
For non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.
The quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.
Minimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that formula_1, minimax may often be simplified into the negamax algorithm.
Example.
Suppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the tree on the right, where the circles represent the moves of the player running the algorithm ("maximizing player"), and squares represent the moves of the opponent ("minimizing player"). Because of the limitation of computation resources, as explained above, the tree is limited to a "look-ahead" of 4 moves.
The algorithm evaluates each "leaf node" using a heuristic evaluation function, obtaining the values shown. The moves where the "maximizing player" wins are assigned with positive infinity, while the moves that lead to a win of the "minimizing player" are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the smallest of the "child node" values, and assign it to that same node (e.g. the node on the left will choose the minimum between "10" and "+∞", therefore assigning the value "10" to itself). The next step, in level 2, consists of choosing for each node the largest of the "child node" values. Once again, the values are assigned to each "parent node". The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the "root node", where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to "minimize" the "maximum" possible loss.
Minimax for individual decisions.
Minimax in the face of uncertainty.
Minimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts. For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are. One approach is to treat this as a game against "nature" (see move by nature), and using a similar mindset as Murphy's law or resistentialism, take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.
In addition, expectiminimax trees have been developed, for two-player games in which chance (for example, dice) is a factor.
Minimax criterion in statistical decision theory.
In classical statistical decision theory, we have an estimator formula_2 that is used to estimate a parameter formula_3. We also assume a risk function formula_4, usually specified as the integral of a loss function. In this framework, formula_5 is called minimax if it satisfies
An alternative criterion in the decision theoretic framework is the Bayes estimator in the presence of a prior distribution formula_7. An estimator is Bayes if it minimizes the "average" risk
Non-probabilistic decision theory.
A key feature of minimax decision making is being non-probabilistic: in contrast to decisions using expected value or expected utility, it makes no assumptions about the probabilities of various outcomes, just scenario analysis of what the possible outcomes are. It is thus robust to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably minimax regret and Info-gap decision theory.
Further, minimax only requires ordinal measurement (that outcomes be compared and ranked), not "interval" measurements (that outcomes include "how much better or worse"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: "this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy". Compare to expected value analysis, whose conclusion is of the form: "this strategy yields E("X")="n."" Minimax thus can be used on ordinal data, and can be more transparent.
Maximin in philosophy.
In philosophy, the term "maximin" is often used in the context of John Rawls's "A Theory of Justice," where he refers to it (Rawls (1971, p. 152)) in the context of The Difference Principle.
Rawls defined this principle as the rule which states that social and economic inequalities should be arranged so that "they are to be of the greatest benefit to the least-advantaged members of society".

</doc>
<doc id="19590" url="http://en.wikipedia.org/wiki?curid=19590" title="Minnesota">
Minnesota

Minnesota (; locally  ) is a state in the Midwestern United States. Minnesota was admitted as the 32nd state on May 11, 1858, created from the eastern half of the Minnesota Territory and part of the Wisconsin Territory. Its name comes from the Dakota word for "clear blue water." Owing to its large number of lakes, the state is informally known as the "Land of 10,000 Lakes." Its official motto is "L'Étoile du Nord ("French:" Star of the North)."
Minnesota is the 12th largest in area and the 21st most populous of the U.S. States. Nearly 60 percent of its residents live in the Minneapolis–Saint Paul metropolitan area (known as the "Twin Cities"), the center of transportation, business, industry, education, and government and home to an internationally known arts community. The remainder of the state consists of western prairies now given over to intensive agriculture; deciduous forests in the southeast, now partially cleared, farmed and settled; and the less populated North Woods, used for mining, forestry, and recreation.
Minnesota is known for its relatively mixed social and political orientations and its high rate of civic participation and voter turnout. Until European settlement, Minnesota was inhabited by the Dakota and Ojibwe/Anishinaabe. The large majority of the original European settlers immigrated from Scandinavia and Germany, and the state remains a center of Scandinavian American and German American culture. In recent decades, immigration from Asia, the Horn of Africa, and Latin America has expanded its demographic and cultural composition. Minnesota's standard of living index is among the highest in the United States, and the state is also among the best-educated and wealthiest in the nation.
Etymology.
The word "Minnesota" comes from the Dakota name for the Minnesota River: "Mnisota". The root "mni" (also spelled "mini" or "minne") means "water" and "tō" ("ta") means "blue". "Mnisota" can be translated as "clear blue water" or "clouded blue water" depending on pronunciation. Native Americans demonstrated the name to early settlers by dropping milk into water and calling it "mnisota". Many places in the state have similar names, such as Minnehaha Falls ("laughing water" (waterfall)), Minneiska ("white water"), Minneota ("much water"), Minnetonka ("big water"), Minnetrista ("crooked water"), and Minneapolis, a combination of "mni" and "polis", the Greek word for "city."
Geography.
Minnesota is the second northernmost U.S. state (after Alaska). Its isolated Northwest Angle in Lake of the Woods county is the only part of the 48 contiguous states lying north of the 49th parallel. The state is part of the U.S. region known as the Upper Midwest and part of North America's Great Lakes Region. It shares a Lake Superior water border with Michigan and a land and water border with Wisconsin to the east. Iowa is to the south, North Dakota and South Dakota are to the west, and the Canadian provinces of Ontario and Manitoba are to the north. With 86,943 sqmi, or approximately 2.25 percent of the United States, Minnesota is the 12th-largest state.
Geology.
Minnesota contains some of the oldest rocks found on earth. Gneisses are about 3.6 billion years old (80 percent as old as the planet). About 2.7 billion years ago, basaltic lava poured out of cracks in the floor of the primordial ocean; the remains of this volcanic rock formed the Canadian Shield in northeast Minnesota. The roots of these volcanic mountains and the action of Precambrian seas formed the Iron Range of northern Minnesota. Following a period of volcanism 1.1 billion years ago, Minnesota's geological activity has been more subdued, with no volcanism or mountain formation, but with repeated incursions of the sea, which left behind multiple strata of sedimentary rock.
In more recent times, massive ice sheets at least one kilometer thick ravaged the landscape of the state and sculpted its current terrain. The Wisconsin glaciation left 12,000 years ago. These glaciers covered all of Minnesota except the far southeast, an area characterized by steep hills and streams that cut into the bedrock. This area is known as the Driftless Zone for its absence of glacial drift. Much of the remainder of the state outside the northeast has 50 feet (15 m) or more of glacial till left behind as the last glaciers retreated. Gigantic Lake Agassiz formed in the northwest 13,000 years ago. Its bed created the fertile Red River valley, and its outflow, glacial River Warren, carved the valley of the Minnesota River and the Upper Mississippi downstream from Fort Snelling. Minnesota is geologically quiet today; it experiences earthquakes infrequently, and most of them are minor.
The state's high point is Eagle Mountain at 2,301 feet (701 m), which is only 13 mi away from the low of 601 feet (183 m) at the shore of Lake Superior. Notwithstanding dramatic local differences in elevation, much of the state is a gently rolling peneplain.
Two major drainage divides meet in the northeastern part of Minnesota in rural Hibbing, forming a triple watershed. Precipitation can follow the Mississippi River south to the Gulf of Mexico, the Saint Lawrence Seaway east to the Atlantic Ocean, or the Hudson Bay watershed to the Arctic Ocean.
The state's nickname, "The Land of 10,000 Lakes", is no exaggeration; there are 11,842 Minnesota lakes over 10 acre in size. The Minnesota portion of Lake Superior is the largest at 962,700 acre and deepest (at 1290 ft) body of water in the state. Minnesota has 6,564 natural rivers and streams that cumulatively flow for 69,000 mi. The Mississippi River begins its journey from its headwaters at Lake Itasca and crosses the Iowa border 680 mi downstream. It is joined by the Minnesota River at Fort Snelling, by the St. Croix River near Hastings, by the Chippewa River at Wabasha, and by many smaller streams. The Red River, in the bed of glacial Lake Agassiz, drains the northwest part of the state northward toward Canada's Hudson Bay. Approximately 10.6 million acres (42,900 km²) of wetlands are contained within Minnesota's borders, the most of any state except Alaska.
Flora and fauna.
Minnesota has four ecological provinces: "Prairie Parkland", in the southwestern and western parts of the state; the "Eastern Broadleaf Forest" (Big Woods) in the southeast, extending in a narrowing strip to the northwestern part of the state, where it transitions into "Tallgrass Aspen Parkland"; and the northern "Laurentian Mixed Forest", a transitional forest between the northern boreal forest and broadleaf forests to the south. These northern forests are a vast wilderness of pine and spruce trees mixed with patchy stands of birch and poplar.
Much of Minnesota's northern forest underwent logging at some time, leaving only a few patches of old growth forest today in areas such as in the Chippewa National Forest and the Superior National Forest where the Boundary Waters Canoe Area Wilderness has some 400000 acres of unlogged land. Although logging continues, regrowth and replanting keeps about one third of the state forested. Nearly all of Minnesota's prairies and oak savannas have been fragmented by farming, grazing, logging, and suburban development.
While loss of habitat has affected native animals such as the pine marten, elk, woodland caribou, and bison, others like whitetail deer and bobcat thrive. The state has the nation's largest population of timber wolves outside Alaska, and supports healthy populations of black bear, moose, and gophers. Located on the Mississippi Flyway, Minnesota hosts migratory waterfowl such as geese and ducks, and game birds such as grouse, pheasants, and turkeys. It is home to birds of prey including the largest number of breeding pairs of bald eagles in the lower 48 states as of 2007, red-tailed hawk, and snowy owl. The lakes teem with sport fish such as walleye, bass, muskellunge, and northern pike, and streams in the southeast and northeast are populated by brook, brown, and rainbow trout.
Climate.
Minnesota experiences temperature extremes characteristic of its continental climate, with cold winters and hot summers. The record high and low span is 174 degrees Fahrenheit (96 °C), from -60 F at Tower on February 2, 1996, to 114 F at Moorhead on July 6, 1936. Meteorological events include rain, snow, blizzards, thunderstorms, hail, derechos, tornadoes, and high-velocity straight-line winds. The growing season varies from 90 days per year in the Iron Range to 160 days in southeast Minnesota near the Mississippi River, and average temperatures range from 37 to. Average summer dew points range from about 58 F in the south to about 48 F in the north. Average annual precipitation ranges from 19 to, and droughts occur every 10 to 50 years.
Protected lands.
Minnesota's first state park, Itasca State Park, was established in 1891, and is the source of the Mississippi River. Today Minnesota has 72 state parks and recreation areas, 58 state forests covering about four million acres (16,000 km²), and numerous state wildlife preserves, all managed by the Minnesota Department of Natural Resources. There are 5.5 million acres (22,000 km²) in the Chippewa and Superior National Forests. The Superior National Forest in the northeast contains the Boundary Waters Canoe Area Wilderness, which encompasses over a million acres (4,000 km²) and a thousand lakes. To its west is Voyageurs National Park. The Mississippi National River and Recreation Area (MNRRA), is a 72-mile-long (116 km) corridor along the Mississippi River through the Minneapolis–St. Paul Metropolitan Area connecting a variety of sites of historic, cultural, and geologic interest.
History.
Before European settlement of North America, Minnesota was populated by the Dakota people. As Europeans settled the east coast, Native American movement away from them caused migration of the Anishinaabe and other Native Americans into the Minnesota area. The first Europeans in the area were French fur traders who arrived in the 17th century. Late that century, Anishinaabe, also known as Ojibwe Indians migrated westward to Minnesota, causing tensions with the Dakota people. Explorers such as Daniel Greysolon, Sieur du Lhut, Father Louis Hennepin, Jonathan Carver, Henry Schoolcraft, and Joseph Nicollet, among others, mapped out the state.
In 1762 the region became part of Spanish Louisiana until 1802. The portion of the state east of the Mississippi River became part of the United States at the end of the American Revolutionary War, when the Second Treaty of Paris was signed. Land west of the Mississippi River was acquired with the Louisiana Purchase, although a portion of the Red River Valley was disputed until the Treaty of 1818. In 1805, Zebulon Pike bargained with Native Americans to acquire land at the confluence of the Minnesota and Mississippi rivers. The construction of Fort Snelling followed between 1819 and 1825. Its soldiers built a grist mill and a sawmill at Saint Anthony Falls, the first of the water-powered industries around which the city of Minneapolis later grew. Meanwhile, squatters, government officials, and tourists had settled near the fort. In 1839, the Army forced them to move downriver and they settled in the area that became St. Paul. Minnesota Territory was formed on March 3, 1849. The first territorial legislature (held September 2, 1849) was dominated by men from New England or of New England ancestry. Thousands of people had come to build farms and cut timber, and Minnesota became the 32nd U.S. state on May 11, 1858. The founding population was so overwhelmingly of New England origins that the state was dubbed "The New England of the West".
Treaties between European settlers and the Dakota and Ojibwe gradually forced the natives off their lands and on to smaller reservations. As conditions deteriorated for the Dakota, tensions rose, leading to the Dakota War of 1862. The result of the six-week war was the execution of 38 Dakota — the largest mass execution in United States history — and the exile of most of the rest of the Dakota to the Crow Creek Reservation in Dakota Territory. As many as 800 white settlers died during the war.
Logging and farming were mainstays of Minnesota's early economy. The sawmills at Saint Anthony Falls, and logging centers like Marine on St. Croix, Stillwater, and Winona, processed high volumes of lumber. These cities were situated on rivers that were ideal for transportation. Later, Saint Anthony Falls was tapped to provide power for flour mills. Innovations by Minneapolis millers led to the production of Minnesota "patent" flour, which commanded almost double the price of "bakers" or "clear" flour, which it replaced. By 1900, Minnesota mills, led by Pillsbury, Northwestern and the Washburn-Crosby Company (a forerunner of General Mills), were grinding 14.1 percent of the nation's grain.
The state's iron-mining industry was established with the discovery of iron in the Vermilion Range and the Mesabi Range in the 1880s, and in the Cuyuna Range in the early 20th century. The ore was shipped by rail to Duluth and Two Harbors, then loaded onto ships and transported eastward over the Great Lakes.
Industrial development and the rise of manufacturing caused the population to shift gradually from rural areas to cities during the early 20th century. Nevertheless, farming remained prevalent. Minnesota's economy was hard-hit by the Great Depression, resulting in lower prices for farmers, layoffs among iron miners, and labor unrest. Compounding the adversity, western Minnesota and the Dakotas were hit by drought from 1931 to 1935. New Deal programs provided some economic turnaround. The Civilian Conservation Corps and other programs around the state established some jobs for Indians on their reservations, and the Indian Reorganization Act of 1934 provided the tribes with a mechanism of self-government. This provided natives a greater voice within the state, and promoted more respect for tribal customs because religious ceremonies and native languages were no longer suppressed.
After World War II, industrial development quickened. New technology increased farm productivity through automation of feedlots for hogs and cattle, machine milking at dairy farms, and raising chickens in large buildings. Planting became more specialized with hybridization of corn and wheat, and the use of farm machinery such as tractors and combines became the norm. University of Minnesota professor Norman Borlaug contributed to these developments as part of the Green Revolution. Suburban development accelerated due to increased postwar housing demand and convenient transportation. Increased mobility, in turn, enabled more specialized jobs.
Minnesota became a center of technology after World War II. Engineering Research Associates was formed in 1946 to develop computers for the United States Navy. It later merged with Remington Rand, and then became Sperry Rand. William Norris left Sperry in 1957 to form Control Data Corporation (CDC). Cray Research was formed when Seymour Cray left CDC to form his own company. Medical device maker Medtronic also started business in the Twin Cities in 1949.
Cities and towns.
Saint Paul, located in east-central Minnesota along the banks of the Mississippi River, has been Minnesota's capital city since 1849, first as capital of the Territory of Minnesota, and then as state capital since 1858.
Saint Paul is adjacent to Minnesota's most populous city, Minneapolis; they and their suburbs are known collectively as the Twin Cities metropolitan area, the 13th-largest metropolitan area in the United States and home to about 60 percent of the state's population. The remainder of the state is known as "Greater Minnesota" or "Outstate Minnesota".
The state has 17 cities with populations above 50,000 (as of the 2010 census). In descending order of population, they are Minneapolis, Saint Paul, Rochester, Duluth, Bloomington, Brooklyn Park, Plymouth, Saint Cloud, Woodbury, Eagan, Maple Grove, Coon Rapids, Eden Prairie, Minnetonka, Burnsville, Apple Valley, Blaine and Lakeville. Of these only Rochester, Duluth, and Saint Cloud are outside the Twin Cities metropolitan area.
Minnesota's population continues to grow, primarily in the urban centers. The populations of metropolitan Sherburne and Scott counties doubled between 1980 and 2000, while 40 of the state's 87 counties lost residents over the same period.
Demographics.
Population.
From fewer than 6,120 people in 1850, Minnesota's population grew to over 1.7 million by 1900. Each of the next six decades saw a 15 percent increase in population, reaching 3.4 million in 1960. Growth then slowed, rising 11 percent to 3.8 million in 1970, and an average of 9 percent over the next three decades to 4.9 million in the 2000 Census. The United States Census Bureau estimates that the population of Minnesota was 5,379,139 on July 1, 2012, a 1.4 percent increase since the 2010 United States Census. The rate of population change, and age and gender distributions, approximate the national average. Minnesota's center of population is in Hennepin County.
Race and ancestry.
The state's estimated racial composition in the 2011 American Census Bureau estimate was:
Hispanics or Latinos (of any race) made up 4.7 percent of the population.
In 2011, non-Hispanic whites were involved in 72.3 percent of all the births. Minnesota's growing minority groups, however, still form a smaller percentage of the population than in the nation as a whole.
The principal ancestries of Minnesota's residents in 2010 were surveyed to be the following:
Ancestries claimed by less than 3 percent of the population include American, Italian, and Dutch, each between 2 and 3 percent; Sub-Saharan African and East African, Scottish, French Canadian, Scotch-Irish and Mexican, each between 1 and 1.9 percent; and less than 1 percent each for Russian, Welsh, Bosnian, Croatian, Serbian, Swiss, Arab, Hungarian, Ukrainian, Greek, Slovak, Lithuanian, Portuguese, and West Indian.
Religion.
The majority of Minnesotans are Protestants, including a significant Lutheran contingent, owing to the state's largely Northern European ethnic makeup, but Roman Catholics (of largely German, Irish, and Slavic descent) make up the largest single Christian denomination. A 2010 survey by the Pew Forum on Religion and Public Life showed that 32 percent of Minnesotans were affiliated with Mainline Protestant traditions, 21 percent were Evangelical Protestants, 28 percent were Roman Catholic, 1 percent each were Jewish, Muslim, Buddhist, and Black Protestant, and smaller amounts were of other faiths, with 13 percent unaffiliated. According to the Association of Religion Data Archives, the denominations with the most adherents in 2010 were the Roman Catholic Church with 1,150,367; the Evangelical Lutheran Church in America with 737,537; and the Lutheran Church Missouri Synod with 182,439. This is broadly consistent with the results of the 2001 American Religious Identification Survey, which also gives detailed percentages for many individual denominations. Although Christianity is dominant, Minnesota has a long history with non-Christian faiths. Ashkenazi Jewish pioneers set up Saint Paul's first synagogue in 1856. Minnesota is home to over 30 mosques, mostly in the Twin Cities metro area.
Economy.
Once primarily a producer of raw materials, Minnesota's economy has transformed to emphasize finished products and services. Perhaps the most significant characteristic of the economy is its diversity; the relative outputs of its business sectors closely match the United States as a whole. The economy of Minnesota had a gross domestic product of $262 billion in 2008. In 2008, thirty-three of the United States' top 1,000 publicly traded companies (by revenue) were headquartered in Minnesota, including Target, UnitedHealth Group, 3M, General Mills, U.S. Bancorp, Ameriprise, Hormel, Land O' Lakes, SuperValu, Best Buy and Valspar. Private companies based in Minnesota include Cargill, the largest privately owned company in the United States, and Carlson Companies, the parent company of Radisson Hotels.
The per capita personal income in 2008 was $42,772, the tenth-highest in the nation. The three-year median household income from 2002 to 2004 was $55,914, ranking fifth in the U.S. and first among the 36 states not on the Atlantic coast.
As of January 2015, the state's unemployment rate was 3.7 percent.
Industry and commerce.
Minnesota's earliest industries were fur trading and agriculture; the city of Minneapolis grew around the flour mills powered by St. Anthony Falls. Although less than one percent of the population is employed in the agricultural sector, it remains a major part of the state's economy, ranking sixth in the nation in the value of products sold. The state is the U.S.'s largest producer of sugar beets, sweet corn, and green peas for processing, and farm-raised turkeys. Minnesota is also a large producer of corn and soybeans. Minnesota has the most food cooperatives per capita in America. Forestry remains strong, including logging, pulpwood processing and paper production, and forest products manufacturing. Minnesota was famous for its soft-ore mines, which produced a significant portion of the world's iron ore for over a century. Although the high-grade ore is now depleted, taconite mining continues, using processes developed locally to save the industry. In 2004, the state produced 75 percent of the country's usable iron ore. The mining boom created the port of Duluth which continues to be important for shipping ore, coal, and agricultural products. The manufacturing sector now includes technology and biomedical firms in addition to the older food processors and heavy industry. The nation's first indoor shopping mall was Edina's Southdale Center and its largest is Bloomington's Mall of America.
Minnesota is one of 42 U.S. states with its own lottery; its games include Powerball, Mega Millions, Hot Lotto (all three multi-state), Northstar Cash and Gopher 5.
Energy use and production.
The state produces ethanol fuel and is the first to mandate its use, a ten percent mix (E10). In 2005 there were more than 310 service stations supplying E85 fuel, comprising 85 percent ethanol and 15 percent gasoline. A two percent biodiesel blend has been required in diesel fuel since 2005. As of December 2006 the state was the country's fourth-largest producer of wind power, with 895 megawatts installed and another 200 megawatts planned, much of it on the windy Buffalo Ridge in the southwest part of the state.
State taxes.
Minnesota has a progressive income tax structure; the four brackets of state income tax rates are 5.35, 7.05, 7.85 and 9.85 percent. As of 2008, Minnesota was ranked 12th in the nation in per capita total state and local taxes. In 2008, Minnesotans paid 10.2 percent of their income in state and local taxes; the U.S. average was 9.7 percent. The state sales tax in Minnesota is 6.875 percent, but there is no sales tax on clothing, prescription drug medications, some services, or food items for home consumption. The state legislature may allow municipalities to institute local sales taxes and special local taxes, such as the 0.5 percent supplemental sales tax in Minneapolis. Excise taxes are levied on alcohol, tobacco, and motor fuel. The state imposes a use tax on items purchased elsewhere but used within Minnesota. Owners of real property in Minnesota pay property tax to their county, municipality, school district, and special taxing districts.
Culture.
Fine and performing arts.
 Minnesota's leading fine art museums include the Minneapolis Institute of Arts, the Walker Art Center, the Frederick R. Weisman Art Museum, and the The Museum of Russian Art (TMORA). All are located in Minneapolis. The Minnesota Orchestra and the Saint Paul Chamber Orchestra are prominent full-time professional musical ensembles that perform concerts and offer educational programs to the Twin Cities' community. The world-renowned Guthrie Theater moved into a new Minneapolis facility in 2006, boasting three stages and overlooking the Mississippi River. Attendance at theatrical, musical, and comedy events in the area is strong. In the United States, the Twin Cities' number of theater seats per capita ranks behind only New York City; with some 2.3 million theater tickets sold annually. The Minnesota Fringe Festival is an annual celebration of theatre, dance, improvisation, puppetry, kids' shows, visual art, and musicals. The summer festival consists of over 800 performances over 11 days in Minneapolis, and is the largest non-juried performing arts festival in the United States.
Literature.
The rigors and rewards of pioneer life on the prairie are the subject of "Giants in the Earth" by Ole Rolvaag and the "Little House" series of children's books by Laura Ingalls Wilder. Small-town life is portrayed grimly by Sinclair Lewis in the novel "Main Street", and more gently and affectionately by Garrison Keillor in his tales of Lake Wobegon. St. Paul native F. Scott Fitzgerald writes of the social insecurities and aspirations of the young city in stories such as "Winter Dreams" and "The Ice Palace" (published in "Flappers and Philosophers"). Henry Wadsworth Longfellow's epic poem "The Song of Hiawatha" was inspired by Minnesota and names many of the state's places and bodies of water.
Entertainment.
Minnesota musicians include Bob Dylan, Eddie Cochran, The Andrews Sisters, The Castaways, The Trashmen, Prince, Soul Asylum, David Ellefson, Rhymesayers Entertainment, Doomtree, Hüsker Dü, and The Replacements. Minnesotans helped shape the history of music through popular American culture: the Andrews Sisters' "Boogie Woogie Bugle Boy" was an iconic tune of World War II, while the Trashmen's "Surfin' Bird" and Bob Dylan epitomize two sides of the 1960s. In the 1980s, influential hit radio groups and musicians included Prince, The Original 7ven, Jimmy Jam & Terry Lewis, The Jets (Minnesota band), Lipps Inc., and Information Society (band).
Minnesotans have also made significant contributions to comedy, theater, media, and film. The comic strip "Peanuts" was created by St. Paul native Charles M. Schulz. Garrison Keillor resurrected old-style radio comedy with "A Prairie Home Companion", which has aired since 1974. The cult shows "Mystery Science Theater 3000" and "Let's Bowl" originated in the Twin Cities and Lizz Winstead and Craig Kilborn helped create the increasingly influential Comedy Central program "The Daily Show". 
Joel and Ethan Coen, Terry Gilliam, Mike Todd, and Bill Pohlad contributed to the art of filmmaking as writers, directors, and producers. Actors from the state include Judy Garland, Jane Russell, Tippi Hedren, Jessica Lange, E.G. Marshall, James Arness, Lea Thompson, Winona Ryder, Julia Duffy, Kelly Lynch, Loni Anderson, Mike Farrell, Richard Dean Anderson, Seann William Scott, Josh Hartnett, Jessica Biel, Vince Vaughn, Rachael Leigh Cook, Steve Zahn, Jesse Ventura, Garrett Hedlund, Chris Pratt, and Kevin Sorbo.
Popular culture.
Stereotypical traits of Minnesotans include "Minnesota nice", Lutheranism, a strong sense of community and shared culture, and a distinctive brand of North Central American English sprinkled with Scandinavian expressions. Potlucks, usually with a variety of hotdishes, are popular small-town church activities. A small segment of the Scandinavian population attend a traditional lutefisk dinner to celebrate Christmas. Many of these Scandinavian cultural characteristics and personality traits are satirized on the National Public Radio program "A Prairie Home Companion". Life in Minnesota is depicted in movies such as "Fargo", "Grumpy Old Men", "Grumpier Old Men", "Juno", "Drop Dead Gorgeous", "Young Adult", "A Serious Man", "New in Town", and in famous television series like "Little House on the Prairie (TV series)", "The Mary Tyler Moore Show", "The Golden Girls", "Coach", "The Rocky and Bullwinkle Show", and "Fargo". Major movies that were shot on location in Minnesota include "That Was Then... This Is Now", "Purple Rain", "Airport (1970 film)", "Beautiful Girls", "North Country", and "A Simple Plan".
The Minnesota State Fair, advertised as "The Great Minnesota Get-Together", is an icon of state culture. In a state of 5.4 million people, there were over 1.8 million visitors to the fair in 2014, setting a new attendance record. The fair covers the variety of Minnesotan life, including fine art, science, agriculture, food preparation, 4-H displays, music, the midway, and corporate merchandising. It is known for its displays of seed art, butter sculptures of dairy princesses, the birthing barn, and the "fattest pig" competition. One can also find dozens of varieties of food on a stick, such as Pronto Pups, cheese curds, and deep-fried candy bars. On a smaller scale, many of these attractions are offered at numerous county fairs.
Other large annual festivals include the Saint Paul Winter Carnival, the Minnesota Renaissance Festival, Minneapolis's Aquatennial and Mill City Music Festival, Moondance Jam in Walker, Sonshine Christian music festival in Willmar, the Judy Garland Festival in Grand Rapids, Eelpout Festival on Leech Lake, and WE Fest in Detroit Lakes.
Health.
Minnesotans have low rates of premature death, infant mortality, cardiovascular disease, and occupational fatalities, long life expectancies, and high rates of health insurance and regular exercise. These and other measures have led two groups to rank Minnesota as the healthiest state in the nation, but in one of these rankings Minnesota descended from first to sixth in the nation between 2005 and 2009 because of low levels of public-health funding and the prevalence of binge drinking.
On October 1, 2007 Minnesota became the 17th state to enact a statewide smoking ban in restaurants and bars, the Freedom to Breathe Act.
Medical care is provided by a comprehensive network of hospitals and clinics headed by two institutions with international reputations. The University of Minnesota Medical School is a high-rated teaching institution that has made a number of breakthroughs in treatment, and its research activities contribute significantly to the state's growing biotechnology industry. The Mayo Clinic, a world-renowned hospital based in Rochester, was founded by William Worrall Mayo, an immigrant from England. "U.S. News and World Report"'s 2014-15 survey ranked 4,743 hospitals in the United States in sixteen specialized fields of care and placed the Mayo Clinic in the top four in all sixteen fields except psychiatry, where it ranked seventh. The hospital ranked #1 in eight fields and #2 in three others. The Mayo Clinic and the University of Minnesota are partners in the Minnesota Partnership for Biotechnology and Medical Genomics, a state-funded program that conducts research into cancer, Alzheimer's disease, heart health, obesity, and other areas.
Education.
One of the Minnesota Legislature's first acts when it opened in 1858 was the creation of a normal school in Winona. Minnesota's commitment to education has contributed to a literate and well-educated populace. In 2009, according to the U.S. Census Bureau, Minnesota had the second-highest proportion of high school graduates, with 91.5% of people 25 and older holding a diploma, and the tenth-highest proportion of people with bachelor's degrees. In 2015, Minneapolis was named the nation's "Most Literate City," while St. Paul placed fourth, according to an major annual survey. In a 2013 study conducted by the National Center for Educational Statistics comparing the performance of eighth-grade students internationally in math and science, Minnesota ranked eighth in the world and third in the United States, behind Massachusetts and Vermont. In 2014, Minnesota students earned the tenth-highest average composite score in the nation on the ACT exam. While Minnesota has chosen not to implement school vouchers, it is home to the first charter school.
The state supports a network of public universities and colleges, including 32 institutions in the Minnesota State Colleges and Universities System, and five major campuses of the University of Minnesota. It is also home to more than 20 private colleges and universities, six of which rank among the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
Transportation.
Transportation in Minnesota is overseen by the Minnesota Department of Transportation (MnDOT for short and used in the local news media). Principal transportation corridors radiate from the Minneapolis–St. Paul metropolitan area and Duluth. The major Interstate highways are Interstate 35 (I-35), I-90, and I-94, with I-35 and I-94 passing through the Minneapolis–St. Paul metropolitan area, and I-90 traveling east-west along the southern edge of the state. In 2006, a constitutional amendment was passed that required sales and use taxes on motor vehicles to fund transportation, with at least 40 percent dedicated to public transit. There are nearly two dozen rail corridors in Minnesota, most of which go through Minneapolis–St. Paul or Duluth. There is water transportation along the Mississippi River system and from the ports of Lake Superior.
Minnesota's principal airport is Minneapolis–St. Paul International Airport (MSP), a major passenger and freight hub for Delta Air Lines and Sun Country Airlines. Most other domestic carriers serve the airport. Large commercial jet service is provided at Duluth and Rochester, with scheduled commuter service to six smaller cities via Delta Connection carriers Comair, Mesaba Airlines, SkyWest Airlines, Compass Airlines' and Pinnacle Airlines.
Amtrak's daily "Empire Builder" (Chicago–Seattle/Portland) train runs through Minnesota, calling at the Saint Paul Union Depot and five other stations. Intercity bus providers include Jefferson Lines, Greyhound, and Megabus. Local public transit is provided by bus networks in the larger cities and by two rail services. The Northstar Line commuter rail service runs from Big Lake to the Target Field station in downtown Minneapolis. From there, light rail runs to Saint Paul Union Depot on the Green Line, and to the MSP airport and the Mall of America via the Blue Line.
Law and government.
As with the federal government of the United States, power in Minnesota is divided into three branches: executive, legislative, and judicial.
Executive.
The executive branch is headed by the governor. Governor Mark Dayton, a Democrat, took office on January 3, 2011, to become the first Democratic Governor to hold the seat in two decades. The governor has a cabinet consisting of the leaders of various state government agencies, called commissioners. The other elected constitutional offices are secretary of state, attorney general, and state auditor.
Legislature.
The Minnesota Legislature is a bicameral body consisting of the Senate and the House of Representatives. The state has sixty-seven districts, each covering about sixty thousand people. Each district has one senator and two representatives (each district being divided into "A" and "B" sections). Senators serve for four years and representatives for two years. In the November 2010 election, the Minnesota Republican Party gained twenty-five house seats, giving them control of the House of Representatives by a 72-62 margin. The 2010 election also saw Minnesota voters elect a Republican majority in the Senate for the first time since 1972. In 2012, the Democrats regained the House of Representatives by a margin of 73-61, picking up 11 seats; the Democrats also regained the Minnesota Senate.
Judiciary.
Minnesota's court system has three levels. Most cases start in the district courts, which are courts of general jurisdiction. There are 279 district court judgeships in ten judicial districts. Appeals from the trial courts and challenges to certain governmental decisions are heard by the Minnesota Court of Appeals, consisting of nineteen judges who typically sit in three-judge panels. The seven-justice Minnesota Supreme Court hears all appeals from the Tax Court, the Workers' Compensation Court of Appeals, first-degree murder convictions, and discretionary appeals from the Court of Appeals; it also has original jurisdiction over election disputes.
Two specialized courts within administrative agencies have been established: the Workers' Compensation Court of Appeals, and the Tax Court, which deals with non-criminal tax cases.
Regional.
In addition to the city and county levels of government found in the United States, Minnesota has other entities that provide governmental oversight and planning. Some actions in the Twin Cities metropolitan area are coordinated by the Metropolitan Council, and many lakes and rivers are overseen by watershed districts and soil and water conservation districts.
Federal.
Minnesota's United States senators are Democrat Amy Klobuchar and Democrat Al Franken. The outcome of the 2008 U.S. Senate election in Minnesota was contested until June 30 the next year; when the Minnesota Supreme Court ruled in favor of Franken, Republican Norm Coleman conceded defeat, and the vacant seat was filled by Franken. The state has eight congressional districts; they are represented by Tim Walz (1st district; DFL), John Kline (2nd; R), Erik Paulsen (3rd; R), Betty McCollum (4th; DFL), Keith Ellison (5th; DFL), Michele Bachmann (6th; R), Collin Peterson (7th; DFL), and Rick Nolan (8th; DFL).
Federal court cases are heard in the United States District Court for the District of Minnesota, which holds court in Minneapolis, St. Paul, Duluth, and Fergus Falls. Appeals are heard by the Eighth Circuit Court of Appeals, which is based in St. Louis, Missouri and routinely also hears cases in St. Paul.
Tribal.
The State of Minnesota was created by the USA out of the homelands of the Dakota and Anishinaabe native peoples. Today the remaining native governments are divided into 11 semi-autonomous reservations that negotiate with the USA and state on a peer nation-to-nation basis:
4 Dakota Mdewakanton communities: 
7 Anishinaabe reservations: 
The first 6 of the Anishinaabe bands compose the Minnesota Chippewa Tribe, the collective federally recognized tribal government of the Bois Forte, Fond du Lac, Grand Portage, Leech Lake, Mille Lacs, and White Earth reservations.
Politics.
Minnesota is known for a politically active citizenry, and populism has been a longstanding force among the state's political parties. Minnesota has a consistently high voter turnout (due in part to its liberal voter registration laws) with virtually no evidence of unlawful voting. In the 2008 U.S. presidential election, 78.2 percent of eligible Minnesotans voted—the highest percentage of any U.S. state—versus the national average of 61.2 percent. Previously unregistered voters can register on election day at their polls with evidence of residency.
Hubert Humphrey brought national attention to the state with his address at the 1948 Democratic National Convention. Eugene McCarthy's anti-war stance and popularity in the 1968 New Hampshire primary likely convinced Lyndon B. Johnson to drop out of the presidential election. Minnesotans have consistently cast their Electoral College votes for Democratic presidential candidates since 1976, longer than any other state. Minnesota is the only state in the nation that did not vote for Ronald Reagan in either of his presidential runs. Minnesota has gone to the Democratic Party in every presidential election since 1960, with the exception of 1972, when it was carried by Richard Nixon and the Republican Party.
Both the Democratic and Republican parties have major party status in Minnesota, but its state-level "Democratic" party is actually a separate party, officially known as the Minnesota Democratic-Farmer-Labor Party (DFL). Formed out of a 1944 alliance of the Minnesota Democratic and Farmer-Labor parties, and its distinction from the national Democratic Party, while still official, is now but a technicality.
The state has had active third party movements. The Reform Party, now the Independence Party, was able to elect former mayor of Brooklyn Park and professional wrestler Jesse Ventura to the governorship in 1998. The Independence Party has received enough support to keep major party status. The Green Party, while no longer having major party status, has a large presence in municipal government, notably in Minneapolis and Duluth, where it competes directly with the DFL party for local offices. Official "Major party" status in Minnesota (which grants state funding for elections) is reserved to parties whose candidates receive five percent or more of the vote in any statewide election (e.g., Governor, Secretary of State, U.S. President).
The state's U.S. Senate seats have generally been split since the early 1990s, and in the 108th and 109th Congresses, Minnesota's congressional delegation was split, with four representatives and one senator from each party. In the 2006 midterm election, Democrats were elected to all state offices except for governor and lieutenant governor, where Republicans Tim Pawlenty and Carol Molnau narrowly won reelection. The DFL also posted double-digit gains in both houses of the legislature, elected Amy Klobuchar to the U.S. Senate, and increased the party's U.S. House caucus by one. Keith Ellison (DFL) was elected as the first African American U.S. Representative from Minnesota as well as the first Muslim elected to Congress nationwide. In 2008 DFLer and former comedian and radio talk show host Al Franken beat incumbent Republican Norm Coleman in the United States Senate race by 312 votes out of 3 million cast.
In the election of 2010, Republicans took control of both chambers of the Minnesota legislature for the first time in 38 years, and with Mark Dayton's election the Democratic-Farmer-Labor party took the governor's office for the first time in 20 years. Two years later, the DFL regained control of both houses, and with Governor Dayton in office, the party has same-party control of both the legislative and executive branches for the first time since 1990. Two years later, the Republicans regained control of the Minnesota House in the 2014 election.
Media.
The Twin Cities area is the fifteenth largest media market in the United States as ranked by Nielsen Media Research. The state's other top markets are Fargo–Moorhead (118th nationally), Duluth–Superior (137th), Rochester–Mason City–Austin (152nd), and Mankato (200th).
Broadcast television in Minnesota and the Upper Midwest started on April 27, 1948, when KSTP-TV began broadcasting. Hubbard Broadcasting, which owns KSTP, is now the only locally owned television company in Minnesota. There are currently 39 analog broadcast stations and 23 digital channels broadcast over Minnesota.
The four largest daily newspapers are the "Star Tribune" in Minneapolis, the "Pioneer Press" in Saint Paul, the "Duluth News Tribune" in Duluth and the "Post-Bulletin" in Rochester. "The Minnesota Daily" is the largest student-run newspaper in the U.S. Sites offering daily news on the Web include "The UpTake", "MinnPost", the Twin Cities "Daily Planet", business news site "Finance and Commerce" () and Washington D.C.-based "Minnesota Independent". Weeklies including "City Pages" and monthly publications such as "Minnesota Monthly" are available.
Two of the largest public radio networks, Minnesota Public Radio (MPR) and Public Radio International (PRI), are based in the state. MPR has the largest audience of any regional public radio network in the nation, broadcasting on 37 radio stations. PRI weekly provides more than 400 hours of programming to almost 800 affiliates. The state's oldest radio station, KUOM-AM, was launched in 1922 and is among the 10 –oldest radio stations in the United States. The University of Minnesota-owned station is still on the air, and since 1993 broadcasts a college rock format.
Sports, recreation and tourism.
Minnesota has a very active program of organized amateur and professional sports. Tourism has become an important industry, especially in the Lake region. In the North Country, what had been an industrial area focused on mining and timber has largely been transformed into a vacation destination. Popular interest in the environment and environmentalism, added to traditional interests in hunting and fishing, has attracted a large urban audience within driving range.
Organized sports.
Minnesota has professional men's teams in all major sports. The Hubert H. Humphrey Metrodome was home to the Minnesota Vikings of the National Football League through the 2013 season; it has been torn down and a new stadium is being constructed. The Dome also hosted the Minnesota Twins of Major League Baseball, winners of the 1987 and 1991 World Series, until 2010, when they began playing at Target Field. The Minnesota Timberwolves of the National Basketball Association play in the Target Center. The National Hockey League's Minnesota Wild play in St. Paul's Xcel Energy Center and reached 300 consecutive sold-out games on January 16, 2008.
Minnesota also has minor-league professional sports. NASL Minnesota United FC replaced the Minnesota Thunder in 2010 and plays at the National Sports Center in Blaine. They will eventually join Major League Soccer in 2017 or 2018. 
The Minnesota Swarm play at the Xcel Energy Center and play in the NLL (National Lacrosse League). 
Minor league baseball is represented both by major league-sponsored teams and independent teams such as the St. Paul Saints.
Professional women's sports include the Minnesota Lynx of the Women's National Basketball Association, winners of the 2011 and 2013 WNBA Championships, the Minnesota Lightning of the United Soccer Leagues W-League, the Minnesota Vixen of the Independent Women's Football League, the Minnesota Valkyrie of the Legends Football League, and the Minnesota Whitecaps of the National Women's Hockey League.
The Twin Cities campus of the University of Minnesota is a National Collegiate Athletic Association (NCAA) Division I school competing in the Big Ten Conference. Four additional schools in the state compete in NCAA Division I ice hockey: the University of Minnesota Duluth; Minnesota State University, Mankato; St. Cloud State University and Bemidji State University. There are nine NCAA Division II colleges in the Northern Sun Intercollegiate Conference, and nineteen NCAA Division III colleges in the Minnesota Intercollegiate Athletic Conference and Upper Midwest Athletic Conference.
The Hazeltine National Golf Club has hosted the U.S. Open, U.S. Women's Open, U.S. Senior Open and PGA Championship.
Interlachen Country Club has hosted the U.S. Open, U.S. Women's Open, and Solheim Cup.
Winter Olympic Games medallists from the state include twelve of the twenty members of the gold medal 1980 ice hockey team (coached by Minnesota native Herb Brooks) and the bronze medallist U.S. men's curling team in the 2006 Winter Olympics. Swimmer Tom Malchow won an Olympic gold medal in the 2000 Summer games and a silver medal in 1996.
Grandma's Marathon is run every summer along the scenic North Shore of Lake Superior, and the Twin Cities Marathon winds around lakes and the Mississippi River during the peak of the fall color season. Farther north, Eveleth is the location of the United States Hockey Hall of Fame.
Outdoor recreation.
Minnesotans participate in high levels of physical activity, and many of these activities are outdoors. The strong interest of Minnesotans in environmentalism has been attributed to the popularity of these pursuits.
In the warmer months, these activities often involve water. Weekend and longer trips to family cabins on Minnesota's numerous lakes are a way of life for many residents. Activities include water sports such as water skiing, which originated in the state, boating, canoeing, and fishing. More than 36 percent of Minnesotans fish, second only to Alaska.
Fishing does not cease when the lakes freeze; ice fishing has been around since the arrival of early Scandinavian immigrants. Minnesotans have learned to embrace their long, harsh winters in ice sports such as skating, hockey, curling, and broomball, and snow sports such as cross-country skiing, alpine skiing, snowshoeing, and snowmobiling. Minnesota is the only U.S. state where bandy is played.
State and national forests and the seventy-two state parks are used year-round for hunting, camping, and hiking. There are almost 20000 mi of snowmobile trails statewide. Minnesota has more miles of bike trails than any other state, and a growing network of hiking trails, including the 235 mi Superior Hiking Trail in the northeast. Many hiking and bike trails are used for cross-country skiing during the winter.

</doc>
<doc id="19591" url="http://en.wikipedia.org/wiki?curid=19591" title="Missouri River">
Missouri River

The Missouri River is the longest river in North America. Rising in the Rocky Mountains of western Montana, the Missouri flows east and south for 2341 mi before entering the Mississippi River north of St. Louis, Missouri. The river takes drainage from a sparsely populated, semi-arid watershed of more than half a million square miles (1,300,000 km2), which includes parts of ten U.S. states and two Canadian provinces. When combined with the lower Mississippi River, it forms the world's fourth longest river system.
For over 12,000 years, people have depended on the Missouri and its tributaries as a source of sustenance and transportation. More than ten major groups of Native Americans populated the watershed, most leading a nomadic lifestyle and dependent on enormous buffalo herds that once roamed through the Great Plains. The first Europeans encountered the river in the late seventeenth century, and the region passed through Spanish and French hands before finally becoming part of the United States through the Louisiana Purchase. The Missouri was long believed to be part of the Northwest Passage – a water route from the Atlantic to the Pacific – but when Lewis and Clark became the first to travel the river's entire length, they confirmed the mythical pathway to be no more than a legend.
The Missouri was one of the main routes for the westward expansion of the United States during the 19th century. The growth of the fur trade in the early 1800s laid much of the groundwork as trappers explored the region and blazed trails. Pioneers headed west "en masse" beginning in the 1830s, first by covered wagon, then by the growing numbers of steamboats entering service on the river. Former Native American lands in the watershed were taken over by settlers, leading to some of the most longstanding and violent wars against indigenous peoples in American history.
During the 20th century, the Missouri River basin was extensively developed for irrigation, flood control and the generation of hydroelectric power. Fifteen dams impound the main stem of the river, with hundreds more on tributaries. Meanders have been cut and the river channelized to improve navigation, reducing its length by almost 200 mi from pre-development times. Although the lower Missouri valley is now a populous and highly productive agricultural and industrial region, heavy development has taken its toll on wildlife and fish populations as well as water quality.
Course.
From the Rocky Mountains of Montana and Wyoming, three streams rise to form the headwaters of the Missouri River. The longest begins near Brower's Spring, 9100 ft above sea level, on the southeastern slopes of Mount Jefferson in the Centennial Mountains. Flowing west then north, it runs first in Hell Roaring Creek, then west into the Red Rock; swings northeast to become the Beaverhead, it finally joins with the Big Hole to form the Jefferson. The Firehole River originates at Madison Lake in Wyoming's Yellowstone National Park and joins with the Gibbon to form the Madison, while the Gallatin River rises out of Gallatin Lake, also in the national park. These two streams then flow north and northwest into Montana.
The Missouri River officially starts at the confluence of the Jefferson and Madison in Missouri Headwaters State Park near Three Forks, Montana, and is joined by the Gallatin a mile (1.6 km) downstream. The Missouri then passes through Canyon Ferry Lake, a reservoir west of the Big Belt Mountains. Issuing from the mountains near Cascade, the river flows northeast to the city of Great Falls, where it drops over the Great Falls of the Missouri, a series of five substantial waterfalls. It then winds east through a scenic region of canyons and badlands known as the Missouri Breaks, receiving the Marias River from the west then widening into the Fort Peck Lake reservoir a few miles above the confluence with the Musselshell River. Farther on, the river passes through the Fort Peck Dam, and immediately downstream, the Milk River joins from the north.
Flowing eastwards through the plains of eastern Montana, the Missouri receives the Poplar River from the north before crossing into North Dakota where the Yellowstone River, its greatest tributary by volume, joins from the southwest. At the confluence, the Yellowstone is actually the larger river. The Missouri then meanders east past Williston and into Lake Sakakawea, the reservoir formed by Garrison Dam. Below the dam the Missouri receives the Knife River from the west and flows south to Bismarck, the capital of North Dakota, where the Heart River joins from the west. It slows into the Lake Oahe reservoir just before the Cannonball River confluence. While it continues south, eventually reaching Oahe Dam in South Dakota, the Grand, Moreau and Cheyenne Rivers all join the Missouri from the west.
The Missouri makes a bend to the southeast as it winds through the Great Plains, receiving the Niobrara River and many smaller tributaries from the southwest. It then proceeds to form the boundary of South Dakota and Nebraska, then after being joined by the James River from the north, forms the Iowa–Nebraska boundary. At Sioux City the Big Sioux River comes in from the north. The Missouri flows south to the city of Omaha where it receives its longest tributary, the Platte River, from the west. Downstream, it begins to define the Nebraska–Missouri border, then flows between Missouri and Kansas. The Missouri swings east at Kansas City, where the Kansas River enters from the west, and so on into north-central Missouri. To the east of the Kansas City, Missouri receives, on the left side, the Grand River. It passes south of Columbia and receives the Osage and Gasconade Rivers from the south downstream of Jefferson City. The river then rounds the northern side of St. Louis to join the Mississippi River on the border between Missouri and Illinois.
Watershed.
There is only one river with a personality, a sense of humor, and a woman's caprice; a river that goes traveling sidewise, that interferes in politics, rearranges geography, and dabbles in real estate; a river that plays hide and seek with you today and tomorrow follows you around like a pet dog with a dynamite cracker tied to his tail. That river is the Missouri.-George Fitch
 
With a drainage basin spanning 529350 sqmi,
the Missouri's catchment encompasses nearly one-sixth of the area of the United States or just over five percent of the continent of North America. Comparable to the size of the Canadian province of Quebec, the watershed encompasses most of the central Great Plains, stretching from the Rocky Mountains in the west to the Mississippi River Valley in the east and from the southern extreme of western Canada to the border of the Arkansas River watershed. Compared with the Mississippi River above their confluence, the Missouri is twice as long and drains an area three times as large. The Missouri accounts for 45 percent of the annual flow of the Mississippi past St. Louis, and as much as 70 percent in certain droughts.
In 1990, the Missouri River watershed was home to about 12 million people. This included the entire population of the U.S. state of Nebraska, parts of the U.S. states of Colorado, Iowa, Kansas, Minnesota, Missouri, Montana, North Dakota, South Dakota, and Wyoming, and small southern portions of the Canadian provinces of Alberta and Saskatchewan. The watershed's largest city is Denver, Colorado, with a population of more than six hundred thousand. Denver is the main city of the Front Range Urban Corridor whose cities had a combined population of over four million in 2005, making it the largest metropolitan area in the Missouri River basin. Other major population centers – mostly located in the southeastern portion of the watershed – include Omaha, Nebraska, situated north of the confluence of the Missouri and Platte Rivers; Kansas City, Missouri – Kansas City, Kansas, located at the confluence of the Missouri with the Kansas River, and the St. Louis metropolitan area, situated south of the Missouri River just above its mouth on the Mississippi. In contrast, the northwestern part of the watershed is sparsely populated. However, many northwestern cities, such as Billings, Montana, are among the fastest growing in the Missouri basin.
With more than 170000 mi2 under the plow, the Missouri River watershed includes roughly one-fourth of all the agricultural land in the United States, providing more than a third of the country's wheat, flax, barley and oats. However, only 11000 mi2 of farmland in the basin is irrigated. A further 281000 mi2 of the basin is devoted to the raising of livestock, mainly cattle. Forested areas of the watershed, mostly second-growth, total about 43700 mi2. Urban areas, on the other hand, comprise less than 13000 mi2 of land. Most built-up areas are located along the main stem and a few major tributaries, including the Platte and Yellowstone Rivers.
Elevations in the watershed vary widely, ranging from just over 400 ft at the Missouri's mouth to the 14293 ft summit of Mount Lincoln in central Colorado. The river itself drops a total of 8626 ft from Brower's Spring, the farthest source. Although the plains of the watershed have extremely little local vertical relief, the land rises about 10 feet per mile (1.9 m/km) from east to west. The elevation is less than 500 ft at the eastern border of the watershed, but is over 3000 ft above sea level in many places at the base of the Rockies.
The Missouri's drainage basin has highly variable weather and rainfall patterns, Overall, the watershed is defined by a Continental climate with warm, wet summers and harsh, cold winters. Most of the watershed receives an average of 8 to of precipitation each year. However, the westernmost portions of the basin in the Rockies as well as southeastern regions in Missouri may receive as much as 40 in. The vast majority of precipitation occurs in winter, although the upper basin is known for short-lived but intense summer thunderstorms such as the one which produced the 1972 Black Hills flood through Rapid City, South Dakota. Winter temperatures in Montana, Wyoming and Colorado may drop as low as -60 F, while summer highs in Kansas and Missouri have reached 120 F at times.
As one of the continent's most significant river systems, the Missouri's drainage basin borders on many other major watersheds of the United States and Canada. The Continental Divide, running along the spine of the Rocky Mountains, forms most of the western border of the Missouri watershed. The Clark Fork and Snake River, both part of the Columbia River basin, drain the area west of the Rockies in Montana, Idaho and western Wyoming. The Columbia, Missouri and Colorado River watersheds meet at Three Waters Mountain in Wyoming's Wind River Range. South of there, the Missouri basin is bordered on the west by the drainage of the Green River, a tributary of the Colorado, then on the south by the mainstem of the Colorado. Both the Colorado and Columbia Rivers flow to the Pacific Ocean. However, a large endorheic drainage called the Great Divide Basin exists between the Missouri and Green watersheds in western Wyoming. This area is sometimes counted as part of the Missouri River watershed, even though its waters do not flow to either side of the Continental Divide.
To the north, the much lower Laurentian Divide separates the Missouri River watershed from those of the Oldman River, a tributary of the South Saskatchewan River, as well as the Souris, Sheyenne, and smaller tributaries of the Red River of the North. All of these streams are part of Canada's Nelson River drainage basin, which empties into Hudson Bay. There are also several large endorheic basins between the Missouri and Nelson watersheds in southern Alberta and Saskatchewan. The Minnesota and Des Moines Rivers, tributaries of the upper Mississippi, drain most of the area bordering the eastern side of the Missouri River basin. Finally, on the south, the Ozark Mountains and other low divides through central Missouri, Kansas and Colorado separate the Missouri watershed from those of the White River and Arkansas River, also tributaries of the Mississippi River.
Major tributaries.
Over 95 significant tributaries and hundreds of smaller ones feed the Missouri River, with most of the larger ones coming in as the river draws close to the mouth. Most rivers and streams in the Missouri River basin flow from west to east, following the incline of the Great Plains; however, some eastern tributaries such as the James, Big Sioux and Grand River systems flow from north to south.
The Missouri's largest tributaries by runoff are the Yellowstone in Montana and Wyoming, the Platte in Wyoming, Colorado, and Nebraska, and the Kansas–Republican/Smoky Hill and Osage in Kansas and Missouri. Each of these tributaries drains an area greater than 50000 mi2, and has an average discharge greater than 5000 cuft/s. The Yellowstone River has the highest discharge, even though the Platte is longer and drains a larger area. In fact, the Yellowstone's flow is about 13800 cuft/s – accounting for sixteen percent of total runoff in the Missouri basin and nearly double that of the Platte. On the other end of the scale is the tiny Roe River in Montana, which at 201 ft long is commonly held to be the world's shortest river.
The table on the right lists the ten longest tributaries of the Missouri, along with their respective catchment areas and flows. Length is measured to the hydrologic source, regardless of naming convention. The main stem of the Kansas River, for example, is 148 mi long. However, including the longest headwaters tributaries, the 453 mi Republican River and the 156 mi Arikaree River, brings the total length to 749 mi. Similar naming issues are encountered with the Platte River, whose longest tributary, the North Platte River, is more than twice as long as its mainstream.
The Missouri's headwaters above Three Forks extend much farther upstream than the main stem. Measured to the farthest source at Brower's Spring, the Jefferson River is 298 mi long. Thus measured to its highest headwaters, the Missouri River stretches for 2639 mi. When combined with the lower Mississippi, the Missouri and its headwaters form part of the fourth-longest river system in the world, at 3745 mi.
Discharge.
By discharge, the Missouri is the ninth largest river of the United States, after the Mississippi, St. Lawrence, Ohio, Columbia, Niagara, Yukon, Detroit, and St. Clair. The latter two, however, are sometimes considered part of a strait between Lake Huron and Lake Erie. Among rivers of North America as a whole, the Missouri is thirteenth largest, after the Mississippi, Mackenzie, St. Lawrence, Ohio, Columbia, Niagara, Yukon, Detroit, St. Clair, Fraser, Slave, and Koksoak.
As the Missouri drains a predominantly semi-arid region, its discharge is much lower and more variable than other North American rivers of comparable length. Before the construction of dams, the river flooded twice each year – once in the "April Rise" or "Spring Fresh", with the melting of snow on the plains of the watershed, and in the "June Rise", caused by snowmelt and summer rainstorms in the Rocky Mountains. The latter was far more destructive, with the river increasing to over ten times its normal discharge in some years. The Missouri's discharge is affected by over 17,000 reservoirs with an aggregate capacity of some 141 e6acre-ft. By providing flood control, the reservoirs dramatically reduce peak flows and increase low flows. Evaporation from reservoirs significantly reduces the river's runoff, causing an annual loss of over 3 e6acre-ft from mainstem reservoirs alone.
The United States Geological Survey operates fifty-one stream gauges along the Missouri River. The river's average discharge at Bismarck, 1314.5 mi from the mouth, is 21920 cuft/s. This is from a drainage area of 186400 mi2, or 35% of the total river basin. At Kansas City, 366.1 mi from the mouth, the river's average flow is 55400 cuft/s. The river here drains about 484100 mi2, representing about 91% of the entire basin.
The lowermost gage with a period of record greater than fifty years is at Hermann, Missouri – 97.9 mi upstream of the mouth of the Missouri – where the average annual flow was 87520 cuft/s from 1897 to 2010. About 522500 mi2, or 98.7% of the watershed, lies above Hermann. The highest annual mean was 181800 cuft/s in 1993, and the lowest was 41690 cuft/s in 2006. Extremes of the flow vary even further. The largest discharge ever recorded was over 750000 cuft/s on July 31, 1993, during a historic flood. The lowest, a mere 602 cuft/s – caused by the formation of an ice dam – was measured on December 23, 1963.
Upper and Lower Missouri River.
The Upper Missouri River is north of Gavins Point Dam the last hydroelectric dam of 15 on the river and it's just upstream from Sioux City, Iowa. The lower Missouri River is the 840 river miles until it meets the Mississippi just above St. Louis. The Lower Missouri River has no Hydroelectric or Lock and dams but it has a plethora of Wing dams that inhibit barge traffic because they restrict the channel width. These wing dams have been blamed for flooding, and there currently are no plans to construct any Lock and dams to replace these wing dams on the Missouri River.
"See also" - List of locks and dams of the Upper Mississippi River, List of locks and dams of the Ohio River
Geology.
The Rocky Mountains of southwestern Montana at the headwaters of the Missouri River first rose in the Laramide Orogeny, a mountain-building episode that occurred from around 70 to 45 million years ago (the end of the Mesozoic through the early Cenozoic). This orogeny uplifted Cretaceous rocks along the western side of the Western Interior Seaway, a vast shallow sea that stretched from the Arctic Ocean to the Gulf of Mexico, and deposited the sediments that now underlie much of the drainage basin of the Missouri River.
This Laramide uplift caused the sea to retreat and laid the framework for a vast drainage system of rivers flowing from the Rocky and Appalachian Mountains, the predecessor of the modern-day Mississippi watershed. The Laramide Orogeny is essential to modern Missouri River hydrology, as snow and ice melt from the Rockies provide the majority of the flow in the Missouri and its tributaries.
The Missouri and many of its tributaries cross the Great Plains, flowing over or cutting into the Ogallala Group and older mid-Cenozoic sedimentary rocks. The lowest major Cenozoic unit, the White River Formation, was deposited between roughly 35 and 29 million years ago and consists of claystone, sandstone, limestone, and conglomerate. Channel sandstones and finer-grained overbank deposits of the fluvial Arikaree Group were deposited between 29 and 19 million years ago. The Miocene-age Ogallala and the slightly younger Pliocene-age Broadwater Formation deposited atop the Arikaree Group, and are formed from material eroded off of the Rocky Mountains during a time of increased generation of topographic relief; these formations stretch from the Rocky Mountains nearly to the Iowa border and give the Great Plains much of their gentle but persistent eastward tilt, and also constitute a major aquifer.
Immediately before the Quaternary Ice Age, the Missouri River was likely split into three segments: an upper portion that drained northwards into Hudson Bay,
and middle and lower sections that flowed eastward down the regional slope.
As the Earth plunged into the Ice Age, a pre-Illinoian (or possibly the Illinoian) glaciation diverted the Missouri River southeastwards towards its present confluence with the Mississippi and caused it to integrate into a single river system that cuts across the regional slope. In western Montana, the Missouri River is thought to have once flowed north then east around the Bear Paw Mountains. Sapphires are found in some spots along the river in western Montana. Advances of the continental ice sheets diverted the river and its tributaries, causing them to pool up into large temporary lakes such as Glacial Lakes Great Falls, Glacial Lake Musselshell|Musselshell and others. As the lakes rose, the water in them often spilled across adjacent local drainage divides, creating now-abandoned channels and coulees including the Shonkin Sag, 100 mi long. When the glaciers retreated, the Missouri flowed in a new course along the south side of the Bearpaws, and the lower part of the Milk River tributary took over the original main channel.
The Missouri's nickname, the "Big Muddy", was inspired by its enormous loads of sediment or silt – some of the largest of any North American river. In its pre-development state, the river transported some 175 to per year. The construction of dams and levees has drastically reduced this to 20 to in the present day. Much of this sediment is derived from the river's floodplain, also called the meander belt; every time the river changed course, it would erode tons of soil and rocks from its banks. However, damming and channeling the river has kept it from reaching its natural sediment sources along most of its course. Reservoirs along the Missouri trap roughly 36.4 e6ST of sediment each year. Despite this, the river still transports more than half the total silt that empties into the Gulf of Mexico; the Mississippi River Delta, formed by sediment deposits at the mouth of the Mississippi, constitutes a majority of sediments carried by the Missouri.
First peoples.
Archaeological evidence, especially in Missouri, suggests that man first made his presence in the watershed of the Missouri River between 10,000 and 12,000 years ago at the end of the Pleistocene. During the end of the last glacial period, a great migration of humans began, traveling via the Bering land bridge from Eurasia into and throughout the Americas. As they traveled slowly over centuries, the Missouri River formed one of the main migration paths. Most settled in the Ohio Valley and the lower Mississippi River Valley, but many, including the Mound builders, stayed along the Missouri, becoming the ancestors of the later indigenous peoples of the Great Plains.
The Native Americans that lived along the Missouri had access to ample food, water, and shelter. Many migratory animals inhabited the plains at the time, providing them meat, clothing, and other everyday items; there were also great riparian areas in the river's floodplain that provided them with natural herbs and staple foods. No written records from the tribes and peoples of the pre-European period exist because they did not use writing. According to the writings of explorers, some of the major tribes along the Missouri River included the Otoe, Missouria, Omaha, Ponca, Brulé, Lakota, Sioux, Arikara, Hidatsa, Mandan, Assiniboine, Gros Ventres and Blackfeet.
Natives used the Missouri, at least to a limited extent, as a path of trade and transport. In addition, the river and its tributaries formed tribal boundaries. Lifestyles of the indigenous mostly centered around a semi-nomadic culture; many tribes would have different summer and winter camps. However, the center of Native American wealth and trade lay along the Missouri River in the Dakotas region on its great bend south. A large cluster of walled Mandan, Hidatsa and Arikara villages situated on bluffs and islands of the river was home to thousands, and later served as a market and trading post used by early French and British explorers and fur traders. Following the introduction of horses to Missouri River tribes, possibly from feral European-introduced populations, natives' way of life changed dramatically. The use of the horse allowed them to travel greater distances, and thus facilitated hunting, communications and trade.
Once, tens of millions of American bison (commonly called buffalo), one of the keystone species of the Great Plains and the Ohio Valley, roamed the plains of the Missouri River basin. Most Native American groups in the basin relied heavily on the bison as a food source, and their hides and bones served to create other household items. In time, the species came to benefit from the indigenous peoples' periodic controlled burnings of the grasslands surrounding the Missouri to clear out old and dead growth. The large bison population of the region gave rise to the term "great bison belt", an area of rich annual grasslands that extended from Alaska to Mexico along the eastern flank of the Continental Divide. However, after the arrival of Europeans in North America, both the bison and the Native Americans saw a rapid decline in population. Hunting eliminated bison populations east of the Mississippi River by 1833 and reduced the numbers in the Missouri basin to a mere few hundred. Foreign diseases such as smallpox raged across the land, decimating Native American populations. Left without their primary source of sustenance, many of the remaining indigenous people were amalgamated into resettlement areas and reservations.
Early explorers.
In May 1673, the French explorers Louis Jolliet and Jacques Marquette left the settlement of St. Ignace on Lake Huron and traveled down the Wisconsin and Mississippi Rivers, aiming to reach the Pacific Ocean. In late June, Jolliet and Marquette became the first documented European discoverers of the Missouri River, which according to their journals was in full flood. "I never saw anything more terrific," Jolliet wrote, "a tangle of entire trees from the mouth of the Pekistanoui [Missouri] with such impetuosity that one could not attempt to cross it without great danger. The commotion was such that the water was made muddy by it and could not clear itself." They recorded "Pekitanoui" or "Pekistanoui" as the local name for the Missouri. However, the party never explored the Missouri beyond its mouth, nor did they linger in the area. In addition, they later learned that the Mississippi drained into the Gulf of Mexico and not the Pacific as they had originally presumed; the expedition turned back about 440 mi short of the Gulf at the confluence of the Arkansas River with the Mississippi.
In 1682, France expanded its territorial claims in North America to include land on the western side of the Mississippi River, which included the lower portion of the Missouri. However, the Missouri itself remained formally unexplored until Étienne de Veniard, Sieur de Bourgmont commanded an expedition in 1714 that reached at least as far as the mouth of the Platte River. It is unclear exactly how far Bourgmont traveled beyond there; he described the blond-haired Mandans in his journals, so it is likely he reached as far as their villages in present-day North Dakota. Later that year, Bourgmont published "The Route To Be Taken To Ascend The Missouri River", the first known document to use the name "Missouri River"; many of the names he gave to tributaries, mostly for the native tribes that lived along them, are still in use today. The expedition's discoveries eventually found their way to cartographer Guillaume Delisle, who used the information to create a map of the lower Missouri. In 1718, Jean-Baptiste Le Moyne, Sieur de Bienville requested that the French government bestow upon Bourgmont the Cross of St. Louis because of his "outstanding service to France".
Bourgmont had in fact been in trouble with the French colonial authorities since 1706, when he deserted his post as commandant of Fort Detroit after poorly handling an attack by the Ottawa that resulted in thirty-one deaths. However, his reputation was enhanced in 1720 when the Pawnee – who had earlier been befriended by Bourgmont – massacred the Spanish Villasur expedition near present-day Columbus, Nebraska on the Missouri River and temporarily ending Spanish encroachment on French Louisiana.
Bourgmont established Fort Orleans, the first European settlement of any kind on the Missouri River, near present-day Brunswick, Missouri, in 1723. The following year Bourgmont led an expedition to enlist Comanche support against the Spanish, who continued to show interest in taking over the Missouri. In 1725 Bourgmont brought the chiefs of several Missouri River tribes to visit France. There he was raised to the rank of nobility and did not accompany the chiefs back to North America. Fort Orleans was either abandoned or its small contingent massacred by Native Americans in 1726.
The French and Indian War erupted when territorial disputes between France and Great Britain in North America reached a head in 1754. By 1763, France was defeated by the much greater strength of the British army and was forced to cede its Canadian possessions to the English and Louisiana to the Spanish in the Treaty of Paris, amounting to most of its colonial holdings in North America. Initially, the Spanish did not extensively explore the Missouri and let French traders continue their activities under license. However, this ended after news of the British Hudson's Bay Company incursions in the upper Missouri River watershed was brought back following an expedition by Jacques D'Eglise in the early 1790s. In 1795 the Spanish chartered the Company of Discoverers and Explorers of the Missouri, popularly referred to as the "Missouri Company", and offered a reward for the first person to reach the Pacific Ocean via the Missouri. In 1794 and 1795 expeditions led by Jean Baptiste Truteau and Antoine Simon Lecuyer de la Jonchšre did not even make it as far north as the Mandan villages in central North Dakota.
Arguably the most successful of the Missouri Company expeditions was that of James MacKay and John Evans. The two set out along the Missouri, and established Fort Charles about 20 mi south of present-day Sioux City as a winter camp in 1795. At the Mandan villages in North Dakota, they expelled several British traders, and while talking to the populace they pinpointed the location of the Yellowstone River, which was called "Roche Jaune" ("Yellow Rock") by the French. Although MacKay and Evans failed to accomplish their original goal of reaching the Pacific, they did create the first accurate map of the upper Missouri River.
In 1795, the young United States and Spain signed Pinckney's Treaty, which recognized American rights to navigate the Mississippi River and store goods for export in New Orleans. Three years later, Spain revoked the treaty and in 1800 secretly returned Louisiana to Napoleonic France in the Third Treaty of San Ildefonso. This transfer was so secret that the Spanish continued to administer the territory. In 1801, Spain restored rights to use the Mississippi and New Orleans to the United States.
Fearing that the cutoffs could occur again, President Thomas Jefferson proposed to buy the port of New Orleans from France for $10 million. Instead, faced with a debt crisis, Napoleon offered to sell the entirety of Louisiana, including the Missouri River, for $15 million – amounting to less than 3¢ per acre. The deal was signed in 1803, doubling the size of the United States with the acquisition of the Louisiana Territory. In 1803, Jefferson instructed Meriwether Lewis to explore the Missouri and search for a water route to the Pacific Ocean. By then, it had been discovered that the Columbia River system, which drains into the Pacific, had a similar latitude as the headwaters of the Missouri River, and it was widely believed that a connection or short portage existed between the two. However, Spain balked at the takeover, citing that they had never formally returned Louisiana to the French. Spanish authorities warned Lewis not to take the journey and forbade him from seeing the MacKay and Evans map of the Missouri, although Lewis eventually managed to gain access to it.
Meriwether Lewis and William Clark began their famed expedition in 1804 with a party of thirty-three people in three boats. Although they became the first Europeans to travel the entire length of the Missouri and reach the Pacific Ocean via the Columbia, they found no trace of the Northwest Passage. The maps made by Lewis and Clark, especially those of the Pacific Northwest region, provided a foundation for future explorers and emigrants. They also negotiated relations with multiple Native American tribes and wrote extensive reports on the climate, ecology and geology of the region. Many present-day names of geographic features in the upper Missouri basin originated from their expedition.
American frontier.
Fur trade.
As early as the 18th century, fur trappers entered the extreme northern basin of the Missouri River in the hopes of finding populations of beaver and river otter, the sale of whose pelts drove the thriving North American fur trade. They came from many different places – some from the Canadian fur corporations at Hudson Bay, some from the Pacific Northwest (see also Maritime Fur Trade), and some from the midwestern United States. Most did not stay in the area for long, as they failed to find significant resources.
The first glowing reports of country rich with thousands of game animals came in 1806 when Meriwether Lewis and William Clark returned from their two-year expedition. Their journals described lands rich with thousands of buffalo, beaver, and river otter; and also an abundant population of sea otters on the Pacific Northwest coast. In 1807, explorer Manuel Lisa organized an expedition which would lead to the explosive growth of the fur trade in the upper Missouri River country. Lisa and his crew traveled up the Missouri and Yellowstone Rivers, trading manufactured items in return for furs from local Native American tribes, and established a fort at the confluence of the Yellowstone and a tributary, the Bighorn, in southern Montana. Although the business started small, it quickly grew into a thriving trade.
Lisa's men started construction of Fort Raymond, which sat on a bluff overlooking the confluence of the Yellowstone and Bighorn, in the fall of 1807. The fort would serve primarily as a trading post for bartering with the Native Americans for furs. This method was unlike that of the Pacific Northwest fur trade, which involved trappers hired by the various fur enterprises, namely Hudson's Bay. Fort Raymond was later replaced by Fort Lisa at the confluence of the Missouri and Yellowstone in North Dakota; a second fort also called Fort Lisa was built downstream on the Missouri River in Nebraska. In 1809 the St. Louis Missouri Fur Company was founded by Lisa in conjunction with William Clark and Pierre Choteau, among others. In 1828, the American Fur Company founded Fort Union at the confluence of the Missouri and Yellowstone Rivers. Fort Union gradually became the main headquarters for the fur trade in the upper Missouri basin.
Fur trapping activities in the early 19th century encompassed nearly all of the Rocky Mountains on both the eastern and western slopes. Trappers of the Hudson's Bay Company, St. Louis Missouri Fur Company, American Fur Company, Rocky Mountain Fur Company, North West Company and other outfits worked thousands of streams in the Missouri watershed as well as the neighboring Columbia, Colorado, Arkansas, and Saskatchewan river systems. During this period, the trappers, also called mountain men, blazed trails through the wilderness that would later form the paths pioneers and settlers would travel by into the West. Transport of the thousands of beaver pelts required ships, providing one of the first large motives for river transport on the Missouri to start.
As the 1830s drew to a close, the fur industry slowly began to die as silk replaced beaver fur as a desirable clothing item. By this time, also, the beaver population of streams in the Rocky Mountains had been decimated by intense hunting. Furthermore, frequent Native American attacks on trading posts made it dangerous for employees of the fur companies. In some regions, the industry continued well into the 1840s, but in others such as the Platte River valley, declines of the beaver population contributed to an earlier demise. The fur trade finally disappeared in the Great Plains around 1850, with the primary center of industry shifting to the Mississippi Valley and central Canada. Despite the demise of the once-prosperous trade, however, its legacy led to the opening of the American West and a flood of settlers, farmers, ranchers, adventurers, hopefuls, financially bereft, and entrepreneurs took their place.
Settlers and pioneers.
The river roughly defined the American frontier in the 19th century, particularly downstream from Kansas City, where it takes a sharp eastern turn into the heart of the state of Missouri. The major trails for the opening of the American West all have their starting points on the river, including the California, Mormon, Oregon, and Santa Fe trails. The first westward leg of the Pony Express was a ferry across the Missouri at St. Joseph, Missouri. Similarly, most emigrants arrived at the eastern terminus of the First Transcontinental Railroad via a ferry ride across the Missouri between Council Bluffs, Iowa and Omaha. The Hannibal Bridge became the first bridge to cross the Missouri River in 1869, and its location was a major reason why Kansas City became the largest city on the river upstream from its mouth at St. Louis.
True to the then-ideal of Manifest Destiny, over 500,000 people set out from the river town of Independence, Missouri to their various destinations in the American West from the 1830s to the 1860s. These people had many reasons to embark on this strenuous year-long journey – economic crisis, and later gold strikes including the California Gold Rush, for example. For most, the route took them up the Missouri to Omaha, Nebraska, where they would set out along the Platte River, which flows from the Rocky Mountains in Wyoming and Colorado eastwards through the Great Plains. An early expedition led by Robert Stuart from 1812 to 1813 proved the Platte impossible to navigate by the dugout canoes they used, let alone the large sidewheelers and sternwheelers that would later ply the Missouri in increasing numbers. One explorer remarked that the Platte was "too thick to drink, too thin to plow". Nevertheless, the Platte provided an abundant and reliable source of water for the pioneers as they headed west. Covered wagons, popularly referred to as "prairie schooners", provided the primary means of transport until the beginning of regular boat service on the river in the 1850s.
During the 1860s, gold strikes in Montana, Colorado, Wyoming and northern Utah attracted another wave of hopefuls to the region. Although some freight was hauled overland, most transport to and from the gold fields was done through the Missouri and Kansas Rivers, as well as the Snake River in western Wyoming and the Bear River in Utah, Idaho and Wyoming. It is estimated that of the passengers and freight hauled from the Midwest to Montana, over 80 percent were transported by boat, a journey that took 150 days in the upstream direction. A route more directly west into Colorado lay along the Kansas River and its tributary the Republican River as well as pair of smaller Colorado streams, Big Sandy Creek and the South Platte River, to near Denver. The gold rushes precipitated the decline of the Bozeman Trail as a popular emigration route, as it passed through land held by often-hostile Native Americans. Safer paths were blazed to the Great Salt Lake near Corinne, Utah during the gold rush period, which led to the large-scale settlement of the Rocky Mountains region and eastern Great Basin.
As settlers expanded their holdings into the Great Plains, they ran into land conflicts with Native American tribes. This resulted in frequent raids, massacres and armed conflicts, leading to the federal government creating multiple treaties with the Plains tribes, which generally involved establishing borders and reserving lands for the indigenous. As with many other treaties between the U.S. and Native Americans, they were soon broken, leading to huge wars. Over 1,000 battles, big and small, were fought between the U.S. military and Native Americans before the tribes were forced out of their land onto reservations.
Conflicts between natives and settlers over the opening of the Bozeman Trail in the Dakotas, Wyoming and Montana led to Red Cloud's War, in which the Lakota and Cheyenne fought against the U.S. Army. The fighting resulted in a complete Native American victory. In 1868, the Treaty of Fort Laramie was signed, which "guaranteed" the use of the Black Hills, Powder River Country and other regions surrounding the northern Missouri River to Native Americans without white intervention. The Missouri River was also a significant landmark as it divides northeastern Kansas from western Missouri; pro-slavery forces from Missouri would cross the river into Kansas and spark mayhem during Bleeding Kansas, leading to continued tension and hostility even today between Kansas and Missouri. Another significant military engagement on the Missouri River during this period was the 1861 Battle of Boonville, which did not affect Native Americans but rather was a turning point in the American Civil War that allowed the Union to seize control of transport on the river, discouraging the state of Missouri from joining the Confederacy.
However, the peace and freedom of the Native Americans did not last for long. The Great Sioux War of 1876–77 was sparked when American miners discovered gold in the Black Hills of western South Dakota and eastern Wyoming. These lands were originally set aside for Native American use by the Treaty of Fort Laramie. When the settlers intruded onto the lands, they were attacked by Native Americans. U.S. troops were sent to the area to protect the miners, and drive out the natives from the new settlements. During this bloody period, both the Native Americans and the U.S. military won victories in major battles, resulting in the loss of nearly a thousand lives. The war eventually ended in an American victory, and the Black Hills were opened to settlement. Native Americans of that region were relocated to reservations in Wyoming and southeastern Montana.
Dam-building era.
In the late 19th and early 20th centuries, a great number of dams were built along the course of the Missouri, transforming 35 percent of the river into a chain of reservoirs. River development was stimulated by a variety of factors, first by growing demand for electricity in the rural northwestern parts of the basin, and also by floods and droughts that plagued rapidly growing agricultural and urban areas along the lower Missouri River. Small, privately owned hydroelectric projects have existed since the 1890s, but the large flood-control and storage dams that characterize the middle reaches of the river today were not constructed until the 1950s.
Between 1890 and 1940, five dams were built in the vicinity of Great Falls to generate power from the Great Falls of the Missouri, a chain of giant waterfalls formed by the river in its path through western Montana. Black Eagle Dam, built in 1891 on Black Eagle Falls, was the first dam of the Missouri. Replaced in 1926 with a more modern structure, the dam was little more than a small weir atop Black Eagle Falls, diverting part of the Missouri's flow into the Black Eagle power plant. The largest of the five dams, Ryan Dam, was built in 1913. The dam lies directly above the 87 ft Great Falls, the largest waterfall of the Missouri.
In the same period, several private establishments – most notably the Montana Power Company – began to develop the Missouri River above Great Falls and below Helena for power generation. A small run-of-the river structure completed in 1898 near the present site of Canyon Ferry Dam became the second dam to be built on the Missouri. This rock-filled timber crib dam generated seven and a half megawatts of electricity for Helena and the surrounding countryside. The nearby steel Hauser Dam was finished in 1907, but failed in 1908 because of structural deficiencies, causing catastrophic flooding all the way downstream past Craig. At Great Falls, a section of the Black Eagle Dam was dynamited to save nearby factories from inundation. Hauser was rebuilt in 1910 as a concrete gravity structure, and stands to this day.
Holter Dam, about 45 mi downstream of Helena, was the third hydroelectric dam built on this stretch of the Missouri River. When completed in 1918 by the Montana Power Company and the United Missouri River Power Company, its reservoir flooded the Gates of the Mountains, a limestone canyon which Meriwether Lewis described as "the most remarkable clifts that we have yet seen… the tow[er]ing and projecting rocks in many places seem ready to tumble on us." In 1949, the U.S. Bureau of Reclamation (USBR) began construction on the modern Canyon Ferry Dam to provide flood control to the Great Falls area. By 1954, the rising waters of Canyon Ferry Lake submerged the old 1898 dam, whose powerhouse still stands underwater about 1.5 mi upstream of the present-day dam.
"[The Missouri's temperament was] uncertain as the actions of a jury or the state of a woman's mind." 
The Missouri basin suffered a series of catastrophic floods around the turn of the 20th century, most notably in 1844, 1881, and 1926–1927. In 1940, as part of the Great Depression-era New Deal, the U.S. Army Corps of Engineers (USACE) completed Fort Peck Dam in Montana. Construction of this massive public works project provided jobs for more than 50,000 laborers during the Depression and was a major step in providing flood control to the lower half of the Missouri River. However, Fort Peck only controls runoff from 11 percent of the Missouri River watershed, and had little effect on a severe snowmelt flood that struck the lower basin three years later. This event was particularly destructive as it submerged manufacturing plants in Omaha and Kansas City, greatly delaying shipments of military supplies in World War II.
Flooding damages on the Mississippi–Missouri river system were one of the primary reasons for which Congress passed the Flood Control Act of 1944, opening the way for the USACE to develop the Missouri on a massive scale. The 1944 act authorized the Pick–Sloan Missouri Basin Program (Pick–Sloan Plan), which was a composite of two widely varying proposals. The Pick plan, with an emphasis on flood control and hydroelectric power, called for the construction of large storage dams along the main stem of the Missouri. The Sloan plan, which stressed the development of local irrigation, included provisions for roughly 85 smaller dams on tributaries.
In the early stages of Pick–Sloan development, tentative plans were made to build a low dam on the Missouri at Riverdale, North Dakota and 27 smaller dams on the Yellowstone River and its tributaries. This was met with controversy from inhabitants of the Yellowstone basin, and eventually the USBR proposed a solution: to greatly increase the size of the proposed dam at Riverdale – today's Garrison Dam, thus replacing the storage that would have been provided by the Yellowstone dams. Because of this decision, the Yellowstone is now the longest free-flowing river in the contiguous United States. In the 1950s, construction commenced on the five mainstem dams – Garrison, Oahe, Big Bend, Fort Randall and Gavins Point – proposed under the Pick-Sloan Plan. Along with Fort Peck, which was integrated as a unit of the Pick-Sloan Plan in the 1940s, these dams now form what is known as the Missouri River Mainstem System.
The six dams of the Mainstem System, chiefly Fort Peck, Garrison and Oahe, are among the largest dams in the world by volume; their sprawling reservoirs also rank within the biggest of the nation. Holding up to 74.1 e6acre-ft in total, the six reservoirs can store more than three year's worth of the river's flow as measured below Gavins Point, the lowermost dam. This enormous capacity makes it the largest reservoir system in the United States and one of the largest in North America. In addition to storing irrigation water, the system also includes an annual flood-control reservation of 16.3 e6acre-ft. Mainstem power plants generate about 9.3 billion KWh annually – equal to a constant output of almost 1,100 megawatts. Along with nearly 100 smaller dams on tributaries, namely the Bighorn, Platte, Kansas, and Osage Rivers, the system provides irrigation water to nearly 7500 mi2 of land.
The table at left lists statistics of all fifteen dams on the Missouri River, ordered downstream. Many of the run-of-the-river dams on the Missouri (marked in yellow) form very small impoundments which may or may not have been given names; those unnamed are left blank. All dams are on the upper half of the river above Sioux City; the lower river is uninterrupted due to its longstanding use as a shipping channel.
Navigation.
"[Missouri River shipping] never achieved its expectations. Even under the very best of circumstances, it was never a huge industry."
Boat travel on the Missouri started with the wood-framed canoes and bull boats of the Native Americans, which were used for thousands of years before the introduction of larger craft to the river upon colonization of the Great Plains. The first steamboat on the Missouri was the "Independence", which started running between St. Louis and Keytesville, Missouri around 1819. By the 1830s, large mail and freight-carrying vessels were running regularly between Kansas City and St. Louis, and many traveled even farther upstream. A handful, such as the "Western Engineer" and the "Yellowstone", were able to make it up the river as far as eastern Montana.
During the early 19th century, at the height of the fur trade, steamboats and keelboats began traveling nearly the whole length of the Missouri from Montana's rugged Missouri Breaks to the mouth, carrying beaver and buffalo furs to and from the areas that the trappers frequented. This resulted in the development of the Missouri River mackinaw, which specialized in carrying furs. Since these boats could only travel downriver, they were dismantled and sold for lumber upon their arrival at St. Louis.
Water transport increased through the 1850s with multiple craft ferrying pioneers, emigrants and miners; many of these runs were from St. Louis or Independence to near Omaha. There, most of these people would set out overland along the large but shallow and unnavigable Platte River, which was described by pioneers as "a mile wide and an inch deep" and "the most magnificent and useless of rivers". Steamboat navigation peaked in 1858 with over 130 boats operating full-time on the Missouri, with many more smaller vessels. Many of the earlier vessels were built on the Ohio River before being transferred to the Missouri. Side-wheeler steamboats were preferred over the larger sternwheelers used on the Mississippi and Ohio because of their greater maneuverability.
The industry's success, however, did not guarantee safety. In the early decades before the river's flow was controlled by man, its sketchy rises and falls and its massive amounts of sediment, which prevented a clear view of the bottom, wrecked some 300 vessels. Because of the dangers of navigating the Missouri River, the average ship's lifespan was short, only about four years. The development of the Transcontinental and Northern Pacific Railroads marked the beginning of the end of steamboat commerce on the Missouri. Outcompeted by trains, the number of boats slowly dwindled, until there was almost nothing left by the 1890s. Transport of agricultural and mining products by barge, however, saw a revival in the early twentieth century.
Passage to Sioux City.
Since the beginning of the 20th century, the Missouri River has been extensively engineered for water transport purposes, and about 32 percent of the river now flows through artificially straightened channels. In 1912, the USACE was authorized to maintain the Missouri to a depth of six feet (1.8 m) from the Port of Kansas City to the mouth, a distance of 368 mi. This was accomplished by constructing levees and wing dams to direct the river's flow into a straight, narrow channel and prevent sedimentation. In 1925, the USACE began a project to widen the river's navigation channel to 200 ft; two years later, they began dredging a deep-water channel from Kansas City to Sioux City. These modifications have reduced the river's length from some 2540 mi in the late 19th century to 2341 mi in the present day.
Construction of dams on the Missouri under the Pick-Sloan Plan in the mid-twentieth century was the final step in aiding navigation. The large reservoirs of the Mainstem System help provide a dependable flow to maintain the navigation channel year-round, and are capable of halting most of the river's annual freshets. However, high and low water cycles of the Missouri – notably the protracted early-21st-century drought in the Missouri River basin and historic floods in 1993 and 2011 – are difficult for even the massive Mainstem System reservoirs to control.
In 1945, the USACE began the Missouri River Bank Stabilization and Navigation Project, which would permanently increase the river's navigation channel to a width of 300 ft and a depth of nine feet (2.7 m). During work that continues to this day, the 735 mi navigation channel from Sioux City to St. Louis has been controlled by building rock dikes to direct the river's flow and scour out sediments, sealing and cutting off meanders and side channels, and dredging the riverbed. However, the Missouri has often resisted the efforts of the USACE to control its depth. In 2006, several U.S. Coast Guard boats ran aground in the Missouri River because the navigation channel had been severely silted. The USACE was blamed for failing to maintain the channel to the minimum depth.
In 1929, the Missouri River Navigation Commission estimated the total amount of goods shipped on the river annually at 15 million tons (13.6 million metric tons), providing widespread consensus for the creation of a navigation channel. However, shipping traffic has since been far lower than expected – shipments of commodities including produce, manufactured items, lumber, and oil averaged only 683,000 tons (616,000 t) per year from 1994 to 2006.
By tonnage of transported material, Missouri is by far the largest user of the river accounting for 83 percent of river traffic, while Kansas has 12 percent, Nebraska three percent and Iowa two percent. Almost all of the barge traffic on the Missouri River ships sand and gravel dredged from the lower 500 mi of the river; the remaining portion of the shipping channel now sees little to no use by commercial vessels.
Traffic decline.
Tonnage of goods shipped by barges on the Missouri River has seen a serious decline from the 1960s to the present. In the 1960s, the USACE predicted an increase to 12 e6ST per year by 2000, but instead the opposite has happened. The amount of goods plunged from 3.3 e6ST in 1977 to just 1.3 e6ST in 2000. One of the largest drops has been in agricultural products, especially wheat. Part of the reason is that irrigated land along the Missouri has only been developed to a fraction of its potential. In 2006, barges on the Missouri hauled only 200000 ST of products which is equal to the amount of "daily" freight traffic on the Mississippi.
Drought conditions in the early 21st century and competition from other modes of transport – mainly railroads – are the primary reason for decreasing river traffic on the Missouri. The failure of the USACE to consistently maintain the navigation channel has also hampered the industry. Currently, efforts are being made to revive the shipping industry on the Missouri River, because of the efficiency and cheapness of river transport to haul agricultural products, and the overcrowding of alternative transportation routes. Solutions such as expanding the navigation channel and releasing more water from reservoirs during the peak of the navigation season are being considered. Drought conditions lifted in 2010, in which about 334000 ST were barged on the Missouri, representing the first significant increase in shipments since 2000. However, flooding in 2011 closed record stretches of the river to boat traffic – "wash[ing] away hopes for a bounce-back year."
There are no lock and dams on the lower Missouri River, but there are plenty of wing dams that jettie out into the river and make it harder for barges to navigate. In contrast, the upper Mississippi has 29 locks and dams and averaged 61.3 million tons of cargo annually from 2008 to 2011, and its locks are closed in the winter.
Ecology.
Natural history.
Historically, the thousands of square miles that comprised the floodplain of the Missouri River supported a wide range of plant and animal species. Biodiversity generally increased proceeding downstream from the cold, subalpine headwaters in Montana to the temperate, moist climate of Missouri. Today, the river's riparian zone consists primarily of cottonwoods, willows and sycamores, with several other types of trees such as maple and ash. Average tree height generally increases farther from the riverbanks for a limited distance, as land adjacent to the river is vulnerable to soil erosion during floods. Because of its large sediment concentrations, the Missouri does not support many aquatic invertebrates. However, the basin does support about 300 species of birds and 150 species of fish, some of which are endangered such as the Pallid sturgeon. The Missouri's aquatic and riparian habitats also support several species of mammals, such as minks, river otters, beavers, muskrats, and raccoons.
The World Wide Fund For Nature divides the Missouri River watershed into three freshwater ecoregions: the Upper Missouri, Lower Missouri and Central Prairie. The Upper Missouri, roughly encompassing the area within Montana, Wyoming, southern Alberta and Saskatchewan, and North Dakota, comprises mainly semiarid shrub-steppe grasslands with sparse biodiversity because of Ice Age glaciations. There are no known endemic species within the region. Except for the headwaters in the Rockies, there is little precipitation in this part of the watershed. The Middle Missouri ecoregion, extending through Colorado, southwestern Minnesota, northern Kansas, Nebraska, and parts of Wyoming and Iowa, has greater rainfall and is characterized by temperate forests and grasslands. Plant life is more diverse in the Middle Missouri, which is also home to about twice as many animal species. Finally, the Central Prairie ecoregion is situated on the lower part of the Missouri, encompassing all or parts of Missouri, Kansas, Oklahoma and Arkansas. Despite large seasonal temperature fluctuations, this region has the greatest diversity of plants and animals of the three. Thirteen species of crayfish are endemic to the lower Missouri.
Human impacts.
Since the beginning of river commerce and industrial development in the 1800s, the Missouri has been severely polluted and its water quality degraded by human activity. Most of the river's floodplain habitat is long gone, replaced by irrigated agricultural land. Development of the floodplain has led to increasing amounts of people and infrastructure within areas at high risk of inundation. Levees have been constructed along more than a third of the river in order to keep floodwater within the channel, but with the consequences of faster stream velocity and a resulting increase of peak flows in downstream areas. Fertilizer runoff, which causes elevated levels of nitrogen and other nutrients, is a major problem along the Missouri River, especially in Iowa and Missouri. This form of pollution also heavily affects the upper Mississippi, Illinois and Ohio Rivers. Low oxygen levels in rivers and the vast Gulf of Mexico dead zone at the end of the Mississippi Delta are both results of high nutrient concentrations in the Missouri and other tributaries of the Mississippi.
Channelization of the lower Missouri waters has made the river narrower, deeper and less accessible to riparian flora and fauna. Numerous dams and bank stabilization projects have been constructed to facilitate the conversion of 300000 acre of Missouri River floodplain to agricultural land. Channel control has significantly reduced the volume of sediment transported downstream by the river and eliminated critical habitat for fish, birds and amphibians. By the early 21st century, declines in populations of native species prompted the U.S. Fish and Wildlife Service to issue a biological opinion recommending restoration of river habitats for federally endangered bird and fish species.
The USACE began work on ecosystem restoration projects along the lower Missouri River in the early 21st century. Because of the low use of the shipping channel in the lower Missouri maintained by the USACE, it is now considered feasible to remove some of the levees, dikes, and wing dams that constrict the river's flow, thus allowing it to naturally restore its banks. By 2001, there were 87000 acre of riverside floodplain undergoing active restoration.
Restoration projects have re-mobilized some of the sediments that had been trapped behind bank stabilization structures, prompting concerns of exacerbated nutrient and sediment pollution locally and downstream in the northern Gulf of Mexico. A 2010 National Research Council report assessed the roles of sediment in the Missouri River, evaluating current habitat restoration strategies and alternative ways to manage sediment. The report found that a better understanding of sediment processes in the Missouri River, including the creation of a "sediment budget" – an accounting of sediment transport, erosion, and deposition volumes for the length of the Missouri River – would provide a foundation for projects to improve water quality standards and protect endangered species.
Tourism and recreation.
With over 1500 mi2 of open water, the six reservoirs of the Missouri River Mainstem System provide some of the main recreational areas within the basin. Visitation has increased from 10 million visitor-hours in the mid-1960s to over 60 million visitor-hours in 1990. Development of visitor facilities was spurred by the Federal Water Project Recreation Act of 1965, which required the USACE to build and maintain boat ramps, campgrounds and other public facilities along major reservoirs. Recreational use of Missouri River reservoirs is estimated to contribute $85–100 million to the regional economy each year.
The Lewis and Clark National Historic Trail, some 3700 mi long, follows nearly the entire Missouri River from its mouth to its source, retracing the route of the Lewis and Clark Expedition. Extending from Wood River, Illinois, in the east, to Astoria, Oregon, in the west, it also follows portions of the Mississippi and Columbia Rivers. The trail, which spans through eleven U.S. states, is maintained by various federal and state government agencies; it passes through some 100 historic sites, notably archaeological locations including the Knife River Indian Villages National Historic Site.
Parts of the river itself are designated for recreational or preservational use. The Missouri National Recreational River consists of portions of the Missouri downstream from Fort Randall and Gavins Point Dams that total 98 mi. These reaches exhibit islands, meanders, sandbars, underwater rocks, riffles, snags, and other once-common features of the lower river that have now disappeared under reservoirs or have been destroyed by channeling. About forty-five steamboat wrecks are scattered along these reaches of the river.
Downstream from Great Falls, Montana, about 149 mi of the river course through a rugged series of canyons and badlands known as the Missouri Breaks. This part of the river, designated a U.S. National Wild and Scenic River in 1976, flows within the Upper Missouri Breaks National Monument, a 375000 acre preserve comprising steep cliffs, deep gorges, arid plains, badlands, archaeological sites, and whitewater rapids on the Missouri itself. The preserve includes a wide variety of plant and animal life; recreational activities include boating, rafting, hiking and wildlife observation.
In north-central Montana, some 1100000 acre along over 125 mi of the Missouri River, centering around Fort Peck Lake, comprise the Charles M. Russell National Wildlife Refuge. The wildlife refuge consists of a native northern Great Plains ecosystem that has not been heavily affected by human development, except for the construction of Fort Peck Dam. Although there are few designated trails, the whole preserve is open to hiking and camping.
Many U.S. national parks, such as Glacier National Park, Rocky Mountain National Park, Yellowstone National Park and Badlands National Park are in the watershed. Parts of other rivers in the basin are set aside for preservation and recreational use – notably the Niobrara National Scenic River, which is a 76 mi protected stretch of the Niobrara River, one of the Missouri's longest tributaries. The Missouri flows through or past many National Historic Landmarks, which include Three Forks of the Missouri, Fort Benton, Montana, Big Hidatsa Village Site, Fort Atkinson, Nebraska and Arrow Rock Historic District.
The Missouri River in Upper Missouri Breaks National Monument, Montana, at the confluence with Cow Creek
Works cited.
</dl>

</doc>
<doc id="19593" url="http://en.wikipedia.org/wiki?curid=19593" title="Missouria">
Missouria

The Missouria or Missouri (in their own language, Niúachi, also spelled Niutachi) are a Native American tribe that originated in the Great Lakes region of United States before European contact. The tribe belongs to the Chiwere division of the Siouan language family, together with the Iowa and Otoe. 
Historically, the tribe lived in bands near the mouth of the Grand River at its confluence with the Missouri River; the mouth of the Missouri at its confluence with the Mississippi River, and in present-day Saline County, Missouri. Since Indian removal, today they live primarily in Oklahoma. They are federally recognized as the Otoe-Missouria Tribe of Indians, based in Red Rock, Oklahoma.
Name.
French colonists adapted a form of the Illinois language-name for the people: "Wimihsoorita". Their name means "One who has dugout canoes". In their own Siouan language, the Missouri call themselves "Niúachi", also spelled "Niutachi", meaning "People of the River Mouth." The Osage called them the "Waçux¢a," and the Quapaw called them the "Wa-ju'-xd¢ǎ." 
The state of Missouri and the Missouri River are named for the tribe.
History.
The tribe's oral history tells that they once lived north of the Great Lakes. They began migrating south in the 16th century. By 1600, the Missouria lived near the confluence of the Grand and Missouri rivers, where they settled through the 18th century. Their tradition says that they split from the Otoe tribe, which belongs to the same Chiwere branch of the Siouan language, because of a love affair between the children of two tribal chiefs. 
The 17th century brought hardships to the Missouria. The Sauk and Fox frequently attacked them. Their society was even more disrupted by the high fatalities from epidemics of smallpox and other Eurasian infectious diseases that accompanied contact with Europeans. The French explorer Jacques Marquette contacted the tribe in 1673 and paved the way for trade with the French.
The Missouria migrated west of the Missouri River into Osage territory. During this time, they acquired horses and hunted buffalo. The French explorer Étienne de Veniard, Sieur de Bourgmont visited the people in the early 1720s. He married the daughter of a Missouria chief. They settled nearby, and Veniard created alliances with the people. He built Fort Orleans in 1723 as a trading post near present-day Brunswick, Missouri. It was occupied until 1726. 
In 1730 an attack by the Sauk/Fox tribe nearly destroyed the Missouria, killing hundreds. Most survivors reunited with the Otoe, while some joined the Osage and Kansa. After a smallpox outbreak in 1829, fewer than 100 Missouria survived, and they all joined the Otoe.
They signed treaties with the US government in 1830 and 1854 to cede their lands in Missouri. They relocated to the Otoe-Missouria reservation, created on the Big Blue River at the Kansas-Nebraska border. The US pressured the two tribes into ceding more lands in 1876 and 1881.
In 1880 the tribes split into two factions, the Coyote, who were traditionalists, and the Quakers, who were assimilationists. The Coyote settled on the Iowa Reservation in Indian Territory. The Quakers negotiated a small separate reservation in Indian Territory. By 1890 most of the Coyote band rejoined the Quakers on their reservation. Under the Dawes Act, by 1907 members of the tribes were registered and allotted individual plots of land per household. The US declared any excess communal land of the tribe as "surplus" and sold it to European-American settlers. The tribe merged with the Otoe tribe.
The Curtis Act required the disbanding of tribal courts and governments in order to assimilate the people and prepare the territory for statehood, but the tribe created their own court system in 1900. The Missouria were primarily farmers in the early 20th century. After oil was discovered on their lands in 1912, the US government forced many of the tribe off their allotments.
Population.
According to the enthnographer James Mooney, the population of the tribe was about 200 families in 1702; 1000 people in 1780; 300 in 1805; 80 in 1829, when they were living with the Otoe; and 13 in 1910. Since then, their population numbers are combined with those of the Otoe.
Today.
The Missouria are members of the federally recognized tribe, the Otoe-Missouria Tribe of Indians, based in Red Rock, Oklahoma.

</doc>
<doc id="19594" url="http://en.wikipedia.org/wiki?curid=19594" title="Missile">
Missile

In modern military usage, a missile, or guided missile, is a self-propelled precision-guided munition system, as opposed to an unguided self-propelled munition, referred to as a rocket. Missiles have four system components: targeting and/or missile guidance, flight system, engine, and warhead. Missiles come in types adapted for different purposes: surface-to-surface and air-to-surface missiles (ballistic, cruise, anti-ship, anti-tank, etc.), surface-to-air missiles (and anti-ballistic), air-to-air missiles, and anti-satellite weapons. All known existing missiles are designed to be propelled during powered flight by chemical reactions inside a rocket engine, jet engine, or other type of engine. Non-self-propelled airborne explosive devices are generally referred to as shells and usually have a shorter range than missiles.
In ordinary British-English usage predating guided weapons, a missile is "any thrown object", such as objects thrown at players by rowdy spectators at a sporting event.
Etymology and usage.
The word "missile" comes from the Latin verb "mittere", meaning "to send".
In military usage, munitions projected towards a target are broadly categorised as follows:
A common further sub-division is to consider "ballistic missile" to mean a munition that follows a ballistic trajectory and "cruise missile" to describe a munition that generates lift, similar to an airplane.
Early development.
The first missiles to be used operationally were a series of missiles developed by Nazi Germany in World War II. Most famous of these are the V-1 flying bomb and V-2 rocket, both of which used a simple mechanical autopilot to keep the missile flying along a pre-chosen route. Less well known were a series of anti-shipping and anti-aircraft missiles, typically based on a simple radio control (command guidance) system directed by the operator. However, these early systems in World War II were only built in small numbers.
Technology.
Guided missiles have a number of different system components:
Guidance systems.
Missiles may be targeted in a number of ways. The most common method is to use some form of radiation, such as infrared, lasers or radio waves, to guide the missile onto its target. This radiation may emanate from the target (such as the heat of an engine or the radio waves from an enemy radar), it may be provided by the missile itself (such as a radar), or it may be provided by a friendly third party (such as the radar of the launch vehicle/platform, or a laser designator operated by friendly infantry). The first two are often known as "fire-and-forget" as they need no further support or control from the launch vehicle/platform in order to function. Another method is to use a TV guidance—using either visible light or infrared—in order to see the target. The picture may be used either by a human operator who steers the missile onto its target or by a computer doing much the same job. One of the more bizarre guidance methods instead used a pigeon to steer the missile to its target. 
Many missiles use a combination of two or more of the above methods to improve accuracy and the chances of a successful engagement.
Targeting systems.
Another method is to target the missile by knowing the location of the target and using a guidance system such as INS, TERCOM or satellite guidance. This guidance system guides the missile by knowing the missile's current position and the position of the target, and then calculating a course between them. This job can also be performed somewhat crudely by a human operator who can see the target and the missile and guide it using either cable- or radio-based remote control, or by an automatic system that can simultaneously track the target and the missile.
Furthermore, some missiles use initial targeting, sending them to a target area, where they will switch to primary targeting, using either radar or IR targeting to acquire the target.
Flight system.
Whether a guided missile uses a targeting system, a guidance system or both, it needs a flight system. The flight system uses the data from the targeting or guidance system to maneuver the missile in flight, allowing it to counter inaccuracies in the missile or to follow a moving target. There are two main systems: vectored thrust (for missiles that are powered throughout the guidance phase of their flight) and aerodynamic maneuvering (wings, fins, canard (aeronautics), etc.).
Engine.
Missiles are powered by an engine, generally either a type of rocket engine or jet engine. Rockets are generally of the solid fuel type for ease of maintenance and fast deployment, although some larger ballistic missiles use Liquid-propellant rockets. Jet engines are generally used in cruise missiles, most commonly of the turbojet type, due to its relative simplicity and low frontal area. Turbofans and ramjets are the only other common forms of jet engine propulsion, although any type of engine could theoretically be used. Missiles often have multiple engine stages, particularly in those launched from the surface. These stages may all be of similar types or may include a mix of engine types − for example, surface-launched cruise missiles often have a rocket booster for launching and a jet engine for sustained flight.
Some missiles may have additional propulsion from another source at launch; for example, the V1 was launched by a catapult, and the MGM-51 Shillelagh was fired out of a tank gun (using a smaller charge than would be used for a shell).
Warhead.
Missiles generally have one or more explosive warheads, although other weapon types may also be used. The warheads of a missile provide its primary destructive power (many missiles have extensive secondary destructive power due to the high kinetic energy of the weapon and unburnt fuel that may be on board). Warheads are most commonly of the high explosive type, often employing shaped charges to exploit the accuracy of a guided weapon to destroy hardened targets. Other warhead types include submunitions, incendiaries, nuclear weapons, chemical, biological or radiological weapons or kinetic energy penetrators.
Warheadless missiles are often used for testing and training purposes.
Basic roles.
Missiles are generally categorized by their launch platform and intended target. In broadest terms, these will either be surface (ground or water) or air, and then sub-categorized by range and the exact target type (such as anti-tank or anti-ship). Many weapons are designed to be launched from both surface or the air, and a few are designed to attack either surface or air targets (such as the ADATS missile). Most weapons require some modification in order to be launched from the air or surface, such as adding boosters to the surface-launched version.
Surface-to-Surface/Air-to-Surface.
Ballistic.
After the boost stage, ballistic missiles follow a trajectory mainly determined by ballistics. The guidance is for relatively small deviations from that.
Ballistic missiles are largely used for land attack missions. Although normally associated with nuclear weapons, some conventionally armed ballistic missiles are in service, such as MGM-140 ATACMS. The V2 had demonstrated that a ballistic missile could deliver a warhead to a target city with no possibility of interception, and the introduction of nuclear weapons meant it could efficiently do damage when it arrived. The accuracy of these systems was fairly poor, but post-war development by most military forces improved the basic Inertial navigation system concept to the point where it could be used as the guidance system on Intercontinental ballistic missiles flying thousands of kilometers. Today, the ballistic missile represents the only strategic deterrent in most military forces; however, some ballistic missiles are being adapted for conventional roles, such as the Russian Iskander or the Chinese DF-21D anti-ship ballistic missile. Ballistic missiles are primarily surface-launched from mobile launchers, silos, ships or submarines, with air launch being theoretically possible with a weapon such as the cancelled Skybolt missile.
The Russian Topol M (SS-27 Sickle B) is the fastest (7,320 m/s) missile currently in service.
Cruise missile.
The V1 had been successfully intercepted during World War II, but this did not make the cruise missile concept entirely useless. After the war, the US deployed a small number of nuclear-armed cruise missiles in Germany, but these were considered to be of limited usefulness. Continued research into much longer-ranged and faster versions led to the US's SM-64 Navaho and its Soviet counterparts, the Burya and Buran cruise missile. However, these were rendered largely obsolete by the ICBM, and none were used operationally. Shorter-range developments have become widely used as highly accurate attack systems, such as the US Tomahawk missile, the Russian Kh-55, the German KEPD 350, and the Pakistani Babur (cruise missile). The BrahMos cruise missile, which is a joint venture between India and Russia, is different in this class, as it is a supersonic cruise missile that can travel much faster than other cruise missiles, which are subsonic. 
Cruise missiles are generally associated with land-attack operations, but also have an important role as anti-shipping weapons. They are primarily launched from air, sea or submarine platforms in both roles, although land-based launchers also exist.
Anti-ship.
Another major German missile development project was the anti-shipping class (such as the Fritz X and Henschel Hs 293), intended to stop any attempt at a cross-channel invasion. However, the British were able to render their systems useless by jamming their radios, and missiles with wire guidance were not ready by D-Day. After the war, the anti-shipping class slowly developed and became a major class in the 1960s with the introduction of the low-flying jet- or rocket-powered cruise missiles known as "sea-skimmers". These became famous during the Falklands War, when an Argentine Exocet missile sank a Royal Navy destroyer.
A number of anti-submarine missiles also exist; these generally use the missile in order to deliver another weapon system such as a torpedo or depth charge to the location of the submarine, at which point the other weapon will conduct the underwater phase of the mission.
Anti-tank.
By the end of WWII, all forces had widely introduced unguided rockets using High-explosive anti-tank warheads as their major anti-tank weapon (see Panzerfaust, Bazooka). However, these had a limited useful range of 100 m or so, and the Germans were looking to extend this with the use of a missile using wire guidance, the X-7. After the war, this became a major design class in the later 1950s and, by the 1960s, had developed into practically the only non-tank anti-tank system in general use. During the 1973 Yom Kippur War between Israel and Egypt, the 9M14 Malyutka (aka "Sagger") man-portable anti-tank missile proved potent against Israeli tanks. While other guidance systems have been tried, the basic reliability of wire guidance means this will remain the primary means of controlling anti-tank missiles in the near future. Anti-tank missiles may be launched from aircraft, vehicles or by ground troops in the case of smaller weapons.
Surface-to-air.
Anti-aircraft.
By 1944, US and British air forces were sending huge air fleets over occupied Europe, increasing the pressure on the Luftwaffe day and night fighter forces. The Germans were keen to get some sort of useful ground-based anti-aircraft system into operation. Several systems were under development, but none had reached operational status before the war's end. The US Navy also started missile research to deal with the Kamikaze threat. By 1950, systems based on this early research started to reach operational service, including the US Army's MIM-3 Nike Ajax and the Navy's "3T's" (Talos, Terrier, Tartar), soon followed by the Soviet S-25 Berkut and S-75 Dvina and French and British systems. Anti-aircraft weapons exist for virtually every possible launch platform, with surface-launched systems ranging from huge, self-propelled or ship-mounted launchers to man-portable systems.
Anti-ballistic.
Like most missiles, the Arrow missile, S-300, S-400 (missile), Advanced Air Defence and MIM-104 Patriot are for defense against short-range missiles and carry explosive warheads.
However, in the case of a large closing speed, a projectile without explosives is used; just a collision is sufficient to destroy the target. See Missile Defense Agency for the following systems being developed:
Air-to-air.
Soviet RS-82 rockets were successfully tested in combat at the Battle of Khalkhin Gol in 1939.
German experience in World War II demonstrated that destroying a large aircraft was quite difficult, and they had invested considerable effort into air-to-air missile systems to do this. Their Messerschmitt Me 262's jets often carried R4M rockets, and other types of "bomber destroyer" aircraft had unguided rockets as well. In the post-war period, the R4M served as the pattern for a number of similar systems, used by almost all interceptor aircraft during the 1940s and 1950s. Lacking guidance systems, such rockets had to be carefully aimed at relatively close range to hit the target successfully. The United States Navy and U.S. Air Force began deploying guided missiles in the early 1950s, most famous being the US Navy's AIM-9 Sidewinder and the USAF's AIM-4 Falcon. These systems have continued to advance, and modern air warfare consists almost entirely of missile firing. In the Falklands War, less powerful British Harriers were able to defeat faster Argentinian opponents using AIM-9G missiles provided by the United States as the conflict began. The latest heat-seeking designs can lock onto a target from various angles, not just from behind, where the heat signature from the engines is strongest. Other types rely on radar guidance (either on board or "painted" by the launching aircraft). Air-to-air missiles also have a wide range of sizes, ranging from helicopter-launched self-defense weapons with a range of a few kilometers, to long-range weapons designed for interceptor aircraft such as the R-37 (missile).
Anti-satellite.
In the 1950s and 1960s, Soviet designers started work on an anti-satellite weapon, called the Istrebitel Sputnik, which literally means "interceptor of satellites" or "destroyer of satellites". After a lengthy development process of roughly twenty years, it was finally decided that testing of the Istrebitel Sputnik be canceled. This was when the U.S. started testing their own systems. The Brilliant Pebbles defense system proposed during the 1980s would have used kinetic energy collisions without explosives. Anti-satellite weapons may be launched either by an aircraft or a surface platform, depending on the design. To date, only a few known tests have occurred.

</doc>
<doc id="19595" url="http://en.wikipedia.org/wiki?curid=19595" title="Mendelian inheritance">
Mendelian inheritance

Mendelian inheritance is inheritance of biological features that follows the laws proposed by Gregor Johann Mendel in 1865 and 1866 and re-discovered in 1900. It was initially very controversial. When Mendel's theories were integrated with the chromosome theory of inheritance by Thomas Hunt Morgan in 1915, they became the core of classical genetics.
History.
The laws of inheritance were derived by Gregor Mendel, a nineteenth-century Austrian monk conducting hybridization experiments in garden peas ("Pisum sativum") he planted in the backyard of the church. Between 1856 and 1863, he cultivated and tested some 5,000 pea plants. From these experiments, he induced two generalizations which later became known as "Mendel's Principles of Heredity" or "Mendelian inheritance". He described these principles in a two-part paper, "Versuche über Pflanzen-Hybriden" ("Experiments on Plant Hybridization"), that he read to the Natural History Society of Brno on February 8 and March 8, 1865, and which was published in 1866.
Mendel's conclusions were largely ignored. Although they were not completely unknown to biologists of the time, they were not seen as generally applicable, even by Mendel himself, who thought they only applied to certain categories of species or traits. A major block to understanding their significance was the importance attached by 19th-century biologists to the apparent blending of inherited traits in the overall appearance of the progeny, now known to be due to multigene interactions, in contrast to the organ-specific binary characters studied by Mendel. In 1900, however, his work was "re-discovered" by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak. The exact nature of the "re-discovery" has been somewhat debated: De Vries published first on the subject, mentioning Mendel in a footnote, while Correns pointed out Mendel's priority after having read De Vries' paper and realizing that he himself did not have priority. De Vries may not have acknowledged truthfully how much of his knowledge of the laws came from his own work, or came only after reading Mendel's paper. Later scholars have accused Von Tschermak of not truly understanding the results at all.
Regardless, the "re-discovery" made Mendelism an important but controversial theory. Its most vigorous promoter in Europe was William Bateson, who coined the terms "genetics" and "allele" to describe many of its tenets. The model of heredity was highly contested by other biologists because it implied that heredity was discontinuous, in opposition to the apparently continuous variation observable for many traits. Many biologists also dismissed the theory because they were not sure it would apply to all species. However, later work by biologists and statisticians such as R. A. Fisher showed that if multiple Mendelian factors were involved in the expression of an individual trait, they could produce the diverse results observed. Thomas Hunt Morgan and his assistants later integrated the theoretical model of Mendel with the chromosome theory of inheritance, in which the chromosomes of cells were thought to hold the actual hereditary material, and created what is now known as classical genetics, which was extremely successful and cemented Mendel's place in history.
Mendel's findings allowed other scientists to predict the expression of traits on the basis of mathematical probabilities. A large contribution to Mendel's success can be traced to his decision to start his crosses only with plants he demonstrated were true-breeding. He also only measured absolute (binary) characteristics, such as color, shape, and position of the offspring, rather than quantitative characteristics. He expressed his results numerically and subjected them to statistical analysis. His method of data analysis and his large sample size gave credibility to his data. He also had the foresight to follow several successive generations (f2, f3) of pea plants and record their variations. Finally, he performed "test crosses" (back-crossing descendants of the initial hybridization to the initial true-breeding lines) to reveal the presence and proportion of recessive characters. 
Mendel's laws.
Mendel discovered that, when he crossed purebred white flower and purple flower pea plants (the parental or P generation), the result was not a blend. Rather than being a mix of the two, the offspring (known as the F1 generation) was purple-flowered. When Mendel self-fertilized the F1 generation pea plants, he obtained a purple flower to white flower ratio in the F2 generation of 3 to 1. The results of this cross are tabulated in the Punnett square to the right.
He then conceived the idea of heredity units, which he called "factors". Mendel found that there are alternative forms of factors—now called genes—that account for variations in inherited characteristics. For example, the gene for flower color in pea plants exists in two forms, one for purple and the other for white. The alternative "forms" are now called alleles. For each biological trait, an organism inherits two alleles, one from each parent. These alleles may be the same or different. An organism that has two identical alleles for a gene is said to be homozygous for that gene (and is called a homozygote). An organism that has two different alleles for a gene is said be heterozygous for that gene (and is called a heterozygote).
Mendel also hypothesized that allele pairs separate randomly, or segregate, from each other during the production of gametes: egg and sperm. Because allele pairs separate during gamete production, a sperm or egg carries only one allele for each inherited trait. When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. This is called the Law of Segregation. Mendel also found that each pair of alleles segregates independently of the other pairs of alleles during gamete formation. This is known as the Law of Independent Assortment. 
The genotype of an individual is made up of the many alleles it possesses. An individual's physical appearance, or phenotype, is determined by its alleles as well as by its environment. The presence of an allele does not mean that the trait will be expressed in the individual that possesses it. If the two alleles of an inherited pair differ (the heterozygous condition), then one determines the organism’s appearance and is called the dominant allele; the other has no noticeable effect on the organism’s appearance and is called the recessive allele. Thus, in the example above dominant purple flower allele will hide the phenotypic effects of the recessive white flower allele. This is known as the Law of Dominance but it is not a transmission law, dominance has to do with the expression of the genotype and not its transmission. The upper case letters are used to represent dominant alleles whereas the lowercase letters are used to represent recessive alleles.
In the pea plant example above, the capital "P" represents the dominant allele for purple flowers and lowercase "p" represents the recessive allele for white flowers. Both parental plants were true-breeding, and one parental variety had two alleles for purple flowers ("PP") while the other had two alleles for white flowers ("pp"). As a result of fertilization, the F1 hybrids each inherited one allele for purple flowers and one for white. All the F1 hybrids ("Pp") had purple flowers, because the dominant "P" allele has its full effect in the heterozygote, while the recessive "p" allele has no effect on flower color. For the F2 plants, the ratio of plants with purple flowers to those with white flowers (3:1) is called the phenotypic ratio. The genotypic ratio, as seen in the Punnnett square, is 1 "PP" : 2 "Pp" : 1 "pp".
Law of Segregation (the "First Law").
The Law of Segregation states that every individual contains a pair of alleles for each particular trait which segregate or separate during cell division (assuming diploidy) for any particular trait and that each parent passes a randomly selected copy (allele) to its offspring. The offspring then receives its own pair of alleles of the gene for that trait by inheriting sets of homologous chromosomes from the parent organisms. Interactions between alleles at a single locus are termed dominance and these influence how the offspring expresses that trait (e.g. the color and height of a plant, or the color of an animal's fur). Book definition: The Law of Segregation states that the two alleles for a heritable character segregate (separate from each other) during gamete formation and end up in different gametes.*
More precisely, the law states that when any individual produces gametes, the copies of a gene separate so that each gamete receives only one copy (allele). A gamete will receive one allele or the other. The direct proof of this was later found following the observation of meiosis by two independent scientists, the German botanist Oscar Hertwig in 1876, and the Belgian zoologist Edouard Van Beneden in 1883. Paternal and maternal chromosomes get separated in meiosis and the alleles with the traits of a character are segregated into two different gametes. Each parent contributes a single gamete, and thus a single, randomly successful allele copy to their offspring and fertilization.
Law of Independent Assortment (the "Second Law").
 In short it states that: In the inheritance of more than one pair of traits in a cross simultaneously, the factor responsible for each pair of traits are distributed to the gametes.
The Law of Independent Assortment, also known as "Inheritance Law", states that separate genes for separate traits are passed independently of one another from parents to offspring. That is, the biological selection of a particular gene in the gene pair for one trait to be passed to the offspring has nothing to do with the selection of the gene for any other trait. More precisely, the law states that alleles of different genes assort independently of one another during gamete formation. While Mendel's experiments with mixing one trait always resulted in a 3:1 ratio (Fig. 1) between dominant and recessive phenotypes, his experiments with mixing two traits (dihybrid cross) showed 9:3:3:1 ratios (Fig. 2). But the 9:3:3:1 table shows that each of the two genes is independently inherited with a 3:1 phenotypic ratio. Mendel concluded that different traits are inherited independently of each other, so that there is no relation, for example, between a cat's color and tail length. This is actually only true for genes that are not linked to each other.
Independent assortment occurs in eukaryotic organisms during meiotic metaphase I, and produces a gamete with a mixture of the organism's chromosomes. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent chromosome along the metaphase plate with respect to the other bivalent chromosomes. Along with crossing over, independent assortment increases genetic diversity by producing novel genetic combinations.
Of the 46 chromosomes in a normal diploid human cell, half are maternally derived (from the mother's egg) and half are paternally derived (from the father's sperm). This occurs as sexual reproduction involves the fusion of two haploid gametes (the egg and sperm) to produce a new organism having the full complement of chromosomes. During gametogenesis—the production of new gametes by an adult—the normal complement of 46 chromosomes needs to be halved to 23 to ensure that the resulting haploid gamete can join with another gamete to produce a diploid organism. An error in the number of chromosomes, such as those caused by a diploid gamete joining with a haploid gamete, is termed aneuploidy.
In independent assortment, the chromosomes that result are randomly sorted from all possible combinations of maternal and paternal chromosomes. Because gametes end up with a random mix instead of a pre-defined "set" from either parent, gametes are therefore considered assorted independently. As such, the gamete can end up with any combination of paternal or maternal chromosomes. Any of the possible combinations of gametes formed from maternal and paternal chromosomes will occur with equal frequency. For human gametes, with 23 pairs of chromosomes, the number of possibilities is 223 or 8,388,608 possible combinations. The gametes will normally end up with 23 chromosomes, but the origin of any particular one will be randomly selected from paternal or maternal chromosomes. This contributes to the genetic variability of progeny.
Law of Dominance (the "Third Law").
Mendel's Law of Dominance states that recessive alleles will always be masked by dominant alleles. Therefore, a cross between a homozygous dominant and a homozygous recessive will always express the dominant phenotype, while still having a heterozygous genotype.
Law of Dominance can be explained easily with the help of a mono hybrid cross experiment:-
In a cross between two organisms pure for any pair (or pairs) of contrasting traits (characters), the character that appears in the F1 generation is called "dominant" and the one which is suppressed (not expressed) is called "recessive."
Each character is controlled by a pair of dissimilar factors. Only one of the characters expresses. The one which expresses in the F1 generation is called Dominant.
It is important to note however, that the law of dominance is significant and true but is not universally applicable.
According to the latest revisions, only two of these rules are considered to be laws. The third one is considered as a basic principle but not a genetic law of Mendel.
Mendelian trait.
A Mendelian trait is one that is controlled by a single locus in an inheritance pattern. In such cases, a mutation in a single gene can cause a disease that is inherited according to Mendel's laws. Examples include sickle-cell anemia, Tay-Sachs disease, cystic fibrosis and xeroderma pigmentosa. A disease controlled by a single gene contrasts with a multi-factorial disease, like arthritis, which is affected by several loci (and the environment) as well as those diseases inherited in a non-Mendelian fashion.
Non-Mendelian inheritance.
Mendel explained inheritance in terms of discrete factors—genes—that are passed along from generation to generation according to the rules of probability. Mendel's laws are valid for all sexually reproducing organisms, including garden peas and human beings. However, Mendel's laws stop short of explaining some patterns of genetic inheritance. For most sexually reproducing organisms, cases where Mendel's laws can strictly account for the patterns of inheritance are relatively rare. Often, the inheritance patterns are more complex.
The F1 offspring of Mendel's pea crosses always looked like one of the two parental varieties. In this situation of "complete dominance," the dominant allele had the same phenotypic effect whether present in one or two copies. But for some characteristics, the F1 hybrids have an appearance "in between" the phenotypes of the two parental varieties. A cross between two four o'clock ("Mirabilis jalapa") plants shows this common exception to Mendel's principles. Some alleles are neither dominant nor recessive. The F1 generation produced by a cross between red-flowered (RR) and white flowered (WW) "Mirabilis jalapa" plants consists of pink-colored flowers (RW). Which allele is dominant in this case? Neither one. This third phenotype results from flowers of the heterzygote having less red pigment than the red homozygotes. Cases in which one allele is not completely dominant over another are called incomplete dominance. In incomplete dominance, the heterozygous phenotype lies somewhere between the two homozygous phenotypes. 
A similar situation arises from codominance, in which the phenotypes produced by both alleles are clearly expressed. For example, in certain varieties of chicken, the allele for black feathers is codominant with the allele for white feathers. Heterozygous chickens have a color described as "erminette," speckled with black and white feathers. Unlike the blending of red and white colors in heterozygous four o'clocks, black and white colors appear separately in chickens. Many human genes, including one for a protein that controls cholesterol levels in the blood, show codominance, too. People with the heterozygous form of this gene produce two different forms of the protein, each with a different effect on cholesterol levels. 
In Mendelian inheritance, genes have only two alleles, such as "a" and "A". In nature, such genes exist in several different forms and are therefore said to have multiple alleles. A gene with more than two alleles is said to have multiple alleles. An individual, of course, usually has only two copies of each gene, but many different alleles are often found within a population. One of the best-known examples is coat color in rabbits. A rabbit's coat color is determined by a single gene that has at least four different alleles. The four known alleles display a pattern of simple dominance that can produce four coat colors. Many other genes have multiple alleles, including the human genes for ABO blood type. 
Furthermore, many traits are produced by the interaction of several genes. Traits controlled by two or more genes are said to be polygenic traits. "Polygenic" means "many genes." For example, at least three genes are involved in making the reddish-brown pigment in the eyes of fruit flies. Polygenic traits often show a wide range of phenotypes. The variety of skin color in humans comes about partly because more than four different genes probably control this trait.

</doc>
<doc id="19597" url="http://en.wikipedia.org/wiki?curid=19597" title="Machinima">
Machinima

Machinima ( or ) is the use of real-time computer graphics engines to create a cinematic production. Most often video games are used to generate the computer animation. Machinima-based artists, sometimes called machinimists or machinimators, are often fan laborers, by virtue of their re-use of copyrighted materials (see below). Machinima offers to provide an archive of gaming performance and access to the look and feel of software and hardware that may already have become unavailable or even obsolete; for game studies, "machinima’s gestures grant access to gaming’s historical conditions of possibility and how machinima offers links to a comparative horizon that informs, changes, and fully participates in videogame culture."
The practice of using graphics engines from video games arose from the animated software introductions of the 1980s demoscene, Disney Interactive Studios' 1992 video game "Stunt Island", and 1990s recordings of gameplay in first-person shooter (FPS) video games, such as id Software's "Doom" and "Quake". Originally, these recordings documented speedruns—attempts to complete a level as quickly as possible—and multiplayer matches. The addition of storylines to these films created ""Quake" movies". The more general term "machinima", a portmanteau of "machine cinema", arose when the concept spread beyond the "Quake" series to other games and software. After this generalization, machinima appeared in mainstream media, including television series and advertisements.
Machinima has advantages and disadvantages when compared to other styles of filmmaking. Its relative simplicity over traditional frame-based animation limits control and range of expression. Its real-time nature favors speed, cost saving, and flexibility over the higher quality of pre-rendered computer animation. Virtual acting is less expensive, dangerous, and physically restricted than live action. Machinima can be filmed by relying on in-game artificial intelligence (AI) or by controlling characters and cameras through digital puppetry. Scenes can be precisely scripted, and can be manipulated during post-production using video editing techniques. Editing, custom software, and creative cinematography may address technical limitations. Game companies have provided software for and have encouraged machinima, but the widespread use of digital assets from copyrighted games has resulted in complex, unresolved legal issues.
Machinima productions can remain close to their gaming roots and feature stunts or other portrayals of gameplay. Popular genres include dance videos, comedy, and drama. Alternatively, some filmmakers attempt to stretch the boundaries of the rendering engines or to mask the original 3-D context. The Academy of Machinima Arts & Sciences (AMAS), a non-profit organization dedicated to promoting machinima, recognizes exemplary productions through Mackie awards given at its annual Machinima Film Festival. Some general film festivals accept machinima, and game companies, such as Epic Games, Blizzard Entertainment and Jagex, have sponsored contests involving it.
History.
Precedent.
1980s software crackers added custom introductory credits sequences (intros) to programs whose copy protection they had removed. Increasing computing power allowed for more complex intros, and the demoscene formed when focus shifted to the intros instead of the cracks. The goal became to create the best 3-D demos in real-time with the least amount of software code. Disk storage was too slow for this; graphics had to be calculated on the fly and without a pre-existing game engine.
In Disney Interactive Studios' 1992 computer game "Stunt Island", users could stage, record, and play back stunts; as Nitsche stated, the game's goal was "not ... a high score but a spectacle." Released the following year, id Software's "Doom" included the ability to record gameplay as sequences of events that the game engine could later replay in real-time. Because events and not video frames were saved, the resulting game demo files were small and easily shared among players. A culture of recording gameplay developed, as Henry Lowood of Stanford University called it, "a context for spectatorship... The result was nothing less than a metamorphosis of the player into a performer." Another important feature of "Doom" was that it allowed players to create their own modifications, maps, and software for the game, thus expanding the concept of game authorship. In machinima, there is a dual register of gestures: the trained motions of the player determine the in-game images of expressive motion.
"Doom"‍ '​s 1996 successor, "Quake", offered new opportunities for both gameplay and customization, while retaining the ability to record demos. Multiplayer games became popular, almost a sport; demos of matches between teams of players (clans) were recorded and studied. Paul Marino, executive director of the AMAS, stated that deathmatches, a type of multiplayer game, became more "cinematic". At this point, however, they still documented gameplay without a narrative.
"Quake" movies.
On October 26, 1996, a well-known gaming clan, the Rangers, surprised the "Quake" community with "Diary of a Camper", the first widely known machinima film. This short, 100-second demo file contained the action and gore of many others, but in the context of a brief story, rather than the usual deathmatch. An example of transformative or emergent gameplay, this shift from competition to theater required both expertise in and subversion of the game's mechanics. The Ranger demo emphasized this transformation by retaining specific gameplay references in its story.
"Diary of a Camper" inspired many other ""Quake" movies," as these films were then called. A community of game modifiers (modders), artists, expert players, and film fans began to form around them. The works were distributed and reviewed on websites such as The Cineplex, Psyk's Popcorn Jungle, and the Quake Movie Library (QML). Production was supported by dedicated demo-processing software, such as Uwe Girlich's Little Movie Processing Center (LMPC) and David "crt" Wright's non-linear editor Keygrip; the latter became known as "Adobe Premiere for Quake demo files". Among the notable films were Clan Phantasm's "Devil's Covenant", the first feature-length "Quake" movie; Avatar and Wendigo's "Blahbalicious", which the QML awarded seven Quake Movie Oscars; and Clan Undead's "Operation Bayshield", which introduced simulated lip synchronization and featured customized digital assets.
Released in December 1997, id Software's "Quake II" improved support for user-created 3-D models. However, without compatible editing software, filmmakers continued to create works based on the original "Quake"; these included the ILL Clan's "Apartment Huntin"' and the Quake done Quick group's "Scourge Done Slick". "Quake II" demo editors became available in 1998; in particular, Keygrip 2.0 introduced "recamming", the ability to adjust camera locations after recording. Paul Marino called the addition of this feature "a defining moment for [m]achinima". With "Quake II" filming now feasible, Strange Company's 1999 production "Eschaton: Nightfall" was the first work to feature entirely custom-made character models.
The December 1999 release of id's "Quake III Arena" posed a problem to the "Quake" movie community. The game's demo file included information needed for computer networking; however, to prevent cheating, id warned of legal action for dissemination of the file format. Thus, it was impractical to enhance software to work with "Quake III". Concurrently, the novelty of "Quake" movies was waning. New productions appeared less frequently, and, according to Marino, the community needed to "reinvent itself" to offset this development.
"Borg War", a 90-minute animated Star Trek fan film, was produced using Elite Force 2 (a "Quake III" variant) and Starfleet Command 3, repurposing the games' voiceover clips to create a new plot. "Borg War" was nominated for two "Mackie" awards by the Academy of Machinima Arts & Sciences. An August 2007 screening at a "Star Trek" convention in Las Vegas was the first time that CBS/Paramount had approved the screening of a non-parody fan film at a licensed convention.
Generalization.
In January 2000, Hugh Hancock, the founder of Strange Company, launched a new website, machinima.com. The new name surprised the community; a misspelled contraction of "machine cinema" ("machinema"), the term "machinima" was intended to dissociate in-game filming from a specific engine. The misspelling stuck because it also referenced anime. The new site featured tutorials, interviews, articles, and the exclusive release of Tritin Films' "Quad God". The first film made with "Quake III Arena", "Quad God" was also the first to be distributed as recorded video frames, not game-specific instructions. This change was initially controversial among machinima producers who preferred the smaller size of demo files. However, demo files required a copy of the game to view. The more accessible traditional video format broadened "Quad God"‍ '​s viewership, and the work was distributed on CDs bundled with magazines. Thus, id's decision to protect "Quake III"‍ '​s code inadvertently caused machinima creators to use more general solutions and thus widen their audience. Within a few years, machinima films were almost exclusively distributed in common video file formats.
Machinima began to receive mainstream notice. Roger Ebert discussed it in a June 2000 article and praised Strange Company's machinima setting of Percy Bysshe Shelley's sonnet "Ozymandias". At Showtime Network's 2001 Alternative Media Festival, the ILL Clan's 2000 machinima film "Hardly Workin won the Best Experimental and Best in SHO awards. Steven Spielberg used "Unreal Tournament" to test special effects while working on his 2001 film ' Eventually, interest spread to game developers. In July 2001, Epic Games announced that its upcoming game "Unreal Tournament 2003" would include Matinee, a machinima production software utility. As involvement increased, filmmakers released fewer new productions to focus on quality.
At the March 2002 Game Developers Conference, five machinima makers—Anthony Bailey, Hugh Hancock, Katherine Anna Kang, Paul Marino, and Matthew Ross—founded the AMAS, a non-profit organization dedicated to promoting machinima. At QuakeCon in August, the new organization held the first Machinima Film Festival, which received mainstream media coverage. ', by Jake Hughes and Tom Hall, won three awards, including Best Picture. The next year, "In the Waiting Line", directed by Tommy Pallotta and animated by Randy Cole, utilizing Fountainhead Entertainment's Machinimation tools, it became the first machinima music video to air on MTV. As graphics technology improved, machinima filmmakers used other video games and consumer-grade video editing software. Using Bungie's 2001 game ', Rooster Teeth Productions created a popular comedy series "Red vs. Blue: The Blood Gulch Chronicles". The series' second season premiered at the Lincoln Center for the Performing Arts in 2004.
Mainstream appearances.
Machinima has appeared on television, starting with G4's series "Portal". In the BBC series "Time Commanders", players re-enacted historic battles using Creative Assembly's real-time game "". MTV2's "Video Mods" re-creates music videos using characters from video games such as "The Sims 2", "BloodRayne", and "Tribes". Blizzard Entertainment helped to set part of "Make Love, Not Warcraft", an Emmy Award–winning 2006 episode of the comedy series "South Park", in its massively multiplayer online role-playing game (MMORPG) "World of Warcraft". By purchasing broadcast rights to Douglas Gayeton's machinima documentary "Molotov Alva and His Search for the Creator" in September 2007, HBO became the first television network to buy a work created completely in a virtual world. In December 2008, machinima.com signed fifteen experienced television comedy writers—including Patric Verrone, Bill Oakley, and Mike Rowe—to produce episodes for the site.
Commercial use of machinima has increased. Rooster Teeth sells DVDs of their "Red vs. Blue" series and, under sponsorship from Electronic Arts, helped to promote "The Sims 2" by using the game to make a machinima series, "The Strangerhood". Volvo Cars sponsored the creation of a 2004 advertisement, "", the first film to combine machinima and live action. Later, Electronic Arts commissioned Rooster Teeth to promote their "Madden NFL 07" video game. Blockhouse TV uses Moviestorm's machinima software to produce its pre-school educational DVD series "Jack and Holly"
Game developers have continued to increase support for machinima. Products such as Lionhead Studios' 2005 business simulation game "The Movies", Linden Research's virtual world "Second Life", and Bungie's 2007 first-person shooter "Halo 3" encourage the creation of user content by including machinima software tools. Using "The Movies", Alex Chan, a French resident with no previous filmmaking experience, took four days to create "The French Democracy", a short political film about the 2005 civil unrest in France. Third-party mods like "Garry's Mod" usually offer the ability to manipulate characters and take advantage of custom or migrated content, allowing for the creation of works like "Counter-Strike For Kids" that can be filmed using multiple games.
In a 2010 interview with PC Magazine, Valve CEO and co-founder Gabe Newell said that they wanted to make a "Half-Life" feature film themselves, rather than hand it off to a big-name director like Sam Raimi, and that their recent "Team Fortress 2" "Meet The Team" machinima shorts were experiments in doing just that. Two years later, Valve released their proprietary non-linear machinima software, Source Filmmaker.
Production.
Comparison to film techniques.
The AMAS defines machinima as "animated filmmaking within a real-time virtual 3-D environment". In other 3-D animation methods, creators can control every frame and nuance of their characters but, in turn, must consider issues such as key frames and inbetweening. Machinima creators leave many rendering details to their host environments, but may thus inherit those environments' limitations. Because game animations focus on dramatic rather than casual actions, the range of character emotions is often limited. However, Kelland, Morris, and Lloyd state that a small range of emotions is often sufficient, as in successful Japanese anime television series.
Another difference is that machinima is created in real time, but other animation is pre-rendered. Real-time engines need to trade quality for speed and use simpler algorithms and models. In the 2001 animated film "", every strand of hair on a character's head was independent; real-time needs would likely force them to be treated as a single unit. Kelland, Morris, and Lloyd argue that improvement in consumer-grade graphics technology will allow more realism; similarly, Paul Marino connects machinima to the increasing computing power predicted by Moore's Law. For cut scenes in video games, issues other than visual fidelity arise. Pre-rendered scenes can require more digital storage space, weaken suspension of disbelief through contrast with real-time animation of normal gameplay, and limit interaction.
Like live action, machinima is recorded in real-time, and real people can act and control the camera. Filmmakers are often encouraged to follow traditional cinematic conventions, such as avoiding wide fields of view, the overuse of slow motion, and errors in visual continuity. Unlike live action, machinima involves less expensive, digital special effects and sets, possibly with a science-fiction or historical theme. Explosions and stunts can be tried and repeated without monetary cost and risk of injury, and the host environment may allow unrealistic physical constraints. University of Cambridge experiments in 2002 and 2003 attempted to use machinima to re-create a scene from the 1942 live-action film "Casablanca". Machinima filming differed from traditional cinematography in that character expression was limited, but camera movements were more flexible and improvised. Nitsche compared this experiment to an unpredictable Dogme 95 production.
Berkeley sees machinima as "a strangely hybrid form, looking forwards and backwards, cutting edge and conservative at the same time". Machinima is a digital medium based on 3-D computer games, but most works have a linear narrative structure. Some, such as "Red vs. Blue" and "The Strangerhood", follow narrative conventions of television situational comedy. Nitsche agrees that pre-recorded ("reel") machinima tends to be linear and offers limited interactive storytelling; he sees more opportunities in machinima performed live and with audience interaction. In creating their improvisational comedy series "On the Campaign Trail with Larry & Lenny Lumberjack" and talk show "Tra5hTa1k with ILL Will", the ILL Clan blended real and virtual performance by creating the works on-stage and interacting with a live audience. In another combination of real and virtual worlds, Chris Burke's talk show "This Spartan Life" takes place in "Halo 2"‍ '​s open multiplayer environment. There, others playing in earnest may attack the host or his interviewee. Although other virtual theatrical performances have taken place in chat rooms and multi-user dungeons, machinima adds "cinematic camera work". Previously, such virtual cinematic performances with live audience interaction were confined to research labs equipped with powerful computers.
Machinima can be less expensive than other forms of filmmaking. Strange Company produced its feature-length machinima film "BloodSpell" for less than £10,000. Before using machinima, Burnie Burns and Matt Hullum of Rooster Teeth Productions spent US$9,000 to produce a live-action independent film; in contrast, the four Xbox game consoles used to make "Red vs. Blue" in 2005 cost $600. The low cost caused a product manager for Electronic Arts to compare machinima to the low-budget independent film "The Blair Witch Project", without the need for cameras and actors. Because these are seen as low barriers to entry, machinima has been called a "democratization of filmmaking". Berkeley weighs increased participation and a blurred line between producer and consumer against concerns that game copyrights limit commercialization and growth of machinima.
Comparatively, machinimists using pre-made virtual platforms like Second Life have indicated that their productions can be made quite successfully with no cost at all. Creators like Dutch director Chantal Harvey, producer of the 48 Hour Film Project Machinima sector, have created upwards of 200 films using the platform. Harvey's advocacy of the genre has resulted in the involvement of film director Peter Greenaway who served as a juror for the Machinima category and gave a keynote speech during the event.
Character and camera control.
Kelland, Morris, and Lloyd list four main methods of creating machinima. From simple to advanced, these are: relying on the game's AI to control most actions, digital puppetry, recamming, and precise scripting of actions. Although simple to produce, AI-dependent results are unpredictable, thus complicating the realization of a preconceived film script. For example, when Rooster Teeth produced "The Strangerhood" using "The Sims 2", a game that encourages the use of its AI, the group had to create multiple instances of each character to accommodate different moods. Individual instances were selected at different times to produce appropriate actions.
In digital puppetry, machinima creators become virtual actors; each crew member controls a character in real-time, as in a multiplayer game. The director can use built-in camera controls, if available. Otherwise, video is captured from the perspectives of one or more puppeteers who serve as camera operators. Puppetry allows for improvisation and offers controls familiar to gamers, but requires more personnel than the other methods and is less precise than scripted recordings. However, some games, such as the "Halo" series, (except for Halo PC and Custom Edition, which allow AI and custom objects and characters), allow filming only through puppetry. According to Marino, other disadvantages are the possibility of disruption when filming in an open multi-user environment and the temptation for puppeteers to play the game in earnest, littering the set with blood and dead bodies. However, Chris Burke intentionally hosts "This Spartan Life" in these unpredictable conditions, which are fundamental to the show. Other works filmed using puppetry are the ILL Clan's improvisational comedy series "On the Campaign Trail with Larry & Lenny Lumberjack" and Rooster Teeth Productions' "Red vs. Blue". In recamming, which builds on puppetry, actions are first recorded to a game engine's demo file format, not directly as video frames. Without re-enacting scenes, artists can then manipulate the demo files to add cameras, tweak timing and lighting, and change the surroundings. This technique is limited to the few engines and software tools that support it.
A technique common in cut scenes of video games, scripting consists of giving precise directions to the game engine. A filmmaker can work alone this way, as J. Thaddeus "Mindcrime" Skubis did in creating the nearly four-hour "The Seal of Nehahra" (2000), the longest work of machinima at the time. However, perfecting scripts can be time-consuming. Unless what-you-see-is-what-you-get (WYSIWYG) editing is available, as in "", changes may need to be verified in additional runs, and non-linear editing may be difficult. In this respect, Kelland, Morris, and Lloyd compare scripting to stop-motion animation. Another disadvantage is that, depending on the game, scripting capabilities may be limited or unavailable. Matinee, a machinima software tool included with "Unreal Tournament 2004", popularized scripting in machinima.
Limitations and solutions.
When "Diary of a Camper" was created, no software tools existed to edit demo files into films. Rangers clan member Eric "ArchV" Fowler wrote his own programs to reposition the camera and to splice footage from the "Quake" demo file. "Quake" movie editing software later appeared, but the use of conventional non-linear video editing software is now common. For example, Phil South inserted single, completely white frames into his work "No Licence" to enhance the visual impact of explosions. In the post-production of "", Rooster Teeth Productions added letterboxing with Adobe Premiere Pro to hide the camera player's head-up display.
Machinima creators have used different methods to handle limited character expression. The most typical ways that amateur-style machinima gets around limitations of expression include taking advantage of speech bubbles seen above players' heads when speaking, relying on the visual matching between a character's voice and appearance, and finding methods available within the game itself. "Garry's Mod" and Source Filmmaker include the ability to manipulate characters and objects in real-time, though the former relies on community addons to take advantage of certain engine features, and the latter renders scenes using non-real-time effects. In the "Halo" video game series, helmets completely cover the characters' faces. To prevent confusion, Rooster Teeth's characters move slightly when speaking, a convention shared with anime. Some machinima creators use custom software. For example, Strange Company uses Take Over GL Face Skins to add more facial expressions to their characters filmed in BioWare's 2002 role-playing video game "Neverwinter Nights". Similarly, Atussa Simon used a "library of faces" for characters in "The Battle of Xerxes". In some cases, some game companies may provide such software; examples include Epic Games' Impersonator for "Unreal Tournament 2004" and Valve Corporation's Faceposer for Source games. Another solution is to blend in non-machinima elements, as nGame did by inserting painted characters with more expressive faces into its 1999 film "Berlin Assassins". It may be possible to point the camera elsewhere or employ other creative cinematography or acting. For example, Tristan Pope combined creative character and camera positioning with video editing to suggest sexual actions in his controversial film "Not Just Another Love Story".
Legal issues.
New machinima filmmakers often want to use game-provided digital assets, but doing so raises legal issues. As derivative works, their films could violate copyright or be controlled by the assets' copyright holder, an arrangement that can be complicated by separate publishing and licensing rights. The software license agreement for "The Movies" stipulates that Activision, the game's publisher, owns "any and all content within... Game Movies that was either supplied with the Program or otherwise made available... by Activision or its licensors..." Some game companies provide software to modify their own games, and machinima makers often cite fair use as a defense, but the issue has never been tested in court. A potential problem with this defense is that many works, such as "Red vs. Blue", focus more on satire, which is not as explicitly protected by fair use as parody. Berkeley adds that, even if machinima artists use their own assets, their works could be ruled derivative if filmed in a proprietary engine. The risk inherent in a fair-use defense would cause most machinima artists simply to yield to a cease-and-desist order. The AMAS has attempted to negotiate solutions with video game companies, arguing that an open-source or reasonably priced alternative would emerge from an unfavorable situation. Unlike "The Movies", some dedicated machinima software programs, such as Reallusion's iClone, have licenses that avoid claiming ownership of users' films featuring bundled assets.
Generally, companies want to retain creative control over their intellectual properties and are wary of fan-created works, like fan fiction. However, because machinima provides free marketing, they have avoided a response demanding strict copyright enforcement. In 2003, Linden Lab was praised for changing license terms to allow users to retain ownership of works created in its virtual world "Second Life". Rooster Teeth initially tried to release "Red vs. Blue" unnoticed by "Halo"‍ '​s owners because they feared that any communication would force them to end the project. However, Microsoft, Bungie's parent company at the time, contacted the group shortly after episode 2, and allowed them to continue without paying licensing fees.
A case in which developer control was asserted involved Blizzard Entertainment's action against Tristan Pope's "Not Just Another Love Story". Blizzard's community managers encouraged users to post game movies and screenshots, but viewers complained that Pope's suggestion of sexual actions through creative camera and character positioning was pornographic. Citing the user license agreement, Blizzard closed discussion threads about the film and prohibited links to it. Although Pope accepted Blizzard's right to some control, he remained concerned about censorship of material that already existed in-game in some form. Discussion ensued about boundaries between MMORPG player and developer control. Lowood asserted that this controversy demonstrated that machinima could be a medium of negotiation for players.
Microsoft and Blizzard.
In August 2007, Microsoft issued its Game Content Usage Rules, a license intended to address the legal status of machinima based on its games, including the "Halo" series. Microsoft intended the rules to be "flexible", and, because it was unilateral, the license was legally unable to reduce rights. However, machinima artists, such as Edgeworks Entertainment, protested the prohibitions on extending Microsoft's fictional universes (a common component of fan fiction) and on selling anything from sites hosting derivative works. Compounding the reaction was the license's statement, "If you do any of these things, you can expect to hear from Microsoft's lawyers who will tell you that you have to stop distributing your items right away."
Surprised by the negative feedback, Microsoft revised and reissued the license after discussion with Hugh Hancock and an attorney for the Electronic Frontier Foundation. The rules allow noncommercial use and distribution of works derived from Microsoft-owned game content, except audio effects and soundtracks. The license prohibits reverse engineering and material that is pornographic or otherwise "objectionable". On distribution, derivative works that elaborate on a game's fictional universe or story are automatically licensed to Microsoft and its business partners. This prevents legal problems if a fan and Microsoft independently conceive similar plots.
A few weeks later, Blizzard Entertainment posted on WorldofWarcraft.com their "Letter to the Machinimators of the World", a license for noncommercial use of game content. It differs from Microsoft's declaration in that it addresses machinima specifically instead of general game-derived content, allows use of game audio if Blizzard can legally license it, requires derivative material to meet the Entertainment Software Rating Board's Teen content rating guideline, defines noncommercial use differently, and does not address extensions of fictional universes.
Hayes states that, although licensees' benefits are limited, the licenses reduce reliance on fair use regarding machinima. In turn, this recognition may reduce film festivals' concerns about copyright clearance; in an earlier analogous situation, festivals were concerned about documentary films until best practices for them were developed. According to Hayes, Microsoft and Blizzard helped themselves through their licenses because fan creations provide free publicity and are unlikely to harm sales. If the companies had instead sued for copyright infringement, defendants could have claimed estoppel or implied license because machinima had been unaddressed for a long time. Thus, these licenses secured their issuers' legal rights. Even though other companies, such as Electronic Arts, have encouraged machinima, they have avoided licensing it. Because of the involved legal complexity, they may prefer to under-enforce copyrights. Hayes believes that this legal uncertainty is a suboptimal solution and that, though limited and "idiosyncratic", the Microsoft and Blizzard licenses move towards an ideal video gaming industry standard for handling derivative works.
Semiotic mode.
Just as machinima can be the cause of legal dispute in copyright ownership and illegal use, it makes heavy use of intertextuality and raises the question of authorship. Machinima takes copyrighted property (such as characters in a game engine) and repurposes it to tell a story, but another common practice in machinima-making is to retell an existing story from a different medium in that engine.
This re-appropriation of established texts, resources, and artistic properties to tell a story or make a statement is an example of a semiotic phenomenon known as intertextuality or resemiosis. A more common term for this phenomenon is “parody”, but not all of these intertextual productions are intended for humor or satire, as demonstrated by the "Few Good G-Men" video. Furthermore, the argument of how well-protected machinima is under the guise of parody or satire is still highly debated; a piece of machinima may be reliant upon a protected property, but may not necessarily be making a statement about that property. Therefore, it is more accurate to refer to it simply as resemiosis, because it takes an artistic work and presents it in a new way, form, or medium. This resemiosis can be manifested in a number of ways. The machinima-maker can be considered an author who restructures the story and/or the world that the chosen game engine is built around. In the popular web series "Red vs. Blue", most of the storyline takes place within the game engine of ' and its subsequent sequels. ' has an extensive storyline already, but "Red vs. Blue" only ever makes mention of this storyline once in the first episode. Even after over 200 episodes of the show being broadcast onto the Internet since 2003, the only real similarities that can be drawn between "Red vs. Blue" and the game-world it takes place in are the character models, props, vehicles, and settings. Yet Burnie Burns and the machinima team at Rooster Teeth created an extensive storyline of their own using these game resources.
The ability to re-appropriate a game engine to film a video demonstrates intertextuality because it is an obvious example of art being a product of creation-through-manipulation rather than creation per se. The art historian Ernst Gombrich likened art to the "manipulation of a vocabulary" and this can be demonstrated in the creation of machinima. When using a game world to create a story, the author is influenced by the engine. For example, since so many video games are built around the concept of war, a significant portion of machinima films also take place in war-like environments.
Intertextuality is further demonstrated in machinima not only in the re-appropriation of content but in artistic and communicatory techniques. Machinima by definition is a form of puppetry, and thus this new form of digital puppetry employs age-old techniques from the traditional artform. It is also, however, a form of filmmaking, and must employ filmmaking techniques such as camera angles and proper lighting. Some machinima takes place in online environments with participants, actors, and "puppeteers" working together from thousands of miles apart. This means other techniques born from long-distance communication must also be employed. Thus, techniques and practices that would normally never be used in conjunction with one another in the creation of an artistic work end up being used intertextually in the creation of machinima.
Another way that machinima demonstrates intertextuality is in its tendency to make frequent references to texts, works, and other media just like TV ads or humorous cartoons such as "The Simpsons" might do. For example, the machinima series "", created by Ross Scott is filmed by taking a recording of Scott playing through the game "Half Life" as a player normally would and combining it with a voiceover (also recorded by Scott) to emulate an inner monologue of the normally voiceless protagonist Gordon Freeman. Scott portrays Freeman as a snarky, sociopathic character who makes frequent references to works and texts including science fiction, horror films, action movies, American history, and renowned novels such as Moby Dick. These references to works outside the game, often triggered by events within the game, are prime examples of the densely intertextual nature of machinima.
Common genres.
Nitsche and Lowood describe two methods of approaching machinima: starting from a video game and seeking a medium for expression or for documenting gameplay ("inside-out"), and starting outside a game and using it merely as animation tool ("outside-in"). Kelland, Morris, and Lloyd similarly distinguish between works that retain noticeable connections to games, and those closer to traditional animation. Belonging to the former category, gameplay and stunt machinima began in 1997 with "Quake done Quick". Although not the first speedrunners, its creators used external software to manipulate camera positions after recording, which, according to Lowood, elevated speedrunning "from cyberathleticism to making movies". Stunt machinima remains popular. Kelland, Morris, and Lloyd state that "" stunt videos offer a new way to look at the game, and compare "Battlefield 1942" machinima creators to the Harlem Globetrotters. Built-in features for video editing and post-recording camera positioning in "Halo 3" were expected to facilitate gameplay-based machinima. MMORPGs and other virtual worlds have been captured in documentary films, such as "Miss Galaxies 2004", a beauty pageant that took place in the virtual world of "Star Wars Galaxies". Footage was distributed in the cover disc of the August 2004 issue of "PC Gamer". Douglas Gayeton's "Molotov Alva and His Search for the Creator" documents the title character's interactions in "Second Life".
Gaming-related comedy offers another possible entry point for new machinima producers. Presented as five-minute sketches, many machinima comedies are analogous to Internet Flash animations. After Clan Undead's 1997 work "Operation Bayshield" built on the earliest "Quake" movies by introducing narrative conventions of linear media and sketch comedy reminiscent of the television show "Saturday Night Live", the New-York-based ILL Clan further developed the genre in machinima through works including "Apartment Huntin and "Hardly Workin. "Red vs. Blue: The Blood Gulch Chronicles" chronicles a futile civil war over five seasons and 100 episodes. Marino wrote that although the series' humor was rooted in video games, strong writing and characters caused the series to "transcend the typical gamer". An example of a comedy film that targets a more general audience is Strange Company's "Tum Raider", produced for the BBC in 2004.
Machinima has been used in music videos, of which the first documented example is Ken Thain's 2002 "Rebel vs. Thug", made in collaboration with Chuck D. For this, Thain used Quake2Max, a modification of "Quake II" that provided cel-shaded animation. The following year, Tommy Pallotta directed "In the Waiting Line" for the British group Zero 7. He told "Computer Graphics World", "It probably would have been quicker to do the film in a 3D animated program. But now, we can reuse the assets in an improvisational way." Scenes of the game "Postal 2" can be seen in the music video of the Black Eyed Peas single "Where Is the Love?". In television, MTV features video game characters on its show "Video Mods". Among "World of Warcraft" players, dance and music videos became popular after dancing animations were discovered in the game.
Others use machinima in drama; these works may or may not retain signs of their video game provenance. "Unreal Tournament" is often used for science fiction and "Battlefield 1942" for war, but some artists subvert their chosen game's setting or completely detach their work from it. In 1999, Strange Company used "Quake II" in "Eschaton: Nightfall", a horror film based on the work of H. P. Lovecraft. A later example is Damien Valentine's series "Consanguinity", made using BioWare's 2002 computer game "Neverwinter Nights" and based on the television series "Buffy the Vampire Slayer". Another genre consists of experimental works that attempt to push the boundaries of game engines. One example, Fountainhead's "Anna", is a short film that focuses on the cycle of life and is reminiscent of "Fantasia". Other productions go farther and completely eschew a 3-D appearance. Friedrich Kirschner's "The Tournament" and "The Journey" deliberately appear hand-drawn, and Dead on Que's "Fake Science" resembles two-dimensional Eastern European modernist animation from the 1970s.
Another derivative genre termed macinima verite, from cinéma vérité, seeks to add a documentary and additional realism to the machinima piece. L.M. Sabo's "CATACLYSM" achieves a machinima verite style through displaying and recapturing the machinima video with a low resolution black and white hand-held video camera to produce a shaky camera effect. Other element of cinéma vérité, such as longer takes, sweeping camera transitions, and jump cuts may be included to complete the effect.
Some have used machinima to make political statements, often from left-wing perspectives. Alex Chan's take on the 2005 civil unrest in France, "The French Democracy", attained mainstream attention and inspired other machinima commentaries on American and British society. Horwatt deemed Thuyen Nguyen's 2006 "An Unfair War", a criticism of the Iraq war, similar in its attempt "to speak for those who cannot". Joshua Garrison mimicked Chan's "political pseudo-documentary style" in his "Virginia Tech Massacre", a controversial "Halo 3"–based re-enactment and explanation of the eponymous real-life events. More recently, "War of Internet Addiction" addressed internet censorship in China using "World of Warcraft".
Competitions.
After the QML's Quake Movie Oscars, dedicated machinima awards did not reappear until the AMAS created the Mackies for its first Machinima Film Festival in 2002. The annual festival has become an important one for machinima creators. Ho Chee Yue, a founder of the marketing company AKQA, helped to organize the first festival for the Asia chapter of the AMAS in 2006. In 2007, the AMAS supported the first machinima festival held in Europe. In addition to these smaller ceremonies, Hugh Hancock of Strange Company worked to add an award for machinima to the more general Bitfilm Festival in 2003. Other general festivals that allow machinima include the Sundance Film Festival and the Florida Film Festival. The Ottawa International Animation Festival opened a machinima category in 2004, but, citing the need for "a certain level of excellence", declined to award anything to the category's four entries that year.
Machinima has been showcased in contests sponsored by game companies. Epic Games' popular Make Something Unreal contest included machinima that impressed event organizer Jeff Morris because of "the quality of entries that really push the technology, that accomplish things that Epic never envisioned". In December 2005, Blizzard Entertainment and Xfire, a gaming-focused instant messaging service, jointly sponsored a "World of Warcraft" machinima contest.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="19599" url="http://en.wikipedia.org/wiki?curid=19599" title="Mutagenesis">
Mutagenesis

Mutagenesis is a process by which the genetic information of an organism is changed in a stable manner, resulting in a mutation. It may occur spontaneously in nature, or as a result of exposure to mutagens. It can also be achieved experimentally using laboratory procedures. In nature mutagenesis can lead to cancer and various heritable diseases, but it is also a driving force of evolution. Mutagenesis as a science was developed based on work done by Hermann Muller, Charlotte Auerbach and J. M. Robson in the first half of the 20th century.
Background.
DNA may be modified, either naturally or artificially, by a number of physical, chemical and biological agents, resulting in mutations. In 1927, Hermann Muller first demonstrated mutation with observable changes in the chromosomes can be caused by irradiating fruit flies with X-ray, and lent support to the idea of mutation as the cause of cancer. His contemporary Lewis Stadler also showed the mutational effect of X-ray on barley in 1928, and ultraviolet (UV) radiation on maize in 1936. In 1940s, Charlotte Auerbach and J. M. Robson, found that mustard gas can also cause mutations in fruit flies.
While changes to the chromosome caused by X-ray and mustard gas were readily observable to the early researchers, other changes to the DNA induced by other mutagens were not so easily observable, and the mechanism may be complex and takes longer to unravel. For example, soot was suggested to be a cause of cancer as early as 1775, and coal tar was demonstrated to cause cancer in 1915. The chemicals involved in both were later shown to be polycyclic aromatic hydrocarbons (PAH). PAHs by themselves are not carcinogenic, and it was proposed in 1950 that the carcinogenic forms of PAHs are the oxides produced as metabolites from cellular processes. The metabolic process was identified in 1960s as catalysis by cytochrome P450 which produces reactive species that can interact with the DNA to form adducts, the mechanism by which the PAH adducts give rise to mutation however is still under investigation.
Mammalian nuclear DNA may sustain more than 60,000 damages per cell per day, as listed with references in DNA damage (naturally occurring). If left uncorrected, these adducts, after misreplication past the damaged sites, can give rise to mutations. In nature, the mutations that arise may be beneficial or deleterious — it is the driving force of evolution, an organism may acquire new traits through genetic mutation, but mutation may also result in impaired function of the genes, and in severe cases, causing the death of the organism. In the laboratory, however, mutagenesis is a useful technique for generating mutations that allows the functions of genes and gene products to be examined in detail, producing proteins with improved characteristics or novel function, as well as mutant strains with useful properties. Initially the ability of radiation and chemical mutagens to cause mutation was exploited to generate random mutations, but later techniques were developed to introduce specific mutations.
The distinction between a mutation and a DNA damage.
DNA damage is an abnormal alteration in the structure of DNA that cannot, itself, be replicated when DNA replicates. In contrast, a mutation is a change in the nucleic acid sequence that can be replicated; and hence a mutation can be inherited from one generation to the next. Damage can occur from chemical addition (adduct), or structural disruption to a base of DNA (creating an abnormal nucleotide or nucleotide fragment); or a break in one or both DNA strands. When DNA containing a damage is replicated, an incorrect base may be inserted in the new complementary strand as it is being synthesized (see DNA repair#Translesion synthesis). The incorrect insertion in the new strand will occur opposite the damaged site in the template strand, and this incorrect insertion can become a mutation (i.e. a changed base pair) in the next round of replication. Furthermore, double-strand breaks in DNA may be repaired by an inaccurate repair process, non-homologous end joining, which produces mutations. Mutations can ordinarily be avoided if accurate DNA repair systems recognize DNA damages and repair them prior to completion of the next round of replication. At least 169 enzymes are either directly employed in DNA repair or influence DNA repair processes. Of these, 83 are directly employed in the 5 types of DNA repair processes indicated in the chart shown in the article DNA repair.
Mechanisms.
Mutagenesis may occur endogenously, for example through spontaneous hydrolysis, or through normal cellular processes that can generate reactive oxygen species and DNA adducts, or through error in replication and repair. Mutagenesis may also arise as a result of the presence of environmental mutagens that induces changes to the DNA. The mechanism by which mutation arises varies according to the causative agent, the mutagen, involved. Most mutagens act either directly, or indirectly via mutagenic metabolites, on the DNA producing lesions. Some however may affect the replication or chromosomal partition mechanism, and other cellular processes.
Many chemical mutagens require biological activation to become mutagenic. An important group of enzymes involved in the generation of mutagenic metabolites is cytochrome P450. Other enzymes that may also produce mutagenic metabolites include glutathione S-transferase and microsomal epoxide hydrolase. Mutagens that are not mutagenic by themselves but require biological activation are called promutagens.
Many mutations arise as a result of problems caused by the DNA lesions during replication, resulting in errors in replication. In bacteria, extensive damage to the DNA due to mutagens results in single-stranded DNA gaps during replication. This induces the SOS response, an emergency repair process that is also error-prone, thereby generating mutations. In mammalian cells, stalling of replication at a damaged sites induces a number of rescue mechanisms that help bypass DNA lesions, but which also may result in errors. The Y family of DNA polymerases specialize in DNA lesion bypass in a process termed translesion synthesis (TLS) whereby these lesion-bypass polymerases replace the stalled high-fidelity replicative DNA polymerase, transits the lesion and extend the DNA until the lesion has been passed so that normal replication can resume. These processes may be error-prone or error-free.
Spontaneous hydrolysis.
DNA is not entirely stable in aqueous solution. Under physiological conditions the glycosidic bond may be hydrolyzed spontaneously and 10,000 purine sites in DNA are estimated to be depurinated each day in a cell. Numerous DNA repair pathways exist for the DNA, however, if the apurinic site failed to be repaired, misincorporation of nucleotide may occur during replication. Adenine is preferentially incorporated by DNA polymerases in an apurinic site.
Cytidine may also become deaminated to uridine at one five-hundredth of the rate of depurination and can result in G to A transition. Eukaryotic cells also contains 5-methylcytosine, thought to be involved in the control of gene transcription, which can become deaminated into thymine.
Modification of bases.
Bases may be modified endogenously by normal cellular molecules. For example DNA may be methylated by S-adenosylmethionine, and glycosylated by reducing sugars.
Many compounds, such as PAHs, aromatic amines, aflatoxin and pyrrolizidine alkaloids, may form reactive oxygen species catalyzed by cytochrome P450. These metabolites form adducts with the DNA, which can cause errors in replication, and the bulky aromatic adducts may form stable intercalation between bases and block replication. The adducts may also induce conformational changes in the DNA. Some adducts may also result in the depurination of the DNA, it is however uncertain how significant such depurination as caused by the adducts is in generating mutation. 
Alkylation and arylation of bases can cause errors in replication. Some alkylating agents such as N-Nitrosamines may require the catalytic reaction of cytochrome-P450 for the formation of a reactive alkyl cation. N7 and O6 of guanine and the N3 and N7 of adenine are most susceptible to attack. N7-guanine adducts form the bulk of DNA adducts, but they appear to be non-mutagenic. Alkylation at O6 of guanine however is harmful because excision repair of O6-adduct of guanine may be poor in some tissues such as the brain. The O6 methylation of guanine can result in G to A transition, while O4-methylthymine can be mispaired with guanine. The type of the mutation generated however may be dependent on the size and type of the adduct as well as the DNA sequence.
Ionizing radiations and reactive oxygen species often oxidize guanine to produce 8-oxoguanine.
DNA damage and spontaneous mutation.
As noted above, the number of DNA damages occurring in a mammalian cell per day is high (more than 60,000 per day). Frequent occurrence of DNA damage is likely a problem for all DNA containing organisms, and the need to cope with DNA damages and to minimize their deleterious effects is likely a fundamental problem for life.
Most spontaneous mutations likely arise from error-prone trans-lesion synthesis past a DNA damage in the template strand during DNA replication. This process can overcome potentially lethal blockages, but at the cost of introducing inaccuracies in daughter DNA. The causal relationship of DNA damage to spontaneous mutation is illustrated by aerobically growing "E. coli" bacteria, in which 89% of spontaneously occurring base substitution mutations are caused by reactive oxygen species (ROS)-induced DNA damage. In yeast, more than 60% of spontaneous single-base pair substitutions and deletions are likely caused by trans-lesion synthesis.
An additional significant source of mutations in eukaryotes is the inaccurate DNA repair process non-homologous end joining, that is often employed in repair of double strand breaks.
In general, it appears that the main underlying cause of spontaneous mutation is error prone trans-lesion synthesis during DNA replication; and that the error-prone non-homologous end joining repair pathway may also be an important contributor in eukaryotes.
Crosslinking.
Some alkylating agents may produce crosslinking of DNA. Some natural occurring chemicals may also promote crosslinking, such as psoralens after activation by UV radiation, and nitrous acid. Interstrand cross-linking is more damaging as it blocks replication and transcription and can cause chromosomal breakages and rearrangements. Some crosslinkers such as cyclophosphamide, mitomycin C and cisplatin are used as anticancer chemotherapeutic because of their high degree of toxicity to proliferating cells.
Dimerization.
UV radiation promotes the formation of a cyclobutyl ring between adjacent thymines, resulting in the formation of pyrimidine dimers. In human skin cells, thousands of dimers may be formed in a day due to normal exposure to sunlight. DNA polymerase η may help bypass these lesions in an error-free manner; however, individuals with defective DNA repair function, such as sufferers of Xeroderma pigmentosum, are sensitive to sunlight and may be prone to skin cancer.
Intercalation between bases.
The planar structure of chemicals such as ethidium bromide and proflavine allows them to insert between bases in DNA. This insert causes the DNA's backbone to stretch and makes slippage in DNA during replication more likely to occur since the bonding between the strands is made less stable by the stretching. Forward slippage will result in deletion mutation, while reverse slippage will result in an insertion mutation. Also, the intercalation into DNA of anthracyclines such as daunorubicin and doxorubicin interferes with the functioning of the enzyme topoisomerase II, blocking replication as well as causing mitotic homologous recombination.
Backbone damage.
Ionizing radiations may produce highly reactive free radicals that can break the bonds in the DNA. Double-stranded breakages are especially damaging and hard to repair, producing translocation and deletion of part of a chromosomes. Alkylating agents like mustard gas may also cause breakages in the DNA backbone. Oxidative stress may also generate highly reactive oxygen species that can damage the DNA. Incorrect repair of other damages induced by the highly reactive species can also lead to mutations.
Insertional mutagenesis.
Transposon and virus may insert DNA sequence into coding region or functional elements of a gene and result in inactivation of the gene.
Error in replication.
While most mutagens produce effects that ultimately result in error in replication, some mutagens may affect directly the replication process. Base analog such as 5-bromouracil may substitute for thymine in replication. Some metals such as cadmium, chromium, and nickel may alter the fidelity of DNA replication.
Mutagenesis as a laboratory technique.
Mutagenesis in the laboratory is an important technique whereby DNA mutations are deliberately engineered to produce mutant genes, proteins, or strains of organism. Various constituents of a gene, such as its control elements and its gene product, may be mutated so that the functioning of a gene or protein can be examined in detail. The mutation may also produce mutant proteins with interesting properties, or enhanced or novel functions that may be of commercial use. Mutants strains may also be produced that have practical application or allow the molecular basis of particular cell function to be investigated.
Early methods of mutagenesis produces entirely random mutations, later methods of mutagenesis however may produce site-specific mutation.

</doc>
<doc id="19602" url="http://en.wikipedia.org/wiki?curid=19602" title="Mackenzie Bowell">
Mackenzie Bowell

Sir Mackenzie Bowell, PC KCMG (; December 27, 1823 – December 10, 1917) was an English born Canadian politician who served as the fifth Prime Minister of Canada from December 21, 1894 to April 27, 1896.
Biography.
Early life.
Bowell was born in Rickinghall, Suffolk, England to John Bowell and Elizabeth Marshall. In 1832 his family emigrated thence to Belleville, Upper Canada, where he apprenticed with the printer at the town newspaper, "The Belleville Intelligencer". He became a successful printer and editor with that newspaper, and later its owner. He was a Freemason but also an outstanding Orangeman, becoming Grandmaster of the Orange Order of British North America, 1870–1878. In 1847 he married Harriet Moore (1829–1884), with whom he had four sons and five daughters.
Early political life.
Bowell was first elected to the House of Commons in 1867, as a Conservative, for the riding of North Hastings, Ontario. He held his seat for the Conservatives when they lost the election of January 1874, in the wake of the Pacific Scandal. Later that year he was instrumental in having Louis Riel expelled from the House. In 1878, with the Conservatives again governing, he joined the cabinet as Minister of Customs. In 1892 he became Minister of Militia and Defence. A competent, hardworking administrator, Bowell remained in Cabinet as Minister of Trade and Commerce, a newly made portfolio, after he became a Senator that same year. His visit to Australia in 1893 led to the first conference of British colonies and territories, held in Ottawa in 1894. He became Leader of the Government in the Senate on October 31, 1893
Prime Minister (1894-1896).
In December 1894, Prime Minister Sir John Thompson died suddenly and Bowell, as the most senior Cabinet minister, was appointed in Thompson's stead by the Governor General. Bowell thus became the second of just two Canadian Prime Ministers to hold that office while serving in the Senate rather than the House of Commons. (The first was John Abbott.)
As Prime Minister, Bowell faced the troublesome Manitoba Schools Question. In 1890 Manitoba had abolished public funding for denominational schools, both Catholic and Protestant, which many thought was contrary to the provisions made for denominational schools in the Manitoba Act of 1870. However, in a court challenge, the Judicial Committee of the Privy Council held that Manitoba's abolition of public funding for denominational schools was consistent with the Manitoba Act provision. In a second court case, the Judicial Committee held that the federal Parliament had the authority to enact remedial legislation to force Manitoba to re-establish the funding.
Bowell and his predecessors had struggled to solve this problem. The issue had divided the country, the government, and even Bowell's own Cabinet. He was further hampered in his handling of the issue by his own indecisiveness on it, and by his inability, as a Senator, to take part in debates in the House of Commons. Bowell backed legislation, already drafted, that would have forced Manitoba to restore its Catholic schools, but then postponed it due to opposition within his Cabinet. With the ordinary business of government at a standstill, Bowell's Cabinet decided he was incompetent to lead and so, to force him to step down, seven ministers resigned, then foiled the appointment of successors. Though Bowell denounced them as "a nest of traitors," he had to agree to resign. After ten days, through an intervention on Bowell's behalf by the Governor General, the government crisis was resolved and matters seemingly returned to normal when six of the ministers were reinstated, but leadership was thenceforth effectively held by Charles Tupper, who had joined Cabinet at the same time, filling the seventh place. Tupper, who had been Canadian High Commissioner to the United Kingdom, had been recalled by the plotters to replace Bowell. Bowell formally resigned in favour of Tupper at the end of the parliamentary session.
Later life.
Bowell stayed on in the Senate, serving as his party's leader there until 1906, and afterward as a plain Senator until his death. He died of pneumonia in Belleville, only days short of his 94th birthday, and was buried in the Belleville Cemetery. His funeral was attended by a full complement of the Orange Order, but not by any currently or formerly elected member of the government.
Bowell's descendants live in Hertfordshire, England and Ontario, Canada.
In their 1999 study of the Canadian Prime Ministers up through Jean Chrétien, J. L. Granatstein and Norman Hillmer found that a survey of Canadian historians ranked Bowell #19 out of the 20 Prime Ministers up until then.
Supreme Court appointments.
The following jurist was appointed to the Supreme Court of Canada by the Governor General during Bowell's tenure:
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="19603" url="http://en.wikipedia.org/wiki?curid=19603" title="Manhattan Project">
Manhattan Project

The Manhattan Project was a research and development project that produced the first atomic bombs during World War II. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers; physicist J. Robert Oppenheimer was the director of the Los Alamos National Laboratory that designed the actual bombs. The Army component of the project was designated the Manhattan District; "Manhattan" gradually superseded the official codename, Development of Substitute Materials, for the entire project. Along the way, the project absorbed its earlier British counterpart, Tube Alloys. The Manhattan Project began modestly in 1939, but grew to employ more than 130,000 people and cost nearly US$2 billion (about $ in 2015 dollars). Over 90% of the cost was for building factories and producing the fissile materials, with less than 10% for development and production of the weapons. Research and production took place at more than 30 sites across the United States, the United Kingdom and Canada.
Two types of atomic bomb were developed during the war. A relatively simple gun-type fission weapon was made using uranium-235, an isotope that makes up only 0.7 percent of natural uranium. Since it is chemically identical to the most common isotope, uranium-238, and has almost the same mass, it proved difficult to separate. Three methods were employed for uranium enrichment: electromagnetic, gaseous and thermal. Most of this work was performed at Oak Ridge, Tennessee. In parallel with the work on uranium was an effort to produce plutonium. Reactors were constructed at Oak Ridge and Hanford, Washington, in which uranium was irradiated and transmuted into plutonium. The plutonium was then chemically separated from the uranium. The gun-type design proved impractical to use with plutonium so a more complex implosion-type weapon was developed in a concerted design and construction effort at the project's principal research and design laboratory in Los Alamos, New Mexico.
The project was also charged with gathering intelligence on the German nuclear energy project. Through Operation Alsos, Manhattan Project personnel served in Europe, sometimes behind enemy lines, where they gathered nuclear materials and documents, and rounded up German scientists. Despite the Manhattan Project's tight security, Soviet atomic spies still penetrated the program.
The first nuclear device ever detonated was an implosion-type bomb at the Trinity test, conducted at New Mexico's Alamogordo Bombing and Gunnery Range on 16 July 1945. Little Boy, a gun-type weapon, and Fat Man, an implosion-type weapon, were used in the atomic bombings of Hiroshima and Nagasaki, respectively. In the immediate postwar years, the Manhattan Project conducted weapons testing at Bikini Atoll as part of Operation Crossroads, developed new weapons, promoted the development of the network of national laboratories, supported medical research into radiology and laid the foundations for the nuclear navy. It maintained control over American atomic weapons research and production until the formation of the United States Atomic Energy Commission in January 1947.
Origins.
In August 1939, prominent physicists Leó Szilárd and Eugene Wigner drafted the Einstein–Szilárd letter, which warned of the potential development of "extremely powerful bombs of a new type". It urged the United States to take steps to acquire stockpiles of uranium ore and accelerate the research of Enrico Fermi and others into nuclear chain reactions. They had it signed by Albert Einstein and delivered to President Franklin D. Roosevelt. Roosevelt called on Lyman Briggs of the National Bureau of Standards to head the Advisory Committee on Uranium to investigate the issues raised by the letter. Briggs held a meeting on 21 October 1939, which was attended by Szilárd, Wigner and Edward Teller. The committee reported back to Roosevelt in November that uranium "would provide a possible source of bombs with a destructiveness vastly greater than anything now known."
Briggs proposed that the National Defense Research Committee (NDRC) spend $167,000 on research into uranium, particularly the uranium-235 isotope, and the recently discovered plutonium. On 28 June 1941, Roosevelt signed Executive Order 8807, which created the Office of Scientific Research and Development (OSRD), with Vannevar Bush as its director. The office was empowered to engage in large engineering projects in addition to research. The NDRC Committee on Uranium became the S-1 Uranium Committee of the OSRD; the word "uranium" was soon dropped for security reasons.
In Britain, Otto Frisch and Rudolf Peierls at the University of Birmingham had made a breakthrough investigating the critical mass of uranium-235 in June 1939. Their calculations indicated that it was within an order of magnitude of 10 kg, which was small enough to be carried by a bomber of the day. Their March 1940 Frisch–Peierls memorandum initiated the British atomic bomb project and its Maud Committee, which unanimously recommended pursuing the development of an atomic bomb. One of its members, the Australian physicist Mark Oliphant, flew to the United States in late August 1941 and discovered that data provided by the Maud Committee had not reached key American physicists. Oliphant then set out to find out why the committee's findings were apparently being ignored. He met with the Uranium Committee, and visited Berkeley, California, where he spoke persuasively to Ernest O. Lawrence. Lawrence was sufficiently impressed to commence his own research into uranium. He in turn spoke to James B. Conant, Arthur Compton and George Pegram. Oliphant's mission was therefore a success; key American physicists were now aware of the potential power of an atomic bomb.
At a meeting between President Roosevelt, Vannevar Bush, and Vice President Henry A. Wallace on 9 October 1941, the President approved the atomic program. To control it, he created a Top Policy Group consisting of himself—although he never attended a meeting—Wallace, Bush, Conant, Secretary of War Henry L. Stimson, and the Chief of Staff of the Army, General George Marshall. Roosevelt chose the Army to run the project rather than the Navy, as the Army had the most experience with management of large-scale construction projects. He also agreed to coordinate the effort with that of the British, and on 11 October he sent a message to Prime Minister Winston Churchill, suggesting that they correspond on atomic matters.
Feasibility.
Proposals.
The S-1 Committee held its first meeting on 18 December 1941 "pervaded by an atmosphere of enthusiasm and urgency" in the wake of the attack on Pearl Harbor and the subsequent United States declaration of war upon Japan and then on Germany. Work was proceeding on three different techniques for isotope separation to separate uranium-235 from uranium-238. Lawrence and his team at the University of California, Berkeley, investigated electromagnetic separation, while Eger Murphree and Jesse Wakefield Beams's team looked into gaseous diffusion at Columbia University, and Philip Abelson directed research into thermal diffusion at the Carnegie Institution of Washington and later the Naval Research Laboratory. Murphree was also the head of an unsuccessful separation project using gas centrifuges.
Meanwhile, there were two lines of research into nuclear reactor technology, with Harold Urey continuing research into heavy water at Columbia, while Arthur Compton brought the scientists working under his supervision at Columbia University and Princeton University to the University of Chicago, where he organized the Metallurgical Laboratory in early 1942 to study plutonium and reactors using graphite as a neutron moderator. Briggs, Compton, Lawrence, Murphree, and Urey met on 23 May 1942 to finalize the S-1 Committee recommendations, which called for all five technologies to be pursued. This was approved by Bush, Conant, and Brigadier General Wilhelm D. Styer, the chief of staff of Major General Brehon B. Somervell's Services of Supply, who had been designated the Army's representative on nuclear matters. Bush and Conant then took the recommendation to the Top Policy Group with a budget proposal for $54 million for construction by the United States Army Corps of Engineers, $31 million for research and development by OSRD and $5 million for contingencies in fiscal year 1943. The Top Policy Group in turn sent it to the President on 17 June 1942 and he approved it by writing "OK FDR" on the document.
Bomb design concepts.
Compton asked the theoretical physicist J. Robert Oppenheimer of the University of California, Berkeley, to take over research into fast neutron calculations—the key to calculations of critical mass and weapon detonation—from Gregory Breit, who had quit on 18 May 1942 because of concerns over lax operational security. John H. Manley, a physicist at the Metallurgical Laboratory, was assigned to assist Oppenheimer by contacting and coordinating experimental physics groups scattered across the country. Oppenheimer and Robert Serber of the University of Illinois examined the problems of neutron diffusion—how neutrons moved in a nuclear chain reaction—and hydrodynamics—how the explosion produced by a chain reaction might behave. To review this work and the general theory of fission reactions, Oppenheimer convened meetings at the University of Chicago in June and at the University of California, Berkeley, in July 1942 with theoretical physicists Hans Bethe, John Van Vleck, Edward Teller, Emil Konopinski, Robert Serber, Stan Frankel, and Eldred C. Nelson, the latter three former students of Oppenheimer, and experimental physicists Felix Bloch, Emilio Segrè, John Manley, and Edwin McMillan. They tentatively confirmed that a fission bomb was theoretically possible.
There were still many unknown factors. The properties of pure uranium-235 were relatively unknown, as were those of plutonium, an element that had only been discovered in February 1941 by Glenn Seaborg and his team. The scientists at the Berkeley conference envisioned creating plutonium in nuclear reactors where uranium-238 atoms absorbed neutrons that had been emitted from fissioning uranium-235 atoms. At this point no reactor had been built, and only tiny quantities of plutonium were available from cyclotrons. Even by December 1943, only two milligrams had been produced. There were many ways of arranging the fissile material into a critical mass. The simplest was shooting a "cylindrical plug" into a sphere of "active material" with a "tamper"—dense material that would focus neutrons inward and keep the reacting mass together to increase its efficiency. They also explored designs involving spheroids, a primitive form of "implosion" suggested by Richard C. Tolman, and the possibility of autocatalytic methods, which would increase the efficiency of the bomb as it exploded.
Considering the idea of the fission bomb theoretically settled—at least until more experimental data was available—the Berkeley conference then turned in a different direction. Edward Teller pushed for discussion of a more powerful bomb: the "super", now usually referred to as a "hydrogen bomb", which would use the explosive force of a detonating fission bomb to ignite a nuclear fusion reaction in deuterium and tritium. Teller proposed scheme after scheme, but Bethe refused each one. The fusion idea was put aside to concentrate on producing fission bombs. Teller also raised the speculative possibility that an atomic bomb might "ignite" the atmosphere because of a hypothetical fusion reaction of nitrogen nuclei. Bethe calculated that it could not happen, and a report co-authored by Teller showed that "no self-propagating chain of nuclear reactions is likely to be started." In Serber's account, Oppenheimer mentioned it to Arthur Compton, who "didn't have enough sense to shut up about it. It somehow got into a document that went to Washington" and was "never laid to rest".
Organization.
Manhattan District.
The Chief of Engineers, Major General Eugene Reybold, selected Colonel James C. Marshall to head the Army's part of the project in June 1942. Marshall created a liaison office in Washington, D.C., but established his temporary headquarters on the 18th floor of 270 Broadway in New York, where he could draw on administrative support from the Corps of Engineers' North Atlantic Division. It was close to the Manhattan office of Stone & Webster, the principal project contractor, and to Columbia University. He had permission to draw on his former command, the Syracuse District, for staff, and he started with Lieutenant Colonel Kenneth Nichols, who became his deputy.
Because most of his task involved construction, Marshall worked in cooperation with the head of the Corps of Engineers Construction Division, Major General Thomas M. Robbins, and his deputy, Colonel Leslie Groves. Reybold, Somervell and Styer decided to call the project "Development of Substitute Materials", but Groves felt that this would draw attention. Since engineer districts normally carried the name of the city where they were located, Marshall and Groves agreed to name the Army's component of the project the Manhattan District. This became official on 13 August, when Reybold issued the order creating the new district. Informally, it was known as the Manhattan Engineer District, or MED. Unlike other districts, it had no geographic boundaries, and Marshall had the authority of a division engineer. Development of Substitute Materials remained as the official codename of the project as a whole, but was supplanted over time by "Manhattan".
Marshall later conceded that, "I had never heard of atomic fission but I did know that you could not build much of a plant, much less four of them for $90 million." A single TNT plant that Nichols had recently built in Pennsylvania had cost $128 million. Nor were they impressed with estimates to the nearest order of magnitude, which Groves compared with telling a caterer to prepare for between ten and a thousand guests. A survey team from Stone & Webster had already scouted a site for the production plants. The War Production Board recommended sites around Knoxville, Tennessee, an isolated area where the Tennessee Valley Authority could supply ample electric power and the rivers could provide cooling water for the reactors. After examining several sites, the survey team selected one near Elza, Tennessee. Conant advised that it be acquired at once and Styer agreed but Marshall temporized, awaiting the results of Conant's reactor experiments before taking action. Of the prospective processes, only Lawrence's electromagnetic separation appeared sufficiently advanced for construction to commence.
Marshall and Nichols began assembling the resources they would need. The first step was to obtain a high priority rating for the project. The top ratings were AA-1 through AA-4 in descending order, although there was also a special AAA rating reserved for emergencies. Ratings AA-1 and AA-2 were for essential weapons and equipment, so Colonel Lucius D. Clay, the deputy chief of staff at Services and Supply for requirements and resources, felt that the highest rating he could assign was AA-3, although he was willing to provide a AAA rating on request for critical materials if the need arose. Nichols and Marshall were disappointed; AA-3 was the same priority as Nichols' TNT plant in Pennsylvania.
Military Policy Committee.
Bush became dissatisfied with Colonel Marshall's failure to get the project moving forward expeditiously, specifically the failure to acquire the Tennessee site, the low priority allocated to the project by the Army and the location of his headquarters in New York City. Bush felt that more aggressive leadership was required, and spoke to Harvey Bundy and Generals Marshall, Somervell, and Styer about his concerns. He wanted the project placed under a senior policy committee, with a prestigious officer, preferably Styer, as overall director.
Somervell and Styer selected Groves for the post, informing him on 17 September of this decision, and that General Marshall ordered that he be promoted to brigadier general, as it was felt that the title "general" would hold more sway with the academic scientists working on the Manhattan Project. Groves' orders placed him directly under Somervell rather than Reybold, with Colonel Marshall now answerable to Groves. Groves established his headquarters in Washington, D.C., on the fifth floor of the New War Department Building, where Colonel Marshall had his liaison office. He assumed command of the Manhattan Project on 23 September. Later that day, he attended a meeting called by Stimson, which established a Military Policy Committee, responsible to the Top Policy Group, consisting of Bush (with Conant as an alternate), Styer and Rear Admiral William R. Purnell. Tolman and Conant were later appointed as Groves' scientific advisers.
On 19 September, Groves went to Donald Nelson, the chairman of the War Production Board, and asked for broad authority to issue a AAA rating whenever it was required. Nelson initially balked but quickly caved in when Groves threatened to go to the President. Groves promised not to use the AAA rating unless it was necessary. It soon transpired that for the routine requirements of the project the AAA rating was too high but the AA-3 rating was too low. After a long campaign, Groves finally received AA-1 authority on 1 July 1944.
One of Groves' early problems was to find a director for Project Y, the group that would design and build the bomb. The obvious choice was one of the three laboratory heads, Urey, Lawrence, or Compton, but they could not be spared. Compton recommended Oppenheimer, who was already intimately familiar with the bomb design concepts. However, Oppenheimer had little administrative experience, and, unlike Urey, Lawrence, and Compton, had not won a Nobel Prize, which many scientists felt that the head of such an important laboratory should have. There were also concerns about Oppenheimer's security status, as many of his associates were Communists, including his brother, Frank Oppenheimer; his wife, Kitty; and his girlfriend, Jean Tatlock. A long conversation on a train in October 1942 convinced Groves and Nichols that Oppenheimer thoroughly understood the issues involved in setting up a laboratory in a remote area and should be appointed as its director. Groves personally waived the security requirements and issued Oppenheimer a clearance on 20 July 1943.
Collaboration with the United Kingdom.
The British and Americans exchanged nuclear information but did not initially combine their efforts. Britain rebuffed attempts by Bush and Conant in 1941 to strengthen cooperation with its own project, codenamed Tube Alloys, because it was reluctant to share its technological lead and help the United States develop its own atomic bomb. An American scientist who brought a personal letter from Roosevelt to Churchill offering to pay for all research and development in an Anglo-American project was poorly treated, and Churchill did not reply to the letter. The United States as a result decided as early as April 1942 that its offer was rejected, and that it should proceed alone. The United Kingdom did not have the manpower or resources of the United States and despite its early and promising start, Tube Alloys soon fell behind its American counterpart. On 30 July 1942, Sir John Anderson, the minister responsible for Tube Alloys, advised Churchill that: "We must face the fact that ... [our] pioneering work ... is a dwindling asset and that, unless we capitalise it quickly, we shall be outstripped. We now have a real contribution to make to a 'merger.' Soon we shall have little or none." That month Churchill and Roosevelt made an informal, unwritten agreement for atomic collaboration.
The opportunity for an equal partnership no longer existed, however, as shown in August 1942 when the British unsuccessfully demanded substantial control over the project while paying none of the costs. By 1943 the roles of the two countries had reversed from late 1941; in January Conant notified the British that they would no longer receive atomic information except in certain areas. While the British were shocked by the abrogation of the Churchill-Roosevelt agreement, head of the Canadian National Research Council C. J. Mackenzie was less surprised, writing "I can't help feeling that the United Kingdom group [over] emphasizes the importance of their contribution as compared with the Americans." As Conant and Bush told the British, the order came "from the top". The British bargaining position had worsened; the American scientists had decided that the United States no longer needed outside help, and they and others on the bomb policy committee wanted to prevent Britain from being able to build a postwar atomic weapon. The committee supported, and Roosevelt agreed to, restricting the flow of information to what Britain could use during the war—especially not bomb design—even if doing so slowed down the American project. By early 1943 the British stopped sending research and scientists to America, and as a result the Americans stopped all information sharing. The British considered ending the supply of Canadian uranium and heavy water to force the Americans to again share, but Canada needed American supplies to produce them. They investigated the possibility of an independent nuclear program, but determined that it could not be ready in time to affect the outcome of the war in Europe.
By March 1943 Conant decided that British help would benefit some areas of the project. James Chadwick and one or two other British scientists were important enough that the bomb design team at Los Alamos needed them, despite the risk of revealing weapon design secrets. In August 1943 Churchill and Roosevelt negotiated the Quebec Agreement, which resulted in a resumption of cooperation between scientists working on the same problem. Britain, however, agreed to restrictions on data on the building of large-scale production plants necessary for the bomb. The subsequent Hyde Park Agreement in September 1944 extended this cooperation to the postwar period. The Quebec Agreement established the Combined Policy Committee to coordinate the efforts of the United States, United Kingdom and Canada. Stimson, Bush and Conant served as the American members of the Combined Policy Committee, Field Marshal Sir John Dill and Colonel J. J. Llewellin were the British members, and C. D. Howe was the Canadian member. Llewellin returned to the United Kingdom at the end of 1943 and was replaced on the committee by Sir Ronald Ian Campbell, who in turn was replaced by the British Ambassador to the United States, Lord Halifax, in early 1945. Sir John Dill died in Washington, D.C., in November 1944 and was replaced both as Chief of the British Joint Staff Mission and as a member of the Combined Policy Committee by Field Marshal Sir Henry Maitland Wilson.
When cooperation resumed after the Quebec agreement, the Americans' progress and expenditures amazed the British. The United States had already spent more than $1 billion ($ today), while in 1943, the United Kingdom had spent about £0.5 million. Chadwick thus pressed for British involvement in the Manhattan Project to the fullest extent and abandon any hopes of a British project during the war. With Churchill's backing, he attempted to ensure that every request from Groves for assistance was honored. The British Mission that arrived in the United States in December 1943 included Niels Bohr, Otto Frisch, Klaus Fuchs, Rudolf Peierls, and Ernest Titterton. More scientists arrived in early 1944. While those assigned to gaseous diffusion left by the fall of 1944, the 35 working with Lawrence at Berkeley were assigned to existing laboratory groups and stayed until the end of the war. The 19 sent to Los Alamos also joined existing groups, primarily related to implosion and bomb assembly, but not the plutonium-related ones. Part of the Quebec Agreement specified that nuclear weapons would not be used against another country without mutual consent. In June 1945, Wilson agreed that the use of nuclear weapons against Japan would be recorded as a decision of the Combined Policy Committee.
The Combined Policy Committee created the Combined Development Trust in June 1944, with Groves as its chairman, to procure uranium and thorium ores on international markets. The Belgian Congo and Canada held much of the world's uranium outside Eastern Europe, and the Belgian government in exile was in London. Britain agreed to give the United States most of the Belgian ore, as it could not use most of the supply without restricted American research. In 1944, the Trust purchased 3440000 lb of uranium oxide ore from companies operating mines in the Belgian Congo. In order to avoid briefing US Secretary of the Treasury Henry Morgenthau Jr. on the project, a special account not subject to the usual auditing and controls was used to hold Trust monies. Between 1944 and the time he resigned from the Trust in 1947, Groves deposited a total of $37.5 million into the Trust's account.
Groves appreciated the early British atomic research and the British scientists' contributions to the Manhattan Project, but stated that the United States would have succeeded without them. Whether or not he was correct, the British wartime participation was crucial to the success of the United Kingdom's independent nuclear weapons program after the war when the McMahon Act of 1946 temporarily ended American nuclear cooperation.
Project sites.
Oak Ridge.
The day after he took over the project, Groves took a train to Tennessee with Colonel Marshall to inspect the proposed site there, and Groves was impressed. On 29 September 1942, United States Under Secretary of War Robert P. Patterson authorized the Corps of Engineers to acquire 56000 acre of land by eminent domain at a cost of $3.5 million. An additional 3000 acre was subsequently acquired. About 1,000 families were affected by the condemnation order, which came into effect on 7 October. Protests, legal appeals, and a 1943 Congressional inquiry were to no avail. By mid-November U.S. Marshals were tacking notices to vacate on farmhouse doors, and construction contractors were moving in. Some families were given two weeks' notice to vacate farms that had been their homes for generations; others had settled there after being evicted to make way for the Great Smoky Mountains National Park in the 1920s or the Norris Dam in the 1930s. The ultimate cost of land acquisition in the area, which was not completed until March 1945, was only about $2.6 million, which worked out to around $47 an acre. When presented with Public Proclamation Number Two, which declared Oak Ridge a total exclusion area that no one could enter without military permission, the Governor of Tennessee, Prentice Cooper, angrily tore it up.
Initially known as the Kingston Demolition Range, the site was officially renamed the Clinton Engineer Works (CEW) in early 1943. While Stone and Webster concentrated on the production facilities, the architectural and engineering firm Skidmore, Owings & Merrill designed and built a residential community for 13,000. The community was located on the slopes of Black Oak Ridge, from which the new town of Oak Ridge got its name. The Army presence at Oak Ridge increased in August 1943 when Nichols replaced Marshall as head of the Manhattan Engineer District. One of his first tasks was to move the district headquarters to Oak Ridge although the name of the district did not change. In September 1943 the administration of community facilities was outsourced to Turner Construction Company through a subsidiary, the Roane-Anderson Company (for Roane and Anderson Counties, in which Oak Ridge was located). Chemical engineers, including William J. Wilcox Jr. and Warren Fuchs, were part of "frantic efforts" to make 10% to 12% enriched uranium 235, known as the code name "tuballoy tetroxide", with tight security and fast approvals for supplies and materials. The population of Oak Ridge soon expanded well beyond the initial plans, and peaked at 75,000 in May 1945, by which time 82,000 people were employed at the Clinton Engineer Works, and 10,000 by Roane-Anderson.
Los Alamos.
The idea of locating Project Y at Oak Ridge was considered, but in the end it was decided that it should be in a remote location. On Oppenheimer's recommendation, the search for a suitable site was narrowed to the vicinity of Albuquerque, New Mexico, where Oppenheimer owned a ranch. In October 1942, Major John H. Dudley of the Manhattan Project was sent to survey the area, and he recommended a site near Jemez Springs, New Mexico. On 16 November, Oppenheimer, Groves, Dudley and others toured the site. Oppenheimer feared that the high cliffs surrounding the site would make his people feel claustrophobic, while the engineers were concerned with the possibility of flooding. The party then moved on to the vicinity of the Los Alamos Ranch School. Oppenheimer was impressed and expressed a strong preference for the site, citing its natural beauty and views of the Sangre de Cristo Mountains, which, it was hoped, would inspire those who would work on the project. The engineers were concerned about the poor access road, and whether the water supply would be adequate, but otherwise felt that it was ideal.
Patterson approved the acquisition of the site on 25 November 1942, authorizing $440,000 for the purchase of the site of 54000 acre, all but 8900 acre of which were already owned by the Federal Government. Secretary of Agriculture Claude R. Wickard granted use of some 45100 acre of United States Forest Service land to the War Department "for so long as the military necessity continues". The need for land for a new road, and later for a right of way for a 25 mi power line, eventually brought wartime land purchases to 45737 acre, but only $414,971 was spent. Construction was contracted to the M. M. Sundt Company of Tucson, Arizona, with Willard C. Kruger and Associates of Santa Fe, New Mexico, as architect and engineer. Work commenced in December 1942. Groves initially allocated $300,000 for construction, three times Oppenheimer's estimate, with a planned completion date of 15 March 1943. It soon became clear that the scope of Project Y was greater than expected, and by the time Sundt finished in 30 November 1943, over $7 million had been spent.
Because it was secret, Los Alamos was referred to as "Site Y" or "the Hill". Birth certificates of babies born in Los Alamos during the war listed their place of birth as PO Box 1663 in Santa Fe. Initially Los Alamos was to have been a military laboratory with Oppenheimer and other researchers commissioned into the Army. Oppenheimer went so far as to order himself a lieutenant colonel's uniform, but two key physicists, Robert Bacher and Isidor Rabi, balked at the idea. Conant, Groves and Oppenheimer then devised a compromise whereby the laboratory was operated by the University of California under contract to the War Department.
Argonne.
An Army-OSRD council on 25 June 1942 decided to build a pilot plant for plutonium production in Red Gate Woods southwest of Chicago. In July, Nichols arranged for a lease of 1025 acre from the Cook County Forest Preserve District, and Captain James F. Grafton was appointed Chicago area engineer. It soon became apparent that the scale of operations was too great for the area, and it was decided to build the plant at Oak Ridge, and keep a research and testing facility in Chicago.
Delays in establishing the plant in Red Gate Woods led Compton to authorize the Metallurgical Laboratory to construct the first nuclear reactor beneath the bleachers of Stagg Field at the University of Chicago. The reactor required an enormous amount of graphite blocks and uranium pellets. At the time, there was a limited source of pure uranium. Frank Spedding of Iowa State University were able to produce only two short tons of pure uranium. Additional three short tons of uranium metal was supplied by Westinghouse Lamp Plant which was produced in a rush with makeshift process. A large square balloon was constructed by Goodyear Tire to encase the reactor. On 2 December 1942, a team led by Enrico Fermi initiated the first artificial self-sustaining nuclear chain reaction in an experimental reactor known as Chicago Pile-1. The point at which a reaction becomes self-sustaining became known as "going critical". Compton reported the success to Conant in Washington, D.C., by a coded phone call, saying, "The Italian navigator [Fermi] has just landed in the new world."
In January 1943, Grafton's successor, Major Arthur V. Peterson, ordered Chicago Pile-1 dismantled and reassembled at Red Gate Woods, as he regarded the operation of a reactor as too hazardous for a densely populated area. After the war, the operations that remained at Red Gate moved to the new Argonne National Laboratory about 6 mi away.
Hanford.
By December 1942 there were concerns that even Oak Ridge was too close to a major population center (Knoxville) in the unlikely event of a major nuclear accident. Groves recruited DuPont in November 1942 to be the prime contractor for the construction of the plutonium production complex. DuPont was offered a standard cost plus fixed fee contract, but the President of the company, Walter S. Carpenter, Jr., wanted no profit of any kind, and asked for the proposed contract to be amended to explicitly exclude the company from acquiring any patent rights. This was accepted, but for legal reasons a nominal fee of one dollar was agreed upon. After the war, DuPont asked to be released from the contract early, and had to return 33 cents.
DuPont recommended that the site be located far from the existing uranium production facility at Oak Ridge. In December 1942, Groves dispatched Colonel Franklin Matthias and DuPont engineers to scout potential sites. Matthias reported that Hanford Site near Richland, Washington, was "ideal in virtually all respects". It was isolated and near the Columbia River, which could supply sufficient water to cool the reactors that would produce the plutonium. Groves visited the site in January and established the Hanford Engineer Works (HEW), codenamed "Site W".
Under Secretary Patterson gave his approval on 9 February, allocating $5 million for the acquisition of 40000 acre of land in the area. The federal government relocated some 1,500 residents of White Bluffs and Hanford, and nearby settlements, as well as the Wanapum and other tribes using the area. A dispute arose with farmers over compensation for crops, which had already been planted before the land was acquired. Where schedules allowed, the Army allowed the crops to be harvested, but this was not always possible. The land acquisition process dragged on and was not completed before the end of the Manhattan Project in December 1946.
The dispute did not delay work. Although progress on the reactor design at Metallurgical Laboratory and DuPont was not sufficiently advanced to accurately predict the scope of the project, a start was made in April 1943 on facilities for an estimated 25,000 workers, half of whom were expected to live on-site. By July 1944, some 1,200 buildings had been erected and nearly 51,000 people were living in the construction camp. As area engineer, Matthias exercised overall control of the site. At its peak, the construction camp was the third most populous town in Washington state. Hanford operated a fleet of over 900 buses, more than the city of Chicago. Like Los Alamos and Oak Ridge, Richland was a gated community with restricted access, but it looked more like a typical wartime American boomtown: the military profile was lower, and physical security elements like high fences, towers and guard dogs were less evident.
Canadian sites.
British Columbia.
Cominco had produced electrolytic hydrogen at Trail, British Columbia, since 1930. Urey suggested in 1941 that it could produce heavy water. To the existing $10 million plant consisting of 3,215 cells consuming 75 MW of hydroelectric power, secondary electrolysis cells were added to increase the deuterium concentration in the water from 2.3% to 99.8%. For this process, Hugh Taylor of Princeton developed a platinum-on-carbon catalyst for the first three stages while Urey developed a nickel-chromia one for the fourth stage tower. The final cost was $2.8 million. The Canadian Government did not officially learn of the project until August 1942. Trail's heavy water production started in January 1944 and continued until 1956. Heavy water from Trail was used for Chicago Pile 3, the first reactor using heavy water and natural uranium, which went critical on 15 May 1944.
Ontario.
The Chalk River, Ontario, site was established to rehouse the Allied effort at the Montreal Laboratory away from an urban area. A new community was built at Deep River, Ontario, to provide residences and facilities for the team members. The site was chosen for its proximity to the industrial manufacturing area of Ontario and Quebec, and proximity to a rail head adjacent to a large military base, Camp Petawawa. Located on the Ottawa River, it had access to abundant water. The first director of the new laboratory was John Cockcroft, later replaced by Bennett Lewis. A pilot reactor known as ZEEP (zero-energy experimental pile) became the first Canadian reactor, and the first to be completed outside the United States, when it went critical in September 1945. A larger 10 MW NRX reactor, which was designed during the war, was completed and went critical in July 1947.
Northwest Territories.
The Eldorado Mine at Port Radium was a source of uranium ore.
Heavy water sites.
Although DuPont's preferred designs for the nuclear reactors were helium cooled and used graphite as a moderator, DuPont still expressed an interest in using heavy water as a backup, in case the graphite reactor design proved infeasible for some reason. For this purpose, it was estimated that 3 LT of heavy water would be required per month. The "P-9 Project" was the government's code name for the heavy water production program. As the plant at Trail, which was then under construction, could produce 0.5 LT per month, additional capacity was required. Groves therefore authorized DuPont to establish heavy water facilities at the Morgantown Ordnance Works, near Morgantown, West Virginia; at the Wabash River Ordnance Works, near Dana and Newport, Indiana; and at the Alabama Ordnance Works, near Childersburg and Sylacauga, Alabama. Although known as Ordnance Works and paid for under Ordnance Department contracts, they were built and operated by the Army Corps of Engineers. The American plants used a process different from Trail's; heavy water was extracted by distillation, taking advantage of the slightly higher boiling point of heavy water.
Uranium.
Ore.
The key raw material for the project was uranium, which was used as fuel for the reactors, as feed that was transformed into plutonium, and, in its enriched form, in the atomic bomb itself. There were four known major deposits of uranium in 1940: in Colorado, in northern Canada, in Joachimstal in Czechoslovakia, and in the Belgian Congo. All but Joachimstal were in allied hands. A November 1942 survey determined that sufficient quantities of uranium were available to satisfy the project's requirements. Nichols arranged with the State Department for export controls to be placed on uranium oxide and negotiated for the purchase of 1200 LT of uranium ore from the Belgian Congo that was being stored in a warehouse on Staten Island and the remaining stocks of mined ore stored in the Congo. He negotiated with Eldorado Gold Mines for the purchase of ore from its refinery in Port Hope, Ontario, and its shipment in 100-ton lots. The Canadian government subsequently bought up the company's stock until it acquired a controlling interest.
While these purchases assured a sufficient supply to meet wartime needs, the American and British leaders concluded that it was in their countries' interest to gain control of as much of the world's uranium deposits as possible. The richest source of ore was the Shinkolobwe mine in the Belgian Congo, but it was flooded and closed. Nichols unsuccessfully attempted to negotiate its reopening and the sale of the entire future output to the United States with Edgar Sengier, the director of the company that owned the mine, Union Minière du Haut Katanga. The matter was then taken up by the Combined Policy Committee. As 30 percent of Union Minière's stock was controlled by British interests, the British took the lead in negotiations. Sir John Anderson and Ambassador John Winant hammered out a deal with Sengier and the Belgian government in May 1944 for the mine to be reopened and 1720 LT of ore to be purchased at $1.45 a pound. To avoid dependence on the British and Canadians for ore, Groves also arranged for the purchase of US Vanadium Corporation's stockpile in Uravan, Colorado. Uranium mining in Colorado yielded about 800 LT of ore.
Mallinckrodt Incorporated in St. Louis, Missouri, took the raw ore and dissolved it in nitric acid to produce uranyl nitrate. Ether was then added in a liquid–liquid extraction process to separate the impurities from the uranyl nitrate. This was then heated to form uranium trioxide, which was reduced to highly pure uranium dioxide. By July 1942, Mallinckrodt was producing a ton of highly pure oxide a day, but turning this into uranium metal initially proved more difficult for contractors Westinghouse and Metal Hydrides. Production was too slow and quality was unacceptably low. A special branch of the Metallurgical Laboratory was established at Iowa State College in Ames, Iowa, under Frank Spedding to investigate alternatives, and its Ames process became available in 1943.
Isotope separation.
Natural uranium consists of 99.3% uranium-238 and 0.7% uranium-235, but only the latter is fissile. The chemically identical uranium-235 has to be physically separated from the more plentiful isotope. Various methods were considered for uranium enrichment, most of which was carried out at Oak Ridge.
The most obvious technology, the centrifuge, failed, but electromagnetic separation, gaseous diffusion, and thermal diffusion technologies were all successful and contributed to the project. In February 1943, Groves came up with the idea of using the output of some plants as the input for others.
Centrifuges.
The centrifuge process was regarded as the only promising separation method in April 1942. Jesse Beams had developed such a process at the University of Virginia during the 1930s, but had encountered technical difficulties. The process required high rotational speeds, but at certain speeds harmonic vibrations developed that threatened to tear the machinery apart. It was therefore necessary to accelerate quickly through these speeds. In 1941 he began working with uranium hexafluoride, the only known gaseous compound of uranium, and was able to separate uranium-235. At Columbia, Urey had Cohen investigate the process, and he produced a body of mathematical theory making it possible to design a centrifugal separation unit, which Westinghouse undertook to construct.
Scaling this up to a production plant presented a formidable technical challenge. Urey and Cohen estimated that producing a kilogram (2.2 lb) of uranium-235 per day would require up to 50,000 centrifuges with 1 m rotors, or 10,000 centrifuges with 4 m rotors, assuming that 4-meter rotors could be built. The prospect of keeping so many rotors operating continuously at high speed appeared daunting, and when Beams ran his experimental apparatus, he obtained only 60% of the predicted yield, indicating that more centrifuges would be required. Beams, Urey and Cohen then began work on a series of improvements which promised to increase the efficiency of the process. However, frequent failures of motors, shafts and bearings at high speeds delayed work on the pilot plant. In November 1942 the centrifuge process was abandoned by the Military Policy Committee following a recommendation by Conant, Nichols and August C. Klein of Stone & Webster.
Electromagnetic separation.
Electromagnetic isotope separation was developed by Lawrence at the University of California Radiation Laboratory. This method employed devices known as calutrons, a hybrid of the standard laboratory mass spectrometer and cyclotron. The name was derived from the words "California", "university" and "cyclotron". In the electromagnetic process, a magnetic field deflected charged particles according to mass. The process was neither scientifically elegant nor industrially efficient. Compared with a gaseous diffusion plant or a nuclear reactor, an electromagnetic separation plant would consume more scarce materials, require more manpower to operate, and cost more to build. Nonetheless, the process was approved because it was based on proven technology and therefore represented less risk. Moreover, it could be built in stages, and rapidly reach industrial capacity.
Marshall and Nichols discovered that the electromagnetic isotope separation process would require 5,000 tons of copper, which was in desperately short supply. However, silver could be substituted, in an 11:10 ratio. On 3 August 1942, Nichols met with Under Secretary of the Treasury Daniel W. Bell and asked for the transfer of 6,000 tons of silver bullion from the West Point Depository. "Young man," Bell told him, "you may think of silver in tons but the Treasury will always think of silver in troy ounces!" Eventually, 14,700 tons were used.
The 1000 ozt silver bars were cast into cylindrical billets and taken to Phelps Dodge in Bayway, New Jersey, where they were extruded into strips 0.625 in thick, 3 in wide and 40 ft long. These were wound onto magnetic coils by Allis-Chalmers in Milwaukee, Wisconsin. After the war, all the machinery was dismantled and cleaned and the floorboards beneath the machinery were ripped up and burned to recover minute amounts of silver. In the end, only 1/3,600,000th was lost. The last silver was returned in May 1970.
Responsibility for the design and construction of the electromagnetic separation plant, which came to be called Y-12, was assigned to Stone & Webster by the S-1 Committee in June 1942. The design called for five first-stage processing units, known as Alpha racetracks, and two units for final processing, known as Beta racetracks. In September 1943 Groves authorized construction of four more racetracks, known as Alpha II. Construction began in February 1943.
When the plant was started up for testing on schedule in October, the 14-ton vacuum tanks crept out of alignment because of the power of the magnets, and had to be fastened more securely. A more serious problem arose when the magnetic coils started shorting out. In December Groves ordered a magnet to be broken open, and handfuls of rust were found inside. Groves then ordered the racetracks to be torn down and the magnets sent back to the factory to be cleaned. A pickling plant was established on-site to clean the pipes and fittings. The second Alpha I was not operational until the end of January 1944, the first Beta and first and third Alpha I's came online in March, and the fourth Alpha I was operational in April. The four Alpha II racetracks were completed between July and October 1944.
Tennessee Eastman was hired to manage Y-12 on the usual cost plus fixed fee basis, with a fee of $22,500 per month plus $7,500 per racetrack for the first seven racetracks and $4,000 per additional racetrack. The calutrons were initially operated by scientists from Berkeley to remove bugs and achieve a reasonable operating rate. They were then turned over to trained Tennessee Eastman operators who had only a high school education. Nichols compared unit production data, and pointed out to Lawrence that the young "hillbilly" girl operators were outperforming his PhDs. They agreed to a production race and Lawrence lost, a morale boost for the Tennessee Eastman workers and supervisors. The girls were "trained like soldiers not to reason why", while "the scientists could not refrain from time-consuming investigation of the cause of even minor fluctuations of the dials."
Y-12 initially enriched the uranium-235 content to between 13% and 15%, and shipped the first few hundred grams of this to Los Alamos in March 1944. Only 1 part in 5,825 of the uranium feed emerged as final product. Much of the rest was splattered over equipment in the process. Strenuous recovery efforts helped raise production to 10% of the uranium-235 feed by January 1945. In February the Alpha racetracks began receiving slightly enriched (1.4%) feed from the new S-50 thermal diffusion plant. The next month it received enhanced (5%) feed from the K-25 gaseous diffusion plant. By April K-25 was producing uranium sufficiently enriched to feed directly into the Beta tracks.
Gaseous diffusion.
The most promising but also the most challenging method of isotope separation was gaseous diffusion. Graham's law states that the rate of effusion of a gas is inversely proportional to the square root of its molecular mass, so in a box containing a semi-permeable membrane and a mixture of two gases, the lighter molecules will pass out of the container more rapidly than the heavier molecules. The gas leaving the container is somewhat enriched in the lighter molecules, while the residual gas is somewhat depleted. The idea was that such boxes could be formed into a cascade of pumps and membranes, with each successive stage containing a slightly more enriched mixture. Research into the process was carried out at Columbia University by a group that included Harold Urey, Karl P. Cohen and John R. Dunning.
In November 1942 the Military Policy Committee approved the construction of a 600-stage gaseous diffusion plant. On 14 December, M. W. Kellogg accepted an offer to construct the plant, which was codenamed K-25. A cost plus fixed fee contract was negotiated, eventually totaling $2.5 million. A separate corporate entity called Kellex was created for the project, headed by Percival C. Keith, one of Kellogg's vice presidents. The process faced formidable technical difficulties. The highly corrosive gas uranium hexafluoride would have to be used, as no substitute could be found, and the motors and pumps would have to be vacuum tight and enclosed in inert gas. The biggest problem was the design of the barrier, which would have to be strong, porous and resistant to corrosion by uranium hexafluoride. The best choice for this seemed to be nickel. Edward Adler and Edward Norris created a mesh barrier from electroplated nickel. A six-stage pilot plant was built at Columbia to test the process, but the Norris-Adler prototype proved to be too brittle. A rival barrier was developed from powdered nickel by Kellex, the Bell Telephone Laboratories and the Bakelite Corporation. In January 1944, Groves ordered the Kellex barrier into production.
Kellex's design for K-25 called for a four-story 0.5 mi long U-shaped structure containing 54 contiguous buildings. These were divided into nine sections. Within these were cells of six stages. The cells could be operated independently, or consecutively within a section. Similarly, the sections could be operated separately or as part of a single cascade. A survey party began construction by marking out the 500 acre site in May 1943. Work on the main building began in October 1943, and the six-stage pilot plant was ready for operation on 17 April 1944. In 1945 Groves canceled the upper stages of the plant, directing Kellex to instead design and build a 540-stage side feed unit, which became known as K-27. Kellex transferred the last unit to the operating contractor, Union Carbide and Carbon, on 11 September 1945. The total cost, including the K-27 plant completed after the war, came to $480 million.
The production plant commenced operation in February 1945, and as cascade after cascade came online, the quality of the product increased. By April 1945, K-25 had attained a 1.1% enrichment and the output of the S-50 thermal diffusion plant began being used as feed. Some product produced the next month reached nearly 7% enrichment. In August, the last of the 2,892 stages commenced operation. K-25 and K-27 achieved their full potential in the early postwar period, when they eclipsed the other production plants and became the prototypes for a new generation of plants.
Thermal diffusion.
The thermal diffusion process was based on Sydney Chapman and David Enskog's theory, which explained that when a mixed gas passes through a temperature gradient, the heavier one tends to concentrate at the cold end and the lighter one at the warm end. Since hot gases tend to rise and cool ones tend to fall, this can be used as a means of isotope separation. This process was first demonstrated by H. Clusius and G. Dickel in Germany in 1938. It was developed by US Navy scientists, but was not one of the enrichment technologies initially selected for use in the Manhattan Project. This was primarily due to doubts about its technical feasibility, but the inter-service rivalry between the Army and Navy also played a part.
The Naval Research Laboratory continued the research under Philip Abelson's direction, but there was little contact with the Manhattan Project until April 1944, when Captain William S. Parsons, the naval officer who was in charge of ordnance development at Los Alamos, brought Oppenheimer news of encouraging progress in the Navy's experiments on thermal diffusion. Oppenheimer wrote to Groves suggesting that the output of a thermal diffusion plant could be fed into Y-12. Groves set up a committee consisting of Warren K. Lewis, Eger Murphree and Richard Tolman to investigate the idea, and they estimated that a thermal diffusion plant costing $3.5 million could enrich 50 kg of uranium per week to nearly 0.9% uranium-235. Groves approved its construction on 24 June 1944.
Groves contracted with the H. K. Ferguson Company of Cleveland, Ohio, to build the thermal diffusion plant, which was designated S-50. Groves' advisers, Karl Cohen and W. I. Thompson from Standard Oil, estimated that it would take six months to build. Groves gave Ferguson just four. Plans called for the installation of 2,142 48 ft diffusion columns arranged in 21 racks. Inside each column were three concentric tubes. Steam, obtained from the nearby K-25 powerhouse at a pressure of 100 psi and temperature of 545 F, flowed downward through the innermost 1.25 in nickel pipe, while water at 155 F flowed upward through the outermost iron pipe. Isotope separation occurred in the uranium hexafluoride gas between the nickel and copper pipes.
Work commenced on 9 July 1944, and S-50 began partial operation in September. Ferguson operated the plant through a subsidiary known as Fercleve. The plant produced just 10.5 lb of 0.852% uranium-235 in October. Leaks limited production and forced shutdowns over the next few months, but in June 1945 it produced 12730 lb. By March 1945, all 21 production racks were operating. Initially the output of S-50 was fed into Y-12, but starting in March 1945 all three enrichment processes were run in series. S-50 became the first stage, enriching from 0.71% to 0.89%. This material was fed into the gaseous diffusion process in the K-25 plant, which produced a product enriched to about 23%. This was, in turn, fed into Y-12, which boosted it to about 89%, sufficient for nuclear weapons.
Aggregate U-235 production.
About 50 kg of uranium enriched to 89% uranium-235 was delivered to Los Alamos by July 1945. The entire 50 kg, along with some 50%-enriched, averaging out to about 85% enriched, were used in Little Boy
Plutonium.
The second line of development pursued by the Manhattan Project used the fissile element plutonium. Although small amounts of plutonium exist in nature, the best way to obtain large quantities of the element is in a nuclear reactor, in which natural uranium is bombarded by neutrons. The uranium-238 is transmuted into uranium-239, which rapidly decays, first into neptunium-239 and then into plutonium-239. Only a small amount of the uranium-238 will be transformed, so the plutonium must be chemically separated from the remaining uranium, from any initial impurities, and from fission products.
X-10 Graphite Reactor.
In March 1943, DuPont began construction of a plutonium plant on a 112 acre site at Oak Ridge. Intended as a pilot plant for the larger production facilities at Hanford, it included the air-cooled X-10 Graphite Reactor, a chemical separation plant, and support facilities. Because of the subsequent decision to construct water-cooled reactors at Hanford, only the chemical separation plant operated as a true pilot. The X-10 Graphite Reactor consisted of a huge block of graphite, 24 ft long on each side, weighing around 1500 LT, surrounded by 7 ft of high-density concrete as a radiation shield.
The greatest difficulty was encountered with the uranium slugs produced by Mallinckrodt and Metal Hydrides. These somehow had to be coated in aluminum to avoid corrosion and the escape of fission products into the cooling system. The Grasselli Chemical Company attempted to develop a hot dipping process without success. Meanwhile Alcoa tried canning. A new process for flux-less welding was developed, and 97% of the cans passed a standard vacuum test, but high temperature tests indicated a failure rate of more than 50%. Nonetheless, production began in June 1943. The Metallurgical Laboratory eventually developed an improved welding technique with the help of General Electric, which was incorporated into the production process in October 1943.
Watched by Fermi and Compton, the X-10 Graphite Reactor went critical on 4 November 1943 with about 30 LT of uranium. A week later the load was increased to 36 LT, raising its power generation to 500 kW, and by the end of the month the first 500 mg of plutonium was created. Modifications over time raised the power to 4,000 kW in July 1944. X-10 operated as a production plant until January 1945, when it was turned over to research activities.
Hanford reactors.
Although an air-cooled design was chosen for the reactor at Oak Ridge to facilitate rapid construction, it was recognized that this would be impractical for the much larger production reactors. Initial designs by the Metallurgical Laboratory and DuPont used helium for cooling, before they determined that a water-cooled reactor would be simpler, cheaper and quicker to build. The design did not become available until 4 October 1943; in the meantime, Matthias concentrated on improving the Hanford site by erecting accommodations, improving the roads, building a railway switch line, and upgrading the electricity, water and telephone lines.
As at Oak Ridge, the most difficulty was encountered while canning the uranium slugs, which commenced at Hanford in March 1944. They were pickled to remove dirt and impurities, dipped in molten bronze, tin, and aluminum-silicon alloy, canned using hydraulic presses, and then capped using arc welding under an argon atmosphere. Finally, they were subjected to a series of tests to detect holes or faulty welds. Disappointingly, most canned slugs initially failed the tests, resulting in an output of only a handful of canned slugs per day. But steady progress was made and by June 1944 production increased to the point where it appeared that enough canned slugs would be available to start Reactor B on schedule in August 1944.
Work began on Reactor B, the first of six planned 250 MW reactors, on 10 October 1943. The reactor complexes were given letter designations A through F, with B, D and F sites chosen to be developed first, as this maximised the distance between the reactors. They would be the only ones constructed during the Manhattan Project. Some 390 LT of steel, 17400 cuyd of concrete, 50,000 concrete blocks and 71,000 concrete bricks were used to construct the 120 ft high building.
Construction of the reactor itself commenced in February 1944. Watched by Compton, Matthias, DuPont's Crawford Greenewalt, Leona Woods and Fermi, who inserted the first slug, the reactor was powered up beginning on 13 September 1944. Over the next few days, 838 tubes were loaded and the reactor went critical. Shortly after midnight on 27 September, the operators began to withdraw the control rods to initiate production. At first all appeared well but around 03:00 the power level started to drop and by 06:30 the reactor had shut down completely. The cooling water was investigated to see if there was a leak or contamination. The next day the reactor started up again, only to shut down once more.
Fermi contacted Chien-Shiung Wu, who identified the cause of the problem as neutron poisoning from xenon-135, which has a half-life of 9.2 hours. Fermi, Woods, Donald J. Hughes and John Archibald Wheeler then calculated the nuclear cross section of xenon-135, which turned out to be 30,000 times that of uranium. Fortunately, DuPont engineer George Graves had deviated from the Metallurgical Laboratory's original design in which the reactor had 1,500 tubes arranged in a circle, and had added an additional 504 tubes to fill in the corners. The scientists had originally considered this overengineering a waste of time and money, but Fermi realized that by loading all 2,004 tubes, the reactor could reach the required power level and efficiently produce plutonium. Reactor D was started on 17 December 1944 and Reactor F on 25 February 1945.
Separation process.
Meanwhile, the chemists considered the problem of how plutonium could be separated from uranium when its chemical properties were not known. Working with the minute quantities of plutonium available at the Metallurgical Laboratory in 1942, a team under Charles M. Cooper developed a lanthanum fluoride process for separating uranium and plutonium, which was chosen for the pilot separation plant. A second separation process, the bismuth phosphate process, was subsequently developed by Seaborg and Stanly G. Thomson. This process worked by toggling plutonium between its +4 and +6 oxidation states in solutions of bismuth phosphate. In the former state, the plutonium was precipitated; in the latter, it stayed in solution and the other products were precipitated.
Greenewalt favored the bismuth phosphate process due to the corrosive nature of lanthanum fluoride, and it was selected for the Hanford separation plants. Once X-10 began producing plutonium, the pilot separation plant was put to the test. The first batch was processed at 40% efficiency but over the next few months this was raised to 90%.
At Hanford, top priority was initially given to the installations in the 300 area. This contained buildings for testing materials, preparing uranium, and assembling and calibrating instrumentation. One of the buildings housed the canning equipment for the uranium slugs, while another contained a small test reactor. Notwithstanding the high priority allocated to it, work on the 300 area fell behind schedule due to the unique and complex nature of the 300 area facilities, and wartime shortages of labor and materials.
Early plans called for the construction of two separation plants in each of the areas known as 200-West and 200-East. This was subsequently reduced to two, the T and U plants, in 200-West and one, the B plant, at 200-East. Each separation plant consisted of four buildings: a process cell building or "canyon" (known as 221), a concentration building (224), a purification building (231) and a magazine store (213). The canyons were each 800 ft long and 65 ft wide. Each consisted of forty 17.7 by cells.
Work began on 221-T and 221-U in January 1944, with the former completed in September and the latter in December. The 221-B building followed in March 1945. Because of the high levels of radioactivity involved, all work in the separation plants had to be conducted by remote control using closed-circuit television, something unheard of in 1943. Maintenance was carried out with the aid of an overhead crane and specially designed tools. The 224 buildings were smaller because they had less material to process, and it was less radioactive. The 224-T and 224-U buildings were completed on 8 October 1944, and 224-B followed on 10 February 1945. The purification methods that were eventually used in 231-W were still unknown when construction commenced on 8 April 1944, but the plant was complete and the methods were selected by the end of the year. On 5 February 1945, Matthias hand-delivered the first shipment of 80 g of 95%-pure plutonium nitrate to a Los Alamos courier in Los Angeles.
Weapon design.
In 1943, development efforts were directed to a gun-type fission weapon with plutonium called Thin Man. Initial research on the properties of plutonium was done using cyclotron-generated plutonium-239, which was extremely pure, but could only be created in very small amounts. Los Alamos received the first sample of plutonium from the Clinton X-10 reactor in April 1944 and within days Emilio Segrè discovered a problem: the reactor-bred plutonium had a higher concentration of plutonium-240, resulting in up to five times the spontaneous fission rate of cyclotron plutonium. Seaborg had correctly predicted in March 1943 that some of the plutonium-239 would absorb a neutron and become plutonium-240.
This made reactor plutonium unsuitable for use in a gun-type weapon. The plutonium-240 would start the chain reaction too quickly, causing a predetonation that would release enough energy to disperse the critical mass with a minimal amount of plutonium reacted (a fizzle). A faster gun was suggested but found to be impractical. The possibility of separating the isotopes was considered and rejected, as plutonium-240 is even harder to separate from plutonium-239 than uranium-235 from uranium-238.
Work on an alternative method of bomb design, known as implosion, had begun earlier at the instigation of the physicist Seth Neddermeyer. Implosion used explosives to crush a subcritical sphere of fissile material into a smaller and denser form. When the fissile atoms are packed closer together, the rate of neutron capture increases, and the mass becomes a critical mass. The metal needs to travel only a very short distance, so the critical mass is assembled in much less time than it would take with the gun method. Neddermeyer's 1943 and early 1944 investigations into implosion showed promise, but also made it clear that the problem would be much more difficult from a theoretical and engineering perspective than the gun design. In September 1943, John von Neumann, who had experience with shaped charges used in armor-piercing shells, argued that not only would implosion reduce the danger of predetonation and fizzle, but would make more efficient use of the fissionable material. He proposed using a spherical configuration instead of the cylindrical one that Neddermeyer was working on.
By July 1944, Oppenheimer had concluded plutonium could not be used in a gun design, and opted for implosion. The accelerated effort on an implosion design, codenamed Fat Man, began in August 1944 when Oppenheimer implemented a sweeping reorganization of the Los Alamos laboratory to focus on implosion. Two new groups were created at Los Alamos to develop the implosion weapon, X (for explosives) Division headed by George Kistiakowsky and G (for gadget) Division under Robert Bacher. The new design that von Neumann and T (for theoretical) Division, most notably Rudolf Peierls, had devised used explosive lenses to focus the explosion onto a spherical shape using a combination of both slow and fast high explosives.
The design of lenses that detonated with the proper shape and velocity turned out to be slow, difficult and frustrating. Various explosives were tested before settling on composition B as the fast explosive and baratol as the slow explosive. The final design resembled a soccer ball, with 20 hexagonal and 12 pentagonal lenses, each weighing about 80 lb. Getting the detonation just right required fast, reliable and safe electrical detonators, of which there were two for each lens for reliability. It was therefore decided to use exploding-bridgewire detonators, a new invention developed at Los Alamos by a group led by Luis Alvarez. A contract for their manufacture was given to Raytheon.
To study the behavior of converging shock waves, Robert Serber devised the RaLa Experiment, which used the short-lived radioisotope lanthanum-140, a potent source of gamma radiation. The gamma ray source was placed in the center of a metal sphere surrounded by the explosive lenses, which in turn were inside in an ionization chamber. This allowed the taking of an X-ray movie of the implosion. The lenses were designed primarily using this series of tests. In his history of the Los Alamos project, David Hawkins wrote: "RaLa became the most important single experiment affecting the final bomb design".
Within the explosives was the 4.5 inch thick aluminum pusher, which provided a smooth transition from the relatively low density explosive to the next layer, the 3 inch thick tamper of natural uranium. Its main job was to hold the critical mass together as long as possible, but it would also reflect neutrons back into the core. Some part of it might fission as well. To prevent predetonation by an external neutron, the tamper was coated in a thin layer of boron. A polonium-beryllium modulated neutron initiator, known as an "urchin" because its shape resembled a sea urchin, was developed to start the chain reaction at precisely the right moment. This work with the chemistry and metallurgy of radioactive polonium was directed by Charles Allen Thomas of the Monsanto Company and became known as the Dayton Project. Testing required up to 500 curies per month of polonium, which Monsanto was able to deliver. The whole assembly was encased in a duralumin bomb casing to protect it from bullets and flak.
The ultimate task of the metallurgists was to determine how to cast plutonium into a sphere. The difficulties became apparent when attempts to measure the density of plutonium gave inconsistent results. At first contamination was believed to be the cause, but it was soon determined that there were multiple allotropes of plutonium. The brittle α phase that exists at room temperature changes to the plastic β phase at higher temperatures. Attention then shifted to the even more malleable δ phase that normally exists in the 300 °C to 450 °C range. It was found that this was stable at room temperature when alloyed with aluminum, but aluminum emits neutrons when bombarded with alpha particles, which would exacerbate the pre-ignition problem. The metallurgists then hit upon a plutonium-gallium alloy, which stabilized the δ phase and could be hot pressed into the desired spherical shape. As plutonium was found to corrode readily, the sphere was coated with nickel.
The work proved dangerous. By the end of the war, half the experienced chemists and metallurgists had to be removed from work with plutonium when unacceptably high levels of the element appeared in their urine. A minor fire at Los Alamos in January 1945 led to a fear that a fire in the plutonium laboratory might contaminate the whole town, and Groves authorized the construction of a new facility for plutonium chemistry and metallurgy, which became known as the DP-site. The hemispheres for the first plutonium pit (or core) were produced and delivered on 2 July 1945. Three more hemispheres followed on 23 July and were delivered three days later.
Trinity.
Because of the complexity of an implosion-style weapon, it was decided that, despite the waste of fissile material, an initial test would be required. Groves approved the test, subject to the active material being recovered. Consideration was therefore given to a controlled fizzle, but Oppenheimer opted instead for a full-scale nuclear test, codenamed "Trinity".
In March 1944, planning for the test was assigned to Kenneth Bainbridge, a professor of physics at Harvard, working under Kistiakowsky. Bainbridge selected the bombing range near Alamogordo Army Airfield as the site for the test. Bainbridge worked with Captain Samuel P. Davalos on the construction of the Trinity Base Camp and its facilities, which included barracks, warehouses, workshops, an explosive magazine and a commissary.
Groves did not relish the prospect of explaining the loss of a billion dollars worth of plutonium to a Senate committee, so a cylindrical containment vessel codenamed "Jumbo" was constructed to recover the active material in the event of a failure. Measuring 25 ft long and 12 ft wide, it was fabricated at great expense from 214 LT of iron and steel by Babcock & Wilcox in Barberton, Ohio. Brought in a special railroad car to a siding in Pope, New Mexico, it was transported the last 25 mi to the test site on a trailer pulled by two tractors. By the time it arrived, however, confidence in the implosion method was high enough, and the availability of plutonium was sufficient, that Oppenheimer decided not to use it. Instead, it was placed atop a steel tower 800 yd from the weapon as a rough measure of how powerful the explosion would be. In the end, Jumbo survived, although its tower did not, adding credence to the belief that Jumbo would have successfully contained a fizzled explosion.
A pre-test explosion was conducted on 7 May 1945 to calibrate the instruments. A wooden test platform was erected 800 yd from Ground Zero and piled with 100 LT of TNT spiked with nuclear fission products in the form of an irradiated uranium slug from Hanford, which was dissolved and poured into tubing inside the explosive. This explosion was observed by Oppenheimer and Groves's new deputy commander, Brigadier General Thomas Farrell. The pre-test produced data that proved vital for the Trinity test.
For the actual test, the weapon, nicknamed "the gadget", was hoisted to the top of a 100 ft steel tower, as detonation at that height would give a better indication of how the weapon would behave when dropped from a bomber. Detonation in the air maximized the energy applied directly to the target, and generated less nuclear fallout. The gadget was assembled under the supervision of Norris Bradbury at the nearby McDonald Ranch House on 13 July, and precariously winched up the tower the following day. Observers included Bush, Chadwick, Conant, Farrell, Fermi, Groves, Lawrence, Oppenheimer and Tolman. At 05:30 on 16 July 1945 the gadget exploded with an energy equivalent of around 20 kilotons of TNT, leaving a crater of Trinitite (radioactive glass) in the desert 250 ft wide. The shock wave was felt over 100 mi away, and the mushroom cloud reached 7.5 mi in height. It was heard as far away as El Paso, Texas, so Groves issued a cover story about an ammunition magazine explosion at Alamogordo Field.
Personnel.
In June 1944, the Manhattan Project employed some 129,000 workers, of whom 84,500 were construction workers, 40,500 were plant operators and 1,800 were military personnel. As construction activity fell off, the workforce declined to 100,000 a year later, but the number of military personnel increased to 5,600. Procuring the required numbers of workers, especially highly skilled workers, in competition with other vital wartime programs proved very difficult. In 1943, Groves obtained a special temporary priority for labor from the War Manpower Commission. In March 1944, both the War Production Board and the War Manpower Commission gave the project their highest priority.
Tolman and Conant, in their role as the project's scientific advisers, drew up a list of candidate scientists and had them rated by scientists already working on the project. Groves then sent a personal letter to the head of their university or company asking for them to be released for essential war work. At the University of Wisconsin–Madison, Stanislaw Ulam gave one of his students, Joan Hinton, an exam early, so she could leave to do war work. A few weeks later, Ulam received a letter from Hans Bethe, inviting him to join the project. Conant personally persuaded the explosives expert George Kistiakowsky to join the project.
One source of skilled personnel was the Army itself, particularly the Army Specialized Training Program. In 1943, the MED created the Special Engineer Detachment (SED), with an authorized strength of 675. Technicians and skilled workers drafted into the Army were assigned to the SED. Another source was the Women's Army Corps (WAC). Initially intended for clerical tasks handling classified material, the WACs were soon tapped for technical and scientific tasks as well. On 1 February 1945, all military personnel assigned to the MED, including all SED detachments, were assigned to the 9812th Technical Service Unit, except at Los Alamos, where military personnel other than SED, including the WACs and Military Police, were assigned to the 4817th Service Command Unit.
An Associate Professor of Radiology at the University of Rochester School of Medicine, Stafford L. Warren, was commissioned as a colonel in the United States Army Medical Corps, and appointed as chief of the MED's Medical Section and Groves' medical advisor. Warren's initial task was to staff hospitals at Oak Ridge, Richland and Los Alamos. The Medical Section was responsible for medical research, but also for the MED's health and safety programs. This presented an enormous challenge, because workers were handling a variety of toxic chemicals, using hazardous liquids and gases under high pressures, working with high voltages, and performing experiments involving explosives, not to mention the largely unknown dangers presented by radioactivity and handling fissile materials. Yet in December 1945, the National Safety Council presented the Manhattan Project with the Award of Honor for Distinguished Service to Safety in recognition of its safety record. Between January 1943 and June 1945, there were 62 fatalities and 3,879 disabling injuries, which was about 62 percent below the rate of private industry.
Secrecy.
A 1945 "Life" article estimated that before the Hiroshima and Nagasaki bombings "[p]robably no more than a few dozen men in the entire country knew the full meaning of the Manhattan Project, and perhaps only a thousand others even were aware that work on atoms was involved." The magazine wrote that the more than 100,000 others employed with the project "worked like moles in the dark". Warned that disclosing the project's secrets was punishable by 10 years in prison or a $10,000 ($ today) fine, they saw enormous quantities of raw materials enter factories with nothing coming out, and monitored "dials and switches while behind thick concrete walls mysterious reactions took place" without knowing the purpose of their jobs.
Oak Ridge security personnel considered any private party with more than seven people as suspicious, and residents—who believed that US government agents were secretly among them—avoided repeatedly inviting the same guests. Although original residents of the area could be buried in existing cemeteries, every coffin was reportedly opened for inspection. Everyone, including top military officials, and their automobiles were searched when entering and exiting project facilities. One Oak Ridge worker stated that "if you got inquisitive, you were called on the carpet within two hours by government secret agents. Usually those summoned to explain were then escorted bag and baggage to the gate and ordered to keep going." Nonetheless, despite being told that their work would help end the war and perhaps all future wars, not seeing or understanding the results of their often tedious duties—or even typical side effects of factory work such as smoke from smokestacks—and the war in Europe ending without the use of their work, caused serious morale problems among workers and caused many rumors to spread. One manager stated after the war:
Well it wasn't that the job was tough ... it was confusing. You see, no one knew what was being made in Oak Ridge, not even me, and a lot of the people thought they were wasting their time here. It was up to me to explain to the dissatisfied workers that they were doing a very important job. When they asked me what, I'd have to tell them it was a secret. But I almost went crazy myself trying to figure out what was going on.
Another worker told of how, working in a laundry, she every day held "a special instrument" to uniforms and listened for "a clicking noise". She learned only after the war that she had been performing the important task of checking for radiation with a geiger counter. To improve morale among such workers Oak Ridge created an extensive system of intramural sports leagues, including 10 baseball teams, 81 softball teams, and 26 football teams.
Censorship.
Voluntary censorship of atomic information began before the Manhattan Project. After the start of the European war in 1939 American scientists began avoiding publishing military-related research, and in 1940 scientific journals began asking the National Academy of Sciences to clear articles. William L. Laurence of "The New York Times", who wrote an article for "The Saturday Evening Post" in September 1940 on atomic fission, later learned that government officials asked librarians nationwide in 1943 to withdraw the issue. The Soviets noticed the silence, however. In April 1942 nuclear physicist Georgy Flyorov wrote to Josef Stalin on the absence of articles on nuclear fission in American journals; this resulted in the Soviet Union establishing its own atomic bomb project.
The Manhattan Project operated under tight security lest its discovery induce Axis powers, especially Germany, to accelerate their own nuclear projects or undertake covert operations against the project. The government's Office of Censorship, by contrast, relied on the press to comply with a voluntary code of conduct it published, and the project at first avoided notifying the office. By early 1943 newspapers began publishing reports of large construction in Tennessee and Washington based on public records, and the office began discussing with the project how to maintain secrecy. In June the Office of Censorship asked newspapers and broadcasters to avoid discussing "atom smashing, atomic energy, atomic fission, atomic splitting, or any of their equivalents. The use for military purposes of radium or radioactive materials, heavy water, high voltage discharge equipment, cyclotrons." The office also asked to avoid discussion of "polonium, uranium, ytterbium, hafnium, protactinium, radium, rhenium, thorium, deuterium"; only uranium was sensitive, but was listed with other elements to hide its importance.
Soviet spies.
The prospect of sabotage was always present, and sometimes suspected when there were equipment failures. While there were some problems believed to be the result of careless or disgruntled employees, there were no confirmed instances of Axis-instigated sabotage. However, on 10 March 1945, a Japanese fire balloon struck a power line, and the resulting power surge caused the three reactors at Hanford to be temporarily shut down. With so many people involved, security was a difficult task. A special Counter Intelligence Corps detachment was formed to handle the project's security issues. By 1943, it was clear that the Soviet Union was attempting to penetrate the project. Lieutenant Colonel Boris T. Pash, the head of the Counter Intelligence Branch of the Western Defense Command, investigated suspected Soviet espionage at the Radiation Laboratory in Berkeley. Oppenheimer informed Pash that he had been approached by a fellow professor at Berkeley, Haakon Chevalier, about passing information to the Soviet Union.
The most successful Soviet spy was Klaus Fuchs, a member of the British Mission who played an important part at Los Alamos. The 1950 revelation of his espionage activities damaged the United States' nuclear cooperation with Britain and Canada. Subsequently, other instances of espionage were uncovered, leading to the arrest of Harry Gold, David Greenglass and Ethel and Julius Rosenberg. Other spies like George Koval and Theodore Hall remained unknown for decades. The value of the espionage is difficult to quantify, as the principal constraint on the Soviet atomic bomb project was a shortage of uranium ore. The consensus is that espionage saved the Soviets one or two years of effort.
Foreign intelligence.
In addition to developing the atomic bomb, the Manhattan Project was charged with gathering intelligence on the German nuclear energy project. It was believed that the Japanese nuclear weapons program was not far advanced because Japan had little access to uranium ore, but it was initially feared that Germany was very close to developing its own weapons. At the instigation of the Manhattan Project, a bombing and sabotage campaign was carried out against heavy water plants in German-occupied Norway. A small mission was created, jointly staffed by the Office of Naval Intelligence, OSRD, the Manhattan Project, and Army Intelligence (G-2), to investigate enemy scientific developments. It was not restricted to those involving nuclear weapons. The Chief of Army Intelligence, Major General George V. Strong, appointed Boris Pash to command the unit, which was codenamed "Alsos", a Greek word meaning "grove".
The Alsos Mission to Italy questioned staff of the physics laboratory at the University of Rome following the capture of the city in June 1944. Meanwhile Pash formed a combined British and American Alsos mission in London under the command of Captain Horace K. Calvert to participate in Operation Overlord. Groves considered the risk that the Germans might attempt to disrupt the Normandy landings with radioactive poisons was sufficient to warn General Dwight D. Eisenhower and send an officer to brief his chief of staff, Lieutenant General Walter Bedell Smith. Under the codename Operation Peppermint, special equipment was prepared and Chemical Warfare Service teams were trained in its use.
Following in the wake of the advancing Allied armies, Pash and Calvert interviewed Frédéric Joliot-Curie about the activities of German scientists. They spoke to officials at Union Minière du Haut Katanga about uranium shipments to Germany. They tracked down 68 tons of ore in Belgium and 30 tons in France. The interrogation of German prisoners indicated that uranium and thorium were being processed in Oranienburg, 20 miles north of Berlin, so Groves arranged for it to be bombed on 15 March 1945.
An Alsos team went to Stassfurt in the Soviet Occupation Zone and retrieved 11 tons of ore from WIFO. In April 1945, Pash, in command of a composite force known as T-Force, conducted Operation Harborage, a sweep behind enemy lines of the cities of Hechingen, Bisingen and Haigerloch that were the heart of the German nuclear effort. T-Force captured the nuclear laboratories, documents, equipment and supplies, including heavy water and 1.5 tons of metallic uranium.
Alsos teams rounded up German scientists including Kurt Diebner, Otto Hahn, Walther Gerlach, Werner Heisenberg and Carl Friedrich von Weizsäcker, who were taken to England where they were interned at Farm Hall, a bugged house in Godmanchester. After the bombs were detonated in Japan, the Germans were forced to confront the fact that the Allies had done what they could not.
Bombing of Hiroshima and Nagasaki.
Preparations.
Starting in November 1943, the Army Air Forces Materiel Command at Wright Field, Ohio, began Silverplate, the codename modification of B-29s to carry the bombs. Test drops were carried out at Muroc Army Air Field, California, and the Naval Ordnance Test Station at Inyokern, California. Groves met with the Chief of United States Army Air Forces (USAAF), General Henry H. Arnold, in March 1944 to discuss the delivery of the finished bombs to their targets. The only Allied aircraft capable of carrying the 17 ft long Thin Man or the 59 in wide Fat Man was the British Avro Lancaster, but using a British aircraft would have caused difficulties with maintenance. Groves hoped that the American Boeing B-29 Superfortress could be modified to carry Thin Man by joining its two bomb bays together. Arnold promised that no effort would be spared to modify B-29s to do the job, and designated Major General Oliver P. Echols as the USAAF liaison to the Manhattan Project. In turn, Echols named Colonel Roscoe C. Wilson as his alternate, and Wilson became Manhattan Project's main USAAF contact. President Roosevelt instructed Groves that if the atomic bombs were ready before the war with Germany ended, he should be ready to drop them on Germany.
The 509th Composite Group was activated on 17 December 1944 at Wendover Army Air Field, Utah, under the command of Colonel Paul W. Tibbets. This base, close to the border with Nevada, was codenamed "Kingman" or "W-47". Training was conducted at Wendover and at Batista Army Airfield, Cuba, where the 393d Bombardment Squadron practiced long-distance flights over water, and dropping dummy pumpkin bombs. A special unit known as Alberta was formed at Los Alamos under Captain William S. Parsons as part of the Manhattan Project to assist in preparing and delivering the bombs. Commander Frederick L. Ashworth from Alberta met with Fleet Admiral Chester W. Nimitz on Guam in February 1945 to inform him of the project. While he was there, Ashworth selected North Field on the Pacific Island Tinian as a base for the 509th Composite Group, and reserved space for the group and its buildings. The group deployed there in July 1945. Farrell arrived at Tinian on 30 July as the Manhattan Project representative.
Most of the components for Little Boy left San Francisco on the cruiser USS "Indianapolis" on 16 July and arrived on Tinian on 26 July. Four days later the ship was sunk by a Japanese submarine. The remaining components, which included six uranium-235 rings, were delivered by three C-54 Skymasters of the 509th Group's 320th Troop Carrier Squadron. Two Fat Man assemblies travelled to Tinian in specially modified 509th Composite Group B-29s. The first plutonium core went in a special C-54. A joint targeting committee of the Manhattan District and USAAF was established to determine which cities in Japan should be targets, and recommended Kokura, Hiroshima, Niigata and Kyoto. At this point, Secretary of War Henry L. Stimson intervened, announcing that he would be making the targeting decision, and that he would not authorize the bombing of Kyoto on the grounds of its historical and religious significance. Groves therefore asked Arnold to remove Kyoto not just from the list of nuclear targets, but from targets for conventional bombing as well. One of Kyoto's substitutes was Nagasaki.
Bombings.
In May 1945, the Interim Committee was created to advise on wartime and postwar use of nuclear energy. The committee was chaired by Stimson, with James F. Byrnes, a former US Senator soon to be Secretary of State, as President Harry S. Truman's personal representative; Ralph A. Bard, the Under Secretary of the Navy; William L. Clayton, the Assistant Secretary of State; Vannevar Bush; Karl T. Compton; James B. Conant; and George L. Harrison, an assistant to Stimson and president of New York Life Insurance Company. The Interim Committee in turn established a scientific panel consisting of Arthur Compton, Fermi, Lawrence and Oppenheimer to advise it on scientific issues. In its presentation to the Interim Committee, the scientific panel offered its opinion not just on the likely physical effects of an atomic bomb, but on its probable military and political impact.
At the Potsdam Conference in Germany, Truman was informed that the Trinity test had been successful. He told Stalin, the leader of the Soviet Union, that the US had a new superweapon, without giving any details. This was the first official communication to the Soviet Union about the bomb, but Stalin already knew about it from spies. With the authorization to use the bomb against Japan already given, no alternatives were considered after the Japanese rejection of the Potsdam Declaration.
On 6 August 1945, the 393d Bombardment Squadron B-29 "Enola Gay", piloted and commanded by Tibbets, lifted off with Parsons on board as weaponeer, and Little Boy in its bomb bay. Hiroshima, the headquarters of the 2nd General Army and Fifth Division and a port of embarkation, was the primary target of the mission, with Kokura and Nagasaki as alternatives. With Farrell's permission, Parsons completed the bomb assembly in the air to minimize the risks during takeoff. The bomb detonated at an altitude of 1750 ft with a blast that was later estimated to be the equivalent of 13 kilotons of TNT. An area of approximately 4.7 sqmi was destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6–7% damaged. About 70,000 to 80,000 people, of whom 20,000 were Japanese soldiers, or some 30% of the population of Hiroshima, were killed immediately, and another 70,000 injured.
On the morning of 9 August 1945, the B-29 "Bockscar", piloted by the 393d Bombardment Squadron's commander, Major Charles W. Sweeney, lifted off with a Fat Man on board. This time, Ashworth served as weaponeer and Kokura was the primary target. Sweeney took off with the weapon already armed but with the electrical safety plugs still engaged. When they reached Kokura, they found cloud cover had obscured the city, prohibiting the visual attack required by orders. After three runs over the city, and with fuel running low, they headed for the secondary target, Nagasaki. Ashworth decided that a radar approach would be used if the target was obscured, but a last-minute break in the clouds over Nagasaki allowed a visual approach as ordered. The Fat Man was dropped over the city's industrial valley midway between the Mitsubishi Steel and Arms Works in the south and the Mitsubishi-Urakami Ordnance Works in the north. The resulting explosion had a blast yield equivalent to 21 kilotons of TNT, roughly the same as the Trinity blast, but was confined to the Urakami Valley, and a major portion of the city was protected by the intervening hills, resulting in the destruction of about 44% of the city. The bombing also crippled the city's industrial production extensively and killed 23,200-28,200 Japanese industrial workers and 150 Japanese soldiers. Overall, an estimated 35,000-40,000 people were killed and 60,000 injured.
Groves expected to have another atomic bomb ready for use on 19 August, with three more in September and a further three in October. Two more Fat Man assemblies were readied, and scheduled to leave Kirtland Field for Tinian on 11 and 14 August. At Los Alamos, technicians worked 24 hours straight to cast another plutonium core. Although cast, it still needed to be pressed and coated, which would take until 16 August. It could therefore have been ready for use on 19 August. On 10 August, Truman secretly requested that additional atomic bombs not be dropped on Japan without his express authority. Groves suspended the third core's shipment on his own authority on 13 August.
On 11 August, Groves phoned Warren with orders to organize a survey team to report on the damage and radioactivity at Hiroshima and Nagasaki. A party equipped with portable Geiger counters arrived in Hiroshima on 8 September headed by Farrell and Warren, with Japanese Rear Admiral Masao Tsuzuki, who acted as a translator. They remained in Hiroshima until 14 September and then surveyed Nagasaki from 19 September to 8 October. This and other scientific missions to Japan would provide valuable scientific and historical data.
The necessity of the bombings of Hiroshima and Nagasaki became a subject of controversy among historians. Some questioned whether an "atomic diplomacy" would not have attained the same goals and disputed whether the bombings or the Soviet declaration of war on Japan was decisive. The Franck Report was the most notable effort pushing for a demonstration but was turned down by the Interim Committee's scientific panel. The Szilárd petition, drafted in July 1945 and signed by dozens of scientists working on the Manhattan Project, was a late attempt at warning President Harry S. Truman about his responsibility in using such weapons.
After the war.
Seeing the work they had not understood produce the Hiroshima and Nagasaki bombs amazed the workers of the Manhattan Project as much as the rest of the world; newspapers in Oak Ridge announcing the Hiroshima bomb sold for $1 ($ today). Although the bombs' existence was public, secrecy continued, and many workers remained ignorant of their jobs; one stated in 1946, "I don't know what the hell I'm doing besides looking into a ——— and turning a ——— alongside a ———. I don't know anything about it, and there's nothing to say". Many residents continued to avoid discussion of "the stuff" in ordinary conversation despite it being the reason for their town's existence.
In anticipation of the bombings, Groves had Henry DeWolf Smyth prepare a history for public consumption. "Atomic Energy for Military Purposes", better known as the "Smyth Report", was released to the public on 12 August 1945. Groves and Nichols presented Army–Navy "E" Awards to key contractors, whose involvement had hitherto been secret. Over 20 awards of the Presidential Medal for Merit were made to key contractors and scientists, including Bush and Oppenheimer. Military personnel received the Legion of Merit, including the commander of the Women's Army Corps detachment, Captain Arlene G. Scheidenhelm.
At Hanford, plutonium production fell off as Reactors B, D and F wore out, "poisoned" by fission products and swelling of the graphite moderator known as the Wigner effect. The swelling damaged the charging tubes where the uranium was irradiated to produce plutonium, rendering them unusable. In order to maintain the supply of polonium for the urchin initiators, production was curtailed and the oldest unit, B pile, was closed down so at least one reactor would be available in the future. Research continued, with DuPont and the Metallurgical Laboratory developing a redox solvent extraction process as an alternative plutonium extraction technique to the bismuth phosphate process, which left unspent uranium in a state from which it could not easily be recovered.
Bomb engineering was carried out by the Z Division, named for its director, Dr. Jerrold R. Zacharias from Los Alamos. Z Division was initially located at Wendover Field but moved to Oxnard Field, New Mexico, in September 1945 to be closer to Los Alamos. This marked the beginning of Sandia Base. Nearby Kirtland Field was used as a B-29 base for aircraft compatibility and drop tests. By October, all the staff and facilities at Wendover had been transferred to Sandia. As reservist officers were demobilized, they were replaced by about fifty hand-picked regular officers.
Nichols recommended that S-50 and the Alpha tracks at Y-12 be closed down. This was done in September. Although performing better than ever, the Alpha tracks could not compete with K-25 and the new K-27, which had commenced operation in January 1946. In December, the Y-12 plant was closed, thereby cutting the Tennessee Eastman payroll from 8,600 to 1,500 and saving $2 million a month.
Nowhere was demobilization more of a problem than at Los Alamos, where there was an exodus of talent. Much remained to be done. The bombs used on Hiroshima and Nagasaki were like laboratory pieces; work would be required to make them simpler, safer and more reliable. Implosion methods needed to be developed for uranium in place of the wasteful gun method, and composite uranium-plutonium cores were needed now that plutonium was in short supply because of the problems with the reactors. However, uncertainty about the future of the laboratory made it hard to induce people to stay. Oppenheimer returned to his job at the University of California and Groves appointed Norris Bradbury as an interim replacement. In fact, Bradbury would remain in the post for the next 25 years. Groves attempted to combat the dissatisfaction caused by the lack of amenities with a construction program that included an improved water supply, three hundred houses, and recreation facilities.
Two Fat Man–type detonations were conducted at Bikini Atoll in July 1946 as part of Operation Crossroads to investigate the effect of nuclear weapons on warships. Able was detonated on 1 July 1946. The more spectacular Baker was detonated underwater on 25 July 1946.
After the bombings at Hiroshima and Nagasaki, a number of Manhattan Project physicists founded the "Bulletin of the Atomic Scientists", which began as an emergency action undertaken by scientists who saw urgent need for an immediate educational program about atomic weapons. In the face of the destructiveness of the new weapons and in anticipation of the nuclear arms race several project members including Bohr, Bush and Conant expressed the view that it was necessary to reach agreement on international control of nuclear research and atomic weapons. The Baruch Plan, unveiled in a speech to the newly formed United Nations Atomic Energy Commission (UNAEC) in June 1946, proposed the establishment of an international atomic development authority, but was not adopted.
Following a domestic debate over the permanent management of the nuclear program, the United States Atomic Energy Commission (AEC) was created by the Atomic Energy Act of 1946 to take over the functions and assets of the Manhattan Project. It established civilian control over atomic development, and separated the development, production and control of atomic weapons from the military. Military aspects were taken over by the Armed Forces Special Weapons Project (AFSWP). Although the Manhattan Project ceased to exist on 31 December 1946, the Manhattan District would remain until it too was abolished on 15 August 1947.
Cost.
The project expenditure through 1 October 1945 was $1.845 billion, equivalent to less than nine days of wartime spending, and was $2.191 billion when the AEC assumed control on 1 January 1947. Total allocation was $2.4 billion. Over 90% of the cost was for building plants and producing the fissionable materials, and less than 10% for development and production of the weapons.
A total of four weapons (the Trinity gadget, Little Boy, Fat Man, and an unused bomb) were produced by the end of 1945, making the average cost per bomb around $500 million in 1945 dollars. By comparison, the project's total cost by the end of 1945 was about 90% of the total spent on the production of US small arms (not including ammunition) and 34% of the total spent on US tanks during the same period.
Legacy.
The political and cultural impacts of the development of nuclear weapons were profound and far-reaching. William Laurence of the "New York Times", the first to use the phrase "Atomic Age", became the official correspondent for the Manhattan Project in spring 1945. In 1943 and 1944 he unsuccessfully attempted to persuade the Office of Censorship to permit writing about the explosive potential of uranium, and government officials felt that he had earned the right to report on the biggest secret of the war. Laurence witnessed both the Trinity test and the bombing of Nagasaki and wrote the official press releases prepared for them. He went on to write a series of articles extolling the virtues of the new weapon. His reporting before and after the bombings helped to spur public awareness of the potential of nuclear technology and motivated its development in the United States and the Soviet Union.
The wartime Manhattan Project left a legacy in the form of the network of national laboratories: the Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, Oak Ridge National Laboratory, Argonne National Laboratory and Ames Laboratory. Two more were established by Groves soon after the war, the Brookhaven National Laboratory at Upton, New York, and the Sandia National Laboratories at Albuquerque, New Mexico. Groves allocated $72 million to them for research activities in fiscal year 1946–1947. They would be in the vanguard of the kind of large-scale research that Alvin Weinberg, the director of the Oak Ridge National Laboratory, would call Big Science.
The Naval Research Laboratory had long been interested in the prospect of using nuclear power for warship propulsion, and sought to create its own nuclear project. In May 1946, Nimitz, now Chief of Naval Operations, decided that the Navy should instead work with the Manhattan Project. A group of naval officers were assigned to Oak Ridge, the most senior of whom was Captain Hyman G. Rickover, who became assistant director there. They immersed themselves in the study of nuclear energy, laying the foundations for a nuclear-powered navy. A similar group of Air Force personnel arrived at Oak Ridge in September 1946 with the aim of developing nuclear aircraft. Their Nuclear Energy for the Propulsion of Aircraft (NEPA) project ran into formidable technical difficulties, and was ultimately cancelled.
The ability of the new reactors to create radioactive isotopes in previously unheard-of quantities sparked a revolution in nuclear medicine in the immediate postwar years. Starting in mid-1946, Oak Ridge began distributing radioisotopes to hospitals and universities. Most of the orders were for iodine-131 and phosphorus-32, which were used in the diagnosis and treatment of cancer. In addition to medicine, isotopes were also used in biological, industrial and agricultural research.
On handing over control to the Atomic Energy Commission, Groves bid farewell to the people who had worked on the Manhattan Project:Five years ago, the idea of Atomic Power was only a dream. You have made that dream a reality. You have seized upon the most nebulous of ideas and translated them into actualities. You have built cities where none were known before. You have constructed industrial plants of a magnitude and to a precision heretofore deemed impossible. You built the weapon which ended the War and thereby saved countless American lives. With regard to peacetime applications, you have raised the curtain on vistas of a new world.
Notes.
Footnotes
Citations
References.
Participant accounts.
</dl>

</doc>
<doc id="19605" url="http://en.wikipedia.org/wiki?curid=19605" title="Main sequence">
Main sequence

In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or "dwarf" stars.
After a star has formed, it generates thermal energy in the dense core region through the nuclear fusion of hydrogen atoms into helium. During this stage of the star's lifetime, it is located along the main sequence at a position determined primarily by its mass, but also based upon its chemical composition and other factors. All main-sequence stars are in hydrostatic equilibrium, where outward thermal pressure from the hot core is balanced by the inward pressure of gravitational collapse from the overlying layers. The strong dependence of the rate of energy generation in the core on the temperature and pressure helps to sustain this balance. Energy generated at the core makes its way to the surface and is radiated away at the photosphere. The energy is carried by either radiation or convection, with the latter occurring in regions with steeper temperature gradients, higher opacity or both.
The main sequence is sometimes divided into upper and lower parts, based on the dominant process that a star uses to generate energy. Stars below about 1.5 times the mass of the Sun (or 1.5 solar masses (M☉)) primarily fuse hydrogen atoms together in a series of stages to form helium, a sequence called the proton–proton chain. Above this mass, in the upper main sequence, the nuclear fusion process mainly uses atoms of carbon, nitrogen and oxygen as intermediaries in the CNO cycle that produces helium from hydrogen atoms. Main-sequence stars with more than two solar masses undergo convection in their core regions, which acts to stir up the newly created helium and maintain the proportion of fuel needed for fusion to occur. Below this mass, stars have cores that are entirely radiative with convective zones near the surface. With decreasing stellar mass, the proportion of the star forming a convective envelope steadily increases, while main-sequence stars below 0.4 M☉ undergo convection throughout their mass. When core convection does not occur, a helium-rich core develops surrounded by an outer layer of hydrogen.
In general, the more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram. The behavior of a star now depends on its mass, with stars below 0.23 M☉ becoming white dwarfs directly, while stars with up to ten solar masses pass through a red giant stage. More massive stars can explode as a supernova, or collapse directly into a black hole.
History.
In the early part of the 20th century, information about the types and distances of stars became more readily available. The spectra of stars were shown to have distinctive features, which allowed them to be categorized. Annie Jump Cannon and Edward C. Pickering at Harvard College Observatory developed a method of categorization that became known as the Harvard Classification Scheme, published in the "Harvard Annals" in 1901.
In Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung noticed that the reddest stars—classified as K and M in the Harvard scheme—could be divided into two distinct groups. These stars are either much brighter than the Sun, or much fainter. To distinguish these groups, he called them "giant" and "dwarf" stars. The following year he began studying star clusters; large groupings of stars that are co-located at approximately the same distance. He published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence.
At Princeton University, Henry Norris Russell was following a similar course of research. He was studying the relationship between the spectral classification of stars and their actual brightness as corrected for distance—their absolute magnitude. For this purpose he used a set of stars that had reliable parallaxes and many of which had been categorized at Harvard. When he plotted the spectral types of these stars against their absolute magnitude, he found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy.
Of the red stars observed by Hertzsprung, the dwarf stars also followed the spectra-luminosity relationship discovered by Russell. However, the giant stars are much brighter than dwarfs and so, do not follow the same relationship. Russell proposed that the "giant stars must have low density or great surface-brightness, and the reverse is true of dwarf stars". The same curve also showed that there were very few faint white stars.
In 1933, Bengt Strömgren introduced the term Hertzsprung–Russell diagram to denote a luminosity-spectral class diagram. This name reflected the parallel development of this technique by both Hertzsprung and Russell earlier in the century.
As evolutionary models of stars were developed during the 1930s, it was shown that, for stars of a uniform chemical composition, a relationship exists between a star's mass and its luminosity and radius. That is, for a given mass and composition, there is a unique solution for determining the star's radius and luminosity. This became known as the Vogt-Russell theorem; named after Heinrich Vogt and Henry Norris Russell. By this theorem, once a star's chemical composition and its position on the main sequence is known, so too is the star's mass and radius. (However, it was subsequently discovered that the theorem breaks down somewhat for stars of non-uniform composition.)
A refined scheme for stellar classification was published in 1943 by W. W. Morgan and P. C. Keenan. The MK classification assigned each star a spectral type—based on the Harvard classification—and a luminosity class. The Harvard classification had been developed by assigning a different letter to each star based on the strength of the hydrogen spectral line, before the relationship between spectra and temperature was known. When ordered by temperature and when duplicate classes were removed, the spectral types of stars followed, in order of decreasing temperature with colors ranging from blue to red, the sequence O, B, A, F, G, K and M. (A popular mnemonic for memorizing this sequence of stellar classes is "Oh Be A Fine Girl/Guy, Kiss Me".) The luminosity class ranged from I to V, in order of decreasing luminosity. Stars of luminosity class V belonged to the main sequence.
Formation.
When a protostar is formed from the collapse of a giant molecular cloud of gas and dust in the local interstellar medium, the initial composition is homogeneous throughout, consisting of about 70% hydrogen, 28% helium and trace amounts of other elements, by mass. The initial mass of the star depends on the local conditions within the cloud. (The mass distribution of newly formed stars is described empirically by the initial mass function.) During the initial collapse, this pre-main-sequence star generates energy through gravitational contraction. Upon reaching a suitable density, energy generation is begun at the core using an exothermic nuclear fusion process that converts hydrogen into helium.
Hertzsprung–Russell diagram
Spectral type
Brown dwarfs
White dwarfs
Red dwarfs
Subdwarfs
Main sequence<br>("dwarfs")
Subgiants
Giants
Bright giants
Supergiants
Hypergiants
absolute
magni-
tude
Once nuclear fusion of hydrogen becomes the dominant energy production process and the excess energy gained from gravitational contraction has been lost, the star lies along a curve on the Hertzsprung–Russell diagram (or HR diagram) called the standard main sequence. Astronomers will sometimes refer to this stage as "zero age main sequence", or ZAMS. The ZAMS curve can be calculated using computer models of stellar properties at the point when stars begin hydrogen fusion. From this point, the brightness and surface temperature of stars typically increase with age.
A star remains near its initial position on the main sequence until a significant amount of hydrogen in the core has been consumed, then begins to evolve into a more luminous star. (On the HR diagram, the evolving star moves up and to the right of the main sequence.) Thus the main sequence represents the primary hydrogen-burning stage of a star's lifetime.
Properties.
The majority of stars on a typical HR diagram lie along the main-sequence curve. This line is pronounced because both the spectral type and the luminosity depend only on a star's mass, at least to zeroth-order approximation, as long as it is fusing hydrogen at its core—and that is what almost all stars spend most of their "active" lives doing.
The temperature of a star determines its spectral type via its effect on the physical properties of plasma in its photosphere. A star's energy emission as a function of wavelength is influenced by both its temperature and composition. A key indicator of this energy distribution is given by the color index, "B" − "V", which measures the star's magnitude in blue ("B") and green-yellow ("V") light by means of filters. This difference in magnitude provides a measure of a star's temperature.
Dwarf terminology.
Main-sequence stars are called dwarf stars, but this terminology is partly historical and can be somewhat confusing. For the cooler stars, dwarfs such as red dwarfs, orange dwarfs, and yellow dwarfs are indeed much smaller and dimmer than other stars of those colors. However, for hotter blue and white stars, the size and brightness difference between so-called "dwarf" stars that are on the main sequence and the so-called "giant" stars that are not becomes smaller; for the hottest stars it is not directly observable. For those stars the terms "dwarf" and "giant" refer to differences in spectral lines which indicate if a star is on the main sequence or off it. Nevertheless, very hot main-sequence stars are still sometimes called dwarfs, even though they have roughly the same size and brightness as the "giant" stars of that temperature.
The common use of "dwarf" to mean main sequence is confusing in another way, because there are dwarf stars which are not main-sequence stars. For example, a white dwarf is the dead core of a star that is left after the star has shed its outer layers, that is much smaller than a main-sequence star-—roughly the size of the Earth. These represent the final evolutionary stage of many main-sequence stars.
Parameters.
By treating the star as an idealized energy radiator known as a black body, the luminosity "L" and radius "R" can be related to the effective temperature formula_1 by the Stefan–Boltzmann law:
where "σ" is the Stefan–Boltzmann constant. As the position of a star on the HR diagram shows its approximate luminosity, this relation can be used to estimate its radius.
The mass, radius and luminosity of a star are closely interlinked, and their respective values can be approximated by three relations. First is the Stefan–Boltzmann law, which relates the luminosity "L", the radius "R" and the surface temperature "Teff". Second is the mass–luminosity relation, which relates the luminosity "L" and the mass "M". Finally, the relationship between "M" and "R" is close to linear. The ratio of "M" to "R" increases by a factor of only three over 2.5 orders of magnitude of "M". This relation is roughly proportional to the star's inner temperature "TI", and its extremely slow increase reflects the fact that the rate of energy generation in the core strongly depends on this temperature, while it has to fit the mass–luminosity relation. Thus, a too high or too low temperature will result in stellar instability.
A better approximation is to take formula_2, the energy generation rate per unit mass, as ε is proportional to "TI"15, where "TI" is the core temperature. This is suitable for stars at least as massive as the Sun, exhibiting the CNO cycle, and gives the better fit "R" ∝ "M"0.78.
Sample parameters.
The table below shows typical values for stars along the main sequence. The values of luminosity ("L"), radius ("R") and mass ("M") are relative to the Sun—a dwarf star with a spectral classification of G2 V. The actual values for a star may vary by as much as 20–30% from the values listed below.
Energy generation.
All main-sequence stars have a core region where energy is generated by nuclear fusion. The temperature and density of this core are at the levels necessary to sustain the energy production that will support the remainder of the star. A reduction of energy production would cause the overlaying mass to compress the core, resulting in an increase in the fusion rate because of higher temperature and pressure. Likewise an increase in energy production would cause the star to expand, lowering the pressure at the core. Thus the star forms a self-regulating system in hydrostatic equilibrium that is stable over the course of its main sequence lifetime.
Main-sequence stars employ two types of hydrogen fusion processes, and the rate of energy generation from each type depends on the temperature in the core region. Astronomers divide the main sequence into upper and lower parts, based on which of the two is the dominant fusion process. In the lower main sequence, energy is primarily generated as the result of the proton-proton chain, which directly fuses hydrogen together in a series of stages to produce helium. Stars in the upper main sequence have sufficiently high core temperatures to efficiently use the CNO cycle. (See the chart.) This process uses atoms of carbon, nitrogen and oxygen as intermediaries in the process of fusing hydrogen into helium.
At a stellar core temperature of 18 Million Kelvin, the PP process and CNO cycle are equally efficient, and each type generates half of the star's net luminosity. As this is the core temperature of a star with about 1.5 M☉, the upper main sequence consists of stars above this mass. Thus, roughly speaking, stars of spectral class F or cooler belong to the lower main sequence, while class A stars or hotter are upper main-sequence stars. The transition in primary energy production from one form to the other spans a range difference of less than a single solar mass. In the Sun, a one solar mass star, only 1.5% of the energy is generated by the CNO cycle. By contrast, stars with 1.8 M☉ or above generate almost their entire energy output through the CNO cycle.
The observed upper limit for a main-sequence star is 120–200 M☉. The theoretical explanation for this limit is that stars above this mass can not radiate energy fast enough to remain stable, so any additional mass will be ejected in a series of pulsations until the star reaches a stable limit. The lower limit for sustained proton–proton nuclear fusion is about 0.08 M☉ or 80 times the mass of Jupiter. Below this threshold are sub-stellar objects that can not sustain hydrogen fusion, known as brown dwarfs.
Structure.
Because there is a temperature difference between the core and the surface, or photosphere, energy is transported outward. The two modes for transporting this energy are radiation and convection. A radiation zone, where energy is transported by radiation, is stable against convection and there is very little mixing of the plasma. By contrast, in a convection zone the energy is transported by bulk movement of plasma, with hotter material rising and cooler material descending. Convection is a more efficient mode for carrying energy than radiation, but it will only occur under conditions that create a steep temperature gradient.
In massive stars (above 10 M☉) the rate of energy generation by the CNO cycle is very sensitive to temperature, so the fusion is highly concentrated at the core. Consequently, there is a high temperature gradient in the core region, which results in a convection zone for more efficient energy transport. This mixing of material around the core removes the helium ash from the hydrogen-burning region, allowing more of the hydrogen in the star to be consumed during the main-sequence lifetime. The outer regions of a massive star transport energy by radiation, with little or no convection.
Intermediate mass stars such as Sirius may transport energy primarily by radiation, with a small core convection region. Medium-sized, low mass stars like the Sun have a core region that is stable against convection, with a convection zone near the surface that mixes the outer layers. This results in a steady buildup of a helium-rich core, surrounded by a hydrogen-rich outer region. By contrast, cool, very low-mass stars (below 0.4 M☉) are convective throughout. Thus the helium produced at the core is distributed across the star, producing a relatively uniform atmosphere and a proportionately longer main sequence lifespan.
Luminosity-color variation.
As non-fusing helium ash accumulates in the core of a main-sequence star, the reduction in the abundance of hydrogen per unit mass results in a gradual lowering of the fusion rate within that mass. Since it is the outflow of fusion-supplied energy that supports the higher layers of the star, the core is compressed, producing higher temperatures and pressures. Both factors increase the rate of fusion thus moving the equilibrium towards a smaller, denser, hotter core producing more energy whose increased outflow pushes the higher layers further out. Thus there is a steady increase in the luminosity and radius of the star over time. For example, the luminosity of the early Sun was only about 70% of its current value. As a star ages this luminosity increase changes its position on the HR diagram. This effect results in a broadening of the main sequence band because stars are observed at random stages in their lifetime. That is, the main sequence band develops a thickness on the HR diagram; it is not simply a narrow line.
Other factors that broaden the main sequence band on the HR diagram include uncertainty in the distance to stars and the presence of unresolved binary stars that can alter the observed stellar parameters. However, even perfect observation would show a fuzzy main sequence because mass is not the only parameter that affects a star's color and luminosity. Variations in chemical composition caused by the initial abundances, the star's evolutionary status, interaction with a close companion, rapid rotation, or a magnetic field can all slightly change a main-sequence star's HR diagram position, to name just a few factors. As an example, there are metal-poor stars (with a very low abundance of elements with higher atomic numbers than helium) that lie just below the main sequence and are known as subdwarfs. These stars are fusing hydrogen in their cores and so they mark the lower edge of main sequence fuzziness caused by variance in chemical composition.
A nearly vertical region of the HR diagram, known as the instability strip, is occupied by pulsating variable stars known as Cepheid variables. These stars vary in magnitude at regular intervals, giving them a pulsating appearance. The strip intersects the upper part of the main sequence in the region of class "A" and "F" stars, which are between one and two solar masses. Pulsating stars in this part of the instability strip that intersects the upper part of the main sequence are called Delta Scuti variables. Main-sequence stars in this region experience only small changes in magnitude and so this variation is difficult to detect. Other classes of unstable main-sequence stars, like Beta Cephei variables, are unrelated to this instability strip.
Lifetime.
The total amount of energy that a star can generate through nuclear fusion of hydrogen is limited by the amount of hydrogen fuel that can be consumed at the core. For a star in equilibrium, the energy generated at the core must be at least equal to the energy radiated at the surface. Since the luminosity gives the amount of energy radiated per unit time, the total life span can be estimated, to first approximation, as the total energy produced divided by the star's luminosity.
For a star with at least 0.5 M☉, once the hydrogen supply in its core is exhausted and it expands to become a red giant, it can start to fuse helium atoms to form carbon. The energy output of the helium fusion process per unit mass is only about a tenth the energy output of the hydrogen process, and the luminosity of the star increases. This results in a much shorter length of time in this stage compared to the main sequence lifetime. (For example, the Sun is predicted to spend 130 million years burning helium, compared to about 12 billion years burning hydrogen.) Thus, about 90% of the observed stars above 0.5 M☉ will be on the main sequence. On average, main-sequence stars are known to follow an empirical mass-luminosity relationship. The luminosity ("L") of the star is roughly proportional to the total mass ("M") as the following power law:
This relationship applies to main-sequence stars in the range 0.1–50 M☉.
The amount of fuel available for nuclear fusion is proportional to the mass of the star. Thus, the lifetime of a star on the main sequence can be estimated by comparing it to solar evolutionary models. The Sun has been a main-sequence star for about 4.5 billion years and it will become a red giant in 6.5 billion years, for a total main sequence lifetime of roughly 1010 years. Hence:
where "M" and "L" are the mass and luminosity of the star, respectively, formula_5 is a solar mass, formula_6 is the solar luminosity and formula_7 is the star's estimated main sequence lifetime.
Although more massive stars have more fuel to burn and might be expected to last longer, they also must radiate a proportionately greater amount with increased mass. Thus, the most massive stars may remain on the main sequence for only a few million years, while stars with less than a tenth of a solar mass may last for over a trillion years.
The exact mass-luminosity relationship depends on how efficiently energy can be transported from the core to the surface. A higher opacity has an insulating effect that retains more energy at the core, so the star does not need to produce as much energy to remain in hydrostatic equilibrium. By contrast, a lower opacity means energy escapes more rapidly and the star must burn more fuel to remain in equilibrium. Note, however, that a sufficiently high opacity can result in energy transport via convection, which changes the conditions needed to remain in equilibrium.
In high-mass main-sequence stars, the opacity is dominated by electron scattering, which is nearly constant with increasing temperature. Thus the luminosity only increases as the cube of the star's mass. For stars below 10 M☉, the opacity becomes dependent on temperature, resulting in the luminosity varying approximately as the fourth power of the star's mass. For very low mass stars, molecules in the atmosphere also contribute to the opacity. Below about 0.5 M☉, the luminosity of the star varies as the mass to the power of 2.3, producing a flattening of the slope on a graph of mass versus luminosity. Even these refinements are only an approximation, however, and the mass-luminosity relation can vary depending on a star's composition.
Evolutionary tracks.
Once a main-sequence star consumes the hydrogen at its core, the loss of energy generation causes its gravitational collapse to resume. Stars with less than 0.23 M☉, are predicted to directly become white dwarfs once energy generation by nuclear fusion of hydrogen at their core comes to a halt. In stars between this threshold and 10 M☉, the hydrogen surrounding the helium core reaches sufficient temperature and pressure to undergo fusion, forming a hydrogen-burning shell. In consequence of this change, the outer envelope of the star expands and decreases in temperature, turning it into a red giant. At this point the star is evolving off the main sequence and entering the giant branch. The path which the star now follows across the HR diagram, to the upper right of the main sequence, is called an evolutionary track.
The helium core of a red giant continues to collapse until it is entirely supported by electron degeneracy pressure—a quantum mechanical effect that restricts how closely matter can be compacted. For stars of more than about 0.5 M☉, 
the core eventually reaches a temperature where it becomes hot enough to burn helium into carbon via the triple alpha process. 
Stars with more than 5–7.5 M☉ can additionally fuse elements with higher atomic numbers. 
For stars with ten or more solar masses, this process can lead to an increasingly dense core that finally collapses, ejecting the star's overlying layers in a Type II supernova explosion, Type Ib supernova or Type Ic supernova.
When a cluster of stars is formed at about the same time, the life span of these stars will depend on their individual masses. The most massive stars will leave the main sequence first, followed steadily in sequence by stars of ever lower masses. Thus the stars will evolve in order of their position on the main sequence, proceeding from the most massive at the left toward the right of the HR diagram. The current position where stars in this cluster are leaving the main sequence is known as the turn-off point. By knowing the main sequence lifespan of stars at this point, it becomes possible to estimate the age of the cluster.
Suggested reading.
General.
Kippenhahn, Rudolf, "100 Billion Suns", Basic Books, New York, 1983.
Technical.
1. Arnett, David, "Supernovae and Nucleosynthesis", Princeton University Press, Princeton, 1996.
2. Bahcall, John N., "Neutrino Astrophysics", Cambridge University Press, Cambridge, 1989.
3. Bahcall, John N., Pinsonneault, M.H., and Basu, Sarbani, "Solar Models: Current Epoch and Time Dependences, Neutrinos, and Helioseismological Properties," The Astrophysical Journal, 555, 990, 2001.
4. Barnes, C. A., Clayton, D. D., and Schramm, D. N.(eds.), "Essays in Nuclear Astrophysics", Cambridge University Press, Cambridge, 1982.
5. Bowers, Richard L., and Deeming, Terry, "Astrophysics I: Stars", Jones and Bartlett, Publishers, Boston, 1984.
6. Chabrier, Gilles, and Baraffe, Isabelle, ""Theory of Low-Mass Stars and Substellar Objects," Annual Review of Astronomy and Astrophysics", 38, 337, 2000.
7. Chandrasekhar, S., "An Introduction to the study of stellar Structure", Dover Publications, Inc., New York, 1967.
8. Clayton, Donald D., "Principles of Stellar Evolution and Nucleosynthesis", University of Chicago Press, Chicago, 1983.
9. Cox, J. P., and Giuli, R. T., "Principles of Stellar Structure", Gordon and Breach, New York, 1968.
10. Fowler, William ., Caughlan, Georgeanne R., and Zimmerman, Barbara A., ""Thermonuclear Reaction Rates, I," Annual Review of Astronomy and Astrophysics", 5, 525, 1967.
11. Fowler, William A., Caughlan, Georgeanne R., and Zimmerman, Barbara A., ""Thermonuclear Reaction Rates, II, " Annual Review of Astronomy and Astrophysics", 13, 69, 1975.
12. Hansen, Carl J., Kawaler, Steven D., and Trimble, "Virginia Stellar Interiors: Physical Principles, Structure, and Evolution, Second Edition", Springer-Verlag, New York, 2004.
13. Harris, Michael J., Fowler, William A., Caughlan, Georgeanne R., and Zimmerman, Barbara A., ""Thermonuclear Reaction Rates, III," Annual Review of Astronomy and Astrophysics", 21, 165, 1983.
14. Iben, Icko, Jr, ""Stellar Evolution Within and Off the Main Sequence," Annual Review of Astronomy and Astrophysics", 5, 571, 1967.
15. Iglesias, Carlos A, and Rogers, Forrest J., ""Updated Opal Opacities," The Astrophysical Journal", 464, 943, 1996.
16. Kippenhahn, Rudolf, and Weigert, Alfred, "Stellar Structure and Evolution", Springer-Verlag, Berlin, 1990.
17. Liebert, James, and Probst, Ronald G., ""Very Low Mass Stars," Annual Review of Astronomy and Astrophysics", 25, 437, 1987.
18. Padmanabhan, T., "Theoretical Astrophysics", Cambridge University Press, Cambridge, 2002.
19. Prialnik, Dina, "An Introduction to the Theory of Stellar Structure and Evolution", Cambridge University Press, Cambridge, 2000.
20. Novotny, Eva, "Introduction to Stellar Atmospheres and Interior", Oxford University Press, New York, 1973.
Shore, Steven N., "The Tapestry of Modern Astrophysics", John Wiley AND Sons, Hoboken, 2003.

</doc>
<doc id="19609" url="http://en.wikipedia.org/wiki?curid=19609" title="Memory leak">
Memory leak

In computer science, a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that memory which is no longer needed is not released. In object-oriented programming, a memory leak may happen when an object is stored in memory but cannot be accessed by the running code. A memory leak has symptoms similar to a number of other problems (see below) and generally can only be diagnosed by a programmer with access to the program's source code. 
Because they can exhaust available system memory as an application runs, memory leaks are often the cause of or a contributing factor to software aging.
Consequences.
A memory leak can diminish the performance of the computer by reducing the amount of available memory. Eventually, in the worst case, too much of the available memory may become allocated and all or part of the system or device stops working correctly, the application fails, or the system slows down unacceptably due to thrashing.
Memory leaks may not be serious or even detectable by normal means. In modern operating systems, normal memory used by an application is released when the application terminates. This means that a memory leak in a program that only runs for a short time may not be noticed and is rarely serious.
Much more serious leaks include those:
An example of memory leak.
The following example, written in pseudocode, is intended to show how a memory leak can come about, and its effects, without needing any programming knowledge. The program in this case is part of some very simple software designed to control an elevator. This part of the program is run whenever anyone inside the elevator presses the button for a floor.
 When a button is pressed:
 Get some memory, which will be used to remember the floor number
 Put the floor number into the memory
 Are we already on the target floor?
 If so, we have nothing to do: finished
 Otherwise:
 Wait until the lift is idle
 Go to the required floor
 Release the memory we used to remember the floor number
The memory leak would occur if the floor number requested is the same floor that the lift is on; the condition for releasing the memory would be skipped. Each time this case occurs, more memory is leaked.
Cases like this wouldn't usually have any immediate effects. People do not often press the button for the floor they are already on, and in any case, the lift might have enough spare memory that this could happen hundreds or thousands of times. However, the lift will eventually run out of memory. This could take months or years, so it might not be discovered despite thorough testing.
The consequences would be unpleasant; at the very least, the lift would stop responding to requests to move to another floor. If other parts of the program need memory (a part assigned to open and close the door, for example), then someone may be trapped inside, since the software cannot open the door.
The memory leak lasts until the system is reset. For example: if the lift's power were turned off, the program would stop running. When power was turned on again, the program would restart and all the memory would be available again, but the slow process of memory leak would restart together with the program, eventually prejudicing the correct running of the system.
Programming issues.
Memory leaks are a common error in programming, especially when using languages that have no built in automatic garbage collection, such as C and C++. Typically, a memory leak occurs because dynamically allocated memory has become unreachable. The prevalence of memory leak bugs has led to the development of a number of debugging tools to detect unreachable memory. "IBM Rational Purify", "BoundsChecker", "Valgrind", "Parasoft Insure++", "Dr. Memory" and "memwatch" are some of the more popular memory debuggers for C and C++ programs. "Conservative" garbage collection capabilities can be added to any programming language that lacks it as a built-in feature, and libraries for doing this are available for C and C++ programs. A conservative collector finds and reclaims most, but not all, unreachable memory.
Although the memory manager can recover unreachable memory, it cannot free memory that is still reachable and therefore potentially still useful. Modern memory managers therefore provide techniques for programmers to semantically mark memory with varying levels of usefulness, which correspond to varying levels of "reachability". The memory manager does not free an object that is strongly reachable. An object is strongly reachable if it is reachable either directly by a strong reference or indirectly by a chain of strong references. (A "strong reference" is a reference that, unlike a weak reference, prevents an object from being garbage collected.) To prevent this, the developer is responsible for cleaning up references after use, typically by setting the reference to null once it is no longer needed and, if necessary, by deregistering any event listeners that maintain strong references to the object.
In general, automatic memory management is more robust and convenient for developers, as they don't need to implement freeing routines or worry about the sequence in which cleanup is performed or be concerned about whether or not an object is still referenced. It is easier for a programmer to know when a reference is no longer needed than to know when an object is no longer referenced. However, automatic memory management can impose a performance overhead, and it does not eliminate all of the programming errors that cause memory leaks.
RAII.
RAII, short for Resource Acquisition Is Initialization, is an approach to the problem commonly taken in C++, D, and Ada. It involves associating scoped objects with the acquired resources, and automatically releasing the resources once the objects are out of scope. Unlike garbage collection, RAII has the advantage of knowing when objects exist and when they do not. Compare the following C and C++ examples:
The C version, as implemented in the example, requires explicit deallocation; the array is dynamically allocated (from the heap in most C implementations), and continues to exist until explicitly freed.
The C++ version requires no explicit deallocation; it will always occur automatically as soon as the object codice_1 goes out of scope, including if an exception is thrown. This avoids some of the overhead of garbage collection schemes. And because object destructors can free resources other than memory, RAII helps to prevent the leaking of input and output resources accessed through a handle, which mark-and-sweep garbage collection does not handle gracefully. These include open files, open windows, user notifications, objects in a graphics drawing library, thread synchronisation primitives such as critical sections, network connections, and connections to the Windows Registry or another database.
However, using RAII correctly is not always easy and has its own pitfalls. For instance, if one is not careful, it is possible to create dangling pointers (or references) by returning data by reference, only to have that data be deleted when its containing object goes out of scope.
D uses a combination of RAII and garbage collection, employing automatic destruction when it is clear that an object cannot be accessed outside its original scope, and garbage collection otherwise.
Reference counting and cyclic references.
More modern garbage collection schemes are often based on a notion of reachability – if you don't have a usable reference to the memory in question, it can be collected. Other garbage collection schemes can be based on reference counting, where an object is responsible for keeping track of how many references are pointing to it. If the number goes down to zero, the object is expected to release itself and allow its memory to be reclaimed. The flaw with this model is that it doesn't cope with cyclic references, and this is why nowadays most programmers are prepared to accept the burden of the more costly mark and sweep type of systems.
The following Visual Basic code illustrates the canonical reference-counting memory leak:
In practice, this trivial example would be spotted straight away and fixed. In most real examples, the cycle of references spans more than two objects, and is more difficult to detect.
A well-known example of this kind of leak came to prominence with the rise of AJAX programming techniques in web browsers in the lapsed listener problem. JavaScript code which associated a DOM element with an event handler, and failed to remove the reference before exiting, would leak memory (AJAX web pages keep a given DOM alive for a lot longer than traditional web pages, so this leak was much more apparent).
Effects.
If a program has a memory leak and its memory usage is steadily increasing, there will not usually be an immediate symptom. Every physical system has a finite amount of memory, and if the memory leak is not contained (for example, by restarting the leaking program) it will sooner or later start to cause problems.
Most modern consumer desktop operating systems have both main memory which is physically housed in RAM microchips, and secondary storage such as a hard drive. Memory allocation is dynamic – each process gets as much memory as it requests. Active pages are transferred into main memory for fast access; inactive pages are pushed out to secondary storage to make room, as needed. When a single process starts consuming a large amount of memory, it usually occupies more and more of main memory, pushing other programs out to secondary storage – usually significantly slowing performance of the system. Even if the leaking program is terminated, it may take some time for other programs to swap back into main memory, and for performance to return to normal.
When all the memory on a system is exhausted (whether there is virtual memory or only main memory, such as on an embedded system) any attempt to allocate more memory will fail. This usually causes the program attempting to allocate the memory to terminate itself, or to generate a segmentation fault. Some programs are designed to recover from this situation (possibly by falling back on pre-reserved memory). The first program to experience the out-of-memory may or may not be the program that has the memory leak.
Some multi-tasking operating systems have special mechanisms to deal with an out-of-memory condition, such as killing processes at random (which may affect "innocent" processes), or killing the largest process in memory (which presumably is the one causing the problem). Some operating systems have a per-process memory limit, to prevent any one program from hogging all of the memory on the system. The disadvantage to this arrangement is that the operating system sometimes must be re-configured to allow proper operation of programs that legitimately require large amounts of memory, such as those dealing with graphics, video, or scientific calculations.
If the memory leak is in the kernel, the operating system itself will likely fail. Computers without sophisticated memory management, such as embedded systems, may also completely fail from a persistent memory leak.
Publicly accessible systems such as web servers or routers are prone to denial-of-service attacks if an attacker discovers a sequence of operations which can trigger a leak. Such a sequence is known as an exploit.
A "sawtooth" pattern of memory utilization may be an indicator of a memory leak if the vertical drops coincide with reboots or application restarts. Care should be taken though because garbage collection points could also cause such a pattern and would show a healthy usage of the heap.
Other memory consumers.
Note that constantly increasing memory usage is not necessarily evidence of a memory leak. Some applications will store ever increasing amounts of information in memory (e.g. as a cache). If the cache can grow so large as to cause problems, this may be a programming or design error, but is not a memory leak as the information remains nominally in use. In other cases, programs may require an unreasonably large amount of memory because the programmer has assumed memory is always sufficient for a particular task; for example, a graphics file processor might start by reading the entire contents of an image file and storing it all into memory, something that is not viable where a very large image exceeds available memory.
To put it another way, a memory leak arises from a particular kind of programming error, and without access to the program code, someone seeing symptoms can only guess that there "might" be a memory leak. It would be better to use terms such as "constantly increasing memory use" where no such inside knowledge exists.
A simple example in C.
The following C function deliberately leaks memory by losing the pointer to the allocated memory. The leak can be said to occur as soon as the pointer 'a' goes out of scope, i.e. when function_which_allocates() returns without freeing 'a'.

</doc>
<doc id="19614" url="http://en.wikipedia.org/wiki?curid=19614" title="Molecular orbital">
Molecular orbital

In chemistry, a molecular orbital (or MO) is a mathematical function describing the wave-like behavior of an electron in a molecule. This function can be used to calculate chemical and physical properties such as the probability of finding an electron in any specific region. The term "orbital" was introduced by Robert S. Mulliken in 1932 as an abbreviation for "one-electron orbital wave function". At an elementary level, it is used to describe the "region" of space in which the function has a significant amplitude. Molecular orbitals are usually constructed by combining atomic orbitals or hybrid orbitals from each atom of the molecule, or other molecular orbitals from groups of atoms. They can be quantitatively calculated using the Hartree–Fock or self-consistent field (SCF) methods.
Overview.
A molecular orbital (MO) can be used to represent the regions in a molecule where an electron occupying that orbital is likely to be found. Molecular orbitals are obtained from the combination of atomic orbitals, which predict the location of an electron in an atom. A molecular orbital can specify the electron configuration of a molecule: the spatial distribution and energy of one (or one pair of) electron(s). Most commonly an MO is represented as a linear combination of atomic orbitals (the LCAO-MO method), especially in qualitative or very approximate usage. They are invaluable in providing a simple model of bonding in molecules, understood through molecular orbital theory.
Most present-day methods in computational chemistry begin by calculating the MOs of the system. A molecular orbital describes the behavior of one electron in the electric field generated by the nuclei and some average distribution of the other electrons. In the case of two electrons occupying the same orbital, the Pauli principle demands that they have opposite spin. Necessarily this is an approximation, and highly accurate descriptions of the molecular electronic wave function do not have orbitals (see configuration interaction).
Formation of molecular orbitals.
Molecular orbitals arise from allowed interactions between atomic orbitals, which are allowed if the symmetries (determined from group theory) of the atomic orbitals are compatible with each other. Efficiency of atomic orbital interactions is determined from the overlap (a measure of how well two orbitals constructively interact with one another) between two atomic orbitals, which is significant if the atomic orbitals are close in energy. Finally, the number of molecular orbitals that form must equal the number of atomic orbitals in the atoms being combined to form the molecule.
Qualitative discussion.
For an imprecise, but qualitatively useful, discussion of the molecular structure, the molecular orbitals can be obtained from the "Linear combination of atomic orbitals molecular orbital method" ansatz. Here, the molecular orbitals are expressed as linear combinations of atomic orbitals.
Linear combinations of atomic orbitals (LCAO).
Molecular orbitals were first introduced by Friedrich Hund and Robert S. Mulliken in 1927 and 1928. The linear combination of atomic orbitals or "LCAO" approximation for molecular orbitals was introduced in 1929 by Sir John Lennard-Jones. His ground-breaking paper showed how to derive the electronic structure of the fluorine and oxygen molecules from quantum principles. This qualitative approach to molecular orbital theory is part of the start of modern quantum chemistry.
Linear combinations of atomic orbitals (LCAO) can be used to estimate the molecular orbitals that are formed upon bonding between the molecule's constituent atoms. Similar to an atomic orbital, a Schrödinger equation, which describes the behavior of an electron, can be constructed for a molecular orbital as well. Linear combinations of atomic orbitals, or the sums and differences of the atomic wavefunctions, provide approximate solutions to the Hartree–Fock equations which correspond to the independent-particle approximation of the molecular Schrödinger equation. For simple diatomic molecules, the wavefunctions obtained are represented mathematically by the equations
where formula_3 and formula_4 are the molecular wavefunctions for the bonding and antibonding molecular orbitals, respectively, formula_5 and formula_6 are the atomic wavefunctions from atoms a and b, respectively, and formula_7 and formula_8 are adjustable coefficients. These coefficients can be positive or negative, depending on the energies and symmetries of the individual atomic orbitals. As the two atoms become closer together, their atomic orbitals overlap to produce areas of high electron density, and, as a consequence, molecular orbitals are formed between the two atoms. The atoms are held together by the electrostatic attraction between the positively charged nuclei and the negatively charged electrons occupying bonding molecular orbitals.
Bonding, antibonding, and nonbonding MOs.
When atomic orbitals interact, the resulting molecular orbital can be of three types: bonding, antibonding, or nonbonding.
Bonding MOs:
Antibonding MOs: 
Nonbonding MOs: 
Sigma and pi labels for MOs.
The type of interaction between atomic orbitals can be further categorized by the molecular-orbital symmetry labels σ (sigma), π (pi), δ (delta), φ (phi), γ (gamma) etc. paralleling the symmetry of the atomic orbitals s, p, d, f and g. The number of nodal planes containing the internuclear axis between the atoms concerned is zero for σ MOs, one for π, two for δ, etc.
σ symmetry.
A MO with σ symmetry results from the interaction of either two atomic s-orbitals or two atomic pz-orbitals. An MO will have σ-symmetry if the orbital is symmetrical with respect to the axis joining the two nuclear centers, the internuclear axis. This means that rotation of the MO about the internuclear axis does not result in a phase change. A σ* orbital, sigma antibonding orbital, also maintains the same phase when rotated about the internuclear axis. The σ* orbital has a nodal plane that is between the nuclei and perpendicular to the internuclear axis.
π symmetry.
A MO with π symmetry results from the interaction of either two atomic px orbitals or py orbitals. An MO will have π symmetry if the orbital is asymmetrical with respect to rotation about the internuclear axis. This means that rotation of the MO about the internuclear axis will result in a phase change. There is one nodal plane containing the internuclear axis, if real orbitals are considered.
A π* orbital, pi antibonding orbital, will also produce a phase change when rotated about the internuclear axis. The π* orbital also has a second nodal plane between the nuclei.
δ symmetry.
A MO with δ symmetry results from the interaction of two atomic dxy or dx2-y2 orbitals. Because these molecular orbitals involve low-energy d atomic orbitals, they are seen in transition-metal complexes. A δ bonding orbital has two nodal planes containing the internuclear axis, and a δ* antibonding orbital also has a third nodal plane between the nuclei.
φ symmetry.
Theoretical chemists have conjectured that higher-order bonds, such as phi bonds corresponding to overlap of f atomic orbitals, are possible. There is as of 2005 only one known example of a molecule purported to contain a phi bond (a U−U bond, in the molecule U2).
Gerade and ungerade symmetry.
For molecules that possess a center of inversion (centrosymmetric molecules) there are additional labels of symmetry that can be applied to molecular orbitals.
Centrosymmetric molecules include:
Non-centrosymmetric molecules include:
If inversion through the center of symmetry in a molecule results in the same phases for the molecular orbital, then the MO is said to have gerade (g) symmetry, from the German word for even.
If inversion through the center of symmetry in a molecule results in a phase change for the molecular orbital, then the MO is said to have ungerade (u) symmetry, from the German word for odd.
For a bonding MO with σ-symmetry, the orbital is σg (s' + s" is symmetric), while an antibonding MO with σ-symmetry the orbital is σu, because inversion of s' – s" is antisymmetric.
For a bonding MO with π-symmetry the orbital is πu because inversion through the center of symmetry for would produce a sign change (the two p atomic orbitals are in phase with each other but the two lobes have opposite signs), while an antibonding MO with π-symmetry is πg because inversion through the center of symmetry for would not produce a sign change (the two p orbitals are antisymmetric by phase).
MO diagrams.
The qualitative approach of MO analysis uses a molecular orbital diagram to visualize bonding interactions in a molecule. In this type of diagram, the molecular orbitals are represented by horizontal lines; the higher a line the higher the energy of the orbital, and degenerate orbitals are placed on the same level with a space between them. Then, the electrons to be placed in the molecular orbitals are slotted in one by one, keeping in mind the Pauli exclusion principle and Hund's rule of maximum multiplicity (only 2 electrons, having opposite spins, per orbital; place as many unpaired electrons on one energy level as possible before starting to pair them). For more complicated molecules, the wave mechanics approach loses utility in a qualitative understanding of bonding (although is still necessary for a quantitative approach). 
Some properties:
The general procedure for constructing a molecular orbital diagram for a reasonably simple molecule can be summarized as follows:
1. Assign a point group to the molecule.
2. Look up the shapes of the SALCs.
3. Arrange the SALCs of each molecular fragment in increasing order of energy, first noting whether they stem from "s", "p", or "d" orbitals 
(and put them in the order "s" < "p" < "d"), and then their number of internuclear nodes.
4. Combine SALCs of the same symmetry type from the two fragments, and from N SALCs form N molecular orbitals.
5. Estimate the relative energies of the molecular orbitals from considerations of overlap and relative energies of the parent orbitals, and draw the levels on a molecular orbital energy level diagram (showing the origin of the orbitals).
6. Confirm, correct, and revise this qualitative order by carrying out a molecular orbital calculation by using commercial software.
Bonding in molecular orbitals.
Orbital degeneracy.
Molecular orbitals are said to be degenerate if they have the same energy. For example, in the homonuclear diatomic molecules of the first ten elements, the molecular orbitals derived from the px and the py atomic orbitals result in two degenerate bonding orbitals (of low energy) and two degenerate antibonding orbitals (of high energy).
Ionic bonds.
When the energy difference between the atomic orbitals of two atoms is quite large, one atom's orbitals contribute almost entirely to the bonding orbitals, and the others atom's orbitals contribute almost entirely to the antibonding orbitals. Thus, the situation is effectively that some electrons have been transferred from one atom to the other. This is called an (mostly) ionic bond.
Bond order.
The bond order, or number of bonds, of a molecule can be determined by combining the number of electrons in bonding and antibonding molecular orbitals. A pair of electrons in a bonding orbital creates a bond, whereas a pair of electrons in an antibonding orbital negates a bond. For example, N2, with eight electrons in bonding orbitals and two electrons in antibonding orbitals, has a bond order of three, which constitutes a triple bond.
Bond strength is proportional to bond order—a greater amount of bonding produces a more stable bond—and bond length is inversely proportional to it—a stronger bond is shorter.
There are rare exceptions to the requirement of molecule having a positive bond order. Although Be2 has a bond order of 0 according to MO analysis, there is experimental evidence of a highly unstable Be2 molecule having a bond length of 245 pm and bond energy of 10 kJ/mol.
HOMO and LUMO.
The highest occupied molecular orbital and lowest unoccupied molecular orbital are often referred to as the HOMO and LUMO, respectively. The difference of the energies of the HOMO and LUMO, termed the band gap, can sometimes serve as a measure of the excitability of the molecule: The smaller the energy the more easily it will be excited.
Molecular orbital examples.
Homonuclear diatomics.
Homonuclear diatomic MOs contain equal contributions from each atomic orbital in the basis set. This is shown in the homonuclear diatomic MO diagrams for H2, He2, and Li2, all of which containing symmetric orbitals.
H2.
As a simple MO example consider the hydrogen molecule, H2 (see molecular orbital diagram), with the two atoms labelled H' and H". The lowest-energy atomic orbitals, 1s' and 1s", do not transform according to the symmetries of the molecule. However, the following symmetry adapted atomic orbitals do:
The symmetric combination (called a bonding orbital) is lower in energy than the basis orbitals, and the antisymmetric combination (called an antibonding orbital) is higher. Because the H2 molecule has two electrons, they can both go in the bonding orbital, making the system lower in energy (and, hence, more stable) than two free hydrogen atoms. This is called a covalent bond. The bond order is equal to the number of bonding electrons minus the number of antibonding electrons, divided by 2. In this example, there are 2 electrons in the bonding orbital and none in the antibonding orbital; the bond order is 1, and there is a single bond between the two hydrogen atoms.
He2.
On the other hand, consider the hypothetical molecule of He2 with the atoms labeled He' and He". As with H2, the lowest-energy atomic orbitals are the 1s' and 1s", and do not transform according to the symmetries of the molecule, while the symmetry adapted atomic orbitals do. The symmetric combination—the bonding orbital—is lower in energy than the basis orbitals, and the antisymmetric combination—the antibonding orbital—is higher. Unlike H2, with two valence electrons, He2 has four in its neutral ground state. Two electrons fill the lower-energy bonding orbital, σg(1s), while the remaining two fill the higher-energy antibonding orbital, σu*(1s). Thus, the resulting electron density around the molecule does not support the formation of a bond between the two atoms; without a stable bond holding the atoms together, molecule would not be expected to exist. Another way of looking at it is that there are two bonding electrons and two antibonding electrons; therefore, the bond order is 0 and no bond exists (the molecule has one bound state supported by the Van der Waals potential).
Li2.
Dilithium Li2 is formed from the overlap of the 1s and 2s atomic orbitals (the basis set) of two Li atoms. Each Li atom contributes three electrons for bonding interactions, and the six electrons fill the three MOs of lowest energy, σg(1s), σu*(1s), and σg(2s). Using the equation for bond order, it is found that dilithium has a bond order of one, a single bond.
Noble gases.
Considering a hypothetical molecule of He2, since the basis set of atomic orbitals is the same as in the case of H2, we find that both the bonding and antibonding orbitals are filled, so there is no energy advantage to the pair. HeH would have a slight energy advantage, but not as much as H2 + 2 He, so the molecule is very unstable and exists only briefly before decomposing into hydrogen and helium. In general, we find that atoms such as He that have full energy shells rarely bond with other atoms. Except for short-lived Van der Waals complexes, there are very few noble gas compounds known.
Heteronuclear diatomics.
While MOs for homonuclear diatomic molecules contain equal contributions from each interacting atomic orbital, MOs for heteronuclear diatomics contain different atomic orbital contributions. Orbital interactions to produce bonding or antibonding orbitals in heteronuclear diatomics occur if there is sufficient overlap between atomic orbitals as determined by their symmetries and similarity in orbital energies.
HF.
In hydrogen fluoride HF overlap between the H 1s and F 2s orbitals is allowed by symmetry but the difference in energy between the two atomic orbitals prevents them from interacting to create a molecular orbital. Overlap between the H 1s and F 2pz orbitals is also symmetry allowed, and these two atomic orbitals have a small energy separation. Thus, they interact, leading to creation of σ and σ* MOs and a molecule with a bond order of 1. Since HF is a non-centrosymmetric molecule, the symmetry labels g and u do not apply to its molecular orbitals.
Quantitative approach.
To obtain quantitative values for the molecular energy levels, one needs to have molecular orbitals that are such that the configuration interaction (CI) expansion converges fast towards the full CI limit. The most common method to obtain such functions is the Hartree–Fock method, which expresses the molecular orbitals as eigenfunctions of the Fock operator. One usually solves this problem by expanding the molecular orbitals as linear combinations of Gaussian functions centered on the atomic nuclei (see linear combination of atomic orbitals and basis set (chemistry)). The equation for the coefficients of these linear combinations is a generalized eigenvalue equation known as the Roothaan equations, which are in fact a particular representation of the Hartree–Fock equation. There are a number of programs in which quantum chemical calculations of MOs can be performed, including Spartan and HyperChem.
Simple accounts often suggest that experimental molecular orbital energies can be obtained by the methods of ultra-violet photoelectron spectroscopy for valence orbitals and X-ray photoelectron spectroscopy for core orbitals. This, however, is incorrect as these experiments measure the ionization energy, the difference in energy between the molecule and one of the ions resulting from the removal of one electron. Ionization energies are linked approximately to orbital energies by Koopmans' theorem. While the agreement between these two values can be close for some molecules, it can be very poor in other cases.

</doc>
<doc id="19615" url="http://en.wikipedia.org/wiki?curid=19615" title="Systems Concepts">
Systems Concepts

Systems Concepts (now the SC Group) is a company co-founded by Stewart Nelson and Mike Levitt focused on making hardware products related to the DEC PDP-10 series of computers. One of its major products was the SA-10, an interface which allowed PDP-10s to be connected to disk and tape drives designed for use with the channel interfaces of IBM mainframes.
Later, Systems Concepts attempted to produce a compatible replacement for the DEC PDP-10 computers. "Mars" was the code name for a family of PDP-10-compatible computers built by Systems Concepts, including the initial SC-30M, the smaller SC-25, and the slower SC-20. These machines were marvels of engineering design; although not much slower than the unique Foonly F-1, they were physically smaller and consumed less power than the much slower DEC KS10 or Foonly F-2, F-3, or F-4 machines. They were also completely compatible with the DEC KL10, and ran all KL10 binaries (including the operating system) with no modifications at about 2-3 times faster than a KL10.
When DEC cancelled the Jupiter project in 1983, Systems Concepts hoped to sell their machine to customers with a software investment in PDP-10s. Their spring 1984 announcement generated excitement in the PDP-10 world. TOPS-10 was running on the Mars by the summer of 1984, and TOPS-20 by early fall. However, people at Systems Concepts were better at designing machines than at mass-producing or selling them; the company continually improved the design, but lost credibility as delivery dates continued to slip. They also overpriced; believing they were competing with the KL10 and VAX 8600 and not startups such as Sun Microsystems building workstations with comparable power at a fraction of the price. By the time SC shipped the first SC-30M to Stanford University in late 1985, most customers had already abandoned the PDP-10, usually for VMS or Unix systems. Nevertheless, a number were purchased by CompuServe, which depended on PDP-10s to run its online service and was eager to move to newer but fully compatible systems. CompuServe's demand for the computers outpaced Systems Concepts' ability to produce them, so CompuServe licensed the design and built SC-designed computers itself.
Other companies that purchased the SC-30 machines included Telmar, Reynolds and Reynolds, The Danish National Railway.
Peter Samson was director of marketing and program development.
SC later designed the SC-40, released in 1993, a faster follow-on to the SC-30M and SC-25. It can perform up to 8 times as fast as a DEC KL-10, and it also supports more physical memory, a larger virtual address space, and more modern input/output devices. These systems were also used at CompuServe.
In 1985, the company contracted to engineer and produce a PC-based cellular automata system for Tommaso Toffoli of MIT, called the CAM-6. The CAM-6 was a 2-card "sandwich" that plugged into an IBM PC slot and ran cellular automata rules at a 60 Hz update rate. Toffoli provided Forth-based software to operate the card. The production problems that plagued the company's computer products were demonstrated here as well, and only a few boards were produced.
Systems Concepts remains in business, having changed its name to the SC Group when it moved from California to Nevada.
External links.
"This article is based in part on the Jargon File, which is in the public domain."

</doc>
<doc id="19616" url="http://en.wikipedia.org/wiki?curid=19616" title="Messiah">
Messiah

A messiah (literally, "anointed one") has come to be seen as a saviour or liberator of a group of people, most commonly in the Abrahamic religions. In the Hebrew Bible, a Hebrew: מָשִׁיחַ,  "mashiaẖ",  "māšîăḥ" ("messiah") is a king or High Priest traditionally anointed with holy anointing oil. However, anointed ones ("messiahs") were not exclusively Jewish, as the Hebrew Bible refers to Cyrus the Great, king of Persia, as a messiah for his decree to rebuild the Jerusalem Temple.
The concept of messianism originated in Judaism. According to Jewish tradition, the Jewish Messiah, HaMashiach ( המשיח , "the Messiah", "the anointed one"), often referred to as "King Messiah" (מלך המשיח, "melekh mashiach"), is plainly distinct from the concept of a divine Christian Messiah or any other concept of a messiah in other Abrahamic religions, especially the Islamic. The future Jewish Messiah to come is thought to be a human leader, physically descended from the paternal Davidic line through King David and King Solomon. He is thought to accomplish predetermined things in only one future arrival, including the unification of the tribes of Israel, the gathering in of all Jews to Eretz Israel, the rebuilding of the Temple in Jerusalem, the ushering in of a Messianic Age of global universal peace, and the annunciation of the World to Come.
The translation of the Hebrew word מָשִׁיחַ "mašíaḥ" as Χριστός ("Khristós", from whence the term "Christ" comes) in the Greek Septuagint became the accepted Christian designation and title of Jesus of Nazareth. Christians believe that messianic prophecies in the Christian Old Testament (especially Isaiah) refer to a spiritual savior and believe that Jesus was the Christian Messiah ("Khristós", "Christ").
Islamic tradition holds that Jesus, the son of Mary, was a Prophet and a "Masîḥ" (مسيح) (messiah) sent to the Israelites, and that he will return to Earth at the end of times, along with the "Mahdi", and defeat "al-Masih ad-Dajjal", the "false Messiah" or Antichrist.
Etymology.
Messiah (Hebrew: מָשִׁיחַ,  "Mashiaẖ",  "Māšîăḥ"; in modern Jewish texts in English spelled "Mashiach"; Aramaic: משיחא‎, Greek: Μεσσίας, Classical Syriac: ܡܫܺܝܚܳܐ, Məšîḥā, Arabic: المسيح‎, al-Masīḥ, Latin: "Messias") literally means "anointed one". In Hebrew, the Messiah is often referred to as מלך המשיח (Meleḵ ha-Mašīaḥ in the Tiberian vocalization, ], literally meaning "the Anointed King."
The Greek Septuagint version of the Old Testament renders all thirty-nine instances of the Hebrew word for "anointed" ("Mašíaḥ") as Χριστός ("Khristós"). The New Testament records the Greek transliteration Μεσσίας, "Messias" twice in John.
"al-Masīḥ" (proper name, ]) is the Arabic word for messiah. In modern Arabic, it is used as one of the many titles of Jesus. "Masīḥ" is used by Arab Christians as well as Muslims, and is written as "Yasūʿ al-Masih" (يسوع المسيح ) by Arab Christians or "ʿĪsā al-Masīḥ" (عيسى المسيح) by Muslims. The word "al-Masīḥ" literally means "the anointed", "the traveller", or the "one who cures by caressing". In Qu'ranic scripture, Jesus is mentioned as having been sent down by Allah, strengthened by the holy spirit, and hence, 'anointed' with the task of being a prophet and a "recipient of sacred scripture". The Israelites, to whom Isa was sent, had a traditional practice of anointing their kings with oil. An Imam Bukhari hadith describes Jesus as having wet hair that looked as if water was dripping from it, possibly meaning he was naturally anointed. Muslims believe that this is just one of the many signs that proves that Jesus is the Messiah.
Judaism.
The literal translation of the Hebrew word "mashiach" (messiah) is "anointed", which refers to a ritual of consecrating someone or something by putting holy oil upon it. It is used throughout the Hebrew Bible in reference to a wide variety of individuals and objects; for example, a Jewish king, Jewish priests, and prophets, the Jewish Temple and its utensils, unleavened bread, and a non-Jewish king (Cyrus king of Persia).
In Jewish eschatology, the term came to refer to a future Jewish king from the Davidic line, who will be "anointed" with holy anointing oil, to be king of God's kingdom, and rule the Jewish people during the Messianic Age. In Judaism, the Messiah is not considered to be God or a pre-existent divine Son of God. He is considered to be a great political leader that has descended from King David. That is why he is referred to as Messiah ben David, which means "Messiah, son of David". The messiah, in Judaism, is considered to be a great, charismatic leader that is well oriented with the laws that are followed in Judaism. He will be the one who will not "judge by what his eyes see" or "decide by what his ears hear".
Belief in the eventual coming of a future messiah is a fundamental part of Judaism, and is one of Maimonides' 13 Principles of Faith.
Maimonides describes the identity of the Messiah in the following terms:
And if a king shall arise from among the House of David, studying Torah and occupied with commandments like his father David, according to the written and oral Torah, and he will impel all of Israel to follow it and to strengthen breaches in its observance, and will fight God's wars, this one is to be treated as if he were the anointed one. If he succeeded and built the Holy Temple in its proper place and gathered the dispersed ones of Israel together, this is indeed the anointed one for certain, and he will mend the entire world to worship the Lord together, as it is stated: "For then I shall turn for the nations a clear tongue, so that they will all proclaim the Name of the Lord, and to worship Him with a united resolve (Zephaniah 3:9)."
Even though the eventual coming of the messiah is a strongly upheld idea in Judaism, trying to predict the actual time when the messiah will come is an act that is frowned upon. These kinds of actions are thought to weaken the faith the people have in the religion. This happened once when Shabbatai Tzvi, from Smirna (now İzmir, Turkey), claimed that he was the messiah that the Jewish community have been waiting for. So in Judaism, there is no specific time when the messiah comes. Rather, is the act of the people that determines when the messiah comes. It is said that the messiah would come either when the world needs his coming the most (when the world is so sinful and in desperate need of saving by the messiah) or deserves it the most (when genuine goodness prevails in the world).
A common modern rabbinic interpretation is that there is a "potential" messiah in every generation. The Talmud, which often uses stories to make a moral point ("aggadah"), tells of a highly respected rabbi who found the Messiah at the gates of Rome and asked him, "When will you finally come?" He was quite surprised when he was told, "Today." Overjoyed and full of anticipation, the man waited all day. The next day he returned, disappointed and puzzled, and asked, "You said messiah would come 'today' but he didn't come! What happened?" The Messiah replied, "Scripture says, 'Today, if you will but hearken to his voice.'"
A Kabbalistic tradition within Judaism is that the commonly discussed messiah who will usher in a period of freedom and peace (Messiah ben David) will be preceded by Messiah ben Joseph, who will gather the children of Israel around him, lead them to Jerusalem. After overcoming the hostile powers in Jerusalem, Messiah ben Joseph, will reestablish the Temple-worship and set up his own dominion. Then Armilus, according to one group of sources, or Gog and Magog, according to the other, will appear with their hosts before Jerusalem, wage war against Messiah ben Joseph, and slay him. His corpse, according to one group, will lie unburied in the streets of Jerusalem; according to the other, it will be hidden by the angels with the bodies of the Patriarchs, until Messiah ben David comes and brings him back to life.
Christianity.
The Greek translation of Messiah is "khristos" (χριστος), anglicized as "Christ", and Christians commonly refer to Jesus as either the "Christ" or the "Messiah." Christians believe the Messianic prophecies were fulfilled in the mission, death, and resurrection of Jesus and that he will return to fulfill the rest of Messianic prophecy.
The majority of historical and mainline Christian theologies consider Jesus to be the Son of God and God the Son, a concept of the Messiah fundamentally different from the Jewish and Islamic concepts. In each of the four New Testament Gospels, the only literal anointing of Jesus is conducted by a woman. In the Gospels of Mark, Matthew, and John, this anointing occurs in Bethany, outside Jerusalem. In the Gospel of Luke, the anointing scene takes place at an indeterminate location, but context suggests it to be in Galilee.
Islam.
The Quran identifies Jesus as the Messiah ("Masih"), referring to him as "Isa". Jesus is one of the most important prophets in the Islamic tradition, as well as Abraham, Noah, Moses, and Muhammed.#Redirect #Redirect #Redirect In Islamic theology, Jesus is the only prophet and messenger of end times. Muslims have great respect for Jesus, but see him only as a prophet not God. Prophecy in a human form is adequate in Islam, but does not represent the true powers of God as Jesus does in Christianity.
The Quran states that Isa, the Son of Mariam (Arabic: "Isa ibn Maryam"), is the Messiah and Prophet sent to the Children of Israel.#Redirect The birth of Isa is described Quran sura 19 verses 1–33,#Redirect and sura 4 verse 171 explicitly states Isa as the Son of Mariam.#Redirect Muslims believe Isa is alive in Heaven and will return to Earth to defeat the Masih ad-Dajjal (false Messiah), a figure similar to the Antichrist in Christianity, who will emerge shortly before "Yawm al-Qiyāmah" ("the Day of Resurrection"). After he has destroyed ad-Dajjal, his final task will be to become leader of the Muslims. Isa will unify the Muslim "Ummah" (the followers of Islam) under the common purpose of worshipping Allah alone in pure Islam, thereby ending divisions and deviations by adherents. Mainstream Muslims believe that at that time Isa will dispel Christian and Jewish claims about him.
A hadith in Abu Dawud ( ) says:
Narrated Abu Hurayrah: The Prophet said: There is no prophet between me and him, that is, Isa. He will descend (to the earth). When you see him, recognise him: a man of medium height & reddish dusky complexion, wearing two light yellow garments, looking as if drops of water were falling down from his head though it will not be wet. He will fight for the cause of Islam. He will break the cross, kill the swine, and put an end to war (in another Tradition, there is the word Jizyah instead of "Harb" (war), meaning that he will abolish jizyah); God will perish all religions except Islam. He [Isa] will destroy the Antichrist who will live on the earth for forty days and then he will die. The Muslims will pray behind him.
Both Sunni and Shia Muslims agree that al-Mahdi will arrive first, and after him, Isa. Isa will proclaim al-Mahdi as the Islamic community leader. A war will be fought—the Dajjal against alMahdi and Isa. This war will mark the approach of the coming of the Last Day. After Isa slays alDajjāl at the Gate of Lud, he will bear witness and reveal that Islam is indeed the true and last word from God to humanity as Yusuf Ali's translation reads: "And there is none of the People of the Book but must believe in him before his death; and on the Day of Judgment he will be a witness against them."#Redirect A "hadith" in Sahih Bukhari (Sahih al-Bukhari, ) says:
The Quran refutes the crucifixion of Jesus, claiming that he was neither killed nor crucified.#Redirect The Quran also emphasizes the difference between Allah (God in Islam) and the Messiah:
"Those who say that Allah is the Messiah, son of Mary, are unbelievers. The Messiah said: "O Children of Israel, worship Allah, my Lord and your Lord... unbelievers too are those who have said that Allah is the third of three... the Messiah, son of Mary, was only a Messenger before whom other Messengers had gone."#Redirect 
Ahmadiyya.
In Ahmadiyya theology, the terms "Messiah" and "Mahdi" are synonymous terms for one and the same person. The term "Mahdi" means "guided" by God, thus implying a direct ordainment by God of a divinely chosen individual.
According to Ahmadiyya thought, Messiahship is a phenomenon through which a special emphasis is given on the transformation of a people by way of offering suffering for the sake of God instead of giving suffering (i.e. refraining from revenge). Ahmadis believe that this special emphasis was given through the person of Jesus and Mirza Ghulam Ahmad (1835–1908). among others.
Ahmadis hold that the prophesied eschatological figures of Christianity and Islam, the Messiah and Mahdi, were in fact to be fulfilled in one person who was to represent all previous prophets. The prophecies concerning the Mahdi or the Second Coming of Jesus are seen by Ahmadis as metaphorical and subject to interpretation. It is argued that one was to be born and rise within the dispensation of Muhammad, who by virtue of his similarity and affinity with Jesus, and the similarity in nature, temperament and disposition of the people of Jesus' time and the people of the time of the promised one (the Mahdi) is called by the same name.
Numerous hadith are presented by the Ahmadis in support of their view, such as one from Sunan Ibn Majah, which says, "There is No Mahdi but Jesus son of Mary."
Ahmadis believe that the prophecies concerning the Mahdi and the second coming of Jesus have been fulfilled in Mirza Ghulam Ahmad (1835–1908), the founder of the Ahmadiyya Movement. Contrary to mainstream Islam, the Ahmadis do not believe that Jesus is alive in heaven, but that he survived the crucifixion and migrated towards the east where he died a natural death and that Ghulam Ahmad was only the promised spiritual second coming and likeness of Jesus, the promised Messiah and Mahdi. He claimed he was an incarnation of Krishna, and stated that the founder of Sikhism was a Muslim saint, who was a reflection of the religious challenges he perceived to be occurring. Ghulam Ahmad wrote Barahin-e-Ahmadiyya, in 1880, which incorporated Indian, Sufi, Islamic and Western aspects in order to give life to Islam in the face of the British Raj, Protestant Christianity, and rising Hinduism. He later declared himself the Promised Messiah and the Mahdi following Divine revelations in 1891. Ghulam Ahmad argued that Jesus had appeared 1300 after the formation of the Muslim community and stressed the need for a current Messiah, in turn claiming that he himself embodied both the Mahdi and the Messiah. Ghulam Ahmad was supported by Muslims who especially felt oppressed by Christian and Hindu missionaries.
Popular culture.
The following are works in which the concept of a messiah as a leader of a cause or liberator of a people is used.

</doc>
<doc id="19617" url="http://en.wikipedia.org/wiki?curid=19617" title="Margaret Mead">
Margaret Mead

Margaret Mead (December 16, 1901 – November 15, 1978) was an American cultural anthropologist who featured frequently as an author and speaker in the mass media during the 1960s and 1970s. She earned her bachelor degree at Barnard College in New York City and her M.A. and Ph.D. degrees from Columbia University.
Mead was a respected and often controversial academic who popularized the insights of anthropology in modern American and Western culture. Her reports detailing the attitudes towards sex in South Pacific and Southeast Asian traditional cultures influenced the 1960s sexual revolution. She was a proponent of broadening sexual mores within a context of traditional Western religious life.
As an Anglican Christian, Mead played a considerable part in the drafting of the 1979 American Episcopal Book of Common Prayer.:347–348
Birth, early family life, and education.
Mead, the first of five children, was born in Philadelphia, but raised in nearby Doylestown, Pennsylvania. Her father, Edward Sherwood Mead, was a professor of finance at the Wharton School of the University of Pennsylvania, and her mother, Emily (née Fogg) Mead, was a sociologist who studied Italian immigrants. Her sister Katharine (1906–1907) died at the age of nine months. This was a traumatic event for Mead, who had named this baby, and thoughts of her lost sister permeated her daydreams for many years. Her family moved frequently, so her early education alternated between home-schooling and traditional schools. Her family owned the Longland farm from 1912 to 1926. Born into a family of varying religious outlooks, she searched for a form of religion that gave an expression of the faith that she had been formally acquainted with, Christianity. In doing so, she found the rituals of the Episcopal Church to fit the expression of religion she was seeking. Margaret studied one year, 1919, at DePauw University, then transferred to Barnard College where she earned her bachelor's degree in 1923.
She studied with professor Franz Boas and Dr. Ruth Benedict at Columbia University before earning her Master's degree in 1924. Mead set out in 1925 to do fieldwork in Samoa. In 1926, she joined the American Museum of Natural History, New York City, as assistant curator. She received her Ph.D. from Columbia University in 1929.
Personal life.
Before departing for Samoa, Mead had a short affair with the linguist Edward Sapir, a close friend of Ruth Benedict. But Sapir's conservative ideas about marriage and the woman's role were anathema to Mead, and as Mead left to do field work in Samoa the two separated permanently. Mead received news of Sapir's remarriage while living in Samoa, where, on a beach, she later burned their correspondence.
Mead was married three times. Her first husband (1923–28) was American Luther Cressman, a theology student at the time who eventually became an anthropologist. Mead dismissively characterized their union as "my student marriage" in "Blackberry Winter", a sobriquet with which Cressman took vigorous issue. Her second husband was New Zealander Reo Fortune, a Cambridge graduate (1928–1935). As an anthropologist, his "Sorcerers of Dobu" remains the "locus classicus" of eastern Papuan anthropology, but he is best known instead for his Fortunate number theory. Mead's third and longest-lasting marriage (1936–50) was to the British Anthropologist Gregory Bateson, with whom she had a daughter, Mary Catherine Bateson, who would also become an anthropologist. 
Mead's pediatrician was Benjamin Spock, whose subsequent writings on child rearing incorporated some of Mead's own practices and beliefs acquired from her ethnological field observations which she shared with him; in particular, breastfeeding on the baby's demand rather than a schedule. She readily acknowledged that Gregory Bateson was the husband she loved the most. She was devastated when he left her, and she remained his loving friend ever after, keeping his photograph by her bedside wherever she traveled, including beside her hospital deathbed.
Mead also had an exceptionally close relationship with Ruth Benedict, one of her instructors. In her memoir about her parents, "With a Daughter's Eye", Mary Catherine Bateson implies that the relationship between Benedict and Mead was partly sexual. While Mead never openly identified herself as lesbian or bisexual, the details of her relationship with Benedict have led others to so identify her. In her writings she proposed that it is to be expected that an individual's sexual orientation may evolve throughout life.
She spent her last years in a close personal and professional collaboration with anthropologist Rhoda Metraux, with whom she lived from 1955 until her death in 1978. Letters between the two published in 2006 with the permission of Mead's daughter clearly express a romantic relationship.
Both of Mead's surviving sisters were married to well-known men. Elizabeth Mead (1909–1983), an artist and teacher, married cartoonist William Steig, and Priscilla Mead (1911–1959) married author Leo Rosten. Mead also had a brother, Richard, who became a professor. Mead was also the aunt of Jeremy Steig.
Career and later life.
During World War II, Mead served as executive secretary of the National Research Council's Committee on Food Habits. She served as curator of ethnology at the American Museum of Natural History from 1946 to 1969. She was elected a Fellow of the American Academy of Arts and Sciences in 1948. She taught at The New School and Columbia University, where she was an adjunct professor from 1954 to 1978 and was a professor of anthropology and chair of the Division of Social Sciences at Fordham University's Lincoln Center campus from 1968 to 1970, founding their anthropology department. In 1970, she joined the faculty of the University of Rhode Island as a Distinguished Professor of Sociology and Anthropology.
Following Ruth Benedict's example, Mead focused her research on problems of child rearing, personality, and culture. She served as president of the American Anthropological Association in 1960. She held various positions in the American Association for the Advancement of Science, notably president in 1975 and chair of the executive committee of the board of directors in 1976. She was a recognizable figure in academia, usually wearing a distinctive cape and carrying a walking-stick.
Mead was featured on two record albums published by Folkways Records. The first, released in 1959, "An Interview With Margaret Mead," explored the topics of morals and anthropology. In 1971, she was included in a compilation of talks by prominent women, "But the Women Rose, Vol.2: Voices of Women in American History".
She is credited with the pluralization of the term "semiotics." 
In later life, Mead was a mentor to many young anthropologists and sociologists, including Jean Houston.
In 1976, Mead was a key participant at UN Habitat I, the first UN forum on human settlements.
Mead died of pancreatic cancer on November 15, 1978.
Work.
"Coming of Age in Samoa".
In the foreword to "Coming of Age in Samoa", Mead's advisor, Franz Boas, wrote of its significance:
Courtesy, modesty, good manners, conformity to definite ethical standards are universal, but what constitutes courtesy, modesty, very good manners, and definite ethical standards is not universal. It is instructive to know that standards differ in the most unexpected ways.
Mead's findings suggested that the community ignores both boys and girls until they are about 15 or 16. Before then, children have no social standing within the community. Mead also found that marriage is regarded as a social and economic arrangement where wealth, rank, and job skills of the husband and wife are taken into consideration.
In 1983, five years after Mead had died, New Zealand anthropologist Derek Freeman published "Margaret Mead and Samoa: The Making and Unmaking of an Anthropological Myth", in which he challenged Mead's major findings about sexuality in Samoan society. Freeman's book was controversial in its turn: later in 1983 the American Anthropological Association declared it to be "poorly written, unscientific, irresponsible and misleading."
In 1999 Freeman published another book, "The Fateful Hoaxing of Margaret Mead: A Historical Analysis of Her Samoan Research", including previously unavailable material. Most anthropologists have since been highly critical of Freeman's arguments. A frequent criticism of Freeman is that he regularly misrepresented Mead's research and views. In a 2009 evaluation of the debate, anthropologist Paul Shankman concluded that:
While nurture-oriented anthropologists are more inclined to agree with Mead's conclusions, there are other non-anthropologists who take a nature-oriented approach following Freeman's lead, among them Harvard psychologist Steven Pinker, biologist Richard Dawkins, evolutionary psychologist David Buss, science writer Matt Ridley and classicist Mary Lefkowitz. The philosopher Peter Singer has also criticized Mead in his book "A Darwinian Left", where he states that "Freeman compiles a convincing case that Mead had misunderstood Samoan customs".
In 1996 Martin Orans examined Mead's notes preserved at the Library of Congress, and credits her for leaving all of her recorded data available to the general public. Orans concludes that Freeman's basic criticisms, that Mead was duped by ceremonial virgin Fa'apua'a Fa'amu (who later swore to Freeman that she had played a joke on Mead) were false for several reasons: first, Mead was well aware of the forms and frequency of Samoan joking; second, she provided a careful account of the sexual restrictions on ceremonial virgins that corresponds to Fa'apua'a Fa'auma'a's account to Freeman, and third, that Mead's notes make clear that she had reached her conclusions about Samoan sexuality before meeting Fa'apua'a Fa'amu. He therefore concludes, contrary to Freeman, that Mead was never the victim of a hoax. Orans points out that Mead's data support several different conclusions, and that Mead's conclusions hinge on an interpretive, rather than positivist, approach to culture. Orans claims remain controversial though since there are still many who claim Mead was hoaxed, these include philosopher Peter Singer and zoologist David Attenborough. Evaluating Mead's work in Samoa from a positivist stance, Martin Orans' assessment of the controversy was that Mead did not formulate her research agenda in scientific terms, and that "her work may properly be damned with the harshest scientific criticism of all, that it is 'not even wrong'."
"Sex and Temperament in Three Primitive Societies" (1935).
Another influential book by Mead was "Sex and Temperament in Three Primitive Societies". This became a major cornerstone of the feminist movement, since it claimed that females are dominant in the Tchambuli (now spelled Chambri) Lake region of the Sepik basin of Papua New Guinea (in the western Pacific) without causing any special problems. The lack of male dominance may have been the result of the Australian administration's outlawing of warfare. According to contemporary research, males are dominant throughout Melanesia (although some believe that female witches have special powers). Others have argued that there is still much cultural variation throughout Melanesia, and especially in the large island of New Guinea. Moreover, anthropologists often overlook the significance of networks of political influence among females. The formal male-dominated institutions typical of some areas of high population density were not, for example, present in the same way in Oksapmin, West Sepik Province, a more sparsely populated area. Cultural patterns there were different from, say, Mt. Hagen. They were closer to those described by Mead.
Mead stated that the Arapesh people, also in the Sepik, were pacifists, although she noted that they do on occasion engage in warfare. Her observations about the sharing of garden plots among the Arapesh, the egalitarian emphasis in child rearing, and her documentation of predominantly peaceful relations among relatives are very different from the "big man" displays of dominance that were documented in more stratified New Guinea cultures – e.g. by Andrew Strathern. They are a different cultural pattern.
In brief, her comparative study revealed a full range of contrasting gender roles:
Deborah Gewertz (1981) studied the Chambri (called Tchambuli by Mead) in 1974–75, and found no evidence of such gender roles. Gewertz states that as far back in history as there is evidence (1850s) Chambri men dominated over the women, controlled their produce and made all important political decisions. In later years there has been a diligent search for societies in which women dominate men, or for signs of such past societies, but none have been found (Bamberger 1974).
Despite its feminist roots, Mead's work on women and men was also criticized by Betty Friedan 
on the basis that it contributes to infantilizing women.
Other research areas.
In 1926, there was much debate about race and intelligence. Mead felt the methodologies involved in the experimental psychology research supporting arguments of racial superiority in intelligence were substantially flawed. In "The Methodology of Racial Testing: Its Significance for Sociology" Mead proposes that there are three unique problems with testing for racial differences in intelligence. First, there are issues with the ability to validly equate one’s test score with what Mead refers to as "racial admixture" or how much "Negro or Indian blood" an individual possesses. She also considers whether or not this information is even relevant to consider when interpreting one’s IQ score. Mead remarks that a genealogical method could be considered valid if it could be “subjected to extensive verification”. In addition, the experiment would need a steady control group to establish racial admixture was actually affecting intelligence scores. Next, Mead argues that it is difficult to measure the effect that social status has on the results of an individual’s intelligence test. By this she meant that one’s environment (i.e., family structure, socioeconomic status, exposure to language) had too much of an impact on an individual to attribute inferior scores to a physical characteristic such as race. Lastly, Mead adds that language barriers create sometimes the biggest problem of all. In similar regards, Stephen J. Gould finds three main problems with intelligence testing, in his book The Mismeasure of Man that relate to Mead's stance on the problems with trying to determine if there are indeed racial differences in intelligence.
In 1929 Mead and Fortune visited Manus, now the northern-most province of Papua New Guinea, reaching there by boat from Rabaul after a stay there which she amply describes in her autobiography and which is mentioned in her 1984 biography by Jane Howard. On Manus she studied the Manus people of the south coast village of Peri. "Over the next five decades Mead would come back oftener to Peri than to any other field site of her career.
Mead has been credited with persuading the American Jewish Committee to sponsor a project to study European Jewish villages, "shtetls", in which a team of researchers would conduct mass interviews with Jewish immigrants living in New York City. The resulting book, widely cited for decades, allegedly created the Jewish mother stereotype, a mother intensely loving but controlling to the point of smothering, and engendering guilt in her children through the suffering she professed to undertake for their sakes.
Mead worked for the RAND Corporation, a U.S. Air Force military funded private research organization, from 1948 to 1950 to study Russian culture and attitudes toward authority.
Criticism.
Mead's research has come under criticism since her death. Shortly after her death anthropologist Derek Freeman published a book refuting many of Mead's conclusions. Freeman initially received considerable backlash and harsh criticism from the anthropology community. Mead's research was seen as supporting various political positions and an attack on Mead was seen as a bigger attack on these political ideals. However, additional anthropologists who studied the Samoans confirmed most of Freeman's findings and contradicted those of Mead. While Mead was careful to shield the identity of all her subjects for confidentiality one participant in her study was found and interviewed and she said that her and her friends were having fun with Mead and telling her stories. Evolutionary psychologist Steven Pinker said:
Margaret Mead disseminated the incredible claim that Samoans have no passions -- no anger between parents and children or between a cuckold and seducer, no revenge, no lasting love or bereavement... no adolescent turmoil. Derek Freeman and other anthropologists found that Samoan society in fact had widespread adolescent resentment and delinquency, a cult of virginity, frequent rape, reprisals by rape victim's families... sexual jealousy and strong religious feeling.
Legacy.
On January 19, 1979, President Jimmy Carter announced that he was awarding the Presidential Medal of Freedom posthumously to Mead. UN Ambassador Andrew Young presented the award to Mead's daughter at a special program honoring Mead's contributions, sponsored by the American Museum of Natural History, where she spent many years of her career. The citation read:
"Margaret Mead was both a student of civilization and an exemplar of it. To a public of millions, she brought the central insight of cultural anthropology: that varying cultural patterns express an underlying human unity. She mastered her discipline, but she also transcended it. Intrepid, independent, plain spoken, fearless, she remains a model for the young and a teacher from whom all may learn."
The 2006 music video for "If Everyone Cared" by Nickelback ends with her quote: "Never doubt that a small group of committed people can change the world. Indeed, it is the only thing that ever has." This quote is also in the 15th episode of the 4th season of The West Wing. 
The 2014 novel "Euphoria" by Lily King is a fictionalized account of Mead's relationships with Reo Fortune and Gregory Bateson.
In addition, there are several schools named after Mead in the United States: a junior high school in Elk Grove Village, Illinois, an elementary school in Sammamish, Washington and another in Sheepshead Bay, Brooklyn, New York.
The USPS have issued a stamp of face value 32¢ on 28 May 1998 as part of the Celebrate the Century stamp sheet series.

</doc>
<doc id="19620" url="http://en.wikipedia.org/wiki?curid=19620" title="Michael Palin">
Michael Palin

Michael Edward Palin (pronounced ; born 5 May 1943) is an English comedian, actor, writer and television presenter. He was one of the members of the comedy group Monty Python and later made a number of travel documentaries.
Palin wrote most of his comedic material with Terry Jones. Before Monty Python, they had worked on other shows such as the "Ken Dodd Show", "The Frost Report", and "Do Not Adjust Your Set". Palin appeared in some of the most famous Python sketches, including "Argument Clinic", "Dead Parrot", "The Lumberjack Song", "The Spanish Inquisition", and "The Fish-Slapping Dance".
Palin continued to work with Jones after Python, co-writing "Ripping Yarns". He has also appeared in several films directed by fellow Python Terry Gilliam and made notable appearances in other films such as "A Fish Called Wanda", for which he won the BAFTA Award for Best Actor in a Supporting Role. In a 2005 poll to find "The Comedians' Comedian", he was voted the 30th favourite by fellow comedians and comedy insiders.
After Python, he began a new career as a travel writer and travel documentarian. His journeys have taken him across the world, including the North and South Poles, the Sahara Desert, the Himalayas, Eastern Europe and Brazil. In 2000 Palin was honoured as a Commander of the Order of the British Empire (CBE) for his services to television. From 2009 to 2012 Palin was the president of the Royal Geographical Society. On 12 May 2013, Palin was made a BAFTA fellow, the highest honour that is conferred by the organisation.
Early life and career.
Palin was born in Broomhill, Sheffield, West Riding of Yorkshire, the second child and only son of Edward Moreton Palin (b. 1900, d. 1977). and Mary Rachel Lockhart (née Ovey) (b. 1903, d. 1990). His father was a Shrewsbury School and Cambridge-educated engineer working for a steel firm. His maternal grandfather, Lieutenant-Colonel Richard Lockhart Ovey, DSO, was High Sheriff of Oxfordshire in 1927. He started his education at Birkdale Preparatory School, Sheffield, and later Shrewsbury School. His sister Angela was nine years older than he was. Despite the age gap the two had a close relationship until her suicide in 1987. He has ancestral roots in Letterkenny, County Donegal.
When he was five years old, Palin had his first acting experience at Birkdale playing Martha Cratchit in a school performance of "A Christmas Carol". At the age of 10, Palin, still interested in acting, made a comedy monologue and read a Shakespeare play to his mother while playing all the parts. After his school days in 1962 he went on to read modern history at Brasenose College, Oxford. With fellow student Robert Hewison he performed and wrote, for the first time, comedy material at a university Christmas party. Terry Jones, also a student in Oxford, saw that performance and began writing together with Hewison and Palin. In the same year Palin joined the Brightside and Carbrook Co-operative Society Players and first gained fame when he won an acting award at a Co-op drama festival. He also performed and wrote in the Oxford Revue (called the Et ceteras) with Jones.
In 1966 he married Helen Gibbins, whom he first met in 1959 on holiday in Southwold in Suffolk. This meeting was later fictionalised in Palin's play "East of Ipswich". The couple have three children (Thomas (b. 1969), William (b. 1971) and Rachel (b. 1975)) and two grandchildren. Rachel is a BBC TV director, whose work includes "", shown on BBC2 throughout October and November 2010. A photograph of William as a baby briefly appeared in "Monty Python and the Holy Grail" as "Sir Not-appearing-in-this-film". His nephew is the theatre designer Jeremy Herbert.
After finishing university in 1965 Palin became a presenter on a comedy pop show called "Now!" for the television contractor Television Wales and the West. At the same time Palin was contacted by Jones, who had left university a year earlier, for assistance in writing a theatrical documentary about sex through the ages. Although this project was eventually abandoned, it brought Palin and Jones together as a writing duo and led them to write comedy for various BBC programmes, such as "The Ken Dodd Show", "The Billy Cotton Bandshow", and "The Illustrated Weekly Hudd". They collaborated in writing lyrics for an album by Barry Booth called . They were also in the team of writers working for "The Frost Report", whose other members included Frank Muir, Barry Cryer, Marty Feldman, Ronnie Barker, Ronnie Corbett, Dick Vosburgh and future Monty Python members Graham Chapman, John Cleese and Eric Idle. Although the members of Monty Python had already encountered each other over the years, "The Frost Report" was the first time all the British members of Monty Python (its sixth member, Terry Gilliam, was at that time an American citizen) worked together. During the run of "The Frost Report" the Palin/Jones team contributed material to two shows starring John Bird: "The Late Show" and "A series of Bird's". For "A series of Bird's" the Palin/Jones team had their first experience of writing narrative instead of the short sketches they were accustomed to conceiving.
Following "The Frost Report" the Palin/Jones team worked both as actors and writers on the show "Twice a Fortnight" with Graeme Garden, Bill Oddie and Jonathan Lynn, and the successful children's comedy show "Do Not Adjust Your Set" with Idle and David Jason. The show also featured musical numbers by the Bonzo Dog Doo-Dah Band, including future Monty Python musical collaborator Neil Innes. The animations for "Do Not Adjust Your Set" were made by Terry Gilliam. Eager to work with Palin sans Jones, Cleese later asked him to perform in "How to Irritate People" together with Chapman and Tim Brooke-Taylor. The Palin/Jones team were reunited for "The Complete and Utter History of Britain".
During this period Cleese contacted Palin about doing the show that would ultimately become "Monty Python's Flying Circus". On the strength of their work on "The Frost Report" and other programmes, Cleese and Chapman had been offered a show by the BBC, but Cleese was reluctant to do a two-man show for various reasons, among them Chapman's reputedly difficult personality. At the same time the success of "Do Not Adjust Your Set" had led Palin, Jones, Idle and Gilliam to be offered their own series and, while it was still in production, Palin agreed to Cleese's proposal and brought along Idle, Jones and Gilliam. Thus the formation of the Monty Python troupe has been referred to as a result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.
"Monty Python".
In "Monty Python", Palin played various roles, which ranged from manic enthusiasm (such as the lumberjack of the Lumberjack Song, or Herbert Anchovy, host of the game show "Blackmail") to unflappable calmness (such as the Dead Parrot vendor, Cheese Shop proprietor, or Postal Clerk). As a straight man he was often a foil to the rising ire of characters portrayed by John Cleese. He also played timid, socially inept characters such as Arthur Putey, the man who sits idly by as a marriage counsellor (Eric Idle) makes love to his wife (Carol Cleveland), and Mr. Anchovy, a chartered accountant who wants to become a lion tamer. He also appeared as the "It's" man at the beginning of most episodes.
Palin frequently co-wrote sketches with Terry Jones and also initiated the "Spanish Inquisition sketch", which included the catchphrase "Nobody expects the Spanish Inquisition!"
He also composed songs with Jones including "The Lumberjack Song", "Every Sperm is Sacred" and "Spam". His solo musical compositions included "Decomposing Composers" and "Finland".
Other work.
After the "Monty Python" television series ended in 1974, the Palin/Jones team worked on "Ripping Yarns", an intermittent television comedy series broadcast over three years from 1976. They had earlier collaborated on the play "Secrets" from the BBC series "Black and Blue" in 1973. He starred as Dennis the Peasant in Terry Gilliam's 1977 film "Jabberwocky". Palin also appeared in "All You Need Is Cash" (1978) as Eric Manchester (based on Derek Taylor), the press agent for the Rutles.
In 1980, Palin co-wrote "Time Bandits" with Terry Gilliam. He also acted in the film.
In 1982, Palin wrote and starred in "The Missionary", co-starring Maggie Smith. In it, he plays the Reverend Charles Fortescue, who is recalled from Africa to aid prostitutes.
In 1984, he reunited with Terry Gilliam to appear in "Brazil". He appeared in the comedy film "A Fish Called Wanda", for which he won the BAFTA Award for Best Actor in a Supporting Role. Cleese reunited the main cast almost a decade later to make "Fierce Creatures".
After filming for "Fierce Creatures" finished, Palin went on a travel journey for a BBC documentary and, returning a year later, found that the end of "Fierce Creatures" had failed at test screenings and had to be reshot.
Apart from "Fierce Creatures", Palin's last film role was a small part in "The Wind in the Willows", a film directed by and starring Terry Jones. Palin also appeared with John Cleese in his documentary, "The Human Face". Palin was in the cast of "You've Got Mail", the Tom Hanks and Meg Ryan romantic comedy as a subplot novelist, but his role was eventually cut entirely.
He also assisted Campaign for Better Transport and others with campaigns on sustainable transport, particularly those relating to urban areas, and has been president of the campaign since 1986.
Palin has also appeared in serious drama. In 1991 Palin worked as producer and actor in the film "American Friends" based upon a real event in the life of his great grandfather, a fellow at St John's College, Oxford. In that same year he also played the part of a headmaster in Alan Bleasdale's Channel 4 drama series "G.B.H.".
Palin also had a small cameo role in Australian soap opera "Home and Away". He played an English surfer with a fear of sharks, who interrupts a conversation between two main characters to ask whether there were any sharks in the sea. This was filmed while he was in Australia for the "Full Circle" series, with a segment about the filming of the role featuring in the series.
In November 2005, he appeared in the "John Peel's Record Box" documentary.
In 2013 Michael Palin appeared in a First World War drama titled "The Wipers Times" written by Ian Hislop and Nick Newman.
Television documentaries.
Travel.
Palin's first travel documentary was part of the 1980 BBC Television series "Great Railway Journeys of the World", in which, humorously reminiscing about his childhood hobby of train spotting, he travelled throughout the UK by train, from London to the Kyle of Lochalsh, via Manchester, York, Newcastle upon Tyne, Edinburgh and Inverness. At the Kyle of Lochalsh, Palin bought the station's long metal platform sign and is seen lugging it back to London with him.
In 1994, Palin travelled through Ireland for the same series, entitled "Derry to Kerry". In a quest for family roots, he attempted to trace his great grandmother – Brita Gallagher – who set sail from Ireland 150 years ago during the Great Famine (1845–1849), bound for a new life in Burlington, New Jersey. The series is a trip along the Palin family line.
Starting in 1989, Palin appeared as presenter in a series of travel programmes made for the BBC. It was after the veteran TV globetrotter Alan Whicker and journalist Miles Kington turned down presenting the first of these, "Around the World in 80 Days", that gave Palin the opportunity to present his first and subsequent travel shows. These programmes have been broadcast around the world in syndication, and were also sold on VHS tape and later on DVD:
Following each trip, Palin wrote a book about his travels, providing information and insights not included in the TV programme. Each book is illustrated with photographs by Basil Pao, the stills photographer who was on the team. (Exception: the first book, "Around the World in 80 Days", contains some pictures by Pao but most are by other photographers.)
All seven of these books were also made available as audio books, and all of them are read by Palin himself. "Around the World in 80 Days" and "Hemingway Adventure" are unabridged, while the other four books were made in both abridged and unabridged versions, although the unabridged versions can be very difficult to find.
For four of the trips a photography book was made by Pao, each with an introduction written by Palin. These are large coffee-table style books with pictures printed on glossy paper. The majority of the pictures are of various people encountered on the trip, as informal portraits or showing them engaged in some interesting activity. Some of the landscape photos are displayed as two-page spreads.
Palin's travel programmes are responsible for a phenomenon termed the "Palin effect": areas of the world that he has visited suddenly become popular tourist attractions – for example, the significant increase in the number of tourists interested in Peru after Palin visited Machu Picchu. In a 2006 survey of "15 of the world's top travel writers" by "The Observer", Palin named Peru's Pongo de Mainique (canyon below the Machu Picchu) his "favourite place in the world".
Palin notes in his book of "Around the World in 80 Days" that the final leg of his journey could originally have taken him and his crew on one of the trains involved in the Clapham Junction rail crash, but they arrived ahead of schedule and caught an earlier train.
Art and history.
In recent years, Palin has written and presented occasional documentary programmes on artists that interest him. The first, on Scottish painter Anne Redpath, was "Palin on Redpath" in 1997. In "The Bright Side of Life" (2000), Palin continued on a Scottish theme, looking at the work of the Scottish Colourists. Two further programmes followed on European painters; "Michael Palin and the Ladies Who Loved Matisse" (2004) and "Michael Palin and the Mystery of Hammershøi" (2005), about the French artist Henri Matisse and Danish artist Vilhelm Hammershøi respectively. The DVD "Michael Palin on Art" contains all these documentaries except for the Matisse programme.
In November 2008, Palin presented a First World War documentary about Armistice Day, 11 November 1918, when thousands of soldiers lost their lives in battle after the war had officially ended. Palin filmed on the battlefields of Northern France and Belgium for the programme, called the "Last Day of World War One", produced for the BBC's "Timewatch" series.
Activism.
In July 2010, Palin sent a message of support for the Dongria Kondh tribe of India, who are resisting a mine on their land by the company Vedanta Resources. Palin said, "I've been to the Nyamgiri Hills in Orissa and seen the forces of money and power that Vedanta Resources have arrayed against a people who have occupied their land for thousands of years, who husband the forest sustainably and make no great demands on the state or the government. The tribe I visited simply want to carry on living in the villages that they and their ancestors have always lived in".
On 2 January 2011, Palin became the first person to sign the UK-based Campaign for Better Transport's Fair Fares Now campaign.
Recognition.
Each member of Monty Python has an asteroid named after him. Palin's is Asteroid 9621 Michaelpalin.
In honour of his achievements as a traveller, especially rail travel, Palin has two British trains named after him. In 2002, Virgin Trains' new £5 million high speed Super Voyager train number 221130 was named "Michael Palin" – it carries his name externally and a plaque is located adjacent to the onboard shop with information on Palin and his many journeys. Also, National Express East Anglia named a British Rail Class 153 (unit number 153335) after him. In 2008, he received the James Joyce Award of the Literary and Historical Society in Dublin.
Palin was instrumental in setting up the Michael Palin Centre for Stammering Children in 1993.
In recognition of his services to the promotion of geography, Palin was awarded the Livingstone Medal of the Royal Scottish Geographical Society in March 2009, along with a Fellowship to the Society. In June 2013, he was similarly honoured in Canada with a gold medal for achievements in geography by the Royal Canadian Geographical Society.
In June 2009, Palin was elected for a three-year term as President of the Royal Geographical Society.
In September 2013, Moorlands School, Leeds named one of their school houses "Palin" after him.
Because of his self-described "amenable, conciliatory character" Michael Palin has been referred to as unofficially "Britain's Nicest Man."
Bibliography.
Travel books.
All his travel books can be read at no charge, complete and unabridged, on his website.

</doc>
<doc id="19622" url="http://en.wikipedia.org/wiki?curid=19622" title="Materials science">
Materials science

Materials science, also commonly known as materials science and engineering, is an interdisciplinary field which deals with the discovery and design of new materials. Though it is a relatively new scientific field that involves studying materials through the materials paradigm (synthesis, structure, properties, and performance), its intellectual origins reach back to the emerging fields of chemistry, mineralogy, and engineering during the Enlightenment. It incorporates elements of physics and chemistry and is at the forefront of nanoscience and nanotechnology research. In recent years, materials science has become more widely known as a specific field of science and engineering.
It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.
History.
The material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are great examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science are a product of the space race: the understanding and engineering of the metallic alloys, and silica and carbon materials, used in the construction of space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as plastics, semiconductors, and biomaterials.
Before the 1960s (and in some cases decades after), many "materials science" departments were named "metallurgy" departments, reflecting the 19th and early 20th century emphasis on metals. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s "to expand the national program of basic research and training in the materials sciences." The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, medical implant materials, biological materials and nanomaterials.
Fundamentals of materials science.
A material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications. There are a myriad of materials around us—they can be found in anything from buildings to spacecraft. Materials can generally be divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, ceramics and polymers. New and advanced materials that are being developed include semiconductors, nanomaterials, biomaterials, etc.
The basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, he/she can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material’s microstructure, and thus its properties.
Structure.
As mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves techniques such as diffraction with x-rays, electrons, or neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy (EDS), chromatography, thermal analysis, electron microscope analysis, etc. Structure is studied at various levels, as detailed below.
Atomic structure.
This deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms (0.1 nm).
The way in which the atoms and molecules are bonded and arranged is fundamental to studying the properties and behavior of any material.
Nanostructure.
Nanostructure deals with objects and structures that are in the 1—100 nm range. In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This leads to many interesting electrical, magnetic, optical and mechanical properties.
In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.
Materials whose atoms/molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.
Microstructure.
Microstructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects in from 100 nm to few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.
The manufacture of a perfect crystal of a material is physically impossible. For example, a crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), interstitial atoms, vacancies or substitutional atoms. The microstructure of materials reveals these defects, so that they can be studied.
Macrostructure.
Macrostructure is the appearance of a material in the scale millimeters to meters—it is the structure of the material as seen with the naked eye.
Crystallography.
Crystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in poly-crystalline form (i.e., as an aggregate of small crystals with different orientations). Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination.
Most materials have a crystalline structure. But, there are some important materials that do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely non-crystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic, as well as mechanical, descriptions of physical properties.
Bonding.
To obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid state chemistry and physical chemistry are also involved in the study of bonding and structure.
Properties.
Materials exhibit myriad properties. The important properties of materials are as follows:
The properties of a materials determine its usability and hence its engineering application.
Synthesis and processing.
Synthesis and processing involves the creation of a material with the desired micro/nanostructure. From an engineering standpoint, a material cannot be used in industry if no economical manufacturing method for it has been developed. Thus, the processing of materials is very important to the field of materials science.
Different materials require different processing/synthesis techniques. For example, the processing of metals has historically been very important and is studied under the branch of materials science known as physical metallurgy. Also, chemical and physical techniques are also used to synthesis other materials such as polymers, ceramics, thin films, etc. Currently, new techniques are being developed to synthesize nanomaterials such as graphene.
Thermodynamics.
Thermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints, that are common to all materials, not the peculiar properties of particular materials. These general constraints are expressed in the four laws of thermodynamics. Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics.
The study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.
Kinetics.
Kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field—it details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change.
Kinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.
Materials in research.
Materials science has received much attention from researchers. In most universities, many departments ranging from physics to chemistry to chemical engineering—in addition to materials science departments—are involved in materials research. Research in materials science is vibrant and consists of many avenues. The following list is in no way exhaustive, it just serves to highlight certain important research areas.
Nanomaterials.
Nanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10−9 meter) but is usually 1—100 nm.
Nanomaterials research takes a materials science-based approach to nanotechnology, leveraging advances in materials metrology and synthesis which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties.
The field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.
Biomaterials.
A biomaterial is any matter, surface, or construct that interacts with biological systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.
Biomaterials can be derived either from nature or synthesized in the laboratory using a variety of chemical approaches utilizing metallic components, polymers, ceramics or composite materials. They are often used and/or adapted for a medical application, and thus comprises whole or part of a living structure or biomedical device which performs, augments, or replaces a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxy-apatite coated hip implants. Biomaterials are also used everyday in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as a transplant material.
Electronic, optical and magnetic materials.
Semiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.
Semiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to impurity concentrations, and this allows for the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.
This field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid state physics or condensed matter physics.
Computational materials science and materials theory.
With the increase in computing power, simulating the behavior of materials has become possible. This enables materials scientists to discover properties of materials previously unknown, as well as to design new materials. Up until now, new materials were found by a time consuming trial and error process. But, now it is hoped that computational techniques could drastically reduce that time, and allow us to tailor materials properties. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, etc.
Materials in industry.
Radical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing techniques (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytical techniques (characterization techniques such as electron microscopy, x-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).
Besides material characterization, the material scientist/engineer also deals with the extraction of materials and their conversion into useful forms. Thus ingot casting, foundry techniques, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence or variation of minute quantities of secondary elements and compounds in a bulk material will have a great impact on the final properties of the materials produced, for instance, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extraction and purification techniques employed in the extraction of iron in the blast furnace will have an impact on the quality of steel that may be produced.
Ceramics and glasses.
Another application of material science is the structures of glass and ceramics, typically associated with the most brittle materials. Bonding in ceramics and glasses use covalent and ionic-covalent types with SiO2 (silica or sand) as a fundamental building block. Ceramics are as soft as clay and as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.
Engineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.
Composite materials.
 Filaments are commonly used for reinforcement in composite materials.
Another application of materials science in industry is the making of composite materials. Composite materials are structured materials composed of two or more macroscopic phases. Applications range from structural elements such as steel-reinforced concrete, to the thermally insulative tiles which play a key and integral role in NASA's Space Shuttle thermal protection system which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material which withstands re-entry temperatures up to 1510 °C (2750 °F) and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured/pyrolized to convert the furfural alcohol to carbon. In order to provide oxidation resistance for reuse capability, the outer layers of the RCC are converted to silicon carbide.
Other examples can be seen in the "plastic" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile-butadiene-styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be referred to as reinforcing fibers, or dispersants, depending on their purpose.
Polymers.
Polymers are also an important part of materials science. Polymers are the raw materials (the resins) used to make what we commonly call plastics. Plastics are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Polymers which have been around, and which are in current widespread use, include polyethylene, polypropylene, PVC, polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates. Plastics are generally classified as "commodity", "specialty" and "engineering" plastics.
PVC (polyvinyl-chloride) is widely used, inexpensive, and annual production quantities are large. It lends itself to an incredible array of applications, from artificial leather to electrical insulation and cabling, packaging and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term "additives" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.
Polycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Engineering plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.
Specialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.
The dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For instance, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable shopping bags and trash bags, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called Ultra-high Molecular Weight Polyethylene UHMWPE is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.
Metal alloys.
The study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron carbon alloy is only considered steel if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties however. Cast Iron is defined as an iron–carbon alloy with more than 2.00% but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.
Other significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength-to-weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations where high strength-to-weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.
Relation to other fields.
Materials science evolved—starting from the 1960s—because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged at the intersection of various fields such as metallurgy, solid state physics, chemistry, chemical engineering, mechanical engineering and electrical engineering.
The field is inherently interdisciplinary, and the materials scientists/engineers must be aware and make use of the methods of the physicist, chemist and engineer. The field thus, maintains close relationships with these fields. Also, many physicists, chemists and engineers also find themselves working in materials science.
The overlap between physics and materials science has led to the offshoot field of "materials physics", which is concerned with the physical properties of materials. The approach is generally more macroscopic and applied than in condensed matter physics. See important publications in materials physics for more details on this field of study.
The field of materials science and engineering is important both from a scientific perspective, as well as from an engineering one. When discovering new materials, one encounters new phenomena that may not have been observe before. Hence, there is lot of science to be discovered when working with materials. Materials science also provides test for theories in condensed matter physics.
Material for an engineer is of utmost importance. The usage of the appropriate materials is crucial when designing systems, and hence, engineers are always involved in materials. Thus, materials science is becoming increasingly important in an engineer's education.

</doc>
<doc id="19623" url="http://en.wikipedia.org/wiki?curid=19623" title="Mitsubishi A6M Zero">
Mitsubishi A6M Zero

The Mitsubishi A6M Zero is a long-range fighter aircraft, manufactured by Mitsubishi Heavy Industries, and operated by the Imperial Japanese Navy from 1940 to 1945. The A6M was designated as the Mitsubishi Navy Type 0 Carrier Fighter (零式艦上戦闘機, rei-shiki-kanjō-sentōki), or the Mitsubishi A6M Rei-sen. The A6M was usually referred to by its pilots as the "Reisen" (zero fighter), "0" being the last digit of the Imperial year 2600 (1940) when it entered service with the Imperial Navy. The official Allied reporting name was "Zeke", although the use of the name "Zero" was later commonly adopted by the Allies as well.
When it was introduced early in World War II, the Zero was considered the most capable carrier-based fighter in the world, combining excellent maneuverability and very long range. In early combat operations, the Zero gained a legendary reputation as a dogfighter, achieving the outstanding kill ratio of 12 to 1, but by mid-1942 a combination of new tactics and the introduction of better equipment enabled the Allied pilots to engage the Zero on generally equal terms.
The Imperial Japanese Navy Air Service ("IJNAS") also frequently used the type as a land-based fighter. By 1943, inherent design weaknesses and the failure to develop more powerful aircraft engines meant that the Zero became less effective against newer enemy fighters, which possessed greater firepower, armor, and speed, and approached the Zero's maneuverability. Although the Mitsubishi A6M was outdated by 1944, design delays and production difficulties of newer Japanese aircraft types meant that it continued to serve in a front line role until the end of the war. During the final years of the War in the Pacific, the Zero was also adapted for use in "kamikaze" operations. During the course of the war, Japan produced more Zeros than any other model of combat aircraft.
Design and development.
The Mitsubishi A5M fighter was just entering service in early 1937, when the Imperial Japanese Navy started looking for its eventual replacement. In May, they issued specification 12-Shi for a new carrier-based fighter, sending it to Nakajima and Mitsubishi. Both firms started preliminary design work while they awaited more definitive requirements to be handed over in a few months.
Based on the experiences of the A5M in China, the Imperial Japanese Navy sent out updated requirements in October calling for a speed of 600 km/h and a climb to 3000 m in 3.5 min. With drop tanks, they wanted an endurance of two hours at normal power, or six to eight hours at economical cruising speed. Armament was to consist of two 20 mm cannons, two 7.7 mm (.303 in) machine guns and two 30 kg (66 lb) or 60 kg (132 lb) bombs. A complete radio set was to be mounted in all aircraft, along with a radio direction finder for long-range navigation. The maneuverability was to be at least equal to that of the A5M, while the wingspan had to be less than 12 m (39 ft) to allow for use on an aircraft carrier. All this was to be achieved with available engines, a significant design limitation. The Zero's powerplant seldom reached 750 kilowatts (about 1,000 hp) in any of its variants.
Nakajima's team considered the new requirements unachievable and pulled out of the competition in January. Mitsubishi's chief designer, Jiro Horikoshi, thought that the requirements could be met, but only if the aircraft could be made as light as possible. Every possible weight-saving measure was incorporated into the design. Most of the aircraft was built of a new top-secret 7075 aluminium alloy developed by Sumitomo Metal Industries in 1936. Called Extra Super Duralumin (ESD), it was lighter and stronger than other alloys (e.g. 24S alloy) used at the time, but was more brittle and prone to corrosion which was countered with an anti-corrosion coating applied after fabrication. No armor was provided for the pilot, engine or other critical points of the aircraft, and self-sealing fuel tanks, which were becoming common at the time, were not used. This made the Zero lighter, more maneuverable, and the longest range single engine fighter of WWII; which made it capable of searching out an enemy hundreds of kilometres (miles) away, bringing them to battle, then returning hundreds of kilometres back to its base or aircraft carrier. However, that trade in weight and construction also made it prone to catching fire and exploding when struck by enemy rounds.
With its low-wing cantilever monoplane layout, retractable, wide-set landing gear and enclosed cockpit, the Zero was one of the most modern aircraft in the world at the time of its introduction. It had a fairly high-lift, low-speed wing with a very low wing loading. This, combined with its light weight, resulted in a very low stalling speed of well below 60 kn. This was the main reason for its phenomenal maneuverability, allowing it to out-turn any Allied fighter of the time. Early models were fitted with servo tabs on the ailerons after pilots complained control forces became too heavy at speeds above 300 km/h. They were discontinued on later models after it was found that the lightened control forces were causing pilots to overstress the wings during vigorous maneuvers.
It has been claimed that the Zero's design showed clear influence from American fighter planes and components exported to Japan in the 1930s, and in particular the Vought V-143 fighter. Chance Vought had sold the prototype for this aircraft and its plans to Japan in 1937. Eugene Wilson, President of Vought, claimed that when shown a captured Zero in 1943, he found that "There on the floor was the Vought V 142 ["sic"] or just the spitting image of it, Japanese-made," while the "power-plant installation was distinctly Chance Vought, the wheel stowage into the wing roots came from Northrop, and the Japanese designers had even copied the Navy inspection stamp from Pratt & Whitney type parts." While the sale of the V-143 was fully legal, Wilson later acknowledged the conflicts of interest that can arise whenever military technology is exported. Counterclaims maintain that there was no significant relationship between the V-143 (which was an unsuccessful design that had been rejected by the U.S. Army Air Corps and several export customers) and the Zero, with only a superficial similarity in layout. Allegations about the Zero being a copy have been mostly discredited.
Name.
The A6M is usually known as the "Zero" from its Japanese Navy type designation, Type 0 Carrier Fighter ("Rei shiki Kanjō sentōki", 零式艦上戦闘機), taken from the last digit of the Imperial year 2600 (1940), when it entered service. In Japan, it was unofficially referred to as both "Rei-sen" and "Zero-sen"; Japanese pilots most commonly called it "Zero-sen," where "sen" is the first syllable of "sentoki," Japanese for "fighter."
In the official designation "A6M" the "A" signified a carrier-based fighter, "6" meant it was the sixth such model built for the Imperial Navy, and "M" indicated the manufacturer, Mitsubishi.
The official Allied code name was "Zeke", in keeping with the practice of giving male names to Japanese fighters, female names to bombers, bird names to gliders, and tree names to trainers. "Zeke" was part of the first batch of "hillbilly" code names assigned by Captain Frank T. McCoy of Nashville, Tennessee (assigned to the Allied Technical Air Intelligence Unit (ATAIU) at Eagle Farm Airport in Australia), who wanted quick, distinctive, easy-to-remember names. When, in 1942, the Allied code for Japanese aircraft was introduced, he logically chose "Zeke" for the "Zero." Later, two variants of the fighter received their own code names: the Nakajima A6M2-N (floatplane version of the Zero) was called "Rufe" and the A6M3-32 variant was initially called "Hap". After objections from General "Hap" Arnold, commander of the USAAF, the name was changed to "Hamp". When captured examples were examined in New Guinea, it was realized it was a variant of the Zero and finally renamed "Zeke 32."
Operational history.
The first Zeros (pre-series A6M2) went into operation in July 1940. On 13 September 1940, the Zeros scored their first air-to-air victories when 13 A6M2s led by Lieutenant Saburo Shindo attacked 27 Soviet-built Polikarpov I-15s and I-16s of the Chinese Nationalist Air Force, shooting down all the fighters without loss to themselves. By the time they were redeployed a year later, the Zeros had shot down 99 Chinese aircraft (266 according to other sources).
At the time of the attack on Pearl Harbor 420 Zeros were active in the Pacific. The carrier-borne Model 21 was the type encountered by the Americans. Its tremendous range of over 2,600 km (1,600 mi) allowed it to range farther from its carrier than expected, appearing over distant battlefronts and giving Allied commanders the impression that there were several times as many Zeros as actually existed.
The Zero quickly gained a fearsome reputation. Thanks to a combination of excellent maneuverability and firepower, it easily disposed of the motley collection of Allied aircraft sent against it in the Pacific in 1941. It proved a severe difficult opponent even for the Supermarine Spitfire. "The RAF pilots were trained in methods that were excellent against German and Italian equipment but suicide against the acrobatic Japs." as Lt.Gen. Claire Lee Chennault had to notice. Although not as fast as the British fighter, the Mitsubishi fighter could out-turn the Spitfire with ease, could sustain a climb at a very steep angle, and could stay in the air for three times as long.
Soon, however, Allied pilots developed tactics to cope with the Zero. Due to its extreme agility, engaging a Zero in a traditional, turning dogfight was likely to be fatal. It was better to roar down from above in a high-speed pass, fire a quick burst, then climb quickly back up to altitude. (A short burst of fire from heavy machine guns or cannon was often enough to bring down the fragile Zero.) Such "boom-and-zoom" tactics were used successfully in the China Burma India Theater (CBI) by the "Flying Tigers" of the American Volunteer Group (AVG) against similarly maneuverable Japanese Army aircraft such as the Nakajima Ki-27 "Nate" and Nakajima Ki-43 "Oscar". AVG pilots were trained under their commander Claire Chennault's instructions to exploit the advantages of their P-40s, which were very sturdy, heavily armed, generally faster in a dive and level flight at low altitude, with a good rate of roll.
Another important maneuver was Lieutenant Commander John S. "Jimmy" Thach's "Thach Weave", in which two fighters would fly about 60 m (200 ft) apart. If a Zero latched onto the tail of one of the fighters, the two aircraft would turn toward each other. If the Zero followed his original target through the turn, he would come into a position to be fired on by the target's wingman. This tactic was first used to good effect during the Battle of Midway, and later over the Solomon Islands. Many highly experienced Japanese aviators were lost in combat, resulting in a progressive decline in the quality of the opponents faced by Allied pilots, which became a significant factor in Allied successes. Unexpected heavy losses of these pilots at the battles of the Coral Sea and Midway dealt the Japanese carrier air force a blow from which it never fully recovered.
However, throughout the engagement at Midway, the Allied Pilots registered a high level of dissatisfaction with the current state of the Grumman F4F Wildcat. The Commanding Officer of the USS "Yorktown" noted: The fighter pilots are very disappointed with the performance and length of sustained fire power of the F4F-4 airplanes. The Zero fighters could easily outmaneuver and out-climb the F4F-3, and the consensus of fighter pilot opinion is that the F4F-4 is even more sluggish and slow than the F4F-3. It is also felt that it was a mistake to put 6 guns on the F4F-4 and thus to reduce the rounds per gun. Many of our fighters ran out of ammunition even before the Jap dive bombers arrived over our forces; these were experienced pilots, not novices.
 They were astounded by the Zero's superiority: In the Coral Sea, they made all their approaches from the rear or high side and did relatively little damage because of our armor. It also is desired to call attention to the fact that there was an absence of the fancy stunting during pull outs or approaches for attacks. In this battle, the Japs dove in, made the attack and then immediately pulled out, taking advantage of their superior climb and maneuverability. In attacking fighters, the Zeros usually attacked from above rear at high speed and recovered by climbing vertically until they lost some speed and then pulled on through to complete a small loop of high wing over which placed them out of reach and in position for another attack. By reversing the turn sharply after each attack the leader may get a shot at the enemy while he is climbing away or head on into a scissor if the Jap turns to meet it.
In contrast, Allied fighters were designed with ruggedness and pilot protection in mind. The Japanese ace Saburō Sakai described how the resilience of early Grumman aircraft was a factor in preventing the Zero from attaining total domination: I had full confidence in my ability to destroy the Grumman and decided to finish off the enemy fighter with only my 7.7 mm machine guns. I turned the 20mm cannon switch to the 'off' position, and closed in. For some strange reason, even after I had poured about five or six hundred rounds of ammunition directly into the Grumman, the airplane did not fall, but kept on flying! I thought this very odd—it had never happened before—and closed the distance between the two airplanes until I could almost reach out and touch the Grumman. To my surprise, the Grumman's rudder and tail were torn to shreds, looking like an old torn piece of rag. With his plane in such condition, no wonder the pilot was unable to continue fighting! A Zero which had taken that many bullets would have been a ball of fire by now.
When the powerfully armed Lockheed P-38 Lightning — possessing four "light barrel" AN/M2 .50 cal. Browning machine guns and one 20 mm autocannon — and the Grumman F6F Hellcat and Vought F4U Corsair, each using six of the AN/M2 heavy calibre Browning guns appeared in the Pacific theater, the A6M, with its low-powered engine and lighter armament, was hard-pressed to remain competitive. In combat with an F6F or F4U, the only positive thing that could be said of the Zero at this stage of the war was that in the hands of a skillful pilot it could maneuver as well as most of its opponents. Nonetheless, in competent hands the Zero could still be deadly.
Due to shortages of high-powered aviation engines and problems with planned successor models, the Zero remained in production until 1945, with over 11,000 of all variants produced.
Allied opinions.
The American military discovered many of the A6M's unique attributes when they recovered a largely intact specimen, the Akutan Zero, on Akutan Island in the Aleutians. During an air raid over Dutch Harbor on 4 June 1942, one A6M fighter was hit by ground fire. Losing oil, Flight Petty Officer Tadayoshi Koga attempted an emergency landing on Akutan Island about 20 miles northeast of Dutch Harbor, but his Zero flipped over in soft ground in a sudden crash landing. Koga died instantly of head injuries, but the relatively undamaged fighter was found over a month later by an American salvage team and shipped to Naval Air Station North Island where testing flights of the repaired A6M revealed both strengths and deficiencies in design and performance.
The experts who evaluated the captured Zero found that the plane weighed about 2,360 kg (5,200 pounds) fully loaded, about half the weight of the standard United States Navy fighter. It was "built like a fine watch"; the Zero was constructed with flush rivets, and even the guns were flush with the wings. The instrument panel was a "marvel of simplicity ... with no superfluities to distract [the pilot]." What most impressed the experts was that the Zero's fuselage and wings were constructed in one piece, unlike the American method that built them separately and joined the two parts together. The Japanese method was much slower, but resulted in a very strong structure and improved close maneuverability.
Captain Eric Brown, the Chief Naval Test Pilot of the Royal Navy, recalled being impressed by the Zero during tests of captured aircraft. "I don’t think I have ever flown a fighter that could match the rate of turn of the Zero. The Zero had ruled the roost totally and was the finest fighter in the world until mid-1943." American test pilots found that the Zero's controls were "very light" at 320 km/h, but stiffened at faster speeds (above 348 km/h, or 216 mph) to safeguard against wing failure. The Zero could not keep up with Allied aircraft in high speed maneuvers, and its low "never exceed speed" (VNE) made it vulnerable in a dive. While stable on the ground despite its light weight, the aircraft was designed purely for the attack role, emphasizing long range, maneuverability, and firepower at the expense of protection of its pilot. Most lacked self-sealing tanks and armor plating.
Variants.
A6M1, Type 0 Prototypes.
The first A6M1 prototype was completed in March 1939, powered by the 580 kW (780 hp) Mitsubishi Zuisei 13 engine with a two-blade propeller. It first flew on 1 April, and passed testing within a remarkably short period. By September, it had already been accepted for Navy testing as the A6M1 Type 0 Carrier Fighter, with the only notable change being a switch to a three-bladed propeller to cure a vibration problem.
A6M2 Type 0 Model 11.
While the navy was testing the first two prototypes, they suggested that the third be fitted with the 700 kW (940 hp) Nakajima Sakae 12 engine instead. Mitsubishi had its own engine of this class in the form of the Kinsei, so they were somewhat reluctant to use the Sakae. Nevertheless, when the first A6M2 was completed in January 1940, the Sakae's extra power pushed the performance of the Zero well past the original specifications.
The new version was so promising that the Navy had 15 built and shipped to China before they had completed testing. They arrived in Manchuria in July 1940, and first saw combat over Chungking in August. There they proved to be completely untouchable by the Polikarpov I-16s and I-153s that had been such a problem for the A5Ms currently in service. In one encounter, 13 Zeros shot down 27 I-15s and I-16s in under three minutes without loss. After hearing of these reports, the navy immediately ordered the A6M2 into production as the Type 0 Carrier Fighter, Model 11.
Reports of the Zero's performance filtered back to the US slowly. There they were dismissed by most military officials, who thought it was impossible for the Japanese to build such an aircraft.
A6M2 Type 0 Model 21.
After the delivery of only 65 aircraft by November 1940, a further change was worked into the production lines, which introduced folding wingtips to allow them to fit on aircraft carriers. The resulting Model 21 would become one of the most produced versions early in the war. A feature was the improved range with 520lt wing tank and 320lt drop tank. When the lines switched to updated models, 740 Model 21s had been completed by Mitsubishi, and another 800 by Nakajima. Two other versions of the Model 21 were built in small numbers, the Nakajima-built A6M2-N "Rufe" floatplane (based on the Model 11 with a slightly modified tail), and the A6M2-K two-seat trainer of which a total of 508 were built by Hitachi and the Sasebo Naval Air Arsenal.
A6M3 Type 0 Model 32.
In 1941, Nakajima introduced the Sakae 21 engine, which used a two-speed supercharger for better altitude performance, and increased power to 840 kW (1,130 hp). A prototype Zero with the new engine was first flown on July 15, 1941.
The new Sakae was slightly heavier and somewhat longer due to the larger supercharger, which moved the center of gravity too far forward on the existing airframe. To correct for this, the engine mountings were cut back by 185 mm to move the engine toward the cockpit. This had the side effect of reducing the size of the main fuselage fuel tank (located between the engine and the cockpit) from 518 L (137 US gal) to 470 L (120 US gal). The cowling was redesigned to enlarge the cowl flaps, revise the oil cooler air intake, and move the carburetor air intake to the upper half of the cowling.
The wings were redesigned to reduce span, eliminate the folding tips, and square off the wingtips. The inboard edge of the aileron was moved outboard by one rib, and the wing fuel tanks were enlarged accordingly to 420 L. The two 20 mm wing cannon were upgraded from the 99 Shiki 1 Gou Koteijū 1 Kata Kai 1 to the 99 Shiki 1 Gou Koteijū 3 Gata, which required a bulge in the sheet metal of the wing below each cannon. The wings also included larger ammunition boxes, allowing for 100 rounds for each of the 20 mm cannon.
The Sakae 21 engine and other changes increased maximum speed by only 11 kph compared to the Model 21, but sacrifed nearly 1,000 km of range. Nevertheless, the navy accepted the type and it entered production in April 1942.
The shorter wing span led to better roll, and the reduced drag allowed the diving speed to be increased to 670 km/h (420 mph). On the downside, turning and range, which were the strengths of the Model 21, suffered due to smaller ailerons, decreased lift and greater fuel consumption. The shorter range proved a significant limitation during the Solomons Campaign, during which Zeros based at Rabaul had to travel nearly to their maximum range to reach Guadalcanal and return. Consequently, the Model 32 was unsuited to that campaign and was used mainly for shorter range offensive missions and interception.
The appearance of the redesigned A6M3-32 prompted the US to assign the Model 32 a new code name, "Hap". This name was short-lived, as a protest from USAAF Commanding General Henry "Hap" Arnold forced a change to "Hamp". Soon after, it was realized that it was simply a new model of the "Zeke" and was termed "Zeke 32".
This variant was flown by only a small number of units, and only 343 were built.
A6M3 Type 0 Model 22.
In order to correct the deficiencies of the Model 32, a new version with the folding wingtips of the Model 21, new fuel tanks in the outer wings, and attachments and fuel lines for a 330 L (90 US gal) drop tank under each wing were introduced. The internal fuel was increased to 570 L (137 US gal) in this model, regaining all of the range lost in the Model 32 variant.
As the wing had significant redesign, this version received a new navy designation, 22 型 (2-2 kata, known in the West as a Model 22). The new model entered service circa December 1942 (before the navy formally accepted the new design) and 560 were eventually produced, all by Mitsubishi. This wing is seen with two armament configurations. The Model 22 has the same wing armament as in the Model 32. The 22型甲 (2-2 kata kou, aka Model 22a) has one 99式2号固定銃3型 (9-9 shiki 2 gou koteijū 3 gata) cannon in each wing. The barrel protrudes.
According to one theory, very late production Model 22s may have had a wing similar to the shortened, rounded-tip wing of the Model 52. One such plane was photographed at Lakunai Airfield ("Rabaul East") in the second half of 1943, and the photo has been published in a number of Japanese books, e.g., Aero Detail 7 and 世界の傑作機 9 at page 33 (Sekai no Kessaku Ki, aka Famous Airplanes of the World 9, 1993). Although jumping to the conclusion that the cowling is that of the Models 32 and 22, the latter book proposes that the plane is an early production Model 52, based upon an unspecified manual (see below). There is a growing body of evidence that this "hybrid" type is simply an early production Model 52 (see below). (The Model 32, 22, 22 kou, 52, 52 kou and 52 otsu were all powered by the Nakajima 栄 (Sakae) 21型 engine. That engine kept its designation in spite of changes in the exhaust system for the Model 52.)
A6M4 Type 0 Model 41/42.
Mitsubishi is unable to state with certainty that it ever used the designation "A6M4" or model numbers for it. However, "A6M4" does appear in a translation of a captured Japanese memo from a Naval Air Technical Arsenal, titled Quarterly Report on Research Experiments, dated 1 October 1942. It mentions a "cross-section of the A6M4 intercooler" then being designed. Some researchers believe "A6M4" was applied to one or two prototype planes fitted with an experimental turbo-supercharged Sakae engine designed for high altitude. Mitsubishi's involvement in the project was probably quite limited or nil; the unmodified Sakae engine was made by Nakajima. The design and testing of the turbo-supercharger was the responsibility of the 第一海軍航空廠 (Dai Ichi Kaigun Kōkūshō, First Naval Air [Technical] Arsenal) at Yokosuka. At least one photo of a prototype plane exists. It shows a turbo unit mounted in the forward left fuselage.
Lack of suitable alloys for use in the manufacture of a turbo-supercharger and its related ducting caused numerous ruptures, resulting in fires and poor performance. Consequently, further development of a turbo-supercharged A6M was cancelled. The lack of acceptance by the navy suggests that the navy did not bestow model number 41 or 42 formally, although it appears that the arsenal did use the designation "A6M4". The prototype engines nevertheless provided useful experience for future engine designs.
A6M5 Type 0 Model 52.
Sometimes considered the most effective variant, the Model 52 was developed to again shorten the wings to increase speed and dispense with the folding wing mechanism. In addition, ailerons, aileron trim tab and flaps were revised. Produced first by Mitsubishi, most Model 52s were made by Nakajima. The prototype was made in June 1943 by modifying an A6M3 and was first flown in August 1943. The first Model 52 is said in the handling manual to have 製造番号 (seizō bangō, manufacture number) 3904, which apparently refers to the prototype.
Research by Mr. Bunzo Komine published by Mr. Kenji Miyazaki states that aircraft 3904 through 4103 had the same exhaust system and cowl flaps as on the Model 22. This is partially corroborated by two wrecks researched by Mr. Stan Gajda and Mr. L. G. Halls, seizō bangō 4007 and 4043, respectively. (The upper cowling was slightly redesigned from that of the Model 22.)
A new exhaust system provided an increment of thrust by aiming the stacks aft and distributing them around the forward fuselage. The new exhaust system required "notched" cowl flaps and heat shields just aft of the stacks. (Note, however, that the handling manual translation states that the new style of exhaust commenced with number 3904. Whether this is correct, indicates retrofitting intentions, refers to the prototype but not to all subsequent planes, or is in error is not clear.) From seizō bangō 4274, the wing fuel tanks received carbon dioxide fire extinguishers. From number 4354, the radio became the Model 3, aerial Mark 1, and at that point it is said the antenna mast was shortened slightly. Through seizō bangō 4550, the lowest exhaust stacks were approximately the same length as those immediately above them. This caused hot exhaust to burn the forward edge of the landing gear doors and heat the tires. Therefore, from number 4551 Mitsubishi began to install shorter bottom stacks. Nakajima manufactured the Model 52 at its Koizumi plant in Gunma Prefecture. The A6M5 had a maximum speed of 565 km/h (351 mph) at 6,000 meters and reached that altitude in 7:01 minutes.
Subsequent variants included:
Some Model 21 and 52 aircraft were converted to "bakusen" (fighter-bombers) by hanging a bomb rack and 250 kg bomb in place of the centerline drop tank.
Perhaps seven Model 52 planes were ostensibly converted into A6M5-K two-seat trainers; mass production was contemplated by Hitachi but not undertaken.
A6M6c Type 0 Model 53c.
This was similar to the A6M5c, but with self-sealing wing tanks and a Nakajima Sakae 31a engine featuring water-methanol engine boost.
A6M7 Type 0 Model 62.
Similar to the A6M6 but intended for attack or Kamikaze role.
A6M8 Type 0 Model 64.
Similar to the A6M6 but with the Sakae (now out of production) replaced by the Mitsubishi Kinsei 62 engine with 1,560 hp (1,164 kW), 60% more powerful than the engine of the A6M2. This resulted in an extensively modified cowling and nose for the aircraft. The carburetor intake was much larger, a long duct like that on the Nakajima B6N Tenzan was added, and a large spinner—like that on the Yokosuka D4Y Suisei with the Kinsei 62—was mounted. The larger cowling meant deletion of the fuselage-mounted machine gun, but armament was otherwise unchanged from the Model 52 Hei (2 x 20 mm cannon; 2 x 13 mm/.51 in MG). In addition, the Model 64 was modified to carry two 150 L (40 US gal) drop tanks on either wing in order to permit the mounting of a 250 kg (550 lb) bomb on the underside of the fuselage. Two prototypes were completed in April 1945 but the chaotic situation of Japanese industry and the end of the war obstructed the start of the ambitious program of production for 6,300 machines, none being completed.
Survivors.
Several Zero fighters survived the war and are on display: 
Another aircraft recovered by the Australian War Memorial Museum in the early 1970s now belongs to Fantasy of Flight in Polk City, Florida. Along with several other Zeros, it was found near Rabaul in the South Pacific. The markings suggest that it was in service after June 1943 and further investigation suggests that it has cockpit features associated with the Nakajima-built Model 52b. If this is correct, it is most likely one of the 123 aircraft lost by the Japanese during the assault of Rabaul. The aircraft was shipped in pieces to the attraction and it was eventually made up for display as a crashed aircraft. Much of the aircraft is usable for patterns and some of its parts can be restored to one day make this a basis for a flyable aircraft.
Four flyable Zero airframes exist. Three have had their engines replaced with similar American units. Only one, the Planes of Fame Museum's A6M5 bearing tail number "61-120" (recovered from Saipan) has the original Sakae engine.
Although not a survivor, the "Blayd" Zero is a reconstruction based on templating original Zero components recovered from the South Pacific. To be considered a "restoration" and not a reproduction, the builders used a small fraction of parts from the original Zero landing gear in the reconstruction. Restored as a A6M2 Model 21, it is currently displayed at the Texas Flying Legends Museum and flown by its owner Warren Pietsch.
The Commemorative Air Force's A6M3 was recovered from Babo Airfield, New Guinea, in 1991. It was partially restored from several A6M3s in Russia, then brought to the United States for restoration. The aircraft was re-registered in 1998 and displayed at the Museum of Flying in Santa Monica, California. It currently uses a Pratt & Whitney R1830 engine. Another A6M3 was also recovered from Babo Airfield and restored with a P&W engine. It currently is owned by the Flying Heritage Collection in Everett, Washington.
The rarity of flyable Zeros accounts for the use of single-seat North American T-6 Texans, with heavily modified fuselages and painted in Japanese markings, as substitutes for Zeros in the films "Tora! Tora! Tora!", "The Final Countdown", and many other television and film depictions of the aircraft, such as "Baa Baa Black Sheep" (renamed "Black Sheep Squadron"). One Model 52 was used during the production of "Pearl Harbor".
References.
Bibliography.
</dl>

</doc>
<doc id="19624" url="http://en.wikipedia.org/wiki?curid=19624" title="May 27">
May 27

May 27 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19626" url="http://en.wikipedia.org/wiki?curid=19626" title="Monasticism">
Monasticism

Monasticism (from Greek μοναχός, "monachos", derived from μόνος, "monos", "alone") or monkhood is a religious way of life in which one renounces worldly pursuits to devote oneself fully to spiritual work. Monastic life plays an important role in many Christian churches, especially in the Catholic and Orthodox traditions. Similar forms of religious life also exist in other faiths, most notably in Buddhism, but also in Hinduism and Jainism, although the expressions differ considerably.
Males pursuing a monastic life are generally called "monks" while female monastics are called "nuns". Many monks and nuns live in monasteries to stay away from the secular world. The way of addressing monastics differs between the Christian traditions. As a general rule, in Roman Catholicism, monks and nuns are called brother or sister, while in Orthodox Christianity, they are called father or mother.
Buddhism.
The Sangha or community of ordained Buddhist bhikkhus (similar to monks) and original bhikkhunis (similar to nuns) was founded by Gautama Buddha during his lifetime over 2500 years ago. This communal monastic lifestyle grew out of the lifestyle of earlier sects of wandering ascetics, some of whom the Buddha had studied under. It was initially fairly eremitic or reclusive in nature. Bhikkhus and bhikkunis were expected to live with a minimum of possessions, which were to be voluntarily provided by the lay community. Lay followers also provided the daily food that bhikkhus required, and provided shelter for bhikkhus when they were needed.
After the Parinibbana (Final Passing) of the Buddha, the Buddhist monastic order developed into a primarily cenobitic or communal movement. The practice of living communally during the rainy vassa season, prescribed by the Buddha, gradually grew to encompass a settled monastic life centered on life in a community of practitioners. Most of the modern disciplinary rules followed by bhikkhus and bhikkhunis — as encoded in the Patimokkha — relate to such an existence, prescribing in great detail proper methods for living and relating in a community of bhikkhus or bhikkhunis. The number of rules observed varies with the order; Theravada bhikkhus follow around 227 rules. There are a larger number of rules specified for bhikkhunis (nuns).
The Buddhist monastic order consists of the male bhikkhu assembly and the female bhikkhuni assembly. Initially consisting only of males, it grew to include females after the Buddha's stepmother, Mahaprajapati, asked for and received permission to live as an ordained practitioner.
Bhikkhus and bhikkhunis are expected to fulfill a variety of roles in the Buddhist community. First and foremost, they are expected to preserve the doctrine and discipline now known as Buddhism. They are also expected to provide a living example for the laity, and to serve as a "field of merit" for lay followers—providing laymen and women with the opportunity to earn merit by giving gifts and support to the bhikkhus. In return for the support of the laity, bhikkhus and bhikkhunis are expected to live an austere life focused on the study of Buddhist doctrine, the practice of meditation, and the observance of good moral character.
A bhikkhu (the term in the Pali language) or Bhikshu (in Sanskrit), first ordains as a "Samanera" (novice). Novices often ordain at a young age, but generally no younger than eight. Samaneras live according to the Ten Precepts, but are not responsible for living by the full set of monastic rules. Higher ordination, conferring the status of a full Bhikkhu, is given only to men who are aged 20 or older. Bhikkhunis follow a similar progression, but are required to live as Samaneras for longer periods of time- typically five years.
The disciplinary regulations for bhikkhus and bhikkhunis are intended to create a life that is simple and focused, rather than one of deprivation or severe asceticism. However, celibacy is a fundamental part of this form of monastic discipline.
Christianity.
Monasticism in Christianity, which provides the origins of the words "monk" and "monastery", comprises several diverse forms of religious living. It began to develop early in the history of the Church, but is not mandated as an institution in the scriptures. It has come to be regulated by religious rules (e.g. the Rule of St Basil, the Rule of St Benedict) and, in modern times, the Church law of the respective apostolic Christian churches that have forms of monastic living.
The Christian monk embraces the monastic life as a vocation for God. His goal is to attain eternal life in his presence. The rules of monastic life are codified in the "counsels of perfection".
In the beginning, in Egypt, Christians felt called to a more reclusive or eremitic form of monastic living (in the spirit of the "Desert Theology" for the purpose of spiritual renewal and return to God). Saint Anthony the Great is cited by Athanasius as one of these early "Hermit monks". Especially in the Middle East, eremitic monasticism continued to be common until the decline of Syriac Christianity in the late Middle Ages.
The need for some form of organized spiritual guidance was obvious; and around 318 Saint Pachomius started to organize his many followers in what was to become the first Christian cenobitic or communal monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Notable monasteries of the East include:
In the West, the most significant development occurred when the rules for monastic communities were written, the Rule of St Basil being credited with having been the first. The precise dating of the Rule of the Master is problematic; but it has been argued on internal grounds that it antedates the so-called Rule of Saint Benedict created by Benedict of Nursia for his monastery in Monte Cassino, Italy (c. 529), and the other Benedictine monasteries he himself had founded (cf. Order of St Benedict). It would become the most common rule throughout the Middle Ages and is still in use today. The Augustinian Rule, due to its brevity, has been adopted by various communities, chiefly the Canons Regular. Around the 12th century, the Franciscan, Carmelite, Dominican, Servite Order (see Servants of Mary) and Augustinian mendicant orders chose to live in city convents among the people instead of being secluded in monasteries.
Today new expressions of Christian monasticism, many of which are ecumenical, are developing in various places such as the Bose Monastic Community in Italy, the Monastic Fraternities of Jerusalem throughout Europe, the New Skete, the Anglo-Celtic Society of Nativitists, the Taizé Community in France, and the mainly Evangelical Protestant New Monasticism.
Hinduism.
In their quest to attain the spiritual goal of life, some Hindus choose the path of monasticism (Sannyasa). Monastics commit themselves to a life of simplicity, celibacy, detachment from worldly pursuits, and the contemplation of God. A Hindu monk is called a s"anyāsī, sādhu", or "swāmi". A nun is called a "sanyāsini", "sādhvi", or "swāmini". Such renunciates are accorded high respect in Hindu society, because their outward renunciation of selfishness and worldliness serves as an inspiration to householders who strive for "mental" renunciation. Some monastics live in monasteries, while others wander from place to place, trusting in God alone to provide for their physical needs. It is considered a highly meritorious act for a lay devotee to provide sadhus with food or other necessaries. Sādhus are expected to treat all with respect and compassion, whether a person may be poor or rich, good or wicked. They are also expected to be indifferent to praise, blame, pleasure, and pain. A sādhu can typically be recognized by his ochre-colored clothing. Generally, Vaisnava monks shave their heads except for a small patch of hair on the back of the head, while Saivite monks let their hair and beard grow uncut.
A "sadhu's" vow of renunciation typically forbids him from:
Islam.
Islam does not allow the practice of monasticism and is critical of its practice. One example is Uthman bin Maz'oon; one of the companions of Muhammad. He was married to Khawlah bint Hakim, both being two of the earliest converts to Islam. There is a narration that, out of religious devotion, Uthman bin Maz'oon decided to dedicate himself to night prayers and take a vow of chastity from his wife. His wife got upset and spoke to Muhammad about this. Muhammad reminded Uthman that he himself, as the Prophet, also had a family life, and that Uthman had a responsibility to his family and should not adopt monasticism as a form of religious practice.
Muhammad told his companions to ease their burden and avoid excess. According to some hadiths, in a message to some companions who wanted to put an end to their sexual life, pray all night long or fast continuously, Muhammad said: “Do not do that! Fast on some days and eat on others. Sleep part of the night, and stand in prayer another part. For your body has rights upon you, your eyes have a right upon you, you wife has a right upon you, your guest has a right upon you.” Muhammad once exclaimed, repeating it three times: “Woe to those who exaggerate [who are too strict]!” And, on another occasion, Muhammad said: “Moderation, moderation! For only with moderation will you succeed.”
It is also mentioned in four places in the following verses of Qur'an:
 We sent other messengers to follow in their footsteps. After those We sent Jesus, son of Mary: We gave him the gospel and put compassion and mercy into the hearts of his followers. But monasticism was something they invented — We did not ordain it for them — only to seek God's pleasure, and even so, they did not observe it properly. So We gave a reward to those of them who believed, but many of them were lawbreakers.
They have taken as lords beside Allah their rabbis and their monks and the Messiah son of Mary, when they were bidden to worship only One God. There is no god save Him. Be He glorified from all that they ascribe as partner (unto Him)!
O ye who believe! Lo! many of the (Jewish) rabbis and the (Christian) monks devour the wealth of mankind wantonly and debar (men) from the way of Allah. They who hoard up gold and silver and spend it not in the way of Allah, unto them give tidings (O Muhammad) of a painful doom
Thou wilt find the most vehement of mankind in hostility to those who believe (to be) the Jews and the idolaters. And thou wilt find the nearest of them in affection to those who believe (to be) those who say: Lo! We are Christians. That is because there are among them priests and monks, and because they are not proud.
Jainism.
In Jainism, monasticism is encouraged and respected. Rules for monasticism are rather strict. A Jain ascetic has neither a permanent home nor any possessions, wandering barefoot from place to place except during the months of Chaturmas. The quality of life they lead is difficult because of the many constraints placed on them. They don't use a vehicle for commuting and always commute barefoot from one place to another, irrespective of the distance. They don't possess any materialistic things and also don't use the basic services like that of a phone, electricity etc. They don't prepare food and live only on what people offer them.
Judaism.
Judaism does not encourage the monastic ideal of celibacy and poverty. To the contrary—all of the Torah's Commandments are a means of sanctifying the physical world. As further disseminated through the teachings of the Yisrael Ba'al Shem Tov, the pursuit of permitted physical pleasures is encouraged as a means to "serve God with joy" (Deut. 28:47).
However, until the Destruction of the Second Temple, about two thousand years ago, taking Nazirite vows was a common feature of the religion. Nazirite Jews (in Hebrew: נזיר) abstained from grape products, haircuts, and contact with the dead. However, they did not withdraw from general society, and they were permitted to marry and own property; moreover, in most cases a Nazirite vow was for a specified time period and not permanent. In Modern Hebrew, the term "Nazir" is most often used to refer to non-Jewish monastics.
Unique among Jewish communities is the monasticism of the Beta Israel of Ethiopia, a practice believed to date to the 15th century.
A form of asceticism was practiced by some individuals in pre–World War II European Jewish communities. Its principal expression was "prishut", the practice of a married Talmud student going into self-imposed exile from his home and family to study in the kollel of a different city or town. This practice was associated with, but not exclusive to, the Perushim.
The Essenes (in Modern but not in Ancient Hebrew: אִסִּיִים, "Isiyim"; Greek: Εσσηνοι, Εσσαιοι, or Οσσαιοι; "Essēnoi", "Essaioi", or "Ossaioi") were a Jewish sect that flourished from the 2nd century BC to AD 100 which some scholars claim seceded from the Zadokite priests. Being much fewer in number than the Pharisees and the Sadducees (the other two major sects at the time), the Essenes lived in various cities but congregated in communal life dedicated to asceticism, voluntary poverty, daily immersion (in mikvah), and abstinence from worldly pleasures, including (for some groups) marriage. Many separate but related religious groups of that era shared similar mystic, eschatological, messianic, and ascetic beliefs. These groups are collectively referred to by various scholars as the "Essenes". Josephus records that Essenes existed in large numbers, and thousands lived throughout Roman Judaea.
The Essenes have gained fame in modern times as a result of the discovery of an extensive group of religious documents known as the Dead Sea Scrolls, which are commonly believed to be the Essenes' library—although there is no proof that the Essenes wrote them. These documents include multiple preserved copies of the Hebrew Bible which were untouched from as early as 300 years before Christ until their discovery in 1946. Some scholars, however, dispute the notion that the Essenes wrote the Dead Sea Scrolls. Rachel Elior, a prominent Israeli scholar, even questions the existence of the Essenes.

</doc>
<doc id="19629" url="http://en.wikipedia.org/wiki?curid=19629" title="May 10">
May 10

May 10 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19631" url="http://en.wikipedia.org/wiki?curid=19631" title="May 17">
May 17

May 17 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19632" url="http://en.wikipedia.org/wiki?curid=19632" title="May 19">
May 19

May 19 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19633" url="http://en.wikipedia.org/wiki?curid=19633" title="March 3">
March 3

March 3 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19635" url="http://en.wikipedia.org/wiki?curid=19635" title="March 15">
March 15

March 15 is the day of the year in the Gregorian calendar.
In the Roman calendar, March 15 was known as the Ides of March.

</doc>
<doc id="19636" url="http://en.wikipedia.org/wiki?curid=19636" title="Mathematical logic">
Mathematical logic

Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics. Topically, mathematical logic bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.
Mathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory. These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.
Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.
Subfields and scope.
The "Handbook of Mathematical Logic" makes a rough division of contemporary mathematical logic into four areas:
Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.
The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.
History.
Mathematical logic emerged in the mid-19th century as a subfield of mathematics independent of the traditional study of logic (Ferreirós 2001, p. 443). Before this emergence, logic was studied with rhetoric, through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.
Early history.
Theories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.
19th century.
In the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic. Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics (Katz 1998, p. 686).
Charles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.
Gottlob Frege presented an independent development of logic with quantifiers in his "Begriffsschrift", published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century. The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.
From 1890 to 1905, Ernst Schröder published "Vorlesungen über die Algebra der Logik" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.
Foundational theories.
Concerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.
In logic, the term "arithmetic" refers to the theory of the natural numbers. Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the recursive definitions of addition and multiplication from the successor function and mathematical induction.
In the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p. 774). In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882). The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line. This would prove to be a major area of research in the first half of the 20th century.
The 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817 (Felscher 2000), but remained relatively unknown.
Cauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34). In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.
Georg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 (Katz 1998, p. 807).
20th century.
In the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.
In 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's "Entscheidungsproblem", posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.
Set theory and paradoxes.
Ernst Zermelo (1904) gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a). This paper led to the general acceptance of the axiom of choice in the mathematics community.
Skepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.
Zermelo (1908b) provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.
In 1910, the first volume of "Principia Mathematica" by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. "Principia Mathematica" is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreirós 2001, p. 445).
Fraenkel (1922) proved that the axiom of choice cannot be proved from the remaining axioms of Zermelo's set theory with urelements. Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.
Symbolic logic.
Leopold Löwenheim (1915) and Thoralf Skolem (1920) obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.
In his doctoral thesis, Kurt Gödel (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.
In 1931, Gödel published "On Formally Undecidable Propositions of Principia Mathematica and Related Systems", which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic. Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.
Gödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen (1936) proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory. Gödel (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intutitionistic arithmetic in higher types.
Beginnings of the other branches.
Alfred Tarski developed the basics of model theory.
Beginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words "bijection", "injection", and "surjection", and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.
The study of computability came to be known as recursion theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.
Numerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.
Formal logical systems.
At its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language. The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties. Stronger classical logics such as second-order logic or infinitary logic are also studied, along with nonclassical logics such as intuitionistic logic.
First-order logic.
First-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.
Early results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.
Gödel's completeness theorem (Gödel 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.
Gödel's incompleteness theorems (Gödel 1931) establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any sufficiently strong, effectively given logical system there exists a statement which is true but not provable within that system. Here a logical system is effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom. A logical system is sufficiently strong if it can express the Peano axioms. When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.
Other classical logics.
Many logics besides first-order logic are studied. These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.
The most well studied infinitary logic is formula_1. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of formula_1 such as
Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type. The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.
Another type of logics are fixed-point logics that allow inductive definitions, like one writes for primitive recursive functions.
One can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic. Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the Downward Löwenheim–Skolem theorem is first-order logic.
Nonclassical and modal logic.
Modal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability (Solovay 1976) and set-theoretic forcing (Hamkins and Löwe 2007).
Intuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true. Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.
Algebraic logic.
Algebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.
Set theory.
Set theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.
Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF). Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.
Two famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians. It states that given a collection of nonempty sets there is a single set "C" that contains exactly one element from each set in the collection. The set "C" is said to "choose" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.
The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory (Cohen 1966). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).
Contemporary research in set theory includes the study of large cardinals and determinacy. Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC. Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line. "Determinacy" refers to the possible existence of winning strategies for certain two-player games (the games are said to be "determined"). The existence of these strategies implies structural properties of the real line and other Polish spaces.
Model theory.
Model theory studies the models of various formal theories. Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.
The set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.
The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with o-minimal structures.
Morley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.
A trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis. Many special cases of this conjecture have been established.
Recursion theory.
Recursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability. Recursion theory also includes the study of generalized computability and definability. Recursion theory grew from the work of Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.
Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems. More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.
Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.
Contemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.
Algorithmically unsolvable problems.
An important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.
There are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959. The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.
Hilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).
Proof theory and constructive mathematics.
Proof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques. Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.
The study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems. An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).
Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability. The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest. Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or "translate") classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.
Recent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.
Connections with computer science.
The study of computability theory in computer science is closely related to the study of computability in mathematical logic. There is a difference of emphasis, however. Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.
The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.
Computer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.
Descriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.
Foundations of mathematics.
In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.
Cantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated "God made the integers; all else is the work of man," endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying "No one shall expel us from the Paradise that Cantor has created."
Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs. In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining "point" to mean a point on a fixed sphere and "line" to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.
With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term "finitary" to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.
A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of "constructive". At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. 
In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to "intuit" the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true. Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.

</doc>
<doc id="19637" url="http://en.wikipedia.org/wiki?curid=19637" title="Molecular nanotechnology">
Molecular nanotechnology

Molecular nanotechnology (MNT) is a technology based on the ability to build structures to complex, atomic specifications by means of mechanosynthesis. This is distinct from nanoscale materials. Based on Richard Feynman's vision of miniature factories using nanomachines to build complex products (including additional nanomachines), this advanced form of nanotechnology (or "molecular manufacturing") would make use of positionally-controlled mechanosynthesis guided by molecular machine systems. MNT would involve combining physical principles demonstrated by chemistry, other nanotechnologies, and the molecular machinery of life with the systems engineering principles found in modern macroscale factories.
Introduction.
While conventional chemistry uses inexact processes obtaining inexact results, and biology exploits inexact processes to obtain definitive results, molecular nanotechnology would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.
A roadmap for the development of MNT is an objective of a broadly based technology project led by Battelle (the manager of several U.S. National Laboratories) and the Foresight Institute. The roadmap was originally scheduled for completion by late 2006, but was released in January 2008. The Nanofactory Collaboration is a more focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development. In August 2005, a task force consisting of 50+ international experts from various fields was organized by the Center for Responsible Nanotechnology to study the societal implications of molecular nanotechnology.
Projected applications and capabilities.
Smart materials and nanosensors.
One proposed application of MNT is so-called smart materials. This term refers to any sort of material designed and engineered at the nanometer scale for a specific task. It encompasses a wide variety of possible commercial applications. One example would be materials designed to respond differently to various molecules; such a capability could lead, for example, to artificial drugs which would recognize and render inert specific viruses. Another is the idea of self-healing structures, which would repair small tears in a surface naturally in the same way as self-sealing tires or human skin.
A MNT nanosensor would resemble a smart material, involving a small component within a larger machine that would react to its environment and change in some fundamental, intentional way. A very simple example: a photosensor might passively measure the incident light and discharge its absorbed energy as electricity when the light passes above or below a specified threshold, sending a signal to a larger machine. Such a sensor would supposedly cost less and use less power than a conventional sensor, and yet function usefully in all the same applications — for example, turning on parking lot lights when it gets dark.
While smart materials and nanosensors both exemplify useful applications of MNT, they pale in comparison with the complexity of the technology most popularly associated with the term: the replicating nanorobot.
Replicating nanorobots.
MNT nanofacturing is popularly linked with the idea of swarms of coordinated nanoscale robots working together, a popularization of an early proposal by K. Eric Drexler in his 1986 discussions of MNT, but . In this early proposal, sufficiently capable nanorobots would construct more nanorobots in an artificial environment containing special molecular building blocks.
Critics have doubted both the feasibility of self-replicating nanorobots and the feasibility of control if self-replicating nanorobots could be achieved: they cite the possibility of mutations removing any control and favoring reproduction of mutant pathogenic variations. Advocates address the first doubt by pointing out that the first macroscale autonomous machine replicator, made of Lego blocks, was built and operated experimentally in 2002. While there are sensory advantages present at the macroscale compared to the limited sensorium available at the nanoscale, proposals for positionally controlled nanoscale mechanosynthetic fabrication systems employ dead reckoning of tooltips combined with reliable reaction sequence design to ensure reliable results, hence a limited sensorium is no handicap; similar considerations apply to the positional assembly of small nanoparts. Advocates address the second doubt by arguing that bacteria are (of necessity) evolved to evolve, while nanorobot mutation could be actively prevented by common error-correcting techniques. Similar ideas are advocated in the Foresight Guidelines on Molecular Nanotechnology, and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous proposed methods by which replicators could, in principle, be safely controlled by good design.
However, the concept of suppressing mutation raises the question: How can design evolution occur at the nanoscale without a process of random mutation and deterministic selection? Critics argue that MNT advocates have not provided a substitute for such a process of evolution in this nanoscale arena where conventional sensory-based selection processes are lacking. The limits of the sensorium available at the nanoscale could make it difficult or impossible to winnow successes from failures. Advocates argue that design evolution should occur deterministically and strictly under human control, using the conventional engineering paradigm of modeling, design, prototyping, testing, analysis, and redesign.
In any event, since 1992 do not include self-replicating nanorobots, and recent ethical guidelines put forth by MNT advocates prohibit unconstrained self-replication.
Medical nanorobots.
One of the most important applications of MNT would be medical nanorobotics or nanomedicine, an area pioneered by Robert Freitas in numerous books and papers. The ability to design, build, and deploy large numbers of medical nanorobots would, at a minimum, make possible the rapid elimination of disease and the reliable and relatively painless recovery from physical trauma. Medical nanorobots might also make possible the convenient correction of genetic defects, and help to ensure a greatly expanded lifespan. More controversially, medical nanorobots might be used to augment natural human capabilities. However, mechanical medical nanodevices would not be allowed (or designed) to self-replicate inside the human body, nor would medical nanorobots have any need for self-replication themselves since they would be manufactured exclusively in carefully regulated nanofactories.
Utility fog.
Another proposed application of molecular nanotechnology is "utility fog" — in which a cloud of networked microscopic robots (simpler than assemblers) would change its shape and properties to form macroscopic objects and tools in accordance with software commands. Rather than modify the current practices of consuming material goods in different forms, utility fog would simply replace many physical objects.
Phased-array optics.
Yet another proposed application of MNT would be phased-array optics (PAO). However, this appears to be a problem addressable by ordinary nanoscale technology. PAO would use the principle of phased-array millimeter technology but at optical wavelengths. This would permit the duplication of any sort of optical effect but virtually. Users could request holograms, sunrises and sunsets, or floating lasers as the mood strikes. PAO systems were described in BC Crandall's "Nanotechnology: Molecular Speculations on Global Abundance" in the Brian Wowk article "Phased-Array Optics."
Potential social impacts.
Benefits.
Nanotechnology (or molecular nanotechnology to refer more specifically to the goals discussed here) will let us continue the historical trends in manufacturing right up to the fundamental limits imposed by physical law. It will let us make remarkably powerful molecular computers. It will let us make materials over fifty times lighter than steel or aluminium alloy but with the same strength. We'll be able to make jets, rockets, cars or even chairs that, by today's standards, would be remarkably light, strong, and inexpensive. Molecular surgical tools, guided by molecular computers and injected into the blood stream could find and destroy cancer cells or invading bacteria, unclog arteries, or provide oxygen when the circulation is impaired.
Nanotechnology will replace our entire manufacturing base with a new, radically more precise, radically less expensive, and radically more flexible way of making products. The aim is not simply to replace today's computer chip making plants, but also to replace the assembly lines for cars, televisions, telephones, books, surgical tools, missiles, bookcases, airplanes, tractors, and all the rest. The objective is a pervasive change in manufacturing, a change that will leave virtually no product untouched. Economic progress and military readiness in the 21st Century will depend fundamentally on maintaining a competitive position in nanotechnology.
Despite the current early developmental status of nanotechnology and molecular nanotechnology, much concern surrounds MNT's anticipated impact on economics and on law. Whatever the exact effects, MNT, if achieved, would tend to reduce the scarcity of manufactured goods and make many more goods (such as food and health aids) manufacturable.
It is generally considered that future citizens of a molecular-nanotechnological society would still need money, in the form of unforgeable digital cash or physical specie (in special circumstances). They might use such money to buy goods and services that are unique, or limited within the solar system. These might include: matter, energy, information, real estate, design services, entertainment services, legal services, fame, political power, or the attention of other people to one's political/religious/philosophical message. Furthermore, futurists must consider war, even between prosperous states, and non-economic goals.
If MNT were realized, some resources would remain limited, because unique physical objects are limited (a plot of land in the real Jerusalem, mining rights to the larger near-earth asteroids) or because they depend on the goodwill of a particular person (the love of a famous person, a live audience in a musical concert). Demand will always exceed supply for some things, and a political economy may continue to exist in any case. Whether the interest in these limited resources would diminish with the advent of virtual reality, where they could be easily substituted, is yet unclear. One reason why it might not is a hypothetical preference for "the real thing", although such an opinion could easily be mollified if virtual reality were to develop to a certain level of quality.
MNT should make possible nanomedical capabilities able to cure any medical condition not already cured by advances in other areas. Good health would be common, and poor health of any form would be as rare as smallpox and scurvy are today. Even cryonics would be feasible, as cryopreserved tissue could be fully repaired.
Risks.
Molecular nanotechnology is one of the technologies that some analysts believe could lead to a Technological Singularity.
Some feel that molecular nanotechnology would have daunting risks. It conceivably could enable cheaper and more destructive conventional weapons. Also, molecular nanotechnology might permit weapons of mass destruction that could self-replicate, as viruses and cancer cells do when attacking the human body. Commentators generally agree that, in the event molecular nanotechnology were developed, its self-replication should be permitted only under very controlled or "inherently safe" conditions.
A fear exists that nanomechanical robots, if achieved, and if designed to self-replicate using naturally occurring materials (a difficult task), could consume the entire planet in their hunger for raw materials, or simply crowd out natural life, out-competing it for energy (as happened historically when blue-green algae appeared and outcompeted earlier life forms). Some commentators have referred to this situation as the "grey goo" or "ecophagy" scenario. K. Eric Drexler considers an accidental "grey goo" scenario extremely unlikely and says so in later editions of "Engines of Creation".
In light of this perception of potential danger, the Foresight Institute (founded by K. Eric Drexler to prepare for the arrival of future technologies) has drafted a set of guidelines for the ethical development of nanotechnology. These include the banning of free-foraging self-replicating pseudo-organisms on the Earth's surface, at least, and possibly in other places.
Technical issues and criticism.
The feasibility of the basic technologies analyzed in "Nanosystems" has been the subject of a formal scientific review by U.S. National Academy of Sciences, and has also been the focus of extensive debate on the internet and in the popular press.
Study and recommendations by the U.S. National Academy of Sciences.
In 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" The study committee reviewed the technical content of "Nanosystems", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:
Assemblers versus nanofactories.
A section heading in Drexler's "Engines of Creation" reads "Universal Assemblers", and the following text speaks of multiple types of assemblers which, collectively, could hypothetically "build almost anything that the laws of nature allow to exist." Drexler's colleague Ralph Merkle has noted that, contrary to widespread legend, Drexler never claimed that assembler systems could build absolutely any molecular structure. The endnotes in Drexler's book explain the qualification "almost": "For example, a delicate structure might be designed that, like a stone arch, would self-destruct unless all its pieces were already in place. If there were no room in the design for the placement and removal of a scaffolding, then the structure might be impossible to build. Few structures of practical interest seem likely to exhibit such a problem, however."
In 1992, Drexler published "Nanosystems: Molecular Machinery, Manufacturing, and Computation", a detailed proposal for synthesizing stiff covalent structures using a table-top factory. Diamondoid structures and other stiff covalent structures, if achieved, would have a wide range of possible applications, going far beyond current MEMS technology. An outline of a path was put forward in 1992 for building a table-top factory in the absence of an assembler. Other researchers have begun advancing tentative, alternative proposed paths for this in the years since Nanosystems was published.
Hard versus soft nanotechnology.
In 2004 wrote Soft Machines (nanotechnology and life), a book for lay audiences published by Oxford University. In this book he describes radical nanotechnology (as advocated by Drexler) as a deterministic/mechanistic idea of nano engineered machines that does not take into account the nanoscale challenges such as wetness, stickness, Brownian motion, and high viscosity. He also explains what is soft nanotechnology or more appropriatelly biomimetic nanotechnology which is the way forward, if not the best way, to design functional nanodevices that can cope with all the problems at a nanoscale. One can think of soft nanotechnology as the development of nanomachines that uses the lessons learned from biology on how things work, chemistry to precisely engineer such devices and stochastic physics to model the system and its natural processes in detail.
The Smalley-Drexler debate.
Several researchers, including Nobel Prize winner Dr. Richard Smalley (1943–2005), attacked the notion of universal assemblers, leading to a rebuttal from Drexler and colleagues, and eventually to an exchange of letters. Smalley argued that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Drexler and colleagues, however, noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in "Nanosystems". Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible. However, Drexler addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. It is noteworthy that, contrary to Smalley's opinion that enzymes require water, "Not only do enzymes work vigorously in anhydrous organic media, but in this unnatural milieu they acquire remarkable properties such as greatly enhanced stability, radically altered substrate and enantiomeric specificities, molecular memory, and the ability to catalyse unusual reactions.""
Design issues.
For the future, some means have to be found for MNT design evolution at the nanoscale which mimics the process of biological evolution at the molecular scale. Biological evolution proceeds by random variation in ensemble averages of organisms combined with culling of the less-successful variants and reproduction of the more-successful variants, and macroscale engineering design also proceeds by a process of design evolution from simplicity to complexity as set forth somewhat satirically by John Gall: "A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works." A breakthrough in MNT is needed which proceeds from the simple atomic ensembles which can be built with, e.g., an STM to complex MNT systems via a process of design evolution. A handicap in this process is the difficulty of seeing and manipulation at the nanoscale compared to the macroscale which makes deterministic selection of successful trials difficult; in contrast biological evolution proceeds via action of what Richard Dawkins has called the "blind watchmaker"
comprising random molecular variation and deterministic reproduction/extinction.
At present in 2007 the practice of nanotechnology embraces both stochastic approaches (in which, for example, supramolecular chemistry creates waterproof pants) and deterministic approaches wherein single molecules (created by stochastic chemistry) are manipulated on substrate surfaces (created by stochastic deposition methods) by deterministic methods comprising nudging them with STM or AFM probes and causing simple binding or cleavage reactions to occur. The dream of a complex, deterministic molecular nanotechnology remains elusive. Since the mid-1990s, thousands of surface scientists and thin film technocrats have latched on to the nanotechnology bandwagon and redefined their disciplines as nanotechnology. This has caused much confusion in the field and has spawned thousands of "nano"-papers on the peer reviewed literature. Most of these reports are extensions of the more ordinary research done in the parent fields.
The feasibility of the proposals in "Nanosystems".
The feasibility of Drexler's proposals largely depends, therefore, on whether designs like those in "Nanosystems" could be built in the absence of a universal assembler to build them and would work as described. Supporters of molecular nanotechnology frequently claim that no significant errors have been discovered in "Nanosystems" since 1992. Even some critics concede that "Drexler has carefully considered a number of physical principles underlying the 'high level' aspects of the nanosystems he proposes and, indeed, has thought in some detail" about some issues.
Other critics claim, however, that "Nanosystems" omits important chemical details about the low-level 'machine language' of molecular nanotechnology. They also claim that much of the other low-level chemistry in "Nanosystems" requires extensive further work, and that Drexler's higher-level designs therefore rest on speculative foundations. Recent such further work by Freitas and Merkle is aimed at strengthening these foundations by filling the existing gaps in the low-level chemistry.
Drexler argues that we may need to wait until our conventional nanotechnology improves before solving these issues: "Molecular manufacturing will result from a series of advances in molecular machine systems, much as the first Moon landing resulted from a series of advances in liquid-fuel rocket systems. We are now in a position like that of the British Interplanetary Society of the 1930s which described how multistage liquid-fueled rockets could reach the Moon and pointed to early rockets as illustrations of the basic principle." However, Freitas and Merkle argue that a focused effort to achieve diamond mechanosynthesis (DMS) can begin now, using existing technology, and might achieve success in less than a decade if their "direct-to-DMS approach is pursued rather than a more circuitous development approach that seeks to implement less efficacious nondiamondoid molecular manufacturing technologies before progressing to diamondoid".
To summarize the arguments against feasibility: First, critics argue that a primary barrier to achieving molecular nanotechnology is the lack of an efficient way to create machines on a molecular/atomic scale, especially in the absence of a well-defined path toward a self-replicating assembler or diamondoid nanofactory. Advocates respond that a preliminary research path leading to a diamondoid nanofactory is being developed.
A second difficulty in reaching molecular nanotechnology is design. Hand design of a gear or bearing at the level of atoms might take a few to several weeks. While Drexler, Merkle and others have created designs of simple parts, no comprehensive design effort for anything approaching the complexity of a Model T Ford has been attempted. Advocates respond that it is difficult to undertake a comprehensive design effort in the absence of significant funding for such efforts, and that despite this handicap much useful design-ahead has nevertheless been accomplished with new software tools that have been developed, e.g., at Nanorex.
In the latest report "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" put out by the National Academies Press in December 2006 (roughly twenty years after Engines of Creation was published), no clear way forward toward molecular nanotechnology could yet be seen, as per the conclusion on page 108 of that report: "Although theoretical calculations can be made today, the eventually attainable
range of chemical reaction cycles, error rates, speed of operation, and thermodynamic
efficiencies of such bottom-up manufacturing systems cannot be reliably
predicted at this time. Thus, the eventually attainable perfection and complexity of
manufactured products, while they can be calculated in theory, cannot be predicted
with confidence. Finally, the optimum research paths that might lead to systems
which greatly exceed the thermodynamic efficiencies and other capabilities of
biological systems cannot be reliably predicted at this time. Research funding that
is based on the ability of investigators to produce experimental demonstrations
that link to abstract models and guide long-term vision is most appropriate to
achieve this goal." This call for research leading to demonstrations is welcomed by groups such as the Nanofactory Collaboration who are specifically seeking experimental successes in diamond mechanosynthesis. The "Technology Roadmap for Productive Nanosystems" aims to offer additional constructive insights.
It is perhaps interesting to ask whether or not most structures consistent with physical law can in fact be manufactured. Advocates assert that to achieve most of the vision of molecular manufacturing it is not necessary to be able to build "any structure that is compatible with natural law." Rather, it is necessary to be able to build only a sufficient (possibly modest) subset of such structures—as is true, in fact, of any practical manufacturing process used in the world today, and is true even in biology. In any event, as Richard Feynman once said, "It is scientific only to say what's more likely or less likely, and not to be proving all the time what's possible or impossible."
Existing work on diamond mechanosynthesis.
There is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as mechanosynthesis). This work is slowly permeating the broader nanoscience community and is being critiqued. For instance, Peng et al. (2006) (in the continuing research effort by Freitas, Merkle and their collaborators) reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C2 carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. Over 100,000 CPU hours were invested in this latest study. The DCB6 tooltip motif, initially described by Merkle and Freitas at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface.
The tooltips modeled in this work are intended to be used only in carefully controlled environments (e. g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in Peng et al. (2006) -- tooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Peng et al. (2006) reports that increasing the handle thickness from 4 support planes of C atoms above the tooltip to 5 planes decreases the resonance frequency of the entire structure from 2.0 THz to 1.8 THz. More importantly, the vibrational footprints of a DCB6Ge tooltip mounted on a 384-atom handle and of the same tooltip mounted on a similarly constrained but much larger 636-atom "crossbar" handle are virtually identical in the non-crossbar directions. Additional computational studies modeling still bigger handle structures are welcome, but the ability to precisely position SPM tips to the requisite atomic accuracy has been repeatedly demonstrated experimentally at low temperature, or even at room temperature constituting a basic existence proof for this capability.
Further research to consider additional tooltips will require time-consuming computational chemistry and difficult laboratory work.
A working nanofactory would require a variety of well-designed tips for different reactions, and detailed analyses of placing atoms on more complicated surfaces. Although this appears a challenging problem given current resources, many tools will be available to help future researchers: Moore's Law predicts further increases in computer power, semiconductor fabrication techniques continue to approach the nanoscale, and researchers grow ever more skilled at using proteins, ribosomes and DNA to perform novel chemistry.

</doc>
<doc id="19638" url="http://en.wikipedia.org/wiki?curid=19638" title="Microelectromechanical systems">
Microelectromechanical systems

Microelectromechanical systems (MEMS) (also written as "micro-electro-mechanical", "MicroElectroMechanical" or "microelectronic and microelectromechanical systems" and the related "micromechatronics") is the technology of very small devices; it merges at the nano-scale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines (in Japan), or "micro systems technology" – "MST" (in Europe).
MEMS are separate and distinct from the hypothetical vision of molecular nanotechnology or molecular electronics. MEMS are made up of components between 1 to 100 micrometres in size (i.e. 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e. 0.02 to 1.0 mm). They usually consist of a central unit that processes data (the microprocessor) and several components that interact with the surroundings such as microsensors. At these size scales, the standard constructs of classical physics are not always useful. Because of the large surface area to volume ratio of MEMS, surface effects such as electrostatics and wetting dominate over volume effects such as inertia or thermal mass. 
The potential of very small machines was appreciated before the technology existed that could make them—see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom. MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electro discharge machining (EDM), and other technologies capable of manufacturing small devices. An early example of a MEMS device is the resonistor – an electromechanical monolithic resonator.
Materials for MEMS manufacturing.
The fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.
Silicon.
Silicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of cheap high-quality materials and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications.
Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking.
Polymers.
Even though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to be produced. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.
Metals.
Metals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.
Ceramics.
The nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus allowing to realize electrostatic MEMS actuation schemes with ultrathin membranes. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments and in biosensors.
MEMS basic processes.
Deposition processes.
One of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between a few nanometres to about 100 micrometres. There are two types of deposition processes, as follows.
Physical deposition.
Physical vapor deposition ("PVD") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and Evaporation (deposition), in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.
Chemical deposition.
Chemical deposition techniques include chemical vapor deposition ("CVD"), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example, LPCVD (Low Pressure chemical vapor deposition) and PECVD (Plasma Enhanced chemical vapor deposition).
Oxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.
Patterning.
Patterning in MEMS is the transfer of a pattern into a material.
Lithography.
Lithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.
This exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching.
Electron beam lithography.
Electron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), ("exposing" the resist) and of selectively removing either exposed or non-exposed regions of the resist ("developing"). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.
The primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer region. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.
The key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.
Ion beam lithography.
It is known that focused-ion-beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.
Ion track technology.
Ion track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable to generate holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 104 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.
X-ray lithography.
X-ray lithography, is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply "resist," on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.
Diamond patterning.
A simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.
Diamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.
Etching processes.
There are two basic categories of etching processes: wet etching and dry etching.
In the former, the material is dissolved when immersed in a chemical solution.
In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant. for a somewhat dated overview of MEMS etching technologies.
Wet etching.
Wet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.
Isotropic etching.
Etching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.
Anisotropic etching.
Some single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.
HF etching.
Hydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (SiO2, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.
Hydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore the damage is not felt until it is too late.
Electrochemical etching.
Electrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant ("etch-stop") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.
Dry etching.
Vapor etching.
Xenon difluoride.
Xenon difluoride (XeF2) is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, XeF2 has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, SiO2, silicon nitride, and various metals for masking. Its reaction to silicon is "plasmaless", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.
Plasma etching.
Modern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.
Plasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.
The source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride (CCl4) etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize ("ash") photoresist and facilitate its removal.
Ion milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10−3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.
Reactive ion etching (RIE).
In reactive ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical. RIE can be deep (Deep RIE or deep reactive ion etching (DRIE)).
Deep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called "Bosch process", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the Bosch Process as used in the Plasma-Therm tool) while the second variation only consists of two steps (ASE used in the STS tool).
In the 1st Variation, the etch cycle is as follows:
(i) SF6 isotropic etch;
(ii) C4F8 passivation;
(iii) SF6 anisoptropic etch for floor cleaning.
In the 2nd variation, steps (i) and (iii) are combined.
Both variations operate similarly.
The C4F8 creates a polymer on the surface of the substrate, and the second gas composition (SF6 and O2) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.
Die preparation.
After preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.
MEMS manufacturing technologies.
Bulk micromachining.
Bulk micromachining is the oldest paradigm of silicon based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.
Surface micromachining.
Surface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.
High aspect ratio (HAR) silicon micromachining.
Both bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.
Applications.
In one viewpoint MEMS application is categorized by type of use.
In another view point MEMS applications are categorized by the field of application (commercial applications include):
Companies with strong MEMS programs come in many sizes. The larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. The successful small firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. In addition, both large and small companies work in R&D to explore MEMS technology.
Industry structure.
The global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Developpement and is forecasted to reach $72 billion by 2011.
MEMS devices are defined as die-level components of first-level packaging, and include pressure sensors, accelerometers, gyroscopes, microphones, digital mirror displays, microfluidic devices, etc. The materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.

</doc>
<doc id="19639" url="http://en.wikipedia.org/wiki?curid=19639" title="Marvin Minsky">
Marvin Minsky

Marvin Lee Minsky (born August 9, 1927) is an American cognitive scientist in the field of artificial intelligence (AI), co-founder of the Massachusetts Institute of Technology's AI laboratory, and author of several texts on AI and philosophy.
Biography.
Marvin Lee Minsky was born in New York City to an eye surgeon and a Jewish activist, where he attended The Fieldston School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He served in the US Navy from 1944 to 1945. He holds a BA in Mathematics from Harvard (1950) and a PhD in mathematics from Princeton (1954). He has been on the MIT faculty since 1958. In 1959 he and John McCarthy founded what is now known as the MIT Computer Science and Artificial Intelligence Laboratory. He is currently the Toshiba Professor of Media Arts and Sciences, and Professor of electrical engineering and computer science.
Isaac Asimov described Minsky as one of only two people he would admit were more intelligent than he was, the other being Carl Sagan.
Minsky's inventions include the first head-mounted graphical display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). He developed, with Seymour Papert, the first Logo "turtle". Minsky also built, in 1951, the first randomly wired neural network learning machine, SNARC.
Minsky wrote the book "Perceptrons" (with Seymour Papert), which became the foundational work in the analysis of artificial neural networks. This book is the center of a controversy in the history of AI, as some claim it to have had great importance in driving research away from neural networks in the 1970s, and contributing to the so-called AI winter. He also founded several other famous AI models. His book "A framework for representing knowledge" created a new paradigm in programming. While his "Perceptrons" is now more a historical than practical book, the theory of frames is in wide use. Minsky has also written on the possibility that extraterrestrial life may think like humans, permitting communication. He was an adviser on the movie and is referred to in the movie and book:
Probably no one would ever know this; it did not matter. In the 1980s, Minsky and Good had shown how neural networks could be generated automatically—self replicated—in accordance with any arbitrary learning program. Artificial brains could be grown by a process strikingly analogous to the development of a human brain. In any given case, the precise details would never be known, and even if they were, they would be millions of times too complex for human understanding.—Arthur C. Clarke, "2001: A Space Odyssey"
In the early 1970s at the MIT Artificial Intelligence Lab, Minsky and Seymour Papert started developing what came to be called The Society of Mind theory. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks. In 1986, Minsky published "The Society of Mind", a comprehensive book on the theory which, unlike most of his previously published work, was written for a general audience.
In November 2006, Minsky published "The Emotion Machine", a book that critiques many popular theories of how human minds work and suggests alternative theories, often replacing simple ideas with more complex ones. Recent drafts of the book are freely available from his webpage.
Awards and affiliations.
Minsky won the Turing Award in 1969, the Japan Prize in 1990, the IJCAI Award for Research Excellence in 1991, and the Benjamin Franklin Medal from the Franklin Institute in 2001. In 2006, he was inducted as a Fellow of the Computer History Museum "for co-founding the field of artificial intelligence, creating early neural networks and robots, and developing theories of human and machine cognition." In 2011, Minsky was inducted into IEEE Intelligent Systems' AI's Hall of Fame for the "significant contributions to the field of AI and intelligent systems". In 2014, Minsky won the Dan David Prize in the field of "Artificial Intelligence, the Digital Mind". He was also awarded with the 2013 BBVA Foundation Frontiers of Knowledge Award in the Information and Communication Technologies category
Marvin Minsky is affiliated with the following organizations:
Minsky is a critic of the Loebner Prize.
Personal life.
Minsky is an actor in an artificial intelligence koan (attributed to his student, Danny Hillis) from the Jargon file:
<poem>
In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
"What are you doing?" asked Minsky.
"I am training a randomly wired neural net to play Tic-tac-toe," Sussman replied.
"Why is the net wired randomly?" asked Minsky.
"I do not want it to have any preconceptions of how to play," Sussman said.
Minsky then shut his eyes.
"Why do you close your eyes?" Sussman asked his teacher.
"So that the room will be empty."
At that moment, Sussman was enlightened.
</poem>
Minsky is an atheist and has signed the Scientists' Open Letter on Cryonics.

</doc>
<doc id="19640" url="http://en.wikipedia.org/wiki?curid=19640" title="Milton Friedman">
Milton Friedman

Milton Friedman (July 31, 1912 – November 16, 2006) was an American economist, statistician and writer who taught at the University of Chicago for more than three decades. He received the 1976 Nobel Memorial Prize in Economic Sciences for his research on consumption analysis, monetary history and theory and the complexity of stabilization policy.
Friedman's challenges to what he later called "naive Keynesian" (as opposed to Neo-Keynesian) theory began with his 1950s reinterpretation of the consumption function, and he became the main advocate opposing Keynesian government policies. In the late 1960s, he described his own approach (along with all of mainstream economics) as using "Keynesian language and apparatus" yet rejecting its "initial" conclusions. During the 1960s, he promoted an alternative macroeconomic policy known as "monetarism". He theorized there existed a "natural" rate of unemployment and argued that governments could only increase employment above this rate, "e.g.", by increasing aggregate demand, only for as long as inflation was accelerating. He argued that the Phillips curve was, in the long run, vertical at the "natural rate" and predicted what would come to be known as stagflation. Though opposed to the existence of the Federal Reserve System, Friedman argued that, given that it does exist, a steady, small expansion of the money supply was the only wise policy.
Friedman actively participated in public debates over numerous policy issues; he was a major advisor to Republican U.S. President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher. His political philosophy extolled the virtues of a free market economic system with minimal intervention. He once stated that his role in eliminating U.S. conscription was his proudest accomplishment, and his support for school choice led him to found the Friedman Foundation for Educational Choice. In his 1962 book "Capitalism and Freedom", Friedman advocated policies such as a volunteer military, freely floating exchange rates, abolition of medical licenses, a negative income tax, and school vouchers.
His ideas concerning monetary policy, taxation, privatization and deregulation influenced government policies, especially during the 1980s. His monetary theory influenced the Federal Reserve's response to the global financial crisis of 2007–08. Edward Nelson, the assistant director of the board of governors of the Federal Reserve System, argues, "in important respects, the overall monetary and financial policy response to the crisis can be viewed as Friedman’s monetary economics in practice." 
Friedman was among the strongest proponents of positivism in the social sciences. In the field of statistics, Friedman developed the sequential sampling method of analysis. He was also a mentor of and collaborator with Leonard Jimmie Savage, laying the foundations of Bayesian statistics and decision theory. With George Stigler and others, Friedman was among the intellectual leaders of the second generation of Chicago price theory, a distinctive intellectual and methodological movement at the University of Chicago's Department of Economics, Law School, and Graduate School of Business from the 1940s onward. A large number of students and young professors that were recruited or mentored by Friedman during this period went on to become leading economists; they include Gary Becker, Robert Fogel, Ronald Coase, and Robert Lucas, Jr..
Milton Friedman's own works include many monographs, books, scholarly articles, papers, magazine columns, television programs, videos, and lectures, and cover a broad range of topics of microeconomics, macroeconomics, economic history, and public policy issues. His books and essays were widely read, and have had an international influence, including in former Communist states. A survey of economists ranked Friedman as the second most popular economist of the twentieth century after John Maynard Keynes, and "The Economist" described him as "the most influential economist of the second half of the 20th century ... possibly of all of it."
Early life.
Friedman was born in Brooklyn, New York on July 31, 1912, to recent Jewish immigrants Sára Ethel (née Landau) and Jenő Saul Friedman, from Beregszász in Carpathian Ruthenia, Kingdom of Hungary (now Berehove in Ukraine), both of whom worked as dry goods merchants. Shortly after Milton's birth, the family relocated to Rahway, New Jersey. In his early teens, Friedman was injured in a car accident, which scarred his upper lip. A talented student, Friedman graduated from Rahway High School in 1928, just before his 16th birthday.
In 1932 Friedman graduated from Rutgers University, where he specialized in Mathematics and Economics and initially intended to become an actuary. During his time at Rutgers, Friedman became influenced by two economics professors, Arthur F. Burns and Homer Jones, who convinced him that modern economics could help end the Great Depression.
After graduating from Rutgers, Friedman was offered two scholarships to do graduate work — one in mathematics at Brown University and the other in economics at the University of Chicago. Friedman chose the latter, thus earning a Master of Arts degree in 1933. He was strongly influenced by Jacob Viner, Frank Knight, and Henry Simons. It was at Chicago that Friedman met his future wife, economist Rose Director. During the 1933–1934 academic year he had a fellowship at Columbia University, where he studied statistics with renowned statistician and economist Harold Hotelling. He was back in Chicago for the 1934–1935 academic year, working as a research assistant for Henry Schultz, who was then working on "Theory and Measurement of Demand". That year, Friedman formed what would prove to be lifelong friendships with George Stigler and W. Allen Wallis.
Public service.
Friedman was initially unable to find academic employment, so in 1935 he followed his friend W. Allen Wallis to Washington, where Franklin D. Roosevelt's New Deal was "a lifesaver" for many young economists. At this stage, Friedman said that he and his wife "regarded the job-creation programs such as the WPA, CCC, and PWA appropriate responses to the critical situation," but not "the price- and wage-fixing measures of the National Recovery Administration and the Agricultural Adjustment Administration." Foreshadowing his later ideas, he believed price controls interfered with an essential signaling mechanism to help resources be used where they were most valued. Indeed, Friedman later concluded that all government intervention associated with the New Deal was "the wrong cure for the wrong disease," arguing that the money supply should simply have been expanded, instead of contracted.
In the publication, "A Monetary History of the United States, 1867–1960" by Friedman and Anna J. Schwartz, they argue that the Great Depression was caused by monetary contraction, which was the consequence of poor policymaking by the Federal Reserve System and the continuous crises of the banking system.
During 1935, he began work for the National Resources Committee, which was then working on a large consumer budget survey. Ideas from this project later became a part of his "Theory of the Consumption Function". Friedman began employment with the National Bureau of Economic Research during autumn 1937 to assist Simon Kuznets in his work on professional income. This work resulted in their jointly authored publication "Incomes from Independent Professional Practice", which introduced the concepts of permanent and transitory income, a major component of the Permanent Income Hypothesis that Friedman worked out in greater detail in the 1950s. The book hypothesizes that professional licensing artificially restricts the supply of services and raises prices.
During 1940, Friedman was appointed an assistant professor teaching Economics at the University of Wisconsin–Madison, but encountered antisemitism in the Economics department and decided to return to government service. From 1941 to 1943 Friedman worked on wartime tax policy for the Federal Government, as an advisor to senior officials of the United States Department of the Treasury. As a Treasury spokesman during 1942 he advocated a Keynesian policy of taxation. He helped to invent the payroll withholding tax system, since the federal government badly needed money in order to fight the war. He later said, "I have no apologies for it, but I really wish we hadn't found it necessary and I wish there were some way of abolishing withholding now."
Academic career.
Early years.
In 1940, Friedman accepted a position at the University of Wisconsin–Madison, but left because of differences with faculty regarding United States involvement in World War II. Friedman believed the United States should enter the war. In 1943, Friedman joined the Division of War Research at Columbia University (headed by W. Allen Wallis and Harold Hotelling), where he spent the rest of World War II working as a mathematical statistician, focusing on problems of weapons design, military tactics, and metallurgical experiments.
In 1945, Friedman submitted "Incomes from Independent Professional Practice" (co-authored with Kuznets and completed during 1940) to Columbia as his doctoral dissertation. The university awarded him a PhD in 1946. Friedman spent the 1945–1946 academic year teaching at the University of Minnesota (where his friend George Stigler was employed). On February 12, 1945, his son, David D. Friedman was born.
University of Chicago.
In 1946, Friedman accepted an offer to teach economic theory at the University of Chicago (a position opened by departure of his former professor Jacob Viner to Princeton University). Friedman would work for the University of Chicago for the next 30 years. There he contributed to the establishment of an intellectual community that produced a number of Nobel Prize winners, known collectively as the Chicago school of economics.
At that time, Arthur F. Burns, who was then the head of the National Bureau of Economic Research, asked Friedman to rejoin the Bureau's staff. He accepted the invitation, and assumed responsibility for the Bureau's inquiry into the role of money in the business cycle. As a result, he initiated the "Workshop in Money and Banking" (the "Chicago Workshop"), which promoted a revival of monetary studies. During the latter half of the 1940s, Friedman began a collaboration with Anna Schwartz, an economic historian at the Bureau, that would ultimately result in the 1963 publication of a book co-authored by Friedman and Schwartz, "A Monetary History of the United States, 1867–1960".
Friedman spent the 1954–1955 academic year as a Fulbright Visiting Fellow at Gonville and Caius College, Cambridge. At the time, the Cambridge economics faculty was divided into a Keynesian majority (including Joan Robinson and Richard Kahn) and an anti-Keynesian minority (headed by Dennis Robertson). Friedman speculated that he was invited to the fellowship, because his views were unacceptable to both of the Cambridge factions. Later his weekly columns for "Newsweek" magazine (1966–84) were well read and increasingly influential among political and business people. From 1968 to 1978, he and Paul Samuelson participated in the Economics Cassette Series, a biweekly subscription series where the economist would discuss the days' issues for about a half-hour at a time.
Friedman was an economic adviser to Republican presidential candidate Barry Goldwater during 1964.
Nobel Prize in Economic Sciences.
Friedman won the Nobel Memorial Prize in Economic Sciences, the sole recipient for 1976, "for his achievements in the fields of consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization policy."
Retirement.
In 1977, at the age of 65, Friedman retired from the University of Chicago after teaching there for 30 years. He and his wife moved to San Francisco where he became a visiting scholar at the Federal Reserve Bank of San Francisco. From 1977 on, he was affiliated with the Hoover Institution at Stanford University. During the same year, Friedman was approached by the Free To Choose Network and asked to create a television program presenting his economic and social philosophy.
The Friedmans worked on this project for the next three years, and during 1980, the ten-part series, titled "Free to Choose", was broadcast by the Public Broadcasting Service (PBS). The companion book to the series (co-authored by Milton and his wife, Rose Friedman), also titled "Free To Choose", was the bestselling nonfiction book of 1980 and has since been translated into 14 foreign languages.
Friedman served as an unofficial adviser to Ronald Reagan during his 1980 presidential campaign, and then served on the President's Economic Policy Advisory Board for the rest of the Reagan Administration. Ebenstein says Friedman was "the 'guru' of the Reagan administration." In 1988 he received the National Medal of Science and Reagan honored him with the Presidential Medal of Freedom.
Milton Friedman is known now as one of the most influential economists of the 20th century. Throughout the 1980s and 1990s, Friedman continued to write editorials and appear on television. He made several visits to Eastern Europe and to China, where he also advised governments. He was also for many years a Trustee of the Philadelphia Society.
Scholarly contributions.
Economics.
Friedman was best known for reviving interest in the money supply as a determinant of the nominal value of output, that is, the quantity theory of money. Monetarism is the set of views associated with modern quantity theory. Its origins can be traced back to the 16th-century School of Salamanca or even further; however, Friedman's contribution is largely responsible for its modern popularization. He co-authored, with Anna Schwartz, "A Monetary History of the United States, 1867–1960" (1963), which was an examination of the role of the money supply and economic activity in the U.S. history. A striking conclusion of their research regarded the way in which money supply fluctuations contribute to economic fluctuations. Several regression studies with David Meiselman during the 1960s suggested the primacy of the money supply over investment and government spending in determining consumption and output. These challenged a prevailing, but largely untested, view on their relative importance. Friedman's empirical research and some theory supported the conclusion that the short-run effect of a change of the money supply was primarily on output but that the longer-run effect was primarily on the price level.
Friedman was the main proponent of the monetarist school of economics. He maintained that there is a close and stable association between inflation and the money supply, mainly that inflation could be avoided with proper regulation of the monetary base's growth rate. He famously used the analogy of "dropping money out of a helicopter.", in order to avoid dealing with money injection mechanisms and other factors that would overcomplicate his models.
Friedman's arguments were designed to counter the popular concept of Cost-push inflation, that the increased General Price Level at the time was the result of increases in the price of oil, or increases in wages; as he wrote, Inflation is always and everywhere a monetary phenomenon.—Milton Friedman, 1963.
Friedman rejected the use of fiscal policy as a tool of demand management; and he held that the government's role in the guidance of the economy should be restricted severely. Friedman wrote extensively on the Great Depression, which he termed the Great Contraction, arguing that it had been caused by an ordinary financial shock whose duration and seriousness were greatly increased by the subsequent contraction of the money supply caused by the misguided policies of the directors of the Federal Reserve.
The Fed was largely responsible for converting what might have been a garden-variety recession, although perhaps a fairly severe one, into a major catastrophe. Instead of using its powers to offset the depression, it presided over a decline in the quantity of money by one-third from 1929 to 1933 ... Far from the depression being a failure of the free-enterprise system, it was a tragic failure of government.—Milton Friedman, Two Lucky People, 233
Friedman also argued for the cessation of government intervention in currency markets, thereby spawning an enormous literature on the subject, as well as promoting the practice of freely floating exchange rates. His close friend George Stigler explained, "As is customary in science, he did not win a full victory, in part because research was directed along different lines by the theory of rational expectations, a newer approach developed by Robert Lucas, also at the University of Chicago."
Friedman was also known for his work on the consumption function, the permanent income hypothesis (1957), which Friedman himself referred to as his best scientific work. This work contended that rational consumers would spend a proportional amount of what they perceived to be their permanent income. Windfall gains would mostly be saved. Tax reductions likewise, as rational consumers would predict that taxes would have to increase later to balance public finances. Other important contributions include his critique of the Phillips curve and the concept of the natural rate of unemployment (1968). This critique associated his name, together with that of Edmund Phelps, with the insight that a government that brings about greater inflation cannot permanently reduce unemployment by doing so. Unemployment may be temporarily lower, if the inflation is a surprise, but in the long run unemployment will be determined by the frictions and imperfections of the labor market.
Friedman's essay "The Methodology of Positive Economics" (1953) provided the epistemological pattern for his own subsequent research and to a degree that of the Chicago School. There he argued that economics as "science" should be free of value judgments for it to be objective. Moreover, a useful economic theory should be judged not by its descriptive realism but by its simplicity and fruitfulness as an engine of prediction. That is, students should measure the accuracy of its predictions, rather than the 'soundness of its assumptions'. His argument was part of an ongoing debate among such statisticians as Jerzy Neyman, Leonard Savage, and Ronald Fisher.
Statistics.
One of his most famous contributions to statistics is sequential sampling. Friedman did statistical work at the Division of War Research at Columbia. He and his colleagues came up with a sampling technique, known as sequential sampling, which became, in the words of "The New Palgrave Dictionary of Economics", "the standard analysis of quality control inspection". The dictionary adds, "Like many of Friedman’s contributions, in retrospect it seems remarkably simple and obvious to apply basic economic ideas to quality control; that however is a measure of his genius."
Public policy positions.
Federal Reserve.
Due to its poor performance, Friedman believed that the Federal Reserve should be abolished. He further believed that if the money supply was to be centrally controlled (as by the Federal Reserve System) that the preferable way to do it would be with a mechanical system that would keep the quantity of money increasing at a steady rate.
Exchange rates.
Friedman was a strong advocate for floating exchange rates throughout the entire Bretton-Woods period. He argued that a flexible exchange rate would make external adjustment possible and allow countries to avoid Balance of Payments crisis. He saw fixed exchange rates as an undesirable form of government intervention. The case was articulated in an influential 1953 paper, "The Case for Flexible Exchange Rates", at a time, when most commentators regarded the possibility of floating exchange rates as a fantasy.
School choice.
In his 1955 article "The Role of Government in Education" Friedman proposed supplementing publicly operated schools with privately run but publicly funded schools through a system of school vouchers. Reforms similar to those proposed in the article were implemented in, for example, Chile in 1981 and Sweden in 1992. In 1996, Friedman, together with his wife, founded The Foundation for Educational Choice to advocate school choice and vouchers.
Conscription.
Milton Friedman was a major proponent of a volunteer military, stating that the draft was "inconsistent with a free society."
In "Capitalism and Freedom", he argued that conscription is inequitable and arbitrary, preventing young men from shaping their lives as they see fit. During the Nixon administration he headed the committee to research a conversion to paid/volunteer armed force. He would later state that his role in eliminating the conscription in the United States was his proudest accomplishment. Friedman did, however, believe a nation could compel military "training" as a reserve in case of war time.
Foreign policy.
Biographer Lanny Ebenstein noted a drift over time in Friedman's views from an interventionist to a more cautious foreign policy. He supported US involvement in the Second World War and initially supported a hard line against Communism, but moderated over time. He opposed the Gulf War and the Iraq War. In a spring 2006 interview, Friedman said that the USA's stature in the world had been eroded by the Iraq War, but that it might be improved if Iraq were to become a peaceful independent country.
Libertarianism and the Republican Party.
He served as a member of President Reagan's Economic Policy Advisory Board starting at 1981. In 1988, he received the Presidential Medal of Freedom and the National Medal of Science. He said that he was a libertarian philosophically, but a member of the U.S. Republican Party for the sake of "expediency" ("I am a libertarian with a small 'l' and a Republican with a capital 'R.' And I am a Republican with a capital 'R' on grounds of expediency, not on principle.") But, he said, "I think the term classical liberal is also equally applicable. I don't really care very much what I'm called. I'm much more interested in having people thinking about the ideas, rather than the person."
Public goods and monopoly.
Friedman was supportive of the state provision of some public goods that private businesses are not considered as being able to provide. However, he argued that many of the services performed by government could be performed better by the private sector. Above all, if some public goods are provided by the state, he believed that they should not be a legal monopoly where private competition is prohibited; for example, he wrote:
There is no way to justify our present public monopoly of the post office. It may be argued that the carrying of mail is a technical monopoly and that a government monopoly is the least of evils. Along these lines, one could perhaps justify a government post office, but not the present law, which makes it illegal for anybody else to carry the mail. If the delivery of mail is a technical monopoly, no one else will be able to succeed in competition with the government. If it is not, there is no reason why the government should be engaged in it. The only way to find out is to leave other people free to enter.—Milton Friedman, Friedman, Milton & Rose D. "Capitalism and Freedom", University of Chicago Press, 1982, 29
Social security, welfare programs, and negative income tax.
After 1960 Friedman attacked Social Security from a free market view stating that it had created welfare dependency.
Friedman proposed that if there had to be a welfare system of any kind, he would replace the existing U.S. welfare system with a negative income tax, a progressive tax system in which the poor receive a basic living income from the government. According to the "New York Times", Friedman's views in this regard were grounded in a belief that while "market forces ... accomplish wonderful things", they "cannot ensure a distribution of income that enables all citizens to meet basic economic needs".
Drug policy.
Friedman also supported libertarian policies such as legalization of drugs and prostitution. During 2005, Friedman and more than 500 other economists advocated discussions regarding the economic benefits of the legalization of marijuana.
LGBT rights.
Friedman was also a supporter of gay rights. He specifically supported same-sex marriage, saying on the issue, "I do not believe there should be any discrimination against gays."
Economic freedom.
Michael Walker of the Fraser Institute and Friedman hosted a series of conferences from 1986 to 1994. The goal was to create a clear definition of economic freedom and a method for measuring it. Eventually this resulted in the first report on worldwide economic freedom, "Economic Freedom in the World". This annual report has since provided data for numerous peer-reviewed studies and has influenced policy in several nations.
Along with sixteen other distinguished economists he opposed the Copyright Term Extension Act and filed an amicus brief in "Eldred v. Ashcroft". He supported the inclusion of the word "no-brainer" in the brief.
Friedman argued for stronger basic legal (constitutional) protection of economic rights and freedoms in order to further promote industrial-commercial growth and prosperity and buttress democracy and freedom and the rule of law generally in society.
Honors, recognition, and influence.
George H. Nash, a leading historian of American conservatism, says that by, "the end of the 1960s he was probably the most highly regarded and influential conservative scholar in the country, and one of the few with an international reputation." Friedman allowed the libertarian Cato Institute to use his name for its biannual Milton Friedman Prize for Advancing Liberty beginning in 2001. A Friedman Prize was given to the late British economist Peter Bauer in 2002, Peruvian economist Hernando de Soto in 2004, Mart Laar, former Estonian Prime Minister in 2006 and a young Venezuelan student Yon Goicoechea in 2008. His wife Rose, sister of Aaron Director, with whom he initiated the Friedman Foundation for Educational Choice, served on the international selection committee. Friedman was also a recipient of the Nobel Prize in Economics.
Upon Friedman's death, Harvard President Lawrence Summers called him "The Great Liberator" saying "... any honest Democrat will admit that we are now all Friedmanites." He said Friedman's great popular contribution was "in convincing people of the importance of allowing free markets to operate."
In 2013 Stephen Moore, a member of the editorial forward of the "Wall Street Journal" said, "Quoting the most-revered champion of free-market economics since Adam Smith has become a little like quoting the Bible." He adds, "There are sometimes multiple and conflicting interpretations."
Hong Kong.
Friedman once said, "If you want to see capitalism in action, go to Hong Kong." He wrote in 1990 that the Hong Kong economy was perhaps the best example of a free market economy.
One month before his death, he wrote the article "Hong Kong Wrong – What would Cowperthwaite say?" in the "Wall Street Journal", criticizing Donald Tsang, the Chief Executive of Hong Kong, for abandoning "positive noninterventionism."
Tsang later said he was merely changing the slogan to "big market, small government," where small government is defined as less than 20% of GDP. In a debate between Tsang and his rival, Alan Leong, before the 2007 Chief Executive election, Leong introduced the topic and jokingly accused Tsang of angering Friedman to death.
Chile.
During 1975, two years after the military coup that brought military dictator President Augusto Pinochet to power and ended the government of Salvador Allende, the economy of Chile experienced a severe crisis. Friedman and Arnold Harberger accepted an invitation of a private Chilean foundation to visit Chile and speak on principles of economic freedom. He spent seven days in Chile giving a series of lectures at the Universidad Católica de Chile. and the (National) University of Chile. One of the lectures was entitled "The Fragility of Freedom" and according to Friedman, "dealt with precisely the threat to freedom from a centralized military government."
In an April 21, 1975, letter to Pinochet, Friedman considered the "key economic problems of Chile are clearly ... inflation and the promotion of a healthy social market economy". He stated that "There is only one way to end inflation: by drastically reducing the rate of increase of the quantity of money ..." and that "... cutting government spending is by far and away the most desirable way to reduce the fiscal deficit, because it ... strengthens the private sector thereby laying the foundations for "healthy" economic growth". As to how rapidly inflation should be ended, Friedman felt that "for Chile where inflation is raging at 10-20% a month ... gradualism is not feasible. It would involve so painful an "operation" over so long a period that the "patient" would not survive." Choosing "a brief period of higher unemployment..." was the lesser evil.. and that "the experience of Germany, ... of Brazil ..., of the post-war adjustment in the U.S. ... all argue for "shock treatment"". In the letter Friedman recommended to deliver what the "shock approach" with "... a package to eliminate the surprise and to relieve acute distress" and "... for definiteness let me sketch the contents of a package proposal ... to be taken as illustrative" although his knowledge of Chile was "too limited to enable [him] to be precise or comprehensive". He listed a "sample proposal" of 8 monetary and fiscal measures including "the removal of as many as obstacles as possible that now hinder the private market. For example, suspend ... the present law against discharging employees". He closed, stating "Such a "shock program" could end inflation in months". His letter suggested that cutting spending to reduce the fiscal deficit would result in less transitional unemployment than raising taxes.
Sergio de Castro, a Chilean Chicago School graduate, became the nation's Minister of Finance in 1975. During his six-year tenure, foreign investment increased, restrictions were placed on striking and labor unions, and GDP rose yearly. A foreign exchange program was created between the Catholic University of Chile and the University of Chicago. Many other Chicago School alumni were appointed government posts during and after the Pinochet years; others taught its economic doctrine at Chilean universities. They became known as the Chicago Boys.
Friedman did not criticize Pinochet's dictatorship at the time, nor the assassinations, illegal imprisonments, torture, or other atrocities that were well known by then.
In 1976 Friedman defended his unofficial adviser position with: "I do not consider it as evil for an economist to render technical economic advice to the Chilean Government, any more than I would regard it as evil for a physician to give technical medical advice to the Chilean Government to help end a medical plague."
Friedman defended his activity in Chile on the grounds that, in his opinion, the adoption of free market policies not only improved the economic situation of Chile but also contributed to the amelioration of Pinochet's rule and to the eventual transition to a democratic government during 1990. That idea is included in "Capitalism and Freedom", in which he declared that economic freedom is not only desirable in itself but is also a necessary condition for political freedom. In his 1980 documentary "Free to Choose", he said the following: "Chile is not a politically free system, and I do not condone the system. But the people there are freer than the people in Communist societies because government plays a smaller role. ... The conditions of the people in the past few years has been getting better and not worse. They would be still better to get rid of the junta and to be able to have a free democratic system." In 1984, Friedman stated that he has "never refrained from criticizing the political system in Chile." In 1991 he said: "I have nothing good to say about the political regime that Pinochet imposed. It was a terrible political regime. The real miracle of Chile is not how well it has done economically; the real miracle of Chile is that a military junta was willing to go against its principles and support a free market regime designed by principled believers in a free market. [...] In Chile, the drive for political freedom, that was generated by economic freedom and the resulting economic success, ultimately resulted in a referendum that introduced political democracy. Now, at long last, Chile has all three things: political freedom, human freedom and economic freedom. Chile will continue to be an interesting experiment to watch to see whether it can keep all three or whether, now that it has political freedom,that political freedom will tend to be used to destroy or reduce economic freedom." He stressed that the lectures he gave in Chile were the same lectures he later gave in China and other socialist states.
During the 2000 PBS documentary "The Commanding Heights" (based on ), Friedman continued to argue that "free markets would undermine [Pinochet's] political centralization and political control.", and that criticism over his role in Chile missed his main contention that freer markets resulted in freer people, and that Chile's unfree economy had caused the military government. Friedman advocated for free markets which undermined "political centralization and political control".
Iceland.
Friedman visited Iceland during the autumn of 1984, met with important Icelanders and gave a lecture at the University of Iceland on the "tyranny of the "status quo"." He participated in on August 31, 1984 with socialist intellectuals, including Ólafur Ragnar Grímsson, who later became the president of Iceland. When they complained that a fee was charged for attending his lecture at the University and that, hitherto, lectures by visiting scholars had been free-of-charge, Friedman replied that previous lectures had not been free-of-charge in a meaningful sense: lectures always have related costs. What mattered was whether attendees or non-attendees covered those costs. Friedman thought that it was fairer that only those who attended paid. In this discussion Friedman also stated that he did not receive any money for delivering that lecture.
Estonia.
Although Friedman never visited Estonia, his book "Free to Choose" exercised a great influence on that nation's then 32-year-old prime minister, Mart Laar, who has claimed that it was the only book on economics he had read before taking office. Laar's reforms are often credited with responsibility for transforming Estonia from an impoverished Soviet Republic to the "Baltic Tiger." A prime element of Laar's program was introduction of the flat tax. Laar won the 2006 Milton Friedman Prize for Advancing Liberty, awarded by the Cato Institute.
United Kingdom.
After 1950 Friedman was frequently invited to lecture in Britain, and by the 1970s his ideas had gained widespread attention in conservative circles. For example he was a regular speaker at the Institute of Economic Affairs (IEA), a libertarian think tank. Conservative politician Margaret Thatcher closely followed IEA programs and ideas, and met Friedman there in 1978. He also strongly influenced Keith Joseph, who became Thatcher's senior advisor on economic affairs, as well as Alan Walters and Patrick Minford, two other key advisers. Major newspapers, including the "Daily Telegraph," "The Times," and "The Financial Times" all promulgated Friedman's monetarist ideas to British decision-makers. Friedman's ideas strongly influenced Thatcher and her allies when she became Prime Minister in 1979.
Critiques.
Econometrician David Hendry criticized part of Friedman's and Anna Schwartz's 1982 "Monetary Trends". When asked about it during an , Friedman said that the critique applied to a different problem than that which he and Schwartz had tackled, and was thus not relevant, and also pointed to the (as of 1984) lack of consequential peer review amongst econometricians on Hendry's work. In 2006, Hendry stated that Friedman was guilty of "serious errors" of misunderstanding that meant "the t-ratios he reported for UK money demand were overstated by nearly 100 per cent", and said that, in a paper published in 1991 with Neil Ericsson, he had refuted "almost every empirical claim […] made about UK money demand" by Friedman and Schwartz. A 2004 paper updated and confirmed the validity of the Hendry–Ericsson findings through 2000.
After Friedman's death in 2006, Keynesian Nobel laureate Paul Krugman praised Friedman as a "great economist and a great man," and acknowledged his many, widely accepted contributions to empirical economics. Nonetheless, Krugman criticized Friedman, writing that "he slipped all too easily into claiming both that markets always work and that only markets work. It's extremely hard to find cases in which Friedman acknowledged the possibility that markets could go wrong, or that government intervention could serve a useful purpose."
In her book "The Shock Doctrine", author and social activist Naomi Klein criticized Friedman's economic liberalism, identifying it with the principles that guided the economic restructuring that followed the military coups in countries such as Chile and Indonesia. Based on their assessments of the extent to which what she describes as neoliberal policies contributed to income disparities and inequality, both Klein and Noam Chomsky have suggested that the primary role of what they describe as neoliberalism was as an ideological cover for capital accumulation by multinational corporations.
Chilean economist Orlando Letelier asserted that Pinochet's dictatorship resorted to oppression because of popular opposition to Chicago School policies in Chile. After a 1991 speech on drug legalisation, Friedman answered a question on his involvement with the Pinochet regime, saying that he was never an advisor to Pinochet (also mentioned in his 1984 Iceland interview), but that a group of his students at the University of Chicago were involved in Chile's economic reforms. Friedman credited these reforms with high levels of economic growth and with the establishment of democracy that has subsequently occurred in Chile.
Personal life.
According to a 2007 article in "Commentary" magazine, his "parents were moderately observant [Jews], but Friedman, after an intense burst of childhood piety, rejected religion altogether." He described himself as an agnostic.
Friedman wrote extensively of his life and experiences, especially in 1998 in his memoirs with his wife Rose, titled "Two Lucky People". He died of heart failure at the age of 94 years in San Francisco on November 16, 2006. He was still a working economist performing original economic research as his last column was published in the "The Wall Street Journal" the day after his death. He was survived by his wife (who died on August 18, 2009) and their two children, David, who is an anarcho-capitalist economist , and Janet. David's son, Patri Friedman, was the executive director of the The Seasteading Institute from 2008–2011.

</doc>
<doc id="19641" url="http://en.wikipedia.org/wiki?curid=19641" title="Mass media">
Mass media

The mass media are diversified media technologies that are intended to reach a large audience by mass communication. The technologies through which this communication takes place varies. Broadcast media such as radio, recorded music, film and television transmit their information electronically. Print media use a physical object such as a newspaper, book, pamphlet or comics, to distribute their information. Outdoor media are a form of mass media that comprises billboards, signs, or placards placed inside and outside of commercial buildings, sports stadiums, shops, and buses. Other outdoor media include flying billboards (signs in tow of airplanes), blimps, skywriting, and AR Advertising. Public speaking and event organising can also be considered forms of mass media. The digital media comprises both Internet and mobile mass communication. Internet media provide many mass media services, such as email, websites, blogs, and Internet-based radio and television. Many other mass media outlets have a presence on the web, by such things as having TV ads that link to a website, or distributing a QR Code in print or outdoor media to direct a mobile user to a website. In this way, they can utilise the easy accessibility that the Internet has, and the outreach that Internet affords, as information can easily be broadcast to many different regions of the world simultaneously and cost-efficiently.
The organizations that control these technologies, such as television stations or publishing companies, are also known as the mass media.#redirect 
Issues with definition.
In the late 20th Century, mass media could be classified into eight mass media industries: books, newspapers, magazines, recordings, radio, movies, television and the internet. With the explosion of digital communication technology in the late 20th and early 21st centuries, the question of what forms of media should be classified as "mass media" has become more prominent. For example, it is controversial whether to include cell phones, video games, and computer games (such as MMORPGs) in the definition. In the 2000s, a classification called the "seven mass media" became popular. In order of introduction, they are:
Each mass media has its own content types, its own creative artists and technicians, and its own business models. For example, the Internet includes web sites, blogs, podcasts, and various other technologies built on top of the general distribution network. The sixth and seventh media, internet and mobile, are often called collectively as digital media; and the fourth and fifth, radio and TV, as broadcast media. Some argue that video games have developed into a distinct mass form of media.
While a telephone is a two-way communication device, mass media refer to medium which can communicate a message to a large group, often simultaneously. However, modern cell phones are no longer a single-use device. Most cell phones are equipped with internet access and capable of connecting to the web which itself is a mass medium. A question arises whether this makes cell phones a mass medium or simply a device used to access a mass medium (the internet). There is currently a system by which marketers and advertisers are able to tap into satellites, and broadcast commercials and advertisements directly to cell phones, unsolicited by the phone's user. This transmission of mass advertising to millions of people is a form of mass communication.
Video games may also be evolving into a mass medium. Video games convey the same messages and ideologies to all their users. Users sometimes share the experience with one another by playing online. Excluding the internet however, it is questionable whether players of video games are sharing a common experience when they play the game separately. It is possible to discuss in great detail the events of a video game with a friend you have never played with because the experience was identical to you both. The question is whether this is then a form of mass communication.
Massively multiplayer online role-playing games (MMORPGs) such as Runescape provide a common gaming experience to millions of users across the globe. It is arguable that the users are receiving the same message, i.e., the game is mass communicating the same messages to the various players.
Characteristics.
Five characteristics of mass communication have been identified by Cambridge University's John Thompson:
Mass vs. mainstream.
The term "mass media" is sometimes used as a synonym for "mainstream media", which are distinguished from alternative media by the content and point of view. Alternative media are also "mass media" outlets in the sense that they use technology capable of reaching many people, even if the audience is often smaller than the mainstream.
In common usage, the term "mass" denotes not that a given number of individuals receives the products, but rather that the products are available in principle to a plurality of recipients.
Mass vs. local.
Mass media are distinguished from local media by the notion that whilst the former aims to reach a very large market such as the entire population of a country, the latter broadcasts to a much smaller population and area, and generally focuses on regional news rather than global events. A third type of media, speciality media, provide for specific demographics, such as specialty channels on TV (sports channels, porn channels, etc.). These definitions are not set in stone, and it is possible for a media outlet to be promoted in status from a local media outlet to a global media outlet. Some local media, which take an interest in state or provincial news, can rise to prominence because of their investigative journalism, and to the local region's preference of updates in national politics rather than regional news. The Guardian, formerly known as the Manchester Guardian is an example of one such media outlet. Once a regional daily newspaper, The Guardian is currently a nationally respected paper.
Forms of mass media.
Broadcast.
The sequencing of content in a broadcast is called a schedule. With all technological endeavours a number of technical terms and slang have developed please see the list of broadcasting terms for a glossary of terms used.
Television and radio programs are distributed through radio broadcasting over frequency bands that are highly regulated by the Federal Communications Commission in the United States. Such regulation includes determination of the width of the bands, range, licencing, types of receivers and transmitters used, and acceptable content.
Cable programs are often broadcast simultaneously with radio and television programs, but have a more limited audience. By coding signals and having a cable converter box in homes, cable also enables subscription-based channels and pay-per-view services.
A broadcasting organisation may broadcast several programs at the same time, through several channels (frequencies), for example BBC One and Two. On the other hand, two or more organisations may share a channel and each use it during a fixed part of the day. Digital radio and digital television may also transmit multiplexed programming, with several channels compressed into one ensemble.
When broadcasting is done via the Internet the term webcasting is often used. In 2004 a new phenomenon occurred when a number of technologies combined to produce podcasting. Podcasting is an asynchronous broadcast/narrowcast medium, with one of the main proponents being Adam Curry and his associates the Podshow.
Film.
'Film' encompasses motion pictures as individual projects, as well as the field in general. The name comes from the photographic film (also called filmstock), historically the primary medium for recording and displaying motion pictures. Many other terms exist—"motion pictures" (or just "pictures" and "picture"), "the silver screen", "photoplays", "the cinema", "picture shows", "flicks"—and commonly "movies".
Films are produced by recording people and objects with cameras or by creating them using animation techniques and/or special effects. Films comprise a series of individual frames, but when these images are shown in rapid succession, the illusion of motion is given to the viewer. Flickering between frames is not seen because of an effect known as persistence of vision—whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.
Film is considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the film message. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.
Video games.
A video game is a computer-controlled game in which a video display such as a monitor or television is the primary feedback device. The term "computer game" also includes games which display only text (and which can therefore theoretically be played on a teletypewriter) or which use other methods, such as sound or vibration, as their primary feedback device, but there are very few new games in these categories. There always must also be some sort of input device, usually in the form of button/joystick combinations (on arcade games), a keyboard & mouse/trackball combination (computer games), or a controller (console games), or a combination of any of the above. Also, more esoteric devices have been used for input. Usually there are rules and goals, but in more open-ended games the player may be free to do whatever they like within the confines of the virtual universe.
In common usage, a "computer game" or a "PC game" refers to a game that is played on a personal computer. "Console game" refers to one that is played on a device specifically designed for the use of such, while interfacing with a standard television set. "Arcade game" refers to a game designed to be played in an establishment in which patrons pay to play on a per-use basis. "Video game" (or "videogame") has evolved into a catchall phrase that encompasses the aforementioned along with any game made for any other device, including, but not limited to, mobile phones, PDAs, advanced calculators, etc.
Audio recording and reproduction.
Sound recording and reproduction is the electrical or mechanical re-creation and/or amplification of sound, often as music. This involves the use of audio equipment such as microphones, recording devices, and loudspeakers. From early beginnings with the invention of the phonograph using purely mechanical techniques, the field has advanced with the invention of electrical recording, the mass production of the 78 record, the magnetic wire recorder followed by the tape recorder, the vinyl LP record. The invention of the compact cassette in the 1960s, followed by Sony's Walkman, gave a major boost to the mass distribution of music recordings, and the invention of digital recording and the compact disc in 1983 brought massive improvements in ruggedness and quality. The most recent developments have been in digital audio players.
An album is a collection of related audio recordings, released together to the public, usually commercially.
The term record album originated from the fact that 78 RPM Phonograph disc records were kept together in a book resembling a photo album. The first collection of records to be called an "album" was Tchaikovsky's "Nutcracker Suite", release in April 1909 as a four-disc set by Odeon records. It retailed for 16 shillings—about £15 in modern currency.
A music video (also promo) is a short film or video that accompanies a complete piece of music, most commonly a song. Modern music videos were primarily made and used as a marketing device intended to promote the sale of music recordings. Although the origins of music videos go back much further, they came into their own in the 1980s, when Music Television's format was based on them. In the 1980s, the term "rock video" was often used to describe this form of entertainment, although the term has fallen into disuse.
Music videos can accommodate all styles of filmmaking, including animation, live action films, documentaries, and non-narrative, abstract film.
Internet.
The Internet (also known simply as "the Net" or less precisely as "the Web") is a more interactive medium of mass media, and can be briefly described as "a network of networks". Specifically, it is the worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It consists of millions of smaller domestic, academic, business, and governmental networks, which together carry various information and services, such as email, online chat, file transfer, and the interlinked web pages and other documents of the World Wide Web.
Contrary to some common usage, the Internet and the World Wide Web are not synonymous: the Internet is the system of interconnected "computer networks", linked by copper wires, fiber-optic cables, wireless connections etc.; the Web is the contents, or the interconnected "documents", linked by hyperlinks and URLs. The World Wide Web is accessible through the Internet, along with many other services including e-mail, file sharing and others described below.
Toward the end of the 20th century, the advent of the World Wide Web marked the first era in which most individuals could have a means of exposure on a scale comparable to that of mass media. Anyone with a web site has the potential to address a global audience, although serving to high levels of web traffic is still relatively expensive. It is possible that the rise of peer-to-peer technologies may have begun the process of making the cost of bandwidth manageable. Although a vast amount of information, imagery, and commentary (i.e. "content") has been made available, it is often difficult to determine the authenticity and reliability of information contained in web pages (in many cases, self-published). The invention of the Internet has also allowed breaking news stories to reach around the globe within minutes. This rapid growth of instantaneous, decentralized communication is often deemed likely to change mass media and its relationship to society.
"Cross-media" means the idea of distributing the same message through different media channels. A similar idea is expressed in the news industry as "convergence". Many authors understand cross-media publishing to be the ability to publish in both print and on the web without manual conversion effort. An increasing number of wireless devices with mutually incompatible data and screen formats make it even more difficult to achieve the objective “create once, publish many”.
The Internet is quickly becoming the center of mass media. Everything is becoming accessible via the internet. Rather than picking up a newspaper, or watching the 10 o'clock news, people can log onto the internet to get the news they want, when they want it. For example, many workers listen to the radio through the Internet while sitting at their desk.
Even the education system relies on the Internet. Teachers can contact the entire class by sending one e-mail. They may have web pages on which students can get another copy of the class outline or assignments. Some classes have class blogs in which students are required to post weekly, with students graded on their contributions.
Blogs (web logs).
Blogging, too, has become a pervasive form of media. A blog is a website, usually maintained by an individual, with regular entries of commentary, descriptions of events, or interactive media such as images or video. Entries are commonly displayed in reverse chronological order, with most recent posts shown on top. Many blogs provide commentary or news on a particular subject; others function as more personal online diaries. A typical blog combines text, images and other graphics, and links to other blogs, web pages, and related media. The ability for readers to leave comments in an interactive format is an important part of many blogs. Most blogs are primarily textual, although some focus on art (artlog), photographs (photoblog), sketchblog, videos (vlog), music (MP3 blog), audio (podcasting) are part of a wider network of social media. Microblogging is another type of blogging which consists of blogs with very short posts.
RSS feeds.
RSS is a format for syndicating news and the content of news-like sites, including major news sites like Wired, news-oriented community sites like Slashdot, and personal blogs. It is a family of Web feed formats used to publish frequently updated content such as blog entries, news headlines, and podcasts. An RSS document (which is called a "feed" or "web feed" or "channel") contains either a summary of content from an associated web site or the full text. RSS makes it possible for people to keep up with web sites in an automated manner that can be piped into special programs or filtered displays.
Podcast.
A podcast is a series of digital-media files which are distributed over the Internet using syndication feeds for playback on portable media players and computers. The term podcast, like broadcast, can refer either to the series of content itself or to the method by which it is syndicated; the latter is also called podcasting. The host or author of a podcast is often called a podcaster.
Mobile.
Mobile phones were introduced in Japan in 1979 but became a mass media only in 1998 when the first downloadable ringing tones were introduced in Finland. Soon most forms of media content were introduced on mobile phones, tablets and other portable devices, and today the total value of media consumed on mobile vastly exceeds that of internet content, and was worth over 31 billion dollars in 2007 (source Informa). The mobile media content includes over 8 billion dollars worth of mobile music (ringing tones, ringback tones, truetones, MP3 files, karaoke, music videos, music streaming services etc.); over 5 billion dollars worth of mobile gaming; and various news, entertainment and advertising services. In Japan mobile phone books are so popular that five of the ten best-selling printed books were originally released as mobile phone books.
Similar to the internet, mobile is also an interactive media, but has far wider reach, with 3.3 billion mobile phone users at the end of 2007 to 1.3 billion internet users (source ITU). Like email on the internet, the top application on mobile is also a personal messaging service, but SMS text messaging is used by over 2.4 billion people. Practically all internet services and applications exist or have similar cousins on mobile, from search to multiplayer games to virtual worlds to blogs. Mobile has several unique benefits which many mobile media pundits claim make mobile a more powerful media than either TV or the internet, starting with mobile being permanently carried and always connected. Mobile has the best audience accuracy and is the only mass media with a built-in payment channel available to every user without any credit cards or PayPal accounts or even an age limit. Mobile is often called the 7th Mass Medium and either the fourth screen (if counting cinema, TV and PC screens) or the third screen (counting only TV and PC).
Print media.
Book.
A book is a collection of sheets of paper, parchment or other material with a piece of text written on them, bound together along one edge within covers. A book is also a literary work or a main division of such a work. A book produced in electronic format is known as an e-book.
Magazine.
A magazine is a periodical publication containing a variety of articles, generally financed by advertising and/or purchase by readers.
Magazines are typically published weekly, biweekly, monthly, bimonthly or quarterly, with a date on the cover that is in advance of the date it is actually published. They are often printed in color on coated paper, and are bound with a soft cover.
Magazines fall into two broad categories: consumer magazines and business magazines. In practice, magazines are a subset of , distinct from those periodicals produced by scientific, artistic, academic or special interest publishers which are subscription-only, more expensive, narrowly limited in circulation, and often have little or no advertising.
Magazines can be classified as:
Newspaper.
A newspaper is a publication containing news and information and advertising, usually printed on low-cost paper called newsprint. It may be general or special interest, most often published daily or weekly. The first printed newspaper was published in 1605, and the form has thrived even in the face of competition from technologies such as radio and television. Recent developments on the Internet are posing major threats to its business model, however. Paid circulation is declining in most countries, and advertising revenue, which makes up the bulk of a newspaper's income, is shifting from print to online; some commentators, nevertheless, point out that historically new media such as radio and television did not entirely supplant existing.
Outdoor media.
Outdoor media is a form of mass media which comprises billboards, signs, placards placed inside and outside of commercial buildings/objects like shops/buses, flying billboards (signs in tow of airplanes), blimps, skywriting, AR Advertising. Many commercial advertisers use this form of mass media when advertising in sports stadiums. Tobacco and alcohol manufacturers used billboards and other outdoor media extensively. However, in 1998, the Master Settlement Agreement between the US and the tobacco industries prohibited the billboard advertising of cigarettes. In a 1994 Chicago-based study, Diana Hackbarth and her colleagues revealed how tobacco- and alcohol-based billboards were concentrated in poor neighbourhoods. In other urban centers, alcohol and tobacco billboards were much more concentrated in African-American neighborhoods than in white neighborhoods.
Purposes.
Mass media encompasses much more than just news, although it is sometimes misunderstood in this way. It can be used for various purposes:
Professions involving mass media.
Journalism.
Journalism is the discipline of collecting, analyzing, verifying and presenting information regarding current events, trends, issues and people. Those who practice journalism are known as journalists.
News-oriented journalism is sometimes described as the "first rough draft of history" (attributed to Phil Graham), because journalists often record important events, producing news articles on short deadlines. While under pressure to be first with their stories, news media organizations usually edit and proofread their reports prior to publication, adhering to each organization's standards of accuracy, quality and style. Many news organizations claim proud traditions of holding government officials and institutions accountable to the public, while media critics have raised questions about holding the press itself accountable to the standards of professional journalism.
Public relations.
Public relations is the art and science of managing communication between an organization and its key publics to build, manage and sustain its positive image. Examples include:
Publishing.
Publishing is the industry concerned with the production of literature or information – the activity of making information available for public view. In some cases, authors may be their own publishers.
Traditionally, the term refers to the distribution of printed works such as books and newspapers. With the advent of digital information systems and the Internet, the scope of publishing has expanded to include websites, blogs, and the like.
As a business, publishing includes the development, marketing, production, and distribution of newspapers, magazines, books, literary works, musical works, software, other works dealing with information.
Publication is also important as a legal concept; (1) as the process of giving formal notice to the world of a significant intention, for example, to marry or enter bankruptcy, and; (2) as the essential precondition of being able to claim defamation; that is, the alleged libel must have been published.
Software publishing.
A software publisher is a publishing company in the software industry between the developer and the distributor. In some companies, two or all three of these roles may be combined (and indeed, may reside in a single person, especially in the case of shareware).
Software publishers often license software from developers with specific limitations, such as a time limit or geographical region. The terms of licensing vary enormously, and are typically secret.
Developers may use publishers to reach larger or foreign markets, or to avoid focussing on marketing. Or publishers may use developers to create software to meet a market need that the publisher has identified.
History.
The history of mass media can be traced back to the days when dramas were performed in various ancient cultures. This was the first time when a form of media was "broadcast" to a wider audience. The first dated printed book known is the "Diamond Sutra", printed in China in 868 AD, although it is clear that books were printed earlier. Movable clay type was invented in 1041 in China. However, due to the slow spread of literacy to the masses in China, and the relatively high cost of paper there, the earliest printed mass-medium was probably European popular prints from about 1400. Although these were produced in huge numbers, very few early examples survive, and even most known to be printed before about 1600 have not survived. The term "mass media" was coined with the creation of print media, which is notable for being the first example of mass media, as we use the term today. This form of media started in Europe in the Middle Ages.
Johannes Gutenberg's invention of the printing press allowed the mass production of books to sweep the nation. He printed the first book on a printing press with movable type in 1453. The Gutenberg Bible, one of the books he published, was translated into many different languages and printed throughout the continent. The invention of the printing press in the late 15th century gave rise to some of the first forms of mass communication, by enabling the publication of books and newspapers on a scale much larger than was previously possible. The invention also transformed the way the world received printed materials, although books remained too expensive really to be called a mass-medium for at least a century after that. Newspapers developed from about 1612, with the first example in English in 1620; but they took until the 19th century to reach a mass-audience directly. The first high-circulation newspapers arose in London in the early 1800s, such as The Times, and were made possible by the invention of high-speed rotary steam printing presses, and railroads which allowed large-scale distribution over wide geographical areas. The increase in circulation, however, led to a decline in feedback and interactivity from the readership, making newspapers a more one-way medium.
The phrase "the media" began to be used in the 1920s. The notion of "mass media" was generally restricted to print media up until the post-Second World War, when radio, television and video were introduced. The audio-visual facilities became very popular, because they provided both information and entertainment, because the colour and sound engaged the viewers/listeners and because it was easier for the general public to passively watch TV or listen to the radio than to actively read. In recent times, the Internet become the latest and most popular mass medium. Information has become readily available through websites, and easily accessible through search engines. One can do many activities at the same time, such as playing games, listening to music, and social networking, irrespective of location. Whilst other forms of mass media are restricted in the type of information they can offer, the internet comprises a large percentage of the sum of human knowledge through such things as Google Books. Modern day mass media consists of the internet, mobile phones, blogs, podcasts and RSS feeds.
During the 20th century, the growth of mass media was driven by technology, including that which allowed much duplication of material. Physical duplication technologies such as printing, record pressing and film duplication allowed the duplication of books, newspapers and movies at low prices to huge audiences. Radio and television allowed the electronic duplication of information for the first time. Mass media had the economics of linear replication: a single work could make money. An example of Riel and Neil's theory. proportional to the number of copies sold, and as volumes went up, unit costs went down, increasing profit margins further. Vast fortunes were to be made in mass media. In a democratic society, the media can serve the electorate about issues regarding government and corporate entities (see Media influence). Some consider the concentration of media ownership to be a threat to democracy.
Influence and sociology.
Limited-effects theory, originally tested in the 1940s and 1950s, considers that because people usually choose what media to interact with based on what they already believe, media exerts a negligible influence. Class-dominant theory argues that the media reflects and projects the view of a minority elite, which controls it. Culturalist theory, which was developed in the 1980s and 1990s, combines the other two theories and claims that people interact with media to create their own meanings out of the images and messages they receive. This theory states that audience members play an active, rather than passive role in relation to mass media.
In an article entitled "Mass Media Influence on Society", rayuso argues that the media "in the US" is dominated by five major companies (Time Warner, VIACOM, Vivendi Universal, Walt Disney and News Corp) which own 95% of all mass media including theme parks, movie studios, television and radio broadcast networks and programing, video news, sports entertainment, telecommunications, wireless phones, video games software, electronic media and music companies. Whilst historically, there was more diversity in companies, they have recently merged to form an elite which have the power to shape the opinion and beliefs of people. People buy after seeing thousands of advertisements by various companies in TV, newspapers or magazines, which are able to affect their purchasing decisions. The definition of what is acceptable by society is dictated by the media. This power can be used for good, for example encouraging children to play sport. However, it can also be used for bad, for example children being influenced by cigars smoked by film stars, their exposure to sex images, their exposure to images of violence and their exposure to junk food ads. The documentary Supersize Me describes how companies like McDonalds have been sued in the past, the plaintiffs claiming that it was the fault of their liminal and subliminal advertising that "forced" them to perchance the product. The Barbie and Ken dolls of the 1950s are sometimes cited as the main cause for the obsession in modern day society for women to be skinny and men to be buff. After the attacks of 9/11, the media gave extensive coverage of the event and exposed Osama Bin Laden's guilt for the attack, information they were told by the authorities. This shaped the public opinion to support the war on terrorism, and later, the war on Iraq. A main concern is that due to this immense power of the mass media (being able to drive the public opinion), media receiving inaccurate information could cause the public opinion to support the wrong cause.
In his book The Commercialization of American Culture, Matthew P. McAllister says that "a well-developed media system, informing and teaching its citizens, helps democracy move toward its ideal state."
In 1997, J. R. Finnegan Jr. and K. Viswanath identified 3 main effects or functions of mass media:
Since the 1950s, when cinema, radio and TV began to be the primary or the only source of information for a larger and larger percentage of the population, these media began to be considered as central instruments of mass control. Up to the point that it emerged the idea that when a country has reached a high level of industrialization, the country itself "belongs to the person who controls communications."
Mass media play a significant role in shaping public perceptions on a variety of important issues, both through the information that is dispensed through them, and through the interpretations they place upon this information. They also play a large role in shaping modern culture, by selecting and portraying a particular set of beliefs, values, and traditions (an entire way of life), as reality. That is, by portraying a certain interpretation of reality, they shape reality to be more in line with that interpretation. Mass media also play a crucial role in the spread of civil unrest activities such as anti-government demonstrations, riots, and general strikes. That is, the use of radio and television receivers has made the unrest influence among cities not only by the geographic location of cities, but also by proximity within the mass media distribution networks.
Racism and stereotyping.
Mass media has played a large role in the way white Americans perceive African-Americans. The media focus on African-American in the contexts of crime, drug use, gang violence, and other forms of anti-social behavior has resulted in a distorted and harmful public perception of African-Americans. African-Americans have been subjected to oppression and discrimination for the past few hundred years. According to Stephen Balkaran in his article Mass Media and Racism, "The media has played a key role in perpetuating the effects of this historical oppression and in contributing to African-Americans' continuing status as second-class citizens". This has resulted in an uncertainty among white Americans as to what the genuine nature of African-Americans really is. Despite the resulting racial divide, the fact that these people are undeniably American has "raised doubts about the white man's value system". This means that there is a somewhat "troubling suspicion" among some Americans that their white America is tainted by the black influence. Mass media as well as propaganda tend to reinforce or introduce stereotypes to the general public.
The mass media has also been criticized for its racial bias, such as downplaying African-Americans committing crimes and publicizing crimes committed against them by races other than their own. Notable examples are 13-month-old Antonio West's murder being ignored for Trayvon Martin's and Dillon Taylor's for Michael Brown's. The media also tends to censor the ethnicity of black perpetrators and not doing the same to those of other races.
Ethical issues and criticism.
Lack of local or specific topical focus is a common criticism of mass media. A mass news media outlet is often forced to cover national and international news due to it having to cater for and be relevant for a wide demographic. As such, it has to skip over many interesting or important local stories because they simply do not interest the large majority of their viewers. An example given by the website WiseGeek is that "the residents of a community might view their fight against development as critical, but the story would only attract the attention of the mass media if the fight became controversial or if precedents of some form were set".
The term "mass" suggests that the recipients of media products constitute a vast sea of passive, undifferentiated individuals. This is an image associated with some earlier critiques of "mass culture" and mass society which generally assumed that the development of mass communication has had a largely negative impact on modern social life, creating a kind of bland and homogeneous culture which entertains individuals without challenging them. However, interactive digital media have also been seen to challenge the read-only paradigm of earlier broadcast media.
Whilst some refer to the mass media as "opiate of the masses", others argue that is a vital aspect of human societies. By understanding mass media, one is then able to analyse and find a deeper understanding of one's population and culture. This valuable and powerful ability is one reason why the field of media studies is popular. As WiseGeek says, "watching, reading, and interacting with a nation's mass media can provide clues into how people think, especially if a diverse assortment of mass media sources are perused".
Since the 1950s, in the countries that have reached a high level of industrialization, the mass media of cinema, radio and TV have a key role in political power.
Contemporary research demonstrates an increasing level of concentration of media ownership, with many media industries already highly concentrated and dominated by a very small number of firms.
Future.
In 2002, Arnold Kling wrote that "the newspaper business is going to die within the next twenty years. Newspaper publishing will continue, but only as a philanthropic venture." Jim Pinkerton said in 2006 of the future of mass media, "Every country with ambitions on the international stage will soon have its own state-supported media."
Leo Laporte, founder of the TWiT network of podcasts, says that "there will always be a need for storytellers, people who dig up facts and explain them".

</doc>
<doc id="19643" url="http://en.wikipedia.org/wiki?curid=19643" title="Mahabharata">
Mahabharata

The Mahabharata or Mahābhārata (US ; UK ; Sanskrit: महाभारतम्, "Mahābhāratam", ]) is one of the two major Sanskrit epics of ancient India, the other being the "Ramayana".
Besides its epic narrative of the Kurukshetra War and the fates of the Kaurava and the Pandava princes, the "Mahabharata" contains philosophical and devotional material, such as a discussion of the four "goals of life" or "purusharthas" (12.161).
Among the principal works and stories in the "Mahabharata" are the "Bhagavad Gita", the story of Damayanti, an abbreviated version of the Ramayana, and the Rishyasringa, often considered as works in their own right.
Traditionally, the authorship of the "Mahabharata" is attributed to Vyasa. There have been many attempts to unravel its historical growth and compositional layers. The oldest preserved parts of the text are thought to be not much older than around 400 BCE, though the origins of the epic probably fall between the 8th and 9th centuries BCE. The text probably reached its final form by the early Gupta period (c. 4th century CE). The title may be translated as "the great tale of the Bhārata dynasty". According to the "Mahabharata" itself, the tale is extended from a shorter version of 24,000 verses called simply "Bhārata".
The "Mahabharata" is the longest known epic poem and has been described as "the longest poem ever written". Its longest version consists of over 100,000 "shloka" or over 200,000 individual verse lines (each shloka is a couplet), and long prose passages. About 1.8 million words in total, the Mahabharata is roughly ten times the length of the "Iliad" and the "Odyssey" combined, or about four times the length of the Ramayana. W. J. Johnson has compared the importance of the "Mahabharata" to world civilization to that of the Bible, the works of Shakespeare, the works of Homer, Greek drama, or the Qur'an.
Textual history and structure.
The epic is traditionally ascribed to the sage Vyasa, who is also a major character in the epic. Vyasa described it as being "itihāsa" (history). He also describes the Guru-shishya parampara, which traces all great teachers and their students of the Vedic times.
The first section of the "Mahabharata" states that it was Ganesha who wrote down the text to Vyasa's dictation. Ganesha is said to have agreed to write it only if Vyasa never paused in his recitation. Vyasa agrees on condition that Ganesha takes the time to understand what was said before writing it down.
The epic employs the story within a story structure, otherwise known as frametales, popular in many Indian religious and non-religious works. It is recited by the sage Vaisampayana, a disciple of Vyasa, to the King Janamejaya who is the great-grandson of the Pandava prince Arjuna. The story is then recited again by a professional storyteller named Ugrasrava Sauti, many years later, to an assemblage of sages performing the 12-year sacrifice for the king Saunaka Kulapati in the Naimisha Forest.
The text has been described by some early 20th-century western Indologists as unstructured and chaotic. Hermann Oldenberg supposed that the original poem must once have carried an immense "tragic force" but dismissed the full text as a "horrible chaos." Moritz Winternitz ("Geschichte der indischen Literatur" 1909) considered that "only unpoetical theologists and clumsy scribes" could have lumped the parts of disparate origin into an unordered whole.
Accretion and redaction.
Research on the "Mahabharata" has put an enormous effort into recognizing and dating layers within the text. Some elements of the present "Mahabharata" can be traced back to Vedic times. The background to the "Mahabharata" suggests the origin of the epic occurs "after the very early Vedic period" and before "the first Indian 'empire' was to rise in the third century B.C." That this is "a date not too far removed from the 8th or 9th century B.C." is likely. It is generally agreed that "Unlike the Vedas, which have to be preserved letter-perfect, the epic was a popular work whose reciters would inevitably conform to changes in language and style," so the earliest 'surviving' components of this dynamic text are believed to be no older than the earliest 'external' references we have to the epic, which may include an allusion in Panini's 4th century BCE grammar "Ashtādhyāyī" 4:2:56. It is estimated that the Sanskrit text probably reached something of a "final form" by the early Gupta period (about the 4th century CE). Vishnu Sukthankar, editor of the first great critical edition of the "Mahabharata", commented: "It is useless to think of reconstructing a fluid text in a literally original shape, on the basis of an archetype and a "stemma codicum". What then is possible? Our objective can only be to reconstruct "the oldest form of the text which it is possible to reach" on the basis of the manuscript material available." That manuscript evidence is somewhat late, given its material composition and the climate of India, but it is very extensive.
The "Mahabharata" itself (1.1.61) distinguishes a core portion of 24,000 verses: the "Bharata" proper, as opposed to additional secondary material, while the "Ashvalayana Grhyasutra" (3.4.4) makes a similar distinction. At least three redactions of the text are commonly recognized: "Jaya" (Victory) with 8,800 verses attributed to Vyasa, "Bharata" with 24,000 verses as recited by Vaisampayana, and finally the "Mahabharata" as recited by Ugrasrava Sauti with over 100,000 verses. However, some scholars such as John Brockington, argue that "Jaya" and "Bharata" refer to the same text, and ascribe the theory of "Jaya" with 8,800 verses to a misreading of a verse in "Adiparvan" (1.1.81).
The redaction of this large body of text was carried out after formal principles, emphasizing the numbers 18 and 12. The addition of the latest parts may be dated by the absence of the "Anushasana-parva" and the "Virata parva" from the "Spitzer manuscript". The oldest surviving Sanskrit text dates to the Kushan Period (200 CE).
According to what one character says at Mbh. 1.1.50, there were three versions of the epic, beginning with "Manu" (1.1.27), "Astika" (1.3, sub-parva 5) or "Vasu" (1.57), respectively. These versions would correspond to the addition of one and then another 'frame' settings of dialogues. The "Vasu" version would omit the frame settings and begin with the account of the birth of Vyasa. The "astika" version would add the "sarpasattra" and "ashvamedha" material from Brahmanical literature, introduce the name "Mahabharata", and identify Vyasa as the work's author. The redactors of these additions were probably Pancharatrin scholars who according to Oberlies (1998) likely retained control over the text until its final redaction. Mention of the Huna in the "Bhishma-parva" however appears to imply that this parva may have been edited around the 4th century.
The Adi-parva includes the snake sacrifice ("sarpasattra") of Janamejaya, explaining its motivation, detailing why all snakes in existence were intended to be destroyed, and why in spite of this, there are still snakes in existence. This "sarpasattra" material was often considered an independent tale added to a version of the "Mahabharata" by "thematic attraction" (Minkowski 1991), and considered to have a particularly close connection to Vedic (Brahmana) literature. The Panchavimsha Brahmana (at 25.15.3) enumerates the officiant priests of a "sarpasattra" among whom the names Dhrtarashtra and Janamejaya, two main characters of the "Mahabharata"'s "sarpasattra", as well as Takshaka, the name of a snake in the "Mahabharata", occur.
Historical references.
The earliest known references to the "Mahabharata" and its core "Bharata" date to the "Ashtadhyayi" (sutra 6.2.38) of Pāṇini ("fl." 4th century BCE) and in the "Ashvalayana Grhyasutra" (3.4.4). This may suggest that the core 24,000 verses, known as the "Bharata", as well as an early version of the extended "Mahabharata", were composed by the 4th century BCE.
A report by the Greek writer Dio Chrysostom (c. 40 - c. 120 CE) about Homer's poetry being sung even in India seems to imply that the "Iliad" had been translated into Sanskrit. However, scholars have, in general, taken this as evidence for the existence of a "Mahabharata" at this date, whose episodes Dio or his sources identify with the story of the "Iliad".
Several stories within the "Mahabharata" took on separate identities of their own in Classical Sanskrit literature. For instance, Abhijñānashākuntala by the renowned Sanskrit poet Kālidāsa (c. 400 CE), believed to have lived in the era of the Gupta dynasty, is based on a story that is the precursor to the "Mahabharata". Urubhanga, a Sanskrit play written by Bhāsa who is believed to have lived before Kālidāsa, is based on the slaying of Duryodhana by the splitting of his thighs by Bhima.
The copper-plate inscription of the Maharaja Sharvanatha (533–534 CE) from Khoh (Satna District, Madhya Pradesh) describes the "Mahabharata" as a "collection of 100,000 verses" ("shatasahasri samhita").
The 18 parvas or books.
The division into 18 parvas is as follows:
Historical context.
The historicity of the Kurukshetra War is unclear. Many historians estimate the date of the Kurukshetra war to Iron Age India of the 10th century BCE. The setting of the epic has a historical precedent in Iron Age (Vedic) India, where the Kuru kingdom was the center of political power during roughly 1200 to 800 BCE. A dynastic conflict of the period could have been the inspiration for the "Jaya", the foundation on which the Mahabharata corpus was built, with a climactic battle eventually coming to be viewed as an epochal event.
Puranic literature presents genealogical lists associated with the "Mahabharata" narrative. The evidence of the Puranas is of two kinds. Of the first kind, there is the direct statement that there were 1015 (or 1050) years between the birth of Parikshit (Arjuna's grandson) and the accession of Mahapadma Nanda (400-329 BCE), which would yield an estimate of about 1400 BCE for the Bharata battle. However, this would imply improbably long reigns on average for the kings listed in the genealogies.
Of the second kind are analyses of parallel genealogies in the Puranas between the times of Adhisimakrishna (Parikshit's great-grandson) and Mahapadma Nanda. Pargiter accordingly estimated 26 generations by averaging 10 different dynastic lists and, assuming 18 years for the average duration of a reign, arrived at an estimate of 850 BCE for Adhisimakrishna, and thus approximately 950 BCE for the Bharata battle.
B. B. Lal used the same approach with a more conservative assumption of the average reign to estimate a date of 836 BCE, and correlated this with archaeological evidence from Painted Grey Ware sites, the association being strong between PGW artifacts and places mentioned in the epic.
Attempts to date the events using methods of archaeoastronomy have produced, depending on which passages are chosen and how they are interpreted, estimates ranging from the late 4th to the mid-2nd millennium BCE.
The late 4th millennium date has a precedent in the calculation of the Kaliyuga epoch, based on planetary conjunctions, by Aryabhata (6th century). Aryabhatta's date of February 18 3102 BCE for Mahabharata war has become widespread in Indian tradition. Co-incidentally ,this marks the disppearance of Krishna from earth from many source. The Aihole inscription of Pulikeshi II, dated to Saka 556 = 634 CE, claims that 3735 years have elapsed since the Bharata battle, putting the date of Mahabharata war at 3137 BCE.
Another traditional school of astronomers and historians, represented by Vriddha-Garga, Varahamihira (author of the "Brhatsamhita") and Kalhana (author of the "Rajatarangini"), place the Bharata war 653 years after the Kaliyuga epoch, corresponding to 2449 BCE.
Synopsis.
The core story of the work is that of a dynastic struggle for the throne of Hastinapura, the kingdom ruled by the Kuru clan. The two collateral branches of the family that participate in the struggle are the Kaurava and the Pandava. Although the Kaurava is the senior branch of the family, Duryodhana, the eldest Kaurava, is younger than Yudhisthira, the eldest Pandava. Both Duryodhana and Yudhisthira claim to be first in line to inherit the throne.
The struggle culminates in the great battle of Kurukshetra, in which the Pandavas are ultimately victorious. The battle produces complex conflicts of kinship and friendship, instances of family loyalty and duty taking precedence over what is right, as well as the converse.
The "Mahabharata" itself ends with the death of Krishna, and the subsequent end of his dynasty and ascent of the Pandava brothers to heaven. It also marks the beginning of the Hindu age of Kali Yuga, the fourth and final age of mankind, in which great values and noble ideas have crumbled, and man is heading towards the complete dissolution of right action, morality and virtue.
The older generations.
King Janamejaya's ancestor Shantanu, the king of Hastinapura, has a short-lived marriage with the goddess Ganga and has a son, Devavrata (later to be called Bhishma, a great warrior), who becomes the heir apparent. Many years later, when King Shantanu goes hunting, he sees Satyavati, the daughter of the chief of fisherman, and asks her father for her hand. Her father refuses to consent to the marriage unless Shantanu promises to make any future son of Satyavati the king upon his death. To resolve his father's dilemma, Devavrata agrees to relinquish his right to the throne. As the fisherman is not sure about the prince's children honouring the promise, Devavrata also takes a vow of lifelong celibacy to guarantee his father's promise.
Shantanu has two sons by Satyavati, Chitrāngada and Vichitravirya. Upon Shantanu's death, Chitrangada becomes king. He lives a very short uneventful life and dies. Vichitravirya, the younger son, rules Hastinapura. Meanwhile, the King of Kāśī arranges a swayamvara for his three daughters, neglecting to invite the royal family of Hastinapur. In order to arrange the marriage of young Vichitravirya, Bhishma attends the swayamvara of the three princesses Amba, Ambika and Ambalika, uninvited, and proceeds to abduct them. Ambika and Ambalika consent to be married to Vichitravirya.
The oldest princess Amba, however, informs Bhishma that she wishes to marry king of Shalva whom Bhishma defeated at their swayamvara. Bhishma lets her leave to marry king of Shalva, but Shalva refuses to marry her, still smarting at his humiliation at the hands of Bhishma. Amba then returns to marry Bhishma but he refuses due to his vow of celibacy. Amba becomes enraged and becomes Bhishma's bitter enemy, holding him responsible for her plight. Later she is reborn to King Drupada as Shikhandi (or Shikhandini) and causes Bhishma's fall, with the help of Arjuna, in the battle of Kurukshetra.
The Pandava and Kaurava princes.
When Vichitravirya dies young without any heirs, Satyavati asks her first son Vyasa to father children with the widows. The eldest, Ambika, shuts her eyes when she sees him, and so her son Dhritarashtra is born blind. Ambalika turns pale and bloodless upon seeing him, and thus her son Pandu is born pale and unhealthy (the term Pandu may also mean 'jaundiced'). Due to the physical challenges of the first two children, Satyavati asks Vyasa to try once again. However, Ambika and Ambalika send their maid instead, to Vyasa's room. Vyasa fathers a third son, Vidura, by the maid. He is born healthy and grows up to be one of the wisest characters in the "Mahabharata". He serves as Prime Minister (Mahamantri or Mahatma) to King Pandu and King Dhritarashtra.
When the princes grow up, Dhritarashtra is about to be crowned king by Bhishma when Vidura intervenes and uses his knowledge of politics to assert that a blind person cannot be king. This is because a blind man cannot control and protect his subjects. The throne is then given to Pandu because of Dhritarashtra's blindness. Pandu marries twice, to Kunti and Madri. Dhritarashtra marries Gandhari, a princess from Gandhara, who blindfolds herself so that she may feel the pain that her husband feels. Her brother Shakuni is enraged by this and vows to take revenge on the Kuru family. One day, when Pandu is relaxing in the forest, he hears the sound of a wild animal. He shoots an arrow in the direction of the sound. However the arrow hits the sage Kindama, who curses him that if he engages in a sexual act, he will die. Pandu then retires to the forest along with his two wives, and his brother Dhritarashtra rules thereafter, despite his blindness.
Pandu's older queen Kunti, however, had been given a boon by Sage Durvasa that she could invoke any god using a special mantra. Kunti uses this boon to ask Dharma the god of justice, Vayu the god of the wind, and Indra the lord of the heavens for sons. She gives birth to three sons, Yudhisthira, Bhima, and Arjuna, through these gods. Kunti shares her mantra with the younger queen Madri, who bears the twins Nakula and Sahadeva through the Ashwini twins. However, Pandu and Madri indulge in sex, and Pandu dies. Madri dies on his funeral pyre out of remorse. Kunti raises the five brothers, who are from then on usually referred to as the Pandava brothers.
Dhritarashtra has a hundred sons through Gandhari, all born after the birth of Yudhishtira. These are the Kaurava brothers, the eldest being Duryodhana, and the second Dushasana. Other Kaurava brothers were Vikarna and Sukarna. The rivalry and enmity between them and the Pandava brothers, from their youth and into manhood, leads to the Kurukshetra war.
Lakshagraha (the house of lac).
After the deaths of their mother (Madri) and father (Pandu), the Pandavas and their mother Kunti return to the palace of Hastinapur. Yudhisthira is made Crown Prince by Dhritarashtra, under considerable pressure from his kingdom. Dhritarashtra wanted his own son Duryodhana to become king and lets his ambition get in the way of preserving justice.
Shakuni, Duryodhana and Dusasana plot to get rid of the Pandavas. Shakuni calls the architect Purochana to build a palace out of flammable materials like lac and ghee. He then arranges for the Pandavas and the Queen Mother Kunti to stay there, with the intention of setting it alight. However, the Pandavas are warned by their wise uncle, Vidura, who sends them a miner to dig a tunnel. They are able to escape to safety and go into hiding. Back at Hastinapur, the Pandavas and Kunti are presumed dead.
Marriage to Draupadi.
Whilst they were in hiding the Pandavas learn of a swayamvara which is taking place for the hand of the Pāñcāla princess Draupadī. The Pandavas enter the competition in disguise as Brahmins. The task is to string a mighty steel bow and shoot a target on the ceiling, which is the eye of a moving artificial fish, while looking at its reflection in oil below. Most of the princes fail, many being unable to lift the bow. Arjuna succeeds however. The Pandavas return home and inform their mother that Arjuna has won a competition and to look at what they have brought back. Without looking, Kunti asks them to share whatever it is Arjuna has won among themselves. On explaining the previous life of Draupadi, she ends up being the wife of all five brothers.
Indraprastha.
After the wedding, the Pandava brothers are invited back to Hastinapura. The Kuru family elders and relatives negotiate and broker a split of the kingdom, with the Pandavas obtaining a new territory. Yudhishtira has a new capital built for this territory at Indraprastha. Neither the Pandava nor Kaurava sides are happy with the arrangement however.
Shortly after this, Arjuna elopes with and then marries Krishna's sister, Subhadra. Yudhishtira wishes to establish his position as king; he seeks Krishna's advice. Krishna advises him, and after due preparation and the elimination of some opposition, Yudhishthira carries out the "rājasūya yagna" ceremony; he is thus recognised as pre-eminent among kings.
The Pandavas have a new palace built for them, by Maya the Danava. They invite their Kaurava cousins to Indraprastha. Duryodhana walks round the palace, and mistakes a glossy floor for water, and will not step in. After being told of his error, he then sees a pond, and assumes it is not water and falls in. Draupadi laughs at him and ridicules him by saying that this is because of his blind father Dhritrashtra. He then decides to avenge his humiliation.
The dice game.
Shakuni, Duryodhana's uncle, now arranges a dice game, playing against Yudhishtira with loaded dice. Yudhishtira loses all his wealth, then his kingdom. He then even gambles his brothers, himself, and finally his wife into servitude. The jubilant Kauravas insult the Pandavas in their helpless state and even try to disrobe Draupadi in front of the entire court, but her honour is saved by Krishna who miraculously creates lengths of cloth to replace the ones being removed.
Dhritarashtra, Bhishma, and the other elders are aghast at the situation, but Duryodhana is adamant that there is no place for two crown princes in Hastinapura. Against his wishes Dhritarashtra orders for another dice game. The Pandavas are required to go into exile for 12 years, and in the 13th year must remain hidden. If discovered by the Kauravas, they will be forced into exile for another 12 years.
Exile and return.
The Pandavas spend thirteen years in exile; many adventures occur during this time. They also prepare alliances for a possible future conflict. They spend their final year in disguise in the court of Virata, and are discovered just after the end of the year.
At the end of their exile, they try to negotiate a return to Indraprastha. However, this fails, as Duryodhana objects that they were discovered while in hiding, and that no return of their kingdom was agreed. War becomes inevitable.
The battle at Kurukshetra.
The two sides summon vast armies to their help and line up at Kurukshetra for a war. The kingdoms of Panchala, Dwaraka, Kasi, Kekaya, Magadha, Matsya, Chedi, Pandyas, Telinga, and the Yadus of Mathura and some other clans like the Parama Kambojas were allied with the Pandavas. The allies of the Kauravas included the kings of Pragjyotisha, Anga, Kekaya, Sindhudesa (including Sindhus, Sauviras and Sivis), Mahishmati, Avanti in Madhyadesa, Madra, Gandhara, Bahlika people, Kambojas and many others. Before war being declared, Balarama had expressed his unhappiness at the developing conflict and left to go on pilgrimage; thus he does not take part in the battle itself. Krishna takes part in a non-combatant role, as charioteer for Arjuna.
Before the battle, Arjuna, seeing the opposing army includes many relatives and loved ones, including his great grandfather Bhishma and his teacher Drona, has doubts about the battle and he fails to lift his Gāndeeva bow. Krishna wakes him up to his call of duty in the famous Bhagavad Gita section of the epic.
Though initially sticking to chivalrous notions of warfare, both sides soon adopt dishonourable tactics. At the end of the 18-day battle, only the Pandavas, Satyaki, Kripa, Ashwatthama, Kritavarma, Yuyutsu and Krishna survive.
The end of the Pandavas.
After "seeing" the carnage, Gandhari, who had lost all her sons, curses Krishna to be a witness to a similar annihilation of his family, for though divine and capable of stopping the war, he had not done so. Krishna accepts the curse, which bears fruit 36 years later.
The Pandavas, who had ruled their kingdom meanwhile, decide to renounce everything. Clad in skins and rags they retire to the Himalaya and climb towards heaven in their bodily form. A stray dog travels with them. One by one the brothers and Draupadi fall on their way. As each one stumbles, Yudhisthira gives the rest the reason for their fall (Draupadi was partial to Arjuna, Nakula and Sahadeva were vain and proud of their looks, and Bhima and Arjuna were proud of their strength and archery skills, respectively). Only the virtuous Yudhisthira, who had tried everything to prevent the carnage, and the dog remain. The dog reveals himself to be the god Yama (also known as Yama Dharmaraja), and then takes him to the underworld where he sees his siblings and wife. After explaining the nature of the test, Yama takes Yudhishthira back to heaven and explains that it was necessary to expose him to the underworld because (Rajyante narakam dhruvam) any ruler has to visit the underworld at least once. Yama then assures him that his siblings and wife would join him in heaven after they had been exposed to the underworld for measures of time according to their vices.
Arjuna's grandson Parikshit rules after them and dies bitten by a snake. His furious son, Janamejaya, decides to perform a snake sacrifice ("sarpasattra") in order to destroy the snakes. It is at this sacrifice that the tale of his ancestors is narrated to him.
The reunion.
The "Mahabharata" mentions that Karna, the Pandavas, and Dhritarashtra's sons eventually ascended to svarga and "attained the state of the gods" and banded together — "serene and free from anger."
Themes.
Just war.
The "Mahabharata" offers one of the first instances of theorizing about "just war", illustrating many of the standards that would be debated later across the world. In the story, one of five brothers asks if the suffering caused by war can ever be justified. A long discussion ensues between the siblings, establishing criteria like "proportionality" (chariots cannot attack cavalry, only other chariots; no attacking people in distress), "just means" (no poisoned or barbed arrows), "just cause" (no attacking out of rage), and fair treatment of captives and the wounded.
Versions, translations, and derivative works.
"Critical Edition".
Between 1919 and 1966, scholars at the Bhandarkar Oriental Research Institute, Pune, compared the various manuscripts of the epic from India and abroad and produced the "Critical Edition" of the "Mahabharata", on 13,000 pages in 19 volumes, followed by the "Harivamsha" in another two volumes and six index volumes. This is the text that is usually used in current "Mahabharata" studies for reference. This work is sometimes called the "Pune" or "Poona" edition of the "Mahabharata".
Regional versions.
Many regional versions of the work developed over time, mostly differing only in minor details, or with verses or subsidiary stories being added. These include the Tamil street theatre, terukkuttu and kattaikkuttu, the plays of which use themes from the Tamil language versions of "Mahabharata", focusing on Draupadi.
Outside the Indian subcontinent, in Indonesia, a version was developed in ancient Java as Kakawin Bhāratayuddha in the 11th century under the patronage of King Dharmawangsa (990–1016), and later it spread to neighboring island of Bali where today remains a Hindu majority island, despite today Indonesia is the most populous Muslim majority nation. It has become the fertile source for Javanese literature, dance drama (wayang wong), and wayang shadow puppet performances. This Javanese version differ slightly from the original Indian version. For example Draupadi is only be wed to Yudhisthira, not to the entire Pandavas brothers, this might demonstrate ancient Javanese opposition of polyandry practice. The author later added some female characters to be wed to the Pandavas. Arjuna for example is described as having many wives and consorts next to Subhadra. Another difference is Shikhandi did not undergone sex change and remains as a woman, to be wed to Arjuna, and took the role as a warrior princess during the war. Another twist is Gandhari was described as antagonist character that hates Pandava so much. Her hate was out of jealousy, because during svayambara for the hand of Gandhari, she was actually in love with Pandu, but later being wed to his blind elder brother instead, whom she does not love, as a protest she then blindfold herself. Another notable difference is the inclusion of Punakawans, the clown servants of the main characters in the storyline, which is not found in Indian version. This characters includes Semar, Petruk, Gareng and Bagong, they are much-loved by Indonesian audiences. There are some spin-off episode developed in ancient Java, such as Arjunawiwaha composed in 11th century.
A Kawi version of the "Mahabharata", of which eight of the eighteen "parvas" survive, is found on the Indonesian island of Bali. It has been translated into English by Dr. I. Gusti Putu Phalgunadi.
Translations.
A Persian translation of "Mahabharta", titled "Razmnameh", was produced at Akbar's orders, by Faizi and `Abd al-Qadir Bada'uni in the 18th century.
The first complete English translation was the Victorian prose version by Kisari Mohan Ganguli, published between 1883 and 1896 (Munshiram Manoharlal Publishers) and by M. N. Dutt (Motilal Banarsidass Publishers). Most critics consider the translation by Ganguli to be faithful to the original text. The complete text of Ganguli's translation is in the public domain and is available online.
Another English prose translation of the full epic, based on the "Critical Edition", is in progress, published by University Of Chicago Press. It was initiated by Indologist J. A. B. van Buitenen (books 1–5) and, following a 20-year hiatus caused by the death of van Buitenen, is being continued by D. Gitomer of DePaul University (book 6), J. L. Fitzgerald of Brown University (books 11–13) and Wendy Doniger of the University of Chicago (books 14–18).
An early poetry translation by Romesh Chunder Dutt and published in 1898 condenses the main themes of the "Mahabharata" into English verse. A later poetic "transcreation" (author's own description) of the full epic into English, done by the poet P. Lal is complete, and in 2005 began being published by Writers Workshop, Calcutta. The P. Lal translation is a non-rhyming verse-by-verse rendering, and is the only edition in any language to include all slokas in all recensions of the work (not just those in the "Critical Edition"). The completion of the publishing project is scheduled for 2010. Sixteen of the eighteen volumes are now available.
A project to translate the full epic into English prose, translated by various hands, began to appear in 2005 from the Clay Sanskrit Library, published by New York University Press. The translation is based not on the "Critical Edition" but on the version known to the commentator Nīlakaṇṭha. Currently available are 15 volumes of the projected 32-volume edition.
Indian economist Bibek Debroy has also begun an unabridged English translation in ten volumes. was published in March 2010.
Many condensed versions, abridgements and novelistic prose retellings of the complete epic have been published in English, including works by Ramesh Menon, William Buck, R. K. Narayan, C. Rajagopalachari, K. M. Munshi, Krishna Dharma, Romesh C. Dutt, Bharadvaja Sarma, John D. Smith and Sharon Maas.
Derivative literature.
Bhasa, the 2nd- or 3rd-century CE Sanskrit playwright, wrote two plays on episodes in the Marabharata, "Urubhanga" (Broken Thigh), about the fight between Duryodhana and Bhima, while "Madhyamavyayoga" (The Middle One) set around Bhima and his son, Ghatotkacha. The first important play of 20th century was "Andha Yug" ("The Blind Epoch"), by Dharamvir Bharati, which came in 1955, found in "Mahabharat", both an ideal source and expression of modern predicaments and discontent. Starting with Ebrahim Alkazi it was staged by numerous directors. V. S. Khandekar's Marathi novel, "Yayati" (1960) and Girish Karnad's debut play "Yayati" (1961) are based on the story of King Yayati found in the "Mahabharat". Bengali writer and playwright, Buddhadeva Bose wrote three plays set in Mahabharat, "Anamni Angana", "Pratham Partha" and "Kalsandhya".
Chitra Banerjee Divakaruni wrote a version from the perspective of Draupadi entitled "", which was published in 2008.
Amar Chitra Katha published a 1,260 page comic book version of the "Mahabharata".
In film and television.
In Indian cinema, several film versions of the epic have been made, dating back to 1920. In Telugu film Daana Veera Soora Karna (1977) directed by and starring N. T. Rama Rao depicts Karna as the lead character. The "Mahabharata" was also reinterpreted by Shyam Benegal in Kalyug. Prakash Jha directed 2010 film Raajneeti was partially inspired by the "Mahabharata". A 2013 animated adaptation holds the record for India's most expensive animated film.
In the late 1980s, the "Mahabharat" TV series, directed by Ravi Chopra, was televised on India's national television (Doordarshan). In the Western world, a well-known presentation of the epic is Peter Brook's nine-hour play, which premiered in Avignon in 1985, and its five-hour movie version "The Mahabharata" (1989).
Uncompleted projects on the "Mahabharata" include a ones by Rajkumar Santoshi, and a theaterical adaptation planned by Satyajit Ray.
Jain version.
Jain version of "Mahabharata" can be found in the various Jain texts like "Harivamsapurana" (the story of Harivamsa) "Trisastisalakapurusa Caritra" (Hagiography of 63 Illustrious persons), "Pandavacaritra" (lives of Pandavas) and "Pandavapurana" (stories of Pandavas). From the earlier canonical literature, "Antakrddaaśāh" (8th cannon) and "Vrisnidasa" ("upangagama" or secondary canon) contain the stories of Neminatha (22nd Tirthankara), Krishna and Balarama. Prof. Padmanabh Jaini notes that, unlike in the Hindu Puranas, the names Baladeva and Vasudeva are not restricted to Balarama and Krishna in Jain puranas. Instead they serve as names of two distinct class of mighty brothers, who appear nine times in each half of time cycles of the Jain cosmology and rule the half the earth as half-chakravartins. Jaini traces the origin of this list of brothers to the Jinacharitra by Bhadrabahu swami (4th–3rd century BCE). According to Jain cosmology Balarama, Krishna and Jarasandha are the ninth and the last set of Baladeva, Vasudeva, and Partivasudeva. The main battle is not the Mahabharata, but the fight between Krishna and Jarasandha (who is killed by Krishna). Ultimately, the Pandavas and Balarama take renunciation as Jain monks and are reborn in heavens, while on the other hand Krishna and Jarasandha are reborn in hell. In keeping with the law of karma, Krishna is reborn in hell for his exploits (sexual and violent) while Jarasandha for his evil ways. Prof. Jaini admits a possibility that perhaps because of his popularity, the Jain authors were keen to rehabilitate Krishna. The Jain texts predict that after his karmic term in hell is over sometime during the next half time-cycle, Krishna will be reborn as a Jain Tirthankara and attain liberation. Krishna and Balrama are shown as contemporaries and cousins of 22nd Tirthankara, Neminatha. According to this story, Krishna arranged young Neminath’s marriage with Rajamati, the daughter of Ugrasena, but Neminatha, empathizing with the animals which were to be slaughtered for the marriage feast, left the procession suddenly and renounced the world.
Kuru family tree.
This shows the line of royal and family succession, not necessarily the parentage. See the notes below for detail.
Key to Symbols
Notes
The birth order of siblings is correctly shown in the family tree (from left to right), except for Vyasa and Bhishma whose birth order is not described, and Vichitravirya and Chitrangada who were born after them. The fact that Ambika and Ambalika are sisters is not shown in the family tree. The birth of Duryodhana took place after the birth of Karna, Yudhishtira and Bhima, but before the birth of the remaining Pandava brothers.
Some siblings of the characters shown here have been left out for clarity; these include Chitrāngada, the eldest brother of Vichitravirya. Vidura, half-brother to Dhritarashtra and Pandu.
Cultural influence.
In the "Bhagavad Gita", Krishna explains to Arjuna his duties as a warrior and prince and elaborates on different Yogic and Vedantic philosophies, with examples and analogies. This has led to the Gita often being described as a concise guide to Hindu philosophy and a practical, self-contained guide to life. In more modern times, Swami Vivekananda, Bal Gangadhar Tilak, Mahatma Gandhi and many others used the text to help inspire the Indian independence movement.

</doc>
<doc id="19644" url="http://en.wikipedia.org/wiki?curid=19644" title="Mein Kampf">
Mein Kampf

Mein Kampf (], "My Struggle") is an autobiographical manifesto by National Socialist leader Adolf Hitler, in which he outlines his political ideology and future plans for Germany. Volume 1 of "Mein Kampf" was published in 1925 and Volume 2 in 1926. The book was edited by Hitler's deputy Rudolf Hess.
Hitler began dictating the book to Hess while imprisoned for what he considered to be "political crimes" following his failed Putsch in Munich in November 1923. Although Hitler received many visitors initially, he soon devoted himself entirely to the book. As he continued, Hitler realized that it would have to be a two-volume work, with the first volume scheduled for release in early 1925. The governor of Landsberg noted at the time that "he [Hitler] hopes the book will run into many editions, thus enabling him to fulfill his financial obligations and to defray the expenses incurred at the time of his trial." 
Title.
Hitler originally wanted to call his forthcoming book "Viereinhalb Jahre (des Kampfes) gegen Lüge, Dummheit und Feigheit", or "Four and a Half Years (of Struggle) Against Lies, Stupidity and Cowardice". Max Amann, head of the Franz Eher Verlag and Hitler's publisher, is said to have suggested the much shorter "Mein Kampf" or "My Struggle".
Contents.
The arrangement of chapters is as follows: 
Analysis.
In "Mein Kampf", Hitler used the main thesis of "the Jewish peril", which posits a Jewish conspiracy to gain world leadership. The narrative describes the process by which he became increasingly antisemitic and militaristic, especially during his years in Vienna. He speaks of not having met a Jew until he arrived in Vienna, and that at first his attitude was liberal and tolerant. When he first encountered the anti-semitic press, he says, he dismissed it as unworthy of serious consideration. Later he accepted the same anti-semitic views, which became crucial in his program of national reconstruction of Germany.
"Mein Kampf" has also been studied as a work on political theory. For example, Hitler announces his hatred of what he believed to be the world's two evils: Communism and Judaism. The new territory that Germany needed to obtain would properly nurture the "historic destiny" of the German people; this goal, which Hitler referred to as "Lebensraum" (living space), explains why Hitler aggressively expanded Germany eastward, specifically the invasions of Czechoslovakia and Poland, before he launched his attack against Russia. In "Mein Kampf" Hitler openly states that the future of Germany "has to lie in the acquisition of land in the East at the expense of Russia."
During his work, Hitler blamed Germany's chief woes on the parliament of the Weimar Republic, the Jews, and Social Democrats, as well as Marxists, though he believed that Marxists, Social Democrats, and the parliament were all working for Jewish interests. He announced that he wanted to completely destroy the parliamentary system, believing it to be corrupt in principle, as those who reach power are inherent opportunists.
Antisemitism.
While historians diverge on the exact date Hitler decided to forcibly emigrate the Jewish people to Madagascar, few place the decision before the mid 1930s. First published in 1925, "Mein Kampf" shows the ideas that crafted Hitler's personal grievances and ambitions for creating a New Order.
The racial laws to which Hitler referred resonate directly with his ideas in "Mein Kampf". In his first edition of "Mein Kampf", Hitler stated that the destruction of the weak and sick is far more humane than their protection. Apart from his allusion to humane treatment, Hitler saw a purpose in destroying "the weak" in order to provide the proper space and purity for the "strong".
Popularity.
Although Hitler originally wrote this book mostly for the followers of National Socialism, it grew in popularity. He accumulated a tax debt of 405,500 Reichsmark (very roughly in 2015 €1.4 million or US$ 1.5 million) from the sale of about 240,000 copies by the time he became chancellor in 1933 (at which time his debt was waived).
After Hitler rose to power, the book gained a large amount of popularity. (Two other books written by party members, Gottfried Feder's "Breaking The Interest Slavery" and Alfred Rosenberg's "The Myth of the Twentieth Century," have since lapsed into comparative literary obscurity, and no translation of Feder's book from the original German is known.) The book was in high demand in libraries and often reviewed and quoted in other publications. Hitler had made about 1.2 million Reichsmarks from the income of his book in 1933, when the average annual income of a teacher was about 4,800 Mark. During Hitler's years in power, the book was given free to every newlywed couple and every soldier fighting at the front . By the end of the war, about 10 million copies of the book had been sold or distributed in Germany.
Following becoming chancellor of Germany in 1933, Hitler began to distance himself from the book and dismissed it as "fantasies behind bars" that were little more than a series of articles for the "Völkischer Beobachter" and later told Hans Frank that "If I had had any idea in 1924 that I would have become Reich chancellor, I never would have written the book."
Contemporary observations.
"Mein Kampf," in essence, lays out the ideological program Hitler established for the German revolution, by identifying the Jews and "Bolsheviks," as racially and ideologically inferior and threatening, and "Aryans" and National Socialists as racially superior and politically progressive. Hitler's revolutionary goals were limited to expulsion of the Jews from Greater Germany and the unification of German-speaking peoples into one Greater Germany. Hitler desired to restore German lands to their greatest historical extent, real or imagined.
Due to its racist content and the historical effect of Nazism upon Europe during World War II and the Holocaust, it is considered a highly controversial book. Criticism has not come solely from opponents of Nazism. Italian Fascist dictator and Nazi ally Benito Mussolini was also critical of the book, saying that it was "a boring tome that I have never been able to read" and remarked that Hitler's beliefs, as expressed in the book, were "little more than commonplace clichés".
One direct opponent of National Socialism, Konrad Heiden, observed that the content of "Mein Kampf" is essentially a political argument with other members of the Nazi Party who had appeared to be Hitler's friends, but whom he was actually denouncing in the book's content – sometimes by not even including references to them.
In "The Second World War," Winston Churchill wrote that he felt that after Hitler's ascension to power, no other book deserved more intensive scrutiny.
The American literary theorist and philosopher Kenneth Burke wrote a rhetorical analysis of the work, "The Rhetoric of Hitler's "Battle"", which revealed its underlying message of aggressive intent.
German publication history.
While Hitler was in power (1933–1945), "Mein Kampf" came to be available in three common editions. The first, the "Volksausgabe" or People's Edition, featured the original cover on the dust jacket and was navy blue underneath with a gold swastika eagle embossed on the cover. The "Hochzeitsausgabe", or Wedding Edition, in a slipcase with the seal of the province embossed in gold onto a parchment-like cover was given free to marrying couples. In 1940, the "Tornister-Ausgabe" was released. This edition was a compact, but unabridged, version in a red cover and was released by the post office, available to be sent to loved ones fighting at the front. These three editions combined both volumes into the same book.
A special edition was published in 1939 in honour of Hitler's 50th birthday. This edition was known as the "Jubiläumsausgabe", or Anniversary Issue. It came in both dark blue and bright red boards with a gold sword on the cover. This work contained both volumes one and two. It was considered a deluxe version, relative to the smaller and more common "Volksausgabe".
The book could also be purchased as a two-volume set during Hitler's reign, and was available in soft cover and hardcover. The soft cover edition contained the original cover (as pictured at the top of this article). The hardcover edition had a leather spine with cloth-covered boards. The cover and spine contained an image of three brown oak leaves.
English translations.
Dugdale abridgement.
The first English translation was an abridgement by Edgar Dugdale who started work on it in 1931, at the prompting of his wife, Blanche. When he learned that the London publishing firm of Hurst & Blackett had secured the rights to publish an abridgement in the United Kingdom, he offered it for free in April 1933. However, a local Nazi Party representative insisted that the translation be further abridged before publication, so it was held back until 13 October 1933, although excerpts were allowed to run in "The Times" in late July. It was published by Hurst & Blackett as part of "The Paternoster Library".
In America, Houghton Mifflin secured the rights to the Dugdale abridgement on 29 July 1933. The only differences between the American and British versions are that the title was translated "My Struggle" in the UK and "My Battle" in America; and that Dugdale is credited as translator in the US edition, while the British version withheld his name. Both Dugdale and his wife were active in the Zionist movement; Blanche was the niece of Lord Balfour, and they wished to avoid publicity.
Reynal and Hitchcock translation.
Houghton and Mifflin licensed Reynal & Hitchcock the rights to publish a full unexpurgated translation in 1938. The book was translated from the two volumes of the first German edition (1925 and 1927), with notations appended noting any changes made in later editions, which were deemed "not as extensive as popularly supposed." The translation, made by a committee from the New School for Social Research headed by Dr. Alvin Johnson, was said to have been made with a view to readability rather than in an effort to rigidly reproduce Hitler's sometimes idiosyncratic German form.
The text was heavily annotated for an American audience with biographical and historical details derived largely from German sources. As the translators deemed the book "a propagandistic essay of a violent partisan" which "often warps historical truth and sometimes ignores it completely," the tone of many of these annotations reflected a conscious attempt to provide "factual information which constitutes an extensive critique of the original." The book appeared for sale on 28 February 1939.
Murphy translation.
One of the first complete English translations of "Mein Kampf" was by James Murphy in 1939. It was the only English translation approved by the Third Reich. The version published by Hutchison & Co. in association with Hurst & Blackett, Ltd (London) in 1939 of the combined volumes I and II is profusely illustrated with many full page drawings and photographs. The opening line, "It has turned out fortunate for me to-day that destiny appointed Braunau-on-the-Inn to be my birthplace," is characteristic of Hitler's sense of destiny that began to develop in the early 1920s. Hurst & Blackett ceased publishing the Murphy translation in 1942 when the original plates were destroyed by German bombing, but it is still published and available in facsimile editions and also on the Internet. An audio reading of volume one is also available online.
Stackpole translation and controversy.
The small Pennsylvania firm of Stackpole and Sons released its own unexpurgated translation by William Soskin on the same day as Houghton Mifflin, amid much legal wrangling. The Second Circuit Court of Appeals ruled in Houghton Mifflin's favour that June and ordered Stackpole to stop selling their version, but litigation followed for a few more years until the case was finally resolved in September 1941.
Among other things, Stackpole argued that Hitler could not have legally transferred his right to a copyright in the United States to Eher Verlag in 1925, because he was not a citizen of any country. "Houghton Mifflin v. Stackpole" was a minor landmark in American copyright law, definitively establishing that stateless persons have the same copyright status in the United States that any other foreigner would. In the three months that Stackpole's version was available it sold 12,000 copies.
Cranston translation and controversy.
Houghton Mifflin's abridged English translation left out some of Hitler's more anti-Semitic and militaristic statements. This motivated Alan Cranston, an American reporter for United Press International in Germany (and later a U.S. Senator from California), to publish his own abridged and annotated translation. Cranston believed this version more accurately reflected the contents of the book and Hitler's intentions. In 1939, Cranston was sued by Hitler's publisher for copyright infringement, and a Connecticut judge ruled in Hitler's favour. By the time the publication of Cranston's version was stopped, 500,000 copies had already been sold. Today, the profits and proceeds are given to various charities.
Manheim translation.
Houghton Mifflin published a translation by Ralph Manheim in 1943. They did this to avoid having to share their profits with Reynal & Hitchcock, and to increase sales by offering a more readable translation. The Manheim translation was first published in the United Kingdom by Hurst & Blackett in 1969 amid some controversy.
Excerpts.
In addition to the above translations and abridgments, the following collections of excerpts were available in English before the start of the war:
Official Nazi translation.
A previously unknown English translation was released in 2008, which was prepared by the official Nazi printing office, the Franz Eher Verlag. In 1939, the Nazi propaganda ministry hired James Murphy to create an English version of "Mein Kampf", which they hoped to use to promote Nazi goals in English-speaking countries. While Murphy was in Germany, he became less enchanted with Nazi ideology and made some statements that the Propaganda Ministry disliked. As a result, they asked him to leave Germany immediately. He was not able to take any of his notes but later sent his wife back to obtain his partial translation. These notes were later used to create the Murphy translation. The Nazi government did not abandon their English translation efforts. They used their own staff to finish the translation and it was published in very small numbers in Germany. At least one copy found its way to a British/American POW camp. It is the only official English translation produced by the Nazi government and printed on Nazi printing presses.
Sales and royalties.
Sales of Dugdale abridgment in the United Kingdom.
Sales of the Houghton Mifflin Dugdale translation in America.
The first printing of the U.S. Dugdale edition, the October 1933 with 7,603 copies, of which 290 were given away as complimentary gifts.
The royalty on the first printing in the U.S. was 15% or $3,206.45 total. Curtis Brown, literary agent, took 20%, or $641.20 total, and the IRS took $384.75, leaving Eher Verlag $2,180.37 or RM 5668.
The January 1937 second printing was c. 4,000 copies.
There were three separate printings from August 1938 to March 1939, totaling 14,000; sales totals by 31 March 1939 were 10,345.
The Murphy and Houghton Mifflin translations were the only ones published by the authorised publishers while Hitler was still alive, and not at war with Britain and America.
There was some resistance from Eher Verlag to Hurst and Blackett's Murphy translation, as they had not been granted the rights to a full translation. However, they allowed it "de facto" permission by not lodging a formal protest, and on 5 May 1939, even inquired about royalties. The British publishers responded on the 12th that the information they requested was "not yet available" and the point would be moot within a few months, on 3 September 1939, when all royalties were halted due to the state of war existing between Britain and Germany.
Royalties were likewise held up in the United States due to the litigation between Houghton Mifflin and Stackpole. Because the matter was only settled in September 1941, only a few months before a state of war existed between Germany and the U.S., all Eher Verlag ever got was a $2,500 advance from Reynal and Hitchcock. It got none from the unauthorised Stackpole edition or the 1943 Manheim edition.
Current availability.
At the time of his suicide, Hitler's official place of residence was in Munich, which led to his entire estate, including all rights to "Mein Kampf", changing to the ownership of the state of Bavaria. As per German copyright law, the entire text is scheduled to enter the public domain on 1 January 2016, 70 years after the author's death.
The government of Bavaria, in agreement with the federal government of Germany, refuses to allow any copying or printing of the book in Germany, and opposes it also in other countries but with less success. Owning and buying the book is legal. Trading in old copies is legal as well, unless it is done in such a fashion as to "promote hatred or war", which is generally illegal. In particular, the unmodified edition is not covered by §86 StGB that forbids dissemination of means of propaganda of unconstitutional organisations, since it is a "pre-constitutional work" and as such cannot be opposed to the free and democratic basic order, according to a 1979 decision of the Federal Court of Justice of Germany. Most German libraries carry heavily commented and excerpted versions of "Mein Kampf." In 2008, Stephan Kramer, secretary-general of the Central Council of Jews in Germany, not only recommended lifting the ban, but volunteered the help of his organization in editing and annotating the text, saying that it is time for the book to be made available to all online.
Restrictions on sale or special circumstances regarding the book in other countries:
Canada.
Though "Mein Kampf" (ISBN 0-395-07801-6) is available in Canada, Heather Reisman, owner of the
Chapters/Indigo chain of bookshops (Canada's largest and only national book chain) has decided not to carry the book.
India.
It has been a popular book in India since its first publication there in 1928. It has gone through hundreds of editions and sold over a hundred thousand copies.
Russia.
In the Russian Federation, "Mein Kampf" has been published at least three times since 1992; the Russian text is also available on websites. In 2006 the Public Chamber of Russia proposed banning the book. In 2009 St. Petersburg's branch of the Russian Ministry of Internal Affairs requested to remove an annotated and hyper-linked Russian translation of the book from a historiography web site. On 13 April 2010, it was announced that "Mein Kampf" is outlawed on grounds of extremism promotion.
Sweden.
It has been reprinted several times since 1945; in 1970, 1992, 2002 and 2010. In 1992 the Government of Bavaria tried to stop the publication of the book, and the case went to the Supreme Court of Sweden which ruled in favour of the publisher, stating that the book is protected by copyright, but that the copyright holder is unidentified (and not the State of Bavaria) and that the original Swedish publisher from 1934 had gone out of business. It therefore refused the Government of Bavaria's claim.
The only translation changes came in the 1970 edition, but they were only linguistic, based on a new Swedish standard.
Turkey.
"Mein Kampf" was widely available and growing in popularity in Turkey, even to the point where it became a bestseller, selling up to 100,000 copies in just two months in 2005. Analysts and commentators believe the popularity of the book to be related to a rise in nationalism, anti-U.S. and antisemitic sentiment "because of what is happening in the Middle East, the Israeli-Palestinian problem and the war in Iraq". Dogu Ergil, a political scientist at Ankara University, said both left-wingers, the far-right and Islamists, had found common ground—"not on a common agenda for the future, but on their anxieties, fears and hate".
United States.
In the United States the book can be found at almost any community library and can be bought, sold and traded in bookshops. The U.S. government seized the copyright during the Second World War under the Trading with the Enemy Act and in 1979, Houghton Mifflin, the U.S. publisher of the book, bought the rights from the government pursuant to . More than 15,000 copies are sold a year.
Online availability.
In 1999, the Simon Wiesenthal Center documented that major Internet booksellers such as Amazon.com and Barnesandnoble.com sell "Mein Kampf" to Germany. After a public outcry, both companies agreed to stop those sales to addresses in Germany. The book is currently available through both companies online. It is also available in various languages including German at the Internet Archive.
Republication in Germany after 2015.
On 3 February 2010, the Institute of Contemporary History (IfZ) in Munich announced plans to republish an annotated version of the text, for educational purposes in schools and universities, in 2015, when the copyright currently held by the Bavarian state government expires (2016). This would be the book's first publication in Germany since 1945. A group of German historians argued that a republication was necessary to get an authoritative annotated edition by the time the copyright runs out, which will open the way for neo-Nazi groups to publish their own versions. "Once Bavaria's copyright expires, there is the danger of charlatans and neo-Nazis appropriating this infamous book for themselves," Wolfgang Heubisch said. The Bavarian government opposed the plan, citing respect for victims of the Holocaust. The Bavarian Finance Ministry said that permits for reprints would not be issued, at home or abroad. This would also apply to a new annotated edition. The republished book might be banned as Nazi propaganda. Even after expiration of the copyright, the Bavarian government emphasised that "the dissemination of Nazi ideologies will remain prohibited in Germany and is punishable under the penal code".
On 12 December 2013 the Bavarian government cancelled its financial support for an annotated edition. The Institute of Contemporary History (IfZ) in Munich, which is preparing the translation, announced that it intended to proceed with publication. 
The IfZs edition of "Mein Kampf" is scheduled for release in 2016.
Sequel.
After the party's poor showing in the 1928 elections, Hitler believed that the reason for his loss was the public's misunderstanding of his ideas. He then retired to Munich to dictate a sequel to "Mein Kampf" to expand on its ideas, with more focus on foreign policy.
Only two copies of the 200-page manuscript were originally made, and only one of these was ever made public. The document was neither edited nor published during the Nazi era and remains known as "Zweites Buch", or "Second Book". To keep the document strictly secret, in 1935 Hitler ordered that it be placed in a safe in an air raid shelter. It remained there until being discovered by an American officer in 1945.
The authenticity of the document found in 1945 has been verified by Josef Berg (former employee of the Nazi publishing house Eher Verlag) and Telford Taylor (former Brigadier General U.S.A.R. and Chief Counsel at the Nuremberg war-crimes trials).
In 1958, the "Zweites Buch" was found in the archives of the United States by American historian Gerhard Weinberg. Unable to find an American publisher, Weinberg turned to his mentor – Hans Rothfels at the Institute of Contemporary History in Munich, and his associate Martin Broszat – who published "Zweites Buch" in 1961. A pirated edition was published in English in New York in 1962. The first authoritative English edition was not published until 2003 ("Hitler's Second Book: The Unpublished Sequel to Mein Kampf," ISBN 1-929631-16-2).

</doc>
<doc id="19645" url="http://en.wikipedia.org/wiki?curid=19645" title="Morpheus">
Morpheus

Morpheus may refer to:
Characters
Technology
Music
Other

</doc>
<doc id="19648" url="http://en.wikipedia.org/wiki?curid=19648" title="May 26">
May 26

May 26 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19649" url="http://en.wikipedia.org/wiki?curid=19649" title="MVS">
MVS

Multiple Virtual Storage, more commonly called MVS, was the most commonly used operating system on the System/370 and System/390 IBM mainframe computers. It was developed by IBM, but is unrelated to IBM's other mainframe operating systems, e.g., VSE, VM, TPF.
First released in 1974, MVS was extended by program products with new names multiple times, first to MVS/SE (MVS/System Extension), next to MVS/SP (MVS/System Product) Version 1, next to MVS/XA (MVS/eXtended Architecture), next to MVS/ESA (MVS/Enterprise Systems Architecture), next to OS/390 and finally to z/OS (when 64-bit support was added with the zSeries models). IBM added Unix support (originally called OPEN EDITION) in MVS/SP V4.3 and has obtained POSIX and Unix certifications at several different levels. The MVS core remains fundamentally the same operating system. By design, programs written for MVS run on z/OS without modification.
At first IBM described MVS as simply a new release of OS/VS2, but it was, in fact a major rewrite. OS/VS2 release 1 was an upgrade of OS/360 MVT that retained most of the original code and, like MVT, was mainly written in assembly language. The MVS core was almost entirely written in Assembler XF, although a few modules were written in PL/S, but not the performance-sensitive ones, in particular not the Input/Output Supervisor (IOS). IBM's use of "OS/VS2" emphasized upwards compatibility: application programs that ran under MVT did not even need recompiling to run under MVS. The same Job Control Language files could be used unchanged; utilities and other non-core facilities like TSO ran unchanged. IBM and users almost unanimously called the new system MVS from the start, and IBM continued to use the term "MVS" in the naming of later "major" versions such as MVS/XA.
Evolution of MVS.
OS/360 MFT (Multitasking with a Fixed number of Tasks) provided multitasking: several memory partitions, each of a fixed size, were set up when the operating system was installed and when the operator redefined them. For example, there could be a small partition, two medium partitions, and a large partition. If there were two large programs ready to run, one would have to wait until the other finished and vacated the large partition.
OS/360 MVT (Multitasking with a Variable number of Tasks) was an enhancement that further refined memory use. Instead of using fixed-size memory partitions, MVT allocated memory to regions for job steps as needed, provided enough "contiguous" physical memory was available. This was a significant advance over MFT's memory management, but had some weaknesses: if a job allocated memory dynamically (as most sort programs and database management systems do), the programmers had to estimate the job's maximum memory requirement and pre-define it for MVT. A job step that contained a mix of small and large programs wasted memory while the small programs ran. Most seriously, memory could become fragmented, i.e., the memory not used by current jobs could be divided into uselessly small chunks between the areas used by current jobs, and the only remedy was to wait some current jobs finished before starting any new ones.
In the early 1970s IBM sought to mitigate these difficulties by introducing virtual memory (which IBM called "virtual storage"), which allowed programs to request address spaces larger than physical memory. The original implementations had a single virtual address space, shared by all jobs. OS/VS1 was OS/360 MFT within a single virtual address space; OS/VS2 SVS was OS/360 MVT within a single virtual address space. So OS/VS1 and SVS in principle had the same disadvantages as MFT and MVT, but the impacts were less severe because jobs could request much larger address spaces and the requests came out of a 16 MB pool even if physical storage was smaller.
In the mid-1970s IBM introduced MVS, which not only supported virtual storage that was larger than the available real storage, as did SVS, but also allowed an indefinite number of applications to run in different address spaces. Two concurrent programs might try to access the same virtual memory address, but the virtual memory system redirected these requests to different areas of physical memory. Each of these address spaces consisted of three areas: an operating system (one instance shared by all jobs), an application area unique for each application, and a shared virtual area used for various purposes, including inter-job communication. IBM promised that application areas would always be at least 8 MB. This made MVS the perfect solution for business problems that resulted from the need to run more applications.
MVS maximized processing potential by providing multiprogramming and multiprocessing capabilities. Like its MVT and OS/VS2 SVS predecessors, MVS supported multiprogramming; program instructions and associated data are scheduled by a control program and given processing cycles. Unlike a single-programming operating system, these systems maximize the use of the processing potential by dividing processing cycles among the instructions associated with several different concurrently running programs. This way, the control program does not have to wait for the I/O operation to complete before proceeding. By executing the instructions for multiple programs, the computer is able to switch back and forth between active and inactive programs.
Early editions of MVS (mid-1970s) were among the first of the IBM OS series to support multiprocessor configurations, though the M65MP variant of OS/360 running on 360 Models 65 and 67 had provided limited multiprocessor support. The 360 Model 67 had also hosted the multiprocessor capable TSS/360, MTS and CP-67 operating systems. Because multiprocessing systems can execute instructions simultaneously, they offer greater processing power than single-processing system. As a result, MVS was able to address the business problems brought on by the need to process large amounts of data.
Multiprocessing systems are either loosely coupled, which means that each computer has access to a common workload, or tightly coupled, which means that the computers share the same real storage and are controlled by a single copy of the operating system. MVS retained both the loosely coupled multiprocessing of Attached Support Processor (ASP) and the tightly coupled multiprocessing of OS/360 Model 65 Multiprocessing. In tightly-coupled systems, two CPUs shared concurrent access to the same memory (and copy of the operating system) and peripherals, providing greater processing power and a degree of graceful degradation if one CPU failed. In loosely-coupled configurations each of a group of processors (single and / or tightly-coupled) had its own memory and operating system but shared peripherals and the operating system component JES3 allowed managing the whole group from one console. This provided greater resilience and let operators decide which processor should run which jobs from a central job queue. MVS JES3 gave users the opportunity to network together two or more data processing systems via shared disks and Channel-to-Channedl Adapters (CTCA's). This capability eventually became available to JES2 users as Multi-Access SPOOL (MAS).
MVS originally supported 24-bit addressing (i.e., up to 16 MB). As the underlying hardware progressed, it supported 31-bit (XA and ESA; up to 2048 MB) and now (as z/OS) 64-bit addressing. The most significant motives for the rapid upgrade to 31-bit addressing were the growth of large transaction-processing networks, mostly controlled by CICS, which ran in a single address space—and the DB2 relational database management system needed more than 8 MB of application address space to run efficiently. (Early versions were configured into two address spaces that communicated via the shared virtual area, but this imposed a significant overhead since all such communications had transmit via the operating system.)
The main user interfaces to MVS are: Job Control Language (JCL), which was originally designed for batch processing but from the 1970s onwards was also used to start and allocate resources to long-running interactive jobs such CICS; and TSO (Time Sharing Option), the interactive time-sharing interface, which was mainly used to run development tools and a few end-user information systems. ISPF is a TSO application for users on 3270-family terminals (and later, on VM as well), which allows the user to accomplish the same tasks as TSO's command line but in a menu and form oriented manner, and with a full screen editor and file browser. TSO's basic interface is command line, although facilities were added later for form-driven interfaces).
MVS took a major step forward in fault-tolerance, built on the earlier STAE facility, that IBM called "software recovery". IBM decided to do this after years of practical real-world experience with MVT in the business world. System failures were now having major impacts on customer businesses, and IBM decided to take a major design jump, to assume that despite the very best software development and testing techniques, that 'problems WILL occur.' This profound assumption was pivotal in adding great percentages of fault-tolerance code to the system and likely contributed to the system's success in tolerating software and hardware failures. Statistical information is hard to come by to prove the value of these design features (how can you measure 'prevented' or 'recovered' problems?), but IBM has, in many dimensions, enhanced these fault-tolerant software recovery and rapid problem resolution features, over time.
This design specified a hierarchy of error-handling programs, in system (kernel/'privileged') mode, called Functional Recovery Routines, and in user ('task' or 'problem program') mode, called "ESTAE" (Extended Specified Task Abnormal Exit routines) that were invoked in case the system detected an error (actually, hardware processor or storage error, or software error). Each recovery routine made the 'mainline' function reinvokable, captured error diagnostic data sufficient to debug the causing problem, and either 'retried' (reinvoke the mainline) or 'percolated' (escalated error processing to the next recovery routine in the hierarchy).
Thus, with each error the system captured diagnostic data, and attempted to perform a repair and keep the system up. The worst thing possible was to take down a user address space (a 'job') in the case of unrepaired errors. Though it was an initial design point, it was not until the most recent MVS version (z/OS), that recovery program was not only guaranteed its own recovery routine, but each recovery routine now has its own recovery routine. This recovery structure was embedded in the basic MVS control program, and programming facilities are available and used by application program developers and 3rd party developers.
Practically, the MVS software recovery made problem debugging both easier and more difficult. Software recovery requires that programs leave 'tracks' of where they are and what they are doing, thus facilitating debugging—but the fact that processing progresses despite an error can overwrite the tracks. Early date capture at the time of the error maximizes debugging, and facilities exist for the recovery routines (task and system mode, both) to do this.
IBM included additional criteria for a major software problem that required IBM service. If a mainline component failed to initiate software recovery, that was considered a valid reportable failure. Also, if a recovery routine failed to collect significant diagnostic data such that the original problem was solvable by data collected by that recovery routine, IBM standards dictated that this fault was reportable and required repair. Thus, IBM standards, when rigorously applied, encouraged continuous improvement.
IBM introduced an on-demand hypervisor, a major serviceability tool, called Dynamic Support System (DSS), in the first release of MVS. This facility could be invoked to initiate a session to create diagnostic procedures, or invoke already-stored procedures. The procedures 'trapped' special events, such as the loading of a program, device I/O, system procedure calls, and then triggered the activation of the previously-defined procedures. These procedures, which could be invoked recursively, allowed for reading and writing of data, and alteration of instruction flow. Program Event Recording hardware was used. Due to the overhead of this tool, it was removed from customer-available MVS systems. Program-Event Recording (PER) exploitation was performed by the enhancement of the diagnostic "SLIP" command with the introduction of the PER support (SLIP/Per) in SU 64/65 (1978).
Multiple copies of MVS (or other IBM operating systems) could share the
same machine if that machine was controlled by VM/370. In this case VM/370 was the real operating system, and regarded the "guest" operating systems as applications with unusually high privileges. As a result of later hardware enhancements one instance of an operating system (either MVS, or VM with guests, or other) could also occupy a Logical Partition (LPAR) instead of an entire physical system.
Multiple MVS instances can be organized and collectively administered in a structure called a "systems complex" or "sysplex", introduced in September, 1990. Instances interoperate through a software component called a Cross-system Coupling Facility (XCF) and a hardware component called a "Hardware Coupling Facility" (CF or Integrated Coupling Facility, ICF, if co-located on the same mainframe hardware). Multiple sysplexes can be joined via standard network protocols such as IBM's proprietary Systems Network Architecture (SNA) or, more recently, via TCP/IP. The z/OS operating system (MVS' most recent descendant) also has native support to execute POSIX applications.
Files are properly called data sets in MVS. Names of those files are organized in "catalogs" that are VSAM files themselves.
The native encoding scheme of MVS and its peripherals is Big Endian EBCDIC, but over time IBM added hardware-accelerated services to perform translation and support of ASCII, Little Endian, and Unicode.
MVS filesystem.
Data set names (DSNs, mainframe term for filenames) are organized in a hierarchy whose levels are separated with dots, e.g. "DEPT01.SYSTEM01.FILE01". Each level in the hierarchy can be up to eight characters long. The total filename length is a maximum of 44 characters including dots. By convention, the components separated by the dots are used to organize files similarly to directories in other operating systems. For example there were utility programs that performed similar functions to those of Windows Explorer (but without the GUI and usually in batch processing mode) - adding, renaming or deleting new elements and reporting all the contents of a specified element. However, unlike in many other systems, these levels are not usually actual directories but just a naming convention (like the original Macintosh File System, where folder hierarchy was an illusion maintained by the Finder). TSO supports a default prefix for files (similar to a "current directory" concept), and RACF supports setting up access controls based on filename patterns, analogous to access controls on directories on other platforms.
As with other members of the OS family, MVS' data sets were record-oriented. MVS inherited three main types from its predecessors:
Sequential and ISAM datasets could store either fixed-length or variable length records, and all types could occupy more than one disk volume.
All of these are based on the VTOC disk structure.
Early IBM database management systems used various combinations of ISAM and BDAM datasets - usually BDAM for the actual data storage and ISAM for indexes.
In the early 1970s IBM's virtual memory operating systems introduced a new file management component, VSAM, which provided similar facilities:
These VSAM formats became the basis of IBM's database management systems, IMS/VS and DB2 - usually ESDS for the actual data storage and KSDS for indexes.
VSAM also included a catalog component used for MVS' master catalog.
Partitioned datasets (PDS) were sequential datasets subdivided into "members" that could be processed as sequential files in their own right. The most important use of PDS was for program libraries - system administrators used the main PDS as a way to allocate disk space to a project and the project team then created and edited the members.
Generation Data Groups (GDGs) were originally designed to support grandfather-father-son backup procedures - if a file was modified, the changed version became the new "son", the previous "son" became the "father", the previous "father" became the "grandfather" and the previous "grandfather" was deleted. But one could set up GDGs with a lot more than 3 generations and some applications used GDGs to collect data from several sources and feed the information to one program - each collecting program created a new generation of the file and the final program read the whole group as a single sequential file (by not specifying a generation in the JCL).
Modern versions of MVS (e.g., z/OS) also support POSIX-compatible "slash" filesystems along with facilities for integrating the two filesystems. That is, the OS can make an MVS dataset appear as a file to a POSIX program or subsystem. These newer filesystems include Hierarchical File System (HFS) (not to be confused with Apple's Hierarchical File System) and zFS (not to be confused with Sun's ZFS).
History and modernity.
MVS is now a part of z/OS, older MVS releases are no longer supported by IBM and since 2007 only 64-bit z/OS releases are supported. z/OS supports running older 24-bit and 31-bit MVS applications alongside 64-bit applications.
MVS releases up to 3.8j (24-bit, released in 1981) were freely available and it is now possible to run the MVS 3.8j release in mainframe emulators for free.
MVS/370.
MVS/370 is a generic term for all versions of the MVS operating system prior to MVS/XA. The System/370 architecture, at the time MVS was released, supported only 24-bit virtual addresses, so the MVS/370 operating system architecture is based on a 24-bit address. Because of this 24-bit address length, programs running under MVS/370 are each given 16 megabytes of contiguous virtual storage.
MVS/XA.
MVS/XA, or Multiple Virtual Storage/Extended Architecture, was a version of MVS that supported the 370-XA architecture, which expanded addresses from 24 bits to 31 bits, providing a 2 gigabyte addressable memory area. It also supported a 24-bit legacy addressing mode for older 24-bit applications (i.e. those that stored a 24-bit address in the lower 24 bits of a 32-bit word and utilized the upper 8 bits of that word for other purposes).
MVS/ESA.
MVS/ESA: MVS Enterprise System Architecture. Version of MVS, first introduced as MVS/SP Version 3 in February 1988. Replaced by/renamed as OS/390 late 1995 and subsequently as z/OS.
MVS/ESA OpenEdition: upgrade to Version 4 Release 3 of MVS/ESA announced February 1993 with support for POSIX and other standards. While the initial release only had National Institute of Standards and Technology (NIST) certification for Federal Information Processing Standard (FIPS) 151 compliance, subsequent releases were certified at higher levels and by other organizations, e.g. X/Open and its successor, The Open Group. It included about 1 million new lines of code, which provide an API shell, utilities, and an extended user interface. Works with a hierarchical file system provided by DFSMS (Data Facility System Managed Storage). The shell and utilities are based on Mortice Kerns' InterOpen products. Independent specialists reckon it was over 80% open systems-compliant—more than most Unix systems. DCE2 support announced February 1994, and many application development tools in March 1995. Mid 1995 IBM started to stop referring to OpenEdition as a separate entity, as all the open features became a standard part of vanilla MVS/ESA SP Version 5 Release 1. Under OS/390, it became UNIX System Services, and has kept that name under z/OS.
Closely related operating systems.
Japanese mainframe manufacturers Fujitsu and Hitachi both repeatedly and illegally obtained IBM's MVS source code and internal documentation in one of the 20th century's most famous cases of industrial espionage. Fujitsu relied heavily on IBM's code in its MSP mainframe operating system, and likewise Hitachi did the same for its VOS3 operating system. MSP and VOS3 were heavily marketed in Japan, where they still hold a substantial share of the mainframe installed base, but also to some degree in other countries, notably Australia. Even IBM's bugs and documentation misspellings were faithfully copied. IBM cooperated with the U.S. Federal Bureau of Investigation in a sting operation, reluctantly supplying Fujitsu and Hitachi with proprietary MVS and mainframe hardware technologies during the course of multi-year investigations culminating in the early 1980s—investigations which implicated senior company managers and even some Japanese government officials. Amdahl, however, was not involved in Fujitsu's theft of IBM's intellectual property. Any communications from Amdahl to Fujitsu were through "Amdahl Only Specifications" which were scrupulously cleansed of any IBM IP or any references to IBM's IP.
Subsequent to the investigations, IBM reached multi-million dollar settlements with both Fujitsu and Hitachi, collecting substantial fractions of both companies' profits for many years. Reliable reports indicate that the settlements exceeded US$500,000,000. The three companies have long since amicably agreed to many joint business ventures. For example, in 2002 IBM and Hitachi collaborated on developing the IBM z800 mainframe model.
Because of this historical copying, MSP and VOS3 are properly classified as "forks" of MVS, and many third party software vendors with MVS-compatible products were able to produce MSP- and VOS3-compatible versions with little or no modification.
When IBM introduced its 64-bit z/Architecture mainframes in the year 2000, IBM also introduced the 64-bit z/OS operating system, the direct successor to OS/390 and MVS. Fujitsu and Hitachi opted not to license IBM's z/Architecture for their quasi-MVS operating systems and hardware systems, and so MSP and VOS3, while still nominally supported by their vendors, maintain most of MVS's 1980s architectural limitations to the present day. Since z/OS still supports MVS-era applications and technologies—indeed, z/OS still contains most of MVS's code, albeit greatly enhanced and improved over decades of evolution—applications (and operational procedures) running on MSP and VOS3 can move to z/OS much more easily than to other operating systems.

</doc>
<doc id="19652" url="http://en.wikipedia.org/wiki?curid=19652" title="Monoid">
Monoid

In abstract algebra, a branch of mathematics, a monoid is an algebraic structure with a single associative binary operation and an identity element. Monoids are studied in semigroup theory as they are semigroups with identity. Monoids occur in several branches of mathematics; for instance, they can be regarded as categories with a single object. Thus, they capture the idea of function composition within a set. Monoids are also commonly used in computer science, both in its foundational aspects and in practical programming. The set of strings built from a given set of characters is a free monoid. The transition monoid and syntactic monoid are used in describing finite state machines, whereas trace monoids and history monoids provide a foundation for process calculi and concurrent computing. Some of the more important results in the study of monoids are the Krohn–Rhodes theorem and the star height problem. The history of monoids, as well as a discussion of additional general properties, are found in the article on semigroups.
Definition.
Suppose that "S" is a set and • is some binary operation "S" × "S" → "S", then "S" with • is a monoid if it satisfies the following two axioms:
In other words, a monoid is a semigroup with an identity element. It can also be thought of as a magma with associativity and identity. The identity element of a monoid is unique. A monoid in which each element has an inverse is a group.
Depending on the context, the symbol for the binary operation may be omitted, so that the operation is denoted by juxtaposition; for example, the monoid axioms may be written formula_1 and formula_2. This notation does not imply that it is numbers being multiplied.
Monoid structures.
Submonoids.
A submonoid of a monoid ("M", •) is a subset "N" of "M" that is closed under the monoid operation and contains the identity element "e" of "M". Symbolically, "N" is a submonoid of "M" if "N" ⊆ "M", "x" • "y" ∈ "N" whenever "x", "y" ∈ "N", and "e" ∈ "N". "N" is thus a monoid under the binary operation inherited from "M".
Generators.
A subset "S" of "M" is said to be a generator of "M" if "M" is the smallest set containing "S" that is closed under the monoid operation, or equivalently "M" is the result of applying the finitary closure operator to "S". If there is a generator of "M" that has finite cardinality, then "M" is said to be finitely generated. Not every set "S" will generate a monoid, as the generated structure may lack an identity element.
Commutative monoid.
A monoid whose operation is commutative is called a commutative monoid (or, less commonly, an abelian monoid). Commutative monoids are often written additively. Any commutative monoid is endowed with its algebraic preordering ≤, defined by "x" ≤ "y" if there exists "z" such that "x" + "z" = "y". An order-unit of a commutative monoid "M" is an element "u" of "M" such that for any element "x" of "M", there exists a positive integer "n" such that "x" ≤ "nu". This is often used in case "M" is the positive cone of a partially ordered abelian group "G", in which case we say that "u" is an order-unit of "G".
Partially commutative monoid.
A monoid for which the operation is commutative for some, but not all elements is a trace monoid; trace monoids commonly occur in the theory of concurrent computation.
Examples.
Moreover, "f" can be considered as a function on the points formula_7 given by
or, equivalently
Multiplication of elements in formula_3 is then given by function composition.
Note also that when formula_11 then the function "f" is a permutation of formula_7
and gives the unique cyclic group of order "n".
Properties.
In a monoid, one can define positive integer powers of an element "x" : "x"1 = "x", and "x"n = "x" • ... • "x" ("n" times) for "n" > 1 . The rule of powers "x""n" + "p" = "x""n" • "x""p" is obvious.
From the definition of a monoid, one can show that the identity element "e" is unique. Then, for any "x", one can set "x"0 = "e" and the rule of powers is still true with nonnegative exponents.
It is possible to define invertible elements: an element "x" is called invertible if there exists an element "y" such that "x" • "y" = "e" and "y" • "x" = "e". The element "y" is called the inverse of "x". If "y" and "z" are inverses of "x", then by associativity "y" = ("zx")"y" = "z"("xy") = "z". Thus inverses, if they exist, are unique.
If "y" is the inverse of "x", one can define negative powers of "x" by setting "x"−1 = "y" and "x"−"n" = "y" • ... • "y" ("n" times) for "n" > 1. And the rule of exponents is still verified for all "n", "p" rational integers. This is why the inverse of "x" is usually written "x"−1. The set of all invertible elements in a monoid "M", together with the operation •, forms a group. In that sense, every monoid contains a group (possibly only the trivial group consisting of only the identity).
However, not every monoid sits inside a group. For instance, it is perfectly possible to have a monoid in which two elements "a" and "b" exist such that "a" • "b" = "a" holds even though "b" is not the identity element. Such a monoid cannot be embedded in a group, because in the group we could multiply both sides with the inverse of "a" and would get that "b" = "e", which isn't true. A monoid ("M", •) has the cancellation property (or is cancellative) if for all "a", "b" and "c" in "M", "a" • "b" = "a" • "c" always implies "b" = "c" and "b" • "a" = "c" • "a" always implies "b" = "c". A commutative monoid with the cancellation property can always be embedded in a group via the Grothendieck construction. That is how the additive group of the integers (a group with operation +) is constructed from the additive monoid of natural numbers (a commutative monoid with operation + and cancellation property). However, a non-commutative cancellative monoid need not be embeddable in a group.
If a monoid has the cancellation property and is "finite", then it is in fact a group. Proof: Fix an element "x" in the monoid. Since the monoid is finite, "x""n" = "x""m" for some "m" > "n" > 0. But then, by cancellation we have that "x""m" − "n" = "e" where "e" is the identity. Therefore "x" • "x""m" − "n" − 1 = "e", so "x" has an inverse.
The right- and left-cancellative elements of a monoid each in turn form a submonoid (i.e. obviously include the identity and not so obviously are closed under the operation). This means that the cancellative elements of any commutative monoid can be extended to a group.
It turns out that requiring the cancellative property in a monoid is not required to perform the Grothendieck construction – commutativity is sufficient. However, if the original monoid has an absorbing element then its Grothendieck group is the trivial group. Hence the homomorphism is, in general, not injective.
An inverse monoid is a monoid where for every "a" in "M", there exists a unique "a"−1 in "M" such that "a" = "a" • "a"−1 • "a" and "a"−1 = "a"−1 • "a" • "a"−1. If an inverse monoid is cancellative, then it is a group.
In the opposite direction, a zerosumfree monoid is an additively written monoid in which "a" + "b" = 0 implies that "a" = 0 and "b" = 0: equivalently, that no element other than zero has an additive inverse.
Acts and operator monoids.
Let "M" be a monoid, with the binary operation denoted by • and the identity element denoted by "e". Then a (left) "M"-act (or left act over "M") is a set "X" together with an operation ⋅ : "M" × "X" → "X" which is compatible with the monoid structure as follows:
This is the analogue in monoid theory of a (left) group action. Right "M"-acts are defined in a similar way. A monoid with an act is also known as an operator monoid. Important examples include transition systems of semiautomata. A transformation semigroup can be made into an operator monoid by adjoining the identity transformation.
Monoid homomorphisms.
A homomorphism between two monoids ("M", ∗) and ("N", •) is a function "f" : "M" → "N" such that
where "e""M" and "e""N" are the identities on "M" and "N" respectively. Monoid homomorphisms are sometimes simply called monoid morphisms.
Not every semigroup homomorphism is a monoid homomorphism, since it may not map the identity to the identity of the target monoid, even though the element it maps the identity to will be an identity of the image of the mapping. In contrast, a semigroup homomorphisms between groups is always a group homomorphism, as it necessarily preserves the identity. Since for monoids this isn't always true, it is necessary to state this as a separate requirement.
A bijective monoid homomorphism is called a monoid isomorphism. Two monoids are said to be isomorphic if there is a monoid isomorphism between them.
Equational presentation.
Monoids may be given a presentation, much in the same way that groups can be specified by means of a group presentation. One does this by specifying a set of generators Σ, and a set of relations on the free monoid Σ∗. One does this by extending (finite) binary relations on Σ∗ to monoid congruences, and then constructing the quotient monoid, as above.
Given a binary relation "R" ⊂ Σ∗ × Σ∗, one defines its symmetric closure as "R" ∪ "R"−1. This can be extended to a symmetric relation "E" ⊂ Σ∗ × Σ∗ by defining "x" ~"E" "y" if and only if "x" = "sut" and "y" = "svt" for some strings "u", "v", "s", "t" ∈ Σ∗ with ("u","v") ∈ "R" ∪ "R"−1. Finally, one takes the reflexive and transitive closure of "E", which is then a monoid congruence.
In the typical situation, the relation "R" is simply given as a set of equations, so that formula_13. Thus, for example,
is the equational presentation for the bicyclic monoid, and
is the plactic monoid of degree 2 (it has infinite order). Elements of this plactic monoid may be written as formula_16 for integers "i", "j", "k", as the relations show that "ba" commutes with both "a" and "b".
Relation to category theory.
Monoids can be viewed as a special class of categories. Indeed, the axioms required of a monoid operation are exactly those required of morphism composition when restricted to the set of all morphisms whose source and target is a given object. That is,
More precisely, given a monoid ("M", •), one can construct a small category with only one object and whose morphisms are the elements of "M". The composition of morphisms is given by the monoid operation •.
Likewise, monoid homomorphisms are just functors between single object categories. So this construction gives an equivalence between the category of (small) monoids Mon and a full subcategory of the category of (small) categories Cat. Similarly, the category of groups is equivalent to another full subcategory of Cat.
In this sense, category theory can be thought of as an extension of the concept of a monoid. Many definitions and theorems about monoids can be generalised to small categories with more than one object. For example, a quotient of a category with one object is just a quotient monoid.
Monoids, just like other algebraic structures, also form their own category, Mon, whose objects are monoids and whose morphisms are monoid homomorphisms.
There is also a notion of monoid object which is an abstract definition of what is a monoid in a category. A monoid object in Set is just a monoid.
Monoids in computer science.
In computer science, many abstract data types can be endowed with a monoid structure. In a common pattern, a sequence of elements of a monoid is "folded" or "accumulated" to produce a final value. For instance, many iterative algorithms need to update some kind of "running total" at each iteration; this pattern may be elegantly expressed by a monoid operation. Alternatively, the associativity of monoid operations ensures that the operation can be parallelized by employing a prefix sum or similar algorithm, in order to utilize multiple cores or processors efficiently.
Given a sequence of values of type "M" with identity element formula_17 and associative operation formula_18, the "fold" operation is defined as follows:
In addition, any data structure can be 'folded' in a similar way, given a serialization of its elements. For instance, the result of "folding" a binary tree might differ depending on pre-order vs. post-order tree traversal.
Complete monoids.
A complete monoid is a commutative monoid equipped with an infinitary sum operation formula_20 for any index set "I" such that:
formula_21
and
formula_22
A continuous monoid is an ordered commutative monoid in which every directed set has a least upper bound compatible with the monoid operation:
formula_23
These two concepts are closely related: a continuous monoid is a complete monoid in which the infinitary sum may be defined as
where the supremum on the right runs over all finite subsets "E" of "I" and each sum on the right is a finite sum in the monoid.

</doc>
<doc id="19653" url="http://en.wikipedia.org/wiki?curid=19653" title="May 31">
May 31

May 31 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19654" url="http://en.wikipedia.org/wiki?curid=19654" title="May 30">
May 30

May 30 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19655" url="http://en.wikipedia.org/wiki?curid=19655" title="May 23">
May 23

May 23 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19659" url="http://en.wikipedia.org/wiki?curid=19659" title="May 16">
May 16

May 16 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19660" url="http://en.wikipedia.org/wiki?curid=19660" title="May 22">
May 22

May 22 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19662" url="http://en.wikipedia.org/wiki?curid=19662" title="Mean value theorem">
Mean value theorem

In mathematics, the mean value theorem states, roughly: that given a planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints. 
The theorem is used to prove global statements about a function on an interval starting from local hypotheses about derivatives at points of the interval.
More precisely, if a function "f" is continuous on the closed interval ["a", "b"], where "a" < "b", and differentiable on the open interval ("a", "b"), then there exists a point "c" in ("a", "b") such that
A special case of this theorem was first described by Parameshvara (1370–1460) from the Kerala school of astronomy and mathematics in his commentaries on Govindasvāmi and Bhaskara II. The mean value theorem in its modern form was later stated by Augustin Louis Cauchy (1789–1857). It is one of the most important results in differential calculus, as well as one of the most important theorems in mathematical analysis, and is useful in proving the fundamental theorem of calculus. The mean value theorem follows from the more specific statement of Rolle's theorem, and can be used to prove the more general statement of Taylor's theorem (with Lagrange form of the remainder term).
Formal statement.
Let "f" : ["a", "b"] → R be a continuous function on the closed interval ["a", "b"], and differentiable on the open interval ("a", "b"), where "a" < "b". Then there exists some "c" in ("a", "b") such that
The mean value theorem is a generalization of Rolle's theorem, which assumes "f"("a") = "f"("b"), so that the right-hand side above is zero.
The mean value theorem is still valid in a slightly more general setting. One only needs to assume that "f" : ["a", "b"] → R is continuous on ["a", "b"], and that for every "x" in ("a", "b") the limit 
exists as a finite number or equals +∞ or −∞. If finite, that limit equals "f′"("x"). An example where this version of the theorem applies is given by the real-valued cube root function mapping "x" to "x"1/3, whose derivative tends to infinity at the origin.
Note that the theorem, as stated, is false if a differentiable function is complex-valued instead of real-valued. For example, define for all real "x". Then
while "f′"("x") ≠ 0 for any real "x".
Proof.
The expression ("f"("b") − "f"("a")) / ("b" − "a") gives the slope of the line joining the points ("a", "f"("a")) and ("b", "f"("b")), which is a chord of the graph of "f", while "f "'("x") gives the slope of the tangent to the curve at the point ("x", "f"("x")). Thus the Mean value theorem says that given any chord of a smooth curve, we can find a point lying between the end-points of the chord such that the tangent at that point is parallel to the chord. The following proof illustrates this idea.
Define "g"("x") = "f"("x") − "rx", where "r" is a constant. Since "f" is continuous on ["a", "b"] and differentiable on ("a", "b"), the same is true for "g". We now want to choose "r" so that "g" satisfies the conditions of Rolle's theorem. Namely
By Rolle's theorem, since "g" is differentiable and "g"("a") = "g"("b"), there is some "c" in ("a", "b") for which "g′"("c") = 0, and it follows from the equality "g"("x") = "f"("x") − "rx" that,
as required.
A simple application.
Assume that "f" is a continuous, real-valued function, defined on an arbitrary interval "I" of the real line. If the derivative of "f" at every interior point of the interval "I" exists and is zero, then "f" is constant.
Proof: Assume the derivative of "f" at every interior point of the interval "I" exists and is zero. Let ("a", "b") be an arbitrary open interval in "I". By the mean value theorem, there exists a point "c" in ("a","b") such that
This implies that "f"("a") = "f"("b"). Thus, "f" is constant on the interior of "I" and thus is constant on "I" by continuity. (See below for a multivariable version of this result.)
Remarks: 
Cauchy's mean value theorem.
Cauchy's mean value theorem, also known as the extended mean value theorem, is a generalization of the mean value theorem. It states: If functions "f" and "g" are both continuous on the closed interval ["a","b"], and differentiable on the open interval ("a"," "b), then there exists some "c" ∈ ("a","b"), such that
Of course, if "g"("a") ≠ "g"("b") and if "g"′("c") ≠ 0, this is equivalent to:
Geometrically, this means that there is some tangent to the graph of the curve
which is parallel to the line defined by the points ("f"("a"),"g"("a")) and ("f"("b"),"g"("b")). However Cauchy's theorem does not claim the existence of such a tangent in all cases where ("f"("a"),"g"("a")) and ("f"("b"),"g"("b")) are distinct points, since it might be satisfied only for some value "c" with , in other words a value for which the mentioned curve is stationary; in such points no tangent to the curve is likely to be defined at all. An example of this situation is the curve given by
which on the interval [−1,1] goes from the point (−1,0) to (1,0), yet never has a horizontal tangent; however it has a stationary point (in fact a cusp) at .
Cauchy's mean value theorem can be used to prove l'Hôpital's rule. The mean value theorem is the special case of Cauchy's mean value theorem when .
Proof of Cauchy's mean value theorem.
The proof of Cauchy's mean value theorem is based on the same idea as the proof of the mean value theorem.
Generalization for determinants.
Assume that formula_13, formula_14, and formula_15 are differentiable functions on formula_16 that are continuous on formula_17. Define 
There exists formula_19 such that formula_20.
Notice that
and if we place formula_22, we get Cauchy's mean value theorem. If we place formula_22 and formula_24 we get Lagrange's mean value theorem.
The proof of the generalization is quite simple: each of formula_25 and formula_26 are determinants with two identical rows, hence formula_27. The Rolle's theorem implies that there exists formula_28 such that formula_29.
Mean value theorem in several variables.
The mean value theorem in one variable generalizes to several variables by applying the theorem in one variable via parametrization. Let "G" be an open connected subset of R"n", and let "f" : "G" → R be a differentiable function. Fix points "x", "y" ∈ "G" such that the interval "x" "y" lies in "G", and define . Since "g" is a differentiable function in one variable, the mean value theorem gives:
for some "c" between 0 and 1. But since and , computing "g′"("c") explicitly we have:
where ∇ denotes a gradient and · a dot product. Note that this is an exact analog of the theorem in one variable (in the case this "is" the theorem in one variable). By the Schwarz inequality, the equation gives the estimate:
In particular, when the partial derivatives of "f" are bounded, "f" is Lipschitz continuous (and therefore uniformly continuous). Note that "f" is not assumed to be continuously differentiable nor continuous on the closure of "G". However, in the above, we used the chain rule so the existence of ∇"f" would not be sufficient.
As an application of the above, we prove that "f" is constant if "G" is open and connected and every partial derivative of "f" is 0. Pick some point "x"0 ∈ "G", and let . We want to show for every "x" ∈ "G". For that, let . Then "E" is closed and nonempty. It is open too: for every "x" ∈ "E",
for every "y" in some neighborhood of "x". (Here, it is crucial that "x" and "y" are sufficiently close to each other.) Since "G" is connected, we conclude .
Remark that all arguments in the above are made in a coordinate-free manner; hence, they actually generalize to the case when "G" is a subset of a Banach space.
Mean value theorem for vector-valued functions.
There is no exact analog of the mean value theorem for vector-valued functions. Jean Dieudonné in his classic treatise "Foundations of Modern Analysis "discards the mean value theorem and replaces it by mean inequality as the proof is not constructive and by no way one can find the mean value. In applications one only needs mean inequality. Serge Lang in "Analysis I "uses the mean value theorem, in integral form, as an instant reflex but this use requires the continuity of the derivative. If one uses the Henstock-Kurzweil integral one can have the mean value theorem in integral form without the additional assumption that derivative should be continuous as every derivative is Henstock-Kurzweil integrable. The problem is roughly speaking the following: If "f" : "U" → R"m" is a differentiable function (where "U" ⊂ R"n" is open) and if "x" + "th", "x, h" ∈ R"n", "t" ∈ [0, 1] is the line segment in question (lying inside "U"), then one can apply the above parametrization procedure to each of the component functions "fi" ("i" = 1, ..., "m") of "f" (in the above notation set "y" = "x" + "h"). In doing so one finds points "x" + "tih" on the line segment satisfying
But generally there will not be a "single" point "x" + "t*h" on the line segment satisfying
for all "i" "simultaneously".
However a certain type of generalization of the mean value theorem to vector-valued functions is obtained as follows: Let "f" be a continuously differentiable real-valued function defined on an open interval "I", and let "x" as well as "x" + "h" be points of "I". The mean value theorem in one variable tells us that there exists some "t*" between 0 and 1 such that
On the other hand we have, by the fundamental theorem of calculus followed by a change of variables,
Thus, the value "f′"("x" + "t*h") at the particular point "t*" has been replaced by the mean value 
This last version can be generalized to vector valued functions:
Let "U" ⊂ R"n" be open, "f" : "U" → R"m" continuously differentiable, and "x" ∈ "U", "h" ∈ R"n" vectors such that the whole line segment "x" + "th", 0 ≤ "t" ≤ 1 remains in "U". Then we have:
where the integral of a matrix is to be understood componentwise. ("Df" denotes the Jacobian matrix of "f".)
From this one can further deduce that if ||"Df"("x" + "th")|| is bounded for "t" between 0 and 1 by some constant "M", then
Proof of (*). Write "fi" ("i" = 1, ..., "m") for the real valued components of "f". Define the functions "gi": [0, 1] → R by "gi"("t") := "fi"("x" + "th").
Then we have
The claim follows since "Df" is the matrix consisting of the components formula_44, q.e.d.
Proof of (**). From (*) it follows that 
Here we have used the following
Lemma. Let "v" : ["a", "b"] → R"m" be a continuous function defined on the interval ["a", "b"] ⊂ R. Then we have
formula_46
Proof of (***). Let "u" in R"m" denote the value of the integral 
Now
thus formula_49 as desired. (Note the use of the Cauchy–Schwarz inequality.) This shows (***) and thereby finishes the proof of (**).
Mean value theorems for integration.
First mean value theorem for integration.
The first mean value theorem for integration states 
In particular, if φ("t") = 1 for all "t" in ["a", "b"], then there exists "x" in ("a", "b") such that 
When presented in the equivalent form
the theorem's conclusion says that the "mean value" of "G"("t") on ["a", "b"]
(which is defined by the left side) is achieved as the "point value" "G"("x") 
for some "x" in ("a", "b").
Proof of the first mean value theorem for integration.
Without loss of generality assume the one-signed function formula_54 for all "t" (the negative case just changes direction of some inequalities).
It follows from the extreme value theorem that the continuous function "G" has a finite infimum "m" and a finite supremum "M" on the interval ["a", "b"]. From the monotonicity of the integral and the fact that "m" ≤ "G"("t") ≤ "M", it follows from the non-negativity of formula_55 that
where
denotes the integral of formula_55. Hence, if "I" = 0, then the claimed equality holds for every "x" in ["a", "b"]. Therefore, we may assume "I" > 0 in the following. Dividing through by "I" we have that 
The extreme value theorem tells us more than just that the infimum and supremum of "G" on ["a", "b"] are finite; it tells us that both are actually attained. Thus we can apply the intermediate value theorem, and conclude that the continuous function "G" attains every value of the interval ["m", "M"], in particular there exists "x" in ["a", "b"] such that
This completes the proof.
Second mean value theorem for integration.
There are various slightly different theorems called the second mean value theorem for integration. A commonly found version is as follows:
Here formula_62 stands for formula_63, the existence of which follows from the conditions. Note that it is essential that the interval ("a", "b"] contains "b". A variant not having this requirement is:
A probabilistic analogue of the mean value theorem.
Let "X" and "Y" be non-negative random variables such that E["X"] < E["Y"] < ∞ and formula_65 (i.e. "X" is smaller than "Y" in the usual stochastic order). Then there exists an absolutely continuous non-negative random variable "Z" having probability density function 
Let "g" be a measurable and differentiable function such that E["g"("X")], E["g"("Y")] < ∞, and let its derivative "g′" be measurable and Riemann-integrable on the interval ["x", "y"] for all "y" ≥ "x" ≥ 0. Then, E["g′"("Z")] is finite and 
Generalization in complex analysis.
As noted above, the theorem does not hold for differentiable complex-valued functions. Instead, a generalization of the theorem is stated such:
Let "f" : Ω → C be a holomorphic function on the open convex set Ω, and let "a" and "b" be distinct points in Ω. Then there exist points "u", "v" on "Lab" (the line segment from "a" to "b") such that
Where Re() is the Real part and Im() is the Imaginary part of a complex-valued function.

</doc>
<doc id="19664" url="http://en.wikipedia.org/wiki?curid=19664" title="Mallow">
Mallow

Mallow or Mallows may refer to:

</doc>
<doc id="19665" url="http://en.wikipedia.org/wiki?curid=19665" title="Marc Bloch">
Marc Bloch

Marc Léopold Benjamin Bloch (]; 6 July 1886 – 16 June 1944) was a French historian who cofounded the highly influential Annales School of French social history. Bloch was a quintessential modernist. An assimilated Alsatian Jew from an academic family in Paris, he was deeply affected in his youth by the Dreyfus Affair. He studied at the elite École Normale Supérieure; in 1908–9 he studied at Berlin and Leipzig. He fought in the trenches of the Western Front for four years. In 1919 he became Lecturer in Medieval history at Strasbourg University, after the German professors were all expelled; he was called to the University of Paris in 1936 as professor of economic history. He is best known for his pioneering studies "French Rural History" and "Feudal Society" and his posthumously-published unfinished meditation on the writing of history, "The Historian's Craft." A French soldier in both World Wars, he was captured and shot by the Gestapo during the German occupation of France for his work in the French Resistance.
Youth and First World War.
Born in Lyon to a Jewish family, the son of the professor of ancient history Gustave Bloch, Marc studied at the École Normale Supérieure and Fondation Thiers in Paris, then at Berlin and Leipzig. He was an officer of infantry in World War I, rising to the rank of captain and being awarded the Légion d'honneur.
After the war, he went to the university at Strasbourg, then in 1936 succeeded Henri Hauser as professor of economic history at the Sorbonne.
Career.
In 1924 he published one of his most famous works "Les rois thaumaturges: étude sur le caractère surnaturel attribué à la puissance royale particulièrement en France et en Angleterre" (translated in English as "The magic-working kings" or "The royal touch: sacred monarchy and scrofula in England and France") in which he collected, described and studied the documents pertaining to the ancient tradition that the kings of the Middle Ages were able to cure the disease of scrofula simply by touching people suffering from it. This tradition has its roots in the magical role of kings in ancient societies. This work by Bloch had a great impact not only on the social history of the Middle Ages but also on cultural anthropology.
Bloch's most important work centered on the study of feudalism. He published a large work, available in a two-volume English translation as "Feudal Society." In some ways, his most innovative work is his monograph "French Rural History."
Annales.
With colleague Lucien Febvre he founded the Annales School in 1929, by starting the new scholarly journal, "Annales d'Histoire Economique et Sociale" ("Annals of economic and social history"), which broke radically with traditional historiography by insisting on the importance of taking all levels of society into consideration and emphasized the collective nature of mentalities.
Bloch has had lasting influence in the field of historiography through his unfinished manuscript "The Historian's Craft", which he was working on at his death. Bloch's book is often considered one of the most important historiographical works of the 20th century.
Historiography.
Bloch was highly interdisciplinary, influenced by the geography of Paul Vidal de la Blache (1845–1918) and the sociology of Émile Durkheim (1858–1917). In "Méthodologie Historique" (written in 1906 but not published until 1988), Bloch rejected the histoire événementielle (event history) of his mentors Charles-Victor Langlois and Charles Seignobos to argue for greater analysis of the role of structural and social phenomena in determining the outcome of historical events. Bloch was trying to reinvent history as a social science, but he departed significantly from Durkheim in his refusal to exclude psychology from history; Bloch maintained that the individual actor should be considered along with social forces. Bloch's methodology was also greatly influenced by his father, Gustave Bloch, a historian of the ancient world, and by 19th-century scholars such as Gabriel Monod, Ernest Renan, and Numa Denis Fustel de Coulanges.
Bloch vigorously supported the idea of international scholarly cooperation and tried unsuccessfully to set up an international journal with American support. Bloch wrote some 500 reviews of German books and articles, While promoting the importance of German historiography and admiring its scholarly rigor, he repeatedly criticized its nationalism and methodological limitations.
Miracles and mentalities.
In "Les Rois Thaumaturges" (1924) Bloch looked at the long-standing folk belief that the king could cure scrofula by touch. The kings of France and England indeed regularly practised the ritual. Bloch was not concerned with the effectiveness of the royal touch—he acted like an anthropologist in asking why people believed it and how it shaped relations between king and commoner. The book was highly influential in introducing comparative studies (in this case France and England), as well as long-duration studies spanning a thousand years (with specific events used as illustrations). By investigating the impact of rituals, the efficacy of myths, and all the possible sources of collective behavior, he became the "father of historical anthropology." Bloch's revolutionary charting of mentalities resonated with scholars who were reading Freud and Proust. Stirling (2007) examines this essentially stylistic trait alongside Bloch's peculiarly quixotic idealism, which tempered and sometimes compromised his work through his hope for a truly cooperative model of historical inquiry. While humanizing and questioning him, Stirling gives credit to Bloch for helping to break through the monotonous methodological alternance between positivism and narrative history, creating a new, synthetic version of the historical practice that has since become so ingrained in the discipline that it is typically overlooked.
Rural history.
Bloch's own ideas on rural history were best expressed in his masterworks, "French Rural History" ("Les caractères originaux de l'histoire rurale française," 1931) and "Feudal Society" (1939).
In "L'Individualisme Agraire du XVIIIe Siècle" (1978), Bloch characterized the agrarian reforms of 18th-century France as a "failed revolution," citing the persistence of regional traditions as evidence for their failure. A typical example of the Annales School's "total history," Bloch's argument weaves the connections between politics, culture, and economics against a backdrop of class conflict to illustrate how "the conscious actions of men have overcome the rhythms of the materialist causality of history." He argued that the anti-feudal sentiment of French peasants expressed in the 1789 cahier de doléances (list of grievances) was linked to the "seigneurial reaction" of the late 18th century in which lords significantly increased feudal dues. Bloch argued that it was this intensified exploitation that provoked peasant revolt, leading to the Revolution.
History of technology.
The November 1935 issue of the "Annales" contains Febvre's introduction that defines three essential approaches to a history of technology: to investigate technology, to understand the progress of technology, and to understand the relationship of technology to other human activities. Bloch's article, "The Advent and Triumph of the Watermill in Medieval Europe," incorporates these approaches by investigating the connections between technology and broader social issues.
Second World War.
In 1939 France declared war on Germany after its invasion and occupation of Poland. As France mobilized its troops, Marc Bloch left his position at the Sorbonne and took up his reserve status as a captain in the French Army at the age of 52. He was encouraged at the time by colleagues both in France and abroad to leave the country. He said it was his personal obligation to stand for the moral imperative.
His memoir of the first days of World War II, "Strange Defeat," written in 1940 but not published until 1946, blamed the French military establishment, along with her social and political culture, for the sudden total military defeat and helped after the war to neutralize the traumatic memory of France's failure and to build a new French identity.
Bloch joined the French Resistance in late 1942, driven by ardent patriotism, identification with his Jewish roots and a conception of France as the champion of liberty. His code name was "Narbonne". He was eventually captured in Lyon by Vichy police in March 1944 and turned over to the Gestapo. He was then imprisoned in Montluc prison, and was tortured by the Gestapo at their headquarters. He was interrogated by Klaus Barbie who was in charge of interrogations at the prison; under such treatment Bloch apparently remained "calm and stoic" throughout, according to his biographer Carole Fink, reportedly providing no further information to his captors than his real name.
Execution.
At around 8pm on the night of 16 June 1944, ten days after D-Day, Marc Bloch was among twenty-eight Resistance prisoners taken by German troops in a camionette (an open truck) along the River Saone to a place called La Rousille just before the village of Saint-Didier-de-Formans in the Ain department. Here in a meadow surrounded by high bushes shortly after 10pm, Bloch was executed by firing squad, one of the first group of four of the twenty-eight, handcuffed in pairs, to face the machine guns, and one of twenty-six men to be murdered that night in a period of twenty minutes. The victims were stripped of all means of identification and left by the Germans in the field. The following day they were found by Marcel Pouveret, a schoolmaster, who informed the mayor of St Didier, to whom he was assistant, and the mayor called in the gendarmerie of Trevoux to bury the bodies. There is today a memorial to those killed in the meadow near where they were shot.
Marc Bloch, one of the greatest historians of the twentieth century and a hero of the Resistance, was murdered less than a month before his fifty-eighth birthday.
As Bloch had spent his final days in prison, he left unfinished one of his most intimate works and a classic of historiography, "The Historian's Craft" (" Apologie pour l'histoire ou Métier d'historien"), which was edited and published posthumously, by which time Marc Bloch had become a national martyr following the Allied liberation.

</doc>
<doc id="19667" url="http://en.wikipedia.org/wiki?curid=19667" title="Michael Ventris">
Michael Ventris

Michael George Francis Ventris, OBE (; 12 July 1922 – 6 September 1956) was an English linguist and architect who, along with John Chadwick and Alice Kober, deciphered Linear B, a previously unknown ancient script discovered at Knossos by Arthur Evans. A prodigy in languages, Ventris had pursued the decipherment as an avocation since his teen-age years. After creating a new field of study, Ventris died in an automobile accident a few weeks before publication of his first definitive work, "Documents in Mycenaean Greek".
Biography.
Early life.
Ventris was born into a traditional army family then coming to an end. His father, Edward Francis Vereker Ventris, reached the rank of Lieutenant Colonel in the Indian Army; he might have gone further had he not contracted tuberculosis and retired. His grandfather, Francis Ventris, was a Major-General who ended his career as Commander of British Forces in China. Both men served in the Middle and Far East, the younger especially in India. During one of his stays in England, Michael's future father married Anna Dorothea Janasz (Dora), the daughter of a wealthy immigrant landholder from Poland. Her photographs reveal a slender, dark-haired beauty. They had one child, Michael.
Health was an important family consideration right from the beginning of Michael's life. He had chronic bronchial asthma. The family resided mainly in Switzerland for eight years, which they could well afford to do. Switzerland had a reputation for being especially healthy. A number of health centers, or spas, catered to the physical well-being of those who could afford to attend. Ventris started school in Gstaad, where classes were taught only in French and German. He was soon reasonably fluent in both languages, learning also the dialect of German spoken in Switzerland. He had the facility of learning a language within a matter of weeks, which led ultimately to his acquisition of roughly a dozen languages. His mother must have spoken Polish, as he learned that as well, all before the age of eight. At that age he was reading Adolf Erman's "Die Hieroglyphen" in German.
In 1931 the Ventrises came home. The senior Ventris's physical condition was visibly worsening year by year. From 1931 to 1935 Michael attended Bickley Hill School in Stowe. His parents, unable to live together since 1932, divorced in 1935, when he was 13. Then he won a scholarship to Stowe School, quartered in an 18th-century stately home. At Stowe he learned some Latin and classical Greek. He did not do outstanding work there. By then he was spending most of his spare time learning as much as he could about Linear B, some of his study time being spent under the covers at night with a flashlight. When he was not away at school, Michael lived with his mother, before 1935 in coastal hotels, after 1935 (when they were built) in the avant garde Berthold Lubetkin's Highpoint modernist apartments in Highgate. His mother's acquaintances, who frequented the house, included many sculptors, painters and writers of the day. The money for her sophisticated life style came from the Polish estates.
Young adult.
Michael's father died in 1938 when Michael was 16 years old. Dora became administrator of the estate. Hard times, however, lay ahead. After the German invasion of Poland in 1939 the family holdings in that country were gone, and all income from there ceased. In 1940 Dora's father died. The family became destitute. Michael lost his mother to clinical depression and an overdose of barbiturates. He never spoke of her, assuming instead an ebullient and energetic manner in whatever he decided to do, a trait which won him numerous friends. At the same time they noted that he had a dark and mysterious side as well, associated with feelings that he was a fraud, and not a true genius. A friend of the family, Russian sculptor Naum Gabo, took Michael under his wing. Michael later said that Gabo was the most family he had ever had. It may have been at Gabo's house that he began the study of Russian. He had resolved on architecture for a career. He enrolled at the Architectural Association School of Architecture. There he met and married Lois, who preferred to be called Betty. Her social background was similar to what Ventris's had been: her family was well-to-do, she had travelled in Europe, and she was interested in architecture, in addition to which she was popular and was considered very beautiful.
He did not complete his architecture studies, being conscripted in 1942. He chose the Royal Air Force (RAF). His preference was for navigator rather than pilot, and he completed the extensive training in the UK and Canada, to qualify early in 1944 and be commissioned. While training, he studied Russian intensively for several weeks, the purpose of which, if any, is not clear. He took part in the bombing of Germany, as aircrew on the Handley Page Halifax with No. 76 Squadron RAF, initially at RAF Breighton and then at RAF Holme-on-Spalding Moor. After the conclusion of the war he served out the rest of his term on the ground in Germany, for which he was chosen because of his knowledge of Russian. His duties are unclear. His friends all assumed he was completing intelligence assignments, interpreting his denials as part of a legal gag. No such assignments have turned up, however, even after these many decades since his service. There is also no evidence that he was ever part of any code-breaking unit, as was Chadwick, even though the public readily believed this explanation of his genius and success with Linear B.
Architect and palaeographer.
After the war he worked briefly in Sweden, learning enough Swedish to communicate with scholars in it. Then he came home to complete his architectural education with honors in 1948 and settled down with Lois working as an architect. He designed schools for the Ministry of Education. Then he and his wife designed a home for themselves and their family. He had two children, a son, Nikki (1942–1984) and a daughter, Tessa (1946–). Concurrently he stepped up his effort on Linear B, discovering finally that it was Greek, a revelation to an academic public that had more or less given up on the mysterious script. No one, not even Ventris, suspected that it is the earliest known form of Greek. Ventris was awarded an OBE in 1955 for "services to Mycenaean paleography." A few years after deciphering Linear B in 1951–1953, Ventris, who lived in Hampstead, died instantly in a late-night collision with a parked truck while driving home, aged 34.
An English Heritage blue plaque commemorates Ventris at his home in Hampstead.
Decipherment.
At the beginning of the 20th century, archaeologist Arthur Evans began excavating Knōssos, an ancient city on the island of Crete. In doing so he uncovered a great many clay tablets inscribed with an unknown script. Some were older and were named Linear A. The bulk were of more recent vintage, and were dubbed Linear B. Evans spent the next several decades trying to decipher both, to no avail.
In 1936, Evans hosted an exhibition of Cretan archaeology at Burlington House in London, home of the Royal Academy. It was the jubilee anniversary (50 years) of the British School of Archaeology in Athens, contemporaneous owners and managers of the Knossos site. Evans had given the site to them some years previously. Villa Ariadne, Evans's home there, was now part of the school. Boys from Stowe school were in attendance at one lecture and tour conducted by Evans himself at age 85. Ventris, 14 years old, was present and remembered Evans walking with a stick. The stick was undoubtedly the cane named Prodger which Evans carried all his life to assist him with his short-sightedness and night blindness. Evans held up tablets of the unknown scripts for the audience to see. During the interview period following the lecture, Ventris immediately confirmed that Linear B was as yet undeciphered, and determined to decipher it.
Ventris's initial theory was that Etruscan and Linear B were related and that this might provide a key to decipherment. Although this proved incorrect, it was a link he continued to explore until the early 1950s.
Shortly after Evans died, Alice Kober noted that certain words in Linear B inscriptions had changing word endings — perhaps declensions in the manner of Latin or Greek. Using this clue, Ventris constructed a series of grids associating the symbols on the tablets with consonants and vowels. While "which" consonants and vowels these were remained mysterious, Ventris learned enough about the structure of the underlying language to begin guessing.
Some Linear B tablets had been discovered on the Greek mainland, and there was reason to believe that some of the chains of symbols he had encountered on the Cretan tablets were names. Noting that certain names appeared only in the Cretan texts, Ventris made the inspired guess that those names applied to cities on the island. This proved to be correct. Armed with the symbols he could decipher from this, Ventris soon unlocked much text and determined that the underlying language of Linear B was in fact Greek. This overturned Evans's theories of Minoan history by establishing that Cretan civilization, at least in the later periods associated with the Linear B tablets, had been part of Mycenean Greece.

</doc>
<doc id="19668" url="http://en.wikipedia.org/wiki?curid=19668" title="Maniac Mansion">
Maniac Mansion

Maniac Mansion is a 1987 graphic adventure game developed and published by Lucasfilm Games. Initially released for the Commodore 64 and Apple II, it was Lucasfilm's foray into video game publishing. The game follows teenager Dave Miller as he ventures into a mansion and attempts to rescue his girlfriend from an evil mad scientist, whose family has been controlled by a sentient meteor that crashed near the mansion 20 years earlier. The player uses a point-and-click interface to guide Dave and two of his friends through the mansion while avoiding its dangerous inhabitants and solving puzzles.
The game was conceived in 1985 by Ron Gilbert and Gary Winnick. They based the story on horror film and B movie clichés with humorous elements, and they based the game's characters on people they knew and characters from movies, comics, and horror magazines. The developers based the mansion's design on the Main House at Skywalker Ranch, outlining the map and pathways prior to programming. The interface came from the designers' desire to improve on contemporary text parser-based graphical adventure games seen in earlier adventure titles. To reduce the effort required for creating the game, Gilbert implemented a game engine called SCUMM, which would be re-used for many other LucasArts titles. The game was ported to several other platforms; the Nintendo Entertainment System (NES) version had to be considerably modified to follow Nintendo of America's content policies, which barred material deemed inappropriate for children.
Regarded as a seminal adventure title, "Maniac Mansion" was critically acclaimed; reviewers lauded its graphics, cutscenes, animation, and humor. Reviewers and other developers have considered its point-and-click interface revolutionary; the system has led competitors to adopt similar interfaces. The game influenced numerous other titles, has been placed in several "hall of fame" lists, and has led fans to create remakes with enhanced visuals. A TV series, written by Eugene Levy and starring Joe Flaherty, was created in 1990 and lasted for three seasons, filming 66 episodes. Lucasfilm Games released the sequel "Day of the Tentacle" in 1993, which also received critical acclaim.
Overview.
"Maniac Mansion" takes place in the mansion of the Edison family: Dr. Fred, Nurse Edna, and their son Weird Ed. Living with the Edisons are two large, disembodied tentacles – one purple and the other green. The intro sequence shows that a meteor crashed near the mansion twenty years earlier. The sentient meteor took control of the family and caused Dr. Fred to start sucking out human brains for use in experiments; his family supported and encouraged him in these efforts. One day, main protagonist Dave Miller's girlfriend, cheerleader Sandy Pantz, disappears without a trace, and he suspects that Dr. Fred has kidnapped her. After the game's introduction, Dave and his two companions prepare to enter the mansion to rescue Sandy; the game starts with a prompt for the player to select two of six characters to accompany Dave.
"Maniac Mansion" is a graphic adventure game in which the player uses a point-and-click interface to guide characters through a two-dimensional (2D) game world and to solve puzzles. Players can select from fifteen different commands with this scheme; examples include "walk to", to move the characters; "new kid", to switch between the three characters; and "pick up", to collect objects. Each character possesses unique abilities; for example, Syd and Razor can play musical instruments, while Bernard can repair appliances. The game may be completed with any character combination, but because many puzzles can be solved only with specific skills, the game can be finished in different ways, depending on the characters the player has chosen.
The gameplay is regularly interrupted by cutscenes, a term Ron Gilbert coined, that advance the story and inform the player about non-player characters' actions. Aside from the green tentacle, the mansion's inhabitants pose a threat and will throw the player characters into the dungeon—or in some situations kill them—if they see them. If one character dies, the player must choose a replacement from the unselected characters; the game ends if all the characters die. "Maniac Mansion" has five possible successful endings that depend on which characters the player uses, which ones survive, and what events occur.
Development.
Conception.
"Maniac Mansion" was first conceived in 1985, when Lucasfilm Games assigned employees Ron Gilbert and Gary Winnick the task of creating an original game. Noah Falstein had recently hired Gilbert at Lucasfilm Games on a three-month contract to program the game "Koronis Rift". At the same time, Winnick was working on "", and it was then in which both Gilbert and Winnick found that they shared similar tastes in humor, movies, and television programs. Eventually, Gilbert was hired full-time. As with earlier Lucasfilm titles, the company's management provided little oversight in the development process, to which Gilbert credited the success of many of its earlier games.
Gilbert and Winnick were co-writers and lead designers of "Maniac Mansion", but they worked separately (Gilbert on programming and Winnick on the art). Together, they brainstormed story ideas and, based on their love of B horror films, decided to create a comedy–horror title set in a haunted house. They drew inspiration from what Winnick called "a ridiculous teen horror movie", which the teens were in a house and were slaughtered one by one and not once thinking about leaving. The pair compared this film to clichés in other popular horror films such as "Friday the 13th" and "A Nightmare on Elm Street", and used them to come up with the game's setting. Early development involved experimentation and was organic; according to Gilbert: "Very little was written down. Gary and I just talked and laughed a lot, and out it came." After development had begun, Lucasfilm Games relocated its office to the Stable House at Skywalker Ranch. The ranch's Main House inspired Winnick's design of the game's mansion and him to create the game's concept art. He recreated several rooms in the Main House for the game, such as a library with a spiral staircase and a media room with a large-screen TV and grand piano. The various rooms at the ranch inspired the design of other rooms in the mansion.
The pair prioritized the story and characters, and wanted to maintain a balance between a "sense of peril and sense of humor". The first character concepts were a set of siblings and their friends, which gradually evolved into the final characters. Gilbert and Winnick based the characters on stereotypes and people they knew. For example, Winnick's girlfriend Ray inspired Razor, and while Gilbert's mother apparently served as the basis for Nurse Edna – Gilbert has denied the connection. Dave and Wendy were based on Gilbert and a fellow employee named Wendy, respectively. According to Winnick, the Edison family were based on various movie characters and elements from EC Comics and Warren Publishing magazines. They sought to give each playable character unique abilities. However, they had to exclude several characters due to size limitations. To parody the horror genre, the developers inserted many film clichés into the story. For instance, the sentient meteor that takes control of Dr. Fred was inspired from a segment of the 1982 anthology film "Creepshow" titled "The Lonesome Death of Jordy Verrill". The designers also included a man-eating plant similar to the villain of the 1986 film "Little Shop of Horrors".
The pair struggled to choose a gameplay genre; Gilbert described their early ideas as "disconnected". While visiting relatives for Christmas, Gilbert saw his cousin playing "". Gilbert was an adventure games fan and decided that the ideas he and Winnick had conceived would work well with the genre. His first exposure to a text adventure with graphics, Gilbert spent the holiday playing the game to familiarize himself with the format.
Gilbert and Winnick created "Maniac Mansion"‍ '​s basic structure and story prior to programming; its earliest version was a simple paper-and-pencil board game, which the mansion's floor plan served as the game board, and cards represented events and characters. Lines connected the rooms to illustrate pathways characters could travel. The designers used layers of cellulose acetate to map out the game's puzzles by tracking which items worked together when used by certain characters. Impressed with the map's complexity, Winnick included it in the game as a poster in one of the mansion's rooms. Because each character contributed different skills and resources, the pair spent months working on the event combinations that could occur. This extended the game's production time beyond that of Lucasfilm Games' previous titles, which almost led to Gilbert's firing. Though they had outlined the game's events, the dialogue was not written until after programming had started; David Fox provided the dialog. Alternate endings were uncommon at the time, and "Maniac Mansion" was one of the first games to feature them. 
Commodore 64 limitations.
Development focused on the Commodore 64 home computer, so a concern was to make the game small enough to fit into its 64 KB memory. Scrolling was used to show objects and characters in rooms during cutscenes. The designers used this technique to force players to explore the mansion's larger rooms by hiding elements off-screen. However, the Commodore's bitmap mode was unsuitable because it needed a large amount of memory (8k) and did not permit scrolling (only character screens can be scrolled on the VIC-II chip fast enough). Thus the game had to use character graphics which were scrollable and only took 1k, but couldn't have the same level of graphical detail as bitmaps. The set uses 8 × 8 pixel tiles. Since the VIC-II could only have 256 tiles (two 128-character sets which may be switched), it limited the level of detail Winnick could design into the graphics. To circumvent this, Gilbert created a program to generate the tiles from Winnick's pixel art. To comply with the tile limit, the program compared similar tiles and created approximations that could replace multiple tiles. Winnick inspected the results for visual errors and then repeated the process until the number of tiles was sufficiently reduced. To make the characters easily recognizable, Winnick made the heads relatively large. Each character consisted of three multicolor sprites stacked on each other. Because the Commodore 64 restricted sprites to 24 pixels horizontally (without doubling in half the quality), the characters' animations never extend outside this width.
SCUMM: game engine and scripting language.
Gilbert started programming the game in assembly language for the 6502 microprocessor which was the norm for Commodore 64 games. However, he quickly realized that the game was too large and complex to easily be written in assembly and that a high-level scripting engine similar to Sierra's AGI system would have to be developed. Gilbert initially considered basing the language on LISP but ultimately chose a syntax that more closely resembles that of the C programming language. He discussed the problem with fellow Lucasfilm employee Chip Morningstar, who helped him build a foundation for the game engine, which Gilbert then extended. In designing the engine and language, Gilbert developed a "system that could be used on many adventure games, cutting down the time it took to make them". He logged considerable overtime with the goal of creating an adventure game superior to those of Lucasfilm's competitors. Gilbert designed the engine to allow for multitasking, allowing designers to isolate and manipulate specific game objects independently. Most of the first six to nine months of "Maniac Mansion"‍ '​s development involved building the engine.
All adventure games of the time required typing, and this is understandable given that most of them were text based. A few games, most notably the Sierra ones, had graphics but they still required typing. I never understood this and felt that it was only taking it halfway.
Ron Gilbert on the then-common input method in adventure games
A primary development goal was to create a control system that not only retained the structure of classic text adventures, but also dispensed with the typing. The two lead designers were frustrated with the text parsers and the inevitable player character deaths that were prominent in the genre. While in college, Gilbert had enjoyed "Colossal Cave Adventure" and Infocom's games but had "really wanted to see graphics". He felt that the visual element Sierra Entertainment added for its games was "a big improvement", but he disliked the games' use of text parsers. While playing "King's Quest", Gilbert found guessing what terms the designer had programmed it to recognize aggravating because he could see the object he wanted to interact with on the screen, but he had to figure out the correct commands. Gilbert reasoned that if he could view the graphic, then he should be able to click on it with a cursor; by extension, the player should also be able to click on verb commands. Gilbert devised a new and simpler interface "because I'm lazy and don't like to type. I hated playing adventure games where I had to type everything in, and I hated playing the 'second guess the parser' game so I figure everything should be point-and-click."
The team originally envisioned 40 verb commands, but it whittled the number down to the 12 it felt were essential. The commands were then integrated into the scripting language in a similar fashion Sierra did with its Adventure Game Interpreter and Sierra's Creative Interpreter. Gilbert believed that a complex game did not need a text parser, but rather an innovative use of the interactions between in-game objects. He showed the team a demonstration of Sierra games and then led a discussion about their user interface and gameplay issues. Gilbert finished the engine – which he later named "Script Creation Utility for Maniac Mansion" (SCUMM) – after around a year of development. It freed the developers from having to code the details in low-level language. Though the game had been designed with the Commodore 64 in mind, the SCUMM engine enabled easy porting of "Maniac Mansion" to other platforms. Lucasfilm developers Aric Wilmunder and Brad Taylor would assist in the PC port of the script.
Scripting and testing.
At Gilbert's request, David Fox, who had previously worked on "Labyrinth: The Computer Game", assisted with "Maniac Mansion"‍ '​s scripting. Fox was between projects and planned to do a month's work on the game; however, he stayed on the project for roughly six months. He discussed the game's events with Gilbert and Winnick, and used that information to create the rooms with the script. The developers added designated areas or "walk-boxes" that characters could traverse in the game world. Gilbert and Fox wrote the characters' dialog and choreographed the action. Fox expanded the game based on ideas he conceived while viewing Winnick's concept art, such as allowing players to place a hamster in the kitchen microwave.
Gilbert wanted players to enjoy "Maniac Mansion" and not be punished for applying real world logic. In the Sierra game Space Quest II, the player can get killed by merely picking up broken glass and bleed to death with no prior warning. Fox asserted that "I know that in the real world I can successfully pick up a broken piece of mirror without dying" and characterized such game design as "sadistic". The team wanted to avoid illogical "surprise deaths" to spare players from having to regularly reload the game from a previous save state. As a result, the group created a number of possibilities to give the player more freedom. While there are several ways players can get the characters killed in Maniac Mansion, they're almost impossible to do except intentionally. While scripting the game, however, the designers realized that the number of characters resulted in a very complex game with a number of flaws, particularly dead ends that prevented the player from completing the game. To address these issues, they often revised the puzzles. In retrospect, Gilbert acknowledged that the fact that Lucasfilm Games had only one tester allowed many errors to go undetected. Gilbert's uncle also helped as an outside play-tester. Each week, Gilbert would mail him a floppy disk of the game's latest build.
The PC, Amiga, and Atari ST versions of the game came with a booklet called "Nuke'm Alarms" – named after the security system that protects the mansion – that served as copy protection for the corresponding piece of software. The booklet contained a list of codes that had to be entered to disarm the mansion's nuclear security device and open the door at the top of the foyer. The codes (which were Commodore graphics characters) were printed on a special paper so that they could not be easily photocopied; players had to use a special cellophane lens to read the codes. They were given three opportunities to enter the codes correctly, after which the mansion would explode, ending the game. Moreover, if the player tries to use Bernard to disarm the security device, the mansion automatically explodes. Although the designers had intended for the copy protection codes to be in the original Commodore and Apple versions, they had to be left out because of insufficient disk space and instead an on-disk protection was used. When the game was ported to the NES, the copy protection device and script was inadvertently left in the game, hidden behind a statue. According to developer David Warhol, players could interact with the now-invisible device and subsequently cause a "game over".
Release.
In contrast to its previous games, where Lucasfilm Games had been only the developer and had used external publishers, the company started taking on the role of publisher with "Maniac Mansion". Lucasfilm Games hired Ken Macklin, whom Winnick knew, to design the packaging's artwork. Gilbert and Winnick collaborated with the marketing department to design the back cover. The two also created an insert that includes hints, a backstory, and jokes.
After around 18 to 24 months of development, the game debuted at the 1987 Consumer Electronics Show in Chicago. The game was initially released for the Commodore 64 and Apple II in October 1987. After a Toys "R" Us customer complained about the word "lust" on the back cover, the store pulled the game from its shelves until Lucasfilm Games altered the box. In March 1988, the PC port was released which used slightly more detailed title screen graphics. Ports for the Amiga, Atari ST, and NES followed.
Nintendo Entertainment System version.
Published by Jaleco in September 1990, "Maniac Mansion" was Lucasfilm Games' first NES release. The developer was unable to properly focus on the project owing to a large workload; therefore, Douglas Crockford volunteered to manage it. The studio used a modified version of the SCUMM engine titled "NES SCUMM" for the port. Crockford noted that "one of the main differences between the NES and PCs is that the NES can do certain things much faster". Developer Tim Schafer, who would go on to develop other Lucasfilm games such as "Maniac Mansion"‍ '​s sequel "Day of the Tentacle", play-tested the port; this was Schafer's first professional credit. The studio had to completely redraw the game's graphics to conform with the NES's display resolution requirements.
During its initial development for home computers, Lucasfilm Games censored profanity in the game; for instance, the company forced the developers to change Dave Miller's opening line of "Don't be a shit head" to "Don't be a tuna head". However, for the NES version, the designers had to remove further content so that it was suitable for younger audiences and according to Nintendo's policies. Jaleco USA president Howie Rubin advised Crockford about what content Nintendo might object, such as any usage of the word "kill" in the game. However, Crockford's interpretation of the NES Game Standards Policy led him to believe that other elements might also conflict with it, so he sent a list of questionable content to Jaleco. Its staff believed that the content was reasonable, and Lucasfilm Games submitted "Maniac Mansion" to Nintendo.
A month after submitting the game, Nintendo of America sent Lucasfilm Games a report that outlined offensive on-screen text and nude graphics that it wanted removed. Crockford further modified the content to comply, while trying to maintain the game's essential aspects. For example, Nintendo wanted the developers to remove graffiti in a room that provided players with hints on how to activate a story event; unable to remove it without also removing the hints, the designers shortened the message. Nintendo listed objectionable dialog lines that needed to be changed, including many of Nurse Edna's sexually suggestive lines. They also removed the line from one of Dr. Fred's cutscene in which he said "getting your pretty brains sucked out"; not saying right away what part of the line was offensive, they clarified, saying that "sucked" was deemed too graphic. Crockford changed the word to "removed" and also removed a poster that said "disco sucks" from the green tentacle's bedroom to be consistent with their wishes. The nudity Nintendo outlined encompassed a poster of a mummy in a playmate pose, a swimsuit calendar, and a classical statue of reclining woman. The studio removed the poster and calendar, but they fought to keep the statue, claiming that it was modeled after a Michelangelo sculpture. The censors suggested an alteration, but Lucasfilm Games ultimately removed the object. Nintendo of America also objected to the phrase "NES SCUMM" in the end credits, which Crockford removed but not without questioning why the censors had overlooked the more offensive content. In retrospect, Crockford felt that such standards resulted in "bland" products and called Nintendo a "jealous god".
After implementing the changes, Lucasfilm Games re-submitted "Maniac Mansion" to Nintendo, which then manufactured 250,000 cartridges. The NES cartridge features a battery back-up to save data, and a prototype NES cartridge with the original content is rumored to exist. In early 1990, Nintendo announced the port in its official magazine and provided further coverage later in the year. The ability to microwave a hamster remained in the game, which Crockford cited as an example of the censors' confusing criteria. However, Nintendo later noticed it and demanded its removal. Originally, there was only one printing of the NTSC version of the game, meaning that only PAL-region copies were affected. A second run of the game for NTSC regions then removed this content. After the first batch of cartridges was sold, Nintendo made Jaleco remove the content in future releases. The Japanese release omitted some graphical and musical elements, featured flip-screen scrolling, and had alterations to the characters' appearances. "Maniac Mansion" was one of four games in the NES library – along with "Shadowgate", "F-15 Strike Eagle", and "Déjà Vu" – to be translated into Swedish.
The port's music was handled by Realtime Associates. Late in development, Jaleco asked the company to provide background music, noting that port lacked it and despite the lack of background music in all previous ports. Realtime Associates' founder and president David Warhol noted that "video games at that time had to have 'wall to wall' music". Warhol went to George "The Fat Man" Sanger and his band "Team Fat", and David Hayes to write the background music, while Warhol worked on translating them to NES chiptune music. The musicians composed music to best suit the characters, such as a punk rock theme for Razor, an electronic rock theme for Bernard, a surfer-based theme for Jeff, and a Hayes-inspired version of Thin Lizzy's "The Boys Are Back in Town" for the main protagonist Dave.
Reception.
"Maniac Mansion" was well received by critics, and several reviewers likened the game to films. "Commodore User"‍ '​s Bill Scolding and "Zzap!64"‍ '​s three reviewers – Paul Summer, Julian Rignall, and Steve Jarratt – compared it to "The Rocky Horror Picture Show". Other comparisons were drawn to "Psycho", "Friday the 13th", "The Texas Chain Saw Massacre", "The Addams Family", and "Scooby-Doo". "Compute!'s Gazette"‍ '​s Keith Farrell cited "Maniac Mansion"‍ '​s similarity to films, particularly with its use of cutscenes to add "information or urgency". He lauded the game's high level of detail along with its graphics and animation, writing, "Each of the teenagers is fully realized, with features and wardrobe that are wholly in character." In later issues, editor Orson Scott Card praised the game's humor, cinematic storytelling, and lack of violence. He called it "a compellingly good game" and evidence that Lucasfilm was helping "to make computer games a valid storytelling art", and Shay Addams wrote in the magazine that "the interface, graphics, and warped comedy make "Maniac Mansion" a must-explore for fans of both horror and humor". "Commodore Magazine"‍ '​s Russ Ceccola praised its cutscenes as creative and high-quality. He called the ending "unforgettable" and praised the game's audio-visuals; Ceccola noted that the "characters are distinctively Lucasfilm's, bringing facial expressions and personality to each individual character". He ended by recommending readers to buy "Maniac Mansion", as it would please fans of the genre.
"Zzap!64"‍ '​s reviewers praised the game's humor and called its point-and-click control "tremendous"; they concluded by describing the game as "innovative and polished". "ACE" magazine's reviewer enjoyed the game's animation, multi-character gameplay, and depth, and called it "one of the better pics n' action games on the market". The reviewer enjoyed the game but commented that "traditional adventurers" wouldn't as much. Scolding noted "Maniac Mansion"‍ '​s "flash graphics and black humour" and finished by calling the game one of the best of its kind. German magazine "Happy-Computer" compared the cinematic cutscene usage to earlier Lucasfilm titles "Koronis Rift" and "Labyrinth: The Computer Game", and the menu system to ICOM Simulations' "Uninvited". The reviewers lauded the game's user-friendly menu system, graphics, originality, and overall enjoyability; one of the reviewers called it the best adventure title at the time. The magazine later reported that it was West Germany's highest-selling video game for three straight months.
In more recent reviews, Eurogamer‍ '​s Kristan Reed praised the game's "ambitious" design, citing the cast of characters, "elegant" interface, and writing. Game designer Sheri Graner Ray listed "Maniac Mansion" as an example of a game that challenged the "damsel in distress" concept by including female protagonists. However, writer Mark Dery commented that rescuing the kidnapped cheerleader reinforced negative gender roles. In choosing the top ten all-time games for the Commodore 64, "Retro Gamer" stated that "Maniac Mansion" and "Zak McKracken" were equally good, but it selected the latter because of "Maniac Mansion"‍ '​s prominence. In another issue, editor Ashley Day listed the game as having his favorite ending – the mansion's explosion upon pressing an unexpected button. In 2009, IGN staff named "Maniac Mansion" one of the ten best LucasArts adventure games. Richard Cobbett of "PC Gamer" called it "one of the most intricate and important adventure games ever made", citing the SCUMM interface and its establishment of a legacy for Lucasfilm Games during this time.
Reception of ports.
"Maniac Mansion" was also well received in multi-format reviews, including the Commodore 64, Apple II, and PC versions. In noting the game's parodic nature, "Questbusters: The Adventurer's Newsletter" editor Shay Addams wrote that the SCUMM system worked better than the wheel used in "Labyrinth: The Computer Game", calling it an improvement from Interplay's title "Tass Times in Tonetown". He concluded by writing that "Maniac Mansion" was Lucasfilm's best title released and that it is a good buy for Commodore 64 and Apple II users who were unable to play games with better visuals such as from Sierra Entertainment. "Computer Gaming World"‍ '​s Charles Ardai praised the game's pacing, cutscenes, and humor, stating that it "strikes the necessary and precarious balance between laughs and suspense that so many comic horror films and novels lack". Despite faulting its small number of commands, he hailed its control system as "one of the most comfortable ever devised". However, Ardai disliked the game's small quantity of sound effects and music. Ardai finished by calling it "a clever and imaginative game[, ... and] a successful stylistic experiment".
In other multi-format reviews, "The Deseret News" staff called it "wonderful fun" and noted that the "art and animation are gorgeous". The writers considered the game's audio "the best [they had] heard". Reviewing the PC and Atari ST ports, a critic from "The Games Machine" called "Maniac Mansion" "an enjoyable romp" with a structure superior to subsequent LucasArts adventure games. However, the magazine writer noted the game's poor pathfinding and stated that "the lack of sound effects reduces atmosphere". Of the two versions, the reviewer believed that the Atari ST audiovisuals were better. Comparing the PC version to "Zak McKracken and the Alien Mindbenders" and "", reviewers from French magazine "Génération 4" praised the game's story, interface, and humor, stating that it was "beautifully done"; however, one reviewer commented that the developers ripped the graphics from "Indiana Jones". Bill Kunkel and Joyce Worley of "VideoGames & Computer Entertainment" called "Maniac Mansion" "the most popular haunted-house adventure" and "a genuine cult classic"; while they found the plot and setup similar to most horror-themed games, the pair praised the game's interface and execution.
The game's Amiga version received a fair amount of praise, despite graphical shortcomings. In a 1993 review, "The One Amiga"‍ '​s Simon Byron noted that the game retained its "charm and humour" six years after its first release. However, he believed that "Maniac Mansion"‍ '​s art direction had become "tacky" compared to more recent games. Byron ended by writing that "if you fancy a cheap edge-of-the-seat challenge then you couldn't really do much better". "Amiga Format" reviewer Stephen Bradly found the game derivative, but he noted that it featured "loads of visual humour"; he added, "Strangely, it's quite compelling after a while." Heinrich Lenhardt of German magazine "Power Play" wrote that the Amiga version "played like a poem" and just as well as the other ports. Michael Labiner of "Amiga Joker", another German magazine, stated that it was one of the best adventure games released for the computer. While he wrote that there were minor graphical flaws, such as limited colors, Labiner stated that the gameplay made up for those shortcomings. Sweden-based "Datormagazin"‍ '​s Ingela Palmér stated that the Amiga version differed little from the Commodore 64 one, and that those who already have the latter need not get the Amiga version. She added that, while the graphics and gameplay were not the best, "Maniac Mansion" remained highly enjoyable and easy. Palmér recommended that people new to the genre play this game first.
Reviewers well received "Maniac Mansion"‍ '​s NES version. Based on the computer release's success, "Game Players"‍ '​ writers speculated that the NES port would be one of 1990's better titles. UK-based "Mean Machines" reviewers lauded the game for its presentation, playability, and replay value, while criticizing the blocky graphics and "ear-bashing tunes". Reviewer Edward Laurence wrote that aside from minor graphical and sound improvements, little had changed from the Commodore 64 version. Julian Rignall compared the game to "Shadowgate" but noted differences between the two; he commented how "Maniac Mansion" had easy controls and that it lacked "Shadowgate"‍ '​s "death-without-warning situations". Despite his criticism of the audiovisuals, he wrote, ""Maniac Mansion"‍ '​s excellent, thoroughly rewarding and genuinely funny gameplay more than makes up for its deficiencies, and the end result is a highly original and very addictive adventure that no Nintendo owner should be without." "Video Games" magazine reviewed the translated German version, and the reviewers labeled the game as a ""Video Games" Classic". Co-reviewer Heinrich Lenhardt said that "Maniac Mansion" was unique and that no similar NES adventure game has since been released. He wrote that it was just as fun as the computer versions with good controls, but he noted that the graphics could be misleading at times. Co-reviewer Winnie Forster wrote that the game was "one of the most original representatives of the [adventure game] genre" and that it was one of Lucasfilm's more successful games. In recent commentary, "Edge" magazine staff described the port as more conservative than the original version, calling it "somewhat neutered". "GamesTM" magazine writers referred to the NES version as "infamous" and heavily censored.
"Maniac Mansion" was featured often in the magazine "Nintendo Power". The game debuted on the magazine's Top 30 list at number 19 in February 1991, peaking at number 16 in August 1991. The magazine reviewed "Maniac Mansion" again in its February 1993 issue, as part of a staff overview on overlooked or otherwise undersold NES games. The editors felt that the popular RPG "Final Fantasy" overshadowed its September 1990 feature and drew more people to that game instead. Seven years after the game's release, the magazine ranked the NES version the 61st best game in its 100th issue in September 1997, calling "Maniac Manion" a "brilliant adventure". In its 20th anniversary issue, the magazine listed "Maniac Mansion" as the 16th best NES title, praising the game for its clever and funny writing and for being unlike any other game on the system. In its November 2010 issue, as part of the NES' 25th anniversary, Chris Hoffman described the game as "unlike anything else out there – a point-and-click adventure with an awesome sense of humor and multiple solutions to almost every puzzle." "Nintendo Power" also commented on the ability to microwave a hamster; in its 25th anniversary retrospective, the staff stated that "it's hard to mention "Maniac Mansion" without it". "Retro Gamer" listed the hamster incident as one of the top 100 video game moments in March 2012.
Impact and legacy.
Referring to "Maniac Mansion" as a "seminal" title, "GamesTM" staff credited it with reinventing the graphical adventures' gameplay. The writer stated that removing the need to guess input verbs allowed players to focus more on the story and puzzles, resulting in less frustration and more enjoyment. Eurogamer's Kristan Reed made similar comments, saying that the design freed players from the "guessing-game frustration" and made the process "infinitely more elegant and intuitive". However, Connie Veugen and Felipe Quérette noted that determining the game's vocabulary was an enjoyable aspect of the genre. "GamesTM" magazine further commented that the game had solidified Lucasfilm Games as a leader in the graphic adventure genre. Authors Mike and Sandie Morrison commented that the studio had brought "serious competition" to the genre in the form of "Maniac Mansion". Authors Rusel DeMaria and Johnny Wilson echoed the sentiment, calling it a "landmark title" for the company. They also stated that the game, along with "" and "Leisure Suit Larry", had inaugurated a "new era of humor-based adventure games". Reed seconded the statement, noting that the game "set in motion a captivating chapter in the history of gaming" that encompassed wit, invention, and style. GameSpy's Christopher Buecheler credited the game's success with making its genre commercially and critically viable. It was also one of the first video games to feature product placement (Pepsi brands); other games, such as "Teenage Mutant Ninja Turtles II: The Arcade Game", "Zool", and "Tapper" followed suit. "Retro Gamer"‍ '​s Stuart Hunt wrote in a September 2011 issue that ""Maniac Mansion" proved that videogames could capture the essence of an entirely different medium and opened our eyes to the wonderful things that happened when they placed their interactive stamp on them". "The Secret of Monkey Island" and "Day of the Tentacle" developer Dave Grossman said that "Maniac Mansion" revolutionized the adventure game genre, also noting the fact that the game was only 64 KB large and that the music was good, especially for PCs. In a Joystiq interview on his development of "The Cave", Gilbert said that some people originally did not classify "Maniac Mansion" as an adventure game because it was not a text-based adventure with stationary graphics and a text parser, just as people did not classify "The Cave" as an adventure game because it is not point-and-click. He concluded by citing "Maniac Mansion" as an example of the evolution of the adventure game genre, saying: "I think adventure games just evolve and they change, and I think you just need to do what's right for them."
The game engine, SCUMM, has been described as "revolutionary." Throughout the following decade Lucasfilm Games used the engine to develop eleven other games, improving the engine with each subsequent game. "GamesTM" attributed this change to a desire to streamline production and produce fun games. Competitors eventually adopted similar systems for their adventure games. Following his departure from LucasArts (Lucasfilm Games had been combined under this name with ILM and Skywalker Sound in 1990) in 1992, Gilbert used the SCUMM technology to create adventure games and "Backyard Sports" games at Humongous Entertainment. The designers built on their experience from "Maniac Mansion" and expanded the process and their ambition in subsequent titles. In retrospect, Gilbert commented that he made a number of mistakes designing the game (for instance, the dead-end situations that arise if certain items are used incorrectly) and applied the lessons to future games. In cutscenes, Gilbert had used a timer rather than a specific event to trigger them, which occasionally resulted in awkward scene changes. The designer aimed to avoid these flaws in the "Monkey Island" series of games. However, Gilbert commented that "Maniac Mansion" is his favorite because of its imperfections.
In popular culture.
Elements of "Maniac Mansion" have appeared elsewhere in popular culture, especially in other Lucasfilm games. An in-game object called "Chuck the Plant" reappeared in other Lucas adventure titles like "Indiana Jones and the Last Crusade" and "Tales of Monkey Island". According to Gilbert, Steve Arnold, the LucasFilm general manager at the time, had a long-running joke in which he continually requested game designers to add a character named Chuck to their game. Gilbert and Winnick were the first to humor Steve's request in "Maniac Mansion". Because the developers were unable to fit an extra character name in the game, they named an existing in-game plant. David Fox included a gasoline item for a nonexistent chainsaw in his game "Zak McKracken and the Alien Mindbenders" as a parody of the chainsaw that required nonexistent gasoline in "Maniac Mansion". Enthusiasts have created fan art depicting the characters, participated in cosplay based on the tentacle characters, and produced a trailer of a fictitious live action film.
Fanmade remakes.
Various fanmade enhanced remakes of "Maniac Mansion" have appeared over the years.
TV adaptation and game sequel.
Lucasfilm had conceived the idea for a television adaptation, which The Family Channel purchased in 1990. A sitcom named after the game debuted in September 1990. It aired on YTV in Canada and The Family Channel in the United States. Partially based on the video game, the show focused on the Edison family's life and featured Joe Flaherty as Dr. Fred. Eugene Levy headed the writing staff. The program was a collaboration between Lucasfilm, The Family Channel, and Atlantis Films. In retrospect, Gilbert commented that the premise gradually changed during production to something that differed greatly from the game's original plot. Upon its debut, the show was well received by critics; "Time magazine" named it one of the best new shows of the year. However, other reviewers, such as "Entertainment Weekly"‍ '​s Ken Tucker, questioned how the show made it on The Family Channel, given Flaherty's usage of "SCTV"-like humor. "PC Gamer"‍ '​s Richard Cobbett, in a retrospective on the series, criticized its generic storylines and lack of relevance to the game. The series lasted for three seasons, filming 66 episodes.
In the early 1990s, LucasArts asked Dave Grossman and Tim Schafer, who had both worked with Gilbert on the "Monkey Island" games, to design a sequel to "Maniac Mansion", eventually titled "Day of the Tentacle". Winnick and Gilbert initially assisted with the writing. Grossman and Schafer were able to include the voices and the improved visuals Gilbert had originally envisioned for "Maniac Mansion". The game discarded the character selection and branching story lines in favor of a simpler format, and introduced time travel as the main puzzle element. The developers retained the Edison family and Bernard characters, but changed the art style to more closely resemble Chuck Jones' works. As a homage to "Maniac Mansion", the designers included a puzzle that involves freezing a hamster; according to Grossman, he gave a happier outcome for the hamster as a response to Gilbert's grim ending for the hamster in "Maniac Mansion". They also made the original game playable on an in-game computer resembling a Commodore 64, which Grossman attributed to Gilbert reminiscing about the original's file size. LucasArts released "Day of the Tentacle" in 1993 to critical acclaim.

</doc>
<doc id="19669" url="http://en.wikipedia.org/wiki?curid=19669" title="Marx Brothers">
Marx Brothers

The Marx Brothers were a family comedy act that was successful in vaudeville, on Broadway, and in motion pictures from 1905 to 1949. Five of the Marx Brothers' thirteen feature films were selected by the American Film Institute (AFI) as among the top 100 comedy films, with two of them ("Duck Soup" and "A Night at the Opera") in the top twelve. The brothers were included in AFI's 100 Years...100 Stars list of the most significant screen legends, the only performers to be inducted collectively.
The core of the act was the three elder brothers, Chico, Harpo, and Groucho; each developed a highly distinctive stage persona. The two younger brothers, Gummo and Zeppo, did not develop their stage characters to the same extent, and eventually left the act to pursue other careers. Gummo was not in any of the movies; Zeppo appeared in the first five films in relatively straight (non-comedic) roles.
Early life.
Born in New York City, the Marx Brothers were the sons of Jewish immigrants from Germany and France. Their mother, Minnie Schönberg, was from Dornum in East Frisia; and their father, Simon Marx (whose name was changed to Samuel Marx, and who was nicknamed "Frenchy") was a native of Alsace and worked as a tailor. The family lived in the then-poor Yorkville section of New York City's Upper East Side, between the Irish, German and Italian quarters.
Brothers.
The brothers were:
A sixth brother, Manfred ("Mannie"), was actually the first child of Samuel and Minnie, born in 1886, though an online family tree states that he was born in 1885: "Family lore told privately of the firstborn son, Manny, born in 1886 but surviving for only three months, and carried off by tuberculosis. Even some members of the Marx family wondered if he was pure myth. But Manfred can be verified. A death certificate of the Borough of Manhattan reveals that he died, aged seven months, on 17 July 1886, of enterocolitis, with 'asthenia' contributing, i.e., probably a victim of influenza. He is buried at New York's Washington Cemetery, beside his grandmother, Fanny Sophie Schönberg (née Salomons), who died on 10 April 1901."
Stage beginnings.
The brothers were from a family of artists, and their musical talent was encouraged from an early age. Harpo was particularly talented, learning to play an estimated six different instruments throughout his career. He became a dedicated harpist, which gave him his nickname. Chico was an excellent pianist, Groucho a guitarist and singer, and Zeppo a vocalist.
They got their start in vaudeville, where their uncle Albert Schönberg performed as Al Shean of Gallagher and Shean. Groucho's debut was in 1905, mainly as a singer. By 1907, he and Gummo were singing together as "The Three Nightingales" with Mabel O'Donnell. The next year, Harpo became the fourth Nightingale and by 1910, the group briefly expanded to include their mother Minnie and their Aunt Hannah. The troupe was renamed "The Six Mascots".
Comedy.
One evening in 1912, a performance at the Opera House in Nacogdoches, Texas, was interrupted by shouts from outside about a runaway mule. The audience hurried out to see what was happening. When the audience returned, Groucho, angered by the interruption, made snide comments about its members, including "Nacogdoches is full of roaches" and "The jackass is the flower of Tex-ass". Instead of becoming angry, the audience laughed. The family then realized it had potential as a comic troupe. (However, in his autobiography, "Harpo Speaks", Harpo Marx states that the runaway mule incident occurred in Ada, Oklahoma. A 1930 article in the "San Antonio Express" newspaper states that the incident took place in Marshall, Texas.)
The act slowly evolved from singing with comedy to comedy with music. The brothers' sketch "Fun in Hi Skule" featured Groucho as a German-accented teacher presiding over a classroom that included students Harpo, Gummo and Chico. The last version of the school act, titled "Home Again", was written by their uncle, Al Shean. When the "Home Again" tour reached Flint, Michigan, in 1915, 14-year-old Zeppo joined his four brothers for what is believed to be the only time that all five Marx Brothers appeared together on stage. Then Gummo left to serve in World War I, reasoning that "anything is better than being an actor!" Zeppo replaced him in their final vaudeville years and in the jump to Broadway, and then to Paramount films.
During World War I, anti-German sentiments were common, and the family tried to conceal its German origin. After learning that farmers were excluded from the draft rolls, mother Minnie purchased a 27 acre poultry farm near Countryside, Illinois, but the brothers soon found that chicken ranching was not in their blood. During this time, Groucho discontinued his "German" stage personality.
By this time, "The Four Marx Brothers" had begun to incorporate their unique style of comedy into their act and to develop their characters. Both Groucho's and Harpo's memoirs say that their now-famous on-stage personae were created by Al Shean. Groucho began to wear his trademark greasepaint mustache and to use a stooped walk. Harpo stopped speaking onstage and began to wear a red fright wig and carry a taxi-cab horn. Chico spoke with a fake Italian accent, developed off-stage to deal with neighborhood toughs, while Zeppo adopted the role of the romantic (and "peerlessly cheesy", according to James Agee) straight man.
The on-stage personalities of Groucho, Chico and Harpo were said to have been based on their actual traits. Zeppo, on the other hand, was considered the funniest brother offstage, despite his straight stage roles. As the youngest, and having grown up watching his brothers, he could fill in for and imitate any of the others when illness kept them from performing. "He was so good as Captain Spaulding [in "Animal Crackers"] that I would have let him play the part indefinitely, if they had allowed me to smoke in the audience", Groucho recalled. (Zeppo did impersonate Groucho in the film version of "Animal Crackers". Groucho was unavailable to film the scene in which the Beaugard painting is stolen, so the script was contrived to include a power failure, which allowed Zeppo to play the Spaulding part in near-darkness.)
By the 1920s, the Marx Brothers had become one of America's favorite theatrical acts. With their sharp and bizarre sense of humor, they satirized high society and human hypocrisy. They became famous for their improvisational comedy in free-form scenarios. A famous early instance was when Harpo arranged to chase a fleeing chorus girl across the stage during the middle of a Groucho monologue to see if Groucho would be thrown off. However, to the audience's delight, Groucho merely reacted by commenting, "First time I ever saw a taxi hail a passenger". When Harpo chased the girl back the other direction, Groucho, calmly checking his watch, ad-libbed, "The 9:20's right on time. You can set your watch by the Lehigh Valley."
Under Chico's management, and with Groucho's creative direction, the brothers' vaudeville act had led to them becoming stars on Broadway, first with a musical revue, "I'll Say She Is" (1924–1925) and then with two musical comedies, "The Cocoanuts" (1925–1926) and "Animal Crackers" (1928–1929). Playwright George S. Kaufman worked on the last two and helped sharpen the brothers' characterizations.
Out of their distinctive costumes the brothers looked alike, even down to their receding hairlines. Zeppo could pass for a younger Groucho, and played the role of his son in "Horse Feathers". A scene in "Duck Soup" finds Groucho, Harpo and Chico all appearing in the famous greasepaint eyebrows, mustache and round glasses, while wearing nightcaps. The three are indistinguishable, enabling them to carry off the "mirror scene" perfectly.
Origin of the stage names.
Zeppo apart, the stage names of the brothers were coined by monologist Art Fisher during a poker game in Galesburg, Illinois, based both on the brothers' personalities and Gus Mager's "Sherlocko the Monk", a popular comic strip of the day which included a supporting character named "Groucho". As Fisher dealt each brother a card, he addressed them, for the very first time, by the names they would keep for the rest of their lives.
The reasons behind Chico's and Harpo's stage names are undisputed, and Gummo's is fairly well established. Groucho's and Zeppo's are far less clear. Arthur was named Harpo because he played the harp, and Leonard became Chico (pronounced "Chick-o") because he was, in the slang of the period, a "chicken chaser". ("Chickens"—later "chicks"—was period slang for women. "In England now," said Groucho, "they were called 'birds'.")
In his autobiography, Harpo explains that Milton became Gummo because he crept about the theater like a gumshoe detective. Other sources report that Gummo was the family's hypochondriac, having been the sickliest of the brothers in childhood, and therefore wore rubber overshoes, called gumshoes, in all kinds of weather. Still others report that Milton was the troupe's best dancer, and dance shoes tended to have rubber soles. Groucho stated that the source of the name was Gummo wearing galoshes. Whatever the details, the name relates to rubber-soled shoes.
The reason Julius was named Groucho is perhaps the most disputed. There are three explanations:
I kept my money in a 'grouch bag'. This was a small chamois bag that actors used to wear around their neck to keep other hungry actors from pinching their dough. Naturally, you're going to think that's where I got my name from. But that's not so. Grouch bags were worn on manly chests long before there was a Groucho.
Herbert was not nicknamed by Art Fisher, since he did not join the act until Gummo had departed. As with Groucho, three explanations exist for Herbert's name, "Zeppo":
Maxine Marx reported in "The Unknown Marx Brothers" that the brothers listed their "real" names (Julius, Leonard, Adolph, Milton and Herbert) on playbills and in programs, and only used the nicknames behind the scenes, until Alexander Woollcott overheard them calling one another by the nicknames; he asked them why they used their real names publicly when they had such wonderful nicknames. They replied, "That wouldn't be dignified." Woollcott answered with a belly laugh. Since Woollcott did not meet the Marx Brothers until the premiere of "I'll Say She Is," which was their first Broadway show, this would mean they used their real names throughout their vaudeville days, and that the name "Gummo" never appeared in print during his time in the act. Other sources report that the Marx Brothers did go by their nicknames during their vaudeville era, but briefly listed themselves by their given names when "I'll Say She Is" opened because they were worried that a Broadway audience would reject a vaudeville act if they were perceived as low class.
Motion pictures.
Paramount.
The Marx Brothers' stage shows became popular just as motion pictures were evolving to "talkies". They signed a contract with Paramount Pictures and embarked on their film career at Paramount's Astoria, New York, studios. Their first two released films (after an unreleased short silent film titled "Humor Risk") were adaptations of the Broadway shows "The Cocoanuts" (1929) and "Animal Crackers" (1930). Both were written by George S. Kaufman and Morrie Ryskind. Production then shifted to Hollywood, beginning with a short film that was included in Paramount's twentieth anniversary documentary, "The House That Shadows Built" (1931), in which they adapted a scene from "I'll Say She Is". Their third feature-length film, "Monkey Business" (1931), was their first movie not based on a stage production, and the only one in which Harpo's voice is heard (singing tenor from inside a barrel in the opening scene). "Horse Feathers" (1932), in which the brothers satirized the American college system and Prohibition, was their most popular film yet, and won them the cover of "Time". It included a running gag from their stage work, in which Harpo produces a ludicrous array of props from his coat, including a wooden mallet, a fish, a coiled rope, a tie, a poster of a woman in her underwear, a cup of hot coffee, a sword; and, just after Groucho warns him that he "can't burn the candle at both ends," a candle burning at both ends.
During this period Chico and Groucho starred in a radio comedy series, "Flywheel, Shyster and Flywheel". Though the series was short lived, much of the material developed for it was used in subsequent films. The show's scripts and recordings were believed lost until copies of the scripts were found in the Library of Congress in the 1980s. After publication in a book they were performed with Marx Brothers impersonators for BBC Radio.
Their last Paramount film, "Duck Soup" (1933), directed by the highly regarded Leo McCarey, is the highest rated of the five Marx Brothers films on the American Film Institute's "100 years ... 100 Movies" list. It did not do as well financially as "Horse Feathers", but was the sixth-highest grosser of 1933. The film sparked a dispute between the Marxes and the village of Fredonia, New York. "Freedonia" was the name of a fictional country in the script, and the city fathers wrote to Paramount and asked the studio to remove all references to Freedonia because "it is hurting our town's image". Groucho fired back a sarcastic retort asking them to change the name of their town, because "it's hurting our picture."
MGM, RKO, and United Artists.
After expiration of the Paramount contract Zeppo left the act to become an agent. He and brother Gummo went on to build one of the biggest talent agencies in Hollywood, helping the likes of Jack Benny and Lana Turner get their starts. Groucho and Chico did radio, and there was talk of returning to Broadway. At a bridge game with Chico, Irving Thalberg began discussing the possibility of the Marxes joining Metro-Goldwyn-Mayer. They signed, now billed as "Groucho, Chico, Harpo, Marx Bros."
Unlike the free-for-all scripts at Paramount, Thalberg insisted on a strong story structure that made the brothers more sympathetic characters, interweaving their comedy with romantic plots and non-comic musical numbers, and targeting their mischief-making at obvious villains. Thalberg was adamant that scripts include a "low point", where all seems lost for both the Marxes and the romantic leads. He instituted the innovation of testing the film's script before live audiences before filming began, to perfect the comic timing, and to retain jokes that earned laughs and replace those that did not. Thalberg restored Harpo's harp solos and Chico's piano solos, which had been omitted from "Duck Soup".
The first Marx Brothers/Thalberg film was "A Night at the Opera" (1935), a satire on the world of opera, where the brothers help two young singers in love by throwing a production of "Il Trovatore" into chaos. The film—including its famous scene where an absurd number of people crowd into a tiny stateroom on a ship—was a great success, and was followed two years later by an even bigger hit, "A Day at the Races" (1937), in which the brothers cause mayhem in a sanitarium and at a horse race. The film features Groucho and Chico's famous "Tootsie Frootsie Ice Cream" sketch. In a 1969 interview with Dick Cavett, Groucho said that the two movies made with Thalberg were the best that they ever produced. Despite the Thalberg films' success, MGM terminated the brothers' contract in 1937; Thalberg had died suddenly during filming of "A Day at the Races", leaving the Marxes without an advocate at the studio.
After a short experience at RKO ("Room Service", 1938), the Marx Brothers returned to MGM and made three more films: "At the Circus" (1939), "Go West" (1940) and "The Big Store" (1941). Prior to the release of "The Big Store" the team announced its retirement from the screen. Four years later, however, Chico persuaded his brothers to make two additional films, "A Night in Casablanca" (1946) and "Love Happy" (1949), to alleviate his severe gambling debts. Both pictures were released by United Artists.
Later years.
From the 1940s onward Chico and Harpo appeared separately and together in nightclubs and casinos. Chico fronted a big band, the Chico Marx Orchestra (with 17-year-old Mel Tormé as a vocalist). Groucho began his solo career with "You Bet Your Life", which ran from 1947 to 1961 on NBC radio and television. He authored several books, including "Groucho and Me" (1959), "Memoirs of a Mangy Lover" (1964) and "The Groucho Letters" (1967).
Groucho and Chico briefly appeared together in a 1957 short film promoting the "Saturday Evening Post" entitled "Showdown at Ulcer Gulch," directed by animator Shamus Culhane, Chico's son-in-law. Groucho, Chico, and Harpo worked together (in separate scenes) in "The Story of Mankind" (1957). In 1959, the three began production of "Deputy Seraph," a TV series starring Harpo and Chico as blundering angels, and Groucho (in every third episode) as their boss, the "Deputy Seraph." The project was abandoned when Chico was found to be uninsurable (and incapable of memorizing his lines) due to severe arteriosclerosis. On March 8 of that year, Chico and Harpo starred as bumbling thieves in "The Incredible Jewel Robbery", a half-hour pantomimed episode of the "General Electric Theater" on CBS. Groucho made a cameo appearance—uncredited, because of constraints in his NBC contract—in the last scene, and delivered the only line of dialogue ("We won't talk until we see our lawyer!").
According to a September 1947 article in "Newsweek", Groucho, Harpo, Chico and Zeppo all signed to appear as themselves in a biopic entitled "The Life and Times of the Marx Brothers". In addition to being a non-fiction biography of the Marxes, the film would have featured the brothers reenacting much of their previously unfilmed material from both their vaudeville and Broadway eras. The film, had it been made, would have been the first performance by the Brothers as a quartet since 1933.
The five brothers made only one television appearance together, in 1957, on an early incarnation of "The Tonight Show" called "Tonight! America After Dark", hosted by Jack Lescoulie. Five years later (October 1, 1962) after Jack Paar's tenure, Groucho made a guest appearance to introduce the "Tonight Show's" new host, Johnny Carson.
Around 1960, the acclaimed director Billy Wilder considered writing and directing a new Marx Brothers film. Tentatively titled "A Day at the U.N.", it was to be a comedy of international intrigue set around the United Nations building in New York. Wilder had discussions with Groucho and Gummo, but the project was put on hold because of Harpo's ill-health and abandoned when Chico died in 1961.
In 1966 Filmation produced a pilot for a Marx Brothers cartoon. Groucho was Pat Harrington Jr. and other voices were Ted Knight and Joe Besser.
In 1970, the four Marx Brothers had a brief reunion of sorts in the animated ABC television special "The Mad, Mad, Mad Comedians", produced by Rankin-Bass animation (of "Rudolph the Red-Nosed Reindeer" fame). The special featured animated reworkings of various famous comedians' acts, including W. C. Fields, Jack Benny, George Burns, Henny Youngman, the Smothers Brothers, Flip Wilson, Phyllis Diller, Jack E. Leonard, George Jessel and the Marx Brothers. Most of the comedians provided their own voices for their animated counterparts, except for Fields and Chico Marx (both had died), and Zeppo Marx (who had left show business in 1933). Voice actor Paul Frees filled in for all three (no voice was needed for Harpo, who had also died). The Marx Brothers' segment was a reworking of a scene from their Broadway play "I'll Say She Is", a parody of Napoleon which Groucho considered among the brothers' funniest routines. The sketch featured animated representations, if not the voices, of all four brothers. Romeo Muller is credited as having written special material for the show, but the script for the classic "Napoleon Scene" was probably supplied by Groucho.
On January 16, 1977, the Marx Brothers were inducted into the Motion Picture Hall of Fame.
Many television shows and movies have used Marx Brothers references. "Animaniacs" and "Tiny Toons", for example, have featured Marx Brothers jokes and skits. Hawkeye Pierce (Alan Alda) on "M*A*S*H" occasionally put on a fake nose and glasses, and, holding a cigar, did a Groucho impersonation to amuse patients recovering from surgery. Early episodes also featured a singing and off-scene character named Captain Spaulding as a tribute. Bugs Bunny impersonated Groucho Marx in the 1947 cartoon "Slick Hare" and in a later cartoon he again impersonated Groucho hosting a TV show called "You Beat Your Wife," asking Elmer Fudd if he had stopped beating his wife. Tex Avery's cartoon "Hollywood Steps Out" (1941) featured appearances by Harpo and Groucho. They appeared, sometimes with Chico and Zeppo caricatured, in cartoons starring Mickey Mouse, Flip the Frog and others. In the "Airwolf" episode 'Condemned', four anti-virus formulae for a deadly plague were named after the four Marx Brothers. In "All In The Family", Rob Reiner often did imitations of Groucho, and Sally Struthers dressed as Harpo in one episode in which she (as Gloria Stivic) and Rob (as Mike Stivic) were going to a Marx Brothers film festival, with Reiner dressing as Groucho. Gabe Kaplan did many Groucho imitations on his sit-com "Welcome Back, Kotter" and Robert Hegyes sometimes imitated both Chico and Harpo on the show. In Woody Allen's film "Hannah and Her Sisters" (1986), Woody's character, after an unsuccessful suicide attempt, is inspired to go on living after seeing a revival showing of "Duck Soup". In "Manhattan" (1979), he names the Marx Brothers as something that makes life worth living. In "Everyone Says I Love You" (1996), he and Goldie Hawn dress as Groucho for a Marx Brothers celebration in France, and the song "Hooray for Captain Spaulding", from "Animal Crackers", is performed, with various actors dressed as the brothers, striking poses famous to Marx fans. (The film itself is named after a song from "Horse Feathers", a version of which plays over the opening credits.)
Harpo Marx appeared as himself in a sketch on "I Love Lucy" in which he and Lucille Ball reprised the mirror routine from "Duck Soup", with Lucy dressed up as Harpo. Lucy had worked with the Marxes when she appeared in a supporting role in an earlier Marx Brothers film, "Room Service". Chico once appeared on "I've Got a Secret" dressed up as Harpo; his secret was shown in a caption reading, "I'm pretending to be Harpo Marx (I'm Chico)". The Marx Brothers were spoofed in the second act of the Broadway Review "A Day in Hollywood/A Night in the Ukraine".
Filmography.
Films with the four Marx Brothers:
Films with the three Marx Brothers (post-Zeppo):
Solo endeavors:
Legacy.
Awards and honors.
The Marx Brothers were collectively named #20 on AFI's list of the Top 25 American male screen legends. They are the only group to be so honored.
The "Sweathogs" of the ABC-TV series "Welcome Back Kotter" (John Travolta, Robert Hegyes, Lawrence Hilton-Jacobs, and Ron Palillo) patterned much of their on-camera banter in that series after the Marx Brothers. Series star Gabe Kaplan was reputedly a big Marx Brothers fan.

</doc>
