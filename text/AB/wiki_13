<doc id="14992" url="http://en.wikipedia.org/wiki?curid=14992" title="Ivory-billed woodpecker">
Ivory-billed woodpecker

The ivory-billed woodpecker ("Campephilus principalis") is one of the largest woodpeckers in the world, at roughly 20 inches in length and 30 inches in wingspan. It was native to the virgin forests of the southeastern United States (along with a separate subspecies native to Cuba). Due to habitat destruction, and to a lesser extent hunting, its numbers have dwindled to the point where it is uncertain whether any remain, though there have been reports that it has been seen again. Almost no forests today can maintain an ivory-billed woodpecker population.
The species is listed as critically endangered and possibly extinct by the International Union for Conservation of Nature (IUCN). The American Birding Association (ABA) lists the ivory-billed woodpecker as a Class 6 species, a category the ABA defines as "definitely or probably extinct."
Reports of at least one male ivory-billed woodpecker in Arkansas in 2004 were investigated and subsequently published in April 2005 by a team led by the Cornell Lab of Ornithology. No definitive confirmation of those reports emerged, despite intensive searching over five years following the initial sightings.
An anonymous $10,000 reward was offered in June 2006 for information leading to the discovery of an ivory-billed woodpecker nest, roost or feeding site. In December 2008, the Nature Conservancy announced a reward of $50,000 to the person who can lead a project biologist to a living ivory-billed woodpecker.
In late September 2006, a team of ornithologists from Auburn University and the University of Windsor published reports of their own sightings of ivory-billed woodpeckers along the Choctawhatchee River in northwest Florida, beginning in 2005. These reports were accompanied by evidence that the authors themselves considered suggestive for the existence of ivory-billed woodpeckers. Searches in this area of Florida through 2009 failed to produce definitive confirmation.
Despite these high-profile reports from Arkansas and Florida and sporadic reports elsewhere in the historic range of the species since the 1940s, there is no conclusive evidence for the continued existence of the ivory-billed woodpecker; i.e., there are no unambiguous photographs, videos, specimens or DNA samples from feathers or feces of the ivory-billed woodpecker. Land acquisition and habitat restoration efforts have been initiated in certain areas where there is a relatively high probability that the species may have survived to protect any possible surviving individuals.
Taxonomy.
The Ivory-billed is the type species for the genus "Campephilus", a group of large American woodpeckers. Although the Ivory-billed looks very similar to the pileated woodpeckers they are not close relatives as the latter is a member of the genus "Dryocopus".
Ornithologists have traditionally recognized two subspecies of this bird: the American Ivory-billed, the more famous of the two, and the Cuban ivory-billed woodpecker. The two look similar despite differences in size and plumage. There is some controversy over whether the Cuban ivory-billed woodpecker is more appropriately recognized as a separate species. A recent study compared DNA samples taken from specimens of both ivory-billed birds along with the imperial woodpecker, a larger but otherwise very similar bird. It concluded not only that the Cuban and American Ivory-billed woodpeckers are genetically distinct, but also that they and the imperial form a North American clade within "Campephilus" that appeared in the Mid-Pleistocene. The study does not attempt to define a lineage linking the three birds, though it does imply that the Cuban bird is more closely related to the imperial.
The American Ornithologists' Union Committee on Classification and Nomenclature has said it is not yet ready to list the American and Cuban as separate species. Lovette, a member of the committee, said that more testing is needed to support that change, but concluded that "These results will likely initiate an interesting debate on how we should classify these birds." Before this study, it was thought that the Cuban Ivory-billed were descended from mainland woodpeckers, either introduced to Cuba by Native Americans or accidentals that flew to the island themselves.
While recent evidence suggesting that American ivory-billed woodpeckers may still exist in the wild has caused excitement in the Ornithology community, no similar evidence exists for the Cuban Ivory-billed bird, believed to be extinct since the last sighting in the late 1980s.
Description.
The ivory-billed woodpecker ranks among the largest woodpeckers in the world and is the largest in the United States. The closely related and likewise possibly extinct imperial woodpecker ("C. imperialis") of western Mexico is, or was, the largest woodpecker. The Ivory-billed has a total length of 48 to and, based on very scant information, weighs about 450 to. It has a typical 76 cm wingspan. Standard measurements attained included a wing chord length of 23.5 -, a tail length of 14 -, a bill length of 5.8 - and a tarsus length of 4 -.
The bird is shiny blue-black with white markings on its neck and back and extensive white on the trailing edge of both the upper- and underwing. The underwing is also white along its forward edge, resulting in a black line running along the middle of the underwing, expanding to more extensive black at the wingtip. In adults, the bill is ivory in color, chalky white in juveniles. Ivory-bills have a prominent crest, although in juveniles it is ragged. The crest is black in juveniles and females. In males, the crest is black along its forward edge, changing abruptly to red on the side and rear. The chin of an ivory-bill is black. When perched with the wings folded, ivory-bills of both sexes present a large patch of white on the lower back, roughly triangular in shape. These characteristics distinguish it from the smaller and darker-billed pileated woodpecker. The pileated normally is brownish-black, smoky, or slaty black in color. It also has a white neck stripe but the back is normally black. Pileated juveniles and adults have a red crest and a white chin. Pileateds normally have no white on the trailing edges of their wings and when perched normally show only a small patch of white on each side of the body near the edge of the wing. However, pileated woodpeckers, apparently aberrant individuals, have been reported with white trailing edges on the wings, forming a white triangular patch on the lower back when perched. Like all woodpeckers, the ivory-bill has a strong and straight bill and a long, mobile, hard-tipped, barbed tongue. Among North American woodpeckers, the ivory-bill is unique in having a bill whose tip is quite flattened laterally, shaped much like a beveled wood chisel.
The bird's drum is a single or double rap. Four fairly distinct calls are reported in the literature and two were recorded in the 1930s. The most common, a kent or hant, sounds like a toy trumpet often repeated in series. When the bird is disturbed, the pitch of the kent note rises, it is repeated more frequently, and is often doubled. A conversational call, also recorded, is given between individuals at the nest, and has been described as "kent-kent-kent". A recording of the bird, made by Arthur A. Allen, can be found .
The ivory-billed woodpecker is sometimes referred to as the Grail Bird, the Lord God Bird, or the Good God Bird, all based on the exclamations of awed onlookers. Other nicknames for the bird are King of the Woodpeckers and Elvis in Feathers.
Habitat and diet.
Ivory-billeds are known to prefer thick hardwood swamps and pine forests, with large amounts of dead and decaying trees. Prior to the American Civil War, much of the Southern United States was covered in vast tracts of primeval hardwood forests that were suitable as habitat for the bird. At that time, the ivory-billed woodpecker ranged from east Texas to North Carolina, and from southern Illinois to Florida and Cuba. After the Civil War, the timber industry deforested millions of acres in the South, leaving only sparse isolated tracts of suitable habitat.
The ivory-billed woodpecker feeds mainly on the larvae of wood-boring beetles, but also eats seeds, fruit, and other insects. The bird uses its enormous white bill to hammer, wedge, and peel the bark off dead trees to find the insects. These birds need about 25 km2 per pair so they can find enough food to feed their young and themselves. Hence they occur at low densities even in healthy populations. The more common pileated woodpecker may compete for food with this species.
Breeding biology.
The ivory-billed woodpecker is thought to pair for life. Pairs are also known to travel together. These paired birds will mate every year between January and May. Both parents work together to excavate a nest in a dead or partially dead tree about 8–15m from the ground before they have their young. Nest openings are typically ovular to rectangular in shape, and measure about 12–14 cm tall by 10 cm wide (4"-5 3/4" by 4")
Usually two to five eggs are laid and incubated for 3 to 5 weeks. Parents incubate the eggs cooperatively, with the male incubating from approximately 4:30 PM–6:30 AM while the female foraged, and vice versa from 6:30 AM–4:30 PM. They feed the chicks for months. Young learn to fly about seven to eight weeks after hatching. The parents will continue feeding them for another two months. The family will eventually split up in late fall or early winter.
Ornithologists speculate that they may live as long as 30 years.
Status.
Heavy logging activity exacerbated by hunting by collectors devastated the population of ivory-billed woodpeckers in the late 19th century. It was generally considered extinct in the 1920s when a pair turned up in Florida, only to be shot for specimens.
In 1932, a Louisiana state representative, Mason Spencer of Tallulah, disproved premature reports of the demise of the species when, armed with a gun and a hunting permit, he killed an ivory-billed woodpecker along the Tensas River and took the specimen to his state wildlife office in Baton Rouge.
By 1938, an estimated twenty woodpeckers remained in the wild, some six to eight of which were in the old-growth forest called the Singer Tract, owned by the Singer Sewing Company in Madison Parish in northeastern Louisiana, where logging rights were held by the Chicago Mill and Lumber Company. The company brushed aside pleas from four Southern governors and the National Audubon Society that the tract be publicly purchased and set aside as a reserve. By 1944, the last known ivory-billed woodpecker, a female, was gone from the cut-over tract.
Reported sightings: 1940s to 1990s.
The first audio and only video recording made of the ivory-billed woodpecker was created as part of a 1935 study by a group of Cornell scientists in the in the Singer Tract in Madison Parish, Louisiana. The ivory-billed woodpecker was listed as an endangered species on 11 March 1967, though the only evidence of its existence at the time was a possible recording of its call made in East Texas. The last reported sighting of the Cuban subspecies ("C. p. bairdii"), after a long interval, was in 1987; it has not been seen since. The Cuban Exile journalist and author John O'Donnell-Rosales, who was born in the area of Cuba with the last confirmed sightings, reported sightings near the Alabama coastal delta in 1994, but these were never properly investigated by state wildlife officials.
Two tantalizing photos were given to Louisiana State University museum director George Lowery in 1971 by a source who wished to remain anonymous but who came forward in 2005 as outdoorsman Fielding Lewis.
The photos, taken with a cheap Instamatic camera, show what appears to be a male Ivory-billed perched on the trunks of two trees in the Atchafalaya Basin of Louisiana. The bird's distinctive bill is not visible in either photo and the photos – taken from a distance – are very grainy. Lowery presented the photos at the 1971 annual meeting of the American Ornithologists Union. Skeptics dismissed the photos as frauds; seeing that the bird is in roughly the same position in both photos, they suggested they may have been of a mounted specimen.
There were numerous unconfirmed reports of the bird, but many ornithologists believed the species had been wiped out completely, and it was assessed as "extinct" by the International Union for Conservation of Nature and Natural Resources in 1994. This assessment was later altered to "critically endangered" on the grounds that the species could still be extant.
2002 Pearl River expedition.
In 1999, there was an unconfirmed sighting of a pair of birds in the Pearl River region of southeast Louisiana by a forestry student, David Kulivan, which some experts considered very compelling. In a 2002 expedition in the forests, swamps, and bayous of the Pearl River Wildlife Management Area by LSU, biologists spent 30 days searching for the bird.
In the afternoon of 27 January 2002, after ten days, a rapping sound similar to the "double knock" made by the ivory-billed woodpecker was heard and recorded. The exact source of the sound was not found because of the swampy terrain, but signs of active woodpeckers were found (i.e., scaled bark and large tree cavities). The expedition was inconclusive, however, as it was determined that the recorded sounds were likely gunshot echoes rather than the distinctive double rap of the ivory-billed woodpecker.
2004/2005 Arkansas reports.
A group of seventeen authors headed by the Cornell Lab of Ornithology (CLO) reported the discovery of at least one ivory-billed woodpecker, a male, in the Big Woods area of Arkansas in 2004 and 2005, publishing the report in the journal "Science" on 28 April 2005.
One of the authors, who was kayaking in the Cache River National Wildlife Refuge, Monroe County, Arkansas, on 11 February 2004, reported on a website the sighting of an unusually large red-crested woodpecker. This report led to more intensive searches in the area and in the White River National Wildlife Refuge, undertaken in secrecy for fear of a stampede of bird-watchers, by experienced observers over the next fourteen months. About fifteen sightings occurred during the period (seven of which were considered compelling enough to mention in the scientific article), possibly all of the same bird. One of these more reliable sightings was on 27 February 2004. Bobby Harrison of Huntsville, Alabama and Tim Gallagher of Ithaca, New York, both reported seeing an ivory-billed woodpecker at the same time. The secrecy of the search permitted The Nature Conservancy and Cornell University to quietly buy up Ivory-billed habitat to add to the 120,000 acre of the Big Woods protected by the Conservancy.
A large woodpecker was videotaped on 25 April 2004; its size, wing pattern at rest and in flight, and white plumage on its back between the wings were cited as evidence that the woodpecker sighted was an ivory-billed woodpecker. That same video included an earlier image of what was suggested to be such a bird perching on a Water Tupelo ("Nyssa aquatica").
The report also notes that drumming consistent with that of ivory-billed woodpecker had been heard in the region. It describes the potential for a thinly distributed population in the area, though no birds have been located away from the primary site.
In the fall of 2006, researchers developed and installed an "autonomous observatory" using robotic video cameras with image processing software that detects and records high resolution video of birds in flight inside a high probability zone in the Cache River area. As of August 2007, hundreds of birds have been recorded, including pileated woodpeckers, but not the ivory-billed woodpecker.
Debate.
In June 2005, ornithologists at Yale University, the University of Kansas, and Florida Gulf Coast University prepared a scientific paper skeptical of the initial reports of rediscovery.
We were very skeptical of the first published reports, and ... data were not sufficient to support this startling conclusion.
The paper was not published. Questions about the evidence for ivory-billed woodpecker persisted. The CLO authors could not say with absolute certainty that the sounds recorded in Arkansas were made by Ivory-bills. Some skeptics, including Richard Prum, believe the video could have been of a pileated woodpecker.
An article by Dina Cappiello in the "Houston Chronicle" published 18 December 2005 presented Richard Prum's position as follows:
Prum, intrigued by some of the recordings taken in Arkansas' Big Woods, said the evidence thus far is refutable.
The American Birding Association largely stayed out of the debate. On page 13 of "Winging It" (November/December 2005), a brief reference was made:
The ABA Checklist Committee has not changed the status of the Ivory-billed Woodpecker from Code 6 (EXTINCT) to another level that would reflect a small surviving population. The Committee is waiting for unequivocal proof that the species still exists.
In a "Perspectives in Ornithology" commentary published in "The Auk" in January 2006, ornithologist Jerome Jackson detailed his skepticism of the Ivory-bill evidence:
Prum, Robbins, Brett Benz, and I remain steadfast in our belief that the bird in the Luneau video is a normal Pileated Woodpecker. Others have independently come to the same conclusion, and publication of independent analyses may be forthcoming [...] For scientists to label sight reports and questionable photographs as 'proof' of such an extraordinary record is delving into 'faith-based' ornithology and doing a disservice to science."
Fitzpatrick and co-authors responded with a lengthy piece in the same scientific journal, protesting Jackson's harsh language, dismissive tone, "factual errors," and "poorly substantiated opinions" about the original paper. One of the most rancorous debates in the history of ornithology had begun in earnest. The two sides each published additional responses that seemed, to many ornithological observers, to have departed markedly from accepted scientific decorum. Jackson accused the Fitzpatrick team of "untruths", and Fitzpatrick accused Jackson of obviating the normal peer-review system with an opinion piece "treated as a scientific contribution by the public media."
In March 2006, a team headed by David A. Sibley of Concord, MA published a response in the journal "Science", asserting that the videotape was most likely of a pileated woodpecker, with mistakes having been made in the interpretation of its posture. They conclude that it lacked certain features of an ivory-billed woodpecker, and had others consistent with the pileated; they asserted positively that the blurry video images belonged to pileated woodpecker. The CLO team responded in the same issue of "Science", standing by their original findings, stating:
Claims that the bird in the Luneau video is a normal pileated woodpecker are based on misrepresentations of a pileated's underwing pattern, interpretation of video artifacts as plumage pattern, and inaccurate models of takeoff and flight behavior. These claims are contradicted by experimental data and fail to explain evidence in the Luneau video of white dorsal plumage, distinctive flight behavior, and a perched woodpecker with white upper parts."
Other workers made claims disputing the validity of the Luneau video, including a web site discussing the evidence by Colby College biologist Louis Bevier, who stated:
In sum, no evidence confirms the alleged rediscovery of the ivory-billed woodpecker. Indeed, confidence in the claim has eroded with failure to verify its existence despite massive searches.
A 2007 paper concluded that the Luneau video was consistent with the pileated woodpecker:
New video analysis of Pileated Woodpeckers in escape flights comparable to that of the putative Ivory-billed Woodpecker filmed in Arkansas shows that Pileated Woodpeckers can display a wingbeat frequency equivalent to that of the Arkansas bird during escape flight. The critical frames from the Arkansas video that were used to identify the bird as an Ivory-billed Woodpecker are shown to be equally, or more, compatible with the Pileated Woodpecker.…The identification of the bird filmed in Arkansas in April 2004 as an Ivory-billed Woodpecker is best regarded as unsafe. The similarities between the Arkansas bird and known Pileated Woodpeckers suggest that it was most likely a Pileated Woodpecker.
Doubt was also cast on some of the auditory evidence (ARU recordings of double-raps) for the presence of ivory-billed woodpeckers in Arkansas and Florida. One group of researchers stated:
All ARU double raps suggesting the presence of an Ivory-billed Woodpecker should be reconsidered in light of the phenomenon of duck wingtip collisions, especially those recorded in the winter months, when duck flocks are common across flooded bottomlands of the southeastern United States.
The recurring unverified sightings of the ivory-billed woodpecker have been cast in terms of cryptozoology.
Cornell search efforts 2005–09.
Cornell-organized searches in Arkansas and elsewhere from 2005 to 2008 did not produce any new photographic evidence of the species. The press release summarizing the 2005–6 search season stated:
There were teasing glimpses and tantalizing sounds, but the 2005–2006 search for the Ivory-billed Woodpecker in Arkansas has concluded without the definitive visual documentation being sought. The search, led by the Cornell Lab of Ornithology, with support from Audubon Arkansas, stretched from November through April when ivory-bill activity would be highest and a lack of leaf-cover permitted clear views through the dense forest.…
“The search teams were very skilled, not only technically but in the execution of the search,” said Dr. John Fitzpatrick, director of the Cornell Lab of Ornithology. “Even though we didn’t get additional definitive evidence of the ivory-bill in Arkansas, we’re not discouraged. The vastness of the forest combined with the highly mobile nature of the bird warrant additional searching.”
It is interesting to note that, despite his harsh criticism of the 2005 evidence, Jerome A. Jackson agrees with the value of additional searches. In May 2006, it was announced that a large search effort led by the Cornell team had been suspended for the season with only a handful of unconfirmed, fleeting sightings to report. At that point, conservation officers allowed the public back into areas of the Cache River National Wildlife Refuge that had been restricted upon the initial reported sightings.
The 2006–07 search season had similar results to those of the previous year:
The Lab and its partners concluded the 2006–07 field season in Arkansas at the end of April with no additional definitive evidence of ivory-bills to complement the data gathered in 2004 and 2005. But [Ronald] Rohrbaugh and others are convinced the research should continue, not only in Arkansas, but in other states that are part of the bird’s historic range. “We’ll return to Arkansas for at least another field season,” says Rohrbaugh. “Searches there and searches conducted by other agencies throughout the Southeast are still turning up reports of sounds that cannot be explained away. However, there’s no way to know for sure yet if reported double knocks and kent-like sounds were made by an ivory-bill or something else.”
Likewise, the 2007–08 search season did not deliver conclusive evidence of the bird:
The search teams covered lots of ground and tried new survey techniques…. Searchers documented more possible sightings and possible ivory-bill double knocks heard, but the definitive photograph, like the bird itself, remained elusive.
Cornell University did not field a search team in Arkansas during 2008–2009, but focused on mangrove habitats in southwest Florida, with a later visit planned for South Carolina. According to a Cornell University press release from January 2009, the 2008–09 season will be the last Cornell-sponsored search, absent confirmation of the bird:
There will be a distinctly different flavor to this season’s search for the Ivory-billed Woodpecker. Seven members of the Cornell Lab of Ornithology’s mobile search team will plunge into some of the most forbidding wilderness in southwestern Florida. …The work begins in Florida in early January and continues through mid-March. …In mid-March the Cornell Lab of Ornithology team will join the South Carolina search along the Congaree, PeeDee, and Santee Rivers.
“The U.S. Fish and Wildlife Service-funded Ivory-billed Woodpecker searches will continue through the 2008–09 search season,” says Laurie Fenwood, Ivory-billed Woodpecker Recovery Team Coordinator for the U.S, Fish and Wildlife Service.…If no birds are confirmed, the Cornell Lab of Ornithology will not send an organized team into the field next year. “We remain committed to our original goal of striving to locate breeding pairs,” says Cornell Lab of Ornithology director John Fitzpatrick. “We will continue to accept and investigate credible reports of Ivory-billed Woodpeckers, and to promote protection and restoration of the old growth conditions upon which this magnificent species depended across the entire southeastern United States.”
The 2008–09 search effort in southwest Florida found no evidence of the bird:
We have found no signs of Ivory-billed Woodpeckers. No sightings, double knocks or calls, no replies to our many double-knock imitations. We have seen a few cavities of the appropriate size and shape for ivory-bills, but these can be old, or exceptionally large Pileated Woodpecker cavities, or mammal-enlarged Pileated Woodpecker cavities.… Given the results, it is unlikely a population of any meaningful size of Ivory-billed Woodpeckers exists in south Florida.
In October 2009, Cornell scientists announced that their search for the ivory-billed woodpecker in North America was being suspended. As of February 2010, the Cornell researchers concluded there was no hope of saving the bird, if it still exists:
But after five years of fruitless searching, hopes of saving the species have faded. "We don't believe a recoverable population of ivory-billed woodpeckers exists," says Ron Rohrbaugh, a conservation biologist at Cornell University in Ithaca, New York, who headed the original search team.
2005/2006 Florida reports.
In September 2006, new claims that the ivory-billed woodpecker may not be extinct were released by a research group consisting of members from Auburn University in Alabama and the University of Windsor in Ontario. Dr. Geoffrey E. Hill of Auburn University and Dr. Daniel Mennill of the University of Windsor have revealed a collection of evidence that the birds may still exist in the cypress swamps of the Florida panhandle. Their evidence includes 14 sightings of the birds and 300 recordings of sounds that can be attributed to the ivory-billed woodpecker, but also includes tell-tale foraging signs and appropriately sized tree nest cavities (Hill "et al.", 2006). This evidence remains inconclusive as it excludes the photographic or DNA evidence that many experts cite as necessary before the presence of the species can be confirmed. While Dr. Hill and Dr. Mennill are themselves convinced of the bird's existence in Florida, they are quick to acknowledge that they have not yet conclusively proven the species' existence. The research team is currently undertaking a more complete survey of the Choctawhatchee River, in hopes of obtaining photographic evidence of the bird's existence. In March 2007 the Florida Ornithological Society Records Committee voted unanimously not to accept the 2005–06 reports of the ivory-billed woodpecker on the Choctawhatchee River:
RC 06-610. Ivory-billed Woodpecker, "Campephilus principalis." 21 May 2005 – 26 April 2006. Choctawhatchee River, Washington/Bay/Walton cos. A population of unknown size has been reported by a team from Auburn University from the lower Choctawhatchee River. There have been a few sightings but no photographs, some interesting recordings of “kent” calls and of double rap drums, and photographs taken of cavities and bark scaling. These observations were made on the heels of the much-publicized “rediscovery” of the species in Arkansas (Fitzpatrick et al. 2005). The species had not been documented to occur since 1944. The video documentation of the bird(s) from Arkansas, however, has been debated by many, although the record was accepted by the Arkansas Bird Records Committee. Our Committee felt that given the controversy of the Arkansas evidence, the species is best considered still extinct. Therefore only evidence that undoubtedly showed a living bird would be considered sufficient to accept a report. 
The last specimen taken in Florida was in 1925; there have been numerous sight reports of varying credibility since, and one record of a feather found in a nest cavity in 1968 that was identified as an Ivory-billed Woodpecker inner secondary by Alexander Wetmore.
VOTE: NOT ACCEPT (0–7)
The Auburn/University of Windsor team continued search efforts but planned to cease updates on their web site in August 2009:
"(12 June 2008)" We completed our 2008 effort to get definitive evidence for ivorybills in the Choctawhatchee River Basin in early May…. Team members had no sightings of ivorybills and only two sound detections in 2008.… So where does all this leave us? Pretty much in the same position as in June 2006. We have a large body of evidence that Ivory-billed Woodpeckers persist along the Choctawhatchee River in the Florida panhandle, but we do not have definitive proof that they exist. Either the excitement of the ivorybill hunt causes competent birders to see and hear things that do not exist and leads competent sound analysts to misidentify hundreds of recorded sounds, or the few ivorybills in the Choctawhatchee River Basin are among the most elusive birds on the planet.
"(9 February 2009)" There has been little to report, and my students and I [Geoff Hill] have been enjoying the calm. We continue to work to get definitive documentation of the Ivory-billed Woodpecker in the Choctawhatchee River Basin.… To my knowledge, there have been no sightings of Ivory-billed Woodpeckers in the Choctawhatchee region since last spring. There were a few double knock detections in January, but not by my paid crew, Brian [Rolek], or me.
"(2 August 2009)" I haven’t posted many updates on this site in the past 9 months because there hasn’t not ["sic"] been much to report.… Since the winter of 2008, we have had few sightings or sound detections by anyone—none by Brian or me—and none that I would rate very highly.… In short, our experience over the past year indicates that ivorybills have moved out of the areas where we encountered them from 2005 to 2008.… There is no way to know whether the birds are in different areas in the Choctawhatchee Basin, different forests in the region, or dead.…I won’t post any more updates on this site.
Publicity and tourism.
In economically struggling east Arkansas, the speculation of a possible return of the Ivory-bill has served as a great source of economic exploitation, with tourist spending up 30%, primarily in and around the city of Brinkley, Arkansas. A woodpecker "festival", a woodpecker hairstyle (a sort of mohawk with red, white, and black dye), and an "Ivory-bill Burger" (made with 100% beef) have been featured locally. The lack of confirmed proof of the bird's existence, and the extremely small chance of actually seeing the bird even if it does exist (especially since the exact locations of the reported sightings are still guarded), have prevented the explosion in tourism some locals had anticipated.
Brinkley hosted "The Call of the Ivory-billed Woodpecker Celebration" in February 2006. The celebration included exhibits, birding tours, educational presentations, a vendor market, and more.
Interviews with residents of Brinkley, Arkansas, heard on National Public Radio following the reported rediscovery were shared with musician Sufjan Stevens, who used the material to write a song titled "The Lord God Bird".
Arkansas has made license plates featuring a graphic of an Ivory-billed Woodpecker.

</doc>
<doc id="14996" url="http://en.wikipedia.org/wiki?curid=14996" title="International English">
International English

International English is the concept of the English language as a global means of communication in numerous dialects, and also the movement towards an international standard for the language. It is also referred to as Global English, World English, Common English, Continental English, General English, Engas (English as associate language), or Globish. Sometimes, these terms refer simply to the array of varieties of English spoken throughout the world.
Sometimes, "international English" and the related terms above refer to a desired standardisation, i.e. Standard English; however, there is no consensus on the path to this goal. There have been many proposals for making International English more accessible to people from different nationalities. Basic English is an example, but it failed to make progress. More recently, there have been proposals for English as a lingua franca (ELF). It has also been argued that International English is held back by its traditional spelling. There has been slow progress in adopting alternate spellings.
Historical context.
The modern concept of International English does not exist in isolation, but is the product of centuries of development of the English language.
The English language evolved in England, from a set of West Germanic dialects spoken by the Angles and Saxons, who arrived from continental Europe in the 5th century. Those dialects came to be known as "Englisc" (literally "Anglish"), the language today referred to as Anglo-Saxon or Old English (the language of the poem "Beowulf"). English is thus more closely related to West Frisian than to any other modern language, although less than a quarter of the vocabulary of Modern English is shared with West Frisian or other West Germanic languages because of extensive borrowings from Norse, Norman, Latin, and other languages. It was during the Viking invasions of the Anglo-Saxon period that Old English was influenced by contact with Norse, a group of North Germanic dialects spoken by the Vikings, who came to control a large region in the North of England known as the Danelaw. Vocabulary items entering English from Norse (including the pronouns "they", and "them") are thus attributable to the on-again-off-again Viking occupation of Northern England during the centuries prior to the Norman Conquest (see, e.g., Canute the Great). Soon after the Norman Conquest of 1066, the "Englisc" language ceased being a literary language (see, e.g., Ormulum) and was replaced by Anglo-Norman as the written language of England. During the Norman Period, English absorbed a significant component of French vocabulary (approximately one-third of the vocabulary of Modern English). With this new vocabulary, additional vocabulary borrowed from Latin (with Greek, another approximately one-third of Modern English vocabulary, though some borrowings from Latin and Greek date from later periods), a simplified grammar, and use of the orthographic conventions of French instead of Old English orthography, the language became Middle English (the language of Chaucer). The "difficulty" of English as a written language thus began in the High Middle Ages, when French orthographic conventions were used to spell a language whose original, more suitable orthography had been forgotten after centuries of nonuse. During the late medieval period, King Henry V of England (lived 1387-1422) ordered the use of the English of his day in proceedings before him and before the government bureaucracies. That led to the development of Chancery English, a standardised form used in the government bureaucracy. (The use of so-called Law French in English courts continued through the Renaissance, however.)
The emergence of English as a language of Wales results from the incorporation of Wales into England and also dates from approximately this time period. Soon afterward, the development of printing by Caxton and others accelerated the development of a standardised form of English. Following a change in vowel pronunciation that marks the transition of English from the medieval to the Renaissance period, the language of the Chancery and Caxton became Early Modern English (the language of Shakespeare's day) and with relatively moderate changes eventually developed into the English language of today. Scots, as spoken in the lowlands and along the east coast of Scotland, developed independently from Modern English and is based on the Northern dialects of Anglo-Saxon, particularly Northumbrian, which also serve as the basis of Northern English dialects such as those of Yorkshire and Newcastle upon Tyne. Northumbria was within the Danelaw and therefore experienced greater influence from Norse than did the Southern dialects. As the political influence of London grew, the Chancery version of the language developed into a written standard across Great Britain, further progressing in the modern period as Scotland became united with England as a result of the Acts of Union of 1707.
There have been two introductions of English to Ireland, a medieval introduction that led to the development of the now-extinct Yola dialect and a modern introduction in which Hibernian English largely replaced Irish as the most widely spoken language during the 19th century, following the Act of Union of 1800. Received Pronunciation (RP) is generally viewed as a 19th-century development and is not reflected in North American English dialects, which are based on 18th-century English.
The establishment of the first permanent English-speaking colony in North America in 1607 was a major step towards the globalisation of the language. British English was only partially standardised when the American colonies were established. Isolated from each other by the Atlantic Ocean, the dialects in England and the colonies began evolving independently.
The British colonisation of Australia in 1788 brought the English language to Oceania. By the 19th century, the standardisation of British English was more settled than it had been in the previous century, and this relatively well-established English was brought to Africa, Asia and New Zealand. It developed both as the language of English-speaking settlers from Britain and Ireland, and as the administrative language imposed on speakers of other languages in the various parts of the British Empire. The first form can be seen in New Zealand English, and the latter in Indian English. In Europe, English received a more central role particularly since 1919, when the Treaty of Versailles was composed not only in French, the common language of diplomacy at the time, but, under special request from American president Woodrow Wilson, also in English - a major milestone in the globalisation of English.
The English-speaking regions of Canada and the Caribbean are caught between historical connections with the UK and the Commonwealth and geographical and economic connections with the U.S. In some things they tend to follow British standards, whereas in others, especially commercial, they follow the U.S. standard.
English as a global language.
Braj Kachru divides the use of English into three concentric circles.
The "inner circle" is the traditional base of English and includes countries such as the United Kingdom and Ireland and the anglophone populations of the former British colonies of the United States, Australia, New Zealand, South Africa, Canada, and various islands of the Caribbean, Indian Ocean, and Pacific Ocean.
In the "outer circle" are those countries where English has official or historical importance ("special significance"). This includes most of the countries of the Commonwealth of Nations (the former British Empire), including populous countries such as India, Pakistan, and Nigeria; and others, such as the Philippines, under the sphere of influence of English-speaking countries. Here English may serve as a useful lingua franca between ethnic and language groups. Higher education, the legislature and judiciary, national commerce, and so on, may all be carried out predominantly in English.
The "expanding circle" refers to those countries where English has no official role, but is nonetheless important for certain functions, e.g. international business and tourism. By the twenty-first century, the number of non-native English speakers has come to significantly outnumber the number of native speakers by a factor of three, according to the British Council. Darius Degher, a professor at Malmö University in Sweden, uses the term "decentered English" to describe this shift, along with attendant changes in what is considered to be important to English users and learners.
Research on English as a lingua franca in the sense of "English in the Expanding Circle" is comparatively recent. Linguists who have been active in this field are Jennifer Jenkins, Barbara Seidlhofer, Christiane Meierkord and Joachim Grzega.
English as a lingua franca in foreign language teaching.
English as an additional language (EAL) is usually based on the standards of either American English or British English as well as incorporating foreign terms. English as an international language (EIL) is EAL with emphasis on learning different major dialect forms; in particular, it aims to equip students with the linguistic tools to communicate internationally. Roger Nunn considers different types of competence in relation to the teaching of English as an International Language, arguing that linguistic competence has yet to be adequately addressed in recent considerations of EIL.
Several models of "simplified English" have been suggested for teaching English as a foreign language:
Furthermore, Randolph Quirk and Gabriele Stein thought about a Nuclear English, which, however, has never been fully developed.
With reference to the term "Globish", Robert McCrum has used this to mean "English as global language". Jean-Paul Nerriere uses it for a constructed language.
Basic Global English.
Basic Global English, or BGE, is a concept of global English initiated by German linguist Joachim Grzega. It evolved from the idea of creating a type of English that can be learned more easily than regular British or American English and that serves as a tool for successful global communication. BGE is guided by creating "empathy and tolerance" between speakers in a global context. This applies to the context of global communication, where different speakers with different mother tongues come together. BGE aims to develop this competence as quickly as possible.
English language teaching is almost always related to a corresponding culture, e. g. learners will either deal with American English and therefore with American culture or British English and therefore with British culture. Basic Global English is supposed to solve this problem by creating one collective version of English. Additionally, it is a system that is suited for self-teaching as well as regular teaching.
BGE is based on 20 elementary grammar rules that provide a certain degree of variation. For example, regular as well as irregular formed verbs are accepted. Pronunciation rules are not as strict as in British or American English, so there is a certain degree of variation for the learners. Exceptions that cannot be used are pronunciations that would be harmful to mutual understanding and therefore minimize the success of communication.
Basic Global English is based on a 750-word vocabulary. Additionally, every learner has to acquire the knowledge of 250 additional words. These words can be chosen freely, according the specific needs and interests of the learner.
BGE provides not only basic language skills, but also so called "Basic Politeness Strategies". These include creating a positive atmosphere, accepting an offer with "Yes, please" or refusing with "No, thank you", and small talk topics to choose and to avoid.
Basic Global English has been tested in two elementary schools in Germany. For the practical test of BGE, 12 lessons were prepared in order to cover half of a school year. After the BGE teaching, students were able to answer questions about themselves, their family, their hobbies etc. Additionally they were able to form questions themselves about the same topics. Besides that, they also learned the numbers from 1 to 31 and vocabulary including things in their school bag and in their classroom. The students as well as the parents had a positive impression of the project.
Varying concepts.
Universality and flexibility.
International English sometimes refers to English as it is actually being used and developed in the world; as a language owned not just by native speakers, but by all those who come to use it.
Basically, it covers the English language at large, often (but not always or necessarily) implicitly seen as standard. It is certainly also commonly used in connection with the acquisition, use, and study of English as the world's lingua franca ('TEIL: Teaching English as an International Language'), and especially when the language is considered as a whole in contrast with "British English", "American English", "South African English", and the like. — McArthur (2002, p. 444–445)
It especially means English words and phrases generally understood throughout the English-speaking world as opposed to localisms. The importance of non-native English language skills can be recognised behind the long-standing joke that the international language of science and technology is broken English.
Neutrality.
International English reaches towards cultural neutrality. This has a practical use:
"What could be better than a type of English that saves you from having to re-edit publications for individual regional markets! Teachers and learners of English as a second language also find it an attractive idea — both often concerned that their English should be neutral, without American or British or Canadian or Australian coloring. Any regional variety of English has a set of political, social and cultural connotations attached to it, even the so-called 'standard' forms."
According to this viewpoint, International English is a concept of English that minimises the aspects defined by either the colonial imperialism of Victorian Britain or the cultural imperialism of the 20th century United States. While British colonialism laid the foundation for English over much of the world, International English is a product of an emerging world culture, very much attributable to the influence of the United States as well, but conceptually based on a far greater degree of cross-talk and linguistic transculturation, which tends to mitigate both U.S. influence and British colonial influence.
The development of International English often centres on academic and scientific communities, where formal English usage is prevalent, and creative use of the language is at a minimum. This formal International English allows entry into Western culture as a whole and Western cultural values in general.
Opposition.
The continued growth of the English language itself is seen by authors such as Alistair Pennycook as a kind of cultural imperialism, whether it is English in one form or English in two slightly different forms.
Robert Phillipson argues against the possibility of such neutrality in his "Linguistic Imperialism" (1992). Learners who wish to use purportedly correct English are in fact faced with the dual standard of American English and British English, and other less known standard Englishes (including Australian, Scottish and Canadian).
Edward Trimnell, author of "Why You Need a Foreign Language & How to Learn One" (2005) argues that the international version of English is only adequate for communicating basic ideas. For complex discussions and business/technical situations, English is not an adequate communication tool for non-native speakers of the language. Trimnell also asserts that native English-speakers have become "dependent on the language skills of others" by placing their faith in international English.
Appropriation theory.
There are also some who reject both linguistic imperialism and David Crystal's theory of the neutrality of English. They argue that the phenomenon of the global spread of English is better understood in the framework of appropriation (e.g. Spichtinger 2000), that is, English used for local purposes around the world. Demonstrators in non-English speaking countries often use signs in English to convey their demands to TV-audiences around the globe, for instance.
In English-language teaching Bobda shows how Cameroon has moved away from a mono-cultural, Anglo-centered way of teaching English and has gradually appropriated teaching material to a Cameroonian context. Non-Western topics treated are, for instance, the rule of Emirs, traditional medicine or polygamy (1997:225). Kramsch and Sullivan (1996) describe how Western methodology and textbooks have been appropriated to suit local Vietnamese culture. The Pakistani textbook "Primary Stage English" includes lessons such as "Pakistan My Country", "Our Flag", or "Our Great Leader" (Malik 1993: 5,6,7) which might well sound jingoistic to Western ears. Within the native culture, however, establishing a connection between English Language Teaching (ELT), patriotism and Muslim faith is seen as one of the aims of ELT, as the chairman of the Punjab Textbook Board openly states: "The board ... takes care, through these books to inoculate in the students a love of the Islamic values and awareness to guard the ideological frontiers of your [the students] home lands" (Punjab Text Book Board 1997).
Many Englishes.
There are many difficult choices that have to be made if there is to be further standardisation of English in the future. These include the choice over whether to adopt a current standard, or move towards a more neutral, but artificial one. A true International English might supplant both current American and British English as a variety of English for international communication, leaving these as local dialects, or would rise from a merger of General American and standard British English with admixture of other varieties of English and would generally replace all these varieties of English.
We may, in due course, all need to be in control of two standard Englishes—the one which gives us our national and local identity, and the other which puts us in touch with the rest of the human race. In effect, we may all need to become bilingual in our own language. — David Crystal (1988: p. 265)
This is the situation long faced by many users of English who possess a "non-standard" dialect of English as their birth tongue but have also learned to write (and perhaps also speak) a more standard dialect. Many academics often publish material in journals requiring different varieties of English and change style and spellings as necessary without great difficulty.
As far as spelling is concerned, the differences between American and British usage became noticeable due to the first influential lexicographers (dictionary writers) on each side of the Atlantic. Samuel Johnson's dictionary of 1755 greatly favoured Norman-influenced spellings such as "centre" and "colour"; on the other hand, Noah Webster's first guide to American spelling, published in 1783, preferred spellings like "center" and the Latinate "color". The difference in strategy and philosophy of Johnson and Webster are largely responsible for the main division in English spelling that exists today. However, these differences are extremely minor. Spelling is but a small part of the differences between dialects of English, and may not even reflect dialect differences at all (except in phonetically spelled dialogue). International English refers to much more than an agreed spelling pattern.
Dual standard.
Two approaches to International English are the individualistic and inclusive approach and the new dialect approach.
The individualistic approach gives control to individual authors to write and spell as they wish (within purported standard conventions) and to accept the validity of differences. The "Longman Grammar of Spoken and Written English", published in 1999, is a descriptive study of both American and British English in which each chapter follows individual spelling conventions according to the preference of the main editor of that chapter.
The new dialect approach appears in "The Cambridge Guide to English Usage" (Peters, 2004) which attempts to avoid any language bias and accordingly uses an idiosyncratic international spelling system of mixed American and British forms (but tending to prefer the American English spellings).
References.
</dl>

</doc>
<doc id="14997" url="http://en.wikipedia.org/wiki?curid=14997" title="International African Institute">
International African Institute

The International African Institute (IAI) was founded (as the International Institute of African Languages and Cultures [IIALC]) in 1926 in London for the study of African languages. Frederick Lugard was the first chairman (1926 to his death in 1945); Diedrich Hermann Westermann (1926 to 1939) and Maurice Delafosse (1926) were the initial co-directors.
Since 1928, the IAI has published a quarterly journal, "Africa".
The IAI's mission is "to promote the education of the public in the study of Africa and its languages and cultures". Its operations includes seminars, journals, monographs, edited volumes and stimulating scholarship within Africa.
Archives.
The archives of the International African Institute are held at the of the Library of the London School of Economics. An of these papers is available.
History.
Africa alphabet.
In 1928, the IAI (then IIALC) published an "Africa Alphabet" to facilitate standardization of Latin-based writing systems for African languages.
Prize for African language literature, 1929-50.
From April 1929 to 1950, the IAI offered prizes for works of literature in African languages.

</doc>
<doc id="14998" url="http://en.wikipedia.org/wiki?curid=14998" title="IAI">
IAI

IAI is an acronym for:

</doc>
<doc id="15000" url="http://en.wikipedia.org/wiki?curid=15000" title="Insulin-like growth factor">
Insulin-like growth factor

The insulin-like growth factors (IGFs) are proteins with high sequence similarity to insulin. IGFs are part of a complex system that cells use to communicate with their physiologic environment. This complex system (often referred to as the IGF "axis") consists of two cell-surface receptors (IGF1R and IGF2R), two ligands (Insulin-like growth factor 1 (IGF-I) and Insulin-like growth factor 2 (IGF-2)), a family of six high-affinity IGF-binding proteins (IGFBP-1 to IGFBP-6), as well as associated IGFBP degrading enzymes, referred to collectively as proteases.
IGF1/GH Axis.
The IGF "axis" is also commonly referred to as the Growth Hormone/IGF-I Axis. Insulin-like growth factor 1 (IGF-1) is mainly secreted by the liver as a result of stimulation by growth hormone (GH). IGF-I is important for both the regulation of normal physiology, as well as a number of pathological states, including cancer. The IGF axis has been shown to play roles in the promotion of cell proliferation and the inhibition of cell death (apoptosis). Insulin-like growth factor 2 (IGF-2) is thought to be a primary growth factor required for early development while IGF-1 expression is required for achieving maximal growth. Gene knockout studies in mice have confirmed this, though other animals are likely to regulate the expression of these genes in distinct ways. While IGF-2 may be primarily fetal in action it is also essential for development and function of organs such as the brain, liver, and kidney.
Factors that are thought to cause variation in the levels of GH and IGF-1 in the circulation include an individual's genetic make-up, the time of day, age, sex, exercise status, stress levels, nutrition level, body mass index (BMI), disease state, race, estrogen status, and xenobiotic intake.
IGF-I has an involvement in regulating neural development including neurogenesis, myelination, synaptogenesis, and dendritic branching and neuroprotection after neuronal damage. Increased serum levels of IGF-I in children have been associated with higher IQ.
IGF-I shapes the development of the cochlea through controlling apoptosis. Its deficit can cause hearing loss. Serum level of it also underlies a correlation between short height and reduced hearing abilities particularly around 3–5 years of age, and at age 18 (late puberty).
IGF receptors.
The IGFs are known to bind the IGF-1 receptor, the insulin receptor, the IGF-2 receptor, the insulin-related receptor and possibly other receptors. The IGF-1 receptor is the "physiological" receptor—IGF-I binds to it at significantly higher affinity than it binds the insulin receptor. Like the insulin receptor, the IGF-I receptor is a receptor tyrosine kinase—meaning the receptor signals by causing the addition of a phosphate molecule on particular tyrosines. The IGF-2 receptor only binds IGF-2 and acts as a "clearance receptor"—it activates no intracellular signaling pathways, functioning only as an IGF-2 sequestering agent and preventing IGF-2 signaling.
Organs and tissues affected by IGF-I.
Since many distinct tissue types express the IGF-1 receptor, IGF-1's effects are diverse. It acts as a neurotrophic factor, inducing the survival of neurons. It may catalyse skeletal muscle hypertrophy, by inducing protein synthesis, and by blocking muscle atrophy. It is protective for cartilage cells, and is associated with activation of osteocytes, and thus may be an anabolic factor for bone. Since at high concentrations it is capable of activating the insulin receptor, it can also complement for the effects of insulin. Receptors for IGF-I are found in vascular smooth muscle, while typical receptors for insulin are not found in vascular smooth muscle.
IGF-Binding Proteins.
IGF-1 and IGF-2 are regulated by a family of proteins known as the IGF-Binding Proteins. These proteins help to modulate IGF action in complex ways that involve both inhibiting IGF action by preventing binding to the IGF-1 receptor as well as promoting IGF action possibly through aiding in delivery to the receptor and increasing IGF half-life. Currently, there are six characterized IGF Binding Proteins (IGFBP-1 to IGFBP-6). There is currently significant data suggesting that IGFBPs play important roles in addition to their ability to regulate IGFs. 
IGF-I and IGFBP-3 are GH dependent, whereas IGFBP-1 is insulin regulated.
IGFBP-1 production from the liver is significantly elevated during insulinopenia while serum levels of bioactive IGF-I is increased by insulin.
Diseases affected by IGF.
Studies of recent interest show that the Insulin/IGF axis play an important role in aging. Nematodes, fruit-flies, and other organisms have an increased life span when the gene equivalent to the mammalian insulin is knocked out. It is somewhat difficult to relate this finding to the mammals, however, because in the smaller organism there are many genes (at least 37 in the nematode "Caenorhabditis elegans") that are "insulin-like" or "IGF-1-like", whereas in the mammals insulin-like proteins comprise only seven members (insulin, IGFs, relaxins, EPIL, and relaxin-like factor). The human insulin-like genes have apparently distinct roles with some but less crosstalk presumably because there are multiple insulin-receptor-like proteins in humans. Simpler organisms typically have fewer receptors; for example, only one insulin-like receptor exists in the nematode "C. elegans". Additionally, "C. elegans" do not have specialized organs such as the (Islets of Langerhans), which sense insulin in response to glucose homeostasis. Moreover, IGF1 affects lifespan in nematodes by causing dauer formation, a developmental stage of C. elegans larva. There is no mammalian correlate. Therefore it is an open question as to whether either IGF-1 or insulin in the mammal may perturb aging, although there is the suggestion that dietary restriction phenomena may be related.
Other studies are beginning to uncover the important role the IGFs play in diseases such as cancer and diabetes, showing for instance that IGF-1 stimulates growth of both prostate and breast cancer cells. Researchers are not in complete agreement about the degree of cancer risk that IGF-1 poses.

</doc>
<doc id="15001" url="http://en.wikipedia.org/wiki?curid=15001" title="IGF">
IGF

IGF may stand for:

</doc>
<doc id="15004" url="http://en.wikipedia.org/wiki?curid=15004" title="Idiot">
Idiot

An idiot, dolt, dullard or (archaically) mome is an intellectually disabled person, or someone who acts in a self-defeating or significantly counterproductive way. The similar terms moron, imbecile, and cretin have all gained specialized meanings in modern times. An idiot is said to be idiotic, and to suffer from idiocy. A dunce is an idiot who is specifically incapable of learning. An idiot differs from a fool (who is unwise) and an ignoramus (who is uneducated/an ignorant), neither of which refers to someone with low intelligence. In modern English usage, the terms "idiot" and "idiocy" describe an extreme folly or stupidity, and its symptoms (foolish or stupid utterance or deed). In psychology, it is a historical term for the state or condition now called profound intellectual disability.
Etymology.
Idiot is a word derived from the Greek ἰδιώτης, "idiōtēs" ("person lacking professional skill", "a private citizen", "individual"), from ἴδιος, "idios" ("private", "one's own"). In Latin the word "idiota" ("ordinary person, layman") preceded the Late Latin meaning "uneducated or ignorant person". Its modern meaning and form dates back to Middle English around the year 1300, from the Old French "idiote" ("uneducated or ignorant person"). The related word "idiocy" dates to 1487 and may have been analogously modeled on the words prophet and prophecy. The word has cognates in many other languages.
An idiot in Athenian democracy was someone who was characterized by "self-centeredness" and concerned almost exclusively with "private"—as opposed to "public"—affairs. Idiocy was the natural state of ignorance into which all persons were born and its opposite, citizenship, was effected through formalized education. In Athenian democracy, idiots were "born" and citizens were "made" through education (although citizenship was also largely hereditary). "Idiot" originally referred to "layman, person lacking professional skill", "person so mentally deficient as to be incapable of ordinary reasoning". Declining to take part in public life, such as democratic government of the polis (city state), was considered dishonorable. "Idiots" were seen as having bad judgment in public and political matters. Over time, the term "idiot" shifted away from its original connotation of selfishness and came to refer to individuals with overall bad judgment–individuals who are "stupid". According to the Bauer-Danker Lexicon, the noun ίδιωτής in ancient Greek meant "civilian" (ref Josephus Bell 2 178), "private citizen" (ref sb 3924 9 25), "private soldier as opposed to officer," (Polybius 1.69), "relatively unskilled, not clever," (Herodotus 2,81 and 7 199). The military connotation in Bauer's definition stems from the fact that ancient Greek armies in the time of total war mobilized all male citizens (to the age of 50) to fight, and many of these citizens tended to fight poorly and ignorantly.
Disability.
In 19th and early 20th century medicine and psychology, an "idiot" was a person with a very severe intellectual disability. In the early 1900s, Dr. Henry H. Goddard proposed a classification system for intellectual disability based on the Binet-Simon concept of mental age. Individuals with the lowest mental age level (less than three years) were identified as "idiots"; "imbeciles" had a mental age of three to seven years, and "morons" had a mental age of seven to ten years. The term "idiot" was used to refer to people having an IQ below 30. IQ, or intelligence quotient, was originally determined by dividing a person's mental age, as determined by standardized tests, by their actual age. The concept of mental age has fallen into disfavor, though, and IQ is now determined on the basis of statistical distributions.
In current American medical classification, these people are now said to have "profound intellectual disability" but this term is not in use in the United Kingdom.
United States law.
Until 2007, the California Penal Code Section 26 stated that "Idiots" were one of six types of people who are not capable of committing crimes. In 2007 the code was amended to read "persons who are mentally incapacitated." In 2008, Iowa voters passed a measure replacing "idiot, or insane person" in the State's constitution with "person adjudged mentally incompetent."
In several U.S. states, "idiots" do not have the right to vote:
The constitution of the state of Arkansas was amended in the general election of 2008 to, among other things, repeal a provision (Article 3, Section 5) which had until its repeal prohibited "idiots or insane persons" from voting.
"Idiots" are also barred from voting in British parliamentary elections.
In literature.
A few authors have used "idiot" characters in novels, plays and poetry. Often these characters are used to highlight or indicate something else (allegory). Examples of such usage are William Faulkner's "The Sound and the Fury" and William Wordsworth's "The Idiot Boy". Idiot characters in literature are often confused with or subsumed within mad or lunatic characters. The most common imbrication between these two categories of mental impairment occurs in the polemic surrounding Edmund from William Shakespeare's "King Lear". In Fyodor Dostoevsky's novel "The Idiot", the idiocy of the main character, Prince Lev Nikolaievich Myshkin, is attributed more to his honesty, trustfulness, kindness, and humility, than to a lack of intellectual ability. Nietzsche claimed, in his "The Antichrist", that Jesus was an idiot. This resulted from his description of Jesus as having an aversion toward the material world.

</doc>
<doc id="15012" url="http://en.wikipedia.org/wiki?curid=15012" title="Islamism">
Islamism

Islamism (Urdu: اسلام پرستی‎; Arabic: الإسلام السياسي‎), also known as Political Islam, is a set of ideologies holding that "Islam should guide social and political as well as personal life". Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, and that the contrary idea that Islam is, or can be, apolitical is an error. Islamists can have varying interpretations on various Quranic suras and ayahs. Islamist views emphasize the implementation of Sharia (Islamic law); of pan-Islamic political unity; and of the selective removal of non-Muslim, particularly Western military, economic, political, social, or cultural influences in the Muslim world that they believe to be incompatible with Islam.
Some observers (Graham Fuller) suggest Islamism's tenets are less strict, and can be defined as a form of identity politics or "support for [Muslim] identity, authenticity, broader regionalism, revivalism, [and] revitalization of the community". Following the Arab Spring, political Islam was described as "increasingly interdependent" with political democracy by Olivier Roy.
Islamists generally oppose the use of the term, claiming that their political beliefs and goals are simply an expression of Islamic religious belief. Similarly, some experts (Bernard Lewis) favor the term "activist Islam", or "political Islam" (Trevor Stanley), and some (Robin Wright) have equated the term "militant Islam" with terrorism.
Central and prominent figures of modern Islamism include Hasan al-Banna, Sayyid Qutb, Abul Ala Maududi, and Ruhollah Khomeini.
Definitions.
Islamism has been defined as:
Islamism takes different forms and spans a wide range of strategies and tactics towards the powers in place -- "destruction, opposition, collaboration, indifference" that have varied as "circumstances have changed" —and thus is not a united movement.
Moderate and reformist Islamists who accept and work within the democratic process include parties like the Tunisian Ennahda Movement. Jamaat-e-Islami of Pakistan is basically a socio-political and democratic Vanguard party but has also gained political influence through military coup d'état in past. The Islamist groups like Hezbollah in Lebanon and Hamas in Palestine participate in democratic and political process as well as armed attacks, seeking to abolish the state of Israel. Radical Islamist organizations like al-Qaeda and the Egyptian Islamic Jihad, and groups such as the Taliban, entirely reject democracy, often declaring as "kuffar" those Muslims who support it (see "takfirism"), as well as calling for violent/offensive jihad or urging and conducting attacks on a religious basis.
Another major division within Islamism is between what Graham E. Fuller has described as the fundamentalist "guardians of the tradition" (Salafis, such as those in the Wahhabi movement) and the "vanguard of change and Islamic reform" centered around the Muslim Brotherhood. Olivier Roy argues that "Sunni pan-Islamism underwent a remarkable shift in the second half of the 20th century" when the Muslim Brotherhood movement and its focus on Islamisation of pan-Arabism was eclipsed by the Salafi movement with its emphasis on "sharia rather than the building of Islamic institutions," and rejection of Shia Islam. Following the Arab Spring, Roy has described Islamism as "increasingly interdependent" with democracy in much of the Arab Muslim world, such that "neither can now survive without the other." While Islamist political culture itself may not be democratic, Islamists need democratic elections to maintain their legitimacy. At the same time, their popularity is such that no government can call itself democratic that excludes mainstream Islamist groups.
History of the term.
The term, which originally denoted the religion of Islam, first appeared in English as "Islamismus" in 1696, and as "Islamism" in 1712. By the turn of the twentieth century it had begun to be displaced by the shorter and purely Arabic term "Islam" and by 1938, when Orientalist scholars completed "The Encyclopaedia of Islam", seems to have virtually disappeared from English usage.
The term "Islamism" acquired its contemporary connotations in French academia in the late 1970s and early 1980s. From French, it began to migrate to the English language in the mid-1980s, and in recent years has largely displaced the term Islamic fundamentalism in academic circles.
The use of the term Islamism was at first "a marker for scholars more likely to sympathize" with new Islamic movements; however, as the term gained popularity it became more specifically associated with political groups such as the Taliban or the Algerian Armed Islamic Group, as well as with highly publicized acts of violence.
"Islamists" who have spoken out against the use of the term insisting they are merely "Muslims", include Ayatollah Mohammad Hussein Fadlallah, the spiritual mentor of Hizbullah, and Abbassi Madani, leader of the Algerian Islamic Salvation Front.
A 2003 article in "Middle East Quarterly" states:
In summation, the term Islamism enjoyed its first run, lasting from Voltaire to the First World War, as a synonym for Islam. Enlightened scholars and writers generally preferred it to Mohammedanism. Eventually both terms yielded to Islam, the Arabic name of the faith, and a word free of either pejorative or comparative associations. There was no need for any other term, until the rise of an ideological and political interpretation of Islam challenged scholars and commentators to come up with an alternative, to distinguish Islam as modern ideology from Islam as a faith... To all intents and purposes, Islamic fundamentalism and Islamism have become synonyms in contemporary American usage.
The Council on American–Islamic Relations complained in 2013 that the Associated Press's definition of "Islamist"—a "supporter of government in accord with the laws of Islam [and] who view the Quran as a political model"—had become a pejorative shorthand for "Muslims we don't like."
The AP Stylebook entry for Islamist now reads as follows: "An advocate or supporter of a political movement that favors reordering government and society in accordance with laws prescribed by Islam. Do not use as a synonym for Islamic fighters, militants, extremists or radicals, who may or may not be Islamists. Where possible, be specific and use the name of militant affiliations: al-Qaida-linked, Hezbollah, Taliban, etc. Those who view the Quran as a political model encompass a wide range of Muslims, from mainstream politicians to militants known as jihadi."
Relation to Islam.
Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error. Scholars and observers who do not believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi. Hayri Abaza argues the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.
Islamists have asked the question, "If Islam is a way of life, how can we say that those who want to live by its principles in legal, social, political, economic, and political spheres of life are not Muslims, but Islamists and believe in Islamism, not [just] Islam?" Similarly, a writer for the International Crisis Group maintains that "the conception of 'political Islam'" is a creation of Americans to explain the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the "short-lived era of the heyday of secular Arab nationalism between 1945 and 1970", and it is quietist/non-political Islam, not Islamism, that requires explanation.
On the other hand, Muslim-owned and run media (not just Western media) have used the terms "Islamist" and "Islamism" — as distinguished from Muslim and Islam — to distinguish groups such as the Islamic Salvation Front in Algeria or Jamaa Islamiya in Egypt, which actively seek to implement Islamic law, from mainstream Muslim groups.
Another source distinguishes Islamist from Islamic "by the fact that the latter refers to a religion and culture in existence over a millennium, whereas the first is a political/religious phenomenon linked to the great events of the 20th century". Islamists have, at least at times, defined themselves as "Islamiyyoun/Islamists" to differentiate themselves from "Muslimun/Muslims".
According to historian Bernard Lewis, Islamism, (or as he terms it "activist" Islam), along with "quietism," form two "particular ... political traditions" in Islam.
 The arguments in favor of both are based, as are most early Islamic arguments, on the Holy Book and on the actions and sayings of the Prophet.
The quietist tradition obviously rests on the Prophet as sovereign, as judge and statesman. But before the Prophet became a head of state, he was a rebel. Before he traveled from Mecca to Medina, where he became sovereign, he was an opponent of the existing order. He led an opposition against the pagan oligarchy of Mecca and at a certain point went into exile and formed what in modern language might be called a "government in exile," with which finally he was able to return in triumph to his birthplace and establish the Islamic state in Mecca. ...
The Prophet as rebel has provided a sort of paradigm of revolution—opposition and rejection, withdrawal and departure, exile and return. Time and time again movements of opposition in Islamic history tried to repeat this pattern, a few of them successfully.
—Bernard Lewis (Islamic Revolution)
Daniel Pipes describes Islamism as a modern ideology that owes more to European utopian political ideologies and "isms" than to the traditional Islamic religion.
Influence.
Few observers contest the influence of Islamism in the Muslim world. Following the collapse of the Soviet Union, political movements based on the liberal ideology of free expression and democratic rule have led the opposition in other parts of the world such as Latin America, Eastern Europe and many parts of Asia; however "the simple fact is that political Islam currently reigns as the most powerful ideological force across the Muslim world today".
Even some of those (such as Olivier Roy) who see Islamism as fraught with contradictions believe "the socioeconomic realities that sustained the Islamist wave are still here and are not going to change: poverty, uprootedness, crises in values and identities, the decay of the educational systems, the North-South opposition, and the problem of immigrant integration into the host societies".
The strength of Islamism draws from the strength of religiosity in general in the Muslim world. Compared to Western societies, "[w]hat is striking about the Islamic world is that ... it seems to have been the least penetrated by irreligion".
Where other peoples may look to the physical or social sciences for answers in areas which their ancestors regarded as best left to scripture, in the Muslim world, religion has become more encompassing, not less, as "in the last few decades, it has been the fundamentalists who have increasingly represented the cutting edge" of Muslim culture.
In Egypt and the rest of the Muslim world "the word secular, a label proudly worn 30 years ago, is shunned" and "used to besmirch" political foes.
The small secular opposition parties "cannot compare" with Islamists in terms of "doggedness, courage," "risk-taking" or "organizational skills".
In the Middle East and Pakistan, religious discourse dominates societies, the airwaves, and thinking about the world. Radical mosques have proliferated throughout Egypt. Book stores are dominated by works with religious themes ... The demand for sharia, the belief that their governments are unfaithful to Islam and that Islam is the answer to all problems, and the certainty that the West has declared war on Islam; these are the themes that dominate public discussion. Islamists may not control parliaments or government palaces, but they have occupied the popular imagination.
Moderate strains of Islamism have been described as "competing in the democratic public square in places like Turkey, Tunisia, Malaysia and Indonesia. In Morocco, the Islamist Justice and Development Party (PJD) supported King Muhammad VI's "Mudawana", a "startlingly progressive family law" which grants women the right to a divorce, raises the minimum age for marriage to 18, and, in the event of separation, stipulates equal distribution of property.
Even before the Arab Spring, Islamists in Egypt and other Muslim countries had been described as "extremely influential. ... They determine how one dresses, what one eats. In these areas, they are incredibly successful. ... Even if the Islamists never come to power, they have transformed their countries." Democratic, peaceful and political Islamists are now dominating the spectrum of Islamist ideology as well as the political system of the Muslim world.
Sources of strength.
Amongst the various reasons for the global strength of Islamism are:
Western alienation.
Muslim alienation from Western ways, including its political ways.
Western patronage.
During the 1970s and sometimes later, Western and pro-Western governments often supported sometimes fledgling Islamists and Islamist groups that later came to be seen as dangerous enemies. Islamists were considered by Western governments bulwarks against—what were thought to be at the time—more dangerous leftist/communist/nationalist insurgents/opposition, which Islamists were correctly seen as opposing. The US spent billions of dollars to aid the mujahideen Muslim Afghanistan enemies of the Soviet Union, and non-Afghan veterans of the war returned home with their prestige, "experience, ideology, and weapons", and had considerable impact.
Although it is a strong opponent of Israel's existence, Hamas, officially created in 1987, traces back its origins to institutions and clerics supported by Israel in the 1970s and 1980s. Israel tolerated and supported Islamist movements in Gaza, with figures like Ahmed Yassin, as Israel perceived them preferable to the secular and then more powerful al-Fatah with the PLO.
Egyptian President Anwar Sadat – whose policies included opening Egypt to Western investment ("infitah"); transferring Egypt's allegiance from the Soviet Union to the United States; and making peace with Israel – released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists. His "encouraging of the emergence of the Islamist movement" was said to have been "imitated by many other Muslim leaders in the years that followed." This "gentlemen's agreement" between Sadat and Islamists broke down in 1975 but not before Islamists came to completely dominate university student unions. Sadat was later assassinated and a formidable insurgency was formed in Egypt in the 1990s. The French government has also been reported to have promoted Islamist preachers "in the hope of channeling Muslim energies into zones of piety and charity."
Resurgence of Islam.
The resurgence of Islamic devotion and the attraction to things Islamic can be traced to several events.
Saudi Arabian funding.
Starting in the mid-1970s the Islamic resurgence was funded by an abundance of money from Saudi Arabian oil exports. The tens of billions of dollars in "petro-Islam" largesse obtained from the recently heightened price of oil funded an estimated "90% of the expenses of the entire faith."
Throughout the Muslim world, religious institutions for people both young and old, from children's maddrassas to high-level scholarships received Saudi funding,
"books, scholarships, fellowships, and mosques" (for example, "more than 1500 mosques were built and paid for with money obtained from public Saudi funds over the last 50 years"), along with training in the Kingdom for the preachers and teachers who went on to teach and work at these universities, schools, mosques, etc.
The funding was also used to reward journalists and academics who followed the Saudis' strict interpretation of Islam; and satellite campuses were built around Egypt for Al Azhar, the world's oldest and most influential Islamic university.
The interpretation of Islam promoted by this funding was the strict, conservative Saudi-based Wahhabism or Salafism. In its harshest form it preached that Muslims should not only "always oppose" infidels "in every way," but "hate them for their religion ... for Allah's sake," that democracy "is responsible for all the horrible wars of the 20th century," that Shia and other non-Wahhabi Muslims were infidels, etc. While this effort has by no means converted all, or even most Muslims to the Wahhabist interpretation of Islam, it has done much to overwhelm more moderate local interpretations, and has set the Saudi-interpretation of Islam as the "gold standard" of religion in minds of some or many Muslims.
Grand Mosque seizure.
The strength of the Islamist movement was manifest in an event which might have seemed sure to turn Muslim public opinion against fundamentalism, but did just the opposite. In 1979 the Grand Mosque in Mecca Saudi Arabia was seized by an armed fundamentalist group and held for over a week. Scores were killed, including many pilgrim bystanders in a gross violation of one of the most holy sites in Islam (and one where arms and violence are strictly forbidden).
Instead of prompting a backlash against the movement from which the attackers originated, however, Saudi Arabia, already very conservative, responded by shoring up its fundamentalist credentials with even more Islamic restrictions. Crackdowns followed on everything from shopkeepers who did not close for prayer and newspapers that published pictures of women, to the selling of dolls, teddy bears (images of animate objects are considered haraam), and dog food (dogs are considered unclean).
In other Muslim countries, blame for and wrath against the seizure was directed not against fundamentalists, but against Islamic fundamentalism's foremost geopolitical enemy – the United States. Ayatollah Khomeini sparked attacks on American embassies when he announced:
It is not beyond guessing that this is the work of criminal American imperialism and international Zionism despite the fact that the object of the fundamentalists' revolt was the Kingdom of Saudi Arabia, America's major ally in the region. Anti-American demonstrations followed in the Philippines, Turkey, Bangladesh, India, the UAE, Pakistan, and Kuwait. The US Embassy in Libya was burned by protesters chanting pro-Khomeini slogans and the embassy in Islamabad, Pakistan was burned to the ground.
Charitable work.
Islamist movements such as the Muslim Brotherhood, "are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups." All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.
Power of identity politics.
Islamism can also be described as part of identity politics, specifically the religiously-oriented nationalism that emerged in the Third World in the 1970s: "resurgent Hinduism in India, Religious Zionism in Israel, militant Buddhism in Sri Lanka, resurgent Sikh nationalism in the Punjab, 'Liberation Theology' of Catholicism in Latin America, and of course, Islamism in the Muslim world." These all challenged Westernized ruling elites on behalf of 'authenticity' and tradition.
Criticism.
Islamism, or elements of Islamism, have been criticised for: repression of free expression and individual rights, rigidity, hypocrisy, lack of true understanding of Islam, misinterpreting the Quran and Sunnah, and for innovations to Islam (bid‘ah), notwithstanding Islamists' proclaimed opposition to any such innovation.
History.
Predecessor movements.
Some Islamic revivalist movements and leaders pre-dating Islamism include:
Early history.
The end of the 19th century saw the dismemberment of most of the Muslim Ottoman Empire by non-Muslim European colonial powers. The empire spent massive sums on Western civilian and military technology to try to modernize and compete with the encroaching European powers, and in the process went deep into debt to these powers.
In this context, the publications of Jamal ad-din al-Afghani (1837–97), Muhammad Abduh (1849–1905) and Rashid Rida (1865–1935) preached Islamic alternatives to the political, economic, and cultural decline of the empire. Muhammad Abduh and Rashid Rida formed the beginning of the Islamist movement, as well as the reformist Islamist movement.
Their ideas included the creation of a truly Islamic society under sharia law, and the rejection of taqlid, the blind imitation of earlier authorities, which they believed deviated from the true messages of Islam. Unlike some later Islamists, Early Salafiyya strongly emphasized the restoration of the Caliphate.
Muhammad Iqbal.
Muhammad Iqbal was a philosopher, poet and politician in British India who is widely regarded as having inspired the Islamic Nationalism and Pakistan Movement in British India. Iqbal is admired as a prominent classical poet by Pakistani, Iranian, Indian and other international scholars of literature. Though Iqbal is best known as an eminent poet, he is also a highly acclaimed "Islamic philosophical thinker of modern times".
While studying law and philosophy in England and Germany, Iqbal became a member of the London branch of the All India Muslim League. He came back to Lahore in 1908. While dividing his time between law practice and philosophical poetry, Iqbal had remained active in the Muslim League. He did not support Indian involvement in World War I and remained in close touch with Muslim political leaders such as Muhammad Ali Johar and Muhammad Ali Jinnah. He was a critic of the mainstream Indian nationalist and secularist Indian National Congress. Iqbal's seven English lectures were published by Oxford University press in 1934 in a book titled The Reconstruction of Religious Thought in Islam. These lectures dwell on the role of Islam as a religion as well as a political and legal philosophy in the modern age.
Iqbal expressed fears that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence. In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences. Sir Muhammad Iqbal was elected president of the Muslim League in 1930 at its session in Allahabad as well as for the session in Lahore in 1932. In his Allahabad Address on 29 December 1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India. This address later inspired the Pakistan movement.
The thoughts and vision of Iqbal later influenced many reformist Islamists, e.g. Muhammad Asad, Sayyid Abul Ala Maududi and Ali Shariati.
Sayyid Abul Ala Maududi.
Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose "Islamization of Knowledge" carried forward some of Maududi's key principles.
Maududi believed that Islam was all-encompassing: "Everything in the universe is 'Muslim' for it obeys God by submission to His laws... The man who denies God is called Kafir (concealer) because he conceals by his disbelief what is inherent in his nature and embalmed in his own soul."
Maududi also believed that Muslim society could not be Islamic without Sharia, and Islam required the establishment of an Islamic state. This state should be a "theo-democracy," based on the principles of: "tawhid" (unity of God), "risala" (prophethood) and "khilafa" (caliphate).
Although Maududi talked about Islamic revolution, by "revolution" he meant not the violence or populist policies of the Iranian Revolution, but the gradual changing the hearts and minds of individuals from the top of society downward through an educational process or "da'wah".
Muslim Brotherhood.
Roughly contemporaneous with Maududi was the founding of the Muslim Brotherhood in Ismailiyah, Egypt in 1928 by Hassan al Banna. His was arguably the first, largest and most influential modern Islamic political/religious organization. Under the motto "the Qur'an is our constitution,"
it sought Islamic revival through preaching and also by providing basic community services including schools, mosques, and workshops. Like Maududi, Al Banna believed in the necessity of government rule based on Shariah law implemented gradually and by persuasion, and of eliminating all imperialist influence in the Muslim world.
Some elements of the Brotherhood, though perhaps against orders, did engage in violence against the government, and its founder Al-Banna was assassinated in 1949 in retaliation for the assassination of Egypt's premier Mahmud Fami Naqrashi three months earlier. The Brotherhood has suffered periodic repression in Egypt and has been banned several times, in 1948 and several years later following confrontations with Egyptian president Gamal Abdul Nasser, who jailed thousands of members for several years.
Despite periodic repression, the Brotherhood has become one of the most influential movements in the Islamic world, particularly in the Arab world. For many years it was
described as "semi-legal" and was the only opposition group in Egypt able to field candidates during elections. In the Egyptian parliamentary election, 2011–2012, the political parties identified as "Islamist" (the Brotherhood's Freedom and Justice Party, Salafi Al-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, an Islamist democrat of Muslim Brotherhood, was the first democratically elected president of Egypt. He was deposed during the 2013 Egyptian coup d'état.
Sayyid Qutb.
Maududi's political ideas influenced Sayyid Qutb, a leading member of the Muslim Brotherhood movement, and one of the key philosophers of Islamism and highly influential thinkers of Islamic universalism. Qutb believed things had reached such a state that the Muslim community had literally ceased to exist. It "has been extinct for a few centuries," having reverted to Godless ignorance (Jahiliyya).
To eliminate jahiliyya, Qutb argued Sharia, or Islamic law, must be established. Sharia law was not only accessible to humans and essential to the existence of Islam, but also all-encompassing, precluding "evil and corrupt" non-Islamic ideologies like communism, nationalism, or secular democracy.
Qutb preached that Muslims must engage in a two-pronged attack of converting individuals through preaching Islam peacefully and also waging what he called militant jihad so as to forcibly eliminate the "power structures" of Jahiliyya – not only from the Islamic homeland but from the face of the earth.
Qutb was both a member of the brotherhood and enormously influential in the Muslim world at large. Qutb is considered by some (Fawaz A. Gerges) to be "the founding father and leading theoretician" of modern jihadists, such as Osama bin Laden. However, the Muslim Brotherhood in Egypt and in Europe has not embraced his vision of undemocratic Islamic state and armed jihad, something for which they have been denounced by radical Islamists.
Six-Day War (1967).
The quick and decisive defeat of the Arab troops during the Six-Day War by Israeli troops constituted a pivotal event in the Arab Muslim world. The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes. A steep and steady decline in the popularity and credibility of secular, socialist and nationalist politics ensued. Ba'athism, Arab socialism, and Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground.
Islamic Republic in Iran.
The first modern "Islamist state" (with the possible exception of Zia's Pakistan) was established among the Shia of Iran. In a major shock to the rest of the world, Ayatollah Ruhollah Khomeini led the Iranian Revolution of 1979 to overthrow the oil-rich, well-armed, Westernized and pro-American secular monarchy ruled by Shah Muhammad Reza Pahlavi.
The views of Ali Shariati, ideologue of the Iranian Revolution, had resemblance with Mohammad Iqbal, ideological father of the State of Pakistan, but Khomeini's beliefs were placed somewhere between beliefs of Sunni Islamic thinkers like Mawdudi and Qutb:
He believed that complete imitation of the early Muslims for restoration of Sharia law was essential to Islam, that secular, Westernizing Muslims were actually agents of the West serving Western interests, and that the "plundering" of Muslim lands was part of a long-term conspiracy against Islam by the Christian West.
But they also differed:
While initial enthusiasm for the Iranian revolution in the Muslim world was intense, it has waned as "purges, executions, and atrocities tarnished its image".
The Islamic Republic has also maintained its hold on power in Iran in spite of US economic sanctions, and has created or assisted like-minded Shia terrorist groups in Iraq, Egypt, Syria, Jordan (SCIRI) and Lebanon (Hezbollah) (two Muslim countries that also have large Shiite populations).
During the 2006 Israel-Lebanon conflict, the Iranian government enjoyed something of a resurgence in popularity amongst the predominantly Sunni "Arab street," due to its support for Hezbollah and to President Mahmoud Ahmadinejad's vehement opposition to the United States and his call that Israel shall vanish.
Bangladesh.
Currently, the Bangladesh Nationalist Party is the second largest party in the Parliament of Bangladesh and the main opposition party, followed by Jamaat-e-Islami Bangladesh. The BNP promotes a center-right policy combining elements of conservatism, Islamism, nationalism and anti-communism. The party believes that Islam is an integral part of the socio-cultural life of Bangladesh, and favors Islamic principles and cultural views. Since 2000, it has been allied with the Islamic parties Jamaat-e-Islami Bangladesh, Islami Oikya Jote and Hefazat-e-Islam Bangladesh.
Pakistan.
Early in the history of the state of Pakistan (12 March 1949), a parliamentary resolution (the Objectives Resolution) was adopted in accordance with the vision of founding fathers of Pakistan (Muhammad Iqbal, Muhammad Ali Jinnah, Liaquat Ali Khan). proclaiming:
Sovereignty belongs to Allah alone but He has delegated it to the State of Pakistan through its people for being exercised within the limits prescribed by Him as a sacred trust.
This resolution later became a key source of inspiration for writers of the Constitution of Pakistan, and is included in the constitution as preamble.
In July 1977, General Zia-ul-Haq overthrew Prime Minister Zulfiqar Ali Bhutto's regime in Pakistan. Ali Bhutto, a leftist in democratic competition with Islamists, had announced banning alcohol and nightclubs within six months, shortly before he was overthrown. Zia-ul-Haq was much more committed to Islamism, and "Islamization" or implementation of Islamic law, became a cornerstone of his eleven-year military dictatorship and Islamism became his "official state ideology". Zia ul Haq was an admirer of Mawdudi and Mawdudi's party Jamaat-e-Islami became the "regime's ideological and political arm". In Pakistan this Islamization from above was "probably" more complete "than under any other regime except those in Iran and Sudan," but Zia-ul-Haq was also criticized by many Islamists for imposing "symbols" rather than substance, and using Islamization to legitimize his means of seizing power. Unlike neighboring Iran, Zia-ul-Haq's policies were intended to "avoid revolutionary excess", and not to strain relations with his American and Persian Gulf state allies. Zia-ul-Haq was killed in 1988 but Islamization remains an important element in Pakistani society.
Afghanistan.
In 1979, the Soviet Union deployed its 40th Army into Afghanistan, attempting to suppress an Islamic rebellion against an allied Marxist regime in the Afghan Civil War. The conflict, pitting indigenous impoverished Muslims (mujahideen) against an anti-religious superpower, galvanized thousands of Muslims around the world to send aid and sometimes to go themselves to fight for their faith. Leading this pan-Islamic effort was Palestinian sheikh Abdullah Yusuf Azzam. While the military effectiveness of these "Afghan Arabs" was marginal, an estimated 16,000 to 35,000 Muslim volunteers came from around the world came to fight in Afghanistan.
When the Soviet Union abandoned the Marxist Najibullah regime and withdrew from Afghanistan in 1989 (the regime finally fell in 1992), the victory was seen by many Muslims as the triumph of Islamic faith over superior military power and technology that could be duplicated elsewhere.
The jihadists gained legitimacy and prestige from their triumph both within the militant community and among ordinary Muslims, as well as the confidence to carry their jihad to other countries where they believed Muslims required assistance.|
The "veterans of the guerrilla campaign" returning home to Algeria, Egypt, and other countries "with their experience, ideology, and weapons," were often eager to continue armed jihad.
The collapse of the Soviet Union itself, in 1991, was seen by many Islamists, including Bin Laden, as the defeat of a superpower at the hands of Islam. Concerning the $6 billion in aid given by the US and Pakistan's military training and intelligence support to the mujahideen, bin Laden wrote: "[T]he US has no mentionable role" in "the collapse of the Soviet Union ... rather the credit goes to God and the mujahidin" of Afghanistan.
Persian Gulf War.
Another factor in the early 1990s that worked to radicalize the Islamist movement was the Gulf War, which brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait. Prior to 1990 Saudi Arabia played an important role in restraining the many Islamist groups that received its aid. But when Saddam, secularist and Ba'athist dictator of neighboring Iraq, attacked Saudi Arabia (his enemy in the war), western troops came to protect the Saudi monarchy. Traditional Muslim belief holds that non-Muslim troops must not be allowed on the Arabian peninsula (including Saudi Arabia). Islamists accused the Saudi regime of being a puppet of the west.
These attacks resonated with conservative Muslims and the problem did not go away with Saddam's defeat either, since American troops remained stationed in the kingdom, and a de facto cooperation with the Palestinian-Israeli peace process developed. Saudi Arabia attempted to compensate for its loss of prestige among these groups by repressing those domestic Islamists who attacked it (bin Laden being a prime example), and increasing aid to Islamic groups (Islamist madrassas around the world and even aiding some violent Islamist groups) that did not, but its pre-war influence on behalf of moderation was greatly reduced. One result of this was a campaign of attacks on government officials and tourists in Egypt, a bloody civil war in Algeria and Osama bin Laden's terror attacks climaxing in the 9/11 attack.
Jihad movements of Egypt.
While Qutb's ideas became increasingly radical during his imprisonment prior to his execution in 1966, the leadership of the Brotherhood, led by Hasan al-Hudaybi, remained moderate and interested in political negotiation and activism. Fringe or splinter movements inspired by the final writings of Qutb in the mid-1960s (particularly the manifesto "Milestones", a.k.a. "Ma'alim fi-l-Tariq") did, however, develop and they pursued a more radical direction. By the 1970s, the Brotherhood had renounced violence as a means of achieving its goals.
The path of violence and military struggle was then taken up by the Egyptian Islamic Jihad organization responsible for the assassination of Anwar Sadat in 1981. Unlike earlier anti-colonial movements the extremist group directed its attacks against what it believed were "apostate" leaders of Muslim states, leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies. Its views were outlined in a pamphlet written by Muhammad Abd al-Salaam Farag, in which he states:
...there is no doubt that the first battlefield for jihad is the extermination of these infidel leaders and to replace them by a complete Islamic Order...
Another of the Egyptian groups which employed violence in their struggle for Islamic order was al-Gama'a al-Islamiyya (Islamic Group). Victims of their campaign against the Egyptian state in the 1990s included the head of the counter-terrorism police (Major General Raouf Khayrat), a parliamentary speaker (Rifaat al-Mahgoub), dozens of European tourists and Egyptian bystanders, and over 100 Egyptian police. Ultimately the campaign to overthrow the government was unsuccessful, and the major jihadi group, Jamaa Islamiya (or al-Gama'a al-Islamiyya), renounced violence in 2003. Other lesser known groups include the Islamic Liberation Party, Salvation from Hell and Takfir wal-Hijra, and these groups have variously been involved in activities such as attempted assassinations of political figures, arson of video shops and attempted takeovers of government buildings.
Sudan and Turabi.
For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.
After al-Nimeiry was overthrown in 1985 the party did poorly in national elections, but in 1989 it was able to overthrow the elected post-al-Nimeiry government with the help of the military. Turabi was noted for proclaiming his support for the democratic process and a liberal government before coming to power, but strict application of sharia law, torture and mass imprisonment of the opposition, and an intensification of the long-running war in southern Sudan, once in power. The NIF regime also harbored Osama bin Laden for a time (before 9/11), and worked to unify Islamist opposition to the American attack on Iraq in the 1991 Gulf War.
After Sudanese intelligence services were implicated in an assassination attempt on the President of Egypt, UN economic sanctions were imposed on Sudan, a poor country, and Turabi fell from favor. He was imprisoned for a time in 2004-5. Some of the NIF policies, such as the war with the non-Muslim south, have been reversed, though the National Islamic Front still holds considerable power in the government of Omar al-Bashir and National Congress Party, another Islamist party in country.
Algeria.
An Islamist movement influenced by Salafism and the jihad in Afghanistan, as well as the Muslim Brotherhood, was the FIS or Front Islamique de Salut (the Islamic Salvation Front) in Algeria. Founded as a broad Islamist coalition in 1989 it was led by Abbassi Madani, and a charismatic Islamist young preacher, Ali Belhadj. Taking advantage of economic failure and unpopular social liberalization and secularization by the ruling leftist-nationalist FLN government, it used its preaching to advocate the establishment of a legal system following Sharia law, economic liberalization and development program, education in Arabic rather than French, and gender segregation, with women staying home to alleviate the high rate of unemployment among young Algerian men. The FIS won sweeping victories in local elections and it was going to win national elections in 1991 when voting was canceled by a military coup d'état.
As Islamists took up arms to overthrow the government, the FIS's leaders were arrested and it became overshadowed by Islamist guerrilla groups, particularly the Islamic Salvation Army, MIA and Armed Islamic Group (or GIA). A bloody and devastating civil war ensued in which between 150,000 and 200,000 people were killed over the next decade.
The civil war was not a victory for Islamists. By 2002 the main guerrilla groups had either been destroyed or had surrendered. The popularity of Islamist parties has declined to the point that "the Islamist candidate, Abdallah Jaballah, came a distant third with 5% of the vote" in the 2004 presidential election.
Taliban in Afghanistan.
In Afghanistan, the mujahideen's victory against the Soviet Union in the 1980s did not lead to justice and prosperity, due to a vicious and destructive civil war between political and tribal warlords, making Afghanistan one of the poorest countries on earth. In 1992, the Democratic Republic of Afghanistan ruled by communist forces collapsed, and democratic Islamist elements of mujahdeen founded the Islamic State of Afghanistan. In 1996, a more conservative and anti-democratic Islamist movement known as the Taliban rose to power, defeated most of the warlords and took over roughly 80% of Afghanistan.
The Taliban were spawned by the thousands of madrasahs the Deobandi movement established for impoverished Afghan refugees and supported by governmental and religious groups in neighboring Pakistan. The Taliban differed from other Islamist movements to the point where they might be more properly described as Islamic fundamentalist or neofundamentalist, interested in spreading "an idealized and systematized version of conservative tribal village customs" under the label of Sharia to an entire country. Their ideology was also described as being influenced by Wahhabism, and the extremist jihadism of their guest Osama bin Laden.
The Taliban considered "politics" to be against Sharia and thus did not hold elections. They were led by Mullah Mohammed Omar who was given the title "Amir al-Mu'minin" or Commander of the Faithful, and a pledge of loyalty by several hundred Taliban-selected Pashtun clergy in April 1996. Taliban were overwhelmingly Pashtun and were accused of not sharing power with the approximately 60% of Afghans who belonged to other ethnic groups. (see: Taliban#Ideology)
The Taliban's hosting of Osama bin Laden led to an American-organized attack which drove them from power following the 9/11 attacks.
Taliban are still very much alive and fighting a vigorous insurgency with suicide bombings and armed attacks being launched against NATO and Afghan government targets.
Turkey.
Turkey had a number of Islamist parties, often changing names as they were banned by the Kemalist constitutional court for anti-secular activities. Necmettin Erbakan (1926-2011) was the leader of several of the parties, the National Order Party ("Milli Nizam Partisi", 1970-1971), the National Salvation Party ("Milli Selamet Partisi", 1972-1981), and the Welfare Party ("Refah Partisi", 1983-1998); he also became a member of the Felicity Party ("Saadet Partisi", 2003-2011).
The Justice and Development Party (AKP), which has dominated Turkish politics from 2002 to 2015, is sometimes described as Islamist, but rejects such labelling.
Ismet Özel, a prominent Islamist intellectual, argued that Mustafa Kemal Atatürk's secular authoritarian policy, ironically, Islamicized the Turkish nation by forcing people to internalize and value their religious identity and not simply to take it for granted as in the past.
Hizb ut-Tahrir.
Hizb ut-Tahrir is an influential international Islamist movement, founded in 1953 by an Islamic Qadi "(judge)" Taqiuddin al-Nabhani. HT is unique from most other Islamist movements in that the party focuses not on implementation of Sharia on local level or on providing social services, but on unifying the Muslim world under its vision of a new Islamic caliphate spanning from North Africa and the Middle East to much of central and South Asia.
To this end it has drawn up and published a constitution for its proposed caliphate state. The constitution's 187 articles specify specific policies such as sharia law, a "unitary ruling system" headed by a caliph elected by Muslims, an economy based on the gold standard, public ownership of utilities, public transport, and energy resources, and Arabic as the "sole language of the State."
In its focus on the Caliphate, HT takes a different view of Muslim history than some other Islamists such as Muhammad Qutb. HT sees Islam's pivotal turning point as occurring not with the death of Ali, or one of the other four rightly guided Caliphs in the 7th century, but with the abolition of the Ottoman Caliphate in 1924. This is believed to have ended the true Islamic system, something for which it blames "the disbelieving (Kafir) colonial powers" working through Turkish modernist Mustafa Kemal Atatürk.
HT does not engage in armed jihad or a democratic system, but works to take power through "ideological struggle" to change Muslim public opinion, and in particular through elites who will "facilitate" a "change of the government," i.e. launch a bloodless coup. It allegedly attempted and failed such coups in 1968 and 1969 in Jordan, and in 1974 in Egypt, and is now banned in both countries. But many HT members have gone on to join terrorist groups and many jihadi terrorists have cited HT as their key influence.
The party is sometimes described as "Leninist" and "rigidly controlled by its central leadership," with its estimated one million members required to spend "at least two years studying party literature under the guidance of mentors "(Murshid)"" before taking "the party oath." HT is particularly active in the ex-soviet republics of Central Asia and in Europe.
In the UK its rallies have drawn thousands of Muslims, and the party has been described by two observers (Robert S. Leiken and Steven Brooke) to have outpaced the Muslim Brotherhood in both membership and radicalism.
London.
Greater London has over 900,000 Muslims, (most of South Asian origins and concentrated in the East London boroughs of Newham, Tower Hamlets and Waltham Forest), and among them are some with a strong Islamist outlook. Their presence, combined with a perceived British policy of allowing them free rein, heightened by exposés such as the 2007 Channel 4 documentary programme "Undercover Mosque", has given rise to the term Londonistan. Following the 9/11 attacks, however, Abu Hamza al-Masri, the imam of the Finsbury Park Mosque, was arrested and charged with incitement to terrorism which has caused many Islamists to leave the UK to avoid internment.
Counter-response.
The U.S. government has engaged in efforts to counter Islamism, or violent Islamism, since 2001. These efforts were centred in the U.S. around public diplomacy programmes conducted by the State Department. There have been calls to create an independent agency in the U.S. with a specific mission of undermining Islamism and jihadism. Christian Whiton, an official in the George W. Bush administration, called for a new agency focused on the nonviolent practice of "political warfare" aimed at undermining the ideology. U.S. Defense Secretary Robert Gates called for establishing something similar to the defunct U.S. Information Agency, which was charged with undermining the communist ideology during the Cold War.

</doc>
<doc id="15014" url="http://en.wikipedia.org/wiki?curid=15014" title="Instructional theory">
Instructional theory

An Instructional theory is "a theory that offers explicit guidance on how to better help people learn and develop." Instructional theories focus on how to structure material for promoting the education of human beings, particularly youth. Originating in the United States in the late 1970s, "instructional theory" is typically influenced by three general influences in educational thought: the behaviorist, the cognitive, and the constructivist schools of thought. Instructional theory is heavily influenced by the 1956 work of Benjamin Bloom, a University of Chicago professor, and the results of his Taxonomy of Education Objectives — one of the first modern codifications of the learning process. 
One of the first instructional theorists was Robert M. Gagne, who in 1965 published "Conditions of Learning" for the Florida State University's Department of Educational Research.
In the context of the modern use of electronic educational technology (also called e-learning), a major discussion in instructional theory is the potential of learning objects to structure and deliver content. A stand-alone educational animation is an example of a learning object that can be re-used as the basis for different learning experiences. There are currently many groups trying to set standards for the development and implementation of learning objects. At the forefront of the standards groups is the Department of Defense's Advanced Distributed Learning initiative with its SCORM standards. SCORM stands for "Shareable Content Object Reference Model."
Definition.
Instructional theory is different than learning theory. A learning theory "describes" how learning take place, and an instructional theory "prescribes" how to better help people learn. Learning theories often inform instructional theory, and three general theoretical stances take part in this influence: behaviorism (learning as response acquisition), cognitivism (learning as knowledge acquisition), and constructivism (learning as knowledge construction).
Critiques.
Paulo Freire's work appears to critique instructional approaches that adhere to the knowledge acquisition stance, and his work "Pedagogy of the Oppressed" has had a broad influence over a generation of American educators with his critique of various "banking" models of education and analysis of the teacher-student relationship. http://www.dsa-atlanta.org/pdf_docs/Macedo_intro_POTO.pdf 
Freire explains, "Narration (with the teacher as narrator) leads the students to memorize mechanically the narrated content. Worse yet, it turns them into “containers,” into “receptacles” to be “filled” by the teacher. The more completely she fills the receptacles, the better a teacher she is. The more meekly the receptacles permit themselves to be filled, the better students they are." http://www.dsa-atlanta.org/pdf_docs/Macedo_intro_POTO.pdf . In this way he explains educator creates an act of depositing knowledge in a student. The student thus becomes a repository of knowledge. Freire explains that this system that diminishes creativity and knowledge suffers. Knowledge, according to Freire, comes about only through the learner by inquiry and pursuing the subjects in the world and through interpersonal interaction. 
Freire further states, "In the banking concept of education, knowledge is a gift bestowed by those who consider themselves knowledgeable upon those whom they consider to know nothing. Projecting an
absolute ignorance onto others, a characteristic of the ideology of oppression, negates
education and knowledge as processes of inquiry. The teacher presents himself to his
students as their necessary opposite; by considering their ignorance absolute, he justifies his
own existence. The students, alienated like the slave in the Hegelian dialectic, accept their
ignorance as justifying the teacher’s existence — but, unlike the slave, they never discover
that they educate the teacher."
Freire then offered an alternative stance and wrote, "The raison d'etre of libertarian education, on the other hand, lies in its drive towards reconciliation. Education must begin with the solution of the teacher-student contradiction, by reconciling the poles of the contradiction so that both are simultaneously teachers and students. " http://www.dsa-atlanta.org/pdf_docs/Macedo_intro_POTO.pdf

</doc>
<doc id="15018" url="http://en.wikipedia.org/wiki?curid=15018" title="Infusoria">
Infusoria

Infusoria is a collective term for minute aquatic creatures such as ciliates, euglenoids, protozoa, unicellular algae and small invertebrates that exist in freshwater ponds. In modern formal classifications, the term is considered obsolete; the microorganisms previously included in the Infusoria are mostly assigned to the kingdom Protista which itself is a polyphyletic assemblage of groups.
Aquarium use.
Infusoria are used by owners of aquariums to feed fish fry; newly hatched fry of many common aquarium species can be successfully raised on this food during early development due to its size and nutritional content. Many home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own supply cultures or use one of the many commercial cultures available. Infusoria can be cultured by soaking any decomposing vegetative matter, such as papaya skin, in a jar of aged water. The culture will be grown in two to three days, depending on temperature and light received. The water will first turn cloudy, but it will clear up once the infusoria eat the bacteria which caused the cloudiness. At this point, the infusoria will be ready, and will usually be visible to the naked eye as small, white specks swimming in the container.

</doc>
<doc id="15019" url="http://en.wikipedia.org/wiki?curid=15019" title="ISO/IEC 8859-1">
ISO/IEC 8859-1

ISO/IEC 8859-1:1998, "Information technology — 8-bit single-byte coded graphic character sets — Part 1: Latin alphabet No. 1", is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1987. It is generally intended for Western European languages (see below for a list). It is the basis for most popular 8-bit character sets, including Windows-1252 and the first block of characters in Unicode.
ISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. The following other aliases are registered for ISO-8859-1: iso-ir-100, csISOLatin1, latin1, l1, IBM819, CP819.
The Windows-1252 codepage coincides with ISO-8859-1 for all codes except the range 128 to 159 (hex 80 to 9F), where the little-used C1 controls are replaced with additional characters including all the missing characters provided by ISO-8859-15. Code page 28591 a.k.a. Windows-28591 is the actual ISO-8859-1 codepage.
Coverage.
ISO 8859-1 encodes what it refers to as "Latin alphabet no. 1," consisting of 191 characters from the Latin script. This character-encoding scheme is used throughout the Americas, Western Europe, Oceania, and much of Africa. It is also commonly used in most standard romanizations of East-Asian languages. 9.6% of all web pages use ISO 8859-1 in July 2014.
Each character is encoded as a single eight-bit code value. These code values can be used in almost any data interchange system to communicate in the following European languages (with a few exceptions due to missing characters, as noted):
Quotation marks.
For some languages listed above, the correct typographical quotation marks are missing, as only « », " ", and ' ' are included. Also, this scheme does not provide for oriented (6- or 9-shaped) single or double quotation marks. Some fonts will display the spacing grave accent (0x60) and the apostrophe (0x27) as a matching pair of oriented single quotation marks, but this is not considered part of the modern standard.
History.
ISO 8859-1 was based on the Multinational Character Set used by Digital Equipment Corporation in the popular VT220 terminal. It was developed within ECMA, the
European Computer Manufacturers Association, and published in March 1985 as ECMA-94, by which name it is still sometimes known.
The (June 1986) also included ISO 8859-2, ISO 8859-3, and ISO 8859-4 as part of the specification.
In 1985 Commodore adopted ISO 8859-1 for its new AmigaOS operating system. The Seikosha MP-1300AI impact dot-matrix printer, used with the Amiga 1000, included this encoding. 
In 1992, the IANA registered the character map ISO_8859-1:1987, more commonly known by its preferred MIME name of ISO-8859-1 (note the extra hyphen over ISO 8859-1), a superset of ISO 8859-1, for use on the Internet. This map assigns the C0 and C1 control characters to the unassigned code values thus provides for 256 characters via every possible 8-bit value.
ISO-8859-1 is (according to the standards at least) the default encoding of documents delivered via HTTP with a MIME type beginning with "text/" (however the draft HTML 5 specification requires that documents advertised as ISO-8859-1 actually be parsed with the Windows-1252 encoding.) It is the default encoding of the values of certain descriptive HTTP headers, and defines the repertoire of characters allowed in HTML 3.2 documents (HTML 4.0, however, is based on Unicode). It and Windows-1252 are often assumed to be the encoding of text on Unix and Microsoft Windows in the absence of locale or other information, this is only gradually being replaced with Unicode encoding such as UTF-8 or UTF-16.
Codepage layout.
Legend:
  Alphabetic
  Control character
  Numeric digit
  Punctuation
  Extended punctuation
  Graphic character
  International
  Undefined
Similar character sets.
ISO-8859-1 was incorporated as the first 256 code points of ISO/IEC 10646 and Unicode.
The lower range 32 to 126 (hex 20 to 7E, the G0 subset) maps exactly to the same coded G0 subset of the ISO 646 US variant (commonly known as ASCII), whose ISO 2022 standard switch sequence is "ESC ( B". The higher range 160 to 255 (hex A0 to FF, the G1 subset) maps exactly to the same subset initiated by the ISO 2022 standard switch sequence "ESC . A".
ISO/IEC 8859-1 is missing some characters for French and Finnish text and the euro sign. In order to provide some of these characters, ISO/IEC 8859-15 was developed as an update of ISO/IEC 8859-1. This required, however, the removal of some infrequently used characters from ISO/IEC 8859-1, including fraction symbols and letter-free diacritics: ¤, ¦, ¨, ´, ¸, ¼, ½, and ¾.
The popular Windows-1252 character set adds all the missing characters provided by ISO/IEC 8859-15, plus a number of typographic symbols, by replacing the rarely used C1 controls in the range 128 to 159 (hex 80 to 9F). It is very common to mislabel text data with the charset label ISO-8859-1, even though the data is really Windows-1252 encoded. Many web browsers and e-mail clients will interpret ISO-8859-1 control codes as Windows-1252 characters in order to accommodate such mislabeling but it is not standard behaviour and care should be taken to avoid generating these characters in ISO-8859-1 labeled content.
The Apple Macintosh computer introduced a character encoding called Mac Roman, or Mac-Roman, in 1984. It was meant to be suitable for Western European desktop publishing. It is a superset of ASCII, like ISO-8859-1, and has most of the characters that are in ISO-8859-1 but in a totally different arrangement. A later version, registered with IANA as "Macintosh", replaced the generic currency sign ¤ with the euro sign €. The few printable characters that are in ISO 8859-1 but not in this set are often a source of trouble when editing text on websites using older Macintosh browsers (including the last version of Internet Explorer for Mac). However the extra characters that Windows-1252 has in the C1 codepoint range are all supported in MacRoman.
DOS had code page 850, which had all printable characters that ISO-8859-1 had (albeit in a totally different arrangement) plus the most widely used graphic characters from code page 437.

</doc>
<doc id="15020" url="http://en.wikipedia.org/wiki?curid=15020" title="ISO/IEC 8859">
ISO/IEC 8859

ISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.
ISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.
Introduction.
While the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7 bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.
The ISO/IEC 8859-"n" encodings only contain printable characters, and were designed to be used in conjunction with control characters mapped to the unassigned bytes. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-"n" as their preferred MIME name or, in cases where a preferred MIME name isn't specified, their canonical name. Many people use the terms ISO/IEC 8859-"n" and ISO-8859-"n" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.
Characters.
The ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.
As a rule of thumb, if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it didn't get in. Hence the directional double quotation marks "«" and "»" used for some European languages were included, but not the directional double quotation marks "“" and "”" used for English and some other languages. French didn't get its "œ" and "Œ" ligatures because they could be typed as 'oe'. Ÿ, needed for all-caps text, was left out as well. These characters were, however, included later with ISO/IEC 8859-15, which also introduced the new euro sign character €. Likewise Dutch did not get the 'ĳ' and 'Ĳ' letters, because Dutch speakers had become used to typing these as two letters instead. Romanian did not initially get its ‹Ș›/‹ș› and ‹Ț›/‹ț› (with comma) letters, because these letters were initially unified with ‹Ş›/‹ş› and ‹Ţ›/‹ţ› (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.
Most of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters although the Thai, Hebrew, and Arabic ones do also contain combining characters. However, the standard makes no provision for the scripts of East Asian languages ("CJK"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, but like several other alphabets of the world they aren't encoded in the ISO/IEC 8859 system.
The Parts of ISO/IEC 8859.
ISO/IEC 8859 is divided into the following parts:
Each part of ISO 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1–4, 9, 10, 13–16), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1–4 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.
Table.
At position 0xA0 there's always the non breaking space and 0xAD is mostly the soft hyphen, which only shows at line breaks. Other empty fields are either unassigned or the system used isn't able to display them.
There are new additions as ISO/IEC 8859-7:2003 and ISO/IEC 8859-8:1999 versions. LRM stands for left-to-right mark (U+200E) and RLM stands for right-to-left mark (U+200F).
Relationship to Unicode and the UCS.
Since 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the "U+nnnn" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).
Single-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.
Development status.
The ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of ISO/IEC 10646.

</doc>
<doc id="15022" url="http://en.wikipedia.org/wiki?curid=15022" title="Infrared">
Infrared

Infrared (IR) is invisible radiant energy, electromagnetic radiation with longer wavelengths than those of visible light, extending from the nominal red edge of the visible spectrum at 700 nanometers (frequency 430 THz) to 1 mm (300 GHz) (although people can see infrared up to at least 1050 nm in experiments). Most of the thermal radiation emitted by objects near room temperature is infrared.
Infrared radiation was discovered in 1800 by astronomer Sir William Herschel, who discovered a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect upon a thermometer. Slightly more than half of the total energy from the Sun was eventually found to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has a critical effect on Earth's climate.
Infrared energy is emitted or absorbed by molecules when they change their rotational-vibrational movements. Infrared energy excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared energy range.
Infrared radiation is used in industrial, scientific, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space, such as molecular clouds; detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus.
Thermal-infrared imaging is used extensively for military and civilian purposes. Military applications include target acquisition, surveillance, night vision, homing and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm (micrometers). Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, remote temperature sensing, short-ranged wireless communication, spectroscopy, and weather forecasting.
Definition and relationship to the electromagnetic spectrum.
Infrared radiation extends from the nominal red edge of the visible spectrum at 700 nanometers (nm) to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Below infrared is the microwave portion of the electromagnetic spectrum.
Natural infrared.
Sunlight, at an effective temperature of 5,780 kelvins, is composed of nearly thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kilowatt per square meter at sea level. Of this energy, 527 watts is infrared radiation, 445 watts is visible light, and 32 watts is ultraviolet radiation.
On the surface of Earth, at far lower temperatures than the surface of the Sun, almost all thermal radiation consists of infrared in various wavelengths. Of these natural thermal radiation processes only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy.
Regions within the infrared.
In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law.
Therefore, the infrared band is often subdivided into smaller sections.
Commonly used sub-division scheme.
A commonly used sub-division scheme is:
NIR and SWIR is sometimes called "reflected infrared," whereas MWIR and LWIR is sometimes referred to as "thermal infrared." Due to the nature of the blackbody radiation curves, typical 'hot' objects, such as exhaust pipes, often appear brighter in the MW compared to the same object viewed in the LW.
CIE division scheme.
The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands:
ISO 20473 scheme.
ISO 20473 specifies the following scheme:
Astronomy division scheme.
Astronomers typically divide the infrared spectrum as follows:
These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space.
Sensor response division scheme.
A third scheme divides up the band based on the response of various detectors:
Near-infrared is the region closest in wavelength to the radiation detectable by the human eye, mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (The common silicon detectors are sensitive to about 1,050 nm, while InGaAs' sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). Unfortunately, international standards for these specifications are not currently available.
The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Sources providing wavelengths as long as 1050 nm can be seen as a dull red glow in intense sources, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the "Wood effect" that consists of IR-glowing foliage.
Telecommunication bands in the infrared.
In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources transmitting/absorbing materials (fibers) and detectors:
The C-band is the dominant band for long-distance telecommunication networks. The S and L bands are based on less well established technology, and are not as widely deployed.
Heat.
Infrared radiation is popularly known as "heat radiation", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 µm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law).
Heat is energy in transit that flows due to temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that is associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiations are associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (i.e., the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth.
The concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the ideal of a black body. To further explain, two objects at the same physical temperature will not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler. For that reason, incorrect selection of emissivity will give inaccurate results when using infrared cameras and pyrometers.
Applications.
Night vision.
 Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source.
The use of infrared light and night vision devices should not be confused with thermal imaging, which creates images based on differences in surface temperature by detecting infrared radiation (heat) that emanates from objects and their surrounding environment.
Thermography.
Infrared radiation can be used to remotely determine the temperature of objects (if the emissivity is known). This is termed thermography, or in the case of very hot objects in the NIR or visible it is termed pyrometry. Thermography (thermal imaging) is mainly used in military and industrial applications but the technology is reaching the public market in the form of infrared cameras on cars due to the massively reduced production costs.
Thermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black body radiation law, thermography makes it possible to "see" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name).
Hyperspectral imaging.
 
A hyperspectral image, a basis for chemical imaging, is a "picture" containing continuous spectrum through a wide spectral range. Hyperspectral imaging is gaining importance in the applied spectroscopy particularly in the fields of NIR, SWIR, MWIR, and LWIR spectral regions. Typical applications include biological, mineralogical, defence, and industrial measurements.
Thermal Infrared Hyperspectral Camera can be applied similarly to a Thermographic camera, with the fundamental difference that each pixel contains a full LWIR spectrum. Consequently, chemical identification of the object can be performed without a need for an external light source such as the Sun or the Moon. Such cameras are typically applied for geological measurements, outdoor surveillance and UAV applications.
Other imaging.
In infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can "see" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy.
Tracking.
Infrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as "heat-seekers", since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background.
Heating.
Infrared radiation can be used as a deliberate heating source. For example it is used in infrared saunas to heat the occupants. It may also be used in other heating applications, such as to remove ice from the wings of aircraft (de-icing). Infrared can be used in cooking and heating food as it predominantly heats the opaque, absorbent objects, rather than the air around them.
Infrared heating is also becoming more popular in industrial manufacturing processes, e.g. curing of coatings, forming of plastics, annealing, plastic welding, and print drying. In these applications, infrared heaters replace convection ovens and contact heating.
Efficiency is achieved by matching the wavelength of the infrared heater to the absorption characteristics of the material.
Communications.
IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to encode the data. The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances.
Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.
Free space optical communication using infrared lasers can be a relatively inexpensive way to install a communications link in an urban area operating at up to 4 gigabit/s, compared to the cost of burying fiber optic cable.
Infrared lasers are used to provide the light for optical fiber communications systems. Infrared light with a wavelength around 1,330 nm (least dispersion) or 1,550 nm (best transmission) are the best choices for standard silica fibers.
IR data transmission of encoded audio versions of printed signs is being researched as an aid for visually impaired people through the RIAS (Remote Infrared Audible Signage) project.
Spectroscopy.
Infrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH2) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm−1, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm−1).
Thin film metrology.
In the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures.
Meteorology.
Weather satellites equipped with scanning radiometers produce thermal or infrared images, which can then enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. The scanning is typically in the range 10.3–12.5 µm (IR4 and IR5 channels).
High, cold ice clouds such as Cirrus or Cumulonimbus show up bright white, lower warmer clouds such as Stratus or Stratocumulus show up as grey with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can be a similar temperature to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 µm) and the near-infrared channel (1.58–1.64 µm), low cloud can be distinguished, producing a "fog" satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied.
These infrared pictures can depict ocean eddies or vortices and map currents such as the Gulf Stream, which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray-shaded thermal images can be converted to color for easier identification of desired information.
The main water vapour channel at 6.40 to 7.08 µm can be imaged by some weather satellites and shows the amount of moisture in the atmosphere.
Climatology.
In the field of climatology, atmospheric infrared radiation is monitored to detect trends in the energy exchange between the earth and the atmosphere. These trends provide information on long-term changes in Earth's climate. It is one of the primary parameters studied in research into global warming, together with solar radiation.
A pyrgeometer is utilized in this field of research to perform continuous outdoor measurements. This is a broadband infrared radiometer with sensitivity for infrared radiation between approximately 4.5 µm and 50 µm.
Astronomy.
Astronomers observe objects in the infrared portion of the electromagnetic spectrum using optical components, including mirrors, lenses and solid state digital detectors. For this reason it is classified as part of optical astronomy. To form an image, the components of an infrared telescope need to be carefully shielded from heat sources, and the detectors are chilled using liquid helium.
The sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy.
The infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.)
Infrared light is also useful for observing the cores of active galaxies, which are often cloaked in gas and dust. Distant galaxies with a high redshift will have the peak portion of their spectrum shifted toward longer wavelengths, so they are more readily observed in the infrared.
Art conservation and analysis.
Infrared reflectography, as called by art conservators, can be applied to paintings to reveal underlying layers in a completely non-destructive manner, in particular the underdrawing or outline drawn by the artist as a guide. This often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Art conservators are looking to see whether the visible layers of paint differ from the underdrawing or layers in between – such alterations are called pentimenti when made by the original artist. This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti the more likely a painting is to be the prime version. It also gives useful insights into working practices.
Among many other changes in the Arnolfini Portrait of 1434 (left), the man's face was originally higher by about the height of his eye; the woman's was higher, and her eyes looked more to the front. Each of his feet was underdrawn in one position, painted in another, and then overpainted in a third. These alterations are seen in infrared reflectograms.
Similar uses of infrared are made by conservators and scientists on various types of objects, especially very old written documents such as the Dead Sea Scrolls, the Roman works in the Villa of the Papyri, and the Silk Road texts found in the Dunhuang Caves. Carbon black used in ink can show up extremely well.
Biological systems.
The pit viper has a pair of infrared sensory pits on its head. There is uncertainty regarding the exact thermal sensitivity of this biological infrared detection system.
Other organisms that have thermoreceptive organs are pythons (family Pythonidae), some boas (family Boidae), the Common Vampire Bat ("Desmodus rotundus"), a variety of jewel beetles ("Melanophila acuminata"), darkly pigmented butterflies ("Pachliopta aristolochiae" and "Troides rhadamantus plateni"), and possibly blood-sucking bugs ("Triatoma infestans").
Although near-infrared vision (780–1000 nm) has long been deemed impossible due to noise in visual pigments, sensation of near-infrared light was reported in the common carp and in three cichlid species. Fish use NIR to capture prey and for phototactic swimming orientation. NIR sensation in fish may be relevant under poor lighting conditions during twilight and in turbid surface waters.
Photobiomodulation.
Near-infrared light, or photobiomodulation, is used for treatment of chemotherapy-induced oral ulceration as well as wound healing. There is some work relating to anti-herpes virus treatment. Research projects include work on central nervous system healing effects via cytochrome c oxidase upregulation and other possible mechanisms.
Health hazard.
Strong infrared radiation in certain industry high-heat settings may be hazardous to the eyes, resulting in damage or blindness to the user. Since the radiation is invisible, special IR-proof goggles must be worn in such places.
Earth as an infrared emitter.
Earth's surface and the clouds absorb visible and invisible radiation from the sun and re-emit much of the energy as infrared back to atmosphere. Certain substances in the atmosphere, chiefly cloud droplets and water vapor, but also carbon dioxide, methane, nitrous oxide, sulfur hexafluoride, and chlorofluorocarbons, absorb this infrared, and re-radiate it in all directions including back to Earth. Thus, the greenhouse effect keeps the atmosphere and surface much warmer than if the infrared absorbers were absent from the atmosphere.
History of infrared science.
The discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them "Calorific Rays". The term 'Infrared' did not appear until late in the 19th century.
Other important dates include:

</doc>
<doc id="15023" url="http://en.wikipedia.org/wiki?curid=15023" title="Icosidodecahedron">
Icosidodecahedron

In geometry, an icosidodecahedron is a polyhedron with twenty triangular faces and twelve pentagonal faces. An icosidodecahedron has 30 identical vertices, with two triangles and two pentagons meeting at each, and 60 identical edges, each separating a triangle from a pentagon. As such it is one of the Archimedean solids and more particularly, a quasiregular polyhedron.
An icosidodecahedron has icosahedral symmetry, and its first stellation is the compound of a dodecahedron and its dual icosahedron, with the vertices of the icosahedron located at the midpoints of the edges of either.
Its dual polyhedron is the rhombic triacontahedron. An icosidodecahedron can be split along any of six planes to form a pair of pentagonal rotundae, which belong among the Johnson solids.
The icosidodecahedron can be considered a "pentagonal gyrobirotunda", as a combination of two rotundae (compare pentagonal orthobirotunda, one of the Johnson solids). In this form its symmetry is D5d, [10,2+], (2*5), order 20.
The wire-frame figure of the icosidodecahedron consists of six flat regular decagons, meeting in pairs at each of the 30 vertices.
Cartesian coordinates.
Convenient Cartesian coordinates for the vertices of an icosidodecahedron with unit edges are given by:
where ϕ is the golden ratio, (1+√5)/2.
Orthogonal projections.
The icosidodecahedron has four special orthogonal projections, centered on a vertex, an edge, a triangular face, and a pentagonal face. The last two correspond to the A2 and H2 Coxeter planes.
Surface area and volume.
The surface area "A" and the volume "V" of the icosidodecahedron of edge length "a" are:
Spherical tiling.
The icosidodecahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.
Related polyhedra.
The icosidodecahedron is a rectified dodecahedron and also a rectified icosahedron, existing as the full-edge truncation between these regular solids.
The icosidodecahedron contains 12 pentagons of the dodecahedron and 20 triangles of the icosahedron:
The icosidodecahedron can be seen in a sequence of quasiregular polyhedrons and tilings:
Dissection.
The icosidodecahedron and is related to the Johnson solid called a pentagonal orthobirotunda created by two pentagonal rotunda connected as mirror images. The "icosidodecahedron" can therefore be called a "pentagonal gyrobirotunda" with the gyration between top and bottom halves.
Related polyhedra.
Eight uniform star polyhedra share the same vertex arrangement. Of these, two also share the same edge arrangement: the small icosihemidodecahedron (having the triangular faces in common), and the small dodecahemidodecahedron (having the pentagonal faces in common). The vertex arrangement is also shared with the compounds of five octahedra and of five tetrahemihexahedra.
Related polytopes.
In four-dimensional geometry the icosidodecahedron appears in the regular 600-cell as the equatorial slice that belongs to the vertex-first passage of the 600-cell through 3D space. In other words: the 30 vertices of the 600-cell which lie at arc distances of 90 degrees on its circumscribed hypersphere from a pair of opposite vertices, are the vertices of an icosidodecahedron. The wire frame figure of the 600-cell consists of 72 flat regular decagons. Six of these are the equatorial decagons to a pair of opposite vertices. They are precisely the six decagons which form the wire frame figure of the icosidodecahedron.
Icosidodecahedral graph.
In the mathematical field of graph theory, a icosidodecahedral graph is the graph of vertices and edges of the icosidodecahedron, one of the Archimedean solids. It has 30 vertices and 60 edges, and is a quartic graph Archimedean graph.

</doc>
<doc id="15024" url="http://en.wikipedia.org/wiki?curid=15024" title="ISO 8601">
ISO 8601

ISO 8601 "Data elements and interchange formats – Information interchange – Representation of dates and times" is an international standard covering the exchange of date and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data are transferred between countries with different conventions for writing numeric dates and times.
In general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, times based on the 24-hour timekeeping system (including optional time zone information), and combinations thereof. The standard does not assign any specific meaning to elements of the date/time to be represented; the meaning will depend on the context of its use. In addition, dates and times to be represented cannot include words with no specified numerical meaning in the standard (e.g. names of years in the Chinese calendar) or that do not use characters (e.g. images, sounds).
In representations for interchange, dates and times are arranged so the largest temporal term (the year) is placed to the left and each successively smaller term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and certain characters (such as "-", "W", and "Z") that are given specific meanings within the standard; The implication is that some commonplace ways of writing parts of dates, such as "January" or "Thursday", are not allowed in interchange representations.
History.
The first edition of the ISO 8601 standard was published in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition in 2000 and by the current third edition published on 3 December 2004. ISO 8601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.
ISO 2014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order [YYYY]-[MM]-[DD]. The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.
Dates.
The standard uses the Gregorian calendar, which serves as an international standard for civil use.
ISO 8601 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the "Convention du Mètre" (Metre Convention) was signed in Paris. However, ISO calendar dates before the Convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on 1582-10-15. Earlier dates, in the proleptic Gregorian calendar, may be used by mutual agreement of the partners exchanging information. The standard states that every date must be consecutive, so usage of the Julian calendar would be contrary to the standard (because at the switchover date, the dates would not be consecutive).
Years.
ISO 8601 prescribes, as a minimum, a four-digit year [YYYY] to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD. However, years prior to 1583 are not automatically allowed by the standard. Instead "values in the range [0000] through [1582] shall only be used by mutual agreement of the partners in information interchange."
To represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [±YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or − sign instead of the more common AD/BC (or BCE/CE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled -0001, and so on.
Calendar dates.
Calendar date representations are in the form shown in the box to the right. [YYYY] indicates a four-digit year, 0000 through 9999. [MM] indicates a two-digit month of the year, 01 through 12. [DD] indicates a two-digit day of that month, 01 through 31. For example, "5 April 1981" may be represented as either "1981-04-05" in the "extended format" or "19810405" in the "basic format".
The standard also allows for calendar dates to be written with reduced precision. For example, one may write "1981-04" to mean "1981 April", and one may simply write "1981" to refer to that year or "19" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the YYYY-MM-DD and YYYYMMDD formats for complete calendar date representations, if the day [DD] is omitted then only the YYYY-MM format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).
Week dates.
Week date representations are in the format as shown in the box to the right. [YYYY] indicates the "ISO week-numbering year" which is slightly different from the traditional Gregorian calendar year (see below). [Www] is the "week number" prefixed by the letter "W", from W01 through W53. [D] is the "weekday number", from 1 through 7, beginning with Monday and ending with Sunday. This form is popular in the manufacturing industries.
There are mutually equivalent descriptions of week 01:
If 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.
The week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.
The "ISO week-numbering year" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The ISO week-numbering year number deviates from the number of the traditional Gregorian calendar year on a Friday, Saturday, and Sunday, or a Saturday and Sunday, or just a Sunday, at the start of the traditional Gregorian calendar year (which are at the end of the previous ISO week-numbering year) and a Monday, Tuesday and Wednesday, or a Monday and Tuesday, or just a Monday, at the end of the traditional Gregorian calendar year (which are in week 01 of the next ISO week-numbering year). For Thursdays, the ISO week-numbering year number is always equal to the traditional Gregorian calendar year number.
Examples:
For an overview of week numbering systems see week number. The US system has weeks from Sunday through Saturday, and partial weeks at the beginning and the end of the year. An advantage is that no separate year numbering like the ISO week-numbering year is needed, while correspondence of lexicographical order and chronological order is preserved.
Ordinal dates.
An ordinal date is a simple form for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. As represented above, [YYYY] indicates a year. [DDD] is the day of that year, from 001 through 365 (366 in leap years). For example, "1981-04-05" is also "1981-095".
This format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes incorrectly referred to as "Julian Date", whereas the astronomical Julian Date is a sequential count of the number of days since day 0 beginning 1 January 4713 BC Greenwich noon, Julian proleptic calendar (or noon on ISO date -4713-11-24 which uses the Gregorian proleptic calendar with a year [0000]).
Times.
ISO 8601 uses the 24-hour clock system. The "basic format" is [hh][mm][ss] and the "extended format" is [hh]:[mm]:[ss].
So a time might appear as either "134730" in the "basic format" or "13:47:30" in the "extended format".
It is also acceptable to omit lower order time elements for reduced precision: [hh]:[mm], [hh][mm] and [hh] are all used. (The use of [hh] alone is considered basic format.)
"Midnight" is a special case and may be referred to as either "00:00" or "24:00". The notation "00:00" is used at the beginning of a calendar day and is the more frequently used. At the end of a day use "24:00". "2007-04-05T24:00" is the same instant as "2007-04-06T00:00" (see "Combined date and time representations" below).
Decimal fractions may be added to any of the three time elements. However, a fraction may only be added to the lowest order time element in the representation. A decimal mark, either a comma or a dot (without any preference as stated in resolution 10 of the 22nd General Conference CGPM in 2003, but with a preference for a comma according to ISO 8601:2004) is used as a separator between the time element and its fraction. To denote "14 hours, 30 and one half minutes", do not include a seconds figure. Represent it as "14:30,5", "1430,5", "14:30.5", or "1430.5". There is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties.
Time zone designators.
Time zones in ISO 8601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.
If no UTC relation information is given with a time representation, the time is assumed to be in local time. While it may be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.
UTC.
If the time is in UTC, add a "Z" directly after the time without a space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". "14:45:15 UTC" would be "14:45:15Z" or "144515Z".
UTC time is also known as 'Zulu' time, since 'Zulu' is the NATO phonetic alphabet word for 'Z'.
Time offsets from UTC.
The offset from UTC is appended to the time in the same way that 'Z' was above, in the form ±[hh]:[mm], ±[hh][mm], or ±[hh]. So if the time being described is one hour ahead of UTC (such as the time in Berlin during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". To represent a time behind UTC the offset is shown with a minus sign (Unicode U+2212, written &minus; in HTML), e.g. the time in New York in winter is . For other time offsets see List of UTC time offsets.
The following times all refer to the same moment: "18:30Z", "22:30+04", "1130−0700", and "15:00−03:30". Nautical time zone letters are not used with the exception of Z. To calculate UTC time one has to subtract the offset from the local time, e.g. for "15:00−03:30" do 15:00 − (−03:30) to get 18:30 UTC.
An offset of zero, in addition to having the special representation "Z", can also be stated numerically as "+00:00", "+0000", or "+00". However, it is not permitted to state it numerically with a negative sign, as "−00:00", "−0000", or "−00". The section dictating sign usage (section 3.4.2 in the 2004 edition of the standard) states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. Contrary to this rule, RFC 3339, which is otherwise a profile of ISO 8601, permits the use of "−00", with the same denotation as "+00" but a differing connotation.
Combined date and time representations.
A single point in time can be represented by concatenating a complete date expression, the letter "T" as a delimiter, and a valid time expression. For example "2007-04-05T14:30".
If a time zone designator is required, it follows the combined date and time. For example "2007-04-05T14:30Z" or "2007-04-05T12:30-02:00".
Either basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time expression may use reduced accuracy. It is permitted to omit the 'T' character by mutual agreement.
Durations.
Durations are a component of time intervals and define the amount of intervening time in a time interval. They should only be used as part of a time interval as prescribed by the standard. Time intervals are discussed in the next section.
Durations are represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W as shown to the right. In these representations, the [n] is replaced by the value for each of the date and time elements that follow the [n]. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters "P", "Y", "M", "W", "D", "T", "H", "M", and "S" are designators for each of the date and time elements and are not replaced.
For example, "P3Y6M4DT12H30M5S" represents a duration of "three years, six months, four days, twelve hours, thirty minutes, and five seconds".
Date and time elements including their designator may be omitted if their value is zero, and lower order elements may also be omitted for reduced precision. For example, "P23DT23H" and "P4Y" are both acceptable duration representations.
To resolve ambiguity, "P1M" is a one-month duration and "PT1M" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in "P0.5Y" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in "P0,5Y" or "P0.5Y". The standard does not prohibit date and time values in a duration representation from exceeding their "carry over points" except as noted below. Thus, "PT36H" could be used as well as "P1DT12H" for representing the same duration.
Alternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]. For example, the first duration shown above would be "P0003-06-04T12:30:05". However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).
Time intervals.
A time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.
There are four ways to express a time interval:
Of these, the first three require two values separated by an "interval designator" which is usually a solidus (more commonly referred to as a forward slash "/"). Section 4.4.2 of the standard notes that: "In certain application areas a double hyphen is used as a separator instead of a solidus." The standard does not define the term "double hyphen", but previous versions used notations like "2000--2002". Use of a double hyphen instead of a solidus allows inclusion in computer filenames. A solidus is a reserved character and not allowed in a filename in common operating systems.
For <start>/<end> expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be simply shown as "2007-12-14T13:30/15:30", where "/15:30" implies "/2007-12-14T15:30" (the same date as the start), or the beginning and end dates of a monthly billing period as "2008-02-15/03-14", where "/03-14" implies "/2008-03-14" (the same year as the start).
If greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted "2007-11-13/15" can start at any time on 2007-11-13 and end at any time on 2007-11-15, whereas "2007-11-13T09:00/15T17:00" includes the start and end times. 
To explicitly include all of the start and end dates, the interval would be represented as "2007-11-13T00:00/15T24:00".
Repeating intervals.
Repeating intervals are specified in clause "4.5 Recurring time interval". They are formed by adding "R[n]/" to the beginning of an interval expression, where "R" is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. If the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of "P1Y2M10DT2H30M" five times starting at "2008-03-01T13:00:00Z", use "R5/2008-03-01T13:00:00Z/P1Y2M10DT2H30M".
Truncated representations.
ISO 8601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used and the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO 8601:2004.
Usage.
On the Internet, the World Wide Web Consortium (W3C) uses ISO 8601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software.
RFC 3339 defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.
RFC 3339 deviates from ISO 8601 in allowing a zero timezone offset to be specified as "-00:00", which ISO 8601 forbids. RFC 3339 intends "-00:00" to carry the connotation that it is not stating a preferred timezone, whereas the conforming "+00:00" or any non-zero offset connotes that the offset being used is preferred. This convention regarding "-00:00" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO 8601, and so was free to use this convention without conflict. 
ISO 8601 is referenced by several specifications, but the full range of options of ISO 8601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO 8601.
The GeneralizedTime makes use of another subset of ISO 8601.
The ISO 8601 week date, as of 2006, appeared in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced to work weeks, and products can be correctly targeted for recall.
See also.
 Media related to at Wikimedia Commons
External links.
Implementation overview

</doc>
<doc id="15027" url="http://en.wikipedia.org/wiki?curid=15027" title="Isa">
Isa

Isa or ISA may refer to:

</doc>
<doc id="15028" url="http://en.wikipedia.org/wiki?curid=15028" title="International Seabed Authority">
International Seabed Authority

The International Seabed Authority (ISA) (French: "Autorité internationale des fonds marins", Spanish: "Autoridad Internacional de los Fondos Marinos") is an intergovernmental body based in Kingston, Jamaica, that was established to organize and control all mineral-related activities in the international seabed area beyond the limits of national jurisdiction, an area underlying most of the world’s oceans. It is an organization established by the Law of the Sea Convention.
Origin.
Following at least ten preparatory meetings over the years, the Authority held its first inaugural meeting in its host country, Jamaica, on 16 November 1994, the day the Convention came into force. The articles governing the Authority have been made "noting the political and economic changes, including market-oriented approaches, affecting the implementation" of the Convention. The Authority obtained its observer status to the United Nations in October 1996.
Currently, the Authority has 159 members and the European Union, composed of all parties to the Law of the Sea Convention.
Two principal organs establish the policies and govern the work of the Authority: the Assembly, in which all members are represented, and a 36-member Council elected by the Assembly. Council members are chosen according to a formula designed to ensure equitable representation of countries from various groups, including those engaged in seabed mineral exploration and the land-based producers of minerals found on the seabed. The Authority holds one annual session, usually of two weeks' duration. Its fifteenth session was held in Kingston 25 May - 5 June 2009 and its sixteenth session is scheduled for 26 April to 7 May 2010.
The Authority operates by contracting with private and public corporations and other entities authorizing them to explore, and eventually exploit, specified areas on the deep seabed for mineral resources essential for building most technological products. The Convention also established a body called the Enterprise which is to serve as the Authority’s own mining operator, but no concrete steps have been taken to bring this into being.
Current activities.
The Authority has a budget of $5.8 million a year (rising to an authorized $6.3 million for each of the years 2009-2010) and a staff of some 35 people. In June 2008, the Assembly of the Authority elected by acclamation Nii Allotey Odunton of Ghana, Deputy to the Secretary-General since 1996, for a four-year term as Secretary-General beginning 1 January 2009. He succeeded Satya Nandan of Fiji, the first Secretary-General of the Authority, who left after three consecutive four-year terms since 1996.
The exploitation system envisaged in the Law of the Sea Convention, overseen by the Authority, came to life with the signature in 2001/02 of 15-year contracts with seven organizations that had applied for specific seabed areas in which they were authorized to explore for polymetallic nodules. In 2006, a German entity was added to the list.
The eight current contractors are: Yuzhmorgeologya (Russian Federation); Interoceanmetal Joint Organization (IOM) (Bulgaria, Cuba, Slovakia, Czech Republic, Poland and Russian Federation); the Government of the Republic of Korea; China Ocean Minerals Research and Development Association (COMRA) (China); Deep Ocean Resources Development Company (DORD) (Japan); Institut français de recherche pour l’exploitation de la mer (IFREMER) (France); the Government of India, and the Federal Institute for Geosciences and Natural Resources of Germany.
All but one of the current areas of exploration are in the Clarion-Clipperton Zone, in the Equatorial North Pacific Ocean south and southeast of Hawaii. The remaining area, being explored by India, is in the Central Indian Basin of the Indian Ocean.
Each area is limited to 150,000 km2, of which half is to be relinquished to the Authority after eight years. Each contractor is required to report once a year on its activities in its assigned area. So far, none of them has indicated any serious move to begin commercial exploitation.
In 2008, the Authority received two new applications for authorization to explore for polymetallic nodules, coming for the first time from private firms in developing island nations of the Pacific. Sponsored by their respective governments, they were submitted by Nauru Ocean Resources Inc. and Tonga Offshore Mining Limited. A 15-year exploration contract was granted by the Authority to Nauru Ocean Resources Inc. on 22 July 2011 and to Tonga Offshore Mining Limited on 12 January 2012.
The Authority's main legislative accomplishment to date has been the adoption, in the year 2000, of regulations governing exploration for polymetallic nodules. These resources, also called manganese nodules, contain varying amounts of manganese, cobalt, copper and nickel. They occur as potato-sized lumps scattered about on the surface of the ocean floor, mainly in the central Pacific Ocean but with some deposits in the Indian Ocean.
The Council of the Authority began work, in August 2002, on another set of regulations, covering polymetallic sulfides and cobalt-rich ferromanganese crusts, which are rich sources of such minerals as copper, iron, zinc, silver and gold, as well as cobalt. The sulphides are found around volcanic hot springs, especially in the western Pacific Ocean, while the crusts occur on oceanic ridges and elsewhere at several locations around the world. The Council decided in 2006 to prepare separate sets of regulations for sulphides and for crusts, with priority given to sulphides. It devoted most of its sessions in 2007 and 2008 to this task, but several issues remained unresolved. Chief among these were the definition and configuration of the area to be allocated to contractors for exploration, the fees to be paid to the Authority and the question of how to deal with any overlapping claims that might arise. Meanwhile, the Legal and Technical Commission reported progress on ferromanganese crusts.
In addition to its legislative work, the Authority organizes annual workshops on various aspects of seabed exploration, with emphasis on measures to protect the marine environment from any harmful consequences. It disseminates the results of these meetings through publications. Studies over several years covering the key mineral area of the Central Pacific resulted in a technical study on biodiversity, species ranges and gene flow in the abyssal Pacific nodule province, with emphasis on predicting and managing the impacts of deep seabed mining A workshop at Manoa, Hawaii, in October 2007 produced a rationale and recommendations for the establishment of "preservation reference areas" in the Clarion-Clipperton Zone, where nodule mining would be prohibited in order to leave the natural environment intact. The most recent workshop, held at Chennai, India, in February 2008, concerned polymetallic nodule mining technology, with special reference to its current status and challenges ahead 
Contrary to early hopes that seabed mining would generate extensive revenues for both the exploiting countries and the Authority, no technology has yet been developed for gathering deep-sea minerals at costs that can compete with land-based mines. Until recently, the consensus has been that economic mining of the ocean depths might be decades away. Moreover, the United States, with some of the most advanced ocean technology in the world, has not yet ratified the Law of the Sea Convention and is thus not a member of the Authority.
In recent years, however, interest in deep-sea mining, especially with regard to ferromanganese crusts and polymetallic sulphides, has picked up among several firms now operating in waters within the national zones of Papua New Guinea, Fiji and Tonga. Papua New Guinea was the first country in the world to grant commercial exploration licenses for seafloor massive sulphide deposits when it granted the initial license to Nautilus Minerals in 1997. Japan’s new ocean policy emphasizes the need to develop methane hydrate and hydrothermal deposits within Japan’s exclusive economic zone and calls for the commercialization of these resources within the next 10 years. Reporting on these developments in his annual report to the Authority in April 2008, Secretary-General Nandan referred also to the upward trend in demand and prices for cobalt, copper, nickel and manganese, the main metals that would be derived from seabed mining, and he noted that technologies being developed for offshore extraction could be adapted for deep sea mining.
In its preamble, UNCLOS defines the international seabed area—the part under ISA jurisdiction—as “the seabed and ocean floor and the subsoil thereof, beyond the limits of national jurisdiction”. There are no maps annexed to the Convention to delineate this area. Rather, UNCLOS outlines the areas of national jurisdiction, leaving the rest for the international portion. National jurisdiction over the seabed normally leaves off at 200 nmi seaward from baselines running along the shore, unless a nation can demonstrate that its continental shelf is naturally prolonged beyond that limit, in which case it may claim up to 350 nmi. ISA has no role in determining this boundary. Rather, this task is left to another body established by UNCLOS, the Commission on the Limits of the Continental Shelf, which examines scientific data submitted by coastal states that claim a broader reach. Maritime boundaries between states are generally decided by bilateral negotiation (sometimes with the aid of judicial bodies), not by ISA.
Recently, there has been much interest in the possibility of exploiting seabed resources in the Arctic Ocean, bordered by Canada, Denmark, Iceland, Norway, Russia and the United States (see Territorial claims in the Arctic). Mineral exploration and exploitation activities in any seabed area not belonging to these states would fall under ISA jurisdiction.
Endowment Fund.
In 2006 the Authority established an Endowment Fund to Support Collaborative Marine Scientific Research on the International Seabed Area. The Fund will aid experienced scientists and technicians from developing countries to participate in deep-sea research organized by international and national institutions. A campaign was launched in February 2008 to identify participants, establish a network of cooperating bodies and seek outside funds to augment the initial $3 million endowment from the Authority.
The International Seabed Authority Endowment Fund promotes and encourages the conduct of collaborative marine scientific research in the international seabed area through two main activities:
The Secretariat of the International Seabed Authority is facilitating these activities by creating and maintaining an ongoing list of opportunities for scientific collaboration, including research cruises, deep-sea sample analysis, and training and internship programmes. This entails building a network of co-operating groups interested in (or presently undertaking) these types of activities and programmes, such as universities, institutions, contractors with the Authority and other entities.
The Secretariat is also actively seeking applications from scientists and other technical personnel from developing nations to be considered for assistance under the Fund. Application guidelines have been prepared for potential recipients to participate in marine scientific research programmes or other scientific co-operation activity, to enroll in training programmes, and to qualify for technical assistance. An advisory panel will evaluate all incoming applications and make recommendations to the Secretary-General of the International Seabed Authority so successful applicants may be awarded with Fund assistance.
To maximize opportunities for and participation in the Fund, the Secretariat is also seeking donations and in-kind contributions to build on the initial investment of US$3 million. This entails raising awareness of the Fund, reporting on its successes and encouraging new activities and participants.
Controversy.
The exact nature of the ISA's mission and authority has been questioned by opponents of the Law of the Sea Treaty who are generally skeptical of multilateral engagement by the United States. The United States is the only major maritime power that has not ratified the Convention (see United States non-ratification of the UNCLOS), with one of the main anti-ratification arguments being a charge that the ISA is flawed or unnecessary. In its original form, the Convention included certain provisions that some found objectionable, such as:
Because of these concerns, the United States pushed for modification of the Convention, obtaining a 1994 Agreement on Implementation that somewhat mitigates them and thus modifies the ISA's authority. Despite this change the United States has not ratified the Convention and so is not a member of ISA, although it sends sizable delegations to participate in meetings as an observer. On 31 October 2007 the Foreign Relations Committee of the United States Senate, by a vote of 17 to 4, recommended ratification, and President George W. Bush publicly supported U.S. accession to the Convention; no date has yet been set for action by the full Senate.

</doc>
<doc id="15029" url="http://en.wikipedia.org/wiki?curid=15029" title="Industry Standard Architecture">
Industry Standard Architecture

 (1993)
Industry Standard Architecture (ISA) is a retronym term for the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles. 
Originally referred to as the PC/AT-bus it was also termed "I/O Channel" by IBM. The ISA concept was coined by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.
The 16-bit ISA bus was used also with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as VESA Local Bus and PCI were used instead, often along with ISA slots on the same mainboard. A derivative of the AT bus structure is still used in the PCMCIA standard, Compact Flash, the PC/104 bus, and internally within Super I/O chips.
History.
The ISA bus was developed by a team led by Mark Dean at IBM as part of the IBM PC project in 1981. It originated as an 8-bit system. The newer 16-bit standard, the IBM AT bus, was introduced in 1984. In 1988, the Gang of Nine IBM PC compatible manufacturers put forth the 32-bit EISA standard and in the process retroactively renamed the AT bus to "ISA" to avoid infringing IBM's trademark on its PC/AT computer. IBM designed the 8 bit version as a buffered interface to the external bus of the Intel 8088 (16/8 bit) CPU used in the original IBM PC and PC/XT, and the 16-bit version as an upgrade for the external bus of the Intel 80286 CPU used in the IBM AT. Therefore, the ISA bus was synchronous with the CPU clock, until sophisticated buffering methods were developed and implemented by chipsets to interface ISA to much faster CPUs.
Designed to connect peripheral cards to the motherboard, ISA allows for bus mastering although only the first 16 MB of main memory are available for direct access. The 8-bit bus ran at 4.77 MHz (the clock speed of the IBM PC and IBM PC/XT's 8088 CPU), while the 16-bit bus operated at 6 or 8 MHz (because the 80286 CPUs in IBM PC/AT computers ran at 6 MHz in early models and 8 MHz in later models.) IBM RT/PC also used the 16-bit bus. It was also available on some non-IBM compatible machines such as Motorola 68k-based Apollo (68020) and Amiga 3000 (68030) workstations, the short-lived AT&T Hobbit and later PowerPC based BeBox.
Companies like Dell improved the AT bus's performance, but in 1987, IBM moved to replace the AT bus with their proprietary Micro Channel Architecture (MCA) in an effort to regain control of the PC architecture and the PC market. (Note the relationship between the IBM term "I/O Channel" for the AT-bus and the name "Micro Channel" for IBM's intended replacement.) MCA had many features that would later appear in PCI, the successor of ISA, but MCA was a closed standard, unlike ISA (PC-bus and AT-bus) for which IBM had released full specifications and even circuit schematics. The system was far more advanced than the AT bus, and computer manufacturers responded with the Extended Industry Standard Architecture (EISA) and later, the VESA Local Bus (VLB). In fact, VLB used some electronic parts originally intended for MCA because component manufacturers already were equipped to manufacture them. Both EISA and VLB were backwards-compatible expansions of the AT (ISA) bus.
Users of ISA-based machines had to know special information about the hardware they were adding to the system. While a handful of devices were essentially "plug-n-play", this was rare. Users frequently had to configure several parameters when adding a new device, such as the IRQ line, I/O address, or DMA channel. MCA had done away with this complication, and PCI actually incorporated many of the ideas first explored with MCA (though it was more directly descended from EISA).
This trouble with configuration eventually led to the creation of ISA PnP, a plug-n-play system that used a combination of modifications to hardware, the system BIOS, and operating system software to automatically manage resource allocations. In reality, ISA PnP can be troublesome, and did not become well-supported until the architecture was in its final days.
PCI slots were the first physically incompatible expansion ports to directly squeeze ISA off the motherboard. At first, motherboards were largely ISA, including a few PCI slots. By the mid-1990s, the two slot types were roughly balanced, and ISA slots soon were in the minority of consumer systems. Microsoft's PC 99 specification recommended that ISA slots be removed entirely, though the system architecture still required ISA to be present in some vestigial way internally to handle the floppy drive, serial ports, etc., which was why the software compatible LPC bus was created. ISA slots remained for a few more years, and towards the turn of the century it was common to see systems with an Accelerated Graphics Port (AGP) sitting near the central processing unit, an array of PCI slots, and one or two ISA slots near the end. In late 2008, even floppy disk drives and serial ports were disappearing, and the extinction of vestigial ISA (by then the LPC bus) from chipsets was on the horizon.
It is also notable that PCI slots are "rotated" compared to their ISA counterparts—PCI cards were essentially inserted "upside-down," allowing ISA and PCI connectors to squeeze together on the motherboard. Only one of the two connectors can be used in each slot at a time, but this allowed for greater flexibility.
The AT Attachment (ATA) hard disk interface is directly descended from ISA (the AT bus). ATA has its origins in hardcards that integrated a hard disk controller (HDC) — usually with an ST-506/ST-412 interface — and a hard disk drive on the same ISA adapter. This was at best awkward from a mechanical structural standpoint, as ISA slots were not designed to support such heavy devices as hard disks (and the 3.5" form-factor hard disks of the time were about twice as tall and heavy as modern drives), so the next generation of Integrated Drive Electronics drives moved both the drive and controller to a drive bay and used a ribbon cable and a very simple interface board to connect it to an ISA slot. ATA, at its essence, is basically a standardization of this arrangement, combined with a uniform command structure for software to interface with the controller on a drive. ATA has since been separated from the ISA bus, and connected directly to the local bus (usually by integration into the chipset), to be clocked much much faster than ISA could support and with much higher throughput. (Notably when ISA was introduced as the AT bus, there "was" no distinction between a "local" and "extension" bus, and there were no chipsets.) Still, ATA retains details which reveal its relationship to ISA. The 16-bit transfer size is the most obvious example; the signal timing, particularly in the PIO modes, is also highly correlated, and the interrupt and DMA mechanisms are clearly from ISA. (The article about ATA has more detail about this history.)
ISA bus architecture.
The PC/XT-bus is an eight-bit ISA bus used by Intel 8086 and Intel 8088 systems in the IBM PC and IBM PC XT in the 1980s. Among its 62 pins were demultiplexed and electrically buffered versions of the eight data and 20 address lines of the 8088 processor, along with power lines, clocks, read/write strobes, interrupt lines, etc. Power lines included -5V and +/-12 V in order to directly support pMOS and enhancement mode nMOS circuits such as dynamic RAMs among other things. The XT bus architecture uses a single Intel 8259 PIC, giving eight vectorized and prioritized interrupt lines. It has four DMA channels originally provided by the Intel 8237, three of the DMA channels are brought out to the XT bus expansion slots; of these, two are normally already allocated to machine functions (diskette drive and hard disk controller): 
The PC/AT-bus, a 16-bit (or 80286-) version of the PC/XT bus, was introduced with the IBM PC/AT. This bus was officially termed "I/O Channel" by IBM. It extends the XT-bus by adding a second shorter edge connector in-line with the eight-bit XT-bus connector, which is unchanged, retaining compatibility with most 8-bit cards. The second connector adds four additional address lines for a total of 24, and eight additional data lines for a total of 16. It also adds new interrupt lines connected to a second 8259 PIC (connected to one of the lines of the first) and four 16-bit DMA channels, as well as control lines to select 8 or 16 bit transfers.
The 16-bit AT bus slot originally used two standard edge connector sockets in early IBM PC/AT machines. However, with the popularity of the AT-architecture and the 16-bit ISA bus, manufacturers introduced specialized 98-pin connectors that integrated the two sockets into one unit. These can be found in almost every AT-class PC manufactured after the mid-1980s. The ISA slot connector is typically black (distinguishing it from the brown EISA connectors and white PCI connectors).
Number of devices.
Motherboard devices have dedicated IRQs (not present in the slots). 16-bit devices can use either PC-bus or PC/AT-bus IRQs. It is therefore possible to connect up to 6 devices that use one 8-bit IRQ each, or up to 5 devices that use one 16-bit IRQ each. At the same time, up to four devices may use one 8-bit DMA channel each, while up to three devices can use one 16-bit DMA channel each.
Varying bus speeds.
Originally, the bus clock was synchronous with the CPU clock, resulting in varying bus clock frequencies among the many different IBM "clones" on the market (sometimes as high as 16 or 20 MHz), leading to software or electrical timing problems for certain ISA cards at bus speeds they were not designed for. Later motherboards or integrated chipsets used a separate clock generator, or a clock divider which either fixed the ISA bus frequency at 4, 6 or 8 MHz or allowed the user to adjust the frequency via the BIOS setup. When used at a higher bus frequency, some ISA cards (certain Hercules-compatible video cards, for instance), could show significant performance improvements.
8/16-bit incompatibilities.
Memory address decoding for the selection of 8 or 16-bit transfer mode was limited to 128 kB sections - A0000..BFFFF, C0000..DFFFF, E0000..FFFFF leading to problems when mixing 8 and 16-bit cards, as they could not co-exist in the same 128 kB area.
Current use.
ISA is still used today for specialized industrial purposes. In 2008 IEI Technologies released a modern motherboard for Intel Core 2 Duo processors which, in addition to other special I/O features, is equipped with two ISA slots. It is marketed to industrial and military users who have invested in expensive specialized ISA bus adaptors, which are not available in PCI bus versions.
Similarly, ADEK Industrial Computers is releasing a motherboard in early 2013 for Intel Core i3/i5/i7 processors, which contains one (non-DMA) ISA slot.
The PC/104 bus, used in industrial and embedded applications, is a derivative of the ISA bus, utilizing the same signal lines with different connectors. The LPC bus has replaced the ISA bus as the connection to the legacy I/O devices on recent motherboards; while physically quite different, LPC looks just like ISA to software, so that the peculiarities of ISA such as the 16 MiB DMA limit (which corresponds to the full address space of the Intel 80286 CPU used in the original IBM AT) are likely to stick around for a while.
ATA.
As explained in the "History" section, ISA was the basis for development of the ATA interface, used for ATA (a.k.a. IDE) and more recently Serial ATA (SATA) hard disks. Physically, ATA is essentially a simple subset of ISA, with 16 data bits, support for exactly one IRQ and one DMA channel, and 3 address bits plus two IDE address select ("chip select") lines, plus a few unique signal lines specific to ATA/IDE hard disks (such as the Cable Select/Spindle Sync. line.) ATA goes beyond and far outside the scope of ISA by also specifying a set of physical device registers to be implemented on every ATA (IDE) drive and accessed using the address bits and address select signals in the ATA physical interface channel; ATA also specifies a full set of protocols and device commands for controlling fixed disk drives using these registers, through which all operations of ATA hard disks are performed. A further deviation between ISA and ATA is that while the ISA bus remained locked into a single standard clock rate (for backward compatibility), the ATA interface offered many different speed modes, could select among them to match the maximum speed supported by the attached drives, and kept adding faster speeds with later versions of the ATA standard (up to 133 MB/s for ATA-6, the latest.) In most forms, ATA ran much faster than ISA.
XT-IDE.
Before the 16-bit ATA/IDE interface, there was an 8-bit XT-IDE (also known as XTA) interface for hard disks. It was not nearly as popular as ATA has become, and XT-IDE hardware is now fairly hard to find. Some XT-IDE adapters were available as 8-bit ISA cards, and XTA sockets were also present on the motherboards of Amstrad's later XT clones. The XTA pinout was very similar to ATA, but only eight data lines and two address lines were used, and the physical device registers had completely different meanings. A few hard drives (such as the Seagate ST351A/X) could support either type of interface, selected with a jumper.
PCMCIA.
The PCMCIA specification can be seen as a superset of ATA. The standard for PCMCIA hard disk interfaces, which included PCMCIA flash drives, allows for the mutual configuration of the port and the drive in an ATA mode. As a de facto extension, most PCMCIA flash drives additionally allow for a simple ATA mode that is enabled by pulling a single pin low, so that PCMCIA hardware and firmware are unnecessary to use them as an ATA drive connected to an ATA port. PCMCIA flash drive to ATA adapters are thus simple and inexpensive, but are not guaranteed to work with any and every standard PCMCIA flash drive. Further, such adapters cannot be used as generic PCMCIA ports, as the PCMCIA interface is much more complex than ATA.
Emulation by embedded chips.
Although most computers do not have physical ISA buses all IBM compatible computers — x86, and x86-64 (most non-mainframe, non-embedded) — have ISA buses allocated in virtual address space. Embedded controller chips (southbridge) and CPUs themselves provide services such as temperature monitoring and voltage readings through these buses as ISA devices.
Standardization.
IEEE started a standardization of the ISA bus in 1985, called the P996 specification. However, despite there even having been books published on the P996 specification, it never officially progressed past draft status.
External links.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="15030" url="http://en.wikipedia.org/wiki?curid=15030" title="Intergovernmental Panel on Climate Change">
Intergovernmental Panel on Climate Change

The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations, set up at the request of member governments. It was first established in 1988 by two United Nations organizations, the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP), and later endorsed by the United Nations General Assembly through Resolution 43/53. Membership of the IPCC is open to all members of the WMO and UNEP.
The IPCC produces reports that support the United Nations Framework Convention on Climate Change (UNFCCC), which is the main international treaty on climate change. The ultimate objective of the UNFCCC is to "stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic [i.e., human-induced] interference with the climate system". IPCC reports cover "the scientific, technical and socio-economic information relevant to understanding the scientific basis of risk of human-induced climate change, its potential impacts and options for adaptation and mitigation."
The IPCC does not carry out its own original research, nor does it do the work of monitoring climate or related phenomena itself. The IPCC bases its assessment on the published literature, which includes peer-reviewed and non-peer-reviewed sources.
Thousands of scientists and other experts contribute (on a voluntary basis, without payment from the IPCC) to writing and reviewing reports, which are then reviewed by governments. IPCC reports contain a "Summary for Policymakers", which is subject to line-by-line approval by delegates from all participating governments. Typically this involves the governments of more than 120 countries.
The IPCC provides an internationally accepted authority on climate change, producing reports which have the agreement of leading climate scientists and the consensus of participating governments. The 2007 Nobel Peace Prize was shared, in two equal parts, between the IPCC and Al Gore.
Aims.
The principles that the IPCC operates under are set out in the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions, as well as on actions in support of the UNFCCC process.
The aims of the IPCC are to assess scientific information relevant to:
Organization.
The chair of the IPCC is presently vacant. The previous chairs were Rajendra K. Pachauri, elected in May 2002; Robert Watson in 1997; and Bert Bolin. The chair is assisted by an elected bureau including vice-chairs, working group co-chairs and a secretariat.
The IPCC Panel is composed of representatives appointed by governments and organizations. Participation of delegates with appropriate expertise is encouraged. Plenary sessions of the IPCC and IPCC Working groups are held at the level of government representatives. Non Governmental and Intergovernmental Organizations may be allowed to attend as observers. Sessions of the IPCC Bureau, workshops, expert and lead authors meetings are by invitation only. Attendance at the 2003 meeting included 350 government officials and climate change experts. After the opening ceremonies, closed plenary sessions were held. The meeting report states there were 322 persons in attendance at Sessions with about seven-eighths of participants being from governmental organizations.
There are several major groups:
The IPCC receives funding from UNEP, WMO, and its own Trust Fund for which it solicits contributions from governments. Its secretariat is hosted by the WMO, in Geneva.
Assessment reports.
The IPCC has published five comprehensive assessment reports reviewing the latest climate science, as well as a number of special reports on particular topics. These reports are prepared by teams of relevant researchers selected by the Bureau from government nominations. Drafts of these reports are made available for comment in open review processes to which anyone may contribute.
The IPCC published its first assessment report in 1990, a supplementary report in 1992, a second assessment report (SAR) in 1995, a third assessment report (TAR) in 2001, a fourth assessment report (AR4) in 2007 and a fifth assessment report (AR5) in 2014.
Each assessment report is in three volumes, corresponding to Working Groups I, II, and III. Unqualified, "the IPCC report" is often used to mean the Working Group I report, which covers the basic science of climate change.
Scope and preparation of the reports.
The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required.
There are generally three stages in the review process:
Review comments are in an open archive for at least five years.
There are several types of endorsement which documents receive:
The Panel is responsible for the IPCC and its endorsement of Reports allows it to ensure they meet IPCC standards.
There have been a range of commentaries on the IPCC's procedures, examples of which are discussed later in the article (see also IPCC Summary for Policymakers). Some of these comments have been supportive, while others have been critical. Some commentators have suggested changes to the IPCC's procedures.
Authors.
Each chapter has a number of authors who are responsible for writing and editing the material. A chapter typically has two "coordinating lead authors", ten to fifteen "lead authors", and a somewhat larger number of "contributing authors". The coordinating lead authors are responsible for assembling the contributions of the other authors, ensuring that they meet stylistic and formatting requirements, and reporting to the Working Group chairs. Lead authors are responsible for writing sections of chapters. Contributing authors prepare text, graphs or data for inclusion by the lead authors.
Authors for the IPCC reports are chosen from a list of researchers prepared by governments and participating organisations, and by the Working Group/Task Force Bureaux, as well as other experts known through their published work. The choice of authors aims for a range of views, expertise and geographical representation, ensuring representation of experts from developing and developed countries and countries with economies in transition.
First assessment report.
The IPCC first assessment report was completed in 1990, and served as the basis of the UNFCCC.
The executive summary of the WG I Summary for Policymakers report says they are certain that emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface. They calculate with confidence that CO2 has been responsible for over half the enhanced greenhouse effect. They predict that under a "business as usual" (BAU) scenario, global mean temperature will increase by about 0.3 °C per decade during the [21st] century. They judge that global mean surface air temperature has increased by 0.3 to 0.6 °C over the last 100 years, broadly consistent with prediction of climate models, but also of the same magnitude as natural climate variability. The unequivocal detection of the enhanced greenhouse effect is not likely for a decade or more.
Supplementary report of 1992.
The 1992 supplementary report was an update, requested in the context of the negotiations on the UNFCCC at the Earth Summit (United Nations Conference on Environment and Development) in Rio de Janeiro in 1992.
The major conclusion was that research since 1990 did "not affect our fundamental understanding of the science of the greenhouse effect and either confirm or do not justify alteration of the major conclusions of the first IPCC scientific assessment". It noted that transient (time-dependent) simulations, which had been very preliminary in the FAR, were now improved, but did not include aerosol or ozone changes.
Second assessment report.
"Climate Change 1995", the IPCC Second Assessment Report (SAR), was finished in 1996. It is split into four parts:
Each of the last three parts was completed by a separate working group, and each has a Summary for Policymakers (SPM) that represents a consensus of national representatives. The SPM of the WG I report contains headings:
Third assessment report.
The Third Assessment Report (TAR) was completed in 2001 and consists of four reports, three of them from its working groups:
A number of the TAR's conclusions are given quantitative estimates of how probable it is that they are correct, e.g., greater than 66% probability of being correct. These are "Bayesian" probabilities, which are based on an expert assessment of all the available evidence.
"Robust findings" of the TAR Synthesis Report include:
Comments on the TAR.
In 2001, 17 national science academies issued a joint-statement on climate change, in which they stated "we support the [TAR's] conclusion that it is at least 90% certain that temperatures will continue to rise, with average global surface temperature projected to increase by between 1.4 and 5.8 °C above 1990 levels by 2100". The TAR has also been endorsed by the Canadian Foundation for Climate and Atmospheric Sciences, Canadian Meteorological and Oceanographic Society, and European Geosciences Union (refer to "Endorsements of the IPCC").
In 2001, the US National Research Council (US NRC) produced a report that assessed Working Group I's (WGI) contribution to the TAR. US NRC (2001) "generally agrees" with the WGI assessment, and describes the full WGI report as an "admirable summary of research activities in climate science".
IPCC author Richard Lindzen has made a number of criticisms of the TAR. Among his criticisms, Lindzen has stated that the WGI Summary for Policymakers (SPM) does not faithfully summarize the full WGI report. For example, Lindzen states that the SPM understates the uncertainty associated with climate models. John Houghton, who was a co-chair of TAR WGI, has responded to Lindzen's criticisms of the SPM. Houghton has stressed that the SPM is agreed upon by delegates from many of the world's governments, and that any changes to the SPM must be supported by scientific evidence.
IPCC author Kevin Trenberth has also commented on the WGI SPM. Trenberth has stated that during the drafting of the WGI SPM, some government delegations attempted to "blunt, and perhaps obfuscate, the messages in the report". However, Trenberth concludes that the SPM is a "reasonably balanced summary".
US NRC (2001) concluded that the WGI SPM and Technical Summary are "consistent" with the full WGI report. US NRC (2001) stated:
[...] the full [WGI] report is adequately summarized in the Technical Summary. The full WGI report and its Technical Summary are not specifically directed at policy. The Summary for Policymakers reflects less emphasis on communicating the basis for uncertainty and a stronger emphasis on areas of major concern associated with human-induced climate change. This change in emphasis appears to be the result of a summary process in which scientists work with policy makers on the document. Written responses from U.S. coordinating and lead scientific authors to the committee indicate, however, that (a) no changes were made without the consent of the convening lead authors (this group represents a fraction of the lead and contributing authors) and (b) most changes that did occur lacked significant impact.
Fourth assessment report.
The Fourth Assessment Report (AR4) was published in 2007. Like previous assessment reports, it consists of four reports:
People from over 130 countries contributed to the IPCC Fourth Assessment Report, which took 6 years to produce. Contributors to AR4 included more than 2500 scientific expert reviewers, more than 800 contributing authors, and more than 450 lead authors.
"Robust findings" of the Synthesis report include:
Global warming projections from AR4 are shown below. The projections apply to the end of the 21st century (2090–99), relative to temperatures at the end of the 20th century (1980–99). Add 0.7 °C to projections to make them relative to pre-industrial levels instead of 1980–99. Descriptions of the greenhouse gas emissions scenarios can be found in Special Report on Emissions Scenarios.
"Likely" means greater than 66% probability of being correct, based on expert judgement.
Response to AR4.
Several science academies have referred to and/or reiterated some of the conclusions of AR4. These include:
The Netherlands Environmental Assessment Agency (PBL, "et al.", 2009; 2010) has carried out two reviews of AR4. These reviews are generally supportive of AR4's conclusions. PBL (2010) make some recommendations to improve the IPCC process. A literature assessment by the US National Research Council (US NRC, 2010) concludes:Climate change is occurring, is caused largely by human activities, and poses significant risks for—and in many cases is already affecting—a broad range of human and natural systems ["emphasis in original text"]. [...] This conclusion is based on a substantial array of scientific evidence, including recent work, and is consistent with the conclusions of recent assessments by the U.S. Global Change Research Program [...], the Intergovernmental Panel on Climate Change’s Fourth Assessment Report [...], and other assessments of the state of scientific knowledge on climate change.
Some errors have been found in the IPCC AR4 Working Group II report. Two errors include the melting of Himalayan glaciers (see later section), and Dutch land area that is below sea level.
Fifth assessment report.
The IPCC's Fifth Assessment Report (AR5) was completed in 2014. AR5 followed the same general format as of AR4, with three Working Group reports and a Synthesis report. The Working Group I report (WG1) was published in September 2013.
Conclusions of AR5 are summarized below:
Representative Concentration Pathways.
Projections in AR5 are based on "Representative Concentration Pathways" (RCPs). The RCPs are consistent with a wide range of possible changes in future anthropogenic greenhouse gas emissions. Projected changes in global mean surface temperature and sea level are given in the main RCP article.
Special reports.
In addition to climate assessment reports, the IPCC is publishing Special Reports on specific topics. The preparation and approval process for all IPCC Special Reports follows the same procedures as for IPCC Assessment Reports. In the year 2011 two IPCC Special Report were finalized, the Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) and the Special Report on Managing Risks of Extreme Events and Disasters to Advance Climate Change Adaptation (SREX). Both Special Reports were requested by governments.
Special Report on Emissions Scenarios (SRES).
The Special Report on Emissions Scenarios (SRES) is a report by the IPCC which was published in 2000. The SRES contains "scenarios" of future changes in emissions of greenhouse gases and sulfur dioxide. One of the uses of the SRES scenarios is to project future changes in climate, e.g., changes in global mean temperature. The SRES scenarios were used in the IPCC's Third and Fourth Assessment Reports.
The SRES scenarios are "baseline" (or "reference") scenarios, which means that they do not take into account any current or future measures to limit greenhouse gas (GHG) emissions (e.g., the Kyoto Protocol to the United Nations Framework Convention on Climate Change). SRES emissions projections are broadly comparable in range to the baseline projections that have been developed by the scientific community.
Comments on the SRES.
There have been a number of comments on the SRES. Parson "et al." (2007) stated that the SRES represented "a substantial advance from prior scenarios". At the same time, there have been criticisms of the SRES.
The most prominently publicized criticism of SRES focused on the fact that all but one of the participating models compared gross domestic product (GDP) across regions using market exchange rates (MER), instead of the more correct purchasing-power parity (PPP) approach. This criticism is discussed in the main SRES article.
Special report on renewable energy sources and climate change mitigation (SRREN).
This assesses existing literature on renewable energy commercialisation for the mitigation of climate change. It covers the six most important renewable energy technologies, as well as their integration into present and future energy systems. It also takes into consideration the environmental and social consequences associated with these technologies, the cost and strategies to overcome technical as well as non-technical obstacles to their application and diffusion.
More than 130 authors from all over the world contributed to the preparation of IPCC Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) on a voluntary basis – not to mention more than 100 scientists, who served as contributing authors.
Special Report on managing the risks of extreme events and disasters to advance climate change adaptation (SREX).
The assesses the effect that climate change has on the threat of natural disasters and how nations can better manage an expected change in the frequency of occurrence and intensity of severe weather patterns. It aims to become a resource for decision-makers to prepare more effectively for managing the risks of these events. A potentially important area for consideration is also the detection of trends in extreme events and the attribution of these trends to human influence.
More than 80 authors, 19 review editors, and more than 100 contributing authors from all over the world contributed to the preparation of SREX.
Methodology reports.
Within IPCC the National Greenhouse Gas Inventory Program develops methodologies to estimate emissions of greenhouse gases. This has been undertaken since 1991 by the IPCC WGI in close collaboration with the Organisation for Economic Co-operation and Development and the International Energy Agency.
The objectives of the National Greenhouse Gas Inventory Program are:
Revised 1996 IPCC Guidelines for National Greenhouse Gas Inventories.
The 1996 Guidelines for National Greenhouse Gas Investories provide the methodological basis for the estimation of national greenhouse gas emissions inventories. Over time these guidelines have been completed with good practice reports: "Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories" and "Good Practice Guidance for Land Use, Land-Use Change and Forestry".
The 1996 guidelines and the two good practice reports are to be used by parties to the UNFCCC and to the Kyoto Protocol in their annual submissions of national greenhouse gas inventories.
2006 IPCC Guidelines for National Greenhouse Gas Inventories.
The 2006 "IPCC Guidelines for National Greenhouse Gas Inventories" is the latest version of these emission estimation methodologies, including a large number of default emission factors. Although the IPCC prepared this new version of the guidelines on request of the parties to the UNFCCC, the methods have not yet been officially accepted for use in national greenhouse gas emissions reporting under the UNFCCC and the Kyoto Protocol.
Activities.
The IPCC concentrates its activities on the tasks allotted to it by the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions as well as on actions in support of the UNFCCC process. While the preparation of the assessment reports is a major IPCC function, it also supports other activities, such as the Data Distribution Centre and the National Greenhouse Gas Inventories Programme, required under the UNFCCC. This involves publishing default emission factors, which are factors used to derive emissions estimates based on the levels of fuel consumption, industrial production and so on.
The IPCC also often answers inquiries from the UNFCCC Subsidiary Body for Scientific and Technological Advice (SBSTA).
Nobel Peace Prize.
In December 2007, the IPCC was awarded the Nobel Peace Prize "for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change". The award is shared with Former U.S. Vice-President Al Gore for his work on climate change and the documentary "An Inconvenient Truth".
Responses.
There is widespread support for the IPCC in the scientific community, which is reflected in publications by other scientific bodies and experts. However, criticisms of the IPCC have been made.
Since 2010 the IPCC has come under yet unparalleled public and political scrutiny. The global IPCC consensus approach has been challenged internally 
 and externally with the 2009 Climatic Research Unit email controversy ("Climategate") an important (but not sole) threshold. It has been deemed an information monopoly with results for both the quality and the impact of the IPCC work as such.
Projected date of melting of Himalayan glaciers.
A paragraph in the 2007 Working Group II report ("Impacts, Adaptation and Vulnerability"), chapter 10 included a projection that Himalayan glaciers could disappear by 2035
This projection was not included in the final summary for policymakers. The IPCC has since acknowledged that the date is incorrect, while reaffirming that the conclusion in the final summary was robust. They expressed regret for "the poor application of well-established IPCC procedures in this instance". The date of 2035 has been correctly quoted by the IPCC from the WWF report, which has misquoted its own source, an ICSI report "Variations of Snow and Ice in the past and at present on a Global and Regional Scale".
Rajendra K. Pachauri responded in an interview with "Science".
Watson criticism.
Former IPCC chairman Robert Watson has said "The mistakes all appear to have gone in the direction of making it seem like climate change is more serious by overstating the impact. That is worrying. The IPCC needs to look at this trend in the errors and ask why it happened". Martin Parry, a climate expert who had been co-chair of the IPCC working group II, said that "What began with a single unfortunate error over Himalayan glaciers has become a clamour without substance" and the IPCC had investigated the other alleged mistakes, which were "generally unfounded and also marginal to the assessment".
Emphasis of the "hockey stick" graph.
The third assessment report (TAR) prominently featured a graph labeled "Millennial Northern Hemisphere temperature reconstruction" based on a 1999 paper by Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99), which has been referred to as the "hockey stick graph". This graph extended the similar graph in from the IPCC Second Assessment Report of 1995, and differed from a schematic in the first assessment report that lacked temperature units, but appeared to depict larger global temperature variations over the past 1000 years, and higher temperatures during the Medieval Warm Period than the mid 20th century. The schematic was not an actual plot of data, and was based on a diagram of temperatures in central England, with temperatures increased on the basis of documentary evidence of Medieval vineyards in England. Even with this increase, the maximum it showed for the Medieval Warm Period did not reach temperatures recorded in central England in 2007. The MBH99 finding was supported by cited reconstructions by , , and , using differing data and methods. The Jones et al. and Briffa reconstructions were overlaid with the MBH99 reconstruction in Figure 2.21 of the IPCC report.
These studies were widely presented as demonstrating that the current warming period is exceptional in comparison to temperatures between 1000 and 1900, and the MBH99 based graph featured in publicity. Even at the draft stage, this finding was disputed by contrarians: in May 2000 Fred Singer's Science and Environmental Policy Project held a press event on Capitol Hill, Washington, D.C., featuring comments on the graph Wibjörn Karlén and Singer argued against the graph at a United States Senate Committee on Commerce, Science and Transportation hearing on 18 July 2000. Contrarian John Lawrence Daly featured a modified version of the IPCC 1990 schematic, which he mis-identified as appearing in the IPCC 1995 report, and argued that "Overturning its own previous view in the 1995 report, the IPCC presented the 'Hockey Stick' as the new orthodoxy with hardly an apology or explanation for the abrupt U-turn since its 1995 report". Criticism of the MBH99 reconstruction in a review paper, which was quickly discredited in the Soon and Baliunas controversy, was picked up by the Bush administration, and a Senate speech by US Republican senator James Inhofe alleged that "manmade global warming is the greatest hoax ever perpetrated on the American people". The data and methodology used to produce the "hockey stick graph" was criticized in papers by Stephen McIntyre and Ross McKitrick, and in turn the criticisms in these papers were examined by other studies and comprehensively refuted by , which showed errors in the methods used by McIntyre and McKitrick.
On 23 June 2005, Rep. Joe Barton, chairman of the House Committee on Energy and Commerce wrote joint letters with Ed Whitfield, Chairman of the Subcommittee on Oversight and Investigations demanding full records on climate research, as well as personal information about their finances and careers, from Mann, Bradley and Hughes. Sherwood Boehlert, chairman of the House Science Committee, said this was a "misguided and illegitimate investigation" apparently aimed at intimidating scientists, and at his request the U.S. National Academy of Sciences arranged for its National Research Council to set up a special investigation. The National Research Council's report agreed that there were some statistical failings, but these had little effect on the graph, which was generally correct. In a 2006 letter to "Nature", Mann, Bradley, and Hughes pointed out that their original article had said that "more widespread high-resolution data are needed before more confident conclusions can be reached" and that the uncertainties were "the point of the article".
The IPCC Fourth Assessment Report (AR4) published in 2007 featured a graph showing 12 proxy based temperature reconstructions, including the three highlighted in the 2001 Third Assessment Report (TAR); as before, and had both been calibrated by newer studies. In addition, analysis of the Medieval Warm Period cited reconstructions by (as cited in the TAR) and . Ten of these 14 reconstructions covered 1,000 years or longer. Most reconstructions shared some data series, particularly tree ring data, but newer reconstructions used additional data and covered a wider area, using a variety of statistical methods. The section discussed the divergence problem affecting certain tree ring data.
Conservative nature of IPCC reports.
Some critics have contended that the IPCC reports tend to underestimate dangers, understate risks, and report only the "lowest common denominator" findings.
On 1 February 2007, the eve of the publication of IPCC's major report on climate, a study was published suggesting that temperatures and sea levels have been rising at or above the maximum rates proposed during the last IPCC report in 2001. The study compared IPCC 2001 projections on temperature and sea level change with observations. Over the six years studied, the actual temperature rise was near the top end of the range given by IPCC's 2001 projection, and the actual sea level rise was above the top of the range of the IPCC projection.
Another example of scientific research which suggests that previous estimates by the IPCC, far from overstating dangers and risks, have actually understated them is a study on projected rises in sea levels. When the researchers' analysis was "applied to the possible scenarios outlined by the Intergovernmental Panel on Climate Change (IPCC), the researchers found that in 2100 sea levels would be 0.5–1.4 m [50–140 cm] above 1990 levels. These values are much greater than the 9–88 cm as projected by the IPCC itself in its Third Assessment Report, published in 2001". This may have been due, in part, to the expanding human understanding of climate.
In reporting criticism by some scientists that IPCC's then-impending January 2007 report understates certain risks, particularly sea level rises, an AP story quoted Stefan Rahmstorf, professor of physics and oceanography at Potsdam University as saying "In a way, it is one of the strengths of the IPCC to be very conservative and cautious and not overstate any climate change risk".
In his December 2006 book, "", and in an interview on Fox News on 31 January 2007, energy expert Joseph Romm noted that the IPCC Fourth Assessment Report is already out of date and omits recent observations and factors contributing to global warming, such as the release of greenhouse gases from thawing tundra.
Political influence on the IPCC has been documented by the release of a memo by ExxonMobil to the Bush administration, and its effects on the IPCC's leadership. The memo led to strong Bush administration lobbying, evidently at the behest of ExxonMobil, to oust Robert Watson, a climate scientist, from the IPCC chairmanship, and to have him replaced by Pachauri, who was seen at the time as more mild-mannered and industry-friendly.
IPCC processes.
Michael Oppenheimer, a long-time participant in the IPCC and coordinating lead author of the Fifth Assessment Report conceded in Science Magazine's State of the Planet 2008-2009 some limitations of the IPCC consensus approach and asks for concurring, smaller assessments of special problems instead of the large scale approach as in the previous IPCC assessment reports. It has become more important to provide a broader exploration of uncertainties. Others see as well mixed blessings of the drive for consensus within the IPCC process and ask to include dissenting or minority positions or to improve statements about uncertainties.
The IPCC process on climate change and its efficiency and success has been compared with dealings with other environmental challenges (compare Ozone depletion and global warming). In case of the Ozone depletion global regulation based on the Montreal Protocol has been successful, in case of Climate Change, the Kyoto Protocol failed. The Ozone case was used to assess the efficiency of the IPCC process.
The lockstep situation of the IPCC is having built a broad science consensus while states and governments still follow different, if not opposing goals. The underlying linear model of policy-making of "more knowledge we have, the better the political response will be" is being doubted.
According to Sheldon Ungar's comparison with global warming, the actors in the ozone depletion case had a better understanding of scientific ignorance and uncertainties. The ozone case communicated to lay persons "with easy-to-understand bridging metaphors derived from the popular culture" and related to "immediate risks with everyday relevance", while the public opinion on climate change sees no imminent danger. The stepwise mitigation of the ozone layer challenge was based as well on successfully reducing regional burden sharing conflicts. In case of the IPCC conclusions and the failure of the Kyoto Protocol, varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions remain an unsolved problem. In the UK, a report for an House of Lords committee asked to urge the IPCC to involve better assessments of costs and benefits of climate change but the Stern Review ordered by the UK government made a stronger argument in favor to combat human-made climate change.
Outdatedness of reports.
Since the IPCC does not carry out its own research, it operates on the basis of scientific papers and independently documented results from other scientific bodies, and its schedule for producing reports requires a deadline for submissions prior to the report's final release. In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. In an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science. However, there has generally been a steady evolution of key findings and levels of scientific confidence from one assessment report to the next.
The submission deadlines for the Fourth Assessment Report (AR4) differed for the reports of each Working Group. Deadlines for the Working Group I report were adjusted during the drafting and review process in order to ensure that reviewers had access to unpublished material being cited by the authors. The final deadline for cited publications was 24 July 2006. The final WG I report was released on 30 April 2007 and the final AR4 Synthesis Report was released on 17 November 2007.
Rajendra Pachauri, the IPCC chair, admitted at the launch of this report that since the IPCC began work on it, scientists have recorded "much stronger trends in climate change", like the unforeseen dramatic melting of polar ice in the summer of 2007, and added, "that means you better start with intervention much earlier".
Burden on participating scientists.
Scientists who participate in the IPCC assessment process do so without any compensation other than the normal salaries they receive from their home institutions. The process is labor-intensive, diverting time and resources from participating scientists' research programs. Concerns have been raised that the large uncompensated time commitment and disruption to their own research may discourage qualified scientists from participating.
In May 2010, Pachauri noted that the IPCC currently had no process for responding to errors or flaws once it issued a report. The problem, according to Pachauri, was that once a report was issued the panels of scientists producing the reports were disbanded.
Proposed organizational overhaul.
In February 2010, in response to controversies regarding claims in the Fourth Assessment Report, five climate scientists – all contributing or lead IPCC report authors – wrote in the journal "Nature" calling for changes to the IPCC. They suggested a range of new organizational options, from tightening the selection of lead authors and contributors, to dumping it in favor of a small permanent body, or even turning the whole climate science assessment process into a moderated "living" Wikipedia-IPCC. Other recommendations included that the panel employ a full-time staff and remove government oversight from its processes to avoid political interference.
InterAcademy Council review.
In March 2010, at the invitation of the United Nations secretary-general and the chair of the IPCC, the InterAcademy Council (IAC) was asked to review the IPCC's processes for developing its reports. The IAC panel, chaired by Harold Tafler Shapiro, convened on 14 May 2010 and released its report on 1 September 2010.
The IAC found that, "The IPCC assessment process has been successful overall". The panel, however, made seven formal recommendations for improving the IPCC's assessment process, including:
The panel also advised that the IPCC avoid appearing to advocate specific policies in response to its scientific conclusions. Commenting on the IAC report, "Nature News" noted that "The proposals were met with a largely favourable response from climate researchers who are eager to move on after the media scandals and credibility challenges that have rocked the United Nations body during the past nine months".
Endorsements of the IPCC.
Various scientific bodies have issued official statements endorsing and concurring with the findings of the IPCC.

</doc>
<doc id="15031" url="http://en.wikipedia.org/wiki?curid=15031" title="IPCC (disambiguation)">
IPCC (disambiguation)

IPCC may refer to:

</doc>
<doc id="15032" url="http://en.wikipedia.org/wiki?curid=15032" title="IBM Personal Computer">
IBM Personal Computer

The IBM Personal Computer, commonly known as the IBM PC, is the original version and progenitor of the IBM PC compatible hardware platform. It is IBM model number 5150, and was introduced on August 12, 1981. It was created by a team of engineers and designers under the direction of Don Estridge of the IBM Entry Systems Division in Boca Raton, Florida.
The generic term "personal computer" was in use before 1981, applied as early as 1972 to the Xerox PARC's Alto, but because of the success of the IBM Personal Computer, the term "PC" came to mean more specifically a desktop microcomputer compatible with IBM's PC products. Within a short time of the introduction, third-party suppliers of peripheral devices, expansion cards, and software proliferated; the influence of the IBM PC on the personal computer market was substantial in standardizing a platform for personal computers. "IBM compatible" became an important criterion for sales growth; only the Apple Macintosh family kept significant market share without compatibility with the IBM personal computer.
History.
Rumors.
International Business Machines (IBM), one of the world's largest companies, had a 62% share of the mainframe computer market in 1981. Its share of the overall computer market, however, had declined from 60% in 1970 to 32% in 1980. Perhaps distracted by , the "colossus of Armonk" completely missed the fast-growing minicomputer market during the 1970s, and was behind rivals such as Wang, Hewlett-Packard, and Control Data in other areas. In 1979 "BusinessWeek" asked "Is IBM just another stodgy, mature company?"
By 1981 IBM's stock price had declined by 22%. Its earnings for the first half the year grew by 5.3%—one third of the inflation rate—while those of minicomputer maker Digital Equipment Corporation (DEC) grew by more than 35%. The company began selling minicomputers, but in January 1982 the United States Department of Justice ended the antitrust suit because, "The New York Times" reported, the government "recognized what computer experts and securities analysts had long since concluded: I.B.M. no longer dominates the computer business".
IBM wished to avoid the same outcome with the new personal computer industry, dominated by the Commodore PET, Atari 8-bit family, Apple II, Tandy Corporation's TRS-80, and various CP/M machines. With $150 million in sales by 1979 and projected annual growth of more than 40% in the early 1980s, the microcomputer market was large enough for IBM's attention. Other large technology companies such as Texas Instruments and Data General had entered it, and some large IBM customers were buying Apples, so the company saw introducing its own personal computer as a defense against rivals, large and small.
In 1980 and 1981 rumors spread of an IBM personal computer, perhaps a miniaturized version of the IBM System/370, while Matsushita acknowledged that it had discussed with IBM the possibility of manufacturing a personal computer for the American company. The Japanese project, codenamed "Go", ended before the 1981 release of the American-designed IBM PC codenamed "Chess", but two simultaneous projects further confused rumors about the forthcoming product.
Too late?
Whether IBM had waited too long to enter an industry Apple and others were already successful in was unclear. Data General and Texas Instruments' small computers were not very successful. Historically selling products developed over several years and which cost as much as the annual sales of a successful microcomputer company, they had to learn how to quickly mass-produce and market new computers. Observers expected AT&T to soon enter the computer industry, and other large companies such as Exxon, Montgomery Ward, Pentel and Sony were designing their own microcomputers.
An observer stated that "IBM bringing out a personal computer would be like teaching an elephant to tap dance." Its least expensive computer was $15,000, the company only sold through its internal sales force and had no experience with resellers or retail stores, and it did not introduce the first product designed to work with non-IBM equipment until 1980. While it traditionally let others pioneer a new market, the personal-computer development and pricing cycles were much faster than for mainframes, with products becoming obsolete quickly. Another observer claimed that IBM made decisions so slowly that, when tested, "what they found is that it would take at least nine months to ship an empty box".
Many in the microcomputer industry resented IBM's power and wealth, and disliked the perception that a company so staid that it had its own employee songbook would legitimize a market founded by startups. The potential importance to microcomputers of a company so prestigious, that a popular saying in American companies stated "No one ever got fired for buying IBM", was nonetheless clear. "InfoWorld", which described itself as "The Newsweekly for Microcomputer Users", stated that "for my grandmother, and for millions of people like her, "IBM " and "computer" are synonymous". "BYTE" ("The Small Systems Journal") stated in an editorial just before the announcement of the IBM PC:
Rumors abound about personal computers to come from giants such as Digital Equipment Corporation and the General Electric Company. But there is no contest. IBM's new personal computer ... is far and away the media star, not because of its features, but because it exists at all. When the number eight company in the Fortune 500 enters the field, that is news ... The influence of a personal computer made by a company whose name has literally come to mean "computer" to most of the world is hard to contemplate.
The editorial acknowledged the fear within the microcomputer industry of a company many viewed as the "enemy", but concluded with optimism: "I want to see personal computing take a giant step."
Predecessors.
Desktop sized programmable calculators by Hewlett Packard had evolved into the HP 9830 BASIC language computer by 1972. In 1972-1973 a team led by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130. In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because it was the first to emulate APL\1130 performance on a portable, single-user computer, "PC Magazine" in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer". The prototype is in the Smithsonian Institution. A non-working industrial design model was also created in 1973 illustrating how the SCAMP engineering prototype could be transformed into a usable product design for the marketplace. IBM executive Bill Lowe used the engineering prototype and design model internally by in his early efforts to demonstrate the viability of creating a single-user computer.
Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer launched in 1975. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton. The IBM 5100 was a complete computer system programmable in BASIC or APL, with a small built-in CRT monitor, keyboard, and tape drive for data storage. It was also very expensive, up to $20,000 USD. It was specifically designed for professional and scientific customers, not business users or hobbyists. Despite news reports that it was the first IBM product without a model number, when the PC was introduced in 1981 it was designated as the IBM 5150, putting it in the "5100" series though its architecture was not directly descended from the IBM 5100. Later models followed in the trend: For example, the PC/XT, IBM Portable Personal Computer, and PC AT are IBM machine types 5160, 5155, and 5170, respectively.
Following SCAMP, the IBM Boca Raton Laboratory created several single-user computer design concepts to support Lowe's ongoing effort to convince IBM there was a strategic opportunity in the personal computer business. A selection of these early IBM design concepts created in the infancy of personal computing is highlighted in the book ‘’DELETE: A Design History of Computer Vapourware.‘’ One such concept in 1977, code-named Aquarius, was a working prototype utilizing advanced bubble memory cartridges. While this design was more powerful and smaller than Apple II launched the same year, the advanced bubble technology was deemed unstable and not ready for mass production.
Project Chess.
Some employees opposed IBM entering the market. One said "Why on earth would you care about the personal computer? It has nothing at all to do with office automation." "Besides", he added, "all it can do is cause embarrassment for IBM". The company had determined from studying the market for years, and building the prototypes during the 1970s, that IBM was unable to internally build a personal computer profitably.
IBM President John Opel was not among those skeptical of personal computers. He and CEO Frank Cary had divided the company into semi-autonomous "Independent Business Units" (IBU) to encourage innovation. After Lowe became the first head of the Entry Level Systems IBU in Boca Raton his team researched the market. Computer dealers were very interested in selling an IBM product, but told Lowe that the company could not design, sell, or service it as IBM had previously done. An IBM microcomputer, they said, must be composed of standard parts that store employees could repair. While dealers disliked Apple's business practices, including a shortage of the Apple II while the company focused on the III, they saw no alternative because they doubted that IBM's traditional sales methods and bureaucracy would change.
As with other large computer companies, new products at IBM typically required about four to five years for development. Atari in 1980 proposed that it act as original equipment manufacturer for an IBM microcomputer. Aware that the company needed to enter the market quickly—even the schools in Broward County, near Boca Raton, purchased Apples—in July 1980 Lowe met with Opel, Cary, and others on the important Corporate Management Committee. He mentioned the Atari proposal, and also suggested acquiring Atari "because we can't do this within the culture of IBM". Lowe demonstrated the concept with an industrial design model based on the Atari 800 platform.
Instead of acquiring Atari, Cary ordered Lowe to build a personal computer within a year. The committee allowed him to form an independent group of employees—"the Dirty Dozen", led by engineer Bill Sydnes—which, Lowe promised, could design a prototype in 30 days. The crude prototype barely worked when he demonstrated it in August, but Lowe presented a detailed business plan that proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM tradition.
The committee agreed that Lowe's approach was the most likely to succeed. With Opel's strong support, in October it approved turning the group into another IBU codenamed "Project Chess" to develop "Acorn", with unusually large funding to help achieve the goal of introducing the product within one year of the August demonstration. After Lowe's promotion in November Don Estridge became the head of Chess, and by January 1981 the team made its first demonstration of the computer within IBM. Other key members included Sydnes, Lewis Eggebrecht, David Bradley, Mark Dean, and David O'Connor. Many were already hobbyists who owned their own computers including Estridge, who had an Apple II. After the team received permission to expand to 150 by the end of 1980, it received more than 500 calls in one day from IBM employees interested in joining the IBU.
Open standards.
The IBM team developed the PC in about a year, and although the company denied doing so, many observers later concluded that IBM intentionally emulated Apple. The many Apple II owners on the team influenced its decision to design the computer with an open architecture and publish technical information so others could build expansion slot peripherals.
Although IBM knew that it could not avoid competition from third-party software on proprietary hardware—Digital Research released CP/M-86 for the IBM Displaywriter, for example—it considered using the IBM 801 RISC processor and its operating system, developed at the Thomas J. Watson Research Center in Yorktown Heights, New York. The 801 processor was more than an order of magnitude more powerful than the Intel 8088, and the operating system more advanced than the PC DOS 1.0 operating system from Microsoft. Ruling out an in-house solution made the team’s job much easier and may have avoided a delay in the schedule, but the ultimate consequences of this decision for IBM were far-reaching. IBM had recently developed the Datamaster business microcomputer, which used an Intel processor and peripheral ICs; familiarity with these chips and the availability of the Intel 8088 processor was a deciding factor in the choice of processor for the new product. Even the 62-pin expansion bus slots were designed to be similar to the Datamaster slots. Delays due to in-house development of the Datamaster software also influenced the design team to a fast-track development process for the PC, with publicly available technical information to encourage third-party developers.
IBM had always used its own components but could not do so profitably with "Acorn". To save time and money, the IBU built the machine with commercial off-the-shelf parts from original equipment manufacturers whenever possible, with assembly occurring in Boca Raton. The IBU would decide whether it would be more economical to "Make or Buy" each manufacturing step. Various IBM divisions for the first time competed with outsiders to build parts of the new computer; a North Carolina IBM factory built the keyboard, the Endicott, New York factory had to lower its bid for printed circuit boards, and a Taiwanese company built the monitor. IBM chose the 8088 over the superior 8086 because Intel offered a better price on the former and could provide more units. Rather than developing unique IBM PC monitor and printer designs, project management used an existing monitor from IBM Japan and an Epson printer. Because of the off-the-shelf parts only the system unit and keyboard has unique IBM industrial design elements, and the IBM copyright appears in only the ROM BIOS and on the company logo.
Because the product would carry the IBM logo, the only corporate division the IBU could not bypass was the Quality Assurance Unit. Another aspect of IBM that did not change was its emphasis on secrecy. Those working on the project were under strict confidentiality agreements. When an individual mentioned in public on a Saturday that his company was working on software for a new IBM computer, IBM security appeared at the company on Monday to investigate the leak. Developers received prototype computers in boxes lined with lead to block X-rays and soldered sealed, and had to keep them in locked, windowless rooms; to develop software Microsoft emulated the PC on a DEC minicomputer and used the prototype for debugging. After the PC's debut, IBM Boca Raton employees continued to decline to discuss their jobs in public. One writer compared the "silence" after asking one about his role at the company to "hit[ting] the wall at the Boston Marathon: the conversation is over".
Debut.
IBM introduced the Personal Computer on 12 August 1981. Pricing started at $1,565 for a configuration with 16K RAM, Color Graphics Adapter, and no disk drives. The company intentionally set prices for it and other configurations that were comparable to Apple's; one analyst stated that IBM "has taken the gloves off". Microsoft, Digital Research, Personal Software, and Peachtree Software provided software for the new computer. The launch titles included both VisiCalc and "Adventure"; the willingness of the colossus of Armonk to sell a video game that, its press release stated, brought "players into a fantasy world of caves and treasures" astounded observers.
The company's rapid development of the computer, and encouragement of outside developers—it did not sell internally developed PC software until April 1984—amazed others; "BYTE" called it "startling", and one developer reported that "it's a very different IBM." In contrast to other companies that did not release technical details—Texas Instruments, for example, intentionally made developing third-party TI 99/4A software difficult, even requiring a lockout chip in cartridges—Sydnes stated that "The definition of a personal computer "is" third-party hardware and software", and Estridge said that IBM did not keep software development proprietary because it could not "out-BASIC Microsoft BASIC. We would have to ... out-VisiCalc VisiCorp and out-Peachtree Peachtree—and you just can't do that."
Outsiders like Microsoft received cooperation that was, one writer said, "unheard of" for IBM. The "IBM PC Technical Reference Manual" included complete circuit schematics, commented ROM BIOS source code, and other engineering and programming information; it was so comprehensive that one reviewer suggested that the manual could serve as a university textbook, and so clear that a developer claimed that he could design an expansion card without seeing the physical computer. Another said "They were very open and helpful about giving us all the technical information we needed. The feeling was so radically different—it's like stepping out into a warm breeze." He concluded, "After years of hassling—fighting the Not-Invented-Here attitude—we're the gods." Estridge explicitly invited small, "cottage" developers to create products, and the company asked users to submit software for publishing by IBM "with", he said, "our logo and our support". IBM sold the PC at a large discount to employees, encouraged them to write software, and distributed a catalog of inexpensive software written by individuals that might not otherwise appear in public.
"BYTE" described IBM as having "the strongest marketing organization in the world", and the PC's sales and marketing also differed from its previous projects. The company was aware of its corporate reputation among potential customers; an early advertisement began "Presenting the IBM of Personal Computers", and another told developers that the company would consider publishing software for "Education. Entertainment. Personal finance. Data management. Self-improvement. Games. Communications. And yes, business." A third advertisement began "My own IBM computer. Imagine that." It emphasized the novelty of an individual owning an IBM computer: "[I]t's yours. For your business, your project, your department, your class, your family and, indeed, for yourself." In addition to its existing corporate sales force IBM opened its own Product Center retail stores, and for the first time sold through others such as ComputerLand. Because retail stores receive revenue from repairing computers and providing warranty service, IBM broke a 70-year tradition by permitting and training non-IBM service personnel to fix the PC.
The Little Tramp.
After considering Alan Alda, Beverly Sills, Kermit the Frog, and Billy Martin as celebrity endorsers IBM chose Charlie Chaplin's The Little Tramp character—played by Billy Scudder—for a series of advertisements based on Chaplin's films. The very popular and award-winning $36-million marketing campaign made the star of "Modern Times"—a film that expresses Chaplin's opposition to big business, mechanization, and technological efficiency—the (as "Creative Computing" described him) "warm cuddly" mascot of one of the world's largest companies.
Chaplin and his character became so widely associated with IBM—"Time" stated that "The Tramp ... has given [it] a human face"—that others used his bowler hat and cane to represent or satirize the company. Although the Chaplin estate sued those like Otrona who used the trademark without permission, "PC Magazine"‍ '​s April 1983 issue had 12 advertisements that referred to the Little Tramp.
Reaction.
"BYTE" was correct in predicting that an IBM personal computer would receive much public attention. David Bunnell recalled that
None of my associates wanted to talk about the Apple II or the Osborne I computer anymore, nor did they want to fantasize about writing the next super-selling program ... All they wanted to talk about was the IBM Personal Computer—what it was, its potential and limitations, and most of all, the impact IBM would have on the business of personal computing.
Within six weeks Bunnell left Osborne/McGraw-Hill and helped found "PC Magazine", the first periodical for the new computer.
Competitors were more skeptical. Adam Osborne said "when you buy a computer from IBM, you buy a la carte. By the time you have a computer that does anything, it will cost more than an Apple. I don't think Apple has anything to worry about." Apple's Mike Markkula agreed that IBM's product was more expensive than the Apple II, and claimed that its Apple III business computer "offers better performance". He denied that the IBM PC offered more memory, stating that his company could offer more than 128K "but frankly we don't know what anyone would do with that memory". Jon Shirley of Tandy admitted that IBM had a "legendary service reputation" but claimed that his company's thousands of Radio Shack stores "can provide better service", while predicting the IBM PC's "major market will be IBM addicts". Many believed that the IBM PC's alleged weaknesses, such as the use of single-sided, single-density disks with less storage than the computer's RAM, existed because the company was uncertain about the market and was experimenting before releasing a better computer.
Rivals had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Radio Shack had 14 million customers and 8,000 stores—more than McDonald's—that only sold its broad range of computers and accessories. Apple had five times as many dealers in the US as IBM, an established international distribution network, and an installed base of more than 250,000 customers. Hundreds of independent developers produced software and peripherals for both companies' computers; at least ten Apple databases and ten word processors were available, while the PC had no databases and one word processor.
Steve Jobs at Apple ordered a team to examine an IBM PC. After finding it unimpressive—Chris Espinosa called the computer "a half-assed, hackneyed attempt"—the company confidently purchased a full-page advertisement in "The Wall Street Journal" with the headline "Welcome, IBM. Seriously". Microsoft head Bill Gates was at Apple headquarters the day of IBM's announcement and later said "They didn't seem to care. It took them a full year to realize what had happened".
Success.
The IBM PC was immediately successful. "BYTE" reported a rumor that more than 40,000 were ordered on the day of the announcement; one dealer reportedly received 22 $1,000 deposits from customers although he could not promise a delivery date. The company could have sold its entire projected first-year production to employees, and IBM customers that were reluctant to purchase Apples were glad to buy microcomputers from its traditional supplier. By October some referred to the computer simply as the "PC".
By COMDEX in November Tecmar developed 20 products including memory expansion and expansion chassis, surprising even IBM. Jerry Pournelle reported after attending the 1982 West Coast Computer Faire that because IBM "encourages amateurs" with "documents that tell all", "an explosion of [third-party] hardware and software" was visible at the convention. "PC World" counted 753 software packages for the PC after one year—more than four times the number available for the Apple Macintosh one year after its 1984 release—including 422 applications and almost 200 utilities and languages. By that time 30 to 40 companies engaged in what "InfoWorld" described as "bloodthirsty" competition to sell memory-expansion cards, and "PC Magazine" renamed its planned "1001 Products to Use with Your IBM PC" special issue after the number of product listings it received exceeded the figure. Tecmar and other companies that benefited from IBM's openness rapidly grew in size and importance, as did "PC Magazine"; within two years it expanded from 96 bimonthly to 800 monthly pages, including almost 500 pages of advertisements.
Although IBM sold fewer than 100,000 PCs in its first year, by the end of 1982 the company was selling one every minute of the business day. It estimated that 50 to 70% of PCs sold in retail stores went to the home, and the publicity from selling a popular product to consumers caused IBM to, a spokesman said, "enter the world" by familiarizing them with the Colossus of Armonk. Although the PC only provided two to three percent of sales the company found that it had underestimated demand by as much as 800%, and because its prices were based on forecasts of much lower volume—250,000 over five years, which would have made the PC a very successful IBM product—the computer became very profitable.
In August 1983 the Chess IBU, with 4,000 employees, became the Entry Systems Division. The PC surpassed the Apple II as the best-selling personal computer with more than 750,000 sold by the end of the year, while DEC only sold 69,000 microcomputers in the first nine months of the year despite offering three models for different markets. "inCider" wrote "This may be an Apple magazine, but let's not kid ourselves, IBM has devoured competitors like a cloud of locusts". Retailers also benefited, with 65% of BusinessLand's revenue coming from the PC. Demand still so exceeded supply two years after its debut that, despite IBM shipping 40,000 PCs a month, dealers reportedly received 60% or less of their desired quantity. Pournelle received the PC he paid for in early July 1983 on 1 November, and IBM Boca Raton employees and neighbors had to wait five weeks to buy the computers asssembled there.
Domination.
By then Apple was less welcoming of the rival "inCider" stated had a "godlike" reputation. Its focus on the III had delayed improvements to the II, and the sophisticated Lisa was unsuccessful in part because, unlike the II and the PC, Apple discouraged third-party developers. The head of a retail chain said "It appears that IBM had a better understanding of why the Apple II was successful than had Apple." Jobs, after trying to recruit Estridge to become Apple's president, admitted that in two years IBM had joined Apple as "the industry's two strongest competitors". He warned in a speech before previewing the forthcoming "1984" Super Bowl commercial: "It appears IBM wants it "all" ... Will Big Blue dominate the entire computer industry? The entire information age? Was George Orwell right about 1984?" IBM had $4 billion in annual PC revenue by 1984, more than twice that of Apple and as much as the sales of Apple, Commodore, HP, and Sperry combined. A "Fortune" survey found that 56% of American companies with personal computers used IBM PCs, compared to Apple's 16%.
One traditional strategy that the company did not abandon was aggressive pricing; as competitors began to affect demand for the PC, it lowered prices to maintain sales. In his 1985 obituary, "The New York Times" wrote that Estridge had led the "extraordinarily successful entry of the International Business Machines Corporation into the personal computer field". The Entry Systems Division had 10,000 employees and by itself would have been the world's third-largest computer company behind IBM and DEC, with more revenue than IBM's minicomputer business despite its much later start. IBM was the only major company with significant minicomputer and microcomputer businesses, in part because rivals like DEC and Wang did not adjust to the retail market.
Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. Other manufacturers soon reverse engineered the BIOS to produce their own non-infringing functional copies. Columbia Data Products introduced the first IBM-PC compatible computer in June 1982. In November 1982, Compaq Computer Corporation announced the "Compaq Portable", the first portable IBM PC compatible. The first models were shipped in January 1983.
IBM PC as standard.
The success of the IBM computer led other companies to develop "IBM Compatibles", which in turn led to branding like diskettes being advertised as "IBM format". An IBM PC clone could be built with off-the-shelf parts, but the BIOS required some reverse-engineering. Companies like Compaq, Phoenix Software Associates, American Megatrends, Award, and others achieved fully functional versions of the BIOS, allowing companies like DELL, Gateway and HP to manufacture PCs that worked like IBM's product. The IBM PC became the industry standard.
Third-party distribution.
Because IBM had no retail experience, the retail chains ComputerLand and Sears Roebuck provided important knowledge of the marketplace. ComputerLand and Sears became the main outlets for the new product. More than 190 Computerland stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product. This guaranteed IBM widespread distribution across the U.S.
Targeting the new PC at the home market, Sears Roebuck sales failed to live up to expectations. This unfavorable outcome revealed that the strategy of targeting the office market was the key to higher sales.
Models.
All IBM personal computers are software backwards-compatible with each other in general, but not every program will work in every machine. Some programs are time sensitive to a particular speed class. Older programs will not take advantage of newer higher-resolution and higher-color display standards, while some newer programs require newer display adapters. (Note that as the display adapter was an adapter card in all of these IBM models, newer display hardware could easily be, and often was, retrofitted to older models.) A few programs, typically very early ones, are written for and require a specific version of the IBM PC BIOS ROM. Most notably, BASICA which was dependent on the BIOS ROM had a sister program called GW-BASIC which supported more functions and was 100% backwards compatible and could run independent from the BIOS ROM.
PC.
The CGA video card, with a suitable modulator, could use an NTSC television set or an RGBi monitor for display; IBM's RGBi monitor was their display model 5153. The other option that was offered by IBM was an MDA and their monochrome display model 5151. It was possible to install both an MDA and a CGA card and use both monitors concurrently if supported by the application program. For example, AutoCAD, Lotus 1-2-3 and others allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Some model 5150 PCs with CGA monitors and a printer port also included the MDA adapter by default, because IBM provided the MDA port and printer port on the same adapter card; it was in fact an MDA/printer port combo card.
Although cassette tape was originally envisioned by IBM as a low-budget storage alternative, the most commonly used medium was the floppy disk. The 5150 was available with one or two 5-1/4" floppy drives - with two drives the program disc(s) would be in drive A, while drive B would hold the disc(s) for working files; with one drive the user had to swap program and file discs into the single drive. For models without any drives or storage medium, IBM intended users to connect their own cassette recorder via the 5150's cassette socket. The cassette tape socket was physically the same DIN plug as the keyboard socket and next to it, but electrically completely different.
A hard disk could not be installed into the 5150's system unit without changing to a higher-rated power supply. The "IBM 5161 Expansion Chassis" came with its own power supply and one 10 MB hard disk and allowed the installation of a second hard disk. The system unit had five expansion slots, and the expansion unit had eight; however, one of the system unit's slots and one of the expansion unit's slots had to be occupied by the Extender Card and Receiver Card, respectively, which were needed to connect the expansion unit to the system unit and make the expansion unit's other slots available, for a total of 11 slots. A working configuration required that some of the slots be occupied by display, disk, and I/O adapters, as none of these were built into the 5150's motherboard; the only motherboard external connectors were the keyboard and cassette ports.
The simple PC speaker sound hardware was also on board.
The original PC's maximum memory using IBM parts was 256 kB, achievable through the installation of 64 kB on the motherboard and three 64 kB expansion cards. The processor was an Intel 8088 running at 4.77 MHz, 4/3 the standard NTSC color burst frequency of 315/88 = 3.57954 MHz. (In early units, the Intel 8088 used was a 1978 version, later were 1978/81/2 versions of the Intel chip; second-sourced AMDs were used after 1983). Some owners replaced the 8088 with an NEC V20 for a slight increase in processing speed and support for real mode 80186 instructions. The V20 gained its speed increase through the use of a hardware multiplier which the 8088 lacked. An Intel 8087 co-processor could also be added for hardware floating-point arithmetic.
IBM sold the first IBM PCs in configurations with 16 or 64 kB of RAM preinstalled using either nine or thirty-six 16-kilobit DRAM chips. (The ninth bit was used for parity checking of memory.) After the IBM XT shipped, the IBM PC motherboard was configured more like the XTs motherboard with 8 narrower slots, as well as the same RAM configuration as the IBM XT. (64 kB in one bank, expandable to 256kB by populating the other 3 banks.)
Although the TV-compatible video board, cassette port and Federal Communications Commission Class B certification were all aimed at making it a home computer, the original PC proved too expensive for the home market. At introduction, a PC with 64 kB of RAM and a single 5.25-inch floppy drive and monitor sold for US $3,005 (<br>{Inflation} - Amount must not have "" prefix: 3005.  equivalent to $ in 2015), while the cheapest configuration (US $1,565) that had no floppy drives, only 16 kB RAM, and no monitor (again, under the expectation that users would connect their existing TV sets and cassette recorders) proved too unattractive and low-spec, even for its time (cf. footnotes to the above IBM PC range table). While the 5150 did not become a top selling home computer, its floppy-based configuration became an unexpectedly large success with businesses.
XT.
The "IBM Personal Computer XT", IBM model 5160, was introduced two years after the PC and featured a 10 megabyte hard drive. It had eight expansion slots but the same processor and clock speed as the PC. The XT had no cassette jack, but still had the Cassette Basic interpreter in ROMs.
The XT could take 256 kB of memory on the main board (using 64 kbit DRAM); later models were expandable to 640 kB. The remaining 384 kilobytes of the 8088 address space were used for the BIOS ROM, adapter ROM and RAM space, including video RAM space. It was usually sold with a Monochrome Display Adapter (MDA) video card or a CGA video card.
The eight expansion slots were the same as the model 5150 but were spaced closer together. Although rare, a card designed for the 5150 would be wide enough to obstruct the adjacent slot in an XT. Because of the spacing, an XT motherboard would not fit into a case designed for the PC motherboard, but the slots and peripheral cards were compatible. The XT expansion bus (later called "8 bit Industry Standard Architecture" (ISA) by competitors) was retained in the IBM AT, which added connectors for some slots to allow 16-bit transfers; 8 bit cards could be used in an AT.
XT/370.
The "IBM Personal Computer XT/370" was an XT with three custom 8-bit cards: the processor card (370PC-P) contained a modified Motorola 68000 chip, microcoded to execute System/370 instructions, a second 68000 to handle bus arbitration and memory transfers, and a modified 8087 to emulate the S/370 floating point instructions. The second card (370PC-M) connected to the first and contained 512 kB of memory. The third card (PC3277-EM), was a 3270 terminal emulator necessary to install the system software for the VM/PC software to run the processors.
The computer booted into DOS, then ran the VM/PC Control Program.
PCjr.
The "IBM PCjr" was IBM's first attempt to enter the market for relatively inexpensive educational and home-use personal computers. The PCjr, IBM model number 4860, retained the IBM PC's 8088 CPU and BIOS interface for compatibility, but its cost and differences in the PCjr's architecture, as well as other design and implementation decisions, eventually led to the PCjr, and the related IBM JX, being commercial failures.
Portable.
The "IBM Portable Personal Computer" 5155 model 68 was an early portable computer developed by IBM after the success of Compaq's suitcase-size portable machine (the Compaq Portable). It was released in February, 1984, and was eventually replaced by the IBM Convertible.
The Portable was an XT motherboard, transplanted into a Compaq-style luggable case. The system featured 256 kilobytes of memory (expandable to 512 kB), an added CGA card connected to an internal monochrome (amber) composite monitor, and one or two half-height 5.25" 360K floppy disk drives. Unlike the Compaq Portable, which used a dual-mode monitor and special display card, IBM used a stock CGA board and a composite monitor, which had lower resolution. It could however, display color if connected to an external monitor or television.
AT.
The "IBM Personal Computer/AT" (model 5170), announced August 15, 1984, used an Intel 80286 processor, originally running at 6 MHz. It had a 16-bit ISA bus and 20 MB hard drive. A faster model, running at 8 MHz and sporting a 30-megabyte hard disk was introduced in 1986.
The AT was designed to support multitasking; the new SysRq (System request key), little noted and often overlooked, is part of this design, as is the 80286 itself, the first Intel 16-bit processor with multitasking features (i.e. the 80286 protected mode). IBM made some attempt at marketing the AT as a multi-user machine, but it sold mainly as a faster PC for power users. For the most part, IBM PC/ATs were used as more powerful DOS (single-tasking) personal computers, in the literal sense of the PC name.
Early PC/ATs were plagued with reliability problems, in part because of some software and hardware incompatibilities, but mostly related to the internal 20 MB hard disk, and High Density Floppy Disk Drive.
While some people blamed IBM's hard disk controller card and others blamed the hard disk manufacturer Computer Memories Inc. (CMI), the IBM controller card worked fine with other drives, including CMI's 33-MB model. The problems introduced doubt about the computer and, for a while, even about the 286 architecture in general, but after IBM replaced the 20 MB CMI drives, the PC/AT proved reliable and became a lasting industry standard.
AT/370.
The "IBM Personal Computer AT/370" was an AT with two custom 16-bit cards, running almost exactly the same setup as the XT/370.
Convertible.
The IBM PC Convertible, released April 3, 1986, was IBM's first laptop computer and was also the first IBM computer to utilize the 3.5" floppy disk which went on to become the standard. Like modern laptops, it featured power management and the ability to run from batteries. It was the follow-up to the IBM Portable and was model number 5140. The concept and the design of the body was made by the German industrial designer Richard Sapper.
It utilized an Intel 80c88 CPU (a CMOS version of the Intel 8088) running at 4.77 MHz, 256 kB of RAM (expandable to 640 kB), dual 720 kB 3.5" floppy drives, and a monochrome CGA-compatible LCD screen at a price of $2,000. It weighed 13 pounds (5.8 kg) and featured a built-in carrying handle.
The PC Convertible had expansion capabilities through a proprietary ISA bus-based port on the rear of the machine. Extension modules, including a small printer and a video output module, could be snapped into place. The machine could also take an internal modem, but there was no room for an internal hard disk.
Next-generation IBM PS/2.
The IBM PS/2 line was introduced in 1987. The Model 30 at the bottom end of the lineup was very similar to earlier models; it used an 8086 processor and an ISA bus. The Model 30 was not "IBM compatible" in that it did not have standard 5.25-inch drive bays; it came with a 3.5-inch floppy drive and optionally a 3.5-inch-sized hard disk. Most models in the PS/2 line further departed from "IBM compatible" by replacing the ISA bus completely with Micro Channel Architecture.
Technology.
Electronics.
The main circuit board in an PC is called the motherboard (IBM terminology calls it a "planar"). This mainly carries the CPU and RAM, and it has a bus with slots for expansion cards. On the motherboard are also the ROM subsystem, DMA and IRQ controllers, coprocessor socket, sound (PC speaker, tone generation) circuitry, and keyboard interface. The original PC also has a cassette interface.
The bus used in the original PC became very popular, and it was subsequently named ISA. While it was popular, it was more commonly known as the PC-bus or XT-bus; the term "ISA" arose later when industry leaders chose to continue manufacturing machines based on the IBM PC AT architecture rather than license the PS/2 architecture and its MCA bus from IBM. The XT-bus was then retroactively named "8-bit ISA" or "XT ISA", while the unqualified term "ISA" usually refers to the 16-bit AT-bus (as better defined in the ISA specifications.) The AT-bus is an extension of the PC-/XT-bus and is in use to this day in computers for industrial use, where its relatively low speed, 5 volt signals, and relatively simple, straightforward design (all by year 2011 standards) give it technical advantages (e.g. noise immunity for reliability).
A monitor and any floppy or hard disk drives are connected to the motherboard through cables connected to graphics adapter and disk controller cards, respectively, installed in expansion slots. Each expansion slot on the motherboard has a corresponding opening in the back of the computer case through which the card can expose connectors; a blank metal cover plate covers this case opening (to prevent dust and debris intrusion and control airflow) when no expansion card is installed. Memory expansion beyond the amount installable on the motherboard was also done with boards installed in expansion slots, and I/O devices such as parallel, serial, or network ports were likewise installed as individual expansion boards. For this reason, it was easy to fill the five expansion slots of the PC, or even the eight slots of the XT, even without installing any special hardware. Companies like Quadram and AST addressed this with their popular multi-I/O cards which combine several peripherals on one adapter card that uses only one slot; Quadram offered the QuadBoard and AST the SixPak.
Intel 8086 and 8088-based PCs require expanded memory (EMS) boards to work with more than 640 kB of memory. (Though the 8088 can address one megabyte of memory, the last 384 kB of that is used or reserved for the BIOS ROM, BASIC ROM, extension ROMs installed on adapter cards, and memory address space used by devices including display adapter RAM and even the 64 kB EMS page frame itself.) The original IBM PC AT used an Intel 80286 processor which can access up to 16 MB of memory (though standard DOS applications cannot use more than one megabyte without using additional APIs.) Intel 80286-based computers running under OS/2 can work with the maximum memory.
Peripheral integrated circuits.
The set of peripheral chips selected for the original IBM PC defined the functionality of an IBM compatible. These became the de facto base for later application specific integrated circuits (ASIC)s used in compatible products.
The original system chips were one Intel 8259 programmable interrupt controller (PIC) (at I/O address 0x20), one Intel 8237 direct memory access (DMA) controller (at I/O address 0x00),and an Intel 8253 programmable interval timer (PIT) (at I/O address 0x40). The PIT provides the 18.2 Hz clock ticks, dynamic memory refresh timing, and can be used for speaker output; one DMA channel is used to perform the memory refresh.
The mathematics coprocessor was the Intel 8087 using I/O address 0xF0. This was an option for users who needed extensive floating-point arithmetic, such as users of computer-aided drafting.
The IBM PC AT added a second, slave 8259 PIC (at I/O address 0xA0), a second 8237 DMA controller for 16-bit DMA (at I/O address 0xC0), a DMA address register (implemented with a 74LS612 IC) (at I/O address 0x80), and a Motorola MC146818 real-time clock (RTC) with nonvolatile memory (NVRAM) used for system configuration (replacing the DIP switches and jumpers used for this purpose in PC and PC/XT models (at I/O address 0x70). On expansion cards, the Intel 8255 programmable peripheral interface (PPI) (at I/O addresses 0x378 is used for parallel I/O controls the printer, and the 8250 universal asynchronous receiver/transmitter (UART) (at I/O address 0x3F8 or 0x3E8) controls the serial communication at the (pseudo-) RS-232 port.
Joystick port.
As part of its bid to target the home computer market, IBM offered the Game Control Adapter for the PC, which supported analog joysticks similar to those on the Apple II. Although analog controls proved inferior for arcade-style games, they were an asset in certain other genres such as flight simulators. The joystick port on the IBM PC supported two controllers, but required a Y-splitter cable to connect both at once. It remained the standard joystick interface on IBM compatibles until being replaced by USB during the 2000s.
Keyboard.
The keyboard that came with the IBM 5150 was an extremely reliable and high-quality electronic keyboard originally developed in North Carolina for the Datamaster system. Each key was rated to be reliable to over 100 million keystrokes. For the IBM PC, a separate keyboard housing was designed with a novel usability feature that allowed users to adjust the keyboard angle for personal comfort. Compared with the keyboards of other small computers at the time, the IBM PC keyboard was far superior and played a significant role in establishing a high-quality impression. For example, the industrial design of the keyboard, together with the system unit, was recognized with a major design award. "Byte" magazine in the fall of 1981 went so far as to state that the keyboard was 50% of the reason to buy an IBM PC. The importance of the keyboard was definitely established when the 1983 IBM PCjr flopped, in very large part for having a much different and mediocre Chiclet keyboard that made a poor impression on customers. Oddly enough, the same thing almost happened to the original IBM PC when in early 1981 management seriously considered substituting a cheaper and lower quality keyboard. This mistake was narrowly avoided on the advice of one of the original development engineers.
However, the original 1981 IBM PC 84-key keyboard was criticized by typists for its non-standard placement of the Return and left Shift keys, and because it did not have separate cursor and numeric pads that were popular on the pre-PC DEC VT100 series video terminals. In 1982, Key Tronic introduced the now standard 101-key PC keyboard. In 1984, IBM corrected the Return and left ⇧ Shift keys on its AT keyboard, but shortened the Backspace key, making it harder to reach. In 1986, IBM changed to the 101 key enhanced keyboard, which added the separate cursor and numeric key pads, relocated all the function keys and the Ctrl keys, and the Esc key was also relocated to the opposite side of the keyboard.
Another feature of the original keyboard is the relatively loud "click" sound each key made when pressed. Since typewriter users were accustomed to keeping their eyes on the hardcopy they were typing from and had come to rely on the mechanical sound that was made as each character was typed onto the paper to ensure that they had pressed the key hard enough (and only once), the PC keyboard used a keyswitch that produced a click and tactile bump intended to provide that same reassurance.
The IBM PC keyboard is very robust and flexible. The low-level interface for each key is the same: each key sends a signal when it is pressed and another signal when it is released. An integrated microcontroller in the keyboard scans the keyboard and encodes a "scan code" and "release code" for each key as it is pressed and released separately. Any key can be used as a shift key, and a large number of keys can be held down simultaneously and separately sensed. The controller in the keyboard handles typematic operation, issuing periodic repeat scan codes for a depressed key and then a single release code when the key is finally released.
An "IBM PC compatible" may have a keyboard that does not recognize every key combination a true IBM PC does, such as shifted cursor keys. In addition, the "compatible" vendors sometimes used proprietary keyboard interfaces, preventing the keyboard from being replaced.
Although the PC/XT and AT used the same style of keyboard connector, the low-level protocol for reading the keyboard was different between these two series. The AT keyboard uses a bidirectional interface which allows the computer to send commands to the keyboard. An AT keyboard could not be used in an XT, nor the reverse. Third-party keyboard manufacturers provided a switch on some of their keyboards to select either the AT-style or XT-style protocol for the keyboard.
Character set.
The original IBM PC used the 7-bit ASCII alphabet as its basis, but extended it to 8 bits with nonstandard character codes. This character set was not suitable for some international applications, and soon a veritable cottage industry emerged providing variants of the original character set in various national variants. In IBM tradition, these variants were called code pages. These codings are now obsolete, having been replaced by more systematic and standardized forms of character coding, such as ISO 8859-1, Windows-1251 and Unicode. The original character set is known as code page 437.
Storage media.
Cassette tape.
IBM equipped the model 5150 with a cassette port for connecting a cassette drive and assumed that home users would purchase the low-end model and save files to cassette tapes as was typical of home computers of the time. However, adoption of the floppy- and monitor-less configuration was low; few (if any) IBM PCs left the factory without a floppy disk drive installed. Also, DOS was not available on cassette tape, only on floppy disks (hence "Disk Operating System"). 5150s with just external cassette recorders for storage could only use the built-in ROM BASIC as their operating system. As DOS saw increasing adoption, the incompatibility of DOS programs with PCs that used only cassettes for storage made this configuration even less attractive. The ROM BIOS supported cassette operations.
The IBM PC cassette interface encodes data using frequency modulation with a variable data rate. Either a one or a zero is represented by a single cycle of a square wave, but the square wave frequencies differ by a factor of two, with ones having the lower frequency. Therefore, the bit periods for zeros and ones also differ by a factor of two, with the unusual effect that a data stream with more zeros than ones will use less tape (and time) than an equal-length (in bits) data stream containing more ones than zeros, or equal numbers of each.
IBM also had an exclusive license agreement with Microsoft to include BASIC in the ROM of the PC; clone manufacturers could not have ROM BASIC on their machines, but it also became a problem as the XT, AT, and PS/2 eliminated the cassette port and IBM was still required to install the (now useless) BASIC with them. The agreement finally expired in 1991 when Microsoft replaced BASICA/GW-BASIC with QBASIC. The main core BASIC resided in ROM and "linked" up with the RAM-resident BASIC.COM/BASICA.COM included with PC-DOS (they provided disk support and other extended features not present in ROM BASIC). Because BASIC was over 50k in size, this served a useful function during the first three years of the PC when machines only had 64k-128k of memory, but became less important by 1985. For comparison, clone makers such as Compaq were forced to include a version of BASIC that resided entirely in RAM.
Floppy diskettes.
Most or all 5150 PCs had one or two 5.25-inch floppy disk drives. These were either single-sided double-density (SSDD) or double-sided double-density (DSDD) drives. The IBM PC never used single density floppy drives. The drives and disks were commonly referred to by capacity, such as "160KB floppy disk" or "360KB floppy drive". DSDD drives were backwards compatible; they could read and write SSDD floppies. The same type of physical diskette media could be used for both drives, but a disk formatted for double-sided use could not be read on a single-sided drive.
The disks were Modified Frequency Modulation (MFM) coded in 512-byte sectors, and were soft-sectored. They contained 40 tracks per side at the 48 track per inch (TPI) density, and initially were formatted to contain eight sectors per track. This meant that SSDD disks initially had a formatted capacity of 160 kB, operating system was later updated to allow formatting the disks with nine sectors per track. This yielded a formatted capacity of 180 kB with SSDD disks while DSDD disks had a capacity of 320 kB. However, the DOS /drives, and 360 kB with DSDD disks/drives. The "unformatted" capacity of the floppy disks was advertised as "250KB" for SSDD and "500KB" for DSDD ("KB" ambiguously referring to either 1000 or 1024 bytes; essentially the same for rounded-off values), however these "raw" 250/500 kB were not the same thing as the usable formatted capacity; under DOS, the maximum capacity for SSDD and DSDD disks was 180 kB and 360 kB, respectively. Regardless of type, the file system of all floppy disks (under DOS) was FAT12.
The earliest IBM PCs had only single-sided floppy drives until double-sided drives became available in the spring of 1982. After the upgraded 64k-256k motherboard PCs arrived in early 1983, single-sided drives and the cassette model were discontinued.
IBM's original floppy disk controller card also included an external 37-pin D-shell connector. This allowed users to connect additional external floppy drives by third party vendors, but IBM did not offer their own external floppies until 1986.
The industry-standard way of setting floppy drive numbers was via setting jumper switches on the drive unit, however IBM chose to instead use a method known as the "cable twist" which had a floppy data cable with a bend in the middle of it that served as a switch for the drive motor control. This eliminated the need for users to adjust jumpers while installing a floppy drive.
Fixed disks.
The 5150 could not itself power hard drives without retrofitting a stronger power supply, but IBM later offered the 5161 Expansion Unit, which not only provided more expansion slots, but also included a 10 MB (later 20 MB) hard drive powered by the 5161's own separate 130-watt power supply. The IBM 5161 Expansion Unit was released in early 1983.
During the first year of the IBM PC, it was commonplace for users to install third-party Winchester hard disks which generally connected to the floppy controller and required a patched version of PC-DOS which treated them as a giant floppy disk (there was no subdirectory support).
IBM began offering hard disks with the XT, however the original PC was never sold with them. Nonetheless, many users installed hard disks and upgraded power supplies in them.
After floppy disks became obsolete in the early 2000s, the letters A and B became unused. But for 25 years, virtually all DOS-based PC software assumed the program installation drive was C, so the primary HDD continues to be "the C drive" even today.
Other operating system families (e.g. Unix) are not bound to these designations.
OS support.
Which operating system IBM customers would choose was at first unclear. Although the company expected that most would use PC DOS IBM supported using CP/M-86—which became available six months after DOS—or UCSD p-System as operating systems. IBM promised that it would not favor one operating system over the others; the CP/M-86 support surprised Gates, who claimed that IBM was "blackmailed into it". IBM was correct, nonetheless, in its expectation; one survey found that 96.3% of PCs were ordered with the $40 DOS compared to 3.4% for the $240 CP/M-86.
The IBM PC's ROM BASIC and BIOS supported cassette tape storage. PC DOS itself did not support cassette tape storage. PC DOS version 1.00 supported only 160 kB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160 kB SSDD and 320 kB DSDD floppies. Support for the slightly larger nine sector per track 180 kB and 360 kB formats arrived 10 months later in March 1983.
BIOS.
The BIOS (Basic Input/Output System) provided the core ROM code for the PC. It contained a library of functions that software could call for basic tasks such as video output, keyboard input, and disk access in addition to interrupt handling, loading the operating system on boot-up, and testing memory and other system components. Thanks to the vectored interrupts of the x86 CPUs, clone makers could easily reverse-engineer the IBM PC BIOS without stealing any copyrighted code.
The original IBM PC BIOS was 8k in size and occupied four 2k ROM chips on the motherboard, with a fifth and sixth empty slot left for any extra ROMs the user wished to install. IBM offered three different BIOS revisions during the PC's lifespan. The initial BIOS was dated April 1981 and came on the earliest models with single-sided floppy drives and PC DOS 1.00. The second version was dated October 1981 and arrived on the "Revision B" models sold with double-sided drives and PC DOS 1.10. It corrected some bugs, but was otherwise unchanged. Finally, the third BIOS version was dated October 1982 and found on all IBM PCs with the newer 64k-256k motherboard. This revision was more-or-less identical to the XT's BIOS. It added support for detecting ROMs on expansion cards as well as the ability to use 640k of memory (the earlier BIOS revisions had a limit of 544k). Unlike the XT, the original PC remained functionally unchanged from 1983 until its discontinuation in early 1987 and did not get support for 101-key keyboards or 3.5" floppy drives, nor was it ever offered with half-height floppies.
Video output.
IBM initially offered two video adapters for the PC, the Color/Graphics Adapter and the Monochrome Display and Printer Adapter. CGA was intended to be a typical home computer display; it had NTSC output and could be connected to a composite monitor or a TV set with an RF modulator in addition to RGB for digital RGBI-type monitors, although IBM did not offer their own RGB monitor until 1983. Supported graphics modes were 40 or 80x25 color text with 8x8 character resolution, 320x200 bitmap graphics with two fixed 4-color palettes, or 640x200 monochrome graphics.
The MDA card and its companion 5151 monitor supported only 80x25 text with a 9x14 character resolution (total pixel resolution was 720x350). It was mainly intended for the business market and so also included a printer port.
During 1982, the first third-party video card for the PC appeared when Hercules Computer Technologies released a clone of the MDA that could use bitmap graphics. Although not supported by the BIOS, the Hercules Graphics Adapter became extremely popular for business use due to allowing sharp, high resolution graphics plus text and itself was widely cloned by other manufacturers.
In 1985, after the launch of the IBM AT, the new Enhanced Graphics Adapter became available which could support 320x200 or 640x200 in 16 colors in addition to high-resolution 640x350 16 color graphics.
IBM also offered a video board for the PC, XT, and AT known as the Professional Graphics Adapter during 1984-86, mainly intended for CAD design. It was extremely expensive, required a special monitor, and was rarely ordered by customers.
VGA graphics cards could also be installed in IBM PCs and XTs, although they were introduced after the computer's discontinuation.
Serial port addresses and interrupts.
The serial port is an 8250 or a derivative (such as the 16450 or 16550), mapped to eight consecutive IO addresses and one interrupt request line.
Only COM1: and COM2: addresses were defined by the original PC. Attempts to share IRQ3 and IRQ4 to use additional ports require special measures in hardware and software, since shared IRQs were not defined in the original PC design. The most typical devices plugged into the serial port were modems and mice. Plotters and serial printers were also among the more commonly-used serial peripherals, and there were numerous other more unusual uses such as operating cash registers, factory equipment, and connecting terminals.
Printer port.
IBM made a deal with Japan-based Epson to producer printers for the PC and all IBM-branded printers were manufactured by that company (Epson of course also sold printers with their own name). There was a considerable amount of controversy when IBM included a printer port on the PC that did not follow the industry-standard Centronics design, and it was rumored that this had been done to prevent customers from using non-Epson/IBM printers with their machines (plugging a Centronics printer into an IBM PC could damage the printer, the parallel port, or both). Although third-party cards were available with Centronics ports on them, PC clones quickly copied the IBM printer port and by the late 80s, it had largely displaced the Centronics standard.
Reception.
"BYTE" wrote in October 1981 that the IBM PC's "hardware is impressive, but even more striking are two decisions made by IBM: to use outside suppliers already established in the microcomputer industry, and to provide information and assistance to independent, small-scale software writers and manufacturers of peripheral devices". It praised the "smart" hardware design and stated that its price was not much higher than the 8-bit machines from Apple and others. The reviewer admitted that the computer "came as a shock. I expected that the giant would stumble by overestimating or underestimating the capabilities the public wants and stubbornly insisting on incompatibility with the rest of the microcomputer world. But IBM didn't stumble at all; instead, the giant jumped leagues in front of the competition ... the only disappointment about the IBM Personal Computer is its dull name".
In a more detailed review in January 1982, "BYTE" called the IBM PC "a synthesis of the best the microcomputer industry has offered to date ... as well designed on the inside as it is on the outside". The magazine praised the keyboard as "bar none, the best ... on any microcomputer", describing the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard". The review also complimented IBM's manuals, which it predicted "will set the standard for all microcomputer documentation in the future. Not only are they well packaged, well organized, and easy to understand, but they are also "complete"". Observing that detailed technical information was available "much earlier ... than it has been for other machines", the magazine predicted that "given a reasonable period of time, plenty of hardware and software will probably be developed for" the computer. The review stated that although the IBM PC cost more than comparably configured Apple II and TRS-80 computers, and the insufficient number of slots for all desirable expansion cards was its most serious weakness, "you get a "lot" more for your money" and concluded, "In two years or so, I think [it] will be one of the most popular and best-supported ... IBM should be proud of the people who designed it".
Longevity.
Many IBM PCs have remained in service long after their technology became largely obsolete. In June 2006, IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, used to process data as it is returned from the ascending radiosonde, attached to a weather balloon, although they have been slowly phased out. Factors that have contributed to the 5150 PC's longevity are its flexible modular design, its open technical standard (making information needed to adapt, modify, and repair it readily available), use of few special nonstandard parts, and rugged high-standard IBM manufacturing, which provided for exceptional long-term reliability and durability. Many newer PCs, by contrast, use proprietary parts and PCs themselves become obsolete quickly. According to Moore's Law the power of a microprocessor doubles every 18 months and it becomes easier to simply dispose of the PC than to upgrade or repair it.
The slot specifications are still used in current PCs as well as the limitation of having four active partitions on a hard disk. Many systems still come with PS/2 style keyboard and mouse connectors, and power supply connectors are based on later standards.
Collectability.
The IBM model 5150 Personal Computer has become a collectable among vintage computer collectors, due to the system being the first true “PC” as we know them today. Today these systems can fetch anywhere from $100 to $4500, depending on cosmetic and operational condition. The IBM model 5150 has proven to be reliable; despite their age of 30 years or more, some still function as they did when new.
References.
</dl>

</doc>
<doc id="15033" url="http://en.wikipedia.org/wiki?curid=15033" title="Counties of Ireland">
Counties of Ireland

The counties of Ireland (Irish: "contaetha na hÉireann"; Ulster-Scots: "coonties o Airlann") are sub-national divisions that have been, and in some cases continue to be, used to geographically demarcate areas of local government. These land divisions were formed following the Norman invasion of Ireland in imitation of the counties then in use as units of local government in the Kingdom of England. The older term "shire" was historically equivalent to "county". The principal function of the county was to impose royal control in the areas of taxation, security and the administration of justice at the local level. Cambro-Norman control was initially limited to the southeastern parts of Ireland; a further four centuries elapsed before the entire island was shired. At the same time, the now obsolete concept of county corporate elevated a small number of towns and cities to a status which was deemed to be no less important than the existing counties in which they lay. This double control mechanism of 32 counties plus 10 counties corporate remained unchanged for a little over two centuries until the early 19th century, since when counties have been adapted and in some cases divided by legislation to meet new administrative and political requirements.
The powers exercised by the Cambro-Norman barons and the Old English nobility waned over time. New offices of political control came to be established at a county level. In the Republic of Ireland, some counties have been split resulting in the creation of new counties. Along with certain defined cities, counties still form the basis for the demarcation of areas of local government in the Republic of Ireland. Currently, there are 26 county level, 3 city level and 2 city and county entities – the modern equivalent of counties corporate – that are used to demarcate areas of local government in the Republic.
In Northern Ireland, counties are no longer used for local government; districts are instead used. Upon the partition of Ireland in 1921, the county became one of the basic land divisions employed, along with county boroughs.
Terminology.
The word "county" has come to be used in different senses for different purposes. In common usage, many people have in mind the 32 counties that existed prior to 1838 – the so-called traditional counties. However, in official usage in the Republic of Ireland, the term often refers to the 28 modern counties. The term is also conflated with the 31 areas currently used to demarcate areas of local government in the Republic of Ireland at the level of LAU 1.
In Ireland, usage of the word "county" nearly always comes before rather than after the county name; thus ""County" Clare" in Ireland as opposed to "Clare "County"" in Michigan, United States. The former "King's County" and "Queen's County" were exceptions; these are now County Offaly and County Laois, respectively. The abbreviation Co. is used, as in "Co. Clare". A further exception occurs in the case of those counties created after 1994 which often drop the word "county" entirely, or use it after the name; thus for example internet search engines show many more uses (on Irish sites) of "Fingal" than of either "County Fingal" or "Fingal County". There appears to be no official guidance in the matter, as even the local council uses all three forms. In informal use, the word "county" is often dropped except where necessary to distinguish between county and town or city; thus "Offaly" rather than "County Offaly", but "County Antrim" to distinguish it from Antrim town. The synonym "shire" is not used for Irish counties, although the Marquessate of Downshire was named in 1789 after County Down.
Parts of some towns and cities were exempt from the jurisdiction of the counties that surrounded them. These towns and cities had the status of a County corporate, many granted by Royal Charter, which had all the judicial, administrative and revenue raising powers of the regular counties.
Interactive map.
The map at the beginning of this article shows the 32 traditional counties that were in place in Ireland by early 17th century. Their area does not always correspond with the area of the counties currently used for local government purposes. The map links to county articles by using an , which allows it to be used for navigating the encyclopaedia in addition to showing geographic locations. "Click on a county" to go to the corresponding article.
History.
Pre-Norman divisions of Ireland.
The political geography of Ireland can be traced with some accuracy from the 6th century. At that time Ireland was divided into a patchwork of petty kingdoms with a fluid political hierarchy which, in general, had three traditional grades of king. The lowest level of political control existed at the level of the "tuath" (pl. "tuatha"). A "tuath" was an autonomous group of people of independent political jurisdiction under a Rí túaithe, that is, a local petty king. About 150 such units of government existed. Each Rí Tuaithe was in turn subject to a regional or "over-king" Irish: "ruiri". There may have been as many as 20 genuine ruiri in Ireland at any time. A "king of over-kings" Irish: "rí ruirech" was often a provincial (Irish: "rí cóicid") or semi-provincial king to whom several ruiri were subordinate. No more than six genuine rí ruirech were ever contemporary. Usually, only five such "king of over-kings" existed contemporaneously and so are described in the Irish annals as "fifths" ("cúigí" in Irish). The areas under the control of these kings were : Ulster (Irish: "Ulaidh"), Leinster (Irish: "Laighin"), Connacht (Irish: "Connachta"), Munster (Irish: "An Mhumhan") and Mide (Irish: "An Mhídhe"). Later record-makers dubbed them "provinces", in imitation of Roman provinces. In the Norman period, the historic fifths of Leinster and Meath gradually merged, mainly due to the impact of the Pale, which straddled both, thereby forming the present-day province of Leinster. The use of provinces as divisions of political power was supplanted by the system of counties after the Norman invasion. In modern times clusters of counties have been attributed to certain provinces but these clusters have no legal status. They are today seen mainly in a sporting context, as Ireland's four professional rugby teams play under the names of the provinces, and the Gaelic Athletic Association has separate Provincial councils and Provincial championships.
Norman areas of control.
With the arrival of Cambro-Norman knights in 1169, the Norman invasion of Ireland commenced. This was followed in 1172 by the invasion of King Henry II of England, commencing English royal involvement. The English governed Ireland using a structure similar to that used in England, by dividing the country into shires or counties in the late 12th and early 13th centuries.
In general, counties were made by amalgamating various smaller Irish territories which suited the colonial administration at the time and had little basis in older tribal boundaries. In many cases, this involved dividing an Irish territory in two. For example, the kingdom of Uí Maine was split to form south County Roscommon and most of east County Galway. Many of the counties of Ulster roughly correspond to the territories controlled by the principal clan in that particular area such as the O Donnells of Tír Chonaill whose political power was concentrated in what would become the County of Donegal.
The counties evolved over time, with the earliest defined being set out by King John, including a then much larger County Dublin. Sir John Davies wrote ‘True it is that King John made twelve shires in Leinster and Munster—namely, Dublin, Kildare, Meath, Urial or Louth, Catherlogh, Kilkenny, Wexford, Waterford, Cork, Limerick, Kerry, and Tipperary. Yet these counties did stretch no further than the lands of the English colonies did extend.’ In the Anglo-Saxon controlled territories, a royal official known as a "shire reeve" or sheriff governed the shire. Large parts in the south east of the island had been shired by the Cambro-Norman overlords by the end of the 13th century. Following the resurgence in the 14th and 15th centuries of the Gaelic nobility, the project was suspended, not to be resumed until the mid 16th century by the House of Tudor. There were two phases in this period under the newly created Kingdom of Ireland: the plantation of the midlands under Mary I and the plantation of Munster under her half-sister, Elizabeth I. By the reign of King James I & VI, the process was completed with the plantation of the last remaining princely domains in the province of Ulster. With the late addition of County Wicklow, the number of counties (Irish: "contae" or "condae" ]). stood at thirty two. This number remained fixed for a little over 200 years until the division of some counties into smaller entities for judicial purposes (1836), initially, followed by complete division in the 20th and 21st centuries.
By 1200 there were also shires of Connacht, Cork, Kerry, Limerick, Louth and Tipperary. Waterford, Kilkenny and Wexford apparently date from this time, too, as remnants of Strongbow's palatine county in Leinster. The process continued over time, and in 1206, for example, a special commission was used to determine borders in the Munster area.
The County of Roscommon was separated from Connacht before 1292, and the first session of the Parliament of Ireland in 1297 created the new shires of County Kildare, Meath and Ulster. Carlow, then larger than today, and extending to and including coastal Arklow, probably dates from around 1306.
In the 14th century, "Counties of the Cross" emerged, consisting of scattered areas of church land within the existing divisions. Unlike the secular counties (or liberties), the counties of the cross were administered by royally-appointed sheriffs.
Tudor areas of control.
The Tudor administrations finalised the division of Ireland into counties. Westmeath was separated from Meath (often "East Meath") in 1543. Queen Mary I of England introduced a new policy to pacify the midland areas – plantation. In 1556 King's County and Queen's County were created and the Kingdom of Connacht was broken up into the counties of Galway, Mayo and Sligo, while Leitrim was separated from Roscommon in 1565. At the same time County Clare was created and moved from Munster to Connacht, being returned to Munster in 1602.
In 1583 County Longford was formed from part of Westmeath and transferred to the Province of Connacht.
The Province of Ulster was the last to be shired. The counties of Antrim and Down originated early in the 16th century. These were joined in 1584/5 by the counties of Armagh, Coleraine, Donegal, Fermanagh, Monaghan and Tyrone. County Cavan was also formed in 1584 and transferred from Connacht to Ulster. County Londonderry was incorporated in 1613 from the merger of County Coleraine with the barony of Loughinsholin (in County Tyrone), the North West Liberties of Londonderry (in County Donegal), and the North East Liberties of Coleraine (in County Antrim).
The last county to be formed was County Wicklow in 1606–07, taking in the southern part of Dublin (with the exception of three "islands," exclaves of (mainly) church property), and the northern part of "Catherlough" or Carlow, including Arklow.
Areas that were shired by 1607 and continued in existence down to the local government reforms of 1836, 1898 and 2001 are sometimes referred to as "traditional" or "historic" counties. These areas were separate from the county corporates that existed in some of the larger towns and cities although linked to the county at large for other purposes. From 1898 to 2001, areas with county councils were known as administrative counties while the counties corporate were designated as county boroughs. In other cases, the "traditional" county was divided to form two administrative counties. From 2001, certain administrative counties, which were originally "traditional" counties, underwent further splitting.
Former counties.
Former counties include: County Coleraine, which formed the basis of County Londonderry, the counties of Nether and Upper Tyrone, and Desmond which was, in 1606, split between counties Cork and Kerry. Other names seen on old maps include Caterlaugh or Caterlagh, archaic designations of County Carlow, in the days before much of the north of that county was taken into Wicklow in the early 17th century.
In 1333, the Earldom of Ulster consisted of seven counties: Antrim, Blathewyc, Cragferus, Coulrath, del Art, Dun (also known as Ladcathel), and Twescard.
Carrickfergus would lose its county town status in 1777. Twescard is preserved as the name of a school house at Campbell College, Belfast.
Sub-divisions of counties.
To correspond with the subdivisions of the English shires into honours or baronies, Irish counties were granted out to the Anglo-Norman noblemen in cantreds, later known as baronies, which in turn were subdivided, as in England, into parishes. Parishes were composed of townlands. However, in many cases, these divisions correspond to earlier, pre-Norman, divisions. While there are 331 baronies in Ireland, and more than a thousand civil parishes, there are around sixty thousand townlands that range in size from one to several thousand hectares. Townlands were often traditionally divided into smaller units called "quarters", but these subdivisions are not legally defined.
Counties corporate.
The following towns/cities had charters specifically granting them the status of a county corporate:
The only entirely new counties created in 1898 were the county boroughs of Londonderry and Belfast. Carrickfergus, Drogheda and Kilkenny were abolished; Galway was also abolished, but recreated in 1986.
Exceptions to the county system of control.
Regional presidencies of Connacht and Munster remained in existence until 1672, with special powers over their subsidiary counties. Tipperary remained a county palatine until the passing of the County Palatine of Tipperary Act 1715, with different officials and procedures from other counties. At the same time, Dublin, until the 19th century, had ecclesiastical liberties with rules outside those applying to the rest of Dublin city and county. Exclaves of the county of Dublin existed in counties Kildare and Wicklow. At least eight other enclaves of one county inside another, or between two others, existed. The various enclaves and exclaves were merged into neighbouring and surrounding counties, primarily in the mid-19th century under a series of Orders in Council.
Evolution of functions.
The Church of Ireland exercised functions at the level of civil parish that would later be exercised by county authorities. Vestigial feudal power structures of major old estates remained well into the 18th century. Urban corporations operated individual royal charters. Management of counties came to be exercised by grand juries. Members of grand juries were the local payers of rates who historically held judicial functions, taking maintenance roles in regard to roads and bridges, and the collection of "county cess" taxes. They were usually composed of wealthy "country gentlemen" (i.e. landowners, farmers and merchants): "A country gentleman as a member of a Grand Jury...levied the local taxes, appointed the nephews of his old friends to collect them, and spent them when they were gathered in. He controlled the boards of guardians and appointed the dispensary doctors, regulated the diet of paupers, inflicted fines and administered the law at petty sessions." The counties were initially used for judicial purposes, but began to take on some governmental functions in the 17th century, notably with grand juries.
19th and 20th centuries.
In 1836, the use of counties as local government units was further developed, with grand-jury powers extended under the Grand Jury (Ireland) Act 1836. The traditional county of Tipperary was split into two judicial counties (or ridings) following the establishment of assize courts in 1838. Also in that year, local poor law boards, with a mix of magistrates and elected "guardians" took over the health and social welfare functions of the grand juries.
Sixty years later, a more radical reorganisation of local government took place with the passage of the Local Government (Ireland) Act 1898. This Act established a county council for each of the thirty-three Irish administrative counties. Elected county councils took over the powers of the grand juries.The boundaries of the traditional counties changed on a number of occasions. The 1898 Act changed the boundaries of Counties Galway, Clare, Mayo, Roscommon, Sligo, Waterford, Kilkenny, Meath and Louth, and others. County Tipperary was divided into two regions: North Riding and South Riding. Areas of the cities of Belfast, Cork, Dublin, Limerick, Derry and Waterford were carved from their surrounding counties to become county boroughs in their own right and given powers equivalent to those of administrative counties.
Under the Government of Ireland Act 1920, the island was partitioned between Southern Ireland and Northern Ireland. For the purposes of the Act, "... Northern Ireland shall consist of the parliamentary counties of Antrim, Armagh, Down, Fermanagh, Londonderry and Tyrone, and the parliamentary boroughs of Belfast and Londonderry, and Southern Ireland shall consist of so much of Ireland as is not comprised within the said parliamentary counties and boroughs."
The county and county borough borders were thus used to determine the line of partition. Southern Ireland shortly afterwards became the Irish Free State. This partition was entrenched in the Anglo-Irish Treaty, which was ratified in 1922, by which the Irish Free State left the United Kingdom with Northern Ireland making the decision to not separate two days later.
Under the Local Government Provisional Order Confirmation Act 1976, part of the urban area of Drogheda, which lay in County Meath, was transferred to County Louth on 1 January 1977. This resulted in the land area of County Louth increasing slightly at the expense of County Meath. The possibility of a similar action with regard to Waterford City has been raised in recent years, though opposition from Kilkenny has been strong.
Current usage.
In the Republic of Ireland.
In the Republic of Ireland the traditional counties are, in general, the basis for local government, planning and community development purposes, are governed by county councils and are still generally respected for other purposes. Administrative borders have been altered to allocate various towns (e.g. Bray) exclusively into one county having been originally split between two counties.
Six of the 26 original counties have more than one local council; there are now 26 county councils, three city councils and two city and county councils – a total of 31 local government entities. County Tipperary was split into North and South Ridings in 1838. These Ridings were established as separate administrative counties under the Local Government (Ireland) Act, 1898. By the terms of the Local Government Act 2001, the counties of North Tipperary and South Tipperary were created. North Tipperary and South Tipperary were abolished, and County Tipperary re-established, by the Local Government Reform Act 2014. County Dublin was abolished as an administrative county in 1994, while also remaining a point of reference for purposes other than local government. Its territory was divided into three administrative counties: Dún Laoghaire–Rathdown, Fingal, and South Dublin. Despite its legal status, Dublin continues to enjoy popular support as points of reference for culture and sport.
Additionally, the cities of Dublin, Cork and Galway are governed by city councils (previously known as corporations or county boroughs or county corporates). The city councils of Limerick and Waterford were merged with their respective county councils in 2014, to form new city and county councils. Anomalously, the city of Kilkenny does not have a "city council" as it was a borough but not a county borough. It is now administered by its eponymous county council but is, exceptionally, permitted to retain the style of "city" for ornament only.
These 31 "county-level" entities correspond to the first level of local administrative unit for EU and Eurostat purposes. The second level of local administrative unit (LAU) is the District electoral division. The 2001 Act also provided for the creation of Town councils. Of the administrative structures established under the 1898 Local Government Act, the only type to have been completely abolished was the Rural District, which was rendered void in the early years of the Irish Free State amidst widespread allegations of corruption. At a level above that of LAU is the Region which clusters counties together for NUTS purposes. The Regions are administered by Regional Authorities which were established by the Local Government Act 1991 and came into existence in 1994.
Education.
In 2013 Education and Training Boards (ETBs) were formed throughout the Republic of Ireland, replacing the system of Vocational Education Committees (VECs) created in 1930. Originally, VECs were formed for each administrative county and county borough, and also in a number of larger towns. In 1997 the majority of town VECs were absorbed by the surrounding county. The 33 VEC areas were reduced to 16 ETB areas, with each consisting of one or more local government county or city.
The Institute of technology system was organised on the committee areas or "functional areas", these still remain legal but are not as important as originally envisioned as the institutes are now more national in character and are only really applied today when selecting governing councils, similarly Dublin Institute of Technology was originally a group of several colleges of the City of Dublin committee.
Elections.
Where possible, parliamentary constituencies in the Republic of Ireland follow county boundaries. Under the Electoral Act 1997 a Constituency Commission is established following the publication of census figures every five years. The Commission is charged with defining constituency boundaries, and the 1997 Act provides that "the breaching of county boundaries shall be avoided as far as practicable". This provision does not apply to the boundaries between cities and counties, or between the three counties in the Dublin area.
This system usually results in more populated counties having several constituencies: Dublin, including Dublin city, is subdivided into twelve constituencies, Cork into five. On the other hand, smaller counties such as Carlow and Kilkenny or Laois and Offaly may be paired to form constituencies. An extreme case is the splitting of Ireland's least populated county of Leitrim between the constituencies of Sligo-North Leitrim and Roscommon-South Leitrim.
Each county or city is divided into Local electoral areas for the election of councillors. The boundaries of the areas and the number of councillors assigned are fixed from time to time by order of the Minister for the Environment, Community and Local Government, following a report by the Local Government Commission, and based on population changes recorded in the census.
In Northern Ireland.
In Northern Ireland, a major reorganisation of local government in 1973 replaced the six traditional counties and two county boroughs (Belfast and Derry) with 26 single-tier districts for local government purposes. The counties remain in use for some purposes, including the three-letter coding of vehicle number plates, the Royal Mail Postcode Address File (which records counties in all addresses although they are no longer required for postcoded mail) and Lord Lieutenancies (for which the former county boroughs are also used). There are no longer official 'county towns'. However the counties are still very widely acknowledged, for example as administrative divisions for sporting and cultural organisations.
Other uses.
The administrative division of the island along the lines of the traditional 32 counties was also adopted by non-governmental and cultural organisations. In particular the Gaelic Athletic Association continues to organise its activities on the basis of GAA counties that, throughout the island, correspond almost exactly to the 32 traditional counties in use at the time of the foundation of that organisation in 1884. The GAA also uses the term "county" for some of its organisational units in Britain and further afield.
List of counties.
The first 32 divisions listed below are the "traditional" counties, 21 of which still have administrative functions as local government divisions in the Republic of Ireland (in some cases with slightly redrawn boundaries). The newer administrative counties established in the Republic are listed at the foot of the table. The Irish-language names of counties in the Republic of Ireland are prescribed by ministerial order, which in the case of three newer counties, omits the word "contae" (county). Irish names form the basis for all English-language county names except Dublin, Waterford, Wexford, and Wicklow, which are of Norse origin. The Ulster-Scot names are principally derived from the North/South Ministerial Council.
In the "Region" column of the table below, except for the six Northern Ireland counties the reference is to NUTS 3 statistical regions of the Republic of Ireland. "County town" is the current or former administrative capital of the county.
Cities which, in the Republic, are currently administered outside the county system, but with the same legal status as administrative counties, are not shown separately: these are Cork, Dublin and, Galway. Also not shown are the former county boroughs of Londonderry and Belfast which in Northern Ireland had the same legal status as the six counties until the reorganisation of local government in 1973.
External links.
Flags
Baronies, Civil Parishes and Townlands

</doc>
<doc id="15034" url="http://en.wikipedia.org/wiki?curid=15034" title="Information Sciences Institute">
Information Sciences Institute

The Information Sciences Institute (ISI) is a research and development unit of the USC Viterbi School of Engineering that focuses on information processing, computing and communications technology. Founded in 1972, ISI helped develop the Internet, including the Domain Name System and refinement of TCP/IP communications protocols. The institute receives about $60 million annually for basic and applied research from more than 20 U.S. federal government agencies, including the Department of Defense Advanced Research Projects Agency (DARPA), National Science Foundation (NSF), National Institutes of Health (NIH) and Department of Homeland Security.
ISI employs approximately 350 research scientists, research programmers, graduate students and administrative staff in Marina del Rey, California (headquarters) and Arlington, Virginia. About half of the research staff hold Ph.D.s, and about 50 are research faculty who teach at USC and/or advise graduate students at ISI. The institute's executive director is , who arrived in mid-2013 from Raytheon BBN Technologies, where he was an executive vice president and principal scientist. Natarajan succeeded interim executive director and USC Viterbi School vice dean John O'Brien, who had followed Herbert Schorr, ISI's executive director from 1988 to 2012.
History.
ISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation. When Uncapher's RAND group funding was cut dramatically, he approached the University of California at Los Angeles about creating an off-campus technology institute. Uncapher was told a decision would take 15 months. USC department of electrical engineering chairman George A. Bekey then helped arrange a meeting with executive vice president Zohrab Karielian, who strongly supported the concept. ISI was born five days later with Uncapher and two other employees. Its first proposal was funded by Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.
Expertise.
ISI researchers work in four broad areas:
Current projects include:
Achievements.
ISI made significant contributions to Internet development, including major contributions to development of the TCP/IP communications standard on which the Net continues to operate. The Domain Name System (DNS) was conceived at ISI, along with the now-ubiquitous "dot-com," "dot-net," "dot-org" and other identifiers. ISI managed the DNS from its inception until 1998. Institute researchers, notably Internet pioneer Jonathan Postel, also edited the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and guided its evolution, from 1977 through 2009. The Internet Assigned Numbers Authority (IANA), which coordinated its unique identifiers, was managed from ISI until 1998. IAANA's authority then was transferred to a new oversight body, the Internet Corporation for Assigned Names and Numbers (ICANN), which was created for that purpose.
ISI developed the world's first e-commerce site, MOSIS, which produces specialty and low-volume chips for corporations, universities and other research entitites worldwide. The Institute helped develop, and was the first entity to implement, Voice Over Internet Protocol (VOIP). Some of the first Net security applications, and one of the world's first portable computers, also originated at ISI.
In 2012, ISI researcher Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained undeciphered for 250 years. The first center to utilize the D-Wave adiabatic quantum system, the USC Lockheed Martin Quantum Computation Center, also opened at ISI. In 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the "Jeopardy!" TV show.
Other achievements include creating or co-creating:
Customers, startups, and spinoffs.
ISI's US government customers include DARPA, the US Army and Air Force Office of Scientific Research, the National Institutes of Health, the National Science Foundation and the US Geological Survey; the Departments of Agriculture, Defense, Energy, Homeland Security and Justice; and the US intelligence community. The Institute has spawned a few startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.

</doc>
<doc id="15036" url="http://en.wikipedia.org/wiki?curid=15036" title="Information security">
Information security

Information security, sometimes shortened to InfoSec, is the practice of defending information from unauthorized access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction. It is a general term that can be used regardless of the form the data may take (e.g. electronic, physical).
Overview.
Threats
Computer system threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks, and trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the IT field. Intellectual property is the ownership of property usually consisting of some form of protection. Theft of software is probably the most common in IT businesses today. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile. Cell phones are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization′s website in an attempt to cause loss of confidence to its customers. Information extortion consists of theft of a company′s property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is user carefulness.
Governments, military, corporations, financial institutions, hospitals and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Most of this information is now collected, processed and stored on electronic computers and transmitted across networks to other computers.
Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. Protecting confidential information is a business requirement and in many cases also an ethical and legal requirement. Hence a key concern for organizations today is to derive the optimal information security investment. The renowned Gordon-Loeb Model actually provides a powerful mathematical economic approach for addressing this critical concern.
For the individual, information security has a significant effect on privacy, which is viewed very differently in different cultures.
The field of information security has grown and evolved significantly in recent years. There are many ways of gaining entry into the field as a career. It offers many areas for specialization including securing network(s) and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning and digital forensics.
History.
Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands, but for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g. the UK Secret Office and Deciphering Branch in 1653).
In the mid-19th century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. The British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. In the United Kingdom this led to the creation of the Government Code and Cypher School in 1919. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than men) and where they should be stored as increasingly complex safes and storage facilities were developed. Procedures evolved to ensure documents were destroyed properly and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g. U-570).
The end of the 20th century and early years of the 21st century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. These computers quickly became interconnected through the Internet.
The rapid growth and widespread use of electronic data processing and electronic business conducted through the Internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations – all sharing the common goals of ensuring the security and reliability of information systems.
Definitions.
The definitions of InfoSec suggested in different sources are summarised below (adopted from).
1. "Preservation of confidentiality, integrity and availability of information. Note: In addition, other properties, such as authenticity, accountability, non-repudiation and reliability can also be involved." (ISO/IEC 27000:2009)
2. "The protection of information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide confidentiality, integrity, and availability." (CNSS, 2010)
3. "Ensures that only authorized users (confidentiality) have access to accurate and complete information (integrity) when required (availability)." (ISACA, 2008)
4. "Information Security is the process of protecting the intellectual property of an organisation." (Pipkin, 2000)
5. "...information security is a risk management discipline, whose job is to manage the cost of information risk to the business." (McDermott and Geer, 2001)
6. "A well-informed sense of assurance that information risks and controls are in balance." (Anderson, J., 2003)
7. "Information security is the protection of information and minimises the risk of exposing information to unauthorised parties." (Venter and Eloff, 2003)
8. "Information Security is a multidisciplinary area of study and professional activity which is concerned with the development and implementation of security mechanisms of all available types (technical, organisational, human-oriented and legal) in order to keep information in all its locations (within and outside the organisation's perimeter) and, consequently, information systems, where information is created, processed, stored, transmitted and destroyed, free from threats.
Threats to information and information systems may be categorised and a corresponding security goal may be defined for each category of threats. A set of security goals, identified as a result of a threat analysis, should be revised periodically to ensure its adequacy and conformance with the evolving environment. The currently relevant set of security goals may include: "confidentiality, integrity, availability, privacy, authenticity & trustworthiness, non-repudiation, accountability and auditability."" (Cherdantseva and Hilton, 2013)
Profession.
Information security is a stable and growing profession. Information security professionals are very stable in their employment; more than 80 percent had no change in employer or employment in the past year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.
Basic principles.
Key concepts.
The CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad — confidentiality, integrity and availability — are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) There is continuous debate about extending this classic trio. Other principles such as Accountability have sometimes been proposed for addition – it has been pointed out that issues such as Non-Repudiation do not fit well within the three core concepts.
In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks proposed the nine generally accepted principles: Awareness, Responsibility, Response, Ethics, Democracy, Risk Assessment, Security Design and Implementation, Security Management, and Reassessment. Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security proposed 33 principles. From each of these derived guidelines and practices.
In 2002, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian hexad are a subject of debate amongst security professionals.
In 2013, based on a thorough analysis of Information Assurance and Security (IAS) literature, the IAS-octave was proposed as an extension of the CIA-triad. The IAS-octave includes Confidentiality, Integrity, Availability, Accountability, Auditability, Authenticity/Trustworthiness, Non-repudiation and Privacy. The completeness and accuracy of the IAS-octave was evaluated via a series of interviews with IAS academics and experts. The IAS-octave is one of the dimensions of a Reference Model of Information Assurance and Security (RMIAS), which summarises the IAS knowledge in one all-encompassing model.
Integrity.
In information security, data integrity means maintaining and assuring the accuracy and consistency of data over its entire life-cycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity in addition to data confidentiality.
Availability.
For any information system to serve its purpose, the information must be available when it is needed. This means that the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system essentially forcing it to shut down.
Authenticity.
In computing and information security, it is necessary to ensure that the data, transactions, communications or documents (electronic or physical) are genuine. It is also important for authenticity to validate that both parties involved are who they claim to be. Some information security systems incorporate authentication features such as "digital signatures", which give evidence that the message data is genuine and was sent by someone possessing the proper signing key.
Non-repudiation.
In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction nor can the other party deny having sent a transaction.
It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message and nobody else could have altered it in transit. The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender himself, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity and thus prevents repudiation.
Electronic commerce uses technology such as digital signatures and public key encryption to establish authenticity and non-repudiation.
Risk management.
The "Certified Information Systems Auditor (CISA) Review Manual 2006" provides the following definition of risk management: "Risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization."
There are two things in this definition that may need some clarification. First, the "process" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.
Risk analysis and risk evaluation processes have their limitations since, when security incidents occur, they emerge in a context, and their rarity and even their uniqueness give rise to unpredictable threats. The analysis of these phenomena which are characterized by breakdowns, surprises and side-effects, requires a theoretical approach which is able to examine and interpret subjectively the detail of each incident.
Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.
The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). It should be pointed out that it is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called "residual risk".
A risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.
The research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:
In broad terms, the risk management process consists of:
For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.
Controls.
Selecting proper controls and implementing those will initially help an organization to bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. has defined 133 controls in different areas, but this is not exhaustive. Organizations can implement additional controls according to requirement of the organization. has cut down the number of controls to 113.
Administrative.
Administrative controls (also called procedural controls) consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.
Administrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls. Administrative controls are of paramount importance.
Logical.
Logical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. For example: passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are
logical controls.
An important logical control that is frequently overlooked is the principle of least privilege. The principle of least privilege requires that an individual, program or system process is not granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, or they are promoted to a new position, or they transfer to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges which may no longer be necessary or appropriate.
Physical.
Physical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities. For example: doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.
An important physical control that is frequently overlooked is the separation of duties. Separation of duties ensures that an individual can not complete a critical task by himself. For example: an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator – these roles and responsibilities must be separated from one another.
Defense in depth.
Information security must protect information throughout the life span of the information, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called defense in depth. The strength of any system is no greater than its weakest link. Using a defense in depth strategy, should one defensive measure fail there are other defensive measures in place that continue to provide protection.
Recall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense-in-depth strategy. With this approach, defense-in-depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense-in- depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid and each provides valuable insight into the implementation of a good defense-in-depth strategy.
Security classification for information.
An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification.
The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.
Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information.
The Business Model for Information Security enables security professionals to examine security from systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.
The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:
All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.
Access control.
Access to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected – the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.
Access control is generally considered in three steps: Identification, Authentication, and Authorization.
Identification.
Identification is an assertion of who someone is or what something is. If a person makes the statement "Hello, my name is John Doe" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming "I am the person the username belongs to".
Authentication.
Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe—a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it 
has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly by entering the correct password, the user is providing evidence that they are the person the username belongs to.
There are three different types of information that can be used for authentication:
Strong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose but in our modern world they are no longer adequate. Usernames and passwords are slowly being replaced with more sophisticated authentication mechanisms.
Authorization.
After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms—some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control or it may be derived from a combination of the three approaches.
The non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the Mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.
Examples of common access control mechanisms in use today include role-based access control available in many advanced database management systems—simple file permissions provided in the UNIX and Windows operating systems, Group Policy Objects provided in Windows network systems, Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.
To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. All failed and successful authentication attempts must be logged, and all access to information must leave some type of audit trail.
Also, need-to-know principle needs to be in affect when talking about access control. Need-to-know principle gives access rights to a person to perform their job functions. This principle is used in the government, when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee least amount privileges to prevent employees access and doing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability (C‑I‑A) triad. Need-to-know directly impacts the confidential area of the triad.
Cryptography.
Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user, who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.
Cryptography provides information security with other useful applications as well including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older less secure applications such as telnet and ftp are slowly being replaced with more secure applications such as ssh that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and Email.
Cryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.
Process.
The terms reasonable and prudent person, due care and due diligence have been used in the fields of Finance, Securities, and Law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S.A. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.
In the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the "reasonable and prudent person" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal ethical manner. A prudent person is also diligent (mindful, attentive, and ongoing) in their due care of the business.
In the field of Information Security, Harris
offers the following definitions of due care and due diligence:
"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees." And, [Due diligence are the] "continual activities that make sure the protection mechanisms are continually maintained and operational."
Attention should be made to two important points in these definitions. First, in due care, steps are taken to show - this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities - this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.
Security governance.
The Software Engineering Institute at Carnegie Mellon University, in a publication titled "Governing for Enterprise Security (GES)", defines characteristics of effective security governance. These include:
Incident response plans.
"1 to 3 paragraphs (non technical) that discuss:"
Change management.
Change management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the
processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.
Any change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of Management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.
Not every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.
Change management is usually overseen by a Change Review Board composed of representatives from key business areas, security, networking, systems administrators, Database administration, applications development, desktop support and the help desk. The tasks of the Change Review Board can be facilitated with the use of automated work flow application. The responsibility of the Change Review Board is to ensure the organizations documented change management procedures are followed. The change management process is as follows:
Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.
ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.
Business continuity.
Business continuity is the mechanism by which an organization continues to operate its critical business units, during planned or unplanned disruptions that affect normal business operations, by invoking planned and managed procedures.
Not only is business continuity simply about the business, but it also an IT system and process. Today disasters or disruptions to business are a reality. Whether the disaster is natural or man-made, it affects normal life and so business. Therefore, planning is important.
The planning is merely getting better prepared to face it, knowing fully well that the best plans may fail. Planning helps to reduce cost of recovery, operational overheads and most importantly sail through some smaller ones effortlessly.
For businesses to create effective plans they need to focus upon the following key questions. Most of these are common knowledge, and anyone can do a BCP.
Disaster recovery planning.
While a business continuity plan (BCP) takes a broad approach to dealing with organizational-wide effects of a disaster, a disaster recovery plan (DRP), which is a subset of the business continuity plan, is instead focused on taking the necessary steps to resume normal business operations as quickly as possible. A disaster recovery plan is executed immediately after the disaster occurs and details what steps are to be taken in order to recover critical information technology infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.
Laws and regulations.
"Below is a partial listing of European, United Kingdom, Canadian and US governmental laws and regulations that have, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security."
Information Security Culture.
Employee’s behavior has a big impact to information security in organizations. Cultural concept can help different segments of the organization to concern about the information security within the organization.″Exploring the Relationship between Organizational Culture and Information Security Culture″ provides the following definition of information security culture: ″ISC is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds.″
Information security culture needs to be improved continuously. In ″Information Security Culture from Analysis to Change″, authors commented, ″It′s a never ending process, a cycle of evaluation and change or maintenance.″ To manage the information security culture, five steps should be taken: Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.
Sources of standards.
International Organization for Standardization (ISO) is a consortium of national standards
institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is
the world's largest developer of standards. ISO 15443: "Information technology - Security techniques - A framework
for IT security assurance", ISO/IEC 27002: "Information technology - Security techniques - Code of practice for information security management", ISO-20000: "Information technology - Service management", and ISO/IEC 27001: "Information technology - Security techniques - Information security management systems - Requirements" are of particular interest to information security professionals.
The US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency
within the U.S. Department of Commerce. The NIST Computer Security Division
develops standards, metrics, tests and validation programs as well as publishes standards and guidelines to
increase secure IT planning, implementation, management and operation. NIST is also the custodian of the US Federal Information Processing Standard publications (FIPS).
The Internet Society is a professional membership society with more than 100 organization
and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the
future of the Internet, and is the organization home for the groups responsible for Internet infrastructure standards,
including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.
The Information Security Forum is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.
The Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The Institute developed the This framework describes the range of competencies expected of Information Security and Information Assurance Professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organisations and world-renowned academics and security leaders.
The German Federal Office for Information Security (in German "Bundesamt für Sicherheit in der Informationstechnik (BSI)") BSI-Standards 100-1 to 100-4 are a set of recommendations including "methods, processes, procedures, approaches and measures relating to information security". The BSI-Standard 100-2 "IT-Grundschutz Methodology" describes how an information security management can be implemented and operated. The Standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005 the catalogs were formerly known as "IT Baseline Protection Manual". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4.400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.
At the European Telecommunications Standards Institute a catalog of Information security indicators have been standardized by the Industrial Specification Group (ISG) ISI.
Conclusion.
The never ending process of information security involves ongoing training, assessment, protection, monitoring and detection, incident response and repair, documentation, and review. This makes information security an indispensable part of all the business operations across different domains.

</doc>
<doc id="15037" url="http://en.wikipedia.org/wiki?curid=15037" title="Income">
Income

Income is the consumption and savings opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. However, for households and individuals, "income is the sum of all the wages, salaries, profits, interests payments, rents and other forms of earnings received... in a given period of time." 
In the field of public economics, the term may refer to the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income.
Increase in income.
Income per capita has been increasing steadily in almost every country. Many factors contribute to people having a higher income such as education, globalisation and favorable political circumstances such as economic freedom and peace. Increase in income also tends to lead to people choosing to work less hours.
Developed countries (defined as countries with a "developed economy") have higher incomes as opposed to developing countries tending to have lower incomes.
Economic definitions.
In economics, "factor income" is the return accruing for a person, or a nation, derived from the "factors of production": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.
From labor services, as well as ownership of land and capital.
In consumer theory 'income' is another name for the "budget constraint," an amount formula_1 to be spent on different goods x and y in quantities formula_2 and formula_3 at prices formula_4 and formula_5. The basic equation for this is
This equation implies two things. First buying one more unit of good x implies buying formula_7 less units of good y. So, formula_7 is the "relative" price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed formula_1, then its relative price falls. The usual hypothesis is that the quantity demanded of x would increase at the lower price, the law of demand. The generalization to more than two goods consists of modelling y as a composite good.
The theoretical generalization to more than one period is a multi-period wealth and income constraint. For example the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis.
Full and Haig-Simons income.
Full income refers to the accumulation of both, monetary and non-monetary consumption ability of any given entity, such a person or household. According to the what economist Nicholas Barr describes as the "classical definition of income:" the 1938 Haig-Simons definition, "income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights..." Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, "i.e." failing to accurately reflect affluence and that is consumption opportunities of any given agent. It omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, "in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full-income prevent a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income." On the macro-economic level, national per-capita income, increases with the consumption of activities that produce harm and omits many variables of societal health.
Income inequality.
Income inequality refers to the extent to which income is distributed in an uneven manner. Within a society can be measured by various methods, including the Lorenz curve and the Gini coefficient. Economists generally agree that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice.
National income, measured by statistics such as the Net National Income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see measures of national income and output.
Income in philosophy and ethics.
Throughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' ( (ASV)).
Some scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both individual and national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his "Theory of Moral Sentiments", and has more recently been developed by Harvard economist Benjamin Friedman in his book "The Moral Consequences of Economic Growth".
Accountancy.
The International Accounting Standards Board (IASB) uses the following definition: "Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants." [F.70] (IFRS Framework)

</doc>
<doc id="15039" url="http://en.wikipedia.org/wiki?curid=15039" title="Iona">
Iona

Iona (Scottish Gaelic: "Ì Chaluim Chille") is a small island in the Inner Hebrides off the Ross of Mull on the western coast of Scotland. It was a centre of Gaelic monasticism for four centuries and is today renowned for its tranquility and natural beauty. It is a popular tourist destination and a place for retreats. Its modern Gaelic name means "Iona of (Saint) Columba" (formerly anglicised "Icolmkill").
Etymology.
The Hebrides have been occupied by the speakers of several languages since the Iron Age, and as a result many of the names of these islands have more than one possible meaning. Nonetheless few, if any, can have accumulated so many different names over the centuries as the island now known in English as "Iona".
The earliest forms of the name enabled place-name scholar William J. Watson to show that the name originally meant something like "yew-place". The element "Ivo-", denoting "yew", occurs in Ogham inscriptions ("Iva-cattos" [genitive], "Iva-geni" [genitive]) and in Gaulish names ("Ivo-rix", "Ivo-magus") and may form the basis of early Gaelic names like "Eogan" (ogham: "Ivo-genos"). It is possible that the name is related to the mythological figure, "Fer hÍ mac Eogabail", foster-son of Manannan, the forename meaning "man of the yew".
Mac an Tàilleir (2003) lists the more recent Gaelic names of "Ì", "Ì Chaluim Chille" and "Eilean Idhe" noting that the first named is "generally lengthened to avoid confusion" to the second, which means "Calum's (i.e. in latinised form "Columba's") Iona" or "island of Calum's monastery". The possible confusion results from "ì", despite its original etymology, becoming a Gaelic noun (now obsolete) meaning simply "island". "Eilean Idhe" means "the isle of Iona", also known as "Ì nam ban bòidheach" ("the isle of beautiful women"). The modern English name comes from an 18th-century misreading of yet another variant, "Ioua", which was either just Adomnán's attempt to make the Gaelic name fit Latin grammar or else a genuine derivative from "Ivova" ("yew place"). "Ioua"'s change to "Iona" results from a transcription mistake resulting from the similarity of "n" and "u" in Insular Minuscule.
Despite the continuity of forms in Gaelic between the pre-Norse and post-Norse eras, Haswell-Smith (2004) speculates that the name may have a Norse connection, "Hiōe" meaning "island of the den of the brown bear", "island of the den of the fox", or just "island of the cave". The medieval English language version was "Icolmkill" (and variants thereof).
Folk etymology.
Murray (1966) claims that the "ancient" Gaelic name was "Innis nan Druinich" ("the isle of Druidic hermits") and repeats a Gaelic story (which he admits is apocryphal) that as Columba's coracle first drew close to the island one of his companions cried out "Chì mi i" meaning "I see her" and that Columba's response was "Henceforth we shall call her Ì".
Geography.
Iona lies about 2 km from the coast of Mull. It is about 2 km wide and 6 km long with a resident population of 125. The geology of the island consists mainly of Precambrian Lewisian gneiss with Torridonian sedimentary rocks on the eastern side and small outcrops of pink granite on the eastern beaches. Like other places swept by ocean breezes, there are few trees; most of them are near the parish church.
Iona's highest point is Dùn Ì, 101 m, an Iron Age hill fort dating from 100 BC – AD 200. Iona's geographical features include the Bay at the Back of the Ocean and "Càrn Cùl ri Éirinn" (the Hill/Cairn of [turning the] Back to Ireland), said to be adjacent to the beach where St. Columba first landed.
The main settlement, located at St. Ronan's Bay on the eastern side of the island, is called "Baile Mòr" and is also known locally as "The Village". The primary school, post office, the island's two hotels, the Bishop's House and the ruins of the Nunnery are here. The Abbey and MacLeod Centre are a short walk to the north. Port Bàn (white port) beach on the west side of the island is home to the Iona Beach Party.
There are numerous offshore islets and skerries: Eilean Annraidh (island of storm) and Eilean Chalbha (calf island) to the north, Rèidh Eilean and Stac MhicMhurchaidh to the west and Eilean Mùsimul (mouse holm island) and Soa Island to the south are amongst the largest. The steamer "Cathcart Park" carrying a cargo of salt from Runcorn to Wick ran aground on Soa on 15 April 1912, the crew of 11 escaping in two boats.
History.
Dál Riata.
In the early Historic Period Iona lay within the Gaelic kingdom of Dál Riata. The island was the site of a highly important monastery (see Iona Abbey) during the Early Middle Ages. According to tradition the monastery was founded in 563 by the monk Columba, also known as Colm Cille, who had been exiled from his native Ireland as a result of his involvement in the Battle of Cul Dreimhne. Columba and twelve companions went into exile on Iona and founded a monastery there. The monastery was hugely successful, and played a crucial role in the conversion to Christianity of the Picts of present-day Scotland in the late 6th century and of the Anglo-Saxon kingdom of Northumbria in 635. Many satellite institutions were founded, and Iona became the centre of one of the most important monastic systems in Great Britain and Ireland.
Iona became a renowned centre of learning, and its scriptorium produced highly important documents, probably including the original texts of the Iona Chronicle, thought to be the source for the early Irish annals. The monastery is often associated with the distinctive practices and traditions known as Celtic Christianity. In particular, Iona was a major supporter of the "Celtic" system for calculating the date of Easter at the time of the Easter controversy, which pitted supporters of the Celtic system against those favoring the "Roman" system used elsewhere in Western Christianity. The controversy weakened Iona's ties to Northumbria, which adopted the Roman system at the Synod of Whitby in 664, and to Pictland, which followed suit in the early 8th century. Iona itself did not adopt the Roman system until 715, according to the Anglo-Saxon historian Bede. Iona's prominence was further diminished over the next centuries as a result of Viking raids and the rise of other powerful monasteries in the system, such as the Abbey of Kells.
The Book of Kells may have been produced or begun on Iona towards the end of the 8th century. Around this time the island's exemplary high crosses were sculpted; these may be the first such crosses to contain the ring around the intersection that became characteristic of the "Celtic cross". The series of Viking raids on Iona began in 794 and, after its treasures had been plundered many times, Columba's relics were removed and divided two ways between Scotland and Ireland in 849 as the monastery was abandoned.
Kingdom of the Isles.
As the Norse domination of the west coast of Scotland advanced, Iona became part of the Kingdom of the Isles. The Norse "Rex plurimarum insularum" Amlaíb Cuarán died in 980 or 981 whilst in "religious retirement" on Iona. Nonetheless the island was sacked twice by his successors, on Christmas night 986 and again in 987. Although Iona was never again important to Ireland, it rose to prominence once more in Scotland following the establishment of the Kingdom of Alba in the later 9th century. The ruling dynasty of Alba traced its origin to Iona, and the island thus became an important spiritual centre of the new kingdom, with many of its early kings buried there.
A convent for Benedictine nuns was established in about 1208, with Bethóc, daughter of Somerled, as first prioress. The present Benedictine abbey, Iona Abbey, was built in about 1203. The monastery itself flourished until the Reformation when buildings were demolished and all but three of the 360 carved crosses destroyed.
Kingdom of Scotland.
Following the 1266 Treaty of Perth the Hebrides were restored to Scottish rule. An Augustine nunnery survives as a number of 13th century ruins, including a church and cloister. The nunnery continued to be active until the Reformation. By the 1760s little more of the nunnery remained standing than at present, though it is the most complete remnant of a medieval nunnery in Scotland.
Post-Union.
After a visit in 1773, the English writer Samuel Johnson described the island as "fruitful", but backward and impoverished:
He estimated the population of the village at 70 families or perhaps 350 inhabitants.
In the 19th century green-streaked marble was commercially mined in the south-east of Iona; the quarry and machinery survive.
Iona Abbey.
Iona Abbey, now an ecumenical church, is of particular historical and religious interest to pilgrims and visitors alike. It is the most elaborate and best-preserved ecclesiastical building surviving from the Middle Ages in the Western Isles of Scotland. Though modest in scale in comparison to medieval abbeys elsewhere in Western Europe, it has a wealth of fine architectural detail, and monuments of many periods. The 8th Duke of Argyll presented the sacred buildings and sites of the island to the Iona Cathedral trust in 1899.
In front of the Abbey stands the 9th century St Martin's Cross, one of the best-preserved Celtic crosses in the British Isles, and a replica of the 8th century St John's Cross (original fragments in the Abbey museum).
The ancient burial ground, called the Rèilig Odhrain (Eng: Oran's "burial place" or "cemetery"), contains the 12th century chapel of St Odhrán (said to be Columba's uncle), restored at the same time as the Abbey itself. It contains a number of medieval grave monuments. The abbey graveyard contains the graves of many early Scottish Kings, as well as kings from Ireland, Norway and France. Iona became the burial site for the kings of Dál Riata and their successors. Notable burials there include:
In 1549 an inventory of 48 Scottish, 8 Norwegian and 4 Irish kings was recorded. None of these graves are now identifiable (their inscriptions were reported to have worn away at the end of the 17th century). Saint Baithin and Saint Failbhe may also be buried on the island. The Abbey graveyard is also the final resting place of John Smith, the former Labour Party leader, who loved Iona. His grave is marked with an epitaph quoting Alexander Pope: "An honest man's the noblest work of God".
Other early Christian and medieval monuments have been removed for preservation to the cloister arcade of the Abbey, and the Abbey museum (in the medieval infirmary). The ancient buildings of Iona Abbey are now cared for by Historic Scotland (entrance charge).
Present day.
The island, other than the land owned by the Iona Cathedral Trust, was purchased from the Duke of Argyll by Hugh Fraser in 1979 and donated to the National Trust for Scotland. In 2001 Iona's population was 125 and by the time of the 2011 census this had grown to 177 usual residents. During the same period Scottish island populations as a whole grew by 4% to 103,702.
Iona Community.
In 1938 George MacLeod founded the Iona Community, an ecumenical Christian community of men and women from different walks of life and different traditions in the Christian church committed to seeking new ways of living the Gospel of Jesus in today's world. This community is a leading force in the present Celtic Christian revival.
The Iona Community runs 3 residential centres on the Isle of Iona and on Mull, where one can live together in community with people of every background from all over the world. Weeks at the centres often follow a programme related to the concerns of the Iona Community.
The 8 tonne "Fallen Christ" sculpture by Ronald Rae was permanently situated outside the MacLeod Centre in 2008.
Transport.
Visitors can reach Iona by the 10-minute ferry trip across the Sound of Iona from Fionnphort on Mull. The most common route is via Oban in Argyll and Bute. Regular ferries connect to Craignure on Mull, from where the scenic road runs 37 miles to Fionnphort. Tourist coaches and local bus services meet the ferries.
There are very few cars on the island, as they are tightly regulated and vehicular access is not allowed for non-residents, who have to leave their car in Fionnphort. Bike hire is available at the pier, and on Mull.
Accommodation.
In addition to the hotels, there are several bed and breakfasts on Iona and various self-catering properties. The Iona Hostel at Lagandorain and the Iona campsite at Cnoc Oran also offer accommodation. 
Iona from Mull. The Abbey lies below Dun Ì at right and the main settlement of Baile Mòr is to the left.
Media and the arts.
Samuel Johnson wrote "That man is little to be envied whose patriotism would not gain force upon the plains of Marathon, or whose piety would not grow warmer amid the ruins of Iona."
"Peace of Iona" is a song written by Mike Scott that appears on the studio album "Universal Hall" and on the live recording "Karma to Burn" by The Waterboys. Iona is the setting for the song "Oran" on the 1997 Steve McDonald album "Stone of Destiny".
Kenneth C. Steven published an anthology of poetry entitled "Iona: Poems" in 2000 inspired by his association with the island and the surrounding area.
Iona is featured prominently in the first episode ("By the Skin of Our Teeth") of the celebrated arts series "" (1969).
Iona is the setting of Jeanne M. Dams' Dorothy Martin mystery "Holy Terror of the Hebrides" (1998).
The Academy Award–nominated Irish animated film "The Secret of Kells" is about the creation of the Book of Kells. One of the characters, Brother Aiden, is a master illuminator from Iona Abbey who had helped to illustrate the Book, but had to escape the island with it during a Viking invasion.
References.
· 

</doc>
<doc id="15040" url="http://en.wikipedia.org/wiki?curid=15040" title="Ido (language)">
Ido (language)

Ido is a constructed language created to be a universal second language for speakers of diverse backgrounds. Ido was specifically designed to be grammatically, orthographically, and lexicographically regular, and above all easy to learn and use. In this sense, Ido is classified as a constructed international auxiliary language.
Ido was created in 1907 out of a desire to reform perceived flaws in Esperanto, a language that had been created for the same purpose 20 years earlier. The name of the language traces its origin to the Esperanto word "ido", meaning "offspring", since the language is a "descendant" of Esperanto. After its inception, Ido gained support from some in the Esperanto community, but following the sudden death in 1914 of one of its most influential proponents, Louis Couturat, it declined in popularity. There were two reasons for this: first, the emergence of further schisms arising from competing reform projects; and second, a general lack of awareness of Ido as a candidate for an international language. These obstacles weakened the movement and it was not until the rise of the Internet that it began to regain momentum.
Ido uses the same 26 letters as the English (Latin) alphabet with no diacritics. It draws its vocabulary from French, Italian, Spanish, English, German, and Russian, and is largely intelligible to those who have studied Esperanto.
Several works of literature have been translated into Ido, including "The Little Prince" and the Gospel of Luke. As of the year 2000, there were approximately 100–200 Ido speakers in the world.
History.
The idea of a universal second language is not new, and constructed languages are not a recent phenomenon. The first known constructed language was Lingua Ignota, created in the 12th century. But the idea did not catch on in large numbers until the language Volapük was created in 1879. Volapük was popular for some time and apparently had a few thousand users, but was later eclipsed by the popularity of Esperanto, which arose in 1887. Several other languages such as Latino sine Flexione and Idiom Neutral had also been put forward. It was during this time that French mathematician Louis Couturat formed the "Delegation for the Adoption of an International Auxiliary Language".
This delegation made a formal request to the International Association of Academies in Vienna to select and endorse an international language; the request was rejected in May 1907. The Delegation then met as a Committee in Paris in October 1907 to discuss the adoption of a standard international language. Among the languages considered was a new language anonymously submitted under the pen name "Ido". In the end the Committee concluded that no language was completely acceptable, but that Esperanto could be accepted "on condition of several modifications to be realized by the permanent Commission in the direction defined by the conclusions of the Report of the Secretaries [Louis Couturat and Léopold Leau] and by the Ido project."
Esperanto's inventor, L. L. Zamenhof, had suggested in an 1894 proposition for a Reformed Esperanto several changes that Ido adopted: eliminating the accented letters and the accusative case, changing the plural to an Italianesque "-i", and replacing the table of correlatives with more Latinate words. However, the Esperanto community rejected Reformed Esperanto, and likewise most rejected the recommendations of the 1907 Committee. Zamenhof deferred to their judgment. Furthermore, controversy ensued when the "Ido project" was found to have been primarily devised by Louis de Beaufront, who represented Esperanto before the Committee.
It is estimated that 20% of the Esperanto leaders and 3–4% of the ordinary Esperantists defected to Ido. Although it fractured the Esperanto movement, the schism gave the remaining Esperantists freedom to concentrate on using and promoting their language as it stood. At the same time, it gave the Idists freedom to continue working on their own language for several more years before actively promoting it. The "Uniono di la Amiki di la Linguo Internaciona" ("Union of Friends of the International Language") was established along with an Ido Academy to work out the details of the new language.
Couturat, who was the leading proponent of Ido, was killed in an automobile accident in 1914. This, along with World War I, practically suspended the activities of the Ido Academy from 1914 to 1920. In 1928 Ido's major intellectual supporter, the Danish linguist Otto Jespersen, published his own planned language, Novial. His defection from the Ido movement set it back even further.
Digital era.
The language still has active speakers today, and the Internet has sparked a renewal of interest in the language in recent years. A sample of 24 Idists on the Yahoo! group "Idolisto" during November 2005 showed that 57% had begun their studies of the language during the preceding three years, 32% from the mid-1990s to 2002, and 8% had known the language from before.
Changes.
Few changes have been made to Ido since 1922.
Camiel de Cock was named secretary of linguistic issues in 1990, succeeding Roger Moureaux. He resigned after the creation of a linguistic committee in 1991. De Cock was succeeded by Robert C. Carnaghan, who held the position from 1992 to 2008. No new words were adopted between 2001 and 2006. Following the 2008–2011 elections of ULI's direction committee, Gonçalo Neves replaced Carnaghan as secretary of linguistic issues in February 2008. Neves resigned in August 2008. A new linguistic committee was formed in 2010. In April 2010, Tiberio Madonna was appointed as secretary of linguistic issues, succeeding Neves. In January 2011, ULI approved eight new words. This was the first addition of words in many years. As of April 2012, the secretary of linguistic issues remains Tiberio Madonna.
Phonology.
Ido has five vowel phonemes. The vowels [] and [] are interchangeable depending on speaker preference, as are [] and []. The combinations /au/ and /eu/ become diphthongs in word roots but not when adding affixes.
All polysyllable words are stressed on the second-to-last syllable except for verb infinitives, which are stressed on the last syllable – skolo, kafeo and lernas for "school", "coffee" and the present tense of "to learn", but irar, savar and drinkar for "to go", "to know" and "to drink". If an i or u precedes another vowel, the pair is considered part of the same syllable when applying the accent rule – thus radio, familio and manuo for "radio", "family" and "hand", unless the two vowels are the only ones in the word, in which case the "i" or "u" is stressed: dio, frua for "day" and "early".
Orthography.
Ido uses the same 26 letters as the English alphabet with three digraphs and no ligatures. Where the table below lists two pronunciations, either is perfectly acceptable.
The digraphs are:
Grammar.
Each word in the Ido vocabulary is built from a root word. A word consists of a root and a grammatical ending. Other words can be formed from that word by removing the grammatical ending and adding a new one, or by inserting certain affixes between the root and the grammatical ending.
Some of the grammatical endings are defined as follows:
These are the same as in Esperanto except for "-i", "-ir", "-ar", "-or" and "-ez". Esperanto marks noun plurals by an "agglutinative" ending "-j" (so plural nouns end in "-oj"), uses "-i" for verb infinitives (Esperanto infinitives are tenseless), and uses "-u" for the imperative. Verbs in Ido do not conjugate depending on person, number or gender; the -as, -is, and -os endings suffice whether the subject is I, you, he, she, they, or anything else.
Syntax.
Ido word order is generally the same as English (subject–verb–object), so the sentence "Me havas la blua libro" is the same as the English "I have the blue book", both in meaning and word order. There are a few differences, however:
Ido generally does not impose rules of grammatical agreement between grammatical categories within a sentence. For example, the verb in a sentence is invariable regardless of the number and person of the subject. Nor must the adjectives be pluralized as well the nouns – in Ido "the large books" would be "la granda libri" as opposed to the French "les grands livres" or the Esperanto "la grandaj libroj".
Negation occurs in Ido by simply adding ne before a verb: Me ne havas libro means "I do not have a book". This as well does not vary, and thus the "I do not", "He does not", "They do not" before a verb are simply Me ne, Il ne, and Li ne. In the same way, past tense and future tense negatives are formed by ne before the conjugated verb. "I will not go" and "I did not go" become Me ne iros and Me ne iris respectively.
Yes/no questions are formed by the particle ka in front of the question. "I have a book" (me havas libro) becomes Ka me havas libro? (do I have a book?). Ka can also be placed in front of a noun without a verb to make a simple question, corresponding to the English "is it?" Ka Mark? can mean, "Are you Mark?", "Is it Mark?", "Do you mean Mark?" depending on the context.
Pronouns.
The pronouns of Ido were revised to make them more acoustically distinct than those of Esperanto, which all end in "i". Especially the singular and plural first-person pronouns "mi" and "ni" may be difficult to distinguish in a noisy environment, so Ido has "me" and "ni" instead. Ido also distinguishes between intimate ("tu") and formal ("vu") second-person singular pronouns as well as plural second-person pronouns ("vi") not marked for intimacy. Furthermore, Ido has a pan-gender third-person pronoun "lu" (it can mean "he", "she", or "it", depending on the context) in addition to its masculine ("il"), feminine ("el"), and neuter ("ol") third-person pronouns.
It should be noted that "ol", like English "it" and Esperanto "ĝi", is not limited to inanimate objects, but can be used "for entities whose sex is indeterminate: "babies, children, humans, youths, elders, people, individuals, horses, [cattle], cats," etc."
"Lu" is often mistakenly labeled an epicene pronoun, that is, one that refers to both masculine and feminine beings, but in fact, "lu" is more properly a "pan-gender" pronoun, as it is also used for referring to inanimate objects. From "Kompleta Gramatiko Detaloza di la Linguo Internaciona Ido" by Beaufront:
"Lu" (like "li") is used "for all three genders." That "lu" does duty for the three genders at will in the singular is not in itself any more astonishing than seeing "li" serve the three genders at will in the plural ... By a decision (1558) the Idist Academy rejected every restriction concerning the use of "lu." One may thus use that pronoun in exactly the same way for a thing and a person of obvious sex as for animals of unknown sex and a person that has a genderless name, like "baby, child, human," etc., these being as truly masculine as feminine.
The motives for this decision were given in "Mondo", XI, 68: "Lu" for the singular is exactly the same as "li" for the plural. Logic, symmetry and ease demand this. Consequently, just as "li" may be used for people, animals, and things whenever nothing obliges one to express the gender, so "lu" may be used for people, animals, and things under the same condition. The proposed distinction would be a bothersome subtlety ...
Vocabulary.
Vocabulary in Ido is derived from French, Italian, Spanish, English, German, and Russian. Basing the vocabulary on various widespread languages was intended to make Ido as easy as possible for the greatest number of people possible. Early on, the first 5,371 Ido word roots were analyzed compared to the vocabulary of the six source languages, and the following result was found:
Another analysis showed that:
Vocabulary in Ido is often created through a number of official prefixes and suffixes that alter the meaning of the word. This allows a user to take existing words and modify them to create neologisms when necessary, and allows for a wide range of expression without the need to learn new vocabulary each time. Though their number is too large to be included in one article, some examples include:
New vocabulary is generally created through an analysis of the word, its etymology, and reference to the six source languages. If a word can be created through vocabulary already existing in the language then it will usually be adopted without need for a new radical (such as wikipedio for "Wikipedia", which consists of wiki + enciklopedio for "encyclopedia"), and if not an entirely new word will be created. The word alternatoro for example was adopted in 1926, likely because five of the six source languages used largely the same orthography for the word, and because it was long enough to avoid being mistaken for other words in the existing vocabulary. Adoption of a word is done through consensus, after which the word will be made official by the union. Care must also be taken to avoid homonyms if possible, and usually a new word undergoes some discussion before being adopted. Foreign words that have a restricted sense and are not likely to be used in everyday life (such as the word "intifada" to refer to the conflict between Israel and Palestine) are left untouched, and often written in italics.
Ido, unlike Esperanto, does not assume the male sex by default. For example, Ido does not derive the word for "waitress" by adding a feminine suffix to "waiter", as Esperanto does. Instead, Ido words are defined as sex-neutral, and two different suffixes derive masculine and feminine words from the root: "servisto" for a waiter of either sex, "servistulo" for a male waiter, and "servistino" for a waitress. There are only two exceptions to this rule: First, "patro" for "father", "matro" for "mother", and "genitoro" for "parent", and second, "viro" for "man", "muliero" for "woman", and "adulto" for "adult".
Sample.
The Lord's Prayer:
Literature and publications.
Ido has a number of publications that can be subscribed to or downloaded for free in most cases. "Kuriero Internaciona" is a magazine produced in France every few months with a range of topics. "Adavane!" is a magazine produced by the Spanish Ido Society every two months that has a range of topics, as well as a few dozen pages of work translated from other languages. "Progreso" is the official organ of the Ido movement and has been around since the inception of the movement in 1908. Other sites can be found with various stories, fables or proverbs along with a few books of the Bible translated into Ido on a smaller scale. The site "publikaji" has a few podcasts in Ido along with various songs and other recorded material.
The online encyclopedia Wikipedia includes an (known in Ido as "Wikipedio"); in January 2012 it was the 81st most visited Wikipedia.

</doc>
<doc id="15041" url="http://en.wikipedia.org/wiki?curid=15041" title="Improvisational theatre">
Improvisational theatre

Improvisational theatre, often called improv or impro, is a form of theater where most or all of what is performed is created at the moment it is performed. In its purest form, the dialogue, the action, the story and the characters are created collaboratively by the players as the improvisation unfolds in present time, without use of an already prepared, written script.
Improvisational theatre exists in performance as a range of styles of improvisational comedy as well as some non-comedic theatrical performances. It is sometimes used in film and television, both to develop characters and scripts and occasionally as part of the final product.
Improvisational techniques are often used extensively in drama programs to train actors for stage, film and television and can be an important part of the rehearsal process. However, the skills and processes of improvisation are used outside of the context of performing arts, as well. It is used in classrooms as an educational tool and in businesses as a way to develop communication skills, creative problem solving and supportive team-work abilities that are used by improvisational, ensemble players. It is sometimes used in psychotherapy as a tool to gain insight into a person's thoughts, feelings and relationships.
History.
The earliest well documented use of improvisational theatre in Western history is found in the Atellan Farce of Rome circa 391 BC. From the 16th to the 18th centuries, Commedia dell'arte performers improvised based on a broad outline in the streets of Italy and in the 1890s theatrical theorists and directors such as Russian, Konstantin Stanislavski and the French, Jacques Copeau, founders of two major streams of acting theory, both heavily utilized improvisation in acting training and rehearsal.
Modern.
Modern theatrical improvisation games began as drama exercises for children, which were a staple of drama education in the early 20th Century thanks in part to the progressive education movement initiated by John Dewey in 1916. Some people credit American Dudley Riggs as the first vaudevillian to use audience suggestions to create improvised sketches on stage. Improvisation exercises were developed further by Viola Spolin in the 1940s, 50s and 60s, and codified in her book "Improvisation For The Theater", the first book that gave specific techniques for learning to do and teach improvisational theater. In the 1970s in Canada, British playwright and director Keith Johnstone wrote , a book outlining his ideas on improvisation, and invented Theatresports which has become a staple of modern improvisational comedy and is the inspiration for the popular television show "Whose Line Is It Anyway?"
Spolin influenced the first generation of modern, American improvisors at The Compass Players in Chicago, which led to The Second City. Her son, Paul Sills, along with David Shepherd, started The Compass Players. Following the demise of the Compass Players, Paul Sills began The Second City. They were the first organized troupes in Chicago, Illinois, and the modern Chicago improvisational comedy movement grew from their success.
Many of the current "rules" of comedic improv were first formalized in Chicago in the late 1950s and early 1960s, initially among The Compass Players troupe, which was directed by Paul Sills. From most accounts David Shepherd provided the philosophical vision of the Compass Players, while Elaine May was central to the development of the premises for its improvisations. Mike Nichols, Ted Flicker, and Del Close were her most frequent collaborators in this regard. When The Second City opened its doors on December 16, 1959, directed by Paul Sills, his mother Viola Spolin began training new improvisers through a series of classes and exercises which became the cornerstone of modern improv training. By the mid-1960s, Viola Spolin's classes were handed over to her protégé, Jo Forsberg, who further developed Spolin's methods into a one-year course, which eventually became The Players Workshop, the first official school of improvisation in the USA. During this time Jo Forsberg trained many of the performers who went on to star on The Second City stage.
Many of the original cast of "Saturday Night Live" came from The Second City and the franchise has produced such comedy stars as Mike Myers, Tina Fey, Bob Odenkirk, Amy Sedaris, Stephen Colbert, Eugene Levy, Jack McBrayer, Steve Carell, Chris Farley, Dan Aykroyd and John Belushi.
Simultaneously, Keith Johnstone's group "The Theatre Machine", which originated in London, was touring Europe. This work gave birth to Theatresports, at first secretly in Johnstone's workshops, and eventually in public when he moved to Canada. Toronto has been home to a rich improv tradition.
In 1984 Dick Chudnow ("Kentucky Fried Theater") founded ComedySportz in Milwaukee, WI. Expansion began with the addition of ComedySportz-Madison (WI), in 1985. The first Comedy League of America National Tournament was held in 1988, with 10 teams participating. The league is now known as World Comedy League and boasts a roster of 24 international cities.
In San Francisco, The Committee theater was active in North Beach during the 1960s. It was founded by alumni of Chicago's Second City, Alan Myerson and his wife Jessica.
When The Committee disbanded in 1972, three major companies were formed: The Pitchell Players, The Wing, and Improvisation Inc. The only company that continued to perform Close’s Harold was the latter one. Its two former members, Michael Bossier and John Elk, in San Francisco’s famous Old Spaghetti Factory formed Spaghetti Jam in 1976, where Short-Form improv and Harolds were performed through 1983. Stand-up comedians performing down the street at the Intersection for the Arts would drop by and sit in. In 1979 John Elk brought Short-Form to England, teaching workshops at Jacksons Lane Theatre and was the first American to perform at The Comedy Store, London, above a Soho strip club.
Modern political improvisation's roots include Jerzy Grotowski's work in Poland during the late 1950s and early 1960s, Peter Brook's "happenings" in England during the late 1960s, Augusto Boal's "Forum Theatre" in South America in the early 1970s, and San Francisco's The Diggers' work in the 1960s. Some of this work led to pure improvisational performance styles, while others simply added to the theatrical vocabulary and were, on the whole, avant garde experiments.
Joan Littlewood, the English actress and director who was active from the 1930s to 1970s, made extensive use of improv in developing plays for performance. However she was successfully prosecuted twice for allowing her actors to improvise in performance. Until 1968, British law required scripts to be approved by the Lord Chamberlain's Office. The department also sent inspectors to some performances to check that the approved script was performed exactly as approved.
In 2012, Lebanese writer and director Lucien Bourjeily used improvisational theater techniques to create a hard-hitting multi-sensory play entitled "66 Minutes in Damascus" that premiered at the London International Festival of Theater and is considered one of the most extreme kinds of interactive improvised theater put on stage, where the audience play the part of kidnapped tourists in today's Syria in a hyperreal sensory environment.
Rob Wittig and Mark C. Marino have developed a form of improv for online theatrical improvisation called netprov. The form relies on social media to engage audiences in the creation of dynamic fictional scenarios that evolve in real-time.
Improvisational Comedy.
Modern improvisational comedy, as it is practiced in the West, falls generally into two categories: shortform and longform.
Shortform improv consists of short scenes usually constructed from a predetermined game, structure, or idea and driven by an audience suggestion. Many shortform exercises were first created by Viola Spolin, which she called Theater Games, influenced by her training from recreational games expert, Neva Boyd. The shortform improv comedy television series "Whose Line Is It Anyway?" has familiarized American and British viewers with shortform.
Longform improv performers create shows in which short scenes are often interrelated by story, characters, or themes. Longform shows may take the form of an existing type of theatre, for example a full-length play or Broadway-style musical such as Spontaneous Broadway. Longform improvisation is especially performed in Chicago, New York City, Los Angeles; has a strong presence in Austin, Boston, Minneapolis, Phoenix, Philadelphia, San Francisco, Seattle, Detroit, Toronto, Vancouver, Washington, D.C.; and is building a growing following in Denver, Kansas City, Columbus, New Orleans, Omaha, Rochester, and Hawaii. One of the better-known longform structures is the Harold, developed by ImprovOlympic co-founder Del Close. Many such longform structures now exist.
Fundamental Rules of Improv.
1. Listen: easier said than done, and that's exactly the point
2. Agreement: say yes and add something, don't reject ideas
3. Team Work: have a group mind, think of others
4. Don't Block: stealing jokes / not listening / changing topic
5. Relationship: focus on connection between characters, not just subject of scene
6. Initiation: who, what, when to set the scene
7. Point of View, Opinion and Intention: have them, these help express and build your character
8. Be in Character: maintain character throughout the scene
9. Don't Ask Questions: too many questions can make your partner do all the work
10. Make Active Choices: Do something. Don't be talking heads
Examples of Improv Comedy Games.
There are several types of improv comedy games that follow these general fundamental rules of improv: Improv games are fun to make up and can be experimental. Games can be adapted to tell a different story with the same or similar triggers.
Good Cop Bad Cop
"Players": (3) good cop, bad cop, the criminal
"How to play:" The game begins with the criminal leaving the room - before any of the callouts are taken. Once out of sounds reach, the host asks the audience 3 questions to establish why the person was arrested.
Call Outs: 
1) What crime did they commit?
2) Where did they commit the crime?
3) Who did they commit the crime with?
The prisoner is brought into the holding cell (stage) by one of the cops. They cops interrogate the prisoner in their personas as good cop (gives the prisoner a break, not as aggressive) and bad cop (forceful, doubts prisoner). Through this back and forth between the prisoner and the cops the prisoner tries to answer the 3 questions above - in this order. Once the prisoner solves all 3, they must write a confession. "I stole a goat from my neighbors closet with Jimmy Fallon."
I Once Dated A ...
"Players": All + host
"How to play:" all the players line up shoulder to shoulder on the stage with one microphone in the center (you can play without a mic too). The host has a list of objects from the audience (distribute paper and pens before the performance to get call outs for games like this). The host begins the game by picking an object from the audience's list and prompts the players by saying, "I once dated an octopus" (repeats it again for clarity). Each player - not necessarily in order - step up to the mic and finish the statement. "I once dated an octopus. It was always awkward when we went out for sushi." After this, either another player goes forward with another idea or the host calls out another object.
Non-comedic and Experimental Improvisational Theater.
Other forms of improvisational theatre training and performance techniques are experimental and Avant-garde in nature and not necessarily intended to be comedic. These include Playback Theatre and Theatre of the Oppressed, the Poor Theatre, the Open Theatre, to name only a few.
The Open Theatre was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director, Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. This avante-garde theatre group explored political, artistic, and social issues. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as "sound and movement" and "transformations", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's "poor theater" in Poland.[1] During the sixties Chaikin and the Open Theatre developed full theatrical productions with nothing but the actors, a few chairs and a bare stage, creating character, time and place through a series of transformations the actors physicalized and discovered through improvisations.
Longform improvisation has been growing on the west coast with such groups as , and the . These formats are designed to allow for a full-length play to be created improvisationally.
Applying improv principles in life.
Many people who have studied improv have noted that the guiding principles of improv are useful, not just on stage, but in everyday life. For example, Stephen Colbert in a commencement address said,
Well, you are about to start the greatest improvisation of all. With no script. No idea what's going to happen, often with people and places you have never seen before. And you are not in control. So say "yes." And if you're lucky, you'll find people who will say "yes" back.
Tina Fey in her book Bossypants lists several rules of improv that apply in the workplace. There has been much interest in bringing lessons from improv into the corporate world. In a New York Times article titled , Stanford professor and author, notes, "executives and engineers and people in transition are looking for support in saying yes to their own voice. Often, the systems we put in place to keep us secure are keeping us from our more creative selves." Madson explores the application of thirteen "maxims of improvisational theater" to real-life in the book .
In film and television.
Many directors have made use of improvisation in the creation of both main-stream and experimental films. Many silent filmmakers such as Charlie Chaplin and Buster Keaton used improvisation in the making of their films, developing their gags while filming and altering the plot to fit. The Marx Brothers were notorious for deviating from the script they were given, their ad libs often becoming part of the standard routine and making their way into their films. Many people, however, make a distinction between ad-libbing and improvising.
The British director Mike Leigh makes extensive use of improvisation in the creation of his films, including improvising important moments in the characters lives that will not even appear in the film. "This Is Spinal Tap" and other mockumentary films of director Christopher Guest are created with a mix of scripted and unscripted material and "Blue in the Face" is a 1995 comedy directed by Wayne Wang and Paul Auster created in part by the improvisations filmed during the production of their movie Smoke.
Some of the best known American film directors who are noted for their use of improvisation in their work with actors was John Cassavetes, Robert Altman and Rob Reiner.
Improv comedy techniques have also been used in television and stand-up comedy, in hit shows such as the recent HBO television show "Curb Your Enthusiasm" created by Larry David, the UK Channel 4 and ABC television series "Whose Line Is It Anyway" (and its spinoffs "Drew Carey's Green Screen Show" and "Drew Carey's Improv-A-Ganza"), Nick Cannon's improv comedy show "Wild 'N Out", and "Thank God You're Here". In Canada, the series "Train 48" was improvised from scripts which contained a minimal outline of each scene, and the comedy series "This Sitcom Is...Not to Be Repeated" incorporated dialogue drawn from a hat during the course of an episode. The American show "Reno 911!" also contained improvised dialogue based on a plot outline. "Fast and Loose" is an improvisational game show, much like "Whose Line Is It Anyway?". The BBC sitcoms "Outnumbered" and "The Thick of It" also had some improvised elements in them.
Psychology.
In the field of the psychology of consciousness, Eberhard Scheiffele explored the altered state of consciousness experienced by actors and improvisers in his scholarly paper "Acting: an altered state of consciousness". According to G. William Farthing in "The Psychology of Consciousness" comparative study, actors routinely enter into an altered state of consciousness (ASC). Acting is seen as altering most of the 14 dimensions of changed subjective experience which characterize ASCs according to Farthing, namely: attention, perception, imagery and fantasy, inner speech, memory, higher-level thought processes, meaning or significance of experiences, time experience, emotional feeling and expression, level of arousal, self-control, suggestibility, body image, and sense of personal identity.
In the growing field of Drama Therapy, psychodramatic improvisation, along with other techniques developed for Drama Therapy, are used extensively. The "Yes, and" rule has been compared to Milton Erickson's "utilization" process and to a variety of acceptance-based psychotherapies. Improv training has been recommended for couples therapy and therapist training, and it has been speculated that improv training may be helpful in some cases of social anxiety disorder.
Structure and process.
Improvisational theatre often allows an interactive relationship with the audience. Improv groups frequently solicit suggestions from the audience as a source of inspiration, a way of getting the audience involved, and as a means of proving that the performance is not scripted. That charge is sometimes aimed at the masters of the art, whose performances can seem so detailed that viewers may suspect the scenes are planned.
In order for an improvised scene to be successful, the improvisers involved must work together responsively to define the parameters and action of the scene, in a process of co-creation. With each spoken word or action in the scene, an improviser makes an "offer", meaning that he or she defines some element of the reality of the scene. This might include giving another character a name, identifying a relationship, location, or using mime to define the physical environment. These activities are also known as "endowment". It is the responsibility of the other improvisers to accept the offers that their fellow performers make; to not do so is known as blocking, negation, or denial, which usually prevents the scene from developing. Some performers may deliberately block (or otherwise break out of character) for comedic effect—this is known as "gagging"—but this generally prevents the scene from advancing and is frowned upon by many improvisers. Accepting an offer is usually accompanied by adding a new offer, often building on the earlier one; this is a process improvisers refer to as "Yes, And..." and is considered the cornerstone of improvisational technique. Every new piece of information added helps the improvisers to refine their characters and progress the action of the scene. The "Yes, And..." rule, however, applies to a scene's early stage since it is in this stage that a "base (or shared) reality" is established in order to be later redefined by applying the "if (this is true), then (what else can also be true)" practice progressing the scene into comedy, as explained in the 2013 manual by the "Upright Citizens Brigade" members.
The unscripted nature of improv also implies no predetermined knowledge about the props that might be useful in a scene. Improv companies may have at their disposal some number of readily accessible props that can be called upon at a moment's notice, but many improvisers eschew props in favor of the infinite possibilities available through mime. In improv, this is more commonly known as 'space object work' or 'space work', not 'mime', and the props and locations created by this technique, as 'space objects' created out of 'space substance,' developed as a technique by Viola Spolin. As with all improv "offers", improvisers are encouraged to respect the validity and continuity of the imaginary environment defined by themselves and their fellow performers; this means, for example, taking care not to walk through the table or "miraculously" survive multiple bullet wounds from another improviser's gun.
Because improvisers may be required to play a variety of roles without preparation, they need to be able to construct characters quickly with physicality, gestures, accents, voice changes, or other techniques as demanded by the situation. The improviser may be called upon to play a character of a different age or sex. Character motivations are an important part of successful improv scenes, and improvisers must therefore attempt to act according to the objectives that they believe their character seeks.
Community.
Many theatre troupes are devoted to staging improvisational performances and growing the improv community through their training centers. Many of these Improv groups around the world can be found .
In addition to for-profit theatre troupes, there are many college-based improv groups in the United States and around the world.
In Europe the special contribution to the theatre of the abstract, the surreal, the irrational and the subconscious have been part of the stage tradition for centuries. From the 1990s onwards a growing number of European Improv groups have been set up specifically to explore the possibilities offered by the use of the abstract in improvised performance, including dance, movement, sound, music, mask work, lighting, and so on. These groups are not especially interested in comedy, either as a technique or as an effect, but rather in expanding the improv genre so as to incorporate techniques and approaches that have long been a legitimate part of European theatre.
Notable contributors to the field.
Some key figures in the development of improvisational theatre are Viola Spolin and her son Paul Sills, founder of Chicago's famed Second City troupe and originator of Theater Games, and Del Close, founder of ImprovOlympic (along with Charna Halpern) and creator of a popular longform improv format known as The Harold. Other luminaries include Keith Johnstone, the British teacher and writer–author of "Impro", who founded the Theatre Machine and whose teachings form the foundation of the popular shortform Theatresports format, Dick Chudnow, founder of ComedySportz which evolved its family-friendly show format from Johnstone's Theatersports, and Bill Johnson, creator/director of , who pioneered the concept of "Commun-edy Outreach" by tailoring performances to non-traditional audiences, such as the homeless and foster children.
David Shepherd, with Paul Sills, founded The Compass Players in Chicago. Shepherd was intent on developing a true "people's Theatre", and hoped to bring political drama to the stockyards. The Compass went on to play in numerous forms and companies, in a number of cities including NY and Hyannis, after the founding of The Second City. A number of Compass members were also founding members of The Second City. In the 1970s, Shepherd began experimenting with group-created videos. He is the author of "That Movie In Your Head", about these efforts. In the 1970s, David Shepherd and Howard Jerome created the Improvisational Olympics, a format for competition based improv. The Improv Olympics were first demonstrated at Toronto's Homemade Theatre in 1976 and have been continued on as the Canadian Improv Games. In the United States, the Improv Olympics were later produced by Charna Halpern under the name "ImprovOlympic" and now as "IO"; IO operates training centers and theaters in Chicago and Los Angeles. At IO, Halpern combined Shepherd's "Time Dash" game with Del Close's "Harold" game; the revised format for the Harold became the fundamental structure for the development of modern "long-form" improvisation.
In 1975 Jonathan Fox founded Playback Theatre, a form of improvised community theatre which is often not comedic and replays stories as shared by members of the audience.
The Groundlings is a popular and influential improv theatre and training center in Los Angeles, California. Gary Austin, founder of The Groundlings, continues to teach improvisation around the country, focusing especially in Los Angeles. He is widely acclaimed as one of the greatest acting teachers in America. His work is grounded in the lessons he learned as an improviser at The Committee with Del Close, as well as in his experiences as founding director of The Groudlings. The Groundlings is often seen as the Los Angeles training ground for the "second generation" of improv luminaries and troupes. Stan Wells developed the "Clap-In" style of longform improvisation here, later using this as the basis for his own theatre, The Empty Stage which in turn bred multiple troupes utilizing this style.
In the late 1990s, Matt Besser, Amy Poehler, Ian Roberts, and Matt Walsh founded the Upright Citizens Brigade Theatre in New York and later they founded one in Los Angeles, each with an accompanying improv/sketch comedy school. In September 2011 the UCB opened a third theatre in New York City's East Village, known as UCBeast.
Gunter Lösel compared the existing improvisational theater theories (from Moreno, Spolin, Johnstone, Close...), structured them and wrote a general theory of improvisational theater.

</doc>
<doc id="15043" url="http://en.wikipedia.org/wiki?curid=15043" title="International Space Station">
International Space Station

The International Space Station (ISS) is a space station, or a habitable artificial satellite, in low Earth orbit. It is a modular structure which was initiated in 1985, with the first component launched into orbit in 1998. Now the largest artificial body in orbit, it can often be seen with the naked eye from Earth. The ISS consists of pressurised modules, external trusses, solar arrays and other components. ISS components have been launched by American Space Shuttles as well as Russian Proton and Soyuz rockets.
The ISS serves as a microgravity and space environment research laboratory in which crew members conduct experiments in biology, human biology, physics, astronomy, meteorology and other fields. The station is suited for the testing of spacecraft systems and equipment required for missions to the Moon and Mars. The ISS maintains an orbit with an altitude of between 330 and by means of reboost manoeuvres using the engines of the Zvezda module or visiting spacecraft. It completes 15.54 orbits per day.
ISS is the ninth space station to be inhabited by crews, following the Soviet and later Russian Salyut, Almaz, and Mir stations as well as Skylab from the US. The station has been continuously occupied for since the arrival of Expedition 1 on 2 November 2000. This is the longest continuous human presence in space, having surpassed the previous record of held by Mir. The station is serviced by a variety of visiting spacecraft: Soyuz, Progress, the Automated Transfer Vehicle, the H-II Transfer Vehicle, Dragon, and Cygnus. It has been visited by astronauts and cosmonauts from 15 different nations.
After the US Space Shuttle program ended in 2011, Soyuz rockets became the only provider of transport for astronauts at the International Space Station, and Dragon became the only provider of bulk cargo-return-to-Earth services (downmass capability of Soyuz capsules is very limited).
The ISS programme is a joint project among five participating space agencies: NASA, Roscosmos, JAXA, ESA, and CSA. The ownership and use of the space station is established by intergovernmental treaties and agreements. The station is divided into two sections, the Russian Orbital Segment (ROS) and the United States Orbital Segment (USOS), which is shared by many nations. , the American portion of ISS was funded until 2024. Roskosmos has endorsed the continued operation of ISS through 2024, but have proposed using elements of the Russian Orbital Segment to construct a new Russian space station called OPSEK.
On March 28, 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. NASA later issued a guarded statement expressing thanks for Russia's interest in future cooperation in space exploration, but fell short of confirming the Russian announcement.
Purpose.
According to the original Memorandum of Understanding between NASA and Rosaviakosmos, the International Space Station was intended to be a laboratory, observatory and factory in low Earth orbit. It was also planned to provide transportation, maintenance, and act as a staging base for possible future missions to the Moon, Mars and asteroids. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic and educational purposes.
Scientific research.
The ISS provides a platform to conduct scientific research. Small unmanned spacecraft can provide platforms for zero gravity and exposure to space, but space stations offer a long term environment where studies can be performed potentially for decades, combined with ready access by human researchers over periods that exceed the capabilities of manned spacecraft.
The Station simplifies individual experiments by eliminating the need for separate rocket launches and research staff. The wide variety of research fields include astrobiology, astronomy, human research including space medicine and life sciences, physical sciences, materials science, space weather, and weather on Earth (meteorology). Scientists on Earth have access to the crew's data and can modify experiments or launch new ones, which are benefits generally unavailable on unmanned spacecraft. Crews fly expeditions of several months duration, providing approximately 160-man-hours per week of labour with a crew of 6.
To detect dark matter and answer other fundamental questions about our universe, engineers and scientists from all over the world built the Alpha Magnetic Spectrometer (AMS), which NASA compares to the Hubble space telescope, and says could not be accommodated on a free flying satellite platform due in part to its power requirements and data bandwidth needs. On 3 April 2013, NASA scientists reported that hints of dark matter may have been detected by the "Alpha Magnetic Spectrometer". According to the scientists, "The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays."
The space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, including small invertebrates called tardigrades can survive in this environment in an extremely dry state called desiccation.
Medical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. This data will be used to determine whether lengthy human spaceflight and space colonisation are feasible. As of 2006, data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars.
Medical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.
Microgravity.
The Earth's gravity is only slightly weaker at the altitude of the ISS than at the surface, but objects in orbit are in a continuous state of freefall, resulting in an apparent state of weightlessness. This perceived weightlessness is disturbed by five separate effects:
Researchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.
The investigation of the physics of fluids in microgravity will allow researchers to model the behaviour of fluids better. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, an examination of reactions that are slowed by low gravity and temperatures will give scientists a deeper understanding of superconductivity.
The study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve current knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the universe.
Exploration.
The ISS provides a location in the relative safety of Low Earth Orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance as well as repair and replacement activities on-orbit, which will be essential skills in operating spacecraft farther from Earth, mission risks can be reduced and the capabilities of interplanetary spacecraft advanced. Referring to the MARS-500 experiment, ESA states that "Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a "shorter version" of MARS-500 may be carried out on the ISS. 
In 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, "When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars." A manned mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other 4 partners that China, India and South Korea be invited to join the ISS partnership. NASA chief Charlie Bolden stated in Feb 2011 "Any mission to Mars is likely to be a global effort". Currently, American legislation prevents NASA co-operation with China on space projects.
Education and cultural outreach.
The ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, videolink and email. ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. In one lesson, students can navigate a 3-D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time.
JAXA aims both to "Stimulate the curiosity of children, cultivating their spirits, and encouraging their passion to pursue craftsmanship", and to "Heighten the child's awareness of the importance of life and their responsibilities in society." Through a series of education guides, a deeper understanding of the past and near-term future of manned space flight, as well as that of Earth and life, will be learned. In the JAXA Seeds in Space experiments, the mutation effects of spaceflight on plant seeds aboard the ISS is explored. Students grow sunflower seeds which flew on the ISS for about nine months as a start to 'touch the Universe'. In the first phase of Kibō utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields.
Cultural activities are another major objective. Tetsuo Tanaka, director of JAXA's Space Environment and Utilization Center, says "There is something about space that touches even people who are not interested in science."
Amateur Radio on the ISS (ARISS) is a volunteer programme which encourages students worldwide to pursue careers in science, technology, engineering and mathematics through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from 9 countries including several countries in Europe as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the station. 
"First Orbit" is a feature-length documentary film about Vostok 1, the first manned space flight around the Earth. By matching the orbit of the International Space Station to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli, during Expedition 26/27, filmed the majority of the footage for this documentary film, and as a result is credited as its director of photography. The film was streamed through the website firstorbit.org in a global YouTube premiere in 2011, under a free license.
In May 2013, commander Chris Hadfield shot a music video of David Bowie's "Space Oddity" on board the station; the film was released freely on YouTube. It was the first music video ever to be filmed in space.
Assembly.
The assembly of the International Space Station, a major endeavour in space architecture, began in November 1998. Russian modules launched and docked robotically, with the exception of "Rassvet". All other modules were delivered by the Space Shuttle, which required installation by ISS and shuttle crewmembers using the SSRMS and EVAs; as of 5 2011[ [update]], they had added 159 components during more than 1,000 hours of EVA. 127 of these spacewalks originated from the station, and the remaining 32 were launched from the airlocks of docked Space Shuttles. The beta angle of the station had to be considered at all times during construction, as the station's beta angle is directly related to the percentage of its orbit that the station (as well as any docked or docking spacecraft) is exposed to the sun; the Space Shuttle would not perform optimally above a limit called the "beta cutoff".
The first module of the ISS, "Zarya", was launched on 20 November 1998 on an autonomous Russian Proton rocket. It provided propulsion, attitude control, communications, electrical power, but lacked long-term life support functions. Two weeks later a passive NASA module "Unity" was launched aboard Space Shuttle flight STS-88 and attached to Zarya by astronauts during EVAs. This module has two Pressurized Mating Adapters (PMAs), one connects permanently to Zarya, the other allows the Space Shuttle to dock to the space station. At this time, the Russian station Mir was still inhabited. The ISS remained unmanned for two years, during which time Mir was de-orbited. On 12 July 2000 Zvezda was launched into orbit. Preprogrammed commands on board deployed its solar arrays and communications antenna. It then became the passive vehicle for a rendezvous with the Zarya and Unity. As a passive "target" vehicle, the Zvezda maintained a stationkeeping orbit as the Zarya-Unity vehicle performed the rendezvous and docking via ground control and the Russian automated rendezvous and docking system. Zarya's computer transferred control of the station to Zvezda's computer soon after docking. Zvezda added sleeping quarters, a toilet, kitchen, CO2 scrubbers, dehumidifier, oxygen generators, exercise equipment, plus data, voice and television communications with mission control. This enabled permanent habitation of the station.
The first resident crew, Expedition 1, arrived in November 2000 on Soyuz TM-31. At the end of the first day on the station, astronaut Bill Shepherd requested the use of the radio call sign ""Alpha", which he and cosmonaut Krikalev preferred to the more cumbersome "International Space Station". The name "Alpha"" had previously been used for the station in the early 1990s, and following the request, its use was authorised for the whole of Expedition 1. Shepherd had been advocating the use of a new name to project managers for some time. Referencing a naval tradition in a pre-launch news conference he had said: "For thousands of years, humans have been going to sea in ships. People have designed and built these vessels, launched them with a good feeling that a name will bring good fortune to the crew and success to their voyage." Yuri Semenov, the President of Russian Space Corporation Energia at the time, disapproved of the name ""Alpha"; he felt that "Mir" was the first space station, and so he would have preferred the names "Beta" or "Mir 2"" for the ISS.
Expedition 1 arrived midway between the flights of STS-92 and STS-97. These two Space Shuttle flights each added segments of the station's Integrated Truss Structure, which provided the station with Ku-band communication for US television, additional attitude support needed for the additional mass of the USOS, and substantial solar arrays supplementing the station's existing 4 solar arrays.
Over the next two years the station continued to expand. A Soyuz-U rocket delivered the "Pirs" docking compartment. The Space Shuttles "Discovery", "Atlantis", and "Endeavour" delivered the "Destiny" laboratory and "Quest" airlock, in addition to the station's main robot arm, the "Canadarm2", and several more segments of the Integrated Truss Structure.
The expansion schedule was interrupted by the Space Shuttle "Columbia" disaster in 2003, with the resulting two year hiatus in the Space Shuttle programme halting station assembly. The space shuttle was grounded until 2005 with STS-114 flown by "Discovery".
Assembly resumed in 2006 with the arrival of STS-115 with "Atlantis", which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116, STS-117, and STS-118. As a result of the major expansion of the station's power-generating capabilities, more pressurised modules could be accommodated, and the "Harmony" node and "Columbus" European laboratory were added. These were followed shortly after by the first two components of "Kibō". In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of "Kibō" was delivered in July 2009 on STS-127, followed by the Russian "Poisk" module. The third node, "Tranquility", was delivered in February 2010 during STS-130 by the Space Shuttle "Endeavour", alongside the Cupola, closely followed in May 2010 by the penultimate Russian module, "Rassvet". Rassvet was delivered by Space Shuttle "Atlantis" on STS-132 in exchange for the Russian Proton delivery of the Zarya Module in 1998 which had been funded by the United States. The last pressurised module of the USOS, "Leonardo", was brought to the station by "Discovery" on her final flight, STS-133, followed by the Alpha Magnetic Spectrometer on STS-134, delivered by "Endeavour".
s of 2011[ [update]], the station consisted of fifteen pressurised modules and the Integrated Truss Structure. Still to be launched are the Russian Multipurpose Laboratory Module "Nauka" and a number of external components, including the European Robotic Arm. Assembly is expected to be completed by April 2014, by which point the station will have a mass in excess of 400 tonnes (440 short tons).
The gross mass of the station changes over time. The total launch mass of the modules on orbit is about 417289 kg (as of 3 September 2011). The mass of experiments, spare parts, personal effects, crew, foodstuff, clothing, propellants, water supplies, gas supplies, docked spacecraft, and other items add to the total mass of the station. Hydrogen gas is constantly vented overboard by the oxygen generators.
Station structure.
The ISS is a third generation modular space station. Modular stations can allow the mission to be changed over time and new modules can be added or removed from the existing structure, allowing greater flexibility.
Below is a diagram of major station components. The blue areas are pressurised sections accessible by the crew without using spacesuits. The station's unpressurised superstructure is indicated in red. Other unpressurised components are yellow. Note that the Unity node joins directly to the Destiny laboratory. For clarity, they are shown apart.
Pressurised modules.
Zarya.
Zarya (Russian: Заря́; lit. dawn), also known as the Functional Cargo Block or FGB (from the Russian "Функционально-грузовой блок", Funktsionalno-gruzovoy blok or ФГБ), was the first module of the International Space Station to be launched. The FGB provided electrical power, storage, propulsion, and guidance to the ISS during the initial stage of assembly. With the launch and assembly in orbit of other modules with more specialized functionality, Zarya is now primarily used for storage, both inside the pressurized section and in the externally mounted fuel tanks. The Zarya is a descendant of the TKS spacecraft designed for the Soviet Salyut program. The name Zarya was given to the FGB because it signified the dawn of a new era of international cooperation in space. Although it was built by a Russian company, it is owned by the United States. Zarya weighs 19300 kg, is 12.55 m long and 4.1 m wide, discounting solar arrays.
Built from December 1994 to January 1998 in Russia at the Khrunichev State Research and Production Space Center (KhSC) in Moscow, Zarya's control system was developed by the Khartron Corp. (Kharkiv, Ukraine).
Zarya was launched on 20 November 1998, on a Russian Proton rocket from Baikonur Cosmodrome Site 81 in Kazakhstan to a 400 km high orbit with a designed lifetime of at least 15 years. After Zarya reached orbit, STS-88 launched on 4 December 1998, to attach the Unity Module.
Although only designed to fly autonomously for six to eight months, Zarya did so for almost two years due to delays with the Russian Service Module, Zvezda, which finally launched on 12 July 2000, and docked with Zarya on 26 July using the Russian Kurs docking system.
Unity.
Unity, or Node 1, is one of three nodes, or passive connecting modules, in the US Orbital Segment of the station. It was the first US-built component of the Station to be launched. Cylindrical in shape, with six berthing locations facilitating connections to other modules, Unity was carried into orbit by Space Shuttle "Endeavour" as the primary cargo of STS-88 in 1998. Essential space station resources such as fluids, environmental control and life support systems, electrical and data systems are routed through Unity to supply work and living areas of the station. More than 50,000 mechanical items, 216 lines to carry fluids and gases, and 121 internal and external electrical cables using six miles of wire were installed in the Unity node. Unity is made of aluminum. Prior to its launch aboard Endeavour, conical Pressurized Mating Adapters (PMAs) were attached to the aft and forward berthing mechanisms of Unity. Unity and the two mating adapters together weighed about 25600 lb. The adapters allow the docking systems used by the Space Shuttle and by Russian modules to attach to the node's hatches and berthing mechanisms.
Unity was carried into orbit as the primary cargo of the Space Shuttle Endeavour on STS-88, the first Space Shuttle mission dedicated to assembly of the station. On 6 December 1998, the STS-88 crew mated the aft berthing port of Unity with the forward hatch of the already orbiting Zarya module.
Zvezda.
Zvezda (Russian: Звезда́, meaning "star"), also known as DOS-8, Service Module or SM (Russian: СМ). It provides all of the station's critical systems, its addition rendered the station permanently habitable for the first time, adding life support for up to six crew and living quarters for two. Zvezda's DMS-R computer handles guidance, navigation and control for the entire space station. A second computer which performs the same functions will be installed in the Nauka module, FGB-2.
The hull of Zvezda was completed in February 1985, with major internal equipment installed by October 1986. The module was launched by a Proton-K rocket from Site 81/23 at Baikonur, on 12 July 2000. Zvezda is at the rear of the station according to its normal direction of travel and orientation, its engines are used to boost the station's orbit. Alternatively Russian and European spacecraft can dock to Zvezda's aft port and use their engines to boost the station.
Destiny.
Destiny is the primary research facility for United States payloads aboard the ISS. In 2011, NASA solicited proposals for a not-for-profit group to manage all American science on the station which does not relate to manned exploration. The module houses 24 International Standard Payload Racks, some of which are used for environmental systems and crew daily living equipment. "Destiny" also serves as the mounting point for the station's Truss Structure.
Quest.
Quest is the only USOS airlock, and hosts spacewalks with both United States EMU and Russian Orlan spacesuits. It consists of two segments: the equipment lock, which stores spacesuits and equipment, and the crew lock, from which astronauts can exit into space. This module has a separately controlled atmosphere. Crew sleep in this module, breathing a low nitrogen mixture the night before scheduled EVAs, to avoid decompression sickness (known as "the bends") in the low-pressure suits.
Pirs and Poisk.
Pirs (Russian: Пирс, meaning "pier"), (Russian: Стыковочный отсек), "docking module", SO-1 or DC-1 (docking compartment), and Poisk (Russian: По́иск; lit. "Search"), also known as the Mini-Research Module 2 (MRM 2), "Малый исследовательский модуль 2", or МИМ 2. Pirs and Poisk are Russian airlock modules. Each of these modules have 2 identical hatches. An outward opening hatch on the MIR space station failed after it swung open too fast after unlatching, due to a small amount of air pressure remaining in the airlock. A different entry was used, and the hatch repaired. All EVA hatches on the ISS open inwards and are pressure sealing. Pirs was used to store, service, and refurbish Russian Orlan suits and provided contingency entry for crew using the slightly bulkier American suits. The outermost docking ports on both airlocks allow docking of Soyuz and Progress spacecraft, and the automatic transfer of propellants to and from storage on the ROS.
Harmony.
Harmony is the second of the station's node modules and the utility hub of the USOS. The module contains four racks that provide electrical power, bus electronic data, and acts as a central connecting point for several other components via its six Common Berthing Mechanisms (CBMs). The European Columbus and Japanese Kibō laboratories are permanently berthed to the starboard and port radial ports respectively. The nadir and zenith ports can be used for docking visiting spacecraft including HTV, Dragon, and Cygnus, with the nadir port serving as the primary docking port. American Shuttle Orbiters docked with the ISS via PMA-2, attached to the forward port.
Tranquility.
Tranquility is the third and last of the station's US nodes, it contains an additional life support system to recycle waste water for crew use and supplements oxygen generation. Three of the four berthing locations are not used. One location has the cupola installed, and one has the docking port adapter installed.
Columbus.
Columbus, the primary research facility for European payloads aboard the ISS, provides a generic laboratory as well as facilities specifically designed for biology, biomedical research and fluid physics. Several mounting locations are affixed to the exterior of the module, which provide power and data to external experiments such as the European Technology Exposure Facility (EuTEF), Solar Monitoring Observatory, Materials International Space Station Experiment, and Atomic Clock Ensemble in Space. A number of expansions are planned for the module to study quantum physics and cosmology. ESA's development of technologies on all the main areas of life support has been ongoing for more than 20 years and are/have been used in modules such as Columbus and the ATV. The German Aerospace Center DLR manages ground control operations for Columbus and the ATV is controlled from the French CNES Toulouse Space Center.
Kibō.
Kibō (Japanese: きぼう, "hope") is the largest single ISS module. This laboratory is used to carry out research in space medicine, biology, Earth observations, materials production, biotechnology, communications research, and has facilities for growing plants and fish. During August 2011, an observatory mounted on Kibō, which utilises the ISS's orbital motion to image the whole sky in the X-ray spectrum, detected for the first time the moment a star was swallowed by a black hole. The laboratory contains a total of 23 racks, including 10 experiment racks and has a dedicated airlock for experiments. In a 'shirt sleeves' environment, crew attach an experiment to the sliding drawer within the airlock, close the inner, and then open the outer hatch. By extending the drawer and removing the experiment using the dedicated robotic arm, payloads are placed on the external platform. The process can be reversed and repeated quickly, allowing access to maintain external experiments without the delays caused by EVAs.
A smaller pressurised module is attached to the top of Kibō, serving as a cargo bay. The dedicated Interorbital communications system allows large amounts of data to be beamed from Kibō's ICS, first to the Japanese KODAMA satellite in geostationary orbit, then to Japanese ground stations. When a direct communication link is used, contact time between the ISS and a ground station is limited to approximately 10 minutes per visible pass. When KODAMA relays data between a LEO spacecraft and a ground station, real-time communications are possible in 60% of the flight path of the spacecraft. Ground staff use telepresence robotics to conduct on-orbit research without crew intervention.
Cupola.
Cupola is a seven window observatory, used to view Earth and docking spacecraft. Its name derives from the Italian word cupola, which means "dome". The Cupola project was started by NASA and Boeing, but cancelled due to budget cuts. A barter agreement between NASA and the ESA resulted in the Cupola's development being resumed in 1998 by the ESA. It was built by Thales Alenia Space in Torino, Italy. The module comes equipped with robotic workstations for operating the station's main robotic arm and shutters to protect its windows from damage caused by micrometeorites. It features 7 windows, with a 80 cm round window, the largest window on the station (and the largest flown in space to date). The distinctive design has been compared to the 'turret' of the fictitious "Millennium Falcon" from the motion picture "Star Wars"; the original prop lightsaber used by actor Mark Hamill as Luke Skywalker in the 1977 film was flown to the station in 2007.
Rassvet.
Rassvet (Russian: Рассве́т; lit. "dawn"), also known as the Mini-Research Module 1 (MRM-1) (Russian: Ма́лый иссле́довательский модуль, МИМ 1) and formerly known as the Docking Cargo Module (DCM), is similar in design to the Mir Docking Module launched on STS-74 in 1995. "Rassvet" is primarily used for cargo storage and as a docking port for visiting spacecraft. It was flown to the ISS aboard NASA's Space Shuttle "Atlantis" on the STS-132 mission and connected in May 2010, Rassvet is the only Russian owned module launched by NASA, to repay for the launch of Zarya, which is Russian designed and built, but partially paid for by NASA. Rassvet was launched with the Russian Nauka Laboratory's Experiments airlock temporarily attached to it, and spare parts for the European Robotic Arm.
Leonardo.
"Leonardo" Permanent Multipurpose Module (PMM) is a storage module attached to the Unity node. The three NASA Space Shuttle MPLM cargo containers—Leonardo, Raffaello and Donatello—were built for NASA in Turin, Italy by Alcatel Alenia Space, now Thales Alenia Space. The MPLMs were provided to NASA's ISS programme by Italy (independent of their role as a member state of ESA) and are considered to be US elements. In a bartered exchange for providing these containers, the US gave Italy research time aboard the ISS out of the US allotment in addition to that which Italy receives as a member of ESA. The Permanent Multipurpose Module was created by converting Leonardo into a module that could be permanently attached to the station.
Scheduled additional modules.
Nauka.
Nauka (Russian: Нау́ка; lit. "science"), also known as the Multipurpose Laboratory Module (MLM) or FGB-2 (Russian: Многофункциональный лабораторный модуль, МЛМ), is the major Russian laboratory module. It was scheduled to arrive at the station in 2014, docking to the port that was occupied by the Pirs module. The date has been postponed to February 2017. Prior to the arrival of the Nauka module, a Progress spacecraft was used to remove Pirs from the station, deorbiting it to reenter over the Pacific Ocean. Nauka contains an additional set of life support systems and attitude control. Originally it would have routed power from the single Science-and-Power Platform, but that single module design changed over the first ten years of the ISS mission, and the two science modules, which attach to Nauka via the Uzlovoy Module, or Russian node, each incorporate their own large solar arrays to power Russian science experiments in the ROS.
Nauka's mission has changed over time. During the mid-1990s, it was intended as a backup for the FGB, and later as a universal docking module (UDM); its docking ports will be able to support automatic docking of both spacecraft, additional modules and fuel transfer. Nauka has its own engines. Smaller Russian modules such as Pirs and Poisk were delivered by modified Progress spacecraft, and the larger modules; Zvezda, Zarya, and Nauka, were launched by Proton rockets. Russia plans to separate Nauka, along with the rest of the Russian Orbital Segment, before the ISS is deorbited, to form the OPSEK space station.
Uzlovoy Module.
The Uzlovoy Module (UM), or Node Module is a 4 metric ton ball shaped module will support the docking of two scientific and power modules during the final stage of the station assembly and provide the Russian segment additional docking ports to receive Soyuz TMA and Progress M spacecraft. UM is to be incorporated into the ISS in 2016. It will be integrated with a special version of the Progress cargo ship and launched by a standard Soyuz rocket. The Progress would use its own propulsion and flight control system to deliver and dock the Node Module to the nadir (Earth-facing) docking port of the Nauka MLM/FGB-2 module. One port is equipped with an active hybrid docking port, which enables docking with the MLM module. The remaining five ports are passive hybrids, enabling docking of Soyuz and Progress vehicles, as well as heavier modules and future spacecraft with modified docking systems. The node module was conceived to serve as the only permanent element of the future Russian successor to the ISS, OPSEK. Equipped with six docking ports, the Node Module would serve as a single permanent core of the future station with all other modules coming and going as their life span and mission required. This would be a progression beyond the ISS and Russia's modular MIR space station, which are in turn more advanced than early monolithic first generation stations such as Skylab, and early Salyut and Almaz stations.
Science Power Modules 1 & 2 (NEM-1, NEM-2) (Russian: Нау́чно-Энергетический Модуль-1 и -2)
Bigelow Expandable Activity Module.
On 16 January 2013, Bigelow Aerospace was contracted by NASA to provide a Bigelow Expandable Activity Module (BEAM), scheduled to arrive at the space station in 2015 for a two-year technology demonstration. BEAM is an inflatable module that will be attached to the aft hatch of the port-side Tranquility module of the International Space Station. During its two-year test run, instruments will measure its structural integrity and leak rate, along with temperature and radiation levels. The hatch leading into the module will remain mostly closed except for periodic visits by space station crew members for inspections and data collection. Following the test run, the module will be detached and jettisoned from the station.
Cancelled components.
Several modules planned for the station have been cancelled over the course of the ISS programme, whether for budgetary reasons, because the modules became unnecessary, or following a redesign of the station after the 2003 "Columbia" disaster. The US Centrifuge Accommodations Module was intended to host science experiments in varying levels of artificial gravity. The US Habitation Module would have served as the station's living quarters. Instead, the sleep stations are now spread throughout the station. The US Interim Control Module and ISS Propulsion Module were intended to replace functions of "Zvezda" in case of a launch failure. The Russian Universal Docking Module, to which the cancelled Russian Research modules and spacecraft would have docked. The Russian Science Power Platform would have provided the Russian Orbital Segment with a power supply independent of the ITS solar arrays, and two Russian Research Modules that were planned to be used for scientific research.
Unpressurised elements.
The ISS features a large number of external components that do not require pressurisation. The largest such component is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. The ITS consists of ten separate segments forming a structure 108.5 m (356 ft) long.
The station in its complete form has several smaller external components, such as the six robotic arms, the three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). Whilst these platforms allow experiments (including MISSE, the STP-H3 and the Robotic Refueling Mission) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, the platforms' primary function is to store Orbital Replacement Units (ORUs). ORUs are spare parts that can be replaced when the item either passes its design life or fails. Examples of ORUs include pumps, storage tanks, antennas and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. Spare parts were routinely transported to and from the station via Space Shuttle resupply missions, with a heavy emphasis on ORU transport once the NASA Shuttle approached retirement. Several shuttle missions were dedicated to the delivery of ORUs, including STS-129, STS-133 and STS-134. , only one other mode of transportation of ORUs had been utilised – the Japanese cargo vessel HTV-2 – which delivered an FHRC and CTC-2 via its Exposed Pallet (EP).
There are also smaller exposure facilities mounted directly to laboratory modules; the JEM Exposed Facility serves as an external 'porch' for the Japanese Experiment Module complex, and a facility on the European "Columbus" laboratory provides power and data connections for experiments such as the European Technology Exposure Facility and the Atomic Clock Ensemble in Space. A remote sensing instrument, SAGE III-ISS, is due to be delivered to the station in 2014 aboard a Dragon capsule, and the NICER experiment in 2016. The largest such scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter.
Cranes and robotic arms.
Canadarm2, the largest robotic arm on the ISS, has a mass of 1,800 kilograms and is used to dock and manipulate spacecraft and modules on the USOS, and hold crew members and equipment during EVAs. The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically, and may be discarded the same way. Crew use the 2 "Strela" (Russian: Стрела́; lit. Arrow) cargo cranes during EVAs for moving crew and equipment around the ROS. Each Strela crane has a mass of 45 kg.
The Integrated Truss Structure serves as a base for the main remote manipulator system called the Mobile Servicing System (MSS). This consists of the Mobile Base System (MBS), the Canadarm2, and Dextre. Dextre is a 1,500 kg agile robotic manipulator with two 'arms' which have 7 degrees of movement each, a 'torso' which bends at the waist and rotates at the base, a tool holster, lights and video. Staff on Earth can operate Dextre via remote control, performing work without crew intervention. The MBS rolls along rails built into some of the ITS segments to allow the arm to reach all parts of the United States segment of the station. The MSS had its reach increased with an Orbiter Boom Sensor System in May 2011, used to inspect tiles on the NASA shuttle, and converted for permanent station use. To gain access to the extreme extents of the Russian Segment the crew also placed a "Power Data Grapple Fixture" to the forward docking section of Zarya, so that the Canadarm2 may inchworm itself onto that point.
The European Robotic Arm, which will service the Russian Orbital Segment, will be launched alongside the Multipurpose Laboratory Module in 2017. The Japanese Experiment Module's Remote Manipulator System (JFM RMS), which services the JEM Exposed Facility, was launched on STS-124 and is attached to the JEM Pressurised Module.
Comparison.
The ISS follows Salyut and Almaz series, Cosmos 557, Skylab, and Mir as the 11th space station launched, as the Genesis prototypes were never intended to be manned. Other examples of modular station projects include the Soviet/Russian Mir and the planned Russian OPSEK and Chinese space station. The first space station, Salyut 1, and other one-piece or 'monolithic' first generation space stations, such as Salyut 2,3,4,5, DOS 2, Kosmos 557, Almaz and NASA's Skylab stations were not designed for re-supply. Generally, each crew had to depart the station to free the only docking port for the next crew to arrive, Skylab had more than one docking port but was not designed for resupply. Salyut 6 and 7 had more than one docking port and were designed to be resupplied routinely during crewed operation.
Station systems.
Life support.
The critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life support systems are contained in the Service Module Zvezda. Some of these systems are supplemented by equipment in the USOS. The MLM Nauka laboratory has a complete set of life support systems.
Atmospheric control systems.
The atmosphere on board the ISS is similar to the Earth's. Normal air pressure on the ISS is 101.3 kPa (14.7 psi); the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than the alternative, a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft.
The "Elektron" system aboard "Zvezda" and a similar system in "Destiny" generate oxygen aboard the station. The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. Carbon dioxide is removed from the air by the Vozdukh system in "Zvezda". Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters.
Part of the ROS atmosphere control system is the oxygen supply, triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The Elektron unit is the primary oxygen supply, O2 and H2 are produced by electrolysis, with the H2 being vented overboard. The 1 kW system uses approximately 1 litre of water per crew member per day from stored water from Earth, or water recycled from other systems. MIR was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning O2-producing Vika cartridges (see also ISS ECLSS). Each 'candle' takes 5–20 minutes to decompose at 450–500 °C, producing 600 litres of O2. This unit is manually operated.
The US Orbital Segment has redundant supplies of oxygen, from a pressurised storage tank on the Quest airlock module delivered in 2001, supplemented ten years later by ESA built Advanced Closed-Loop System (ACLS) in the Tranquility module (Node 3), which produces O2 by electrolysis. Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane.
Power and thermal control.
Double-sided solar, or Photovoltaic arrays, provide electrical power for the ISS. These bifacial cells are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth, by collecting sunlight on one side and light reflected off the Earth on the other. 
The Russian segment of the station, like the Space Shuttle and most spacecraft, uses 28 volt DC from four rotating solar arrays mounted on "Zarya" and "Zvezda". The USOS uses 130–180 V DC from the USOS PV array, power is stabilised and distributed at 160 V DC and converted to the user-required 124 V DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The ROS uses low voltage. The two station segments share power with converters.
The USOS solar arrays are arranged as four wing pairs, with each wing producing nearly 32.8 kW. These arrays normally track the sun to maximise power generation. Each array is about 375 m2 (450 yd2) in area and 58 m long. In the complete configuration, the solar arrays track the sun by rotating the "alpha gimbal" once per orbit; the "beta gimbal" follows slower changes in the angle of the sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude.
The station uses rechargeable nickel-hydrogen batteries (NiH2) for continuous power during the 35 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the Earth. They have a 6.5-year lifetime (over 37,000 charge/discharge cycles) and will be regularly replaced over the anticipated 20-year life of the station.
The station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units (PCU)s create current paths between the station and the ambient plasma field.
The large amount of electrical power consumed by the station's systems and experiments is turned almost entirely into heat. The heat which can be dissipated through the walls of the stations modules is insufficient to keep the internal ambient temperature within comfortable, workable limits. Ammonia is continuously pumped through pipework throughout the station to collect heat, then into external radiators exposed to the cold of space, and back into the station.
The International Space Station (ISS) External Active Thermal Control System (EATCS) maintains an equilibrium when the ISS environment or heat loads exceed the capabilities of the Passive Thermal Control System (PTCS). Note Elements of the PTCS are external surface materials, insulation such as MLI, or Heat Pipes. The EATCS provides heat rejection capabilities for all the US pressurised modules, including the JEM and COF as well as the main power distribution electronics of the S0, S1 and P1 Trusses. The EATCS consists an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop capable of withstanding the much lower temperature of space, which is then circulated through radiators to remove the heat. The EATCS is capable of rejecting up to 70 kW, and provides a substantial upgrade in heat rejection capacity from the 14 kW capability of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss.
Communications and computers.
Radio communications provide telemetry and scientific data links between the station and Mission Control Centres. Radio links are also used during rendezvous and docking procedures and for audio and video communication between crewmembers, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes.
The Russian Orbital Segment communicates directly with the ground via the "Lira" antenna mounted to "Zvezda". The "Lira" antenna also has the capability to use the "Luch" data relay satellite system. This system, used for communications with "Mir", fell into disrepair during the 1990s, and as a result is no longer in use, although two new "Luch" satellites—"Luch"-5A and "Luch"-5B—were launched in 2011 and 2012 respectively to restore the operational capability of the system. Another Russian communications system is the Voskhod-M, which enables internal telephone communications between "Zvezda", "Zarya", "Pirs", "Poisk" and the USOS, and also provides a VHF radio link to ground control centres via antennas on "Zvezda"‍‍ '​‍s exterior.
The US Orbital Segment (USOS) makes use of two separate radio links mounted in the Z1 truss structure: the S band (used for audio) and Ku band (used for audio, video and data) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit, which allows for almost continuous real-time communications with NASA's Mission Control Center (MCC-H) in Houston. Data channels for the Canadarm2, European "Columbus" laboratory and Japanese "Kibō" modules are routed via the S band and Ku band systems, although the European Data Relay System and a similar Japanese system will eventually complement the TDRSS in this role. Communications between modules are carried on an internal digital wireless network.
UHF radio is used by astronauts and cosmonauts conducting EVAs. UHF is employed by other spacecraft that dock to or undock from the station, such as Soyuz, Progress, HTV, ATV and the Space Shuttle (except the shuttle also makes use of the S band and Ku band systems via TDRSS), to receive commands from Mission Control and ISS crewmembers. Automated spacecraft are fitted with their own communications equipment; the ATV uses a laser attached to the spacecraft and equipment attached to "Zvezda", known as the Proximity Communications Equipment, to accurately dock to the station.
The ISS is equipped with approximately 100 IBM and Lenovo ThinkPad model A31 and T61P laptop computers. Each computer is a commercial off-the-shelf purchase which is then modified for safety and operation including updates to connectors, cooling and power to accommodate the station's 28V DC power system and weightless environment. Heat generated by the laptops does not rise, but stagnates surrounding the laptop, so additional forced ventilation is required. Laptops aboard the ISS are connected to the station's wireless LAN via Wi-Fi and to the ground via Ku band. This provides speeds of 10 Mbit/s to and 3 Mbit/s from the station, comparable to home DSL connection speeds.
The operating system used for key station functions is the Debian GNU/Linux distribution. The migration from Microsoft Windows was made in May 2013 for reasons of reliability, stability and flexibility.
Station operations.
Expeditions and private flights.
"See also the list of International Space Station expeditions (professional crew), space tourism (private travellers), and the list of human spaceflights to the ISS (both)."
Each permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo ships and all activities. Expeditions 1 to 6 consisted of 3 person crews, Expeditions 7 to 12 were reduced to the safe minimum of two following the destruction of the NASA Shuttle Columbia. From Expedition 13 the crew gradually increased to 6 around 2010. With the arrival of the American Commercial Crew vehicles in the middle of the 2010s, expedition size may be increased to seven crew members, the number ISS is designed for.
Sergei Krikalev, member of Expedition 1 and Commander of Expedition 11 has spent more time in space than anyone else, a total of 803 days and 9 hours and 39 minutes. His awards include the Order of Lenin, Hero of the Soviet Union, Hero of the Russian Federation, and 4 NASA medals. On 16 August 2005 at 1:44 am EDT he passed the record of 748 days held by Sergei Avdeyev, who had 'time travelled' 1/50th of a second into the future on board MIR. He participated in psychosocial experiment SFINCSS-99 (Simulation of Flight of International Crew on Space Station), which examined inter-cultural and other stress factors affecting integration of crew in preparation for the ISS spaceflights. Commander Michael Fincke has spent a total of 382 days in space – more than any other American astronaut.
Travellers who pay for their own passage into space are termed spaceflight participants by Roskosmos and NASA, and are sometimes informally referred to as space tourists, a term they generally dislike. All seven were transported to the ISS on Russian Soyuz spacecraft. When professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. When the space shuttle retired in 2011, and the station's crew size was reduced to 6, space tourism was halted, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increase after 2013, allowing 5 Soyuz flights (15 seats) with only two expeditions (12 seats) required. The remaining seats are sold for around US$ to members of the public who can pass a medical. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito, the first man to pay for his own passage to the ISS. Toyohiro Akiyama was flown to Mir for a week, he was classed as a business traveller, as his employer, Tokyo Broadcasting System, paid for his ticket, and he gave a daily TV broadcast from orbit.
Anousheh Ansari (Persian: انوشه انصاری‎) became the first Iranian in space and the first self-funded woman to fly to the station. Officials reported that her education and experience make her much more than a tourist, and her performance in training had been "excellent." Ansari herself dismisses the idea that she is a tourist. She did Russian and European studies involving medicine and microbiology during her 10-day stay. The documentary "Space Tourists" follows her journey to the station, where she fulfilled "an age-old dream of man: to leave our planet as a «normal person» and travel into outer space." In the film, some Kazakhs are shown waiting in the middle of the steppes for four rocket stages to literally fall from the sky. Film-maker Christian Frei states "Filming the work of the Kazakh scrap metal collectors was anything but easy. The Russian authorities finally gave us a film permit in principle, but they imposed crippling preconditions on our activities. The real daily routine of the scrap metal collectors could definitely not be shown. Secret service agents and military personnel dressed in overalls and helmets were willing to "re-enact" their work for the cameras – in an idealised way that officials in Moscow deemed to be presentable, but not at all how it takes place in reality."
Spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. This is currently the only non-terrestrial geocache in existence.
Orbit.
The ISS is maintained in a nearly circular orbit with a minimum mean altitude of 330 km (205 mi) and a maximum of 410 km (255 mi), in the centre of the thermosphere, at an inclination of 51.6 degrees to Earth's equator, necessary to ensure that Russian Soyuz and Progress spacecraft launched from the Baikonur Cosmodrome may be safely launched to reach the station. Spent rocket stages must be dropped into uninhabited areas and this limits the directions rockets can be launched from the spaceport. The orbital inclination chosen was also low enough to allow American space shuttles launched from Florida to reach the ISS.
It travels at an average speed of 27,724 kilometres (17,227 mi) per hour, and completes 15.54 orbits per day (93 minutes per orbit). The station's altitude was allowed to fall around the time of each NASA shuttle mission. Orbital boost burns would generally be delayed until after the shuttle's departure. This allowed shuttle payloads to be lifted with the station's engines during the routine firings, rather than have the shuttle lift itself and the payload together to a higher orbit. This trade-off allowed heavier loads to be transferred to the station. After the retirement of the NASA shuttle, the nominal orbit of the space station was raised in altitude. Other, more frequent supply ships do not require this adjustment as they are substantially lighter vehicles.
Orbital boosting can be performed by the station's two main engines on the "Zvezda" service module, or Russian or European spacecraft docked to Zvezda's aft port. The ATV has been designed with the possibility of adding a second docking port to its other end, allowing it to remain at the ISS and still allow other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. In December 2008 NASA signed an agreement with the Ad Astra Rocket Company which may result in the testing on the ISS of a VASIMR plasma propulsion engine. This technology could allow station-keeping to be done more economically than at present.
The Russian Orbital Segment contains the station's engines and control bridge, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. Initially, Zarya, the first module of the station, controlled the station until a short time after the Russian service module Zvezda docked and was transferred control. Zvezda contains the ESA built DMS-R Data Management System. Using two fault-tolerant computers (FTC), Zvezda computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting. Zvezda uses gyroscopes and thrusters to turn itself around. Gyroscopes do not require propellant, rather they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer controlled gyroscopes to handle the extra mass of that section. When gyroscopes 'saturate', reaching their maximum speed, thrusters are used to cancel out the stored momentum. During Expedition 10, an incorrect command was sent to the station's computer, using about 14 kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, it can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which has no thrusters. When an ATV, NASA Shuttle, or Soyuz is docked to the station, it can also be used to maintain station attitude such as for troubleshooting. Shuttle control was used exclusively during installation of the S3/S4 truss, which provides electrical power and data interfaces for the station's electronics.
Mission controls.
The components of the ISS are operated and monitored by their respective space agencies at mission control centres across the globe, including:
Space centres involved with the ISS programme
Repairs.
Orbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Many are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms which also hold science experiments. Both kinds of pallets have electricity as many parts which could be damaged by the cold of space require heating. The larger logistics carriers also have computer local area network connections (LAN) and telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload.
Unexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons, had these problems not been resolved. During STS-120 in 2007, following the relocation of the P6 truss and solar arrays, it was noted during the redeployment of the array that it had become torn and was not deploying properly. An EVA was carried out by Scott Parazynski, assisted by Douglas Wheelock. The men took extra precautions to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic race ring at the heart of the joint, and so the joint was locked to prevent further damage. Repairs to the joint were carried out during STS-126 with lubrication of both joints and the replacement of 11 out of 12 trundle bearings on the joint.
2009 saw damage to the S1 radiator, one of the components of the station's cooling system. The problem was first noticed in Soyuz imagery in September 2008, but was not thought to be serious. The imagery showed that the surface of one sub-panel has peeled back from the underlying central structure, possibly due to micro-meteoroid or debris impact. It is also known that a Service Module thruster cover, jettisoned during an EVA in 2008, had struck the S1 radiator, but its effect, if any, has not been determined. On 15 May 2009 the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was used immediately afterwards to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak from the cooling system via the damaged panel.
Early on 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down.
Planned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed due to an ammonia leak in one of four quick-disconnects. A second EVA on 11 August successfully removed the failed pump module. A third EVA was required to restore Loop A to normal functionality.
The USOS's cooling system is largely built by the American company Boeing, which is also the manufacturer of the failed pump.
An air leak from the USOS in 2004, the venting of fumes from an "Elektron" oxygen generator in 2006, and the failure of the computers in the ROS in 2007 during STS-117 left the station without thruster, "Elektron", "Vozdukh" and other environmental control system operations, the root cause of which was found to be condensation inside the electrical connectors leading to a short-circuit.
The four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. In late 2011 MBSU-1, while still routing power correctly, ceased responding to commands or sending data confirming its health, and was scheduled to be swapped out at the next available EVA. In each MBSU, two power channels feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. A spare MBSU was already on board, but 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before electrical connection was secured. The loss of MBSU-1 limits the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem can be addressed.
On 5 September 2012, in a second, 6 hr, EVA to replace MBSU-1, astronauts Sunita Williams and Akihiko Hoshide successfully restored the ISS to 100% power.
On 24 December 2013, astronauts made a rare Christmas Eve space walk, installing a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a "mini blizzard" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history.
Fleet operations.
Progress M-25M (ISS-57P) was the 58th progress spacecraft to arrive at the ISS, including M-MIM2 and M-SO1 which installed modules. 35 flights of the retired NASA Space Shuttle were made to the station. TMA-15M is the 41st Soyuz flight, and there have been 5 European ATV, 4 Japanese Kounotori 'White Stork', 5 SpaceX Dragon and 3 OSC Cygnus arrivals.
Currently docked/berthed.
"See also the list of professional crew, private travellers, both or just unmanned spaceflights."
  Uncrewed cargoships are in light blue
  Crewed spacecraft are in light green
Scheduled launches and dockings/berthings.
  Uncrewed cargoships are in light blue colour
  Crewed spacecraft are in light green colour
  Modules are in wheat colour
Docking.
All Russian spacecraft and self-propelled modules are able to rendezvous and dock to the space station without human intervention using the Kurs docking system. Radar allows these vehicles to detect and intercept ISS from over 200 kilometres away. The European ATV uses star sensors and GPS to determine its intercept course. When it catches up it then uses laser equipment to optically recognise Zvezda, along with the Kurs system for redundancy. Crew supervise these craft, but do not intervene except to send abort commands in emergencies. The Japanese H-II Transfer Vehicle parks itself in progressively closer orbits to the station, and then awaits 'approach' commands from the crew, until it is close enough for a robotic arm to grapple and berth the vehicle to the USOS. The American Space Shuttle was manually docked, and on missions with a cargo container, the container would be berthed to the Station with the use of manual robotic arms. Berthed craft can transfer International Standard Payload Racks. Japanese spacecraft berth for one to two months. Russian and European Supply craft can remain at the ISS for six months, allowing great flexibility in crew time for loading and unloading of supplies and trash. NASA Shuttles could remain docked for 11–12 days.
The American manual approach to docking allows greater initial flexibility and less complexity. The downside to this mode of operation is that each mission becomes unique and requires specialised training and planning, making the process more labour-intensive and expensive. The Russians pursued an automated methodology that used the crew in override or monitoring roles. Although the initial development costs were high, the system has become very reliable with standardisations that provide significant cost benefits in repetitive routine operations. An automated approach could allow assembly of modules orbiting other worlds prior to crew arrival.
Soyuz spacecraft used for crew rotation also serve as lifeboats for emergency evacuation; they are replaced every six months and have been used once to remove excess crew after the Columbia disaster. Expeditions require, on average, 2 722 kg of supplies, and as of 2011[ [update]], crews had consumed a total of around 22 000 meals. Soyuz crew rotation flights and Progress resupply flights visit the station on average two and three times respectively each year, with the ATV and HTV planned to visit annually from 2010 onwards. Following retirement of the NASA Shuttle Cygnus and Dragon were contracted to fly cargo to the station.
From 26 February 2011 to 7 March 2011 four of the governmental partners (United States, ESA, Japan and Russia) had their spacecraft (NASA Shuttle, ATV, HTV, Progress and Soyuz) docked at the ISS, the only time this has happened to date. On 25 May 2012, SpaceX became the world's first privately held company to send cargo, via the Dragon spacecraft, to the International Space Station.
Launch and docking windows.
Prior to a ship's docking to the ISS, navigation and attitude control (GNC) is handed over to the ground control of the ships' country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming ships, so residue from its thrusters does not damage the cells. When a NASA shuttle docked to the station, other ships were grounded, as the carbon wingtips, cameras, windows, and instruments aboard the shuttle were at too much risk from damage from thruster residue from other ships movements.
Approximately 30% of NASA shuttle launch delays were caused by poor weather. Occasional priority was given to the Soyuz arrivals at the station where the Soyuz carried crew with time-critical cargoes such as biological experiment materials, also causing shuttle delays. Departure of the NASA shuttle was often delayed or prioritised according to weather over its two landing sites. Whilst the Soyuz is capable of landing anywhere, anytime, its planned landing time and place is chosen to give consideration to helicopter pilots and ground recovery crew, to give acceptable flying weather and lighting conditions. Soyuz launches occur in adverse weather conditions, but the cosmodrome has been shut down on occasions when buried by snow drifts up to 6 metres in depth, hampering ground operations.
Life aboard.
Crew activities.
A typical day for the crew begins with a wake-up at 06:00, followed by post-sleep activities and a morning inspection of the station. The crew then eats breakfast and takes part in a daily planning conference with Mission Control before starting work at around 08:10. The first scheduled exercise of the day follows, after which the crew continues work until 13:05. Following a one-hour lunch break, the afternoon consists of more exercise and work before the crew carries out its pre-sleep activities beginning at 19:30, including dinner and a crew conference. The scheduled sleep period begins at 21:30. In general, the crew works ten hours per day on a weekday, and five hours on Saturdays, with the rest of the time their own for relaxation or work catch-up.
The time zone used on board the ISS is Coordinated Universal Time (UTC). The windows are covered at night hours to give the impression of darkness because the station experiences 16 sunrises and sunsets a day. During visiting space shuttle missions, the ISS crew will mostly follow the shuttle's Mission Elapsed Time (MET), which is a flexible time zone based on the launch time of the shuttle mission.
The station provides crew quarters for each member of the expedition's crew, with two 'sleep stations' in the "Zvezda" and four more installed in "Harmony". The American quarters are private, approximately person-sized soundproof booths. The Russian crew quarters include a small window, but do not provide the same amount of ventilation or block the same amount of noise as their American counterparts. A crewmember can sleep in a crew quarter in a tethered sleeping bag, listen to music, use a laptop, and store personal items in a large drawer or in nets attached to the module's walls. The module also provides a reading lamp, a shelf and a desktop. Visiting crews have no allocated sleep module, and attach a sleeping bag to an available space on a wall—it is possible to sleep floating freely through the station, but this is generally avoided because of the possibility of bumping into sensitive equipment. It is important that crew accommodations be well ventilated; otherwise, astronauts can wake up oxygen-deprived and gasping for air, because a bubble of their own exhaled carbon dioxide has formed around their heads.
Food.
Most of the food on board is vacuum sealed in plastic bags. Cans are heavy and expensive to transport, so there are not as many. The preserved food is generally not held in high regard by the crew, and when combined with the reduced sense of taste in a microgravity environment, a great deal of effort is made to make the food more palatable. More spices are used than in regular cooking, and the crew looks forward to the arrival of any ships from Earth, as they bring fresh fruit and vegetables with them. Care is taken that foods do not create crumbs. Sauces are often used to ensure station equipment is not contaminated. Each crew member has individual food packages and cooks them using the on-board galley. The galley features two food warmers, a refrigerator added in November 2008, and a water dispenser that provides both heated and unheated water. Drinks are provided in dehydrated powder form and are mixed with water before consumption. Drinks and soups are sipped from plastic bags with straws; solid food is eaten with a knife and fork, which are attached to a tray with magnets to prevent them from floating away. Any food that floats away, including crumbs, must be collected to prevent it from clogging up the station's air filters and other equipment.
Hygiene.
Showers on space stations were introduced in the early 1970s on Skylab and Salyut 3.:139 By Salyut 6, in the early 1980s, the crew complained of the complexity of showering in space, which was a monthly activity. The ISS does not feature a shower; instead, crewmembers wash using a water jet and wet wipes, with soap dispensed from a toothpaste tube-like container. Crews are also provided with rinseless shampoo and edible toothpaste to save water.
There are two space toilets on the ISS, both of Russian design, located in "Zvezda" and "Tranquility". These Waste and Hygiene Compartments use a fan-driven suction system similar to the Space Shuttle Waste Collection System. Astronauts first fasten themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a good seal. A lever operates a powerful fan and a suction hole slides open: the air stream carries the waste away. Solid waste is collected in individual bags which are stored in an aluminium container. Full containers are transferred to Progress spacecraft for disposal. Liquid waste is evacuated by a hose connected to the front of the toilet, with anatomically correct "urine funnel adapters" attached to the tube so both men and women can use the same toilet. Waste is collected and transferred to the Water Recovery System, where it is recycled back into drinking water.
Crew health and safety.
Radiation.
The ISS is partially protected from the space environment by the Earth's magnetic field. From an average distance of about 70,000 km, depending on Solar activity, the magnetosphere begins to deflect solar wind around the Earth and ISS. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. The crew of Expedition 10 took shelter as a precaution in 2005 in a more heavily shielded part of the ROS designed for this purpose during the initial 'proton storm' of an X-3 class solar flare, but without the limited protection of the Earth's magnetosphere, interplanetary manned missions are especially vulnerable.
Subatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by the Earth's atmosphere. When they interact in sufficient quantity, their effect becomes visible to the naked eye in a phenomenon called an aurora. Without the protection of the Earth's atmosphere, which absorbs this radiation, crews are exposed to about 1 millisievert each day, which is about the same as someone would get in a year on Earth from natural sources. This results in a higher risk of astronauts developing cancer. Radiation can penetrate living tissue, damage DNA, and cause damage to the chromosomes of lymphocytes. These cells are central to the immune system, and so any damage to them could contribute to the lowered immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and protective drugs may lower the risks to an acceptable level.
The radiation levels experienced on the ISS are about five times greater than those experienced by airline passengers and crew. The Earth's electromagnetic field provides almost the same level of protection against solar and other radiation in low Earth orbit as in the stratosphere. Airline passengers experience this level of radiation for no more than 15 hours for the longest intercontinental flights. For example, on a 12-hour flight an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; only 1/5 the rate experienced by an astronaut in LEO.
Stress.
There has been considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. Cosmonaut Valery Ryumin, wrote in his journal during a particularly difficult period on board the Salyut 6 space station: "All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 and leave them together for two months."
NASA's interest in psychological stress caused by space travel, initially studied when their manned missions began, was rekindled when astronauts joined cosmonauts on the Russian space station Mir. Common sources of stress in early American missions included maintaining high performance under public scrutiny, as well as isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA Astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child.
A study of the longest spaceflight concluded that the first three weeks represent a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. Skylab's 3 crews remained one, two, and three months respectively, long term crews on Salyut 6, Salyut 7, and the ISS last about five to six months and Mir's expeditions often lasted longer. The ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First generation space stations had crews who spoke a single language; second and third-generation stations have crew from many cultures who speak many languages. The ISS is unique because visitors are not classed automatically into 'host' or 'guest' categories as with previous stations and spacecraft, and may not suffer from feelings of isolation in the same way. Crew members with a military pilot background and those with an academic science background or teachers and politicians may have problems understanding each other's jargon and worldview.
Medical.
Medical effects of long-term weightlessness include muscle atrophy, deterioration of the skeleton (osteopenia), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face.
Sleep is disturbed on the ISS regularly due to mission demands, such as incoming or departing ships. Sound levels in the station are unavoidably high; because the atmosphere is unable to thermosyphon, fans are required at all times to allow processing of the atmosphere which would stagnate in the freefall (zero-g) environment.
To prevent some of these adverse physiological effects, the station is equipped with two treadmills (including the COLBERT), and the aRED (advanced Resistive Exercise Device) which enables various weightlifting exercises which add muscle but do nothing for bone density, and a stationary bicycle; each astronaut spends at least two hours per day exercising on the equipment. Astronauts use bungee cords to strap themselves to the treadmill.
Microbiological environmental hazards.
Hazardous moulds which can foul air and water filters may develop aboard space stations. They can produce acids which degrade metal, glass, and rubber. They can also be harmful for the crew health. Microbiological hazards have lead into a development of the LOCAD-PTS that can identify common bacteria and moulds faster than standard methods of culturing, which may require a sample to be sent back to Earth. s of 2012[ [update]], 76 types of unregulated micro-organisms have been detected on the ISS.
Reduced humidity, paint with mould killing chemical and antiseptic solutions can be used to prevent contamination in space stations. All materials used in the ISS are tested for resistance against fungi.
Threat of orbital debris.
At the low altitudes at which the ISS orbits there are a variety of space debris, consisting of many different objects including entire spent rocket stages, defunct satellites, explosion fragments—including materials from anti-satellite weapon tests, paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids, are a significant threat. Large objects could destroy the station, but are less of a threat as their orbits can be predicted. Objects too small to be detected by optical and radar instruments, from approximately 1 cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are still a threat because of their kinetic energy and direction in relation to the station. Spacesuits of spacewalking crew could puncture, causing exposure to vacuum.
The station's shields and structure are divided between the ROS and the USOS, with completely different designs. On the USOS, a thin aluminium sheet is held apart from the hull, the sheet causes objects to shatter into a cloud before hitting the hull thereby spreading the energy of the impact. On the ROS, a carbon plastic honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top. It is about 50% less likely to be punctured, and crew move to the ROS when the station is under threat. Punctures on the ROS would be contained within the panels which are 70 cm square.
Space debris objects are tracked remotely from the ground, and the station crew can be notified. This allows for a Debris Avoidance Manoeuvre (DAM) to be conducted, which uses thrusters on the Russian Orbital Segment to alter the station's orbital altitude, avoiding the debris. DAMs are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. Eight DAMs had been performed prior to March 2009, the first seven between October 1999 and May 2003. Usually the orbit is raised by one or two kilometres by means of an increase in orbital velocity of the order of 1 m/s. Unusually there was a lowering of 1.7 km on 27 August 2008, the first such lowering for 8 years. There were two DAMs in 2009, on 22 March and 17 July. If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their Soyuz spacecraft, so that they would be able to evacuate in the event the station was seriously damaged by the debris. This partial station evacuation has occurred on 13 March 2009, 28 June 2011 and 24 March 2012.
Ballistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels varies depending upon their predicted exposure to damage.
End of mission.
According to a 2009 report, Space Corporation Energia is considering methods to remove from the station some modules of the Russian Orbital Segment when the end of mission is reached and use them as a basis for a new station, known as the Orbital Piloted Assembly and Experiment Complex (OPSEK). The modules under consideration for removal from the current ISS include the Multipurpose Laboratory Module (MLM), currently scheduled to be launched in 2017, with other Russian modules which are currently planned to be attached to the MLM afterwards. Neither the MLM nor any additional modules attached to it would have reached the end of their useful lives in 2016 or 2020. The report presents a statement from an unnamed Russian engineer who believes that, based on the experience from "Mir", a thirty-year life should be possible, except for micrometeorite damage, because the Russian modules have been built with on-orbit refurbishment in mind.
According to the Outer Space Treaty the United States and Russia are legally responsible for all modules they have launched. In ISS planning, NASA examined options including returning the station to Earth via shuttle missions (deemed too expensive, as the station (USOS) is not designed for disassembly and this would require at least 27 shuttle missions), natural orbital decay with random reentry similar to Skylab, boosting the station to a higher altitude (which would delay reentry) and a controlled targeted de-orbit to a remote ocean area.
The technical feasibility of a controlled targeted deorbit into a remote ocean was found to be possible only with Russia's assistance. The Russian Space Agency has experience from de-orbiting the Salyut 4, 5, 6, 7 and Mir space stations; NASA's first intentional controlled de-orbit of a satellite (the Compton Gamma Ray Observatory) occurred in 2000. As of late 2010, the preferred plan is to use a slightly modified Progress spacecraft to de-orbit the ISS. This plan was seen as the simplest, most cost efficient one with the highest margin. Skylab, the only space station built and launched entirely by the US, decayed from orbit slowly over 5 years, and no attempt was made to de-orbit the station using a deorbital burn. Remains of Skylab hit populated areas of Esperance, Western Australia without injuries or loss of life.
The Exploration Gateway Platform, a discussion by NASA and Boeing at the end of 2011, suggested using leftover USOS hardware and 'Zvezda 2' ["sic"] as a refuelling depot and servicing station located at one of the Earth Moon Lagrange points, L1 or L2. The entire USOS cannot be reused and will be discarded, but some other Russian modules are planned to be reused. Nauka, the Node module, two science power platforms and Rassvet, launched between 2010 and 2015 and joined to the ROS may be separated to form OPSEK. The Nauka module of the ISS will be used in the station, whose main goal is supporting manned deep space exploration. OPSEK will orbit at a higher inclination of 71 degrees, allowing observation to and from all of the Russian Federation.
In February 2015, Roscosmos announced that it would remain a part of the international space station program until 2024. Nine months earlier—in response to US sanctions against Russia over the conflict in the Crimea—Russian Deputy Prime Minister Dmitry Rogozin had stated that Russia would reject a US request to prolong the orbiting station's use beyond 2020, and would only supply rocket engines to the US for non-military satellite launches.
A proposed modification that would allow some of the ISS American and European segments to be reused would be to attach a VASIMR drive module to the vacated Node with its own onboard power source. It would allow long term reliability testing of the concept for less cost than building a dedicated space station from scratch.
On March 28, 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. Igor Komarov, the head of Russia's Roscosmos, made the announcement with NASA administrator Charles Bolden at his side. Komarov said "Roscosmos together with NASA will work on the programme of a future orbital station," "We agreed that the group of countries taking part in the ISS project will work on the future project of a new orbital station," "The first step is that the ISS will operate until 2024," and that Roscosmos and NASA "do not rule out that the station's flight could be extended," In a statement provided to SpaceNews March 28, NASA spokesman David Weaver said the agency appreciated the Russian commitment to extending the ISS, but did not confirm any plans for a future space station.
Cost.
The ISS is arguably the most expensive single item ever constructed. In 2010 the cost was expected to be $150 billion. It includes NASA's budget of $58.7 billion (inflation unadjusted) for the station from 1985 to 2015 ($72.4 billion in 2010 dollars), Russia's $12 billion ISS budget, Europe's $5 billion, Japan's $5 billion, Canada's $2 billion, and the cost of 36 shuttle flights to build the station; estimated at $1.4 billion each, or $50.4 billion total. Assuming 20,000 person-days of use from 2000 to 2015 by two to six-person crews, each person-day would cost $7.5 million, less than half the inflation adjusted $19.6 million ($5.5 million before inflation) per person-day of Skylab.
Sightings from Earth.
Naked eye.
The ISS is visible to the naked eye as a slow-moving, bright white dot due to reflected sunlight, and can be seen in the hours after sunset and before sunrise when the station remains sunlit but the ground and sky are dark. The ISS takes about ten minutes to move from one horizon to another, and will only be visible part of that time due to moving into or out of the Earth's shadow. Because of the size of its reflective surface area, the ISS is the brightest man-made object in the sky excluding flares, with an approximate maximum magnitude of −4 when overhead, similar to Venus. The ISS, like many satellites including the Iridium constellation, can also produce flares of up to 8 or 16 times the brightness of Venus as sunlight glints off reflective surfaces. The ISS is also visible during broad daylight conditions, albeit with a great deal more effort.
Tools are provided by a number of websites such as Heavens-Above (see "Live viewing" below) as well as smartphone applications that use the known orbital data and the observer's longitude and latitude to predict when the ISS will be visible (weather permitting), where the station will appear to rise to the observer, the altitude above the horizon it will reach and the duration of the pass before the station disappears to the observer either by setting below the horizon or entering into Earth's shadow.
In November 2012 NASA launched its 'Spot the Station' service, which sends people text and email alerts when the station is due to fly above their town.
The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes.
Astrophotography.
Using a telescope mounted camera to photograph the station is a popular hobby for astronomers, whilst using a mounted camera to photograph the Earth and stars is a popular hobby for crew. The use of a telescope or binoculars allows viewing of the ISS during daylight hours.
Parisian engineer and astrophotographer Thierry Legault, known for his photos of spaceships crossing the Sun (called occultation), travelled to Oman in 2011, to photograph the Sun, moon and space station all lined up. Legault, who received the Marius Jacquemetton award from the Société astronomique de France in 1999, and other hobbyists, use websites that predict when the ISS will pass in front of the Sun or Moon and from what location those passes will be visible.

</doc>
<doc id="15044" url="http://en.wikipedia.org/wiki?curid=15044" title="Irish">
Irish

 
Irish may refer to something of, from, or related to Ireland, an island situated off the north-western coast of continental Europe including:

</doc>
<doc id="15045" url="http://en.wikipedia.org/wiki?curid=15045" title="Cosmicomics">
Cosmicomics

Cosmicomics is a book of short stories by Italo Calvino first published in Italian in 1965 and in English in 1968. Each story takes a scientific "fact" (though sometimes a falsehood by today's understanding), and builds an imaginative story around it. An always extant being called Qfwfq narrates all of the stories save two, each of which is a memory of an event in the history of the universe. Qfwfq also narrates some stories in Calvino's "t zero".
All of the stories in "Cosmicomics", together with those from "t zero" and other sources, are now available in a single volume collection, "The Complete Cosmicomics" (Penguin UK, 2009).
The first U.S. edition, translated by William Weaver, won the National Book Award in the Translation category.
Contents.
All of the stories feature non-human characters which have been heavily anthropomorphized.

</doc>
<doc id="15046" url="http://en.wikipedia.org/wiki?curid=15046" title="IA-32">
IA-32

IA-32 (short for "Intel Architecture, 32-bit", sometimes also called i386 through metonymy) is the third generation of the x86 architecture, first implemented in the Intel 80386 microprocessors in 1985. It was the first incarnation of x86 to support 32-bit computing. As such, "IA-32" may be used as a metonym to refer to all x86 versions that support 32-bit computing.
The IA-32 instruction set was introduced in the Intel 80386 microprocessor in 1985 and remains the basis of most PC microprocessors over twenty years later. Even though the instruction set has remained intact, the successive generations of microprocessors that run it have become much faster. Within various programming language directives, IA-32 is still sometimes referred to as the "i386" architecture.
Intel is the inventor and the biggest supplier of IA-32 processors. The second biggest supplier is AMD. s of 2013[ [update]], Intel, AMD and VIA have moved to x86-64, but still produce IA-32 processors such as Intel Atom (N2xx and Z5xx series), AMD Geode and the VIA C7 family. For a time, Transmeta and others, produced IA-32 processors.
Architectural features.
The primary defining characteristic of IA-32 is the availability of 32-bit general-purpose processor registers (for example, EAX and EBX), 32-bit integer arithmetic and logical operations, 32-bit offsets within a segment in protected mode, and the translation of segmented addresses to 32-bit linear addresses. The designers took the opportunity to make other improvements as well. Some of the most significant changes are described below.

</doc>
<doc id="15047" url="http://en.wikipedia.org/wiki?curid=15047" title="Internalism and externalism">
Internalism and externalism

Internalism and externalism are two opposing ways of explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings. Usually 'internalism' refers to the belief that an explanation can be given of the given subject by pointing to things which are internal to the person or their mind which is considering them. Conversely, externalism holds that it is things about the world which motivate us, justify our beliefs, determine meaning, etc.
Moral philosophy.
Motivation.
In contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary, internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper ""Ought" and Motivation").
These views in moral psychology have various implications. In particular, if motivational internalism is true, then an amoralist is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational "externalist", because the motivational externalist thinks that moral judgments about the right thing to do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire—such as the desire to do the right thing—is required (Brink, 2003),(Rosati, 2006).
Reasons.
There is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for action. An "internal reason" is, roughly, something that one has in light of one's own "subjective motivational set"---one's own commitments, desires (or wants), goals, etc. On the other hand, an "external reason" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not kill oneself no matter what—regardless of whether one wants to die.
Some philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called "internalism about reasons" (or "reasons internalism"). "Externalism about reasons" (or "reasons externalism") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.
Consider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative ("Yes, Sasha has a reason not to steal from that poor person."), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative ("No, Sasha does not have a reason not to steal from that poor person, though others might."). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason "for Sasha's action" not to steal from the poor person next to him only if he currently "wants" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals—that is, part of what Williams calls his "subjective motivational set"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.
Epistemology.
Justification.
In contemporary epistemology, internalism about justification is the idea that everything necessary to provide justification for a belief must be immediately available to an agent's consciousness. Externalism in this context is the view that factors other than those internal to the believer can affect the justificatory status of a belief. One strand of externalism is reliabilism, and the causal theory of knowledge is sometimes considered to be another strand. It is important to distinguish internalism about justification from internalism about knowledge. An internalist about knowledge will likely hold that the conditions that distinguish mere true belief from knowledge are similarly internal to the individual's perspective or grounded in the subject's mental states. Whereas internalism about justification is a widely endorsed view, there is debate about knowledge internalism, due to Edmund Gettier and his Gettier-examples. These are claimed to show that knowledge is not simply justified true belief. In a short but influential paper published in 1963, Gettier produced examples that seemed to show that someone could be justified in believing something which is actually false, and inferring from it a further belief, this belief being coincidentally true. In this way, he claimed that someone could be justified in believing something true but nevertheless not be considered to have knowledge of that thing.
One line of argument in favor of externalism begins with the observation that if what justified our beliefs failed to eliminate significantly the risk of error, then it does not seem that knowledge would be attainable as it would appear that when our beliefs did happen to be correct, this would really be a matter of good fortune. While many will agree with this last claim, the argument seems inconclusive. Setting aside sceptical concerns about the possession of knowledge, Gettier cases have suggested the need to distinguish justification from warrant where warrant is that which distinguishes justified true belief from knowledge by eliminating the kind of accidentality often present in Gettier-type cases. Even if something must significantly reduce the risk of error, it is not clear why justification is what must fill the bill.
One of the more popular arguments for internalism begins with the observation, perhaps first due to Stewart Cohen, that when we imagine subjects completely cut off from their surroundings (thanks to a malicious Cartesian demon, perhaps) we do not think that in cutting these individuals off from their surroundings, these subjects cease to be rational in taking things to be as they appear. The 'new evil demon' argument for internalism (and against externalism) begins with the observation that individuals like us on the inside will be as justified as we are in believing what we believe. As it is part of the story that these individuals' beliefs are not produced by reliable mechanisms or backed by veridical perceptual experiences, the claim that the justification of our beliefs depends upon such things appears to be seriously challenged. Externalists have offered a variety of responses but there is no consensus among epistemologists as to whether these replies are successful (Cohen, 1984; Sosa, 1991).
As a response to skepticism.
In responding to skepticism, Hilary Putnam (1982 ) claims that semantic externalism yields "an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word "water" referred to the substance whose chemical composition is H2O even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling "water" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999 ):
Either I am a BIV, or I am not a BIV.
If I am not a BIV, then when I say "I am not a BIV", it is true.
If I am a BIV, then, when I say "I am not a BIV", it is true (because "brain" and "vat" would only pick out the brains and vats being simulated, not real brains and real vats).
My utterance of "I am not a BIV" is true.
To clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived "Steve." When Steve is given an experience of walking through a park, semantic externalism allows for his thought, "I am walking through a park" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, "I am a brain in a vat," to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.
Apart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words "brain" and "vat" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe "I am not a brain in a vat," then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).
Another attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are "about" things, unlike a BIV's thoughts, which cannot be "about" things (DeRose, 1999 ).
Semantics.
Semantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.
Externalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.
See also:
Philosophy of mind.
Within the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.
The traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various form of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.
As to the traditional discussion on semantic externalism (often dubbed "content externalism"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, "The Meaning of 'Meaning'," (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.
For example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, "I want my mommy," he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, "I want my mommy," will be different from what Tina wants and what she says she wants when she says, "I want my mommy."
Externalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.
Philosophers now tend to distinguish between "wide content" (externalist mental content) and "narrow content" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.
Critics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.
Critics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains H2O. It seems this speaker could know a priori that she thinks that water is wet. This is the thesis of privileged access. It also seems that she could know on the basis of simple thought experiments that she can only think that water is wet if she lives in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.
As mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.
See also
Historiography of science.
Externalism in the historiography of science is the view that the history of science is due to its social context – the socio-political climate and the surrounding economy determines scientific progress.
Internalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity.

</doc>
<doc id="15048" url="http://en.wikipedia.org/wiki?curid=15048" title="Isolationism">
Isolationism

Isolationism is a category of foreign policies institutionalized by leaders who asserted that their nations' best interests were best served by keeping the affairs of other countries at a distance. One possible motivation for limiting international involvement is to avoid being drawn into dangerous and otherwise undesirable conflicts. There may also be a perceived benefit from avoiding international trade agreements or other mutual assistance pacts.
Introduction.
"Isolationism" is currently a somewhat controversial style of policy. Whether or not a country should be isolationist affects both its people's living standards and the ability of its political rulers to benefit favored firms and industries.
The policy or doctrine of trying to isolate one's country from the affairs of other nations by declining to enter into alliances, foreign economic commitments, international agreements, and generally attempting to make one's economy entirely self-reliant; seeking to devote the entire efforts of one's country to its own advancement, both diplomatically and economically, while remaining in a state of peace by avoiding foreign entanglements and responsibilities.
 Free trade eliminates the economic barriers otherwise posed by geopolitical borders, such as tariffs and various taxes that would be inconvenient for both manufacturers and consumers. However, isolationism on the other hand, can preserve local jobs that would otherwise be outsourced overseas. There is no universally accepted opinion regarding isolationism, although western countries often criticise North Korea, Cuba, and other countries for pursuing isolationist policies. These countries, conversely, generally rebut that their policies are in resistance to western imperialism.
Isolationism by country.
Albania.
Albania was isolated from other countries while it was under communist control from 1944 to 1990. Known officially as the People's Republic of Albania from 1946 to 1976, and then as the People's Socialist Republic of Albania from 1976 to 1991, Albania spent much of this time under the regime of socialist leader Enver Hoxha, who ruled from 1944 to his death in 1985.
Bhutan.
Before 1999, Bhutan had banned television and the Internet to preserve its culture, environment, identity etc. Eventually, Jigme Singye Wangchuck lifted the ban on television and the Internet. His son, Jigme Khesar Namgyel Wangchuck was elected as Druk Gyalpo of Bhutan which is being transformed into a democracy.
China.
After Zheng He's voyages in the 15th century, the foreign policy of the Ming Dynasty in China became increasingly isolationist. Emperor Hongwu was the first to propose the policy to ban all maritime shipping in 1371. The Qing Dynasty that came after the Ming often continued the latter dynasty's isolationist policies. Wokou, which literally translates to "Japanese pirates" or "dwarf pirates" were pirates who raided the coastlines of China, Japan and Korea, were one of the key primary concerns, although the maritime ban was not without some control.
At the end of China’s bloody civil war, the country quickly closed off its borders to many outside countries and only maintained diplomatic ties with the Soviet Union. In 1949 Mao turned China into an isolationist, and communist country, along the lines of its Soviet benefactors. For a period of time the Chinese attempted to become self-reliant, but found that in doing so the country could not break even economically, especially when attempting to maintain a communist vision when it came to economics. In the 1970s the People's Republic of China began large radical economic reforms, which forced the country to change from a zero competition nation to one of the most capitalistic nations in the world. In doing so it quickly began to open its borders to the trade of various other countries thus adding itself to a global trade economy. While the government still regulates many of the country's cultural interactions with others, it is very open to the concept of an open market and competition with other countries, allowing the flow of technological innovations to flow in and out of its borders freely. 
Japan.
From 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy which it called "kaikin". The policy prohibited foreign contact with most outside countries. However, the commonly held idea that Japan was entirely closed is misleading. In fact, Japan maintained limited-scale trade and diplomatic relations with China, Korea, the Ryukyu Islands and the Netherlands.
The culture of Japan developed with limited influence from the outside world and had one of the longest stretches of peace in history. During this period, Japan developed thriving cities and castle towns and increasing commodification of agriculture and domestic trade, wage labor, increasing literacy and concomitant print culture, laying the groundwork for modernization, even as the shogunate itself grew weak.
Korea.
Joseon Dynasty.
In 1863, King Gojong took the throne of the Joseon Dynasty when he was a child. His father, Regent Heungseon Daewongun, ruled for him until Gojong reached adulthood. During the mid-1860s he was the main proponent of isolationism and the principal instrument of the persecution of both native and foreign Catholics.
North Korea.
The foreign relations of North Korea are often tense and unpredictable. Since the ended the armed conflict that existed during the active part of the Korean War in 1953, leaving a de facto truce in place ever since, the North Korean government has been largely isolationist, becoming one of the world's most authoritarian societies. While no formal peace treaty exists between North and South Korea, both diplomatic discussions and clashes have occurred between the two. North Korea has maintained close relations with the People's Republic of China and has often limited its contact with other nations. The North Korean government has banned all media from other countries (such as video games, newspapers, and goods), especially South Korea and the United States, and smuggling these products is illegal.
Paraguay.
Just after independence was achieved, Paraguay was governed from 1814 by the dictator José Gaspar Rodríguez de Francia, who closed the country's borders and prohibited trade or any relation with the outside world until his death in 1840. The Spanish settlers who had arrived just before independence had to intermarry with either the old colonists or with the native Guarani, in order to create a single Paraguayan people.
Francia had a particular dislike of foreigners and any who came to Paraguay during his rule (which would have been very difficult) were not allowed to leave for the rest of their lives. An independent character, he hated European influences and the Catholic Church, turning church courtyards into artillery parks and confession boxes into border sentry posts, in an attempt to keep foreigners at bay.
Switzerland.
Switzerland has been neutral in foreign relations since the Battle of Marignano in 1515. Switzerland did not participate in either of the World Wars and it joined the United Nations as late as 2002, leaving only the Vatican City as the last widely recognized non-UN member at the time of joining. Switzerland is not a member of the European Union or the European Economic Area and the general public remains opposed to full EU membership.
In February 2014, Swiss voters narrowly approved a referendum to restrict immigration and reintroduce quotas on foreigners originating from the EU.
United States.
While some scholars, such as Robert J. Art, believe that the United States has an isolationist history, other scholars dispute this by describing the United States as following a strategy of unilateralism or non-interventionism instead. Robert Art makes his argument in "A Grand Strategy for America" (2003). Books that have made the argument that the United States followed unilaterism instead of isolationism include Walter A. McDougall's "Promised Land, Crusader State" (1997), John Lewis Gaddis's "Surprise, Security, and the American Experience" (2004), and Bradley F. Podliska's "Acting Alone" (2010). Both sides claim policy prescriptions from George Washington's Farewell Address as evidence for their argument.

</doc>
<doc id="15049" url="http://en.wikipedia.org/wiki?curid=15049" title="Indianapolis Colts">
Indianapolis Colts

The Indianapolis Colts are an American football team based in Indianapolis, Indiana; they play their games in Lucas Oil Stadium, since 2008. Previously, the team had played for over two decades (1984-2007) at the RCA Dome. The team is a member of the South Division of the American Football Conference (AFC) in the National Football League (NFL).
The Colts have been members of the National Football League since their founding in 1953 in Baltimore and were one of three NFL teams to join the teams of the American Football League to form the American Football Conference following the 1970 merger. While in Baltimore, the team advanced to the postseason ten times and won three NFL Championship games, in 1958, 1959, and 1968. The Colts had two Super Bowl appearances while in Baltimore, losing to the New York Jets in Super Bowl III, while defeating the Dallas Cowboys in Super Bowl V. The Colts moved to Indianapolis in 1984 and have since appeared in the playoffs sixteen times, won two conference championships, and won one Super Bowl, defeating the Chicago Bears in Super Bowl XLI.
The 2014 season was the team's thirty-first in Indianapolis, equaling their length of tenure in Baltimore.
Franchise history.
1953–1983: the Baltimore era.
Following World War II, a competing professional football league was organized known as the All America Football Conference which began play in the 1946 season. In its second year the franchise assigned to the Miami Seahawks was relocated to Maryland's major commercial and manufacturing city of Baltimore, which after a fan contest was renamed the Baltimore Colts and used the team colors of silver and green. These Colts played for the next three seasons in the old A.A.F.C. until it agreed to merge with the old National Football League (of 1920/1922 to 1950), bringing into the merger of the new reorganized NFL of three former AAFC powerhouse teams: the San Francisco 49ers, Cleveland Browns and the Baltimore Colts (known by the designation of "AAFC" or "1947-50"). This new Colts team, now in the "big league" of professional American football for the first time, although with shaky financing and ownership however, played only one season of 1950 in the new reorganized "third" NFL, and was later disbanded and moved.
Two years later, in 1953, a new Baltimore-based group, heavily supported by the City's municipal government and with a large subscription-base of fan-purchased season tickets, led by local owner Carroll Rosenbloom won the rights to a new Baltimore NFL franchise. Rosenbloom was awarded the remains of the former Dallas Texans team, who themselves had a long and winding history starting as the Boston Yanks in 1944, merging later with the Brooklyn Tigers, and who were previously known as the Dayton Triangles, one of the original old NFL teams established even before the League itself, in 1913. With the organization in 1920 of the original "American Professional Football Conference" [A.P.F.C.], (soon renamed the "American Professional Football Association", [A.P.F.A.]), then two years later in 1922, renamed a second time, now permanently as the "National Football League". That team later became the New York Yanks in 1950, and many of the players from the New York Yankees of the former competing All-America Football Conference, (1946-1949), were added to the team to begin playing in the newly merged League for the 1950 season. The Yanks then moved to Dallas in Texas after the 1951 season having competed for two seasons, but played their final two "home" games of the 1952 season as a so-called "road team" at the Rubber Bowl football stadium in Akron, Ohio. The NFL considers the Texans and Colts to be separate teams, although many of the earlier teams shared the same colors of blue and white. Thus, the Indianapolis Colts are legally considered to be a 1953 expansion team.
The third (and current) version of the Colts football team played their first season in Baltimore in 1953, where the team compiled a 3–9 record under first year head coach Keith Molesworth. The franchise struggled during the first few years in Baltimore, with the team not achieving their first winning record until the 1957 season. However, under head coach Weeb Ewbank and the leadership of quarterback Johnny Unitas, the Colts went on to a 9–3 record during the 1958 season and reached the NFL Championship Game for the first time in their history by winning the NFL Western Conference. The Colts faced the New York Giants in the 1958 NFL Championship Game in what is considered to be among the greatest contests in professional football history. The Colts defeated the Giants 23–17 in the first game ever to utilize the overtime rule, a game seen by 45 million people.
Following the Colts first NFL championship, the team once again posted a 9–3 record during the 1959 season and once again defeated the Giants in the NFL Championship Game to claim their second title in back to back fashion. Following the two championships in 1958 and 1959, the Colts did not return to the NFL Championship for four seasons and saw a transition from head coach Ewbank to a young Don Shula in 1963. In Shula's second season the Colts compiled a 12–2 record, but lost to the Cleveland Browns in the NFL Championship. However, in 1968 the Colts returned with the continued leadership of Unitas and Shula and went on to win the Colts' third NFL Championship and made an appearance in Super Bowl III.
Leading up to the Super Bowl and following the 34–0 trouncing of the Cleveland Browns in the NFL Championship, many were calling the 1968 Colts team one of the "greatest pro football teams of all time" and were favored by 18 points against their counterparts from the American Football League, the New York Jets. The Colts, however, were stunned by the Jets, who won the game 16–7 in the first Super Bowl victory for the young AFL. The result of the game surprised many in the sports media as Joe Namath and Matt Snell led the Jets to the Super Bowl victory under head coach Weeb Ewbank, who had previously won two NFL Championships with the Colts.
Rosenbloom of the Colts, Art Modell of the Browns, and Art Rooney of the Pittsburgh Steelers agreed to have their teams join the ten AFL teams in the American Football Conference as part of the AFL-NFL merger in 1970. The Colts immediately went on a rampage in the new league, as new head coach Don McCafferty led the 1970 team to an 11–2–1 regular season record, winning the AFC East title. In the first round of the NFL Playoffs, the Colts beat the Cincinnati Bengals 17–0; one week later in the first ever AFC Championship Game, they beat the Oakland Raiders 27–17. Baltimore went on to win the first post-merger Super Bowl (Super Bowl V), defeating the National Football Conference's Dallas Cowboys 16–13 on a Jim O'Brien field goal with five seconds left to play. The victory gave the Colts their fourth NFL championship and first Super Bowl victory. Following the championship, the Colts returned to the playoffs in 1971 and defeated the Cleveland Browns in the first round, but lost to the Miami Dolphins in the AFC Championship.
Citing friction with the City of Baltimore and the local press, Rosenbloom traded the Colts franchise to Robert Irsay on July 13, 1972 and received the Los Angeles Rams in return. Under the new ownership, the Colts did not reach the postseason for three consecutive seasons after 1971, and after the 1972 season, starting quarterback and legend Johnny Unitas was traded to the San Diego Chargers. Following Unitas' departure, the Colts made the playoffs three consecutive seasons from 1975 to 1977, losing in the divisional round each time. The Colts 1977 playoff loss in double overtime against the Oakland Raiders was famous for the fact that it was the last playoff game for the Colts in Baltimore and is also known for the Ghost to the Post play. These consecutive championship teams featured 1976 NFL Most Valuable Player Bert Jones at quarterback and an outstanding defensive line, nicknamed the "Sack Pack."
Following the 1970s success, the team endured nine consecutive losing seasons beginning in 1978. In 1981, the Colts defense allowed an NFL-record 533 points, set an all-time record for fewest sacks (13), and also set a modern record for fewest punt returns (12). The following year the offense collapsed, including a game against the Buffalo Bills where the Colts' offense did not cross mid-field the entire game. The Colts finished 0–8–1 in the strike-shortened 1982 season, thereby earning the right to select Stanford quarterback John Elway with the first overall pick. Elway, however, refused to play for Baltimore, and using leverage as a draftee of the New York Yankees baseball club, forced a trade to Denver. Behind an improved defense the team finished 7–9 in 1983, but that would be their last season in Baltimore.
Relocation to Indianapolis.
The Baltimore Colts played their final home game in Baltimore on December 18, 1983 against the then Houston Oilers. Irsay continued to request upgrades to Memorial Stadium or the building of a new stadium, like the previous owner had. As a result of poor performance on the field and stadium issues, fan attendance and team revenue continued to dwindle. City officials were precluded from using tax-payer funds for the building of a new stadium and the modest proposals that were offered by the city were not acceptable to either the Colts or the city's MLB franchise the Orioles. However, all sides continued to negotiate. Relations between Irsay and the city of Baltimore deteriorated, Irsay assured fans that his ultimate desire was to stay in Baltimore, he nevertheless began discussions with several other cities willing to build new football stadiums, eventually narrowing the list of cities to two: Phoenix and Indianapolis. Under the administration of mayors Richard Lugar and then William Hudnut, Indianapolis undertook an ambitious effort to reinvent itself into a 'Great American City'. The Hoosier Dome, which was later renamed the RCA Dome, had been built specifically for and was ready to host an NFL expansion team.
Meanwhile in Baltimore, the situation worsened and the Maryland General Assembly intervened and began to pass a law giving the city of Baltimore the right to seize ownership of the team by eminent domain. As a result, Irsay began serious negotiations with Indianapolis Mayor William Hudnut in order to move the team before the Maryland legislature could pass the bill. Indianapolis offered loans as well as the Hoosier Dome and a training complex. After the deal was reached, moving vans from Indianapolis-based Mayflower Transit were dispatched overnight to the team's Maryland training complex arriving on the morning of March 29, 1984. Once in Maryland, workers loaded all of the team's belongings, and by midday the trucks departed for Indianapolis, leaving nothing of the Colts organization that could be seized by Baltimore.
The move triggered a flurry of legal activity that ended when representatives of the city of Baltimore and the Colts organization reached a settlement in March 1986 in which all lawsuits regarding the relocation were dismissed, and the Colts agreed to endorse a new NFL team for Baltimore.
1984–97.
Upon the Colts' arrival in Indianapolis over 143,000 requests for season tickets were received in just two weeks. The move to Indianapolis, however, did not change the recent fortune of the Colts, with the team appearing in the postseason only once in the first eleven seasons in Indianapolis. During the 1984 season, the first in Indianapolis, the team went 4–12 and accounted for the lowest offensive yardage in the league that season. The 1985 and 1986 teams combined for only eight wins, including an 0–13 start in 1986 which prompted the firing of head coach Rod Dowhower, who was replaced by Ron Meyer. The Colts, however, did receive eventual Hall of Fame running back Eric Dickerson as a result of a trade during the 1987 season, and went on to compile a 9–6 record, thereby winning the AFC East and advancing to the postseason for the first time in Indianapolis; they lost that game to the Cleveland Browns.
Following 1987, the Colts did not see any real success for quite some time, with the team missing the postseason for seven consecutive seasons. The struggles came to a climax in 1991 when the team went 1–15 and was just one point away from the first "imperfect" season in the history of a 16 game schedule. The season resulted in the firing of head coach Ron Meyer and the return of former head coach Ted Marchibroda to the organization in 1992; he had coached the team from 1975 to 1979. The team continued to struggle under Marchibroda and Jim Irsay, son of Robert Irsay and general manager at the time. It was in 1994 that Robert Irsay brought in Bill Tobin to become the general manager of the Indianapolis Colts.
Under Tobin, the Colts drafted running back Marshall Faulk with the second overall pick in the 1994 and acquired quarterback Jim Harbaugh as well. These moves along with others saw the Colts begin to turn their fortunes around with playoff appearances in 1995 and 1996. The Colts won their first postseason game as the Indianapolis Colts in 1995 and advanced to the AFC Championship Game against the Pittsburgh Steelers, coming just a Hail Mary pass reception away from a trip to Super Bowl XXX.
Marchibroda retired following the 1995 season and was replaced by Lindy Infante in 1996. After two consecutive playoff appearances, the Colts regressed and went 3–13 during the 1997 season. Along with the disappointing season, the principal owner and man who moved the team to Indianapolis, Robert Irsay, died in January 1997 after years of declining health. Jim Irsay, Robert Irsay's son, entered the role of principal owner following his father's death and quickly began to change the organization. Irsay replaced general manager Tobin with Bill Polian in 1997 as the team decided to build through their number one overall pick in the 1998 draft.
1998–2011: the Peyton Manning era.
Jim Irsay began to shape the Colts one year after assuming control from his father by firing head coach Lindy Infante and hiring Bill Polian as the general manager of the organization. Polian in turn hired Jim Mora to become the next head coach of the team and drafted Tennessee Volunteer quarterback Peyton Manning, the son of New Orleans Saints legend Archie Manning, with the first overall pick in the 1998 draft.
The team and Manning struggled during the 1998 season, winning only three games; Manning threw a league high 28 interceptions. However, Manning did pass for 3,739 yards and threw 26 touchdown passes while also being named to the NFL All-Rookie First Team. The Colts began to improve towards the end of the 1998 season and showed continued growth in 1999. Indianapolis drafted Edgerrin James in 1999 and continued to improve their roster heading into the upcoming season. The Colts went 13–3 in 1999 and finished first in the AFC East, their first division title since 1987. Indianapolis lost to the eventual AFC champion Tennessee Titans in the divisional playoffs.
The 2000 and 2001 Colts teams were considerably less successful compared to the 1999 team, and pressure began to mount on team administration and the coaching staff following a 6–10 season in 2001. Head coach Jim Mora was fired at the end of the season and was replaced by former Tampa Bay Buccaneers head coach Tony Dungy. Dungy and the team quickly changed the atmosphere of the organization and returned to the playoffs in 2002 with a 10–6 record. The Colts also returned to the playoffs in 2003 and 2004 with 12–4 records and AFC South championships. The Colts lost to the New England Patriots and Tom Brady in the 2003 AFC Championship Game and in the 2004 divisional playoffs, thereby beginning a rivalry between the two teams, and between Manning and Brady. Following two consecutive playoff losses to the Patriots, the Colts began the 2005 season with a 13–0 record, including a regular season victory over the Patriots, the first in the Manning era. During the season Manning and Marvin Harrison broke the NFL record for touchdowns by a quarterback and receiver tandem. Indianapolis finished the 2005 season with a 14–2 record, the best record in the league that year and the best in a 16 games season for the franchise, but lost to the Pittsburgh Steelers in the divisional round, a disappointing end to the season.
Indianapolis entered the 2006 season with a veteran quarterback, receivers, and defenders, and chose running back Joseph Addai in the 2006 draft. As in the previous season, the Colts began the season undefeated and went 9–0 before losing their first game against the Dallas Cowboys. Indianapolis finished the season with a 12–4 record and entered the playoffs for the fifth consecutive year, this time as the number three seed in the AFC. The Colts won their first two playoff games against the Kansas City Chiefs and the Baltimore Ravens to return to the AFC Championship Game for the first time since the 2003 playoffs, where they faced their rivals, the New England Patriots. In a classic game, the Colts overcame a 21–3 first half deficit to win the game 38–34 and earned a trip to Super Bowl XLI, the franchise's first Super Bowl appearance since 1970 and for the first as Indianapolis. The Colts faced the Chicago Bears in the Super Bowl, winning the game 29–17 and giving Manning, Polian, Irsay, and Dungy, as well as the city of Indianapolis, their first Super Bowl title.
 Following their Super Bowl championship, the Colts compiled a 13–3 record during the 2007 season; they lost to the San Diego Chargers in the divisional playoffs, in what was the final game the Colts played at the RCA Dome before moving into Lucas Oil Stadium in 2008. The 2008 season began with Manning being sidelined for most of the pre-season due to surgery. Indianapolis began the season with a 3–4 record, but then won nine consecutive games to end the season at 12–4 and make in into the playoffs as a wild card team, eventually losing to the Chargers in the wild card round. Following the season, Tony Dungy announced his retirement after seven seasons as head coach, having compiled an overall record of 92–33 with the team.<ref name="Dungy Retires/Caldwell Hired"></ref>
Jim Caldwell was hired as head coach of the team following Dungy, and led the team during the 2009 season. The Colts went 14–0 during the season to finish with an overall record of 14–2 after controversially benching their starters during the last two games. The Colts for the second time in the Manning era entered the playoffs with the best record in the AFC. The Colts managed victories over the Baltimore Ravens and New York Jets to advance to Super Bowl XLIV against the New Orleans Saints, but lost to the Saints 31–17 to end the season in disappointment.
At the completion of the 2009 season, the Colts had finished the first decade of the 2000s (2000–2009) with the most regular season wins (115) and highest winning percentage (.719) of any team in the NFL during that span.
The 2010 team compiled a 10–6 record, the first time the Colts did not win 12 games since 2002, and lost to the New York Jets in the wild card round of the playoffs. The loss to the Jets would be the last game for Peyton Manning as a Colt.
After missing the preseason, Manning was ruled out for the Colts' opening game in Houston and eventually the entire 2011 season. Taking over as starter was veteran quarterback Kerry Collins, who had been signed to the team after dissatisfaction with backup quarterback Curtis Painter and Dan Orlovsky. However, even with a veteran quarterback, the Colts lost their first 13 games and finished the season with a 2–14 record, enough to receive the first overall pick in the 2012 draft. Immediately following the season, team president Bill Polian was fired, ending his fourteen-year tenure with the team. The change built the anticipation of the organization's decision regarding Manning's future with the team. The Peyton Manning era came to an end on March 8, 2012 when Jim Irsay announced that Manning was being released from the roster after thirteen seasons.
2012–present: the Andrew Luck era.
During the 2012 off-season owner Jim Irsay hired Ryan Grigson to be the General Manager. Grigson decided to let Head Coach Jim Caldwell go and Chuck Pagano was hired as the new Head Coach shortly thereafter. The Colts also began to release some higher paid and oft-injured veteran players, including Joseph Addai, Dallas Clark, and Gary Brackett. The Colts used their number one overall draft pick in 2012 to draft Stanford Cardinal quarterback Andrew Luck and also drafted his teammate Coby Fleener in the second round. The team also switched to a 3–4 defensive scheme.
With productive seasons from both quarterback Andrew Luck and veteran receiver Reggie Wayne, the Colts rebounded from the 2–14 season of 2011 with a 2012 season record of 11–5. The franchise, team, and fan base rallied behind Head Coach Chuck Pagano during his fight with leukemia. Clinching an unexpected playoff spot in the 2012–13 NFL playoffs, the fourteenth playoff berth for the club since 1995. The season ended in a 24–9 playoff loss to the eventual Super Bowl Champion Baltimore Ravens.
Two weeks into the 2013 season, the Colts traded their 1st round selection in the 2014 NFL Draft to the Cleveland Browns for running back Trent Richardson. In week 7, Luck led the Colts to a 39–33 win over his predecessor, Peyton Manning, and the undefeated Broncos. Luck went on to lead the Colts to a 15th division championship later that season. In the first round of the 2013 NFL Playoffs, Andrew Luck led the Colts to a 45-44 victory over Kansas City, outscoring the Chiefs 35-13 in the second half in 
the 2nd biggest comeback in NFL Playoff history.
During the 2014 season, Luck would lead the Colts to the AFC Championship game for the first time in his career after breaking the Colts' single season passing yardage record previously held by Manning.
Logos and uniforms.
The Colts' helmets in 1953 were white with a blue stripe. In 1954–55 they were blue with a white stripe and a pair of horseshoes at the rear of the helmet. For 1956 the colors were reversed. white helmet, blue stripe and horseshoes at the rear. In 1957 the horseshoes moved to their current location, one per side The blue jerseys have white shoulder stripes while the white jerseys have blue stripes. The team also wears white pants with blue stripes down the sides.
From 1982 through 1986, the Colts wore gray pants with their blue jerseys. The gray pants featured a horseshoe on the top of the sides with the player's number inside the horseshoe. The Colts continued to wear white pants with their white jerseys throughout this period, and in 1987, the gray pants were retired.
The Colts wore blue pants with their white jerseys for the first three games of the 1995 season, but then returned to white pants with both the blue and white jerseys. The team made some minor uniform adjustments before the start of the 2004 season, including reverting from blue to the traditional gray face masks, darkening their blue colors from a royal blue to speed blue, as well as adding two white stripes to the socks. In 2006, the stripes were removed from the socks.
In 2002, the Colts made a minor striping pattern change on their jerseys, having the stripes only on top of the shoulders then stop completely. Previously, the stripes used to go around to underneath the jersey sleeves. This was done because the Colts, like many other football teams, were beginning to manufacture the jerseys to be tighter to reduce holding calls and reduce the size of the sleeves. Although the white jerseys of the Minnesota Vikings at the time also had a similar striping pattern and continued as such (as well as the throwbacks the New England Patriots wore in the Thanksgiving game against the Detroit Lions in 2002, though the Patriots later wore the same throwbacks in 2009 with truncated stripes and in 2010 became their official alternate uniform), the Colts and most college teams with this striping pattern did not make this adjustment.
Lucas Oil Stadium.
After 24 years of playing at the RCA Dome, the Colts moved to their new home Lucas Oil Stadium in the fall of 2008. In December 2004, the City of Indianapolis and Jim Irsay agreed to a new stadium deal at an estimated cost of $1 billion (including the Indiana Convention Center upgrades). In a deal estimated at $122 million, Lucas Oil Products won the naming rights to the stadium for 20 years.
It is a seven-level stadium which seats 63,000 for football. It can be reconfigured to seat 70,000 or more for NCAA basketball and football and concerts. It covers 1800000 sqft. The stadium features a retractable roof allowing the Colts to play home games outdoors for the first time since arriving in Indianapolis. Using FieldTurf, the playing surface is roughly 25 ft below ground level. In addition to being larger than the RCA Dome, the new stadium features: 58 permanent concession stands, 90 portable concession stands, 13 escalators, 11 passenger elevators, 800 restrooms, high definition video displays from Daktronics and replay monitors and 142 luxury suites. The stadium also features a retractable roof, with electrification technology developed by VAHLE, Inc. Other than being the home of the Colts, the stadium will host games in both the Men's and Women's NCAA Basketball Tournaments and will serve as the back up host for all NCAA Final Four Tournaments. The stadium hosted the Super Bowl for the 2011 season (Super Bowl XLVI) and has a potential economic impact estimated at $286,000,000. Lucas Oil Stadium will also host the Drum Corps International World Championships from 2009 until 2018.
Rivalries.
New England Patriots.
The rivalry between the Indianapolis Colts and New England Patriots is one of the NFL's newest rivalries. The rivalry is fueled by the quarterback comparison between Peyton Manning and Tom Brady. The Patriots owned the beginning of the series, defeating the Colts in six consecutive contests including the 2003 AFC Championship game and a 2004 AFC Divisional game. The Colts won the next three matches, notching two regular season victories and a win in the 2006 AFC Championship game on the way to their win in Super Bowl XLI. On November 4, 2007 the Patriots defeated the Colts 24–20; in the next matchup on November 2, 2008, the Colts won 18–15 in a game that was one of the reasons the Patriots failed to make the playoffs; in the 2009 meeting, the Colts staged a spirited comeback to beat the Patriots 35–34; in 2010 the Colts almost staged another comeback, pulling within 31–28 after trailing 31–14 in the fourth quarter, but fell short due to a Patriots interception of a Manning pass late in the game; it turned out to be Manning's final meeting against the Patriots as a member of the Colts. After a dismal 2011 season that included a 31–24 loss to the Patriots, the Colts drafted Andrew Luck and in November of 2012 the two teams met with identical 6–3 records; the Patriots erased a 14–7 gap to win 59–24. The nature of this rivalry is ironic because while the Colts and Patriots were division rivals from 1970 to 2001, it did not become prominent in league circles until after Indianapolis was relocated to the AFC South. On November 16, 2014, the New England Patriots traveled at 7-2 to play the 6-3 Colts at Lucas Oil Stadium. After a stellar 4 touchdown performance by New England running back Jonas Gray, the Patriots defeated the Colts 42-20.
Earliest rivalries.
In the years 1953–66 the Colts played in the NFL Western Conference (also known as division), but were never known to have a significant rivalry with any of the other franchises in that alignment, seeing as they were the eastern-most team and the rest of the division included the Great Lakes franchises Green Bay, Detroit Lions, Chicago Bears, and after 1961, the Minnesota Vikings. The closest team to Baltimore was the Washington Redskins, but they were not in the same division, and they were not very competitive at that time.
New York Giants.
In 1958 Baltimore played its first NFL Championship Game against the 10–3 New York Giants. The Giants qualified for the championship after a tie-breaking playoff against the Cleveland Browns. Having already been defeated by the Giants in the regular season, Baltimore was not favored to win, yet proceeded to take the title in sudden death overtime. The Colts then repeated the feat by posting an identical record and routing the Giants in the 1959 final. Up until the Colts' back-to-back titles, the Giants had been the premier club in the NFL, and would continue to be post-season stalwarts the next decade losing three straight finals. The situation was reversed by the end of the decade, with Baltimore winning the 1968 NFL title while New York would arrive at continuously less impressive results. In recent years, the Colts and Giants featured brothers as their starting quarterbacks (Peyton and Eli Manning respectively) leading to their occasional match-up being referred to as the "Manning Bowl".
New York Jets.
Super Bowl III became the most famous upset in pro sports history as the American Football League's New York Jets won 16–7 over the overwhelmingly favored Colts. With the merger of the AFL and NFL the Colts and Jets were placed in the new AFC East. The two teams met twice a year (interrupted in 1982 by a player strike) 1970–2001; with the move of the Colts to the AFC South the two teams' rivalry actually escalated, as they met three times in the playoffs in the South's first nine seasons of existence; the Jets crushed the Colts 41–0 in the 2002 Wild Card playoff round; the Colts then defeated the Jets 30–17 in the 2009 AFC Championship Game; but the next year in the Wild Card round the Jets pulled off another playoff upset of the Colts, winning 17–16; it was Peyton Manning's final game with the Colts. The Jets defeated the Colts 35–9 in 2012 in Andrew Luck's debut season.
Joe Namath and Johnny Unitas were the focal point of the rivalry at its beginning, but they did not meet for a full game until September 24, 1972. Namath erupted with six touchdowns and 496 passing yards despite only 28 throws and 15 completions. Unitas threw for 376 yards and two scores but was sacked six times as the Jets won 44–34; the game was considered one of the top ten passing duels in NFL history.
Miami Dolphins.
Baltimore's post NFL-AFL merger passage to the AFC saw them thrust into a new environment with little in common with its fellow divisional teams, the Jets, Miami Dolphins, Buffalo Bills, and Boston Patriots. One angle where the two clubs did have something in common, however, lay in new Miami coach Don Shula. Shula had coached the Colts the previous seven pre-merger seasons (1963–9) and was signed by Joe Robbie after the merger was consummated; because the signing came after the merger the NFL's rules on tampering came into play, and the Dolphins had to give up their first-round pick to the Colts.
Powered by QB Earl Morrall Baltimore would be the first non-AFL franchise to win a division title in the conference, outlasting the Miami Dolphins by one game, and leading the division since Week 3 of 1970. The two franchises were denied a playoff confrontation by Miami's first-round defeat to the Oakland Raiders, whereas Baltimore would win its first Super Bowl title that year.
Yet in 1971 the teams were engaged in a heated race that went down to the final week of the season, where Miami won its first division title with a 10–3–1 title compared to the 10–4 Baltimore record after the Colts won the Week 13 matchup between them at home, but proceeded to lose the last game of the season to Boston. In the playoffs Baltimore advanced to the AFC title game after a 20–3 rout of the Cleveland Browns, whereas Miami survived a double-overtime nailbiter against the Kansas City Chiefs. This set up a title game that was favored for the defending league champion Colts. Yet Miami won the AFC championship with a 21–0 shutout and advanced to lose Super Bowl VI to Dallas. In 1975 Baltimore and Miami tied with 10–4 records, yet the Colts advanced to the playoffs based on a head-to-head sweep of their series. In 1977 Baltimore tied for first for the third straight year (in 1976 they tied with the now-New England Patriots) with Miami, and this time advanced to the playoffs on even slimmer pretenses, with a conference record of 9–3 compared to Miami's 8–4, as they had split the season series. The rivalry would in the following years be virtually negated by very poor play of the Colts; the Colts would win just 117 games in the twenty-one seasons (1978–98) that bracketed their 1977 playoff loss to the Oakland Raiders and the 1999 trade of star running back Marshall Faulk; this included a 0–8–1 record during the NFL's strike shortened 1982 season.
In 1995, now as Indianapolis, the two both posted borderline 9–7 records to tie for second against Buffalo, yet the Colts once again reached the post-season having swept the season series. The following season they edged out Miami by posting a 9–7 record and winning the ordinarily meaningless 3rd place position, but qualifying for the wild card. The two clubs' 1999 meetings were dramatic affairs between Hall Of Fame-bound Dan Marino and up-and-coming star Peyton Manning. Marino led a 25-point fourth quarter comeback for a 34–31 Dolphins win at the RCA Dome, then in Miami Marino led another comeback to tie the game 34–34 with 36 seconds remaining; Manning, however, drove the Colts in range for a 53-yard field goal as time expired (37–34 Colts win).
The last truly meaningful matchup between the two franchises would be in the 2000 season, when Miami edged out Indianapolis with an 11–5 record for the division championship. The two then met in the wild-card round where the Dolphins won 23–17 before being blown out by Oakland 27–0 (the Colts themselves had suffered a bitter loss to the Raiders in Week Two of the season when the Raiders erased a 24–7 gap to win 38–31). In 2002 the Colts moved to the newly created AFC South division; the two clubs met at the RCA Dome on September 15 where the Dolphins edged the Colts 21–13 after stopping a late Colts drive. The rivalry was effectively retired after this; the two clubs did meet in a memorable "Monday Night Football" matchup in 2009 where the Colts, despite having the ball for only fifteen minutes, defeated the Dolphins 27–23.
The rivalry saw a rekindling after the 2012 NFL Draft brought new quarterbacks to both teams in Ryan Tannehill and Luck. The two met during the 2012 season with Luck breaking the rookie record for passing yards in a game in a 23-20 win over the Dolphins, although Tannehill and the Dolphins would beat the Colts 24-20 the next season.
Players.
Pro Football Hall of Famers.
Contributors.
Indianapolis Colts
Ring of Honor.
The Ring of Honor was established on September 23, 1996. There have been eleven inductees.
Radio and television coverage.
The Colts' flagship radio station since relocating from Baltimore in 1984 to 1998 and again starting in the 2007 season is WIBC 1070 AM (later renamed WFNI as of December 26, 2007); under the new contract, games are also simulcast on WLHK at 97.1 FM. From 1998 through 2006, the Colts' flagship radio station was WFBQ at 94.7 FM (with additional programming on station WNDE at 1260 AM). Bob Lamey is the team's play-by-play announcer, holding that title from 1984 to 1991 and again since 1995. Former Colts offensive lineman, Will Wolford serves as the "color commentator". Ted Marchibroda, who had been the head coach of the Colts in both Baltimore and Indianapolis and who served as color commentator from 1999 to 2006, is now an analyst on the Colts pre-game show. Mike Jansen serves as the public address announcer at all Colts home games. Mike has been the public address announcer since the 1998 season.
Until 2011, WTTV (channel 4/29) carried the team's preseason games, when WNDY-TV (channel 23) began to carry them as part of an agreement with sister station WISH-TV (channel 8) to become the team's official station; WISH had carried most of the team's games through the NFL on CBS since the 1998 season. Indiana University's "Hoosiers" announcer Don Fischer provides play-by-play. "Monday Night Football" broadcasts are usually carried by ABC affiliate WRTV (channel 6).
The team's carriage rights were shaken up in mid-2014 when WTTV's owner Tribune Media came to terms with CBS to become the network's Indianapolis affiliate as of January 1, 2015, leaving WISH with the market's affiliation with The CW. With the deal, both Tribune Media stations, including WXIN (channel 59) will carry the bulk of the team's regular season games starting with the 2015 NFL season (WXIN will carry a minimum of two home games against NFC opponents under the NFL on Fox deal, along with flex-scheduled Sunday games no matter the division matchup), with the team's Wild Card playoff game against the Cincinnati Bengals on January 4, 2015 on WTTV rather than new CW affiliate WISH. Also as of the 2015 season, WTTV and WXIN will become the official Colts stations and air the team's preseason games, along with official team programming and coach's shows, and have some kind of signage and presence at Lucas Oil Stadium.

</doc>
<doc id="15051" url="http://en.wikipedia.org/wiki?curid=15051" title="Immigration to the United States">
Immigration to the United States

Immigration to the United States is a complex demographic phenomenon that has been a major source of population growth and cultural change throughout much of the history of the United States. The economic, social, and political aspects of immigration have caused controversy regarding ethnicity, economic benefits, jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior. On a per capita basis, the United States lets in fewer immigrants than half the countries in the OECD. Prior to 1965, policies such as the national origins formula limited immigration and naturalization opportunities for people from areas outside Western Europe.
The civil rights movement of the 1960s led to the replacement of these ethnic quotas with per-country limits. Since then, the number of first-generation immigrants living in the United States has quadrupled, from 9.6 million in 1970 to about 38 million in 2007. Nearly 14 million immigrants entered the United States from 2000 to 2010, and over one million persons were naturalized as U.S. citizens in 2008. The per-country limit applies the same maximum on the number of visas to all countries regardless of their population and has therefore had the effect of significantly restricting immigration of persons born in populous nations such as Mexico, China, India, and the Philippines – the leading countries of origin for legally admitted immigrants to the United States in 2013; nevertheless, China, India, and Mexico were the leading countries of origin for immigrants overall to the United States in 2013, regardless of legal status, according to a U.S. Census Bureau study. s of 2009[ [update]], 66% of legal immigrants were admitted on the basis of family ties, along with 13% admitted for their employment skills and 17% for humanitarian reasons.
Migration is difficult, expensive, and dangerous for those who enter the US illegally across the Mexico–United States border. Virtually all undocumented immigrants have no avenues for legal entry to the United States due the restrictive legal limits on green cards, and lack of immigrant visas for low skilled workers. Participants in debates on immigration in the early twenty-first century called for increasing enforcement of existing laws governing illegal immigration to the United States, building a barrier along some or all of the 2000 mi U.S.-Mexico border, or creating a new guest worker program. Through much of 2006 the country and Congress was immersed in a debate about these proposals. s of 2010[ [update]] few of these proposals had become law, though a partial border fence had been approved and subsequently canceled.
History.
American immigration history can be viewed in four epochs: the colonial period, the mid-19th century, the start of the 20th century, and post-1965. Each period brought distinct national groups, races and ethnicities to the United States. During the 17th century, approximately 400,000 English people migrated to Colonial America. Over half of all European immigrants to Colonial America during the 17th and 18th centuries arrived as indentured servants. The mid-19th century saw mainly an influx from northern Europe; the early 20th-century mainly from Southern and Eastern Europe; post-1965 mostly from Latin America and Asia.
Historians estimate that fewer than 1 million immigrants came to the United States from Europe between 1600 and 1799. The 1790 Act limited naturalization to "free white persons"; it was expanded to include blacks in the 1860s and Asians in the 1950s. In the early years of the United States, immigration was fewer than 8,000 people a year, including French refugees from the slave revolt in Haiti. After 1820, immigration gradually increased. From 1836 to 1914, over 30 million Europeans migrated to the United States. The death rate on these transatlantic voyages was high, during which one in seven travelers died. In 1875, the nation passed its first immigration law, the Page Act of 1875.
The peak year of European immigration was in 1907, when 1,285,349 persons entered the country. By 1910, 13.5 million immigrants were living in the United States. In 1921, the Congress passed the Emergency Quota Act, followed by the Immigration Act of 1924. The 1924 Act was aimed at further restricting immigrants from Southern and Eastern Europe, particularly Jews, Italians, and Slavs, who had begun to enter the country in large numbers beginning in the 1890s. 
Immigration patterns of the 1930s were dominated by the Great Depression. In the final prosperous year, 1929, there were 279,678 immigrants recorded, but in 1933, only 23,068 came to the U.S. In the early 1930s, more people emigrated from the United States than to it. The U.S. government sponsored a Mexican Repatriation program which was intended to encourage people to voluntarily move to Mexico, but thousands were deported against their will. Altogether about 400,000 Mexicans were repatriated. Most of the Jewish refugees fleeing the Nazis and World War II were barred from coming to the United States. In the post-war era, the Justice Department launched Operation Wetback, under which 1,075,168 Mexicans were deported in 1954.
 First, our cities will not be flooded with a million immigrants annually. Under the proposed bill, the present level of immigration remains substantially the same... Secondly, the ethnic mix of this country will not be upset... Contrary to the charges in some quarters, [the bill] will not inundate America with immigrants from any one country or area, or the most populated and deprived nations of Africa and Asia... In the final analysis, the ethnic pattern of immigration under the proposed measure is not expected to change as sharply as the critics seem to think.
 — Ted Kennedy, chief Senate sponsor of the Immigration and Nationality Act of 1965.
The Immigration and Nationality Act of 1965, also known as the Hart-Cellar Act, abolished the system of national-origin quotas. By equalizing immigration policies, the act resulted in new immigration from non-European nations, which changed the ethnic make-up of the United States. In 1970, 60% of immigrants were from Europe; this decreased to 15% by 2000. In 1990, George H. W. Bush signed the Immigration Act of 1990, which increased legal immigration to the United States by 40%. In 1991, Bush signed the Armed Forces Immigration Adjustment Act 1991, allowing foreign service members who had serve 12 or more years in the US Armed Forces to qualify for permanent residency and, in some cases, citizenship.
Appointed by Bill Clinton, the U.S. Commission on Immigration Reform recommended reducing legal immigration from about 800,000 people per year to approximately 550,000. While an influx of new residents from different cultures presents some challenges, "the United States has always been energized by its immigrant populations," said President Bill Clinton in 1998. "America has constantly drawn strength and spirit from wave after wave of immigrants [...] They have proved to be the most restless, the most adventurous, the most innovative, the most industrious of people."
Nearly 8 million people immigrated to the United States from 2000 to 2005; 3.7 million of them entered without papers. Since 1986 Congress has passed seven amnesties for undocumented immigrants. In 1986 president Ronald Reagan signed immigration reform that gave amnesty to 3 million undocumented immigrants in the country. Hispanic immigrants suffered job losses during the late-2000s recession, but since the recession's end in June 2009, immigrants posted a net gain of 656,000 jobs. Over 1 million immigrants were granted legal residence in 2011.
Source: US Department of Homeland Security, "Persons Obtaining Legal Permanent Resident Status: Fiscal Years 1950 to 2013"
Contemporary immigration.
Until the 1930s most legal immigrants were male. By the 1990s women accounted for just over half of all legal immigrants. Contemporary immigrants tend to be younger than the native population of the United States, with people between the ages of 15 and 34 substantially overrepresented. Immigrants are also more likely to be married and less likely to be divorced than native-born Americans of the same age.
Immigrants are likely to move to and live in areas populated by people with similar backgrounds. This phenomenon has held true throughout the history of immigration to the United States. Seven out of ten immigrants surveyed by Public Agenda in 2009 said they intended to make the U.S. their permanent home, and 71% said if they could do it over again they would still come to the US. In the same study, 76% of immigrants say the government has become stricter on enforcing immigration laws since the September 11, 2001 attacks ("9/11"), and 24% report that they personally have experienced some or a great deal of discrimination.
Public attitudes about immigration in the U.S. were heavily influenced in the aftermath of the 9/11 attacks. After the attacks, 52% of Americans believed that immigration was a good thing overall for the U.S., down from 62% the year before, according to a 2009 Gallup poll. A 2008 Public Agenda survey found that half of Americans said tighter controls on immigration would do "a great deal" to enhance U.S. national security. Harvard political scientist and historian Samuel P. Huntington argued in Who Are We? The Challenges to America's National Identity that a potential future consequence of continuing massive immigration from Latin America, especially Mexico, might lead to the bifurcation of the United States.
The population of illegal Mexican immigrants in the US fell from approximately 7 million in 2007 to 6.1 million in 2011 Commentators link the reversal of the immigration trend to the economic downturn that started in 2008 and which meant fewer available jobs, and to the introduction of tough immigration laws in many states. According to the Pew Hispanic Center the total number of Mexican born persons had stagnated in 2010, and tended toward going into negative figures.
More than 80 cities in the United States, including Washington D.C., New York City, Los Angeles, Chicago, San Francisco, San Diego, San Jose, Salt Lake City, Phoenix, Dallas, Fort Worth, Houston, Detroit, Jersey City, Minneapolis, Miami, Denver, Baltimore, Seattle, Portland, Oregon and Portland, Maine, have sanctuary policies, which vary locally.
Ethnicity.
Inflow of New Legal Permanent Residents by continent in 2013:
   Asia
 (40.43%)   Americas
 (40.04%)   Africa
 (9.92%)   Europe
 (8.74%)   Australia and Oceania
 (0.53%)  Unknown (0.33%)
Source: US Department of Homeland Security, Office of Immigration Statistics
New reasons for immigrating to the US.
Froma Harrop, of the "Providence Journal", has written about "environmental immigration," specifically wealthier Chinese nationals moving to or buying real estate in the US to escape China's heavy industrial pollution.
Demography.
The United States admitted more legal immigrants from 1991 to 2000, between ten to eleven million, than in any previous decade. In the most recent decade, the ten million legal immigrants that settled in the U.S. represent an annual growth of only about 0.3% as the U.S. population grew from 249 million to 281 million. By comparison, the highest previous decade was the 1900s, when 8.8 million people arrived, increasing the total U.S. population by one percent every year. Specifically, "nearly 15% of Americans were foreign-born in 1910, while in 1999, only about 10% were foreign-born."
By 1970, immigrants accounted for 4.7 percent of the US population and rising to 6.2 percent in 1980, with an estimated 12.5 percent in 2009. s of 2010[ [update]], 25% of US residents under age 18 were first- or second-generation immigrants. Eight percent of all babies born in the U.S. in 2008 belonged to illegal immigrant parents, according to a recent analysis of U.S. Census Bureau data by the Pew Hispanic Center.
Legal immigration to the U.S. increased from 250,000 in the 1930s, to 2.5 million in the 1950s, to 4.5 million in the 1970s, and to 7.3 million in the 1980s, before resting at about 10 million in the 1990s. Since 2000, legal immigrants to the United States number approximately 1,000,000 per year, of whom about 600,000 are "Change of Status" who already are in the U.S. Legal immigrants to the United States now are at their highest level ever, at just over 37,000,000 legal immigrants. Illegal immigration may be as high as 1,500,000 per year with a net of at least 700,000 illegal immigrants arriving every year. Immigration led to a 57.4% increase in foreign born population from 1990 to 2000.
While immigration has increased drastically over the last century, the foreign born share of the population was still higher in 1900 (about 20%) than it is today (about 10%). A number of factors may be attributed to the decrease in the representation of foreign born residents in the United States. Most significant has been the change in the composition of immigrants; prior to 1890, 82% of immigrants came from North and Western Europe. From 1891 to 1920, that number dropped to 25%, with a rise in immigrants from East, Central, and South Europe, summing up to 64%. Animosity towards these different and foreign immigrants rose in the United States, resulting in much legislation to limit immigration.
Contemporary immigrants settle predominantly in seven states, California, New York, Florida, Texas, Pennsylvania, New Jersey and Illinois, comprising about 44% of the U.S. population as a whole. The combined total immigrant population of these seven states was 70% of the total foreign-born population in 2000. If current birth rate and immigration rates were to remain unchanged for another 70 to 80 years, the U.S. population would double to nearly 600 million.
The top twelve emigrant countries in 2006 were Mexico (173,753), People's Republic of China (87,345), Philippines (74,607), India (61,369), Cuba (45,614), Colombia (43,151), Dominican Republic (38,069), El Salvador (31,783), Vietnam (30,695), Jamaica (24,976), South Korea (24,386), and Guatemala (24,146). Other countries comprise an additional 606,370.
In 1900, when the U.S. population was 76 million, there were an estimated 500,000 Hispanics. The Census Bureau projects that by 2050, one-quarter of the population will be of Hispanic descent. This demographic shift is largely fueled by immigration from Latin America.
Origin.
A country is included in the table if it exceeded 50,000 in either category.
Note: Counts of immigrants since 1986 for Russia includes "Soviet Union (former)", and for Czech Republic includes "Czechoslovakia (former)".
Effects of immigration.
Demographics.
The Census Bureau estimates the US population will grow from 281 million in 2000 to 397 million in 2050 with immigration, but only to 328 million with no immigration. A 2008 report from the Pew Research Center projects that by 2050, non-Hispanic whites will account for 47% of the population, down from the 2005 figure of 67%. Non-Hispanic whites made up
85% of the population in 1960. It also foresees the Hispanic population rising from 14% in 2005 to 29% by 2050. The Asian population is expected to more than triple by 2050. Overall, the Pew Report predicts the population of the United States will rise from 296 million in 2005 to 438 million in 2050, with 82% of the increase from immigrants.
In 35 of the country's 50 largest cities, non-Hispanic whites were at the last census or are predicted to be in the minority. In California, non-Hispanic whites slipped from 80% of the state's population in 1970 to 42.3% in 2001 and 39% in 2013.
Immigrant segregation declined in the first half of the 20th century, but has been rising over the past few decades. This has caused questioning of the correctness of describing the United States as a melting pot. One explanation is that groups with lower socioeconomic status concentrate in more densely populated area that have access to public transit while groups with higher socioeconomic status move to suburban areas. Another is that some recent immigrant groups are more culturally and linguistically different from earlier groups and prefer to live together due to factors such as communication costs. Another explanation for increased segregation is white flight.
Source: 1990 and 2000 decennial Census and 2010 American Community Survey
Economic.
In a late 1980s study, economists overwhelmingly viewed immigration, including illegal immigration, as a positive for the economy. According to James Smith, a senior economist at Santa Monica-based RAND Corporation and lead author of the United States National Research Council's study "", immigrants contribute as much as $10 billion to the U.S. economy each year. The NRC report found that although immigrants, especially those from Latin America, caused a net loss in terms of taxes paid versus social services received, immigration can provide an overall gain to the domestic economy due to an increase in pay for higher-skilled workers, lower prices for goods and services produced by immigrant labor, and more efficiency and lower wages for some owners of capital. The report also notes that although immigrant workers compete with domestic workers for low-skilled jobs, some immigrants specialize in activities that otherwise would not exist in an area, and thus can be beneficial for all domestic residents. A non-partisan report in 2007 from the Congressional Budget Office concluded that most estimates show that illegal immigrants impose a net cost to state and local governments, but "that no agreement exists as to the size of, or even the best way of measuring, the cost on a national level." Estimates of the net national cost that illegal immigrants impose on the United States vary greatly, with the Urban Institute saying it was $1.9 billion in 1992, and a Rice University professor putting it at $19.3 billion in 1993. About twenty-one million immigrants, or about fifteen percent of the labor force, hold jobs in the United States; however, the number of unemployed is only seven million, meaning that immigrant workers are not taking jobs from domestic workers, but rather are doing jobs that would not have existed had the immigrant workers not been in the United States. U.S. Census Bureau's "Survey of Business Owners: Hispanic-Owned Firms: 2002" indicated that the number of Hispanic-owned businesses in the United States grew to nearly 1.6 million in 2002. Those businesses generated about $222 billion in gross revenue. The report notes that the burden of poor immigrants is not borne equally among states, and is most heavy in California. Another claim supporting expanding immigration levels is that immigrants mostly do jobs Americans do not want. A 2006 Pew Hispanic Center report added evidence to support this claim, when they found that increasing immigration levels have not hurt employment prospects for American workers. Research shows an economic consensus that, taken as a whole, immigrants raise living standards for American workers by boosting demand and increasing productivity, contributing to innovation, and lowering prices.
In 2009, a study by the Cato Institute, a free market think tank, found that legalization of low-skilled illegal resident workers in the US would result in a net increase in US GDP of $180 billion over ten years. The Cato Institute study did not examine the impact on per capita income for most Americans. Jason Riley notes that because of progressive income taxation, in which the top 1% of earners pay 37% of federal income taxes (even though they actually pay a lower tax percentage based on their income), 60% of Americans collect more in government services than they pay in, which also reflects on immigrants. In any event, the typical immigrant and his children will pay a net $80,000 more in their lifetime than they collect in government services according to the NAS. Legal immigration policy is set to maximize net taxation. Illegal immigrants even after an amnesty tend to be recipients of more services than they pay in taxes. In 2010, an econometrics study by a Rutgers economist found that immigration helped increase bilateral trade when the incoming people were connected via networks to their country of origin, particularly boosting trade of final goods as opposed to intermediate goods, but that the trade benefit weakened when the immigrants became assimilated into American culture.
The Kauffman Foundation's index of entrepreneurial activity is nearly 40% higher for immigrants than for natives. Immigrants were involved in the founding of many prominent American high-tech companies, such as Google, Yahoo, YouTube, Sun Microsystems, and eBay. On the poor end of the spectrum, the "New Americans" report found that low-wage immigration does not, on aggregate, lower the wages of most domestic workers. The report also addresses the question of if immigration affects black Americans differently from the population in general: "While some have suspected that blacks suffer disproportionately from the inflow of low-skilled immigrants, none of the available evidence suggests that they have been particularly hard-hit on a national level. Some have lost their jobs, especially in places where immigrants are concentrated. But the majority of blacks live elsewhere, and their economic fortunes are tied to other factors."
A study done in 2005 showed that a third of adult immigrants had not finished high school, and a third had no health insurance. Robert Samuelson points out that poor immigrants strain public services such as local schools and health care. He points out that "from 2000 to 2006, 41 percent of the increase in people without health insurance occurred among Hispanics." According to the immigration reduction advocacy group Center for Immigration Studies, 25.8% of Mexican immigrants live in poverty, which is more than double the rate for natives in 1999. In another report, The Heritage Foundation notes that from 1990 to 2006, the number of poor Hispanics increased by 3.2 million, from 6 million to 9.2 million.
U.S. citizens will not take certain jobs usually done by foreign workers, like manual labor involving agriculture. Fruit picking labor costs are estimated at $0.36 per pound, so a production rate of 1 pound per minute is required to earn minimum wage after fees are deducted. Hard physical labor and dangerous jobs with a small paycheck create labor shortages in certain job markets that can only be satisfied using foreign labor. Foreign laborers often work for no pay for several months each year to earn enough to pay their employer for the cost of their H series visa. Hispanic immigrants in the United States were hit hard by the subprime mortgage crisis. There was a disproportionate level of foreclosures in some immigrant neighborhoods. The banking industry provided home loans to undocumented immigrants, viewing it as an untapped resource for growing their own revenue stream. In October 2008, KFYI reported that according to the U.S. Department of Housing and Urban Development, five million illegal immigrants held fraudulent home mortgages. The story was later pulled from their website and replaced with a correction. The Phoenix Business Journal cited a HUD spokesman saying that there was no basis to news reports that more than five million bad mortgages were held by illegal immigrants, and that the agency had no data showing the number of illegal immigrants holding foreclosed or bad mortgages.
Immigration and foreign labor documentation fees increased over 80% in 2007, with over 90% of funding for USCIS derived from immigration application fees, creating many USCIS jobs involving immigration to US, such as immigration interview officials, finger print processor, Department of Homeland Security, etc. An article by American Enterprise Institute researcher Jason Richwine states that while earlier European immigrants were often poor when they arrived, by the third generation they had economically assimilated to be indistinguishable from the general population. However, for the Hispanic immigrants the process stalls at the second generation and the third generation continues to be substantially poorer than whites. Despite apparent disparities between different communities, Asians, a significant number of whom arrived in the United States after 1965, had the highest median income per household among all race groups as of 2008.
According to NPR in 2005, about 3% of illegal immigrants were working in agriculture. The H-2A visa allows U.S. employers to bring foreign nationals to the United States to fill temporary agricultural jobs. The passing of tough immigration laws in several states from around 2009 provides a number of practical case studies. The state of Georgia passed immigration law HB 87 in 2011; this led, according to the coalition of top Kansas businesses, to 50% of its agricultural produce being left to rot in the fields, at a cost to the state of more than $400m. Overall losses caused by the act were $1bn; it was estimated that the figure would become over $20bn if all the estimated 325,000 undocumented workers left Georgia. The cost to Alabama of its crackdown in June 2011 has been estimated at almost $11bn, with up to 80,000 unauthorised immigrant workers leaving the state.
While immigration from Latin America has kept the United States from falling off a Japanese or European style demographic cliff, this is a limited resource as fertility rates continue to decline throughout the Americas and the world.
According to Congressional Research Service, since the 1970s while immigration to the United States increased, the lower 90% of tax filers incomes became stagnant, and eventually began to decrease since 2000.
Social.
Irish immigration was opposed in the 1850s by the nativist Know Nothing movement, originating in New York in 1843. It was engendered by popular fears that the country was being overwhelmed by Irish Catholic immigrants. In 1891, a lynch mob stormed a local jail and hanged several Italians following the acquittal of several Sicilian immigrants alleged to be involved in the murder of New Orleans police chief David Hennessy. The Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at limiting immigration overall, and making sure that the nationalities of new arrivals matched the overall national profile.
After the September 11 attacks, many Americans entertained doubts and suspicions about people apparently of Middle-Eastern origins. NPR in 2010 fired a prominent black commentator, Juan Williams, when he talked publicly about his fears on seeing people dressed like Muslims on airplanes.
Racist thinking among and between minority groups does occur; examples of this are conflicts between blacks and Korean immigrants, notably in the 1992 Los Angeles Riots, and between African Americans and non-white Latino immigrants. There has been a long running racial tension between African American and Mexican prison gangs, as well as significant riots in California prisons where they have targeted each other, for ethnic reasons. There have been reports of racially motivated attacks against African Americans who have moved into neighborhoods occupied mostly by people of Mexican origin, and vice versa. There has also been an increase in violence between non-Hispanic Anglo Americans and Latino immigrants, and between African immigrants and African Americans.
A 2007 study on assimilation found that Mexican immigrants are less fluent in English than both non-Mexican Hispanic immigrants and other immigrants. While English fluency increases with time stayed in the United States, although further improvements after the first decade are limited, Mexicans never catch up with non-Mexican Hispanics, who never catch up with non-Hispanics. The study also writes that "Even among immigrants who came to the United States before they were ﬁve years old and whose entire schooling was in the United States, those Mexican born have average education levels of 11.7 years, whereas those from other countries have average levels of education of 14.1 years." Unlike other immigrants, Mexicans have a tendency to live in communities with many other Mexicans which decreases incentives for assimilation. Correcting for this removes about half the fluency difference between Mexicans and other immigrants.
Religious diversity.
Immigration from South Asia and elsewhere has contributed to enlarging the religious composition of the United States. Islam in the United States is growing mainly due to immigration. Hinduism in the United States, Buddhism in the United States, and Sikhism in the United States are other examples.
Since 1992, an estimated 1.7 million Muslims, approximately 1 million Hindus, and approximately 1 million Buddhists have immigrated legally to the United States.
Political.
A "Boston Globe" article attributed Barack Obama's win in the 2008 U.S. Presidential election to a marked reduction over the preceding decades in the percentage of whites in the American electorate, attributing this demographic change to the Immigration Act of 1965. The article quoted Simon Rosenberg, president and founder of the New Democrat Network, as having said that the Act is "the most important piece of legislation that no one's ever heard of," and that it "set America on a very different demographic course than the previous 300 years."
Immigrants differ on their political views; however, the Democratic Party is considered to be in a far stronger position among immigrants overall. Research shows that religious affiliation can also significantly impact both their social values and voting patterns of immigrants, as well as the broader American population. Hispanic evangelicals, for example, are more strongly conservative than non-Hispanic evangelicals. This trend is often similar for Hispanics or others strongly identifying with the Catholic Church, a religion that strongly opposes abortion and gay marriage.
The key interests groups that lobby on immigration are religious, ethnic and business groups, together with some liberals and some conservative public policy organizations. Both the pro- and anti- groups affect policy.
Studies have suggested that some special interest group lobby for less immigration for their own group and more immigration for other groups since they see effects of immigration, such as increased labor competition, as detrimental when affecting their own group but beneficial when affecting other groups.
A 2007 paper found that both pro- and anti-immigration special interest groups play a role in migration policy. "Barriers to migration are lower in sectors in which business lobbies incur larger lobbying expenditures and higher in sectors where labor unions are more important." A 2011 study examining the voting of US representatives on migration policy suggests that "representatives from more skilled labor abundant districts are more likely to support an open immigration policy towards the unskilled, whereas the opposite is true for representatives from more unskilled labor abundant districts."
After the 2010 election, Gary Segura of Latino Decisions stated that Hispanic voters influenced the outcome and "may have saved the Senate for Democrats". Several ethnic lobbies support immigration reforms that would allow illegal immigrants that have succeeded in entering to gain citizenship. They may also lobby for special arrangements for their own group. The Chairman for the Irish Lobby for Immigration Reform has stated that "the Irish Lobby will push for any special arrangement it can get — 'as will every other ethnic group in the country.'" The irrendentist and ethnic separatist movements for Reconquista and Aztlán see immigration from Mexico as strengthening their cause.
The book "Ethnic Lobbies and US Foreign Policy" (2009) states that several ethnic special interest groups are involved in pro-immigration lobbying. Ethnic lobbies also influence foreign policy. The authors write that "Increasingly, ethnic tensions surface in electoral races, with House, Senate, and gubernatorial contests serving as proxy battlegrounds for antagonistic ethnoracial groups and communities. In addition, ethnic politics affect party politics as well, as groups compete for relative political power within a party". However, the authors argue that currently ethnic interest groups, in general, do not have too much power in foreign policy and can balance other special interest groups.
In a 2012 news story, "Reuters" reported, "Strong support from Hispanics, the fastest-growing demographic in the United States, helped tip President Barack Obama's fortunes as he secured a second term in the White House, according to Election Day polling."
Lately, there is talk among several Republican leaders, such as governors Bobby Jindal and Susana Martinez, of taking a new, friendlier approach to immigration. Former US Secretary of Commerce Carlos Gutierrez is promoting the creation of Republicans for Immigration Reform.
Health.
The issue of the health of immigrants and the associated cost to the public has been largely discussed. The non-emergency use of emergency rooms ostensibly indicates an incapacity to pay, yet some studies allege disproportionately lower access to unpaid health care by immigrants. For this and other reasons, there have been various disputes about how much immigration is costing the United States public health system. University of Maryland economist and Cato Institute scholar Julian Lincoln Simon concluded in 1995 that while immigrants probably pay more into the health system than they take out, this is not the case for elderly immigrants and refugees, who are more dependent on public services for survival.
Immigration from areas of high incidences of disease is thought to have fueled the resurgence of tuberculosis (TB), chagas, and hepatitis in areas of low incidence. According to Centers for Disease Control and Prevention (CDC), TB cases among foreign-born individuals remain disproportionately high, at nearly nine times the rate of U.S.-born persons. To reduce the risk of diseases in low-incidence areas, the main countermeasure has been the screening of immigrants on arrival. HIV/AIDS entered the United States in around 1969, likely through a single infected immigrant from
Haiti. Conversely, many new HIV infections in Mexico can be traced back to the United States. People infected with HIV were banned from entering the United States in 1987 by executive order, but the 1993 statute supporting the ban was lifted in 2009. The executive branch is expected to administratively remove HIV from the list of infectious diseases barring immigration, but immigrants generally would need to show that they would not be a burden on public welfare. Researchers have also found what is known as the "healthy immigrant effect", in which immigrants in general tend to be healthier than individuals born in the U.S.
Crime.
Empirical studies on links between immigration and crime are mixed.
According to the Bureau of Justice Statistics in 2001, 4% of Hispanic males in their twenties and thirties were in prison or jail, compared to 1.8% of non-Hispanic white males. Hispanic men are almost four times as likely to go to prison at some point in their lives than non-Hispanic white males, although less likely than non-Hispanic African American males.
Other writers have suggested that immigrants are under-represented in criminal statistics. In his 1999 book "Crime and Immigrant Youth", sociologist Tony Waters argued that immigrants themselves are less likely to be arrested and incarcerated; he also argued, however, that the children of some immigrant groups are more likely to be arrested and incarcerated. This is a by-product of the strains that emerge between immigrant parents living in poor, inner city neighborhoods. This occurs particularly in immigrant groups with many children as they begin to form particularly strong peer sub-cultures. A 1999 paper by John Hagan and Alberto Palloni estimated that the involvement in crime by Hispanic immigrants is less than that of other citizens. A 2006 Op-Ed in "The New York Times" by Harvard University Professor in Sociology Robert J. Sampson says that immigration of Hispanics may in fact be associated with decreased crime.
A 2006 article by Migration Policy Institute cited data from the 2000 US Census as evidence for that foreign-born men had lower incarceration rates than native-born men.
According to a 2007 report by the Immigration Policy Center, the American Immigration Law Foundation, citing data from the 2000 US Census, native-born American men between 18–39 are five times more likely to be incarcerated than immigrants in the same demographic.
A 2008 study by the Public Policy Institute of California, found that, "...on average, between 2000 and 2005, cities that had a higher share of recent immigrants (those arriving between 2000 and 2005) saw their crime rates fall further than cities with a lower share" but adds, "As with most studies, we do not have ideal data. This lack of data restricts the questions we will be able to answer. In particular, we cannot focus on the undocumented population explicitly". In a study released by the same Institute, immigrants were ten times less likely to be incarcerated than native born Americans.
Explanations for the lower incarceration rates of immigrants include:
Heather MacDonald at the Manhattan Institute in a 2004 article argued that sanctuary policies has caused large problems with crime by illegal aliens since the police cannot report them for deportation before a felony or a series of misdemeanors takes place. In Los Angeles, 95 percent of all outstanding warrants for homicide are for illegal aliens. Up to two-thirds of all fugitive felony warrants (17,000) are for illegal aliens. 60 percent of the 20,000-strong 18th Street Gang in southern California were illegal aliens in a 1995 report.
The Center for Immigration Studies in a 2009 report argued that "New government data indicate that immigrants have high rates of criminality, while older academic research found low rates. The overall picture of immigrants and crime remains confused due to a lack of good data and contrary information." It also criticized the reports by the Public Policy Institute of California and Immigration Policy Center for using data from the 2000 Census according to which 4% of prisoners were immigrants. Non-citizens often have a strong incentive to deny this in order to prevent deportation and there are also other problems. Better methods have found 20–22% immigrants. It also criticized studies looking at percentages of immigrants in a city and crime for only looking at overall crime and not immigrant crime. A 2009 analysis by the Department of Homeland Security found that crime rates were higher in metropolitan areas that received large numbers of legal immigrants, contradicting several older cross-city comparisons.
Environment.
Some commentators have suggested that increased immigration has a negative effect on the environment, especially as the level of economic development of the United States (and by extension, its energy, water and other needs that underpin its prosperity) means that the impact of a larger population is greater than what would be experienced in other countries.
Perceived heavy immigration, especially in the southwest, has led to some fears about population pressures on the water supply in some areas. California continues to grow by more than a half-million a year and is expected to reach 48 million in 2030. According to the California Department of Water Resources, if more supplies are not found by 2020, residents will face a water shortfall nearly as great as the amount consumed today. Los Angeles is a coastal desert able to support at most one million people on its own water. California is considering using desalination to solve this problem.
Education.
Scientific laboratories and startup internet opportunities have been a powerful American magnet. By 2000, 23% of scientists with a PhD in the U.S. were immigrants, including 40% of those in engineering and computers. Roughly a third of the United State's college and universities graduate students in STEM fields are foreign nationals – in some states it is well over half of their graduate students. On Ash Wednesday, March 5, 2014, the presidents of 28 Catholic and Jesuit colleges and universities, joined the "Fast for Families" movement. The "Fast for Families" movement reignited the immigration debate in the fall of 2013 when the movement's leaders, supported by many members of Congress and the President, fasted for twenty-two days on the National Mall in Washington, D.C.
A study on public schools in California found that white enrollment declined in response to increases in the number of Spanish-speaking Limited English Proficient and Hispanic students. This white flight was greater for schools with relatively larger proportions of Spanish-speaking Limited English Proficient.
Among 25 to 44-year-olds, 55% of Hispanic immigrants that arrived after age 13 had not completed high school.
Effects on African Americans.
Harvard economist George J. Borjas stated that there is controversy over the "huge redistribution [of wealth] away from [unskilled U.S. Citizen] workers to [American employers] who use illegal immigrants." He suggested that immigration had detrimental effects on African-American employment in terms of lower wages and the number of persons employed. He also reported that a 10% increase in the supply of workers reduced the black wage of that group by 2.5%, lowered the employment rate by 5.9% and increased the black incarceration rate by 1.3%.
Public opinion.
The ambivalent feeling of Americans toward immigrants is shown by a positive attitude toward groups that have been visible for a century or more, and much more negative attitude toward recent arrivals. For example a 1982 national poll by the Roper Center at the University of Connecticut showed respondents a card listing a number of groups and asked, "Thinking both of what they have contributed to this country and have gotten from this country, for each one tell me whether you think, on balance, they've been a good or a bad thing for this country," which produced the results shown in the table. "By high margins, Americans are telling pollsters it was a very good thing that Poles, Italians, and Jews emigrated to America. Once again, it's the newcomers who are viewed with suspicion. This time, it's the Mexicans, the Filipinos, and the people from the Caribbean who make Americans nervous."
In a 2002 study, which took place soon after the September 11 attacks, 55% of Americans favored decreasing legal immigration, 27% favored keeping it at the same level, and 15% favored increasing it.
In 2006, the immigration-reduction advocacy think tank the Center for Immigration Studies released a poll that found that 68% of Americans think U.S. immigration levels are too high, and just 2% said they are too low. They also found that 70% said they are less likely to vote for candidates that favor increasing legal immigration. In 2004, 55% of Americans believed legal immigration should remain at the current level or increased and 41% said it should be decreased. The less contact a native-born American has with immigrants, the more likely one would have a negative view of immigrants.
One of the most important factors regarding public opinion about immigration is the level of unemployment; anti-immigrant sentiment is where unemployment is highest, and vice versa.
Surveys indicate that the U.S. public consistently makes a sharp distinction between legal and illegal immigrants, and generally views those perceived as "playing by the rules" with more sympathy than immigrants that have entered the country illegally.
Legal issues.
Laws concerning immigration and naturalization.
Laws concerning immigration and naturalization include:
AEDPA and IIRARA exemplify many categories of criminal activity for which immigrants, including green card holders, can be deported and have imposed mandatory detention for certain types of cases.
Asylum for refugees.
In contrast to economic migrants, who generally do not gain legal admission, refugees, as defined by international law, can gain legal status through a process of seeking and receiving asylum, either by being designated a refugee while abroad, or by physically entering the United States and requesting asylum status thereafter. A specified number of legally defined refugees, who either apply for asylum overseas or after arriving in the U.S., are admitted annually. Refugees compose about one-tenth of the total annual immigration to the United States, though some large refugee populations are very prominent. In the years 2005 through 2007, the number of asylum seekers accepted into the U.S. was about 40,000 per year. This compared with about 30,000 per year in the UK and 25,000 in Canada. Japan accepted just 41 refugees for resettlement in 2007.
Since 1975, more than 1.3 million refugees from Asia have been resettled in the United States. Since 2000 the main refugee-sending regions have been Somalia, Liberia, Sudan, and Ethiopia. The ceiling for refugee resettlement for fiscal year 2008 was 80,000 refugees. The United States expected to admit a minimum of 17,000 Iraqi refugees during fiscal year 2009. The U.S. has resettled more than 42,000 Bhutanese refugees from Nepal since 2008.
In fiscal year 2008, the Office of Refugee Resettlement (ORR) appropriated over $655 million for long-term services provided to refugees after their arrival in the US. The Obama administration has kept to about the same level.
Miscellaneous documented immigration.
In removal proceedings in front of an immigration judge, cancellation of removal is a form of relief that is available for certain long-time residents of the United States. It allows a person being faced with the threat of removal to obtain permanent residence if that person has been physically present in the U.S. for at least ten years, has had good moral character during that period, has not been convicted of certain crimes, and can show that removal would result in exceptional and extremely unusual hardship to his or her U.S. citizen or permanent resident spouse, children, or parent. This form of relief is only available when a person is served with a Notice to Appear to appear in the proceedings in the court.
Members of Congress may submit private bills granting residency to specific named individuals. A special committee vets the requests, which require extensive documentation. The Central Intelligence Agency has the statutory authority to admit up to one hundred people a year outside of normal immigration procedures, and to provide for their settlement and support. The program is called "PL110", named after the legislation that created the agency, Public Law 110, the Central Intelligence Agency Act.
Illegal immigration.
The Illegal immigrant population of the United States is estimated to be between 7 and 20 million. The majority of the illegal immigrants are from Mexico.
In a 2011 news story, "Los Angeles Times" reported, "The annual report, relied upon by both sides in the contentious immigration debate, found 11.2 million illegal immigrants living in the U.S., statistically identical to the 11.1 million estimated in 2009. ... The report also found that illegal immigrants in 2010 were parents of 5.5 million children, 4.5 million of whom were born in the U.S. and are citizens. Because illegal immigrants are younger and more likely to be married, they represented a disproportionate share of births — 8% of the babies born in the U.S. between March 2009 and March 2010 were to at least one illegal immigrant parent."
In June 2012, President Obama issued a memorandum instructing officers of the federal government to defer deporting young illegal immigrants who had been brought to the U.S. as children. Obama's new non-deportation policy allows 1.7 million illegal immigrants to apply for the temporary right to live and work in the United States. The memorandum is the move by the Obama administration to use its executive powers to revise immigration procedures without changing the law. Beginning March 4, 2013, illegal immigrants who can show that time apart from a U.S. spouse, child or parent would create "extreme hardship" can apply for legal visas without leaving the U.S.
On 25 November 2013, Ju Hong, a 24-year-old South Korean immigrant without legal documentation, shouted at Obama to use his executive power to stop deportation of illegal immigrants. Obama said "If, in fact, I could solve all these problems without passing laws in Congress, then I would do so." "But we're also a nation of laws, that's part of our tradition," he continued. "And so the easy way out is to try to yell and pretend like I can do something by violating our laws. And what I'm proposing is the harder path, which is to use our democratic processes to achieve the same goal."
On 20 November 2014, President Obama announced a set of executive actions which could allow up to an estimated 45% of undocumented immigrants to legally stay and work in the United States. Although not without precedent under prior presidents, these actions do amount to at least a change in tactics on the part of the Obama administration. In his announcement, the president said he still hoped for "a more permanent legislative solution" that would make his actions "no longer necessary."
Immigration in popular culture.
The history of immigration to the United States is the history of the country itself, and the journey from beyond the sea is an element found in American folklore, appearing over and over again in everything from "The Godfather" to "Gangs of New York" to "The Song of Myself" to Neil Diamond's "America" to the animated feature "An American Tail".
From the 1880s to the 1910s, vaudeville dominated the popular image of immigrants, with very popular caricature portrayals of ethnic groups. The specific features of these caricatures became widely accepted as accurate portrayals.
In "The Melting Pot" (1908), playwright Israel Zangwill (1864–1926) explored issues that dominated Progressive Era debates about immigration policies. Zangwill's theme of the positive benefits of the American melting pot resonated widely in popular culture and literary and academic circles in the 20th century; his cultural symbolism – in which he situated immigration issues – likewise informed American cultural imagining of immigrants for decades, as exemplified by Hollywood films.
The popular culture's image of ethnic celebrities often includes stereotypes about immigrant groups. For example, Frank Sinatra's public image as a superstar contained important elements of the "American Dream" while simultaneously incorporating stereotypes about Italian Americans that were based in nativist and Progressive responses to immigration.
The process of assimilation has been a common theme of popular culture. For example, "lace-curtain Irish" refers to middle-class Irish Americans desiring assimilation into mainstream society in counterpoint to the older, more raffish "shanty Irish". The occasional malapropisms and left-footed social blunders of these upward mobiles were gleefully lampooned in vaudeville, popular song, and the comic strips of the day such as "Bringing Up Father", starring Maggie and Jiggs, which ran in daily newspapers for 87 years (1913 to 2000). In "The Departed" (2006), Staff Sergeant Dignam regularly points out the dichotomy between the lace curtain Irish lifestyle Billy Costigan enjoyed with his mother, and the shanty Irish lifestyle of Costigan's father. In recent years the popular culture has paid special attention to Mexican immigration and the film "Spanglish" (2004) tells of a friendship of a Mexican housemaid (Paz Vega) and her boss played by Adam Sandler.
Immigration in literature.
Novelists and writers have captured much of the color and challenge in their immigrant lives through their writings.
Regarding Irish women in the 19th century, there were numerous novels and short stories by Harvey O'Higgins, Peter McCorry, Bernard O'Reilly and Sarah Orne Jewett that emphasize emancipation from Old World controls, new opportunities and expansiveness of the immigrant experience.
On the other hand Hladnik studies three popular novels of the late 19th century that warned Slovenes not to immigrate to the dangerous new world of the United States.
Jewish American writer Anzia Yezierska wrote her novel "Bread Givers" (1925) to explore such themes as Russian-Jewish immigration in the early 20th century, the tension between Old and New World Yiddish culture, and women's experience of immigration. A well established author Yezierska focused on the Jewish struggle to escape the ghetto and enter middle- and upper-class America. In the novel, the heroine, Sara Smolinsky, escape from New York City's "down-town ghetto" by breaking tradition. She quits her job at the family store and soon becomes engaged to a rich real-estate magnate. She graduates college and takes a high-prestige job teaching public school. Finally Sara restores her broken links to family and religion.
The Swedish author Vilhelm Moberg in the mid-20th century wrote a series of four novels describing one Swedish family's migration from Småland to Minnesota in the late 19th century, a destiny shared by almost one million people. The author emphasizes the authenticity of the experiences as depicted (although he did change names). These novels have been translated into English ("The Emigrants", 1951, "Unto a Good Land", 1954, "The Settlers", 1961, "The Last Letter Home", 1961). The musical Kristina från Duvemåla by ex-ABBA members Björn Ulvaeus and Benny Andersson is based on this story.
"The Immigrant" is a musical by Steven Alper, Sarah Knapp, and Mark Harelik. The show is based on the story of Harelik's grandparents, Matleh and Haskell Harelik, who traveled to Galveston, Texas in 1909.
Documentary films.
In their documentary "", filmmakers Shari Robertson and Michael Camerini examine the American political system through the lens of immigration reform from 2001 to 2007. Since the debut of the first five films, the series has become an important resource for advocates, policy-makers and educators.
That film series premiered nearly a decade after the filmmakers' landmark documentary film "Well-Founded Fear" which provided a behind-the-scenes look at the process for seeking asylum in the United States. That film still marks the only time that a film-crew was privy to the private proceedings at the U.S. Immigration and Naturalization Service (INS), where individual asylum officers ponder the often life-or-death fate of immigrants seeking asylum.
Legal perspectives.
University of North Carolina law professor Hiroshi Motomura has identified three approaches the United States has taken to the legal status of immigrants in his book "Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States". The first, dominant in the 19th century, treated immigrants as in transition; in other words, as prospective citizens. As soon as people declared their intention to become citizens, they received multiple low-cost benefits, including the eligibility for free homesteads in the Homestead Act of 1869, and in many states, the right to vote. The goal was to make the country more attractive, so large numbers of farmers and skilled craftsmen would settle new lands. By the 1880s, a second approach took over, treating newcomers as "immigrants by contract". An implicit deal existed where immigrants who were literate and could earn their own living were permitted in restricted numbers. Once in the United States, they would have limited legal rights, but were not allowed to vote until they became citizens, and would not be eligible for the New Deal government benefits available in the 1930s. The third and more recent policy is "immigration by affiliation", which Motomura argues is the treatment which depends on how deeply rooted people have become in the country. An immigrant who applies for citizenship as soon as permitted, has a long history of working in the United States, and has significant family ties, is more deeply affiliated and can expect better treatment.
It has been suggested that the US should adopt policies similar to those in Canada and Australia and select for desired qualities such as education and work experience. Another suggestion is to reduce legal immigration because of being a relative, except for nuclear family members, since such immigrations of extended relatives, who in turn bring in their own extended relatives, may cause a perpetual cycle of "chain immigration".
Interpretive perspectives.
The American Dream is the belief that through hard work and determination, any United States immigrant can achieve a better life, usually in terms of financial prosperity and enhanced personal freedom of choice. According to historians, the rapid economic and industrial expansion of the U.S. is not simply a function of being a resource rich, hard working, and inventive country, but the belief that anybody could get a share of the country's wealth if he or she was willing to work hard. This dream has been a major factor in attracting immigrants to the United States.

</doc>
<doc id="15052" url="http://en.wikipedia.org/wiki?curid=15052" title="Image and Scanner Interface Specification">
Image and Scanner Interface Specification

Image and Scanner Interface Specification (ISIS) is an industry standard interface for image scanning technologies, developed by Pixel Translations in 1990 (today: EMC captiva).
ISIS is an open standard for scanner control and a complete image-processing framework. It is currently supported by a number of application and scanner vendors.
Functions.
The modular design allows the scanner to be accessed both directly or with built-in routines to handle most situations automatically.
A message-based interface with tags is used so that features, operations, and formats not yet supported by ISIS can be added as desired without waiting for a new version of the specification.
The standard addresses all of the issues that an application using a scanner needs to be concerned with. Functions include but are not limited to selecting, installing, and configuring a new scanner; setting scanner-specific parameters; scanning, reading and writing files, and fast image scaling, rotating, displaying, and printing. Drivers have been written to dynamically process data for operations such as converting grayscale to binary image data.
An ISIS interface can run scanners at or above their rated speed by linking drivers together in a pipe so that data flows from a scanner driver to compression driver, to packaging driver, to a file, viewer, or printer in a continuous stream, usually without the need to buffer more than a small portion of the full image. As a result of using the piping method, each driver can be optimised to perform one function well. Drivers are typically small and modular in order to make it simple to add new functionality to an existing application.

</doc>
<doc id="15053" url="http://en.wikipedia.org/wiki?curid=15053" title="Ivo Caprino">
Ivo Caprino

Ivo Caprino (17 February 1920 – 8 February 2001) was a Norwegian film director and writer, best known for his puppet films. His most famous film is "Flåklypa Grand Prix" ("Pinchcliffe Grand Prix"), made in 1975.
Early career.
In the mid-1940s, Caprino helped his mother design puppets for a puppet theatre, which inspired him to try making a film using his mother's designs. The result of their collaboration was "Tim og Tøffe", an 8-minute film released in 1949 . Several films followed in the next couple of years, including two 15-minute shorts that are still shown regularly in Norway today, "Veslefrikk med Fela" (Little Freddy and his Fiddle), based on a Norwegian folk tale, and "Karius og Baktus", a story by Thorbjørn Egner of two little trolls, representing Caries and Bacterium, living in a boy's teeth. Ingeborg Gude made the puppets for these films as well, as she would continue to do up until her death in the mid sixties.
When making "Tim og Tøffe", Caprino invented an ingenious method for controlling the puppet's movements in real time. The technique can be described as a primitive, mechanical version of animatronics.
Caprino's films received rave reviews, and he quickly became a celebrity in Norway. In particular, the public were fascinated with the secret technology used to make his films. When he switched to traditional stop motion, Caprino tried to maintain the impression that he was still using some kind of "magic" technology to make the puppets move, even though all his later films were made with traditional stop motion techniques.
In addition to the short films, Caprino produced dozens of advertising films with puppets. In 1959, he directed a live action feature film, "Ugler i Mosen", which also contained stop motion sequences. He then embarked on his most ambitious project, a feature film about Peter Christen Asbjørnsen, who travelled around Norway in the 19th century collecting traditional folk tales. The plan was to use live action for the sequences showing Asbjørnsen, and then to realise the folk tales using stop motion. Unfortunately, Caprino was unable to secure funding for the project, so he ended up making the planned folk tale sequences as separate 16-minute puppet films, bookended by live action sequences showing Asbjørnsen.
"The Pinchcliffe Grand Prix".
In 1970, Caprino and his small team of collaborators, started work on a 25 minutes TV special, which would eventually become "The Pinchcliffe Grand Prix". Based on a series of books by Norwegian cartoonist and author Kjell Aukrust, it featured a group of eccentric characters all living in the small village of Pinchcliffe. The TV special was a collection of sketches based on Aukrust's books, with no real story line. After 1.5 years of work, it was decided that it didn't really work as a whole, so production on the TV special was stopped (with the exception of some very short clips, no material from it has ever been seen by the public), and Caprino and Aukrust instead wrote a screenplay for a feature film using the characters and environments that had already been built.
The result was "The Pinchcliffe Grand Prix", which stars Theodore Rimspoke (No. Reodor Felgen) and his two assistants, Sonny Duckworth (No. Solan Gundersen), a cheerful and optimistic bird, and Lambert (No. Ludvig), a nervous, pessimistic and melancholic hedgehog. Theodore works as a bicycle repairman, though he spends most of his time inventing weird Rube Goldberg-like contraptions. One day, the trio discover that one of Theodore's former assistants, Rudolph Gore-Slimey (Norwegian: "Rudolf Blodstrupmoen"), has stolen his design for a race car engine, and has become a world champion Formula One driver.
Sonny secures funding from an Arab oil sheik who happens to be vacationing in Pinchcliffe, and the trio then build a gigantic racing car, "Il Tempo Gigante" – a fabulous construction with two engines, radar and its own blood bank. Theodore then enters a race, and ends up winning, beating Gore-Slimey despite his attempts at sabotage.
The film was made in 3.5 years by a team of approximately 5 people. Caprino directed and animated, Bjarne Sandemose (Caprino's principal collaborator throughout his career) built the sets and the cars, and was in charge of the technical side, Ingeborg Riiser modeled the puppets and Gerd Alfsen made the costumes and props.
When it came out in 1975, The Pinchcliffe Grand Prix was an enormous success in Norway, selling 1 million tickets in its first year of release. It remains the biggest box office hit of all time in Norway (Caprino Studios claim it has sold 5.5 million tickets to date) and was also released in many other countries.
To help promote the film abroad, Caprino and Sandemose built a full-scale replica of Il Tempo Gigante that is legal for public roads, but is usually exposited at Hunderfossen Familiepark.
Later career.
Except for some TV work in the late seventies, Caprino made no more puppet films, focusing instead on creating attractions for the "Hunderfossen" theme park outside Lillehammer based on his folk tale movies, and making tourist films using a custom built multi camera setup of his own design that shoots 280 degrees panorama movies.
Personal life.
Caprino was the son of furniture designer Mario Caprino and the artist Ingeborg Gude, who was a granddaughter of the painter Hans Gude. He was born and died in Oslo, but lived all of his life at Snarøya in Bærum. He died in 2001 after having lived several years with a cancer diagnosis. Since Caprino's death, his son Remo has had great success developing a computer game based on "Flåklypa Grand Prix".

</doc>
<doc id="15054" url="http://en.wikipedia.org/wiki?curid=15054" title="Intel 80286">
Intel 80286

The Intel 80286 (also marketed as the iAPX 286 and often called Intel 286) is a 16-bit microprocessor that was introduced on 1 February 1982. It was the first 8086 based CPU with separate, non-multiplexed, address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186, it could correctly execute most software written for the earlier Intel 8086 and 8088 processors. 
The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.
History and performance.
After the 6 and 8 MHz initial releases, Intel subsequently scaled it up to 12.5 MHz. AMD and Harris later pushed the architecture to 20 MHz and 25 MHz, respectively. Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery powered devices.
On average, the 80286 was reportedly measured to have a speed of about 0.21 instructions per clock on "typical" programs, although it could be significantly faster on optimized code and in tight loops as many instructions could execute in 2 clock cycles. The 6 MHz, 10 MHz and 12 MHz models were reportedly measured to operate at 0.9 MIPS, 1.5 MIPS and 2.66 MIPS respectively.
The later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones).
Architecture.
The 80286 was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: address unit, bus unit, instruction unit and execution unit organized into a loosely coupled (buffered) pipeline just as in the 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address calculation hardware (most importantly a dedicated adder) and a faster (more hardware based) multiplier. It was produced in a 68-pin package including PLCC (Plastic Leaded Chip Carrier), LCC (Leadless chip carrier) and PGA (Pin Grid Array) packages.
The performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubling, at the same clock speed). This was a large increase in performance over the 8086, fully comparable to the large generational speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286 while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.
The intel 80286 had a 24-bit address bus and was able to address up to 16 MB of RAM, compared to 1 MB for its predecessor. However cost and initial rarity of software using the memory above 1 MB meant that 80286 computers were rarely shipped with more than one megabyte of RAM. Additionally, there was a performance penalty involved in accessing extended memory from real mode, as noted below.
Features.
Protected mode.
The 286 was the first of the x86 CPU family to support protected mode. In addition, it was the first commercially-available microprocessor with on-chip MMU capabilities. (Systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller.) This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market.
Several additional instructions were introduced in protected mode of 80286, which are helpful for multitasking operating systems.
Another important feature of 80286 is Prevention of Unauthorized Access. This is achieved by:
In 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers:
By design, the 286 could not revert from protected mode to the basic 8086-compatible "real mode" without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry as well as specialized code in the ROM BIOS to enable special series of program instructions to cause the reset, allowing real-mode reentry (while retaining active memory and control). Though it worked correctly, the method imposed a huge performance penalty.
In theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules were followed; however, as many DOS programs broke those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode.
OS support.
The protected mode of the 80286 was not utilized until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A DOS could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory. The difficulty lay in the incompatibility of older real mode DOS programs with protected mode. They simply could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs were not allowed to access or modify data and code segments that did not belong to them, as they could in real mode. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly but would still run a majority of the old programs. The 286 also did not provide a significant enough performance advantage over the 8086 to justify supporting its capabilities. Registers were still 16-bit and when put into protected mode, the programmer was forced to use a memory map composed of 64k segments just like in real mode. 
In January 1985, Digital Research previewed the Concurrent DOS 286 operating system made in cooperation with Intel. The product would function strictly as an 80286 native mode operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation. This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip. In August, after extensive testing E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster, incorporated into the E-2 step. Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and Point-of-Sale terminals in 1986. The same limitations affected Digital Research's FlexOS 286 version 1.0, a derivation of Concurrent DOS 286, developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS.
The problems led to Bill Gates famously referring to the 80286 as a "brain dead chip", since it was clear that the new Microsoft Windows environment would not be able to run multiple MS-DOS applications with the 286. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode).
Other operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984), Coherent, and Minix. These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS programs. In its successor 80386 chip, Intel enhanced the protected mode to address more memory, but also added the separate virtual 8086 mode, which had much better MS-DOS compatibility, in order to satisfy the diverging needs of the market.
Moving 16 unaligned bits from the last byte of a segment is impossible in a single instruction (such as MOV AX,[$FFFF]). The reason is that, due to wrap around of the offset (FFFF), the processor would read 1 byte from the last byte of the segment and then 1 byte from the start of the segment. This has been stumbled upon by some assembly-language programmers, such as the team at Core Design who came across this during the conversion of Thunderhawk from Amiga (32-bit CPU) to the PC (16-bit 8086).

</doc>
<doc id="15055" url="http://en.wikipedia.org/wiki?curid=15055" title="Ivanhoe">
Ivanhoe

Ivanhoe is a historical novel by Sir Walter Scott, first published in 1820 in three volumes and subtitled "A Romance". "Ivanhoe", set in 12th-century England, is sometimes credited for increasing interest in romance and medievalism; John Henry Newman claimed Scott "had first turned men's minds in the direction of the Middle Ages", while Carlyle and Ruskin made similar assertions of Scott's overwhelming influence over the revival based primarily on the publication of this novel.
Plot introduction.
"Ivanhoe" is the story of one of the remaining Saxon noble families at a time when the nobility in England was overwhelmingly Norman. It follows the Saxon protagonist, Wilfred of Ivanhoe, who is out of favour with his father for his allegiance to the Norman king, Richard I. The story is set in 1194, after the failure of the Third Crusade, when many of the Crusaders were still returning to their homes in Europe. King Richard, who had been captured by Leopold of Austria on his return journey to England, was believed to be still in captivity.
The legendary Robin Hood, initially under the name of Locksley, is also a character in the story, as are his "merry men". The character that Scott gave to Robin Hood in "Ivanhoe" helped shape the modern notion of this figure as a cheery noble outlaw.
Other major characters include Ivanhoe's intractable father, Cedric, one of the few remaining Saxon lords; various Knights Templar, most notable of which is Brian de Bois-Guilbert, Ivanhoe's main rival; a number of clergymen; the loyal serfs Gurth the swineherd and the jester Wamba, whose observations punctuate much of the action; and the Jewish moneylender, Isaac of York, who is equally passionate about his people and his beautiful (Jewish) daughter, Rebecca. The book was written and published during a period of increasing struggle for the emancipation of the Jews in England, and there are frequent references to injustices against them.
Plot summary.
Opening.
Wilfred of Ivanhoe is disinherited by his father Cedric of Rotherwood for supporting the Norman King Richard and for falling in love with the Lady Rowena, Cedric's ward and a descendant of the Saxon Kings of England. Cedric had planned to marry her to the powerful Lord Aethelstane, pretender to the Crown of England through his descent from the last Saxon King, Harold Godwinson, thus cementing a Saxon political alliance between two rivals for the same claim. Ivanhoe accompanies King Richard on the Crusades, where he is said to have played a notable role in the Siege of Acre by enduring with great fortitude the privations of life in the city and Christian camp after their containment by Saladin; Ivanhoe also tends to Louis of Thuringia who suffers from malaria.
The book opens with a scene of Norman knights and prelates seeking the hospitality of Cedric. They are guided there by a palmer, who has recently returned from the Holy Land. The same night, seeking refuge from inclement weather and bandits, Isaac of York, a Jewish moneylender, arrives at Rotherwood. Following the night's meal, the palmer observes one of the Normans, the Templar Brian de Bois-Guilbert, issue orders to his Saracen soldiers to follow Isaac of York after he leaves Rotherwood in the morning and take him captive to a noble's castle.
The palmer then warns the moneylender of his peril and assists in his escape from Rotherwood. The swineherd Gurth refuses to open the gates until the palmer whispers a few words in his ear, which turns Gurth as helpful as he was recalcitrant earlier. This is but one of the many mysterious incidents that occur throughout the book.
Isaac of York offers to repay his debt to the palmer by offering him a suit of armour and a war horse to participate in the tournament at Ashby-de-la-Zouch, where he was bound. He makes the offer on his inference that the palmer was in reality a knight, having observed his knight's chain and spurs (a fact that he mentions to the palmer). The palmer is taken by surprise but accepts the offer.
The tournament.
The story then moves to the scene of the tournament, which is presided over by Prince John, King Richard's younger brother. Other characters in attendance are Cedric, Aethelstane, Lady Rowena, Isaac of York, his daughter Rebecca, Robin of Locksley and his men, Prince John's advisor Waldemar Fitzurse, and numerous Norman knights.
On the first day of the tournament, a bout of individual jousting, a mysterious masked knight, identifying himself only as "Desdichado" (which is described in the book as Spanish for the "Disinherited One", though actually meaning "Unfortunate"), makes his appearance and manages to defeat some of the best Norman lances, including Bois-Guilbert, Maurice de Bracy, a leader of a group of "Free Companions" (mercenary knights), and the baron Reginald Front-de-Boeuf. The masked knight declines to reveal himself despite Prince John's request, but is nevertheless declared the champion of the day and is permitted to choose the Queen of the Tournament. He bestows this honour upon the Lady Rowena.
On the second day, which is a melee, Desdichado is chosen to be leader of one party. Most of the leading knights of the realm, however, flock to the opposite standard under which Desdichado's vanquished opponents fought. Desdichado's side is soon hard pressed and he himself beset by multiple foes, when a knight who had until then taken no part in the battle, thus earning the sobriquet Le Noir Faineant (or the Black Sluggard), rides to Desdichado's rescue. The rescuing knight, having evened the odds by his action, then slips away. Though Desdichado was instrumental in the victory, Prince John, being displeased with his behaviour of the previous day, wishes to bestow his accolades on the vanished Black Knight. Since the latter has departed, he is forced to declare Desdichado the champion. At this point, being forced to unmask himself to receive his coronet, Desdichado is revealed to be Wilfred of Ivanhoe himself, returned from the Crusades. This causes much consternation to Prince John and his court who now fear the imminent return of King Richard.
Because he is severely wounded in the competition and because Cedric refuses to have anything to do with him, Ivanhoe is taken into the care of Rebecca, the beautiful daughter of Isaac, who is a skilled healer. She convinces her father to take him with them to York, where he can be best treated. The story then goes over the conclusion of the tournament including feats of archery by Locksley.
Capture and rescue.
Meanwhile, de Bracy finds himself infatuated with the Lady Rowena and, with his companions-in-arms, makes plans to abduct her. In the forests between Ashby and York, the Lady Rowena, Cedric and Aethelstane encounter Isaac, Rebecca and the wounded Ivanhoe, who have been abandoned by their servants for fear of bandits. The Lady Rowena, in response to the requests of Isaac and Rebecca, urges Cedric to take the group under his protection to York. Cedric, unaware that the wounded man is his son, agrees. En route, the party is captured by de Bracy and his companions and taken to Torquilstone, the castle of Front-de-Boeuf. However, the swineherd Gurth, who had run away from Rotherwood to serve Ivanhoe as squire at the tournament and who was recaptured by Cedric when Ivanhoe was identified, manages to escape.
The Black Knight, having taken refuge for the night in the hut of a local friar, the Holy Clerk of Copmanhurst, volunteers his assistance on learning about the captives from Robin of Locksley, who had come to rouse the friar for an attempt to free them. They then besiege the Castle of Torquilstone with Robin's own men, including the friar and assorted Saxon yeomen whom they had managed to raise due to their hatred of Front-de-Boeuf and his neighbour, Philip de Malvoisin.
At Torquilstone, de Bracy expresses his love for the Lady Rowena, but is refused. In the meantime, Brian de Bois-Guilbert, who had accompanied de Bracy on the raid, takes Rebecca for his captive, and tries to force his attentions on her, which are rebuffed. Front-de-Boeuf, in the meantime, tries to wring a hefty ransom, by torture over a fire, from Isaac of York. However, Isaac refuses to pay a farthing unless his daughter is freed from her Templar captor.
When the besiegers deliver a note to yield up the captives, their Norman captors retort with a message for a priest to administer the Final Sacrament to the captives. It is then that Cedric's jester Wamba slips in disguised as a priest, and takes the place of Cedric, who then escapes and brings important information to the besiegers on the strength of the garrison and its layout.
Then follows an account of the storming of the castle. Front-de-Boeuf is killed while de Bracy surrenders to the Black Knight, who identifies himself as King Richard. Showing mercy, he releases de Bracy. Bois-Guilbert escapes with Rebecca while Isaac is released from his underground dungeon by the Clerk of Copmanhurst. The Lady Rowena is saved by Cedric, while the still-wounded Ivanhoe is rescued from the burning castle by King Richard. In the fighting, Aethelstane is wounded and believed by all to be killed while attempting to rescue Rebecca, whom he mistakes for Rowena.
Rebecca's trial and Ivanhoe's reconciliation.
Following the battle, Locksley plays host to King Richard. Word is also conveyed by de Bracy to Prince John of the King's return and the fall of Torquilstone. In the meantime, Bois-Guilbert rushes with his captive to the nearest Templar Preceptory, which is under his friend Albert de Malvoisin, expecting to be able to flee the country. However, Lucas de Beaumanoir, the Grand-Master of the Templars is unexpectedly present there. He takes umbrage at Bois-Guilbert's sinful passion, which is in violation of his Templar vows; and decides to subject Rebecca, who he thinks has cast a spell on Bois-Guilbert, to a trial for witchcraft. She is found guilty through a flawed trial, but claims the right to trial by combat. Bois-Guilbert, who had hoped to fight as her champion incognito, is devastated when the Grand-Master orders him to fight against Rebecca's champion. Rebecca then writes to her father to procure a champion for her.
Meanwhile Cedric organises Aethelstane's funeral at Coningsburgh, in the midst of which the Black Knight arrives with a companion. Cedric, who had not been present at Locksley's carousal, is ill-disposed towards the knight upon learning his true identity. However, King Richard calms Cedric and reconciles him with his son, convincing him to agree to the marriage of Ivanhoe and Rowena. During this conversation, Aethelstane emerges – not dead, but having been laid in his coffin alive by avaricious monks desirous of the funeral money. Over Cedric's renewed protests, Aethelstane pledges his homage to the Norman King Richard and urges Cedric to marry Rowena to Ivanhoe; to which Cedric finally agrees.
Soon after this reconciliation, Ivanhoe receives word from Isaac beseeching him to fight on Rebecca's behalf. Upon arriving at the scene of the witch-burning, Ivanhoe engages Brian de Bois-Guilbert in single combat and goes down along with his mount, but the Templar reels in the saddle and falls from his horse. Ivanhoe recovers to put his foot on Bois-Guilbert's chest but does not kill him. The Templar has suffered a seizure and died "a victim to the violence of his own contending passions", which is pronounced by the Grand Master as the judgement of God and proof of Rebecca's innocence. King Richard, who had left Kyningestun soon after Ivanhoe's departure, arrives at the Templar Preceptory, banishes the Templars and declares that the Malvoisins' lives are forfeit for having aided in the plots against him.
Fearing further persecution, Rebecca and her father leave England for Granada. Before leaving, Rebecca comes to bid Rowena a fond farewell. Finally, Ivanhoe and Rowena marry and live a long and happy life together, though the final paragraphs of the book note that Ivanhoe's long service ended with the death of King Richard.
Characters.
Wilfred of Ivanhoe, the titular character, is a knight and son of Cedric the Saxon. Ivanhoe, though of a more noble lineage than some of the other characters, represents a middling individual in the medieval class system who is not exceptionally outstanding in his abilities, as is expected of other quasi-historical fictional characters, such as the Greek heroes. Critic Georg Lukács points to middling main characters like Ivanhoe in Sir Walter Scott's other novels as one of the primary reasons Scott's historical novels depart from previous historical works and better explore social and cultural history.
Style.
Critics of the novel have treated it as a romance intended mainly to entertain boys. "Ivanhoe" maintains many of the elements of the Romance genre, including the quest, a chivalric setting, and the overthrowing of a corrupt social order in order to bring on a time of happiness. Other critics assert that the novel creates a realistic and vibrant story, idealising neither the past nor its main character.
Themes.
Scott treats themes similar to those of some of his earlier novels, like "Rob Roy" and "The Heart of Midlothian", examining the conflict between heroic ideals and modern society. In the latter novels, industrial society becomes the centre of this conflict as the backward Scottish nationalists and the "advanced" English have to arise from chaos to create unity. Similarly, the Normans in "Ivanhoe", who represent a more sophisticated culture, and the Saxons, who are poor, disenfranchised, and resentful of Norman rule, band together and begin to mould themselves into one people. The conflict between the Saxons and Normans focuses on the losses both groups must experience before they can be reconciled and thus forge a united England. The particular loss is in the extremes of their own cultural values, which must be disavowed in order for the society to function. For the Saxons, this value is the final admission of the hopelessness of the Saxon cause. The Normans must learn to overcome the materialism and violence in their own codes of chivalry. Ivanhoe and Richard represent the hope of reconciliation for a unified future.
Allusions to real history and geography.
The location of the novel is centred upon South Yorkshire and North Nottinghamshire in England. Castles mentioned within the story include Ashby de la Zouch Castle (now a ruin in the care of English Heritage), York (though the mention of Clifford's Tower, likewise an extant English Heritage property, is anachronistic, it not having been called that until later after various rebuilds) and 'Coningsburgh', which is based upon Conisbrough Castle, in the ancient town of Conisbrough near Doncaster (the castle also being a popular English Heritage site). Reference is made within the story to the York Minster, where the climactic wedding takes place, and to the Bishop of Sheffield, although the Diocese of Sheffield was not founded until 1914. Such references suggest that Robin Hood lived or travelled in the region.
Conisbrough is so dedicated to the story of "Ivanhoe" that many of its streets, schools, and public buildings are named after characters from the book.
Lasting influence on the Robin Hood legend.
Our modern conception of Robin Hood as a cheerful, decent, patriotic rebel owes much to "Ivanhoe".
"Locksley" becomes Robin Hood's title in the Scott novel, and it has been used ever since to refer to the fictional outlaw. Scott appears to have taken the name from an anonymous manuscript – written in 1600 – that employs "Locksley" as an epithet for Robin Hood. Owing to Scott's decision to make use of the manuscript, Robin Hood from Locksley has been transformed for all time into "Robin of Locksley", alias Robin Hood. (There is, incidentally, a village called Loxley in Yorkshire.)
Scott makes the 12th-century's Saxon-Norman conflict a major theme in his novel. Recent re-tellings of the story retain his emphasis. Scott also shunned the late 16th-century depiction of Robin as a dispossessed nobleman (the Earl of Huntingdon). This, however, has not prevented Scott from making an important contribution to the noble-hero strand of the legend, too, because some subsequent motion picture treatments of the Robin Hood's adventures give Robin traits that are characteristic of Ivanhoe as well. The most notable Robin Hood films are the lavish Douglas Fairbanks 1922 silent film, the 1938 triple Academy Award winning "Adventures of Robin Hood" with Errol Flynn as Robin (which contemporary reviewer Frank Nugent links specifically with "Ivanhoe"), and the 1991 box-office success "" with Kevin Costner). There is also the Mel Brooks spoof, . In most versions of Robin Hood, both Ivanhoe and Robin, for instance, are returning Crusaders. They have quarreled with their respective fathers, they are proud to be Saxons, they display a highly evolved sense of justice, they support the rightful king even though he is of Norman-French ancestry, they are adept with weapons, and they each fall in love with a "fair maid" (Rowena and Marian, respectively).
This particular time-frame was popularised by Scott. He borrowed it from the writings of the 16th-century chronicler John Mair or a 17th-century ballad presumably to make the plot of his novel more gripping. Medieval balladeers had generally placed Robin about two centuries later in the reign of Edward I, II or III.
Robin's familiar feat of splitting his competitor's arrow in an archery contest appears for the first time in "Ivanhoe".
Historical accuracy.
The general political events depicted in the novel are relatively accurate; the novel tells of the period just after King Richard's imprisonment in Austria following the Crusade and of his return to England after a ransom is paid. Yet the story is also heavily fictionalised. Scott himself acknowledged that he had taken liberties with history in his "Dedicatory Epistle" to "Ivanhoe". Modern readers are cautioned to understand that Scott's aim was to create a compelling novel set in a historical period, not to provide a book of history.
There has been criticism of Scott's portrayal of the bitter extent of the "enmity of Saxon and Norman, represented as persisting in the days of Richard" as "unsupported by the evidence of contemporary records that forms the basis of the story." However, Scott may have intended to suggest parallels between the Norman conquest of England, about 130 years previously, and the prevailing situation in Scott's native Scotland (Scotland's union with England in 1707 – about the same length of time had elapsed before Scott's writing and the resurgence in his time of Scottish nationalism evidenced by the cult of Robert Burns, the famous poet who deliberately chose to work in Scots vernacular though he was an educated man and spoke modern English eloquently). Indeed, some experts suggest that Scott deliberately used "Ivanhoe" to illustrate his own combination of Scottish patriotism and pro-British Unionism.
The novel generated a new name in English – Cedric. The original Saxon name had been "Cerdic" but Sir Walter misspelled it – an example of metathesis. "It is not a name but a misspelling," said satirist H. H. Munro.
In 1194 England, it would have been unlikely for Rebecca to face the threat of being burned at the stake on charges of witchcraft. It is thought that it was shortly afterwards, from the 1250s, that the Church began to undertake the finding and punishment of witches and death did not become the usual penalty until the 15th century. Even then, the form of execution used for witches in England (unlike Scotland and Continental Europe) was hanging, burning being reserved for those also convicted of treason. There are various minor errors e.g. the description of the tournament at Ashby owes more to the 14th century, and most of the coins mentioned by Scott are exotic.
"For a [Scottish] writer whose early novels [all set in Scotland] were prized for their historical accuracy, Scott was remarkably loose with the facts when he wrote Ivanhoe... But it is crucial to remember that Ivanhoe, unlike the Waverly books, is entirely a romance. It is meant to please, not to instruct, and is more an act of imagination than one of research. Despite this fancifulness, however, Ivanhoe does make some prescient historical points. The novel is occasionally quite critical of King Richard, who seems to love adventure more than he loves the well-being of his subjects. This criticism did not match the typical idealised, romantic view of Richard the Lion-Hearted that was popular when Scott wrote the book, and yet it accurately echoes the way King Richard is often judged by historians today."
It has been conjectured that the character of Rebecca in the book was inspired by Rebecca Gratz, a Philadelphia teacher and philanthropist and the first Jewish female college student in America. Scott's attention had been drawn to Gratz's character by novelist Washington Irving, who was a close friend of the Gratz family. The assertion has been disputed, but it has been supported by "The Original of Rebecca in Ivanhoe", an article that appeared in "The Century Magazine" in 1882.
Moreover, there are some inaccuracies about English kings' history and genealogy. For instance, William II of England, cited as William Rufus (in the scene of the Joust in Ashby-de-la-Zouch), is said to have been John Lackland's grandfather, whereas he was actually his great-grand-uncle. Furthermore, while describing the violence and lack of respect of Norman barons towards women, Scott refers to Matilda's temporary vows in a nunnery, but it is unclear whether he refers to Matilda of Scotland, wife of Henry I of England (since she is defined as queen of England and daughter of the king of Scotland) or to her daughter Empress Matilda (since she is said to have been Empress of Germany and daughter and mother of kings, characteristics which can be applied only to her).
Legacy.
Film, TV or theatrical adaptations.
The novel has been the basis for several movies:
There have also been many television adaptations of the novel, including:
Victor Sieg's dramatic cantata "Ivanhoé" won the Prix de Rome in 1864 and premiered in Paris the same year. An operatic adaptation of the novel by Sir Arthur Sullivan (entitled "Ivanhoe") ran for over 150 consecutive performances in 1891. Other operas based on the novel have been composed by Gioachino Rossini ("Ivanhoé"), Thomas Sari ("Ivanhoé"), Bartolomeo Pisani ("Rebecca"), A. Castagnier ("Rébecca"), Otto Nicolai ("Il Templario"), and Heinrich Marschner ("Der Templer und die Jüdin"). Rossini's opera is a "pasticcio" (an opera in which the music for a new text is chosen from pre-existent music by one or more composers). Scott attended a performance of it and recorded in his journal, "It was an opera, and, of course, the story sadly mangled and the dialogue, in part nonsense."
External links.
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="15056" url="http://en.wikipedia.org/wiki?curid=15056" title="Isoelectric point">
Isoelectric point

The isoelectric point (pI, pH(I), IEP), is the pH at which a particular molecule carries no net electrical charge. The standard nomenclature to represent the isoelectric point is pH(I), although pI is also commonly seen, and is used in this article for brevity. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H+).
Surfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H+/OH−, the net surface charge is affected by the pH of the liquid in which the solid is submerged.
The pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated according to their isoelectric point (overall charge) on a polyacrylamide gel using either QPNC-PAGE or a technique called isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.
Calculating pI values.
For an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.
The pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.
Examples.
In these two examples the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution
is not known.
The other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H32+ is negligible at the isoelectric point in this case.
If the pI is greater than the pH, the molecule will have a positive charge.
Ceramic materials.
The isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O−, while at pH values below the IEP, M-OH2+ species predominate. Some approximate values of common ceramics are listed below (Haruta and Brunelle, except where noted). The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. In addition, precise measurement of isoelectric points is difficult and requires careful techniques, even with modern methods. Thus, many sources often cite differing values for isoelectric points of these materials.
Examples of isoelectric points.
The following list gives the pH25 °C of isoelectric point at 25 °C for selected materials in water:
"Note: The list is ordered by increasing pH values."
Mixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, Jara "et al." measured an IEP of 4.5 for a synthetically prepared amorphous aluminosilicate (Al2O3-SiO2). The researchers noted that the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value. Significantly higher IEP values (pH 6 to 8) have been reported for 3Al2O3-2SiO2 by others (see Lewis). Lewis also lists the IEP of barium titanate, BaTiO3 as being between pH 5 and 6, while Vamvakaki et al. reported a value of 3, although these authors note that a wide range of values have been reported, a result of either residual barium carbonate on the surface or TiO2-rich surfaces.
The farther the pH of an Amino Acid solution is from its pl the greater the electric charge on that population of molecules.
Isoelectric point versus point of zero charge.
The terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.
In systems in which H+/OH− are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.
According to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p"K"− and p"K"+ to define the two conditions in terms of the relative number of charged sites:
For large Δp"K" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp"K", there are many charged species in approximately equal numbers, so one speaks of the IEP.

</doc>
<doc id="15058" url="http://en.wikipedia.org/wiki?curid=15058" title="International reply coupon">
International reply coupon

An international reply coupon (IRC) is a coupon that can be exchanged for one or more postage stamps representing the minimum postage for an unregistered priority airmail letter of up to twenty grams sent to another Universal Postal Union (UPU) member country. IRCs are accepted by all UPU member countries.
UPU member postal services are obliged to exchange an IRC for postage, but are not obliged to sell them.
The purpose of the IRC is to allow a person to send someone in another country a letter, along with the cost of postage for a reply. If the addressee is within the same country, there is no need for an IRC because a self-addressed stamped envelope (SASE) or return postcard will suffice; but if the addressee is in another country an IRC removes the necessity of acquiring foreign postage or sending appropriate currency.
History.
The IRC was introduced in 1906 at a Universal Postal Union congress in Rome. At the time an IRC could be exchanged for a single-rate, ordinary postage stamp for surface delivery to a foreign country, as this was before the introduction of airmail services. An IRC is exchangeable in a UPU member country for the minimum postage of a priority or unregistered airmail letter to a foreign country.
The current IRC, which features the theme "Water for Life," designed by Czech artist and graphic designer Michal Sindelar, was issued in 2013 and is valid until 31 December 2017. IRCs are ordered from the UPU headquarters in Bern, Switzerland, by postal authorities. They are generally available at large post offices; in the U.S., they are requisitioned along with regular domestic stamps by any post office that has sufficient demand for them.
Prices for IRCs vary by country. In the United States in November 2012, the purchase price was $2.20USD; however, the US Postal Service discontinued sales of IRCs on 27 January 2013 due to declining demand. Britain's Royal Mail also stopped selling IRCs on 18 February 2012, citing minimal sales and claiming that the average post office sold less than one IRC per year. IRCs purchased in foreign countries may be used in the United States toward the purchase of postage stamps and embossed stamped envelopes at the current one-ounce First Class International rate ($1.05 USD as of April 2012) per coupon.
IRCs are often used by amateur radio operators sending QSL cards to each other; it has traditionally been considered good practice and common courtesy to include an IRC when writing to a foreign operator and expecting a reply by mail.
Previous editions of the IRC, the "Beijing" model and all subsequent versions, bear an expiration date. Consequently, a new IRC will be issued every three years.
The Ponzi scheme.
The profit that could be made by taking advantage of the differing postal rates in different countries to buy IRCs cheaply in one country and exchange them for stamps of a higher value in another country was the intended profit generator for a scheme operated by Charles Ponzi, which became the fraudulent Ponzi scheme. In practice, the overhead on buying and selling large numbers of the very low-value IRCs precluded any profitability.
The selling price and exchange value in stamps in each country have been adjusted to some extent to remove some of the potential for profit, but ongoing fluctuations in cost of living and exchange rates make it impossible to achieve this completely.
U.S. Postal Service Description of International reply coupons.
International reply coupons (in French, Coupons-Reponse Internationaux) are printed in blue ink on paper that has the letters “UPU” in large characters in the watermark. The front of each coupon is printed in French. The reverse side of the coupon, which has text relating to its use, is printed in German, English, Arabic, Chinese, Spanish, or Russian. Under Universal Postal Union’s regulations, participating member countries are not required to place a control stamp or postmark on the international reply coupons that they sell. Therefore, some foreign issue reply coupons that are tendered for redemption may bear the name of the issuing country (generally in French) rather than the optional control stamp or postmark.
USPS Item Number 330800 is an international reply coupon printed by the Universal Postal Union which is approximately 3.75 inches by 6 inches, has a barcode on the reverse side, and has an expiration date of December 31, 2013 (in French, A échanger jusqu'au 31 décembre 2013 (31.12.2013)). New coupons are valid until December 31, 2017.
The Nairobi Model was designed by Rob Van Goor, a graphic artist from the Luxembourg Post. It was selected from among 10 designs presented by Universal Postal Union member countries. Van Goor interpreted the theme of the contest -- "The Postage Stamp: A Vehicle for Exchange" -- by depicting the world being cradled by a hand and the perforated outline of a postage stamp.

</doc>
<doc id="15059" url="http://en.wikipedia.org/wiki?curid=15059" title="Isaac Bonewits">
Isaac Bonewits

Phillip Emmons Isaac Bonewits (October 1, 1949 – August 12, 2010) was an influential American Druid who published a number of books on the subject of Neopaganism and magic. He was also a public speaker, liturgist, singer and songwriter, and founded the Druidic organization Ár nDraíocht Féin, as well as the Neopagan civil rights group, the Aquarian Anti-Defamation League. Born in Royal Oak, Michigan, Bonewits had been heavily involved in occultism since the 1960s. He died in 2010.
Early life and education.
Bonewits was born on October 1, 1949 in Royal Oak, Michigan, as the fourth of five children. His mother and father were Roman Catholics. Spending much of his childhood in Ferndale, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966; he graduated from the university in 1970 with a Bachelor of Arts in Magic, becoming the first and only person to have ever received any kind of academic degree in Magic from an accredited university.
Personal life.
Bonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years. Paperwork and legalities caught up on December 31, 2007, making them legally married.
Bonewits' only child, Arthur Shaffrey Lipp-Bonewits, was born to Deborah Lipp in 1990.
Career.
Early years.
In 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America, or RDNA. Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18 year old Bonewits was recruited by the Church of Satan, but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary "Satanis: The Devil's Mass". Bonewits, in his article "My Satanic Adventure", asserts that the rituals in "Satanis" were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.
Author and Druid.
His first book, "Real Magic", was published in 1972. Between 1973 and 1975 Bonewits was employed as editor of "Gnostica" magazine in Minnesota (published by Llewellyn Publications), established an offshoot group of the RDNA called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite his lifelong status as a "gentile"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.
In 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.
Founder of Ár nDraíocht Féin.
Over the years Bonewits also had varying degrees of involvement with mystical organizations such as Ordo Templi Orientis, Gardnerian Wicca, and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn) as well as others. Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book 'Authentic Thaumaturgy' to gamers as a way of organising Dungeons and Dragons games and to give a background to games of .
In 1983, Bonewits founded Ár nDraíocht Féin (also known as "A Druid Fellowship" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization. He made the organization's first public announcement in 1984, and began the membership sign-up at the first WinterStar Symposium in 1984. Since that time, ADF has developed into one of the world's largest forms of contemporary Druidism practiced as a religion.
Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.
Musician and activist.
A songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).
Bonewits encouraged charity programs to help Neopagan seniors, and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.
Illness and death.
In 1990, Bonewits was diagnosed with Eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.
On October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer, for which he underwent treatment. He died at home, on August 12, 2010, surrounded by his family.
Contributions to Neopaganism.
In his book "Real Magic" (1971), Bonewits proposed his "Laws of Magic." These "laws" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970's to be part of his publishing project 'Library of the Occult'.
Bonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.

</doc>
<doc id="15062" url="http://en.wikipedia.org/wiki?curid=15062" title="Intel 8080">
Intel 8080

The Intel 8080 ("eighty-eighty") was the second 8-bit microprocessor designed and manufactured by Intel and was released in April 1974. It was an extended and enhanced variant of the earlier 8008 design, although without binary compatibility. The initial specified clock frequency limit was 2 MHz, and with common instructions having execution times of 4, 5, 7, 10, or 11 cycles this meant that it operated at an effective speed of a few hundred thousand instructions per second.
The 8080 has sometimes been labeled "the first truly usable microprocessor", although earlier microprocessors were used for calculators, cash registers, computer terminals, industrial robots and other applications. The architecture of the 8080 strongly influenced Intel's 8086 CPU architecture, which spawned the x86 family of processors.
The 8080 required two support chips to function, the i8224 clock generator/driver and (most often) the i8228 bus controller and it was implemented using non-saturated enhancement-load NMOS, demanding an extra +12 V and a −5 V supply in addition to the main TTL compatible +5 V Supply.
Description.
Programming model.
The Intel 8080 was the successor to the 8008. It used the same basic instruction set and register model as the 8008 (developed by Computer Terminal Corporation), even though it was not source code compatible nor binary compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080 (even though the actual opcodes differ between the two CPUs). The 8080 also added a few 16-bit operations to its instruction set as well. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 added addressing modes to allow direct access to its full 16-bit memory space. In addition, the internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack pointer (SP) register. The 8080's large 40-pin DIP packaging permitted it to provide a 16-bit address bus and an 8-bit data bus, allowing easy access to 64 KB of memory.
Registers.
The processor has seven 8-bit registers (A, B, C, D, E, H, and L), where A is the primary 8-bit accumulator and the other six registers can be used as either individual 8-bit registers or as three 16-bit register pairs (BC, DE, and HL) depending on the particular instruction. Some instructions also enable the HL register pair to be used as a (limited) 16-bit accumulator, and a pseudo-register, M, can be used almost anywhere that any other register can be used, referring to the memory address pointed to by the HL pair. It also has a 16-bit stack pointer to memory (replacing the 8008's internal stack), and a 16-bit program counter.
Flags.
The processor maintains internal flag bits (a status register) which indicates the results of arithmetic and logical instructions. The flags are:
The carry bit can be set, or complemented, by specific instructions. Conditional branch instructions test the various flag status bits. The flags can be copied as a group to the accumulator. The A accumulator and the flags together are called the AF register.
Commands/instructions.
As with many other 8-bit processors, all instructions are encoded in a single byte (including register-numbers, but excluding immediate data), for simplicity. Some of them are followed by one or two bytes of data, which can be an immediate operand, a memory address, or a port number. Like larger processors, it has automatic CALL and RET instructions for multi-level procedure calls and returns (which can even be conditionally executed, like jumps), and instructions to save and restore any 16-bit register-pair on the machine stack. There are also eight one-byte call instructions (codice_1) for subroutines located at the fixed addresses 00h, 08h, 10h, ..., and 38h. These were intended to be supplied by external hardware in order to invoke a corresponding interrupt-service routine, but were also often employed as fast system calls. The most sophisticated command is codice_1, which is used for exchanging the register pair HL with the value stored at the address indicated by the stack pointer.
8-bit instructions.
Most 8-bit operations can only be performed on the 8-bit accumulator (the A register). For 8-bit operations with two operands, the other operand can be either an immediate value, another 8-bit register, or a memory byte addressed by the 16-bit register pair HL. Direct copying is supported between any two 8-bit registers and between any 8-bit register and an HL-addressed memory byte. Due to the regular encoding of the codice_1 instruction (using a quarter of available opcode space), there are redundant codes to copy a register into itself (codice_1, for instance), which were of little use, except for delays. However, what would have been a copy from the HL-addressed cell into itself (i.e., codice_1) is instead used to encode the halt (codice_1) instruction, halting execution until an external reset or interrupt occurs.
16-bit operations.
Although the 8080 is generally an 8-bit processor, it also has limited abilities to perform 16-bit operations: Any of the three 16-bit register pairs (BC, DE, or HL) or SP can be loaded with an immediate 16-bit value (using codice_1), incremented or decremented (using codice_1 and codice_1), or added to HL (using codice_1). The codice_1 instruction exchanges the values of the HL and DE register pairs. By adding HL to itself, it is possible to achieve the same result as a 16-bit arithmetical left shift with one instruction. The only 16-bit instructions that affect any flag are codice_1, which set the CY (carry) flag in order to allow for programmed 24-bit or 32-bit arithmetics (or larger), needed to implement floating point arithmetics, for instance.
Input/output scheme.
Input output port space.
The 8080 supported up to 256 input/output (I/O) ports, accessed via dedicated I/O instructions—taking port addresses as operands. This I/O mapping scheme was regarded as an advantage, as it freed up the processor's limited address space. Many CPU architectures instead use so-called memory mapped I/O, in which a common address space is used for both RAM and peripheral chips. This removes the need for dedicated I/O instructions, although a drawback in such designs may be that special hardware must be used to insert wait states as peripherals are often slower than memory. However, in some simple 8080 computers, I/O was indeed addressed as if they were memory cells, "memory mapped", leaving the I/O commands unused. I/O addressing could also sometimes employ the fact that the processor would output the same 8-bit port address to both the lower and the higher address byte (i.e. IN 05h would put the address 0505h on the 16-bit address bus). Similar I/O-port schemes were used in the backward compatible Zilog Z80 and Intel 8085 as well as the closely related x86 families of microprocessors.
Separate stack space.
One of the bits in the processor state word (see below) indicates that the processor is accessing data from the stack. Using this signal, it is possible to implement a separate stack memory space. However, this feature was seldom used.
The internal state word.
For more advanced systems, during one phase of its working loop, the processor set its "internal state byte" on the data bus. This byte contains flags which determine if the memory or I/O port is accessed, and whether it was necessary to handle an interrupt.
The interrupt system state (enabled or disabled) was also output on a separate pin. For simple systems, where the interrupts were not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio86RK computer made in the Soviet Union, for instance).
Example code.
The following 8080/8085 assembler source code is for a subroutine named codice_13 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.
Pin usage.
The address bus had its own 16 pins, and the data bus had eight pins that were possible to use without any multiplexing. Using the two additional pins (read and write signals), it was possible to assemble simple microprocessor devices very easily. Only the separate IO space, interrupts and DMA required additional chips to decode the processor pin signals. However, the processor load capacity was limited, and even simple computers frequently contained bus amplifiers.
The processor required three power sources (−5, +5 and +12 V) and two non-interlacing high-amplitude synchronization signals. However, at least the late Soviet version КР580ВМ80А was able to work with a single +5 V power source, the +12 V pin being connected to +5 V and the −5 V pin to ground. The processor consumed about 1.3 W of power.
The pinout table, from the chip's accompanying documentation, described the pins as follows:
Support chips.
A key factor in the success of the 8080 was the broad range of support chips available, providing serial communications, counter/timing, input/output, direct memory access, and programmable interrupt control amongst other functions.
Physical implementation.
The 8080 integrated circuit used non-saturated enhancement load nMOS gates, demanding extra voltages (for the load-gate bias). It was manufactured in a silicon gate process using a minimum feature size of 6 µm. A single layer of metal was used to interconnect the approximately 6,000 transistors in the design, but the higher resistance polysilicon layer, which required higher voltage for some interconnects, was implemented with transistor gates. The die size was approximately 20 mm2.
The industrial impact.
Applications and successors.
The 8080 was used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more capable, Zilog Z80 processor would capitalize on this, with Z80 & CP/M becoming the dominant CPU & OS combination of the period circa 1976 to 1983 much as did the x86 & MS-DOS for the PC a decade later). Even in 1979 after introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 per unit. The first single-board microcomputers, such as MYCRO-1 and the "dyna-micro" were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, CA in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 was as the "brain" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which took large amounts of user data from reel-to-reel tape and imaged it onto microfiche. The Auto-COM instruments also included an entire automated film cutting, processing, washing, and drying sub-system – quite a feat, both then and in the 21st century, to all be accomplished successfully with only an 8-bit microprocessor running at a clock speed of less than 1 MHz with a 64 KB memory limit. In addition, several early arcade video games were built around the 8080 microprocessor. "Space Invaders" was perhaps the most popular such title.
Shortly after the launch of the 8080, the Motorola 6800 competing design was introduced, and after that, the MOS Technology 6502 variation of the 6800. Zilog introduced the Z80, which had a compatible machine-language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085, and later by the assembly language compatible 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made a NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supported an 8080 emulation mode. This was also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its ISA, made a lasting impact on computer history.
In the Soviet Union, manufacturers cloned the 8080 microprocessor's layout geometry, even using an identical pin arrangement, and started to produce the clone under the name KP580ИK80 (later marked as KP580BM80). This processor was the base of the Radio86RK ( in Russian), probably the most popular amateur single-board computer in the Soviet Union. Radio86RK's predecessor was the Micro-80 ( in Russian), and its successor the Orion-128 ( in Russian) which had a graphical display. Both were built on the KP580 processor.
Another model compatible with Intel 8080A, named MMN8080, was produced at Microelectronica Bucharest in Romania. There was also a compatible Polish CPU named MCY7880 and the Slovak-made Tesla MHB 8080A.
Industry change.
The 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett Packard, or IBM. A manufacturer would produce the entire computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was actually designed for just about any application "except" a complete computer system. Hewlett Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 was a terminal which ran BASIC on the 8080. Microsoft would market as its founding product the first popular programming language for the 8080, and would later acquire DOS for the IBM-PC.
The 8080 and 8085 gave rise to the 8086, which was designed as a source compatible (although not binary compatible) extension of the 8085. This design, in turn, later spawned the x86 family of chips, the basis for most CPUs in use today. Many of the 8080's core machine instructions and concepts, for example, registers named "A", "B", "C" and "D", as well as many of the flags used to control conditional jumps, are still in use in the widespread x86 platform. 8080 Assembler code can still be directly translated into x86 instructions; all of its core elements are still present.
PCs based upon the 8086 design and its successors evolved into workstations and servers of 16, 32 and 64 bits, with advanced memory protection, segmentation, and multiprocessing features, blurring the difference between small and large computers (the 80286 and 80386's protected mode were important in doing so). The size of chips has grown so that the size and power of large x86 chips is not much different from high end architecture chips, and a common strategy to produce a very large computer is to network many x86 processors.
The basic architecture of the 8080 and its successors has replaced many proprietary midrange and mainframe computers, and withstood challenges of technologies such as RISC. Most computer manufacturers have abandoned producing their own processors below the highest performance points. Though x86 may not be the most elegant, or theoretically most efficient design, the sheer market force of so many dollars going into refining a design has made the x86 family today, and will remain for some time, the dominant processor architecture, even bypassing Intel's attempts to replace it with incompatible architectures such as the iAPX 432 and Itanium.
History.
Federico Faggin, the originator of the 8080 architecture in early 1972, proposed it to Intel's management and pushed for its implementation. He finally got the permission to develop it six months later. Faggin hired Masatoshi Shima from Japan who did the detailed design under his direction, using the design methodology for random logic with silicon gate that Faggin had created for the 4000 family. Stanley Mazor contributed a couple of instructions to the instruction set.
Patent.
 , "MOS computer employing a plurality of separate chips", issued 1977-03-01<span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=4,010,449&rft.cc=US&rft.title=MOS%20computer%20employing%20a%20plurality%20of%20separate%20chips&rft.inventor=%5B%5BFederico%20Faggin%5D%5D%2C%20%5B%5BMasatoshi%20Shima%5D%5D%2C%20Stanley%20Mazor&rft.date=1977-03-01"> 
External links.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="15063" url="http://en.wikipedia.org/wiki?curid=15063" title="Intel 8086">
Intel 8086

The 8086 ("eighty eighty-six", also called iAPX 86) is a 16-bit microprocessor chip designed by Intel between early 1976 and mid-1978, when it was released. The Intel 8088, released in 1979, was a slightly modified chip with an external 8-bit data bus (allowing the use of cheaper and fewer supporting ICs), and is notable as the processor used in the original IBM PC design, including the widespread version called IBM PC XT.
The 8086 gave rise to the x86 architecture which eventually became Intel's most successful line of processors.
History.
Background.
In 1972, Intel launched the 8008, the first 8-bit microprocessor. It implemented an instruction set designed by Datapoint corporation with programmable CRT terminals in mind, which also proved to be fairly general purpose. The device needed several additional ICs to produce a functional computer, in part due to it being packaged in a small 18-pin "memory package", which ruled out the use of a separate address bus (Intel was primarily a DRAM manufacturer at the time).
Two years later, Intel launched the 8080, employing the new 40-pin DIL packages originally developed for calculator ICs to enable a separate address bus. It had an extended instruction set that was source (not binary) compatible with the 8008 and also included some 16-bit instructions to make programming easier. The 8080 device, often described as "the first truly useful microprocessor", was eventually replaced by the depletion-load based 8085 (1977) which sufficed with a single +5 V power supply instead of the three different operating voltages of earlier chips. Other well known 8-bit microprocessors that emerged during these years were Motorola 6800 (1974), General Instrument PIC16X (1975), MOS Technology 6502 (1975), Zilog Z80 (1976), and Motorola 6809 (1978).
The first x86 design.
The 8086 project started in May 1976 and was originally intended as a temporary substitute for the ambitious and delayed iAPX 432 project. It was an attempt to draw attention from the less-delayed 16- and 32-bit processors of other manufacturers (such as Motorola, Zilog, and National Semiconductor) and at the same time to counter the threat from the Zilog Z80 (designed by former Intel employees), which became very successful. Both the architecture and the physical chip were therefore developed rather quickly by a small group of people, and using the same basic microarchitecture elements and physical implementation techniques as employed for the slightly older 8085 (and for which the 8086 also would function as a continuation).
Marketed as source compatible, the 8086 was designed to allow assembly language for the 8008, 8080, or 8085 to be automatically converted into equivalent (suboptimal) 8086 source code, with little or no hand-editing. The programming model and instruction set was (loosely) based on the 8080 in order to make this possible. However, the 8086 design was expanded to support full 16-bit processing, instead of the fairly basic 16-bit capabilities of the 8080/8085.
New kinds of instructions were added as well; full support for signed integers, base+offset addressing, and self-repeating operations were akin to the Z80 design but were all made slightly more general in the 8086. Instructions directly supporting nested ALGOL-family languages such as Pascal and PL/M were also added. According to principal architect Stephen P. Morse, this was a result of a more software centric approach than in the design of earlier Intel processors (the designers had experience working with compiler implementations). Other enhancements included microcoded multiply and divide instructions and a bus structure better adapted to future coprocessors (such as 8087 and 8089) and multiprocessor systems.
The first revision of the instruction set and high level architecture was ready after about three months, and as almost no CAD tools were used, four engineers and 12 layout people were simultaneously working on the chip. The 8086 took a little more than two years from idea to working product, which was considered rather fast for a complex design in 1976–1978.
The 8086 was sequenced using a mixture of random logic and microcode and was implemented using depletion-load nMOS circuitry with approximately 20,000 active transistors (29,000 counting all ROM and PLA sites). It was soon moved to a new refined nMOS manufacturing process called HMOS (for High performance MOS) that Intel originally developed for manufacturing of fast static RAM products. This was followed by HMOS-II, HMOS-III versions, and, eventually, a fully static CMOS version for battery powered devices, manufactured using Intel's CHMOS processes. The original chip measured 33 mm² and minimum feature size was 3.2 μm.
The architecture was defined by Stephen P. Morse with some help and assistance by Bruce Ravenel (the architect of the 8087) in refining the final revisions. Logic designer Jim McKevitt and John Bayliss were the lead engineers of the hardware-level development team and Bill Pohlman the manager for the project. The legacy of the 8086 is enduring in the basic instruction set of today's personal computers and servers; the 8086 also lent its last two digits to later extended versions of the design, such as the Intel 286 and the Intel 386, all of which eventually became known as the x86 family. (Another reference is that the PCI Vendor ID for Intel devices is 8086h.)
Details.
Buses and operation.
All internal registers, as well as internal and external data buses, are 16 bits wide, which firmly established the "16-bit microprocessor" identity of the 8086. A 20-bit external address bus provides a 1 MB physical address space (220 = 1,048,576). This address space is addressed by means of internal memory "segmentation". The data bus is multiplexed with the address bus in order to fit all of the control lines into a standard 40-pin dual in-line package. It provides a 16-bit I/O address bus, supporting 64 KB of separate I/O space. The maximum linear address space is limited to 64 KB, simply because internal address/index registers are only 16 bits wide. Programming over 64 KB memory boundaries involves adjusting the segment registers (see below); this difficulty existed until the 80386 architecture introduced wider (32-bit) registers (the memory management hardware in the 80286 did not help in this regard, as its registers are still only 16 bits wide).
Some of the control pins, which carry essential signals for all external operations, have more than one function depending upon whether the device is operated in "min" or "max" mode. The former mode was intended for small single-processor systems, while the latter was for medium or large systems using more than one processor.
Registers and instructions.
The 8086 has eight more or less general 16-bit registers (including the stack pointer but excluding the instruction pointer, flag register and segment registers). Four of them, AX, BX, CX, DX, can also be accessed as twice as many 8-bit registers (see figure) while the other four, BP, SI, DI, SP, are 16-bit only.
Due to a compact encoding inspired by 8-bit processors, most instructions are one-address or two-address operations, which means that the result is stored in one of the operands. At most one of the operands can be in memory, but this memory operand can also be the "destination", while the other operand, the "source", can be either "register" or "immediate". A single memory location can also often be used as both "source" and "destination" which, among other factors, further contributed to a code density comparable to (and often better than) most eight-bit machines at the time.
The degree of generality of most registers are much greater than in the 8080 or 8085. However, 8086 registers were more specialized than in most contemporary minicomputers and are also used implicitly by some instructions. While perfectly sensible for the assembly programmer, this made register allocation for compilers more complicated compared to more orthogonal 16-bit and 32-bit processors of the time such as the PDP-11, VAX, 68000, 32016 etc. On the other hand, being more regular than the rather minimalistic but ubiquitous 8-bit microprocessors such as the 6502, 6800, 6809, 8085, MCS-48, 8051, and other contemporary accumulator based machines, it was significantly easier to construct an efficient code generator for the 8086 architecture.
Another factor for this was that the 8086 also introduced some new instructions (not present in the 8080 and 8085) to better support stack-based high-level programming languages such as Pascal and PL/M; some of the more useful instructions were push "mem-op", and ret "size", supporting the "Pascal calling convention" directly. (Several others, such as push "immed" and enter, were added in the subsequent 80186, 80286, and 80386 processors.)
A 64 KB (one segment) stack growing towards lower addresses is supported in hardware; 16-bit words are pushed onto the stack, and the top of the stack is pointed to by SS:SP. There are 256 interrupts, which can be invoked by both hardware and software. The interrupts can cascade, using the stack to store the return addresses.
The 8086 has 64 K of 8-bit (or alternatively 32 K of 16-bit word) I/O port space.
Flags.
8086 has a 16-bit flags register. Nine of these condition code flags are active, and indicate the current state of the processor: Carry flag (CF), Parity flag (PF), Auxiliary carry flag (AF), Zero flag (ZF), Sign flag (SF), Trap flag (TF), Interrupt flag (IF), Direction flag (DF), and Overflow flag (OF).
Segmentation.
There are also four 16-bit segment registers (see figure) that allow the 8086 CPU to access one megabyte of memory in an unusual way. Rather than concatenating the segment register with the address register, as in most processors whose address space exceeded their register size, the 8086 shifts the 16-bit segment only four bits left before adding it to the 16-bit offset (16×segment + offset), therefore producing a 20-bit external (or effective or physical) address from the 32-bit segment:offset pair. As a result, each external address can be referred to by 212 = 4096 different segment:offset pairs.
Although considered complicated and cumbersome by many programmers, this scheme also has advantages; a small program (less than 64 KB) can be loaded starting at a fixed offset (such as 0000) in its own segment, avoiding the need for relocation, with at most 15 bytes of alignment waste.
Compilers for the 8086 family commonly support two types of pointer, "near" and "far". Near pointers are 16-bit offsets implicitly associated with the program's code or data segment and so can be used only within parts of a program small enough to fit in one segment. Far pointers are 32-bit segment:offset pairs resolving to 20-bit external addresses. Some compilers also support "huge" pointers, which are like far pointers except that pointer arithmetic on a huge pointer treats it as a linear 20-bit pointer, while pointer arithmetic on a far pointer wraps around within its 16-bit offset without touching the segment part of the address.
To avoid the need to specify "near" and "far" on numerous pointers, data structures, and functions, compilers also support "memory models" which specify default pointer sizes. The "tiny" (max 64K), "small" (max 128K), "compact" (data > 64K), "medium" (code > 64K), "large" (code,data > 64K), and "huge" (individual arrays > 64K) models cover practical combinations of near, far, and huge pointers for code and data. The "tiny" model means that code and data are shared in a single segment, just as in most 8-bit based processors, and can be used to build ".com" files for instance. Precompiled libraries often came in several versions compiled for different memory models.
According to Morse et al., the designers actually contemplated using an 8-bit shift (instead of 4-bit), in order to create a 16 MB physical address space. However, as this would have forced segments to begin on 256-byte boundaries, and 1 MB was considered very large for a microprocessor around 1976, the idea was dismissed. Also, there were not enough pins available on a low cost 40-pin package for the additional four address bus pins.
In principle, the address space of the x86 series "could" have been extended in later processors by increasing the shift value, as long as applications obtained their segments from the operating system and did not make assumptions about the equivalence of different segment:offset pairs. In practice the use of "huge" pointers and similar mechanisms was widespread and the flat 32-bit addressing made possible with the 32-bit offset registers in the 80386 eventually extended the limited addressing range in a more general way (see below).
Intel could have decided to implement memory in 16 bit words (which would have eliminated the BHE signal along with much of the address bus complexities already described). This would mean that all instruction object codes and data would have to be accessed in 16-bit units. Users of the 8080 long ago realized, in hindsight, that the processor makes very efficient use of its memory. By having a large number of 8-bit object codes, the 8080 produces object code as compact as some of the most powerful minicomputers on the market at the time.:5–26
If the 8086 is to retain 8-bit object codes and hence the efficient memory use of the 8080, then it cannot guarantee that (16-bit) opcodes and data will lie on an even-odd byte address boundary. The first 8-bit opcode will shift the next 8-bit instruction to an odd byte or a 16-bit instruction to an odd-even byte boundary. By implementing the BHE signal and the extra logic needed, the 8086 has allowed instructions to exist as 1-byte, 3-byte or any other odd byte object codes.:5–26
Simply put: this is a trade off. If memory addressing is simplified so that memory is only accessed in 16-bit units, memory will be used less efficiently. Intel decided to make the logic more complicated, but memory use more efficient. This was at a time when memory size was considerably smaller, and at a premium, than that which users are used to today.:5–26
Porting older software.
Small programs could ignore the segmentation and just use plain 16-bit addressing. This allowed 8-bit software to be quite easily ported to the 8086. The authors of MS-DOS took advantage of this by providing an Application Programming Interface very similar to CP/M as well as including the simple ".com" executable file format, identical to CP/M. This was important when the 8086 and MS-DOS were new, because it allowed many existing CP/M (and other) applications to be quickly made available, greatly easing acceptance of the new platform.
Example code.
The following 8086/8088 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.
The code above uses the BP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by most ALGOL-like languages since the late 1950s. The ES segment register is saved on the stack and replaced with the value of the DS segment register, so that the codice_2 codice_2 instructions will operate within the same source and destination data segment. Before returning, the subroutine restores the previous value of the ES register.
The above routine is a rather cumbersome way to copy blocks of data. Provided the source and the destination blocks reside within single 65,536 byte segments (a requirement of the above routine), advantage can be taken of the 8086's block codice_4 instructions. The loop section of the above can be replaced by:
<source lang=asm>
0000:1011 F2 loop rep ; Repeat until CX=0
0000:1012 A5 movsw ; Move the data block
</Source>
This copies the block of data one word at a time. The codice_5 instruction causes the following codice_6 to repeat until CX=0, automatically incrementing SI and DI as it repeats. Alternatively the codice_7 or codice_8 instructions can be used to copy single bytes or double words at a time. Most assemblers will assemble correctly if the codice_5 instruction is used as a prefix to codice_6 as in codice_11.
This routine will operate correctly if interrupted, because the program counter will continue to point to the codice_5 instruction until the block copy is completed. The copy will therefore continue from where it left off when the interrupt service routine returns control.
Performance.
Although partly shadowed by other design choices in this particular chip, the multiplexed address and data buses limited performance slightly; transfers of 16-bit or 8-bit quantities were done in a four-clock memory access cycle, which was faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions varied from one to six bytes, fetch and execution were made concurrent and decoupled into separate units (as it remains in today's x86 processors): The "bus interface unit" fed the instruction stream to the "execution unit" through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations unfortunately became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, as well as other enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).
As can be seen from these tables, operations on registers and immediates were fast (between 2 and 4 cycles), while memory-operand instructions and jumps were quite slow; jumps took more cycles than on the simple 8080 and 8085, and the 8088 (used in the IBM PC) was additionally hampered by its narrower bus. The reasons why most memory related instructions were slow were threefold:
However, memory access performance was drastically enhanced with Intel's next generation chips. The 80186 and 80286 both had dedicated address calculation hardware, saving many cycles, and the 80286 also had separate (non-multiplexed) address and data buses.
Floating point.
The 8086/8088 could be connected to a mathematical coprocessor to add hardware/microcode-based floating point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek ("non" 8087-compatible) eventually came up with high performance floating point coprocessors that competed with the 8087 as well as with the subsequent, higher performing Intel 80387.
Chip versions.
The clock frequency was originally limited to 5 MHz (IBM PC used 4.77 MHz, 4/3 the standard NTSC color burst frequency), but the last versions in HMOS were specified for 10 MHz. HMOS-III and CMOS versions were manufactured for a long time (at least a while into the 1990s) for embedded systems, although its successor, the 80186/80188 (which includes some on-chip peripherals), has been more popular for embedded use.
The 80C86, the CMOS version of the 8086, was used in the GRiDPad, Toshiba T1200, HP 110, and finally the 1998-1999 Lunar Prospector.
Derivatives and clones.
Compatible—and, in many cases, enhanced—versions were manufactured by Fujitsu, Harris/Intersil, OKI, Siemens AG, Texas Instruments, NEC, Mitsubishi, AMD. For example, the NEC V20 and NEC V30 pair were hardware compatible with the 8088 and 8086 even though NEC made original Intel clones μPD8088D and μPD8086D, respectively, but incorporated the instruction set of the 80186 along with some (but not all) of the 80186 speed enhancements, providing a drop-in capability to upgrade both instruction set and processing speed without manufacturers having to modify their designs. Such relatively simple and low-power 8086-compatible processors in CMOS are still used in embedded systems.
The electronics industry of the Soviet Union was able to replicate the 8086 through The resulting chip, K1810BM86, was binary and pin-compatible with the 8086.
i8088 and i8086 were respectively the cores of the Soviet-made PC-compatible EC1831 and EC1832 desktops (EC1831 is the EC identification of IZOT 1037C and EC1832 is the EC identification of IZOT 1036C, developed and manufactured in Bulgaria). However, EC1832 computer (IZOT 1036C) had significant hardware differences from its authentic prototype, and the data/address bus circuitry was designed independently of Intel products. EC1832 was the first PC compatible computer with dynamic bus sizing (US Pat. No 4,831,514). Later some of the ES1832 principles were adopted in PS/2 (US Pat. No 5,548,786) and some other machines (UK Patent Application, Publication No. GB-A-2211325, Published June 28, 1989).
Hardware modes.
The 8086 and 8088 support two hardware modes: maximum mode and minimum mode. Maximum mode is for large applications such as multiprocessing and is also required to support the 8087 coprocessor. The mode is usually hardwired into the circuit and cannot be changed by software. Specifically, pin #33 (MN/MX) is either wired to voltage or to ground to determine the mode. Changing the state of pin #33 changes the function of certain other pins, most of which have to do with how the CPU handles the (local) bus. The IBM PC and PC/XT use an Intel 8088 running in maximum mode, which allows the CPU to work with an optional 8087 coprocessor installed in the math coprocessor socket on the PC or PC/XT mainboard. (The PC and PC/XT may require Max mode for other reasons, such as perhaps to support the DMA controller.)

</doc>
<doc id="15064" url="http://en.wikipedia.org/wiki?curid=15064" title="Intel 8088">
Intel 8088

The Intel 8088 ("eighty-eighty-eight", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on July 1, 1979, the 8088 had an 8-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range were unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)—only the bus interface unit (BIU) is different. The original IBM PC was based on the 8088.
History and description.
The 8088 was designed in Israel, at Intel's Haifa laboratory, as with a large number of Intel's processors. The 8088 was targeted at economical systems by allowing the use of an 8-bit data path and 8-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then new design office and laboratory in Haifa, Israel.
Variants of the 8088 with more than 5 MHz maximum clock frequency include the 8088-2, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximum frequency of 8 MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8 MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16 MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licenced Dynalogic Hyperion clone, in a move that was regarded as signalling a major new direction for the company.
When announced, the list price of the 8088 was US $124.80.
Differences to the 8086.
The 8088 is architecturally very similar to the 8086. The main difference is that there are only 8 data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer BHE (this is the high order byte select on the 8086 - the 8088 does not have a high order byte on its 8 bit data bus).:5–97 Instead it outputs a maximum mode status, SSO. Combined with the IO/M and DT/R signals, the bus cycles can be decoded (It generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals if a memory access or input/output access is being made has had it sense reversed. The pin on the 8088 is IO/M. On the 8086 part it is IO/M. The reason for the reversal is that it makes the 8088 compatible with the 8085.:5–98
Performance.
Depending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the average performance for the Intel 8088 ranged from approximately 0.33–1 million instructions per second. Meanwhile, the mov "reg,reg" and ALU "reg,reg" instructions taking two and three cycles respectively yielded an "absolute peak" performance of between 1/3 and 1/2 MIPS per MHz, that is, somewhere in the range 3–5 MIPS at 10 MHz.
The speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to 8 bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the 4-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means 4 clocks to transfer 2 bytes, on the 8088 it is 4 clocks per byte. Therefore, for example, a 2-byte shift or rotate instruction, which takes the EU only 2 clock cycles to execute, actually takes eight clocks to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and 
In general, because so many basic instructions execute in fewer than four clocks per instruction byte—including almost all the ALU and data-movement instructions on register operands and some of these on memory operands—it is practically impossible to avoid idling the EU in the 8088 at least 1/4 of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).
A side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.
The 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. (There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU.) Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four clock period bus transfer cycle is not particularly streamlined. (Contrast the two clock period bus cycle of the 6502 CPU and the 80286's three clock period bus cycle with pipelining down to two cycles for most transfers.) Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. (The same is probably not true on the 80286 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.)
Finally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires 4 clock cycles if not taken, but if taken it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.
Intel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100-200 clock cycles each. Many simple multiplications by small constants (besides powers of two, for which shifts can be used) can be done much faster using dedicated short subroutines. (The 80286 and 80386 each greatly increased the execution speed of these multiply and divide instructions.)
Selection for use in the IBM PC.
The original IBM PC was the most influential microcomputer to use the 8088. It used a clock frequency of 4.77 MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some would have preferred the new Motorola 68000, while others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which had been used in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.
IBM chose the 8088 over the 8086 because Intel offered a better price for the former, and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's) which were already well known by many engineers, further reducing cost.
The descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software compatible processors, which are in use today.

</doc>
<doc id="15066" url="http://en.wikipedia.org/wiki?curid=15066" title="Insulator (electricity)">
Insulator (electricity)

An electrical insulator is a material whose internal electric charges do not flow freely, and therefore make it very hard to conduct an electric current under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. 
A perfect insulator does not exist, because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics.
Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called "insulation". The term "insulator" is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.
Physics of conduction in solids.
Electrical insulation is the absence of electrical conduction. Electronic band theory (a branch of physics) says that a charge flows if states are available into which electrons can be excited. This allows electrons to gain energy and thereby move through a conductor such as a metal. If no such states are available, the material is an insulator.
Most (though not all, see Mott insulator) insulators have a large band gap. This occurs because the "valence" band containing the highest energy electrons is full, and a large energy gap separates this band from the next band above it. There is always some voltage (called the breakdown voltage) that gives electrons enough energy to be excited into this band. Once this voltage is exceeded the material ceases being an insulator, and charge begins to pass through it. However, it is usually accompanied by physical or chemical changes that permanently degrade the material's insulating properties.
Materials that lack electron conduction are insulators if they lack other mobile charges as well. For example, if a liquid or gas contains ions, then the ions can be made to flow as an electric current, and the material is a conductor. Electrolytes and plasmas contain ions and act as conductors whether or not electron flow is involved.
Breakdown.
When subjected to a high enough voltage, insulators suffer from the phenomenon of electrical breakdown. When the electric field applied across an insulating substance exceeds in any location the threshold breakdown field for that substance, the insulator suddenly becomes a conductor, causing a large increase in current, an electric arc through the substance. Electrical breakdown occurs when the electric field in the material is strong enough to accelerate free charge carriers (electrons and ions, which are always present at low concentrations) to a high enough velocity to knock electrons from atoms when they strike them, ionizing the atoms. These freed electrons and ions are in turn accelerated and strike other atoms, creating more charge carriers, in a chain reaction. Rapidly the insulator becomes filled with mobile charge carriers, and its resistance drops to a low level. In a solid, the breakdown voltage is proportional to the band gap energy. The air in a region around a high-voltage conductor can break down and ionise without a catastrophic increase in current; this is called "corona discharge". However if the region of air breakdown extends to another conductor at a different voltage it creates a conductive path between them, and a large current flows through the air, creating an "electric arc". Even a vacuum can suffer a sort of breakdown, but in this case the breakdown or vacuum arc involves charges ejected from the surface of metal electrodes rather than produced by the vacuum itself.
In case of some insulators, the conduction may take place at a very high temperature as then the energy acquired by the valence electrons is sufficient to take them into conduction band.
Uses.
A very flexible coating of an insulator is often applied to electric wire and cable, this is called "insulated wire". Since air is an insulator, in principle no other substance is needed to keep power where it should be. High-voltage power lines commonly use just air, since a solid (e.g. plastic) coating is impractical. However, wires that touch each other produce cross connections, short circuits, and fire hazards. In coaxial cable the center conductor must be supported exactly in the middle of the hollow shield in order to prevent EM wave reflections. Finally, wires that expose voltages higher than 60 V can cause human shock and electrocution hazards. Insulating coatings help to prevent all of these problems.
Some wires have a mechanical covering with no voltage rating—e.g.: service-drop, welding, doorbell, thermostat wire. An insulated wire or cable has a voltage rating and a maximum conductor temperature rating. It may not have an ampacity (current-carrying capacity) rating, since this is dependent upon the surrounding environment (e.g. ambient temperature).
In electronic systems, printed circuit boards are made from epoxy plastic and fibreglass. The nonconductive boards support layers of copper foil conductors. In electronic devices, the tiny and delicate active components are embedded within nonconductive epoxy or phenolic plastics, or within baked glass or ceramic coatings.
In microelectronic components such as transistors and ICs, the silicon material is normally a conductor because of doping, but it can easily be selectively transformed into a good insulator by the application of heat and oxygen. Oxidised silicon is quartz, i.e. silicon dioxide, the primary component of glass.
In high voltage systems containing transformers and capacitors, liquid insulator oil is the typical method used for preventing arcs. The oil replaces air in spaces that must support significant voltage without electrical breakdown. Other high voltage system insulation materials include ceramic or glass wire holders, gas, vacuum, and simply placing wires far enough apart to use air as insulation.
Telegraph and power transmission insulators.
Overhead conductors for high-voltage electric power transmission are bare, and are insulated by the surrounding air. Conductors for lower voltages in distribution may have some insulation but are often bare as well. Insulating supports called "insulators" are required at the points where they are supported by utility poles or transmission towers. Insulators are also required where the wire enters buildings or electrical devices, such as transformers or circuit breakers, to insulate the wire from the case. These hollow insulators with a conductor inside them are called bushings.
Material.
Insulators used for high-voltage power transmission are made from glass, porcelain or composite polymer materials. Porcelain insulators are made from clay, quartz or alumina and feldspar, and are covered with a smooth glaze to shed water. Insulators made from porcelain rich in alumina are used where high mechanical strength is a criterion. Porcelain has a dielectric strength of about 4–10 kV/mm. Glass has a higher dielectric strength, but it attracts condensation and the thick irregular shapes needed for insulators are difficult to cast without internal strains. Some insulator manufacturers stopped making glass insulators in the late 1960s, switching to ceramic materials.
Recently, some electric utilities have begun converting to polymer composite materials for some types of insulators. These are typically composed of a central rod made of fibre reinforced plastic and an outer weathershed made of silicone rubber or ethylene propylene diene monomer rubber (EPDM). Composite insulators are less costly, lighter in weight, and have excellent hydrophobic capability. This combination makes them ideal for service in polluted areas. However, these materials do not yet have the long-term proven service life of glass and porcelain.
Design.
The electrical breakdown of an insulator due to excessive voltage can occur in one of two ways:
Most high voltage insulators are designed with a lower flashover voltage than puncture voltage, so they flash over before they puncture, to avoid damage.
Dirt, pollution, salt, and particularly water on the surface of a high voltage insulator can create a conductive path across it, causing leakage currents and flashovers. The flashover voltage can be reduced by more than 50% when the insulator is wet. High voltage insulators for outdoor use are shaped to maximise the length of the leakage path along the surface from one end to the other, called the creepage length, to minimise these leakage currents. To accomplish this the surface is moulded into a series of corrugations or concentric disc shapes. These usually include one or more "sheds"; downward facing cup-shaped surfaces that act as umbrellas to ensure that the part of the surface leakage path under the 'cup' stays dry in wet weather. Minimum creepage distances are 20–25 mm/kV, but must be increased in high pollution or airborne sea-salt areas.
Types of Insulators.
These are the common classes of insulator:
Cap and pin insulators.
Higher voltage transmission lines usually use modular "cap and pin" insulator designs "(pictures, left)". The wires are suspended from a 'string' of identical disc-shaped insulators that attach to each other with metal clevis pin or ball and socket links. The advantage of this design is that insulator strings with different breakdown voltages, for use with different line voltages, can be constructed by using different numbers of the basic units. Also, if one of the insulator units in the string breaks, it can be replaced without discarding the entire string.
Each unit is constructed of a ceramic or glass disc with a metal cap and pin cemented to opposite sides. In order to make defective units obvious, glass units are designed with Class B construction, so that an overvoltage causes a puncture arc through the glass instead of a flashover. The glass is heat-treated so it shatters, making the damaged unit visible. However the mechanical strength of the unit is unchanged, so the insulator string stays together.
Standard disc insulator units are 25 cm in diameter and 15 cm long, can support a load of 80-120 kN (18-27 klbf), have a dry flashover voltage of about 72 kV, and are rated at an operating voltage of 10-12 kV. However, the flashover voltage of a string is less than the sum of its component discs, because the electric field is not distributed evenly across the string but is strongest at the disc nearest to the conductor, which flashes over first. Metal "grading rings" are sometimes added around the disc at the high voltage end, to reduce the electric field across that disc and improve flashover voltage.
In very high voltage lines the insulator may be surrounded by corona rings. These typically consist of toruses of aluminium (most commonly) or copper tubing attached to the line. They are designed to reduce the electric field at the point where the insulator is attached to the line, to prevent corona discharge, which results in power losses.
History.
The first electrical systems to make use of insulators were telegraph lines; direct attachment of wires to wooden poles was found to give very poor results, especially during damp weather.
The first glass insulators used in large quantities had an unthreaded pinhole. These pieces of glass were positioned on a tapered wooden pin, vertically extending upwards from the pole's crossarm (commonly only two insulators to a pole and maybe one on top of the pole itself). Natural contraction and expansion of the wires tied to these "threadless insulators" resulted in insulators unseating from their pins, requiring manual reseating.
Amongst the first to produce ceramic insulators were companies in the United Kingdom, with Stiff and Doulton using stoneware from the mid-1840s, Joseph Bourne (later renamed Denby) producing them from around 1860 and Bullers from 1868. Utility patent number was granted to Louis A. Cauvet on 25 July 1865 for a process to produce insulators with a threaded pinhole: pin-type insulators still have threaded pinholes.
The invention of suspension-type insulators made high-voltage power transmission possible. Pin-type insulators were unsatisfactory over about 60,000 volts.
A large variety of telephone, telegraph and power insulators have been made; some people collect them, both for their historic interest and for the aesthetic quality of many insulator designs and finishes. One collectors organisation is the US National Insulator Association, which has over 9,000 members.
Insulation of antennas.
Often a broadcasting radio antenna is built as a mast radiator, which means that the entire mast structure is energised with high voltage and must be insulated from the ground. Steatite mountings are used. They have to withstand not only the voltage of the mast radiator to ground, which can reach values up to 400 kV at some antennas, but also the weight of the mast construction and dynamic forces. Arcing horns and lightning arresters are necessary because lightning strikes to the mast are common.
Guy wires supporting antenna masts usually have strain insulators inserted in the cable run, to keep the high voltages on the antenna from short circuiting to ground or creating a shock hazard. Often guy cables have several insulators, placed to break up the cable into lengths unwanted electrical resonances in the guy. These insulators are usually ceramic and cylindrical or egg-shaped (see picture). This construction has the advantage that the ceramic is under compression rather than tension, so it can withstand greater load, and that if the insulator breaks, the cable ends are still linked.
These insulators also have to be equipped with overvoltage protection equipment. For the dimensions of the guy insulation, static charges on guys have to be considered. At high masts these can be much higher than the voltage caused by the transmitter, requiring guys divided by insulators in multiple sections on the highest masts. In this case, guys which are grounded at the anchor basements via a coil - or if possible, directly - are the better choice.
Feedlines attaching antennas to radio equipment, particularly twin lead type, often must be kept at a distance from metal structures. The insulated supports used for this purpose are called "standoff insulators".
Insulation in electrical apparatus.
The most important insulation material is air. A variety of solid, liquid, and gaseous insulators are also used in electrical apparatus. In smaller transformers, generators, and electric motors, insulation on the wire coils consists of up to four thin layers of polymer varnish film. Film insulated magnet wire permits a manufacturer to obtain the maximum number of turns within the available space. Windings that use thicker conductors are often wrapped with supplemental fiberglass insulating tape. Windings may also be impregnated with insulating varnishes to prevent electrical corona and reduce magnetically induced wire vibration. Large power transformer windings are still mostly insulated with paper, wood, varnish, and mineral oil; although these materials have been used for more than 100 years, they still provide a good balance of economy and adequate performance. Busbars and circuit breakers in switchgear may be insulated with glass-reinforced plastic insulation, treated to have low flame spread and to prevent tracking of current across the material.
In older apparatus made up to the early 1970s, boards made of compressed asbestos may be found; while this is an adequate insulator at power frequencies, handling or repairs to asbestos material can release dangerous fibers into the air and must be carried cautiously. Wire insulated with felted asbestos was used in high-temperature and rugged applications from the 1920s. Wire of this type was sold by General Electric under the trade name "Deltabeston."
Live-front switchboards up to the early part of the 20th century were made of slate or marble. Some high voltage equipment is designed to operate within a high pressure insulating gas such as sulfur hexafluoride. Insulation materials that perform well at power and low frequencies may be unsatisfactory at radio frequency, due to heating from excessive dielectric dissipation.
Electrical wires may be insulated with polyethylene, crosslinked polyethylene (either through electron beam processing or chemical crosslinking), PVC, Kapton, rubber-like polymers, oil impregnated paper, Teflon, silicone, or modified ethylene tetrafluoroethylene (ETFE). Larger power cables may use compressed inorganic powder, depending on the application.
Flexible insulating materials such as PVC (polyvinyl chloride) are used to insulate the circuit and prevent human contact with a 'live' wire – one having voltage of 600 volts or less. Alternative materials are likely to become increasingly used due to EU safety and environmental legislation making PVC less economic.
Class 1 and Class 2 insulation.
All portable or hand-held electrical devices are insulated to protect their user from harmful shock.
Class 1 insulation requires that the metal body and other exposed metal parts of the device is connected to earth via a "grounding wire" that is earthed at the main service panel—but only needs basic insulation on the conductors. This equipment needs an extra pin on the power plug for the grounding connection.
Class 2 insulation means that the device is "double insulated". This is used on some appliances such as electric shavers, hair dryers and portable power tools. Double insulation requires that the devices have both basic and supplementary insulation, each of which is sufficient to prevent electric shock. All internal electrically energized components are totally enclosed within an insulated body that prevents any contact with "live" parts. In the EU, double insulated appliances all are marked with a symbol of two squares, one inside the other.

</doc>
<doc id="15067" url="http://en.wikipedia.org/wiki?curid=15067" title="Internetworking">
Internetworking

Internetworking is the practice of connecting a computer network with other networks through the use of gateways that provide a common method of routing information packets between the networks. The resulting system of interconnected networks is called an "internetwork", or simply an "internet". Internetworking is a combination of the words "inter" ("between") and networking; not "internet-working" or "international-network".
The most notable example of internetworking is the Internet, a network of networks based on many underlying hardware technologies, but unified by an internetworking protocol standard, the Internet Protocol Suite, often also referred to as TCP/IP.
The smallest amount of effort to create an internet (an internetwork, not "the" Internet), is to have two LANs of computers connected to each other via a router. Simply using either a switch or a hub to connect two local area networks together doesn't imply internetworking, it just expands the original LAN.
Interconnection of networks.
Internetworking started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network. The original term for an internetwork was catenet.
The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks.
The network elements used to connect individual networks in the ARPANET, the predecessor of the Internet, were originally called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. Today the interconnecting gateways are called Internet routers.
Another type of interconnection of networks often occurs within enterprises at the Link Layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers.
The Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate Transport Layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.
Networking models.
Two architectural models are commonly used to describe the protocols and methods used in internetworking.
The Open System Interconnection (OSI) reference model was developed under the auspices of the International Organization for Standardization (ISO) and provides a rigorous description for layering protocol functions from the underlying hardware to the software interface concepts in user applications. Internetworking is implemented in the Network Layer (Layer 3) of the model.
The Internet Protocol Suite, also called the TCP/IP model of the Internet was not designed to conform to the OSI model and does not refer to it in any of the normative specifications in Requests for Comment and Internet standards. Despite similar appearance as a layered model, it uses a much less rigorous, loosely defined architecture that concerns itself only with the aspects of logical networking. It does not discuss hardware-specific low-level interfaces, and assumes availability of a Link Layer interface to the local network link to which the host is connected. Internetworking is facilitated by the protocols of its Internet Layer.

</doc>
<doc id="15068" url="http://en.wikipedia.org/wiki?curid=15068" title="Infantry">
Infantry

Infantry is the branch of a military force that fights on foot. As the troops who are intended to engage, fight, and defeat the enemy in face-to-face combat, they bear the brunt of warfare and typically suffer the greatest number of casualties. Historically, as the oldest branch of the combat arms, the infantry are the tip of the spear of a modern army, and continually undergo training that is more physically stressful and psychologically demanding than that of any other branch of the combat arms.
Infantry can enter and maneuver in terrain that is inaccessible to military vehicles and employ crew-served infantry support weapons that provide greater and more sustained firepower. The transport and delivery techniques of modern infantrymen to engage in battle include marching, mechanised transport, airborne (by parachute or by helicopter) and amphibious landing from the sea.
History and etymology.
In English, the 16th-century term Infantry (ca. 1570s) describes soldiers who walk to the battlefield, and there engage, fight, and defeat the enemy in direct combat, usually to take and occupy the terrain. As describing the branch of the combat arms, the term "Infantry" derives from the French Infanterie, which, in turn, is derived from the Italian Fanteria and ultimately from the Latin Infantera; the individual-soldier term Infantryman (1837) was not coined until the 19th century. Historically, before the invention and the introduction of firearms to warfare, the foot soldiers of previous eras—armed with blunt and edged weapons, and a shield—also are considered and identified as infantrymen.
The term arose in Sixteenth-Century Spain, which boasted the first professional standing army seen in Europe since the days of Rome. It was common to appoint royal princes (Infantes) to military commands, and the men under them became known as Infanteria.
In the Western world, during the Græco–Roman Antiquity (8th–7th centuries BC), and during the Middle Ages (AD 476–1453), infantry soldiers were categorized, characterised, and identified according to the type of weapons and armour with which they were armed, thus heavy infantry (hoplite) and light infantry (Greek peltasts, Roman velites). Since the application of firearms to warfare, the classifications of infantrymen have changed to reflect their formations on the battlefield, such as line infantry, and to reflect the modes of transporting them to the battlefield, and the tactics deployed by specific types of combat units, such as mechanized infantry and airborne infantry.
Combat role.
As a branch of the armed forces, the role of the infantry in warfare is to engage, fight, and kill the enemy at close range—using either a firearm (rifle, pistol, machine gun), an edged-weapon (knife, bayonet), or bare hands (close quarters combat)—as required by the mission to hand; thus
Beginning with the Napoleonic Wars of the early 19th century, artillery has become an increasingly dominant force on the battlefield. Since World War I, combat aircraft and armoured vehicles have also become dominant. However, the most effective method for locating all enemy forces on a battlefield is still the infantry patrol, and it is the presence or absence of infantry that ultimately determines whether a particular piece of ground has been captured or held. In 20th and 21st century warfare, infantry functions most effectively as part of a combined arms team including artillery, armour, and combat aircraft. Studies have shown that of all casualties, 50% or more were caused by artillery; about 10% were caused by machine guns; 2–5% by rifle fire; and 1% or less by hand grenades, bayonets, knives, and unarmed combat combined. Several infantry divisions both Allied and Axis in the European theatre of WWII suffered higher than 100% combat and non combat casualties and some above 200%, meaning that the number of service personnel that became casualties was greater than the sum of the divisions' available service positions at full strength.
Organization.
Infantry is notable by its reliance on organized formations to be employed in battle. These have been developed over time, but remain a key element to effective infantry development and deployment. Until the end of the 19th century, infantry units were for the most part employed in closely organized formations up until the actual moment of contact with the enemy. This was necessary to allow commanders to retain control of the unit, especially while maneuvering, as well as allowing officers to retain discipline amongst the ranks.
With the development of machine guns and other weapons with increased firepower, it became necessary to disperse soldiers in infantry units to make them less vulnerable to such weapons. From World War I, it was recognized that infantry were most successfully employed when using their ability to maneuver in constricted terrain, and evade detection in ways not possible for other weapons such as vehicles. This decentralization of command was made possible by improved communications equipment and greater focus on small unit training.
Among the various subtypes of infantry is "Medium infantry." This refers to infantry which are less heavily armed and armored than heavy infantry, but more so than light infantry. In the early modern period, medium infantry were largely eliminated due to discontinued use of body armour up until the 20th century. In the United States Army, Stryker Infantry is considered Medium Infantry, since they are "heavier" than light infantry but "lighter" than mechanized infantry.
Doctrine.
Infantry doctrine is the concise expression of how infantry forces contribute to campaigns, major operations, battles, and engagements. It is a guide to action, not a set of hard and fast rules.
Doctrine provides a very common frame of reference across the military forces, allowing the infantry to function cooperatively in what are now called combined arms operations. Doctrine helps standardise operations, facilitating readiness by establishing common ways of accomplishing infantry tasks. Doctrine links theory, history, experimentation, and practice. Its objective is to foster initiative and creative thinking in the infantry's tactical combat environment.
Doctrine provides the infantry with an authoritative body of statements on how infantry forces conduct operations and provides a common lexicon for use by infantry planners and leaders.
Until the development of effective artillery doctrines, and more recently precision guided air delivered ordnance, the most recent important role of the infantry has been as the primary force of inflicting casualties on the enemy through aimed fire. The infantry is also the only combat arm which can ultimately decide whether any given tactical position is occupied, and it is the presence of infantry that assures control of terrain. While the tactics of employment in battle have changed, the basic missions of the infantry have not.
Retractions to the Infantry Concept: Although it has been argued that infantrymen and infantry tactics are an antiquated and careless use of military manpower and resources, the infantryman has proven quite capable against many units, some much more technological and modern. For instance, light infantry has proven to be extremely effective against tank units by being able to take advantage of a tank's limited field of fire and sight by swarming enemy armor units and utilizing anti-armor rockets at long range or grenades in close quarters. Furthermore, air bombardment that can flatten entire cities has been shown to be completely useless against a dug in infantry force. (see Battle of Stalingrad 1942–1943) Even an occupying enemy police force has sometimes been shown to be a poor match against a clandestine infantry that has secreted itself away in a civilian population. (see French Resistance WWII, Iraqi Insurgency in Fallujah, American Revolution)
Operations.
Attack operations are the most basic role of the infantry, and along with defense, form the main stances of the infantry on the battlefield. Traditionally, in an open battle, or meeting engagement, two armies would maneuver to contact, at which point they would form up their infantry and other units opposite each other. Then one or both would advance and attempt to defeat the enemy force. The goal of an attack remains the same: to advance into an enemy-held "objective," most frequently a hill, river crossing, city or other dominant terrain feature, and dislodge the enemy, thereby establishing control of the objective.
Attacks are often feared by the infantry conducting them because of the high number of casualties suffered while advancing to close with and destroy the enemy while under enemy fire. In mechanized infantry the armored personnel carrier (APC) is considered the assaulting position. These APCs can deliver infantrymen through the front lines to the battle and—in the case of infantry fighting vehicles—contribute supporting firepower to engage the enemy. Successful attacks rely on sufficient force, preparative reconnaissance and battlefield preparation with bomb assets. Retention of discipline and cohesion throughout the attack is paramount to success. A subcategory of attacks is the ambush, where infantrymen lie in wait for enemy forces before attacking at a vulnerable moment. This gives the ambushing infantrymen the combat advantage of surprise, concealment and superior firing positions, and causes confusion. The ambushed unit does not know what it is up against, or where they are attacking from.
Defense operations are the natural counter to attacks, in which the mission is to hold an objective and defeat enemy forces attempting to dislodge the defender. Defensive posture offers many advantages to the infantry, including the ability to use terrain and constructed fortifications to advantage; these reduce exposure to enemy fire compared with advancing forces. Effective defense relies on minimizing losses to enemy fire, breaking the enemy's cohesion before their advance is completed, and preventing enemy penetration of defensive positions.
Patrol is the most common infantry mission. Full-scale attacks and defensive efforts are occasional, but patrols are constant. Patrols consist of small groups of infantry moving about in areas of possible enemy activity to locate the enemy and destroy them when found. Patrols are used not only on the front-lines, but in rear areas where enemy infiltration or insurgencies are possible.
Pursuit is a role that the infantry often assumes. The objective of pursuit operations is the destruction of withdrawing enemy forces which are not capable of effectively engaging friendly units, before they can build their strength to the point where they are effective. Infantry traditionally have been the main force to overrun these units in the past, and in modern combat are used to pursue enemy forces in constricted terrain (urban areas in particular), where faster forces, such as armoured vehicles are incapable of going or would be exposed to ambush.
Escort consists of protecting support units from ambush, particularly from hostile infantry forces. Combat support units (a majority of the military) are not as well armed or trained as infantry units and have a different mission. Therefore, they need the protection of the infantry, particularly when on the move. This is one of the most important roles for the modern infantry, particularly when operating alongside armored vehicles. In this capacity, infantry essentially conducts patrol on the move, scouring terrain which may hide enemy infantry waiting to ambush friendly vehicles, and identifying enemy strong points for attack by the heavier units.
Maneuver operations consume much of an infantry unit's time. Infantry, like all combat arms units, are often maneuvered to meet battlefield needs, and often must do so under enemy attack. The infantry must maintain their cohesion and readiness during the move to ensure their usefulness when they reach their objective. Traditionally, infantry have relied on their own legs for mobility, but mechanised or armoured infantry often uses trucks and armored vehicles for transport. These units can quickly disembark and transition to light infantry, without vehicles, to access terrain which armoured vehicles can't effectively access.
Reconnaissance/intelligence gathering Surveillance operations are often carried out with the employment of small recon units or sniper teams which gather information about the enemy, reporting on characteristics such as size, activity, location, unit and equipment. These infantry units typically are known for their stealth and ability to operate for periods of time within close proximity of the enemy without being detected. They may engage high profile targets, or be employed to hunt down terrorist cells and insurgents within a given area. These units may also entice the enemy to engage a located recon unit, thus disclosing their location to be destroyed by more powerful friendly forces.
Reserve assignments for infantry units involve deployment behind the front, although patrol and security operations are usually maintained in case of enemy infiltration. This is usually the best time for infantry units to integrate replacements into units and to maintain equipment. Additionally, soldiers can be rested and general readiness should improve. However, the unit must be ready for deployment at any point.
Construction can be undertaken either in reserve or on the front, but consists of using infantry troops as labor for construction of field positions, roads, bridges, airfields, and all other manner of structures. The infantry is often given this assignment because of the physical quantity of strong men within the unit, although it can lessen a unit's morale and limit the unit's ability to maintain readiness and perform other missions. More often, such jobs are given to specialist engineering corps.
Base defense – Infantry units are tasked to protect certain areas like command posts or airbases. Units assigned to this job usually have a large number of military police attached to them for control of checkpoints and prisons.
Raid/Hostage Rescue – Infantry units are trained to quickly mobilise, infiltrate, enter and neutralise threat forces when appropriate combat intelligence indicates to secure a location, rescue or capture high profile targets.
Urban Combat – Urban combat poses unique challenges to the combat forces. It is one of the most complicated type of operations an infantry unit will undertake. With many places for the enemy to hide and ambush from, infantry units must be trained in how to enter a city, and systematically clear the buildings, which most likely will be booby trapped, in order to kill or capture enemy personnel within the city. Care must be taken to differentiate innocent civilians who often hide and support the enemy from the nonuniformed armed enemy forces. Civilian and military casualties both are usually very high.
Day to day service.
Because of an infantryman's duties with firearms, explosives, physical and emotional stress, physical violence, casualties and deaths are not uncommon in both war and in peacetime training or operations. It is a highly dangerous and demanding combat service and in World War II military doctors concluded that even physically unwounded soldiers were psychologically worn out after about 200 days of combat.
The physical, mental and environmental operating demands of the infantryman are high. All of the combat necessities such as ammunition, weapon systems, food, water, clothing and shelter are carried on the backs of the infantrymen, at least in light role as opposed to mounted/mechanised. Combat loads of over 36 kg (80 lbs) are standard, and greater loads in excess of 45 kg (100 lbs) are very common. , These heavy loads, combined with long foot patrols of over 40 km a day, in any climate from 43 to in temperature, require the infantryman to be in good physical and mental shape. Infantrymen live, fight and die outdoors in all types of brutal climates, often with no physical shelter. Poor climate conditions adds misery to this already demanding existence. Disease epidemics, frostbite, heat stroke, trench foot, insect and wild animal bites are common along with stress disorders and these have on multiple occsions caused more casualties than enemy action.
Despite the hardships, infantrymen are expected to continue with their combat missions despite death and injury of friends, fear, despair, fatigue and bodily injury.
Some infantry units are considered Special Forces. The earliest Special Forces commando units were more highly trained infantrymen, with special weapons, equipment and missions. Special Forces units recruit heavily from regular infantry units to fill their ranks.
Foreign and domestic militaries typically have a slang term for their infantrymen. In the U.S. military, the slang term among both Marine and Army infantrymen for themselves is "grunt." In the British Army, they are the "squaddies." The infantry is a small close-knit community, and the slang names are terms of endearment that convey mutual respect and shared experiences.
Equipment and training.
In the past infantrymen were just a mass of hastily trained conscripts hastily armed with whatever could be quickly provided. In modern times, the infantryman can be a highly trained and equipped specialist in his own right.
The equipment of infantry forces has evolved along with the development of military technology and tactics in general, but certain constants remain regarding the design and selection of this equipment. Primary types of equipment are weaponry, protective gear, survival gear, and special, mission specific equipment. Infantry tactics have become much more involved, and yet must be learned and rehearsed until they become second nature when the infantry soldier is stumbling with fatigue and in the middle of the "fog of war." Spreading out, making use of cover and concealment, monitoring team-mates and leaders, and watching for the enemy must all become instinctive and simultaneous.
Infantry weapons have included all types of personal weapons, i.e., anything that can be handled by individual soldiers, as well as some light crew-served weapons that can be carried. During operations, especially in modern times, the infantry often scavenge and employ whatever weapons and equipment they can acquire from both friend and foe, in addition to those issued to them by their available supply chain.
Infantry of ancient times through the Renaissance wielded a wide array of non-gunpowder weaponry. Infantry formations used all sorts of melee weapons, such as various types of swords, axes, and maces; shock weapons, such as spears and pikes; and ranged weapons such as javelins, bows, and slings. Their crew-served weapons were the ballista and the battering ram. Infantry of these premodern periods also wore a variety of personal body armour, including chain mail and cuirasses. Many of the premodern infantry weapons evolved over time to counter these advances in body armor, such as the falchion and crossbow, which were designed to pierce chain mail armor and wound the underlying body.
Modern infantrymen may be trained to use equipment in addition to their personal rifles, such as hand guns or pistols, shotguns, machine guns, anti-tank missiles, anti-personnel mines, other incendiary and explosive devices, bayonets, GPS, map and compass, encrypted communications equipment, booby traps, surveillance equipment, night vision equipment, sensitive intelligence documents, classified weapon systems and other sensitive equipment.
Protective equipment and survival gear.
Infantry protective gear includes all equipment designed to protect the soldier against enemy attack. Most protective gear comprises personal armor of some type. Ancient and medieval infantry used shields and wore leather and metal alloys for armour, as defense against both missile and hand-to-hand weapons. With the advent of effective firearms such as the arquebus, large numbers of men could be quickly trained into effective fighting forces, and such armour became thicker while providing less overall coverage to meet the threat of early firearms, which could only pierce this armour at close range. Generally, only pikemen were armoured in this fashion; gunners went largely unarmoured, due to the expense as well as the impracticality of armouring large numbers of men who were not expected to fight in close quarters where it would be most useful. As firearms became more powerful and armour became less useful against gunfire, the ratio of gunners to pikemen increased, until the advent of the bayonet rendered the latter entirely obsolete. While it became clear to most military leaders that the pikeman was now outdated, some armies stubbornly clung to the pike, though pikemen, too, would abandon their armour, until only specialized and prestigious cavalry units retained any significant armour coverage; the infantryman from this point went entirely unarmoured. The return to the use of the helmet was prompted by the need to defend against high explosive fragmentation and concussion, and further developments in materials led to effective bullet-defeating body armour such as Kevlar, within the weight acceptable for infantry use.
Beginning in the Vietnam War, the use of personal body armour has again become widespread among infantry units. Infantrymen must also often carry protective measures against chemical and biological attack, including gas masks, counter-agents, and protective suits. All of these protective measures add to the weight an infantryman must carry, and may decrease combat efficiency. Modern militaries are struggling to balance the value of personal body protection versus the weight burden and ability to move under such weight.
Infantry survival gear includes all of the items soldiers require for day-to-day survival in the combat environment. These include basic environmental protections, medical supplies, food, and sundries. As the amount of equipment a soldier can carry is very limited, efforts have been made to make equipment light and compact. Equipment is carried in tactical gear (such as ALICE), which should be comfortable to wear for extended periods of time, hamper movement as little as possible and be compatible with other things a soldier can be expected to carry, such as field radios and spare ammunition. Infantry have suffered high casualty rates from disease, exposure, and privation—often in excess of the casualties suffered from enemy attacks. Better equipment of troops to protect against these environmental factors greatly reduces these rates of loss. One of the most valuable pieces of gear is the entrenching tool—basically a folding spade—which can be employed not only to dig important defenses, but also in a variety of other daily tasks and even as a weapon.
Specialized equipment consists of a variety of gear which may or may not be carried, depending on the mission and the level of equipment of an army. Communications gear has become a necessity, as it allows effective command of infantry units over greater distances, and communication with artillery and other support units. In some units, individual communications are being used to allow the greatest level of flexibility. Engineering equipment, including explosives, mines, and other gear, is also commonly carried by the infantry or attached specialists. A variety of other gear, often relating to a specific mission, or to the particular terrain in which the unit is employed, can be carried by infantry units.
Other infantry.
Infantry in air forces, such as the Royal Air Force Regiment and the Royal Australian Air Force Airfield Defence Guards, are used primarily for ground-based defence of air bases and other, air force facilities. They also have a number of other, specialist roles, including Chemical, Biological, Radiological and Nuclear (CBRN) defence and training other, air force personnel in basic ground defence tactics.
Naval infantry, commonly known as marines, are a category of infantry that form part of a state’s naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

</doc>
<doc id="15069" url="http://en.wikipedia.org/wiki?curid=15069" title="Identity function">
Identity function

In mathematics, an identity function, also called an identity relation or identity map or identity transformation, is a function that always returns the same value that was used as its argument. In equations, the function is given by "f"("x") = "x".
Definition.
Formally, if "M" is a set, the identity function "f" on "M" is defined to be that function with domain and codomain "M" which satisfies
In other words, the function assigns to each element "x" of "M" the element "x" of "M".
The identity function "f" on "M" is often denoted by id"M".
In set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or "diagonal" of "M".
Algebraic property.
If "f" : "M" → "N" is any function, then we have "f" ∘ id"M" = "f" = id"N" ∘ "f" (where "∘" denotes function composition). In particular, id"M" is the identity element of the monoid of all functions from "M" to "M".
Since the identity element of a monoid is unique, one can alternately define the identity function on "M" to be this identity element. Such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of "M" need not be functions.

</doc>
<doc id="15070" url="http://en.wikipedia.org/wiki?curid=15070" title="Intel 80386">
Intel 80386

The Intel 80386 ("eighty-three-eighty-six"), also known as i386 or just 386, is a 32-bit microprocessor introduced in 1985. The first versions had 275,000 transistors and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture, the 80386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the "i386-architecture", "x86", or "IA-32", depending on context.
The 32-bit 80386 can correctly execute most code intended for the earlier 16-bit processors such as 8088 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086). A 33 MHz 80386 was reportedly measured to operate at about 11.4 MIPS.
The 80386 was introduced in October 1985, while manufacturing of the chips in significant quantities commenced in June 1986. Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was rationalized upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq and marked the first time a fundamental component in the IBM PC compatible "de facto"-standard was updated by a company other than IBM.
In May 2006, Intel announced that 80386 production would stop at the end of September 2007. Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an 80386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the 80386 processor, such as BlackBerry 950 and Nokia 9000 Communicator.
Architecture.
The processor was a significant evolution in the x86 architecture, and extended a long line of processors that stretched back to the Intel 8008. The predecessor of the 80386 was the Intel 80286, a 16-bit processor with a segment-based memory management and protection system. The 80386 added a 32-bit architecture and a paging translation unit, which made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging.
The 80386 featured three operating modes: real mode, protected mode and virtual mode. The protected mode which debuted in the 286 was extended to allow the 386 to address up to 4 GB of memory. The all new virtual 8086 mode (or "VM86") made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible.
The ability for a 386 to be set up to act like it had a flat memory model in protected mode despite the fact that it uses a segmented memory model in all modes would arguably be the most important feature change for the x86 processor family until AMD released x86-64 in 2003.
Chief architect in the development of the 80386 was John H. Crawford. He was responsible for extending the 80286 architecture and instruction set to 32-bit, and then led the microprogram development for the 80386 chip.
The 80486 and P5 Pentium line of processors were descendants of the 80386 design.
Datatypes of 80386.
The following data types are directly supported and thus implemented by one or more 80386 machine instructions; these data types are described here in brief. (source:, ):
Example code.
The following 80386 assembly source code is for a subroutine named codice_1 that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time.
The example code uses the EBP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory.
Chip variants.
The 80386SX variant.
In 1988, Intel introduced the 80386SX, most often referred to as the 386SX, a cut-down version of the 80386 with a 16-bit data bus mainly intended for lower cost PCs aimed at the home, educational, and small business markets while the 386DX would remain the high end variant used in workstations, servers, and other demanding tasks. The CPU remained fully 32-bit internally, but the 16-bit bus was intended to simplify circuit board layout and reduce total cost. The 16-bit bus simplified designs but hampered performance. Only 24 pins were connected to the address bus, therefore limiting addressing to 16 MB, but this was not a critical constraint at the time. Performance differences were due not only to differing data bus-widths, but also due to performance-enhancing cache memories often employed on boards using the original chip.
The original 80386 was subsequently renamed 80386DX to avoid confusion. However, Intel subsequently used the 'DX' suffix to refer to the floating-point capability of the 80486DX. The 80387SX was an 80387 part that was compatible with the 386SX (i.e. with a 16-bit databus). The 386SX was packaged in a surface-mount QFP, and sometimes offered in a socket to allow for an upgrade.
The i386SL variant.
The i386SL was introduced as a power efficient version for laptop computers. The processor offered several power management options (e.g. SMM), as well as different "sleep" modes to conserve battery power. It also contained support for an external cache of 16 to 64 kB. The extra functions and circuit implementation techniques caused this variant to have over 3 times as many transistors as the i386DX. The i386SL was first available at 20 MHz clock speed, with the 25 MHz model later added.
Business importance.
The first company to design and manufacture a PC based on the Intel 80386 was Compaq. By extending the 16/24-bit IBM PC/AT standard into a natively 32-bit computing environment, Compaq became the first third party to implement a major technical hardware advance on the PC platform. IBM was offered use of the 80386, but had manufacturing rights for the earlier 80286. IBM therefore chose to rely on that processor for a couple of more years. The early success of the Compaq 386 PC played an important role in legitimizing the PC "clone" industry, and in de-emphasizing IBM's role within it.
Prior to the 386, the difficulty of manufacturing microchips and the uncertainty of reliable supply made it desirable that any mass-market semiconductor be multi-sourced, that is, made by two or more manufacturers, the second and subsequent companies manufacturing under license from the originating company. The 386 was for "a time" (4.7 yrs) only available from Intel, since Andy Grove, Intel's CEO at the time, made the decision not to encourage other manufacturers to produce the processor as second sources. This decision was ultimately crucial to Intel's success in the market. The 386 was the first significant microprocessor to be single-sourced. Single-sourcing the 386 allowed Intel greater control over its development and substantially greater profits in later years.
AMD introduced its compatible Am386 processor in March 1991 after overcoming legal obstacles, thus ending Intel's 4.7 yr monopoly on 386-compatible processors. From 1991 IBM also manufactured 386 chips under license for use only in IBM PCs and boards.
Early problems.
Intel originally intended for the 80386 to debut at 16 MHz. However, due to poor yields, it was instead introduced at 12 MHz.
Early in production, Intel discovered a marginal circuit that could cause a system to return incorrect results from 32-bit multiply operations. Not all of the processors already manufactured were affected, so Intel tested its inventory. Processors that were found to be bug-free were marked with a double-sigma (ΣΣ), and affected processors were marked "16 BIT S/W ONLY". These latter processors were sold as good parts, since at the time 32 bit capability was not relevant for most users. Such chips are now extremely rare.
The i387 math coprocessor was not ready in time for the introduction of the 80386, and so many of the early 80386 motherboards instead provided a socket and hardware logic to make use of an 80287. In this configuration the FPU would operate asynchronously to the CPU, usually with a clock rate of 10 MHz. The original Compaq Deskpro 386 is an example of such design. However, this was an annoyance to those who depended on floating point performance, as the performance advantages of the 80387 over the 80287 were significant.
Pin-compatible upgrades.
Intel later offered a modified version of its 80486DX in 80386 packaging, branded as the Intel RapidCAD. This provided an upgrade path for users with 80386-compatible hardware. The upgrade was a pair of chips that replaced both the 80386 and 80387. Since the 80486DX design contained an FPU, the chip that replaced the 80386 contained the floating point functionality, and the chip that replaced the 80387 served very little purpose. However, the latter chip was necessary in order to provide the FERR signal to the mainboard and appear to function as a normal floating point unit. The CAD branding referred to the ease of upgrading existing OEM designs from 386 to 486 CPUs with rapid turn-around in the CAD room.
Third parties offered a wide range of upgrades, for both SX and DX systems. The most popular ones were based on the Cyrix 486DLC/SLC core, which typically offered a substantial speed improvement due to its more efficient instruction pipeline and internal L1 SRAM cache. The cache was usually 1 kB, or sometimes 8 kB in the TI variant. Some of these upgrade chips (such as the 486DRx2/SRx2) were marketed by Cyrix themselves, but they were more commonly found in kits offered by upgrade specialists such as Kingston, Evergreen and Improve-It Technologies. Some of the fastest CPU upgrade modules featured the IBM SLC/DLC family (notable for its 16 kB L1 cache), or even the Intel 486 itself. Many 386 upgrade kits were advertised as being simple drop-in replacements, but often required complicated software to control the cache or clock doubling. Part of the problem was that on most 386 motherboards, the A20 line was controlled entirely by the motherboard with the CPU being unaware, which caused problems on CPUs with internal caches.
Overall it was very difficult to configure upgrades to produce the results advertised on the packaging, and upgrades were often less than 100% stable or less than 100% compatible.
Models and variants.
Early 5V models.
80386DX.
Original version, released in October 1985.
RapidCAD.
A specially packaged Intel 486DX and a dummy floating point unit (FPU) designed as pin-compatible replacements for an Intel 80386 processor and 80387 FPU.
Versions for embedded systems.
80376.
This was an embedded version of the 80386SX which did not support real mode and paging in the MMU.
i386EX, i386EXTB and i386EXTC.
System and power management and built in peripheral and support functions: Two 82C59A interrupt controllers; Timer, Counter (3 channels); Asynchronous SIO (2 channels); Synchronous SIO (1 channel); Watchdog timer (Hardware/Software); PIO. Usable with 80387SX or i387SL FPUs.
i386CXSA and i386SXSA (or i386SXTA).
Transparent power management mode, integrated MMU and TTL compatible inputs (only 386SXSA). Usable with i387SX or i387SL FPUs.
i386CXSB.
Transparent power management mode and integrated MMU. Usable with i387SX or i387SL FPUs.

</doc>
<doc id="15072" url="http://en.wikipedia.org/wiki?curid=15072" title="Instruction register">
Instruction register

In computing, an instruction register (IR) is the part of a CPU's control unit that stores the instruction currently being executed or decoded. In simple processors each instruction to be executed is loaded into the instruction register which holds it while it is decoded, prepared and ultimately executed, which can take several steps. 
Some of the complicated processors use a pipeline of instruction registers where each stage of the pipeline does part of the decoding, preparation or execution and then passes it to the next stage for its step. Modern processors can even do some of the steps out of order as decoding on several instructions is done in parallel.
Decoding the op-code in the instruction register includes determining the instruction, determining where its operands are in memory, retrieving the operands from memory, allocating processor resources to execute the command (in super scalar processors), etc.
The output of IR is available to control circuits which generate the timing signals that control the various processing elements involved in executing the instruction.
In the Instruction cycle, the instruction is loaded into the Instruction register after the processor fetches it from the memory location pointed by the Program counter.

</doc>
<doc id="15073" url="http://en.wikipedia.org/wiki?curid=15073" title="List of islands">
List of islands

This is a list of islands in the world grouped by oceans and by continents. For rank-order lists, see the other lists below.

</doc>
<doc id="15075" url="http://en.wikipedia.org/wiki?curid=15075" title="INTERCAL">
INTERCAL

The Compiler Language With No Pronounceable Acronym, abbreviated INTERCAL, is an esoteric programming language that was created as a parody by Don Woods and James M. Lyon, two Princeton University students, in 1972. It satirizes aspects of the various programming languages at the time, as well as the proliferation of proposed language constructs and notations in the 1960s.
There are two currently maintained versions of INTERCAL: C-INTERCAL, formerly maintained by Eric S. Raymond, and CLC-INTERCAL, maintained by Claudio Calvelli.
History.
According to the original manual by the authors,
The full name of the compiler is "Compiler Language With No Pronounceable Acronym," which is, for obvious reasons, abbreviated "INTERCAL."
The original Princeton implementation used punched cards and the EBCDIC character set. To allow INTERCAL to run on computers using ASCII, substitutions for two characters had to be made: $ substituted for ¢ as the "mingle" operator, "represent[ing] the increasing cost of software in relation to hardware", and ? was substituted for ⊻ as the unary exclusive-or operator to "correctly express the average person's reaction on first encountering exclusive-or". In recent versions of C-INTERCAL, the older operators are supported as alternatives; INTERCAL programs may now be encoded in ASCII, Latin-1, or UTF-8.
Details.
INTERCAL was intended to be completely different from all other computer languages. Common operations in other languages have cryptic and redundant syntax in INTERCAL. From the INTERCAL Reference Manual:
It is a well-known and oft-demonstrated fact that a person whose work is incomprehensible is held in high esteem. For example, if one were to state that the simplest way to store a value of 65536 in a 32-bit INTERCAL variable is:
any sensible programmer would say that that was absurd. Since this is indeed the simplest method, the programmer would be made to look foolish in front of his boss, who would of course happen to turn up, as bosses are wont to do. The effect would be no less devastating for the programmer having been correct.
INTERCAL has many other features designed to make it even more aesthetically unpleasing to the programmer: it uses statements such as "READ OUT", "IGNORE", "FORGET", and modifiers such as "PLEASE". This last keyword provides two reasons for the program's rejection by the compiler: if "PLEASE" does not appear often enough, the program is considered insufficiently polite, and the error message says this; if too often, the program could be rejected as excessively polite. Although this feature existed in the original INTERCAL compiler, it was undocumented.
Despite the language's intentionally obtuse and wordy syntax, INTERCAL is nevertheless Turing-complete: given enough memory, INTERCAL can solve any problem that a Universal Turing machine can solve. Most implementations of INTERCAL do this very slowly, however. A Sieve of Eratosthenes benchmark, computing all prime numbers less than 65536, was tested on a Sun SPARCstation 1. In C, it took less than half a second; the same program in INTERCAL took over seventeen hours.
Documentation.
The INTERCAL Reference Manual contains many paradoxical, nonsensical, or otherwise humorous instructions:
Caution! Under no circumstances confuse the mesh with the interleave operator, except under confusing circumstances!
The manual also contains a "tonsil", as explained in this footnote: "4) Since all other reference manuals have Appendices, it was decided that the INTERCAL manual should contain some other type of removable organ."
The INTERCAL manual gives unusual names to all non-alphanumeric ASCII characters: single and double quotes are "sparks" and "rabbit ears" respectively. (The exception is the ampersand: as the Jargon File states, "what could be sillier?") The assignment operator, represented as an equals sign (INTERCAL's "half mesh") in many other programming languages, is in INTERCAL a left-arrow, codice_1, referred to as "gets" and made up of an "angle" and a "worm".
Syntax.
Input (using the codice_2 instruction) and output (using the codice_3 instruction) do not use the usual formats; in INTERCAL-72, WRITE IN inputs a number written out as digits in English (such as SIX FIVE FIVE THREE FIVE), and READ OUT outputs it in "butchered" Roman numerals. More recent versions have their own I/O systems.
Comments can be achieved by using the inverted statement identifiers involving NOT or N'T; these cause lines to be initially ABSTAINed so that they have no effect. (A line can be ABSTAINed from even if it doesn't have valid syntax; syntax errors happen at runtime, and only then when the line is un-ABSTAINed.)
Data structures.
INTERCAL-72 (the original version of INTERCAL) had only four data types: the 16-bit integer (represented with a codice_4, called a "spot"), the 32-bit integer (codice_5, a "twospot"), the array of 16-bit integers (codice_6, a "tail"), and the array of 32-bit integers (codice_7, a "hybrid"). There are 65535 available variables of each type, numbered from codice_8 to codice_9 for 16-bit integers, for instance. However, each of these variables has its own stack on which it can be pushed and popped (STASHed and RETRIEVEd, in INTERCAL terminology), increasing the possible complexity of data structures. (More modern versions of INTERCAL have by and large kept the same data structures, with appropriate modifications; TriINTERCAL, which modifies the radix with which numbers are represented, can use a 10-trit type rather than a 16-bit type), and CLC-INTERCAL implements many of its own data structures, such as "classes and lectures", by making the basic data types store more information rather than adding new types. Arrays are dimensioned by assigning to them as if they were a scalar variable. Constants can also be used, and are represented by a codice_10 ("mesh") followed by the constant itself, written as a decimal number; only integer constants from 0 to 65535 are supported.
Operators.
There are only five operators in INTERCAL-72. Implementations vary in which characters represent which operation, and many accept more than one character, so more than one possibility is given for many of the operators.
Contrary to most other languages, AND, OR, and XOR are unary operators, which work on consecutive bits of their argument; the most significant bit of the result is the operator applied to the most significant and least significant bits of the input, the second-most-significant bit of the result is the operator applied to the most and second-most significant bits, the third-most-significant bit of the result is the operator applied to the second-most and third-most bits, and so on. The operator is placed between the punctuation mark specifying a variable name or constant and the number that specifies which variable it is, or just inside grouping marks (i.e. one character later than it would be in programming languages like C.) SELECT and INTERLEAVE (which is also known as MINGLE) are infix binary operators; SELECT takes the bits of its first operand that correspond to "1" bits of its second operand and removes the bits that correspond to "0" bits, shifting towards the least significant bit and padding with zeroes (so 51 (110011 in binary) SELECT 21 (10101 in binary) is 5 (101 in binary)); MINGLE alternates bits from its first and second operands (in such a way that the least significant bit of its second operand is the least significant bit of the result). There is no operator precedence; grouping marks must be used to disambiguate the precedence where it would otherwise be ambiguous (the grouping marks available are codice_11 ("spark"), which matches another spark, and codice_12 ("rabbit ears"), which matches another rabbit ears; the programmer is responsible for using these in such a way that they make the expression unambiguous).
Control structures.
INTERCAL statements all start with a "statement identifier"; in INTERCAL-72, this can be codice_13, codice_14, or codice_15, all of which mean the same to the program (but using one of these too heavily causes the program to be rejected, an undocumented feature in INTERCAL-72 that was mentioned in the C-INTERCAL manual), or an inverted form (with codice_16 or codice_17 appended to the identifier). Backtracking INTERCAL, a modern variant, also allows variants using codice_18 (possibly combined with PLEASE or DO) as a statement identifier, which introduces a choice-point. Before the identifier, an optional line number (an integer enclosed in parentheses) can be given; after the identifier, a percent chance of the line executing can be given in the format codice_19, which defaults to 100%.
In INTERCAL-72, the main control structures are NEXT, RESUME, and FORGET. codice_20 branches to the line specified, remembering the next line that would be executed if it weren't for the NEXT on a call stack (other identifiers than DO can be used on any statement, DO is given as an example); codice_21 removes "expression" entries from the top of the call stack (this is useful to avoid the error that otherwise happens when there are more than 80 entries), and codice_22 removes "expression" entries from the call stack and jumps to the last line remembered.
C-INTERCAL also provides the COME FROM instruction, written codice_23; CLC-INTERCAL and the most recent C-INTERCAL versions also provide computed COME FROM (codice_24 and NEXT FROM, which is like COME FROM but also saves a return address on the NEXT STACK.
Alternative ways to affect program flow, originally available in INTERCAL-72, are to use the IGNORE and REMEMBER instructions on variables (which cause writes to the variable to be silently ignored and to take effect again, so that instructions can be disabled by causing them to have no effect), and the ABSTAIN and REINSTATE instructions on lines or on types of statement, causing the lines to have no effect or to have an effect again respectively.
Hello, world.
The traditional "Hello, world!" program demonstrates how different INTERCAL is from standard programming languages. In C, it could read as follows:
The equivalent program in C-INTERCAL is longer and harder to read:
Dialects.
The original Woods–Lyon INTERCAL was very limited in its input/output capabilities: the only acceptable input were numbers with the digits spelled out, and the only output was an extended version of Roman numerals. A while later, there was an "Atari implementation", about which notes are provided in the INTERCAL reference manual; it "differs from the original Princeton version primarily in the use of ASCII rather than EBCDIC".
The C-INTERCAL reimplementation, being available on the Internet, has made the language more popular with devotees of esoteric programming languages. The C-INTERCAL dialect has a few differences from original INTERCAL and introduced a few new features, such as a COME FROM statement and a means of doing text I/O based on the Turing Text Model.
The authors of C-INTERCAL also created the TriINTERCAL variant, based on the Ternary numeral system and generalizing INTERCAL's set of operators.
A more recent variant is Threaded Intercal, which extends the functionality of COME FROM to support multithreading.
Impact and discussion.
In the article "A Box, Darkly: Obfuscation, Weird Languages, and Code Aesthetics", INTERCAL is described under the heading "Abandon all sanity, ye who enter here: INTERCAL". The compiler and commenting strategy are among the "weird" features described:
The compiler, appropriately named "ick", continues the parody. Anything the compiler can't understand, which in a normal language would result in a compilation error, is just skipped. This "forgiving" feature makes finding bugs very difficult; it also introduces a unique system for adding program comments. The programmer merely inserts non-compileable text anywhere in the program, being careful not to accidentally embed a bit of valid code in the middle of their comment.
In "Technomasochism", Lev Bratishenko characterizes the INTERCAL compiler as a dominatrix:
If PLEASE was not encountered often enough, the program would be rejected; that is, ignored without explanation by the compiler. Too often and it would still be rejected, this time for sniveling. Combined with other words that are rarely used in programming languages but appear as statements in INTERCAL, the code reads like someone pleading.

</doc>
<doc id="15076" url="http://en.wikipedia.org/wiki?curid=15076" title="International Data Encryption Algorithm">
International Data Encryption Algorithm

In cryptography, the International Data Encryption Algorithm (IDEA), originally called Improved Proposed Encryption Standard (IPES), is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991. The algorithm was intended as a replacement for the Data Encryption Standard (DES). IDEA is a minor revision of an earlier cipher, Proposed Encryption Standard (PES).
The cipher was designed under a research contract with the Hasler Foundation, which became part of Ascom-Tech AG. The cipher was patented in a number of countries but was freely available for non-commercial use. The name “IDEA” is also a trademark. The last patents expired in 2012 and IDEA is now patent-free and thus free to use.
IDEA was used in Pretty Good Privacy (PGP) v2.0, and was incorporated after the original cipher used in v1.0, BassOmatic, was found to be insecure. IDEA is an optional algorithm in the OpenPGP standard.
Operation.
IDEA operates on 64-bit blocks using a 128-bit key, and consists of a series of eight identical transformations (a "round", see the illustration) and an output transformation (the "half-round"). The processes for encryption and decryption are similar. IDEA derives much of its security by interleaving operations from different groups — modular addition and multiplication, and bitwise eXclusive OR (XOR) — which are algebraically "incompatible" in some sense. In more detail, these operators, which all deal with 16-bit quantities, are:
After the eight rounds comes a final “half round”, the output transformation illustrated below (the swap of the middle two values cancels out the swap at the end of the last round, so that there is no net swap):
Structure.
The overall structure of IDEA follows the Lai-Massey scheme. XOR is used for both subtraction and addition. IDEA uses a key-dependent half-round function. To work with 16 bit words (meaning four inputs instead of two for the 64 bit block size), IDEA uses the Lai-Massey scheme twice in parallel, with the two parallel round functions being interwoven with each other. To ensure sufficient diffusion, two of the sub-blocks are swapped after each round.
Key schedule.
Each round uses six 16-bit sub-keys, while the half-round uses four, a total of 52 for 8.5 rounds. The first eight sub-keys are extracted directly from the key, with K1 from the first round being the lower sixteen bits; further groups of eight keys are created by rotating the main key left 25 bits between each group of eight. This means that it is rotated less than once per round, on average, for a total of six rotations.
Decryption.
Decryption works like encryption, but the order of the round keys is inverted, and each value of subkeys K1 – K4 is replaced by its inverse for the respective group operation (K5 and K6 of each group should not be changed for decryption).
Security.
The designers analysed IDEA to measure its strength against differential cryptanalysis and concluded that it is immune under certain assumptions. No successful linear or algebraic weaknesses have been reported. s of 2007[ [update]], the best attack which applied to all keys could break IDEA reduced to 6 rounds (the full IDEA cipher uses 8.5 rounds). Note that a "break" is any attack which requires less than 2128 operations; the 6-round attack requires 264 known plaintexts and 2126.8 operations.
Bruce Schneier thought highly of IDEA in 1996, writing, "In my opinion, it is the best and most secure block algorithm available to the public at this time." ("Applied Cryptography", 2nd ed.) However, by 1999 he was no longer recommending IDEA due to the availability of faster algorithms, some progress in its cryptanalysis, and the issue of patents.
In 2011 full 8.5-found IDEA was broken using a meet-in-the-middle attack. Independently in 2012, full 8.5 round IDEA was broken using a narrow-bicliques attack, with a reduction of cryptographic strength of about two bits, similar to the effect of the previous bicliques attack on AES.
Weak keys.
The very simple key schedule makes IDEA subject to a class of weak keys; some keys containing a large number of 0 bits produce weak encryption. These are of little concern in practice, being sufficiently rare that they are unnecessary to avoid explicitly when generating keys randomly. A simple fix was proposed: exclusive-ORing each subkey with a 16-bit constant, such as codice_1.
Larger classes of weak keys were found in 2002.
This is still of negligible probability to be a concern to a randomly chosen key, and some of the problems are fixed by the constant XOR proposed earlier, but the paper is not certain if all of them are. A more comprehensive redesign of the IDEA key schedule may be desirable.
Availability.
A patent application for IDEA was first filed in Switzerland (CH A 1690/90) on May 18, 1990, then an international patent application was filed under the Patent Cooperation Treaty on May 16, 1991. Patents were eventually granted in Austria, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland, the United Kingdom, (European Patent Register entry for , filed May 16, 1991, issued June 22, 1994 and expired May 16, 2011), the United States (U.S. Patent , issued May 25, 1993 and expired January 7, 2012) and Japan (JP 3225440) (expired May 16, 2011).
MediaCrypt AG is now offering a successor to IDEA and focuses on its new cipher (official release on May 2005) IDEA NXT, which was previously called FOX.

</doc>
<doc id="15077" url="http://en.wikipedia.org/wiki?curid=15077" title="Indoor rower">
Indoor rower

An indoor rower, or rowing machine, is a machine used to simulate the action of watercraft rowing for the purpose of exercise or training for rowing. Indoor rowing has become established as a sport in its own right. The term also refers to a participant in this sport.
Modern indoor rowers are often known as ergometers (colloquially erg or ergo), an ergometer being a device which measures the amount of work performed. The indoor rower is calibrated to measure the amount of energy the rower is using through their use of the equipment.
History.
Chabrias, an Athenian admiral of the 4th century BCE, introduced the first rowing machines as supplemental military training devices. "To train inexperienced oarsmen, Chabrias built wooden rowing frames on shore where beginners could learn technique and timing before they went on board ship." 
Early rowing machines are known to have existed from the mid-1800s, a US patent being issued to WB Curtis in 1872 for a particular hydraulic based damper design. Machines using linear pneumatic resistance were common around 1900—one of the most popular was the Narragansett hydraulic rower, manufactured in Rhode Island from around 1900–1960. However they did not simulate actual rowing very accurately nor measure power output. 
In the 1950s and 1960s, coaches in many countries began using specially made rowing machines for training and improved power measurement. One original design incorporated a large, heavy, solid iron flywheel with a mechanical friction brake, developed by John Harrison of Leichhardt Rowing Club in Sydney, later to become a professor of mechanical engineering at the University of New South Wales. Harrison, a dual Australian Champion Beach Sprinter who went on to row in the coxless four at the 1956 Melbourne Olympics, had been introduced to rowing after a chance meeting with one of the fathers of modern athletic physiological training and testing, and the coach of the Leichhardt "Guinea Pigs", Professor Frank Cotton. Professor Cotton had produced a rudimentary friction-based machine for evaluating potential rowers by exhausting them, without any pretence of accurately measuring power output. Harrison realised the importance of using a small braking area with a non-absorbent braking material, combined with a large flywheel. The advantage of this design (produced by Ted Curtain Engineering, Curtain being a fellow Guinea Pig) was the virtual elimination of factors able to interfere with accurate results—for instance ambient humidity or temperature. The Harrison-Cotton machine represents the very first piece of equipment able to accurately quantify human power output; power calculation within an accuracy range as achieved by his machine of less than 1% remains an impressive result today. The friction brake was adjusted according to a rower's weight to give an accurate appraisal of boat-moving ability (drag on a boat is proportional to weight). Inferior copies of Harrison's machine were produced in several countries utilising a smaller flywheel and leather straps—unfortunately the leather straps were sensitive to humidity, and the relatively large braking area made results far less accurate than Harrison's machine. The weight correction factor tended to make them unpopular among rowers of the time. Harrison, arguably the father of modern athletic power evaluation, died in February 2012.
In the 1970s, the Gjessing-Nilson ergometer from Norway used a friction brake mechanism with industrial strapping applied over the broad rim of the flywheel. Weights hanging from the strap ensured that an adjustable and predictable friction could be calculated. The cord from the handle mechanism ran over a helical pulley with varying radius, thereby adjusting the gearing and speed of the handle in a similar way to the changing mechanical gearing of the oar through the stroke, derived from changes in oar angle and other factors. This machine was for many years the internationally accepted standard for measurement.
The first air resistance ergometers were introduced around 1980 by Repco. 
The Concept2 ergometer was introduced in 1980 by the Dreissigacker brothers. The first, the Model A, was a fixed-frame sliding-seat design using a bicycle wheel with fins attached for air resistance. The Model B, introduced in 1986, introduced a solid cast flywheel (now enclosed by a cage) and the first digital performance monitor, which proved revolutionary. This machine's capability of accurate calibration combined with easy transportability spawned the sport of competitive indoor rowing, and revolutionised training and selection procedures for watercraft rowing. The later Models C (1993) and D (2003) became some of the best-selling fitness equipment pieces of all time.
Design summary.
All rowing-machine designs consist of an energy damper or braking mechanism connected to a chain and/or handle. A foot stretcher (where rowers places their feet) is attached to the same mounting as the energy damper. Most include a rail which either the seat or the mechanism slide upon. Different machines have a variety of layouts and damping mechanisms, each of which have certain advantages and disadvantages.
Machines with a digital display calculate the user's power by measuring the speed of the flywheel during the stroke and then recording the rate at which it decelerates during the recovery. Using this and the known moment of inertia of the flywheel, the computer is able to calculate speed, power, distance and energy usage. Some ergometers can be connected to a personal computer using software, and data on individual exercise sessions can be collected and analysed. In addition, some software packages allows users to connect multiple ergometers either directly or over the internet for virtual races and workouts.
Motion type.
There are three possible designs to allow the foot stretcher (with flywheel) and handle to move relatively nearer and apart from each other.
The first option is the most common, with the foot stretcher and flywheel both fixed, with only the seat sliding on a rail. This is generally analogous to the seat sliding on rails in the boat. Commonly called a 'Fixed head' ergometer.
The second option is where both the seat and the foot stretcher slide on a rail. This is analogous to both the seat sliding on the boat, and the boat sliding relative to the rower, on the water. The relative movement of seat and flywheel are similar to the result of the rower moving at steadier average speed while the boat's speed varies much more relative to the rower. Commonly called a 'Floating head' ergometer.
The third option has the seat fixed. Only the foot stretcher slides backward and away from the rower.
In addition, some indoor rowers include a pivoting handle or handles (as opposed to a simple chain) in order to more completely simulate the action of rowing or sculling. Such machines are known as "rowing simulators".
Damper type.
Piston resistance comes from hydraulic cylinders that are attached to the handles of the rowing machine. The length of the rower handles on this class of rower is typically adjustable, however, during the row the handle length is fixed which in turn fixes the trajectory that the hands must take on the stroke and return, thus making the stroke less accurate than is possible on the other types of resistance models where it is possible to emulate the difference in hand height on the stroke and return. Furthermore, many models in this class have a fixed seat position that eliminates the leg drive which is the foundation of competitive on water rowing technique. Because of the compact size of the pistons and mechanical simplicity of design, these models are typically not as large or as expensive as the others types.
Braked flywheel resistance models comprise magnetic, air and water resistance rowers. These machines are mechanically similar since all three types use a handle connected to a flywheel by rope, chain, or strap to provide resistance to the user – the types differ only in braking mechanism. Because the handle is attached to the resistance source by rope or similarly flexible media, the trajectory of the hands in the vertical plane is free making it possible for the rower to emulate the hand height difference between the stroke and the return. Most of these models have the characteristic sliding seat typical of competitive on-the-water boats.
Rowing machines with monitors calculate performance using an algorithm unique to the individual manufacturer; it will be affected by the type of resistance used and other factors.
Exercise.
Indoor rowing primarily works the cardiovascular systems with typical workouts consisting of steady pieces of 20–40 minutes, although the standard trial distance for record attempts is 2000 m, which can take from five and a half minutes (best elite male rowers) to nine minutes or more. Like other forms of cardio focused exercise, interval training is also commonly used in indoor rowing. While cardio-focused, rowing also stresses many muscle groups throughout the body anaerobically, thus rowing is often referred to as a strength-endurance sport.
Unlike high impact exercises, which can damage knees and the connective tissues of the lower body, rowing's most common injury site is the lower back. Proper technique is a necessity for staying injury free, with a focus on both mechanics and breathing, as correct rhythm, exhaling on the drive and inhaling on the recovery, is a stabilizing force for the upper body. Non-rowers commonly overemphasize the muscles of the upper body, while correct technique uses the large muscle of the thighs to drive much of the stroke. Also, good technique requires that the angle of the upper body is never too far forward, nor too far back, both of which jeopardize the lower back and compression injuries on the knees and hip flexor muscles.
In addition to the high levels of fitness attained, rowing is an intense calorie-burning exercise. Although rowers with less ability and training will burn fewer calories, the ergometer is an excellent tool for use in a weight-loss program.
The standard measurement of speed on an ergometer is generally known as the "split", or the amount of time in minutes and seconds required to travel 500 m at the current pace — a split of 2:00 represents a speed of two minutes per 500 metres, or about 4.17 m/s. The split does not necessarily correspond to how many strokes the rower takes (the "rating") since strokes can vary in power.
Ergometer testing.
Ergometer tests are used by rowing coaches to evaluate rowers and is part of athlete selection for many senior and junior national rowing teams. During a test, rowers will row a set distance and try to clock the fastest time possible, or a set time and try to row the longest distance possible. The most common distances for erg tests are 2000, 5000, 6000 or 10000 metres. The most common times for erg tests are 3 min, 5 min, 20 min, 30 min, and 1 hour. Results of these tests are an objective measure of an athlete's fitness; however, weight, technique and team coordination also impact performance in a boat, thus assembling a crew based purely on erg scores is not an optimal strategy. In fact it is not unheard of for teams that are considerably faster on the ergometer to be beaten on the water.
Rowing technique.
Rowing technique on the erg broadly follows the same pattern as that of a normal rowing stroke on water, but with minor modifications: it is not necessary to "tap down" at the finish, since there are no blades to extract from water; but many who also row on water do this anyway. Sometimes an exaggerated finish, pulling the hands further up the chest than would be possible on water, is used.
Rowing on an ergometer requires four basics phases to complete one stroke; the catch, the drive, the finish and the recovery. The catch is the initial part of the stroke. The drive is where the power from the rower is generated while the finish is the final part of the stroke. Then, the recovery is the initial phase to begin taking a new stroke. The phases repeat until a time duration or a distance is completed.
Catch.
Knees are bent with the shins in a vertical position. The back should be roughly parallel to the thigh without hyperflexion (leaning forward too far). The arms and shoulders should be extended forward and relaxed. The arms should be level.
Drive.
The drive is initiated by the extension of the legs; the body remains in the catch posture at this point of the drive. As the legs continue to full extension, the rower engages the core to begin the motion of the body levering backward, adding to the work of the legs. When the legs are flat, the rower begins to pull the handle toward the chest with their arms while keeping their arms straight and parallel to the floor.
Finish (or release).
The legs are at full extension and flat. The shoulders are slightly behind the pelvis, and the arms are in full contraction with the elbows bent and hands against the chest below the nipples. The back of the rower is still maintained in an upright posture and wrists should be flat.
Recovery.
The recovery is a slow slide back to the initial part of the stroke, it gives the rower time to recover from the previous stroke. During the recovery the actions are in reverse order of the drive. The arms are fully extended so that they are straight. The torso is engaged to move forward back over the pelvis. Weight transfers from the back of the seat to the front of the seat at this time. When the hands come over the knees, the legs contract back towards the foot stretcher. Slowly the back becomes more parallel to the thighs until the recovery becomes the catch.
Competitions.
A large number of indoor rowing competitions are held all over the world, including the indoor rowing world championships (also known as CRASH-B Sprints) held in Boston, Massachusetts, United States in February and the British Indoor Rowing Championships held in Birmingham, England in November; both are rowed on Concept2s. The core event for most competitions is the individual 2000-m; less common are the mile (e.g., Evesham), the 2500-m (e.g., Basingstoke—also the original distance of the CRASH-B Sprints). Many competitions also include a sprint event (100m-500m) and sometimes team relay events. The machines used are consistent although the resistance may be adjusted. The resistance adjustment does not affect the energy measurement so a result on one machine can be fairly compared with results on other machines regardless of resistance level.
Most competitions are organized into categories based on sex, age, and weight class. While the fastest times are generally achieved by rowers between 20 and 40 years old, teenagers and rowers over 90 are common at competitions. There is a nexus between performance on-water and performance on the ergometer, with open events at the World Championships often being dominated by elite on-water rowers. Former men's Olympic single scull champions Pertti Karppinen and Rob Waddell and five-time Gold Medalist Sir Steven Redgrave have all won world championships or set world records in indoor rowing.
In addition to live venue competitions, many erg racers compete by internet, either offline by posting scores to challenges, or live online races facilitated by computer connection. Online Challenges sponsored by Concept2 include the annual ultra-rowing challenge, the Virtual Team Challenge.

</doc>
<doc id="15078" url="http://en.wikipedia.org/wiki?curid=15078" title="Internetwork Packet Exchange">
Internetwork Packet Exchange

Internetwork Packet Exchange (IPX) is the network layer protocol in the IPX/SPX protocol suite. IPX is derived from Xerox Network Systems' IDP. It may act as a transport layer protocol as well.
The IPX/SPX protocol suite was very popular through the late 1980s into the mid-1990s because it was used by the Novell NetWare network operating system. Because of Novell Netware popularity the IPX became a prominent internetworking protocol.
A big advantage of IPX was a small memory footprint of the IPX driver, which was vital for MS-DOS and Microsoft Windows up to the version Windows 95 because of limited size of the conventional memory. Another IPX advantage is an easy configuration of the client computers. However, IPX does not scale well for large networks such as the Internet, and as such, IPX usage decreased as the boom of the Internet made TCP/IP nearly universal. Computers and networks can run multiple network protocols, so almost all IPX sites will be running TCP/IP as well to allow for Internet connectivity. It has also been possible to run Novell products without IPX for some time, as they have supported both IPX and TCP/IP since NetWare reached version 5 in late 1998.
Description.
A big advantage of IPX protocol is its little or no need for configuration. In the time when protocols for dynamic host configuration did not exist and the BOOTP protocol for centralized assigning addresses was not common, the IPX network could be configured almost automatically. A client computer uses the MAC address of its network card as the node address, and learns the network number from the server or router. Network number is derived from MAC address of the server.
A small IPX network administrator had to care only
IPX packet structure.
Each IPX packet begins with a header with the following structure:
The Packet Type values:
IPX addressing.
An IPX address has the following structure:
Network number.
The network number allows to address (and communicate with) the IPX nodes which do not belong to the same network or "cabling system". The cabling system is a network in which a data link layer protocol can be used for communication. To allow communication between different networks, they must be connected with IPX routers. A set of interconnected networks is called an internetwork. Any Novell Netware server may serve as an IPX router. Novell also supplied stand-alone routers. Multiprotocol routers of other vendors often support IPX routing. Using different frame formats in one cabling system is possible, but it works similarly as if separate cabling systems were used (i.e. different network numbers must be used for different frame formats even in the same cabling system and a router must be to allow communication between nodes using different frame formats in the same cabling system).
Node number.
The node number is used to address an individual computer (or more exactly, a network interface) in the network. Client stations use its network interface card MAC address as the node number.
The value FF:FF:FF:FF:FF:FF may be used as a node number in a destination address to broadcast a packet to "all nodes in the current network".
Socket number.
The socket number serves to select a process or application in the destination node.
The presence of a socket number in the IPX address allows the IPX to act as a transport layer protocol, comparable with the User Datagram Protocol (UDP) in the Internet protocol suite.
Comparison with IP.
The IPX network number is conceptually identical to the network part of the IP address (the parts with netmask bits set to 1); the node number has the same meaning as the bits of IP address with netmask bits set to 0. The difference is that the boundary between network and node part of address in IP is variable, while in IPX it is fixed. As the node address is usually identical to the MAC address of the network adapter, the Address Resolution Protocol is not needed in IPX.
For routing, the entries in the IPX routing table are similar to IP routing tables; routing is done by network address, and for each network address a network:node of the next router is specified in a similar fashion an IP address/netmask is specified in IP routing tables.
There are 3 routing protocols available for IPX networks: In early IPX networks a version of Routing Information Protocol (RIP) was the only available protocol to exchange routing information. Unlike RIP for IP, it uses delay time as the main metric, retaining the hop count as a secondary metric. Since NetWare 3, the NetWare Link Services Protocol (NSLP) based on IS-IS is available which is more suitable for larger networks. Cisco routers implement an IPX version of EIGRP protocol as well.
Frame Formats.
IPX can be transmitted over Ethernet using one of the following 4 frame formats or encapsulation types:
In non-Ethernet networks only 802.2 and SNAP frame types are available.

</doc>
<doc id="15079" url="http://en.wikipedia.org/wiki?curid=15079" title="International human rights instruments">
International human rights instruments

International human rights instruments are treaties and other international documents relevant to international human rights law and the protection of human rights in general. They can be classified into two categories: "declarations", adopted by bodies such as the United Nations General Assembly, which are not legally binding although they may be politically so as soft law; and "conventions", which are legally binding instruments concluded under international law. International treaties and even declarations can, over time, obtain the status of customary international law.
International human rights instruments can be divided further into "global instruments", to which any state in the world can be a party, and "regional instruments", which are restricted to states in a particular region of the world.
Most conventions establish mechanisms to oversee their implementation. In some cases these mechanisms have relatively little power, and are often ignored by member states; in other cases these mechanisms have great political and legal authority, and their decisions are almost always implemented. Examples of the first case include the UN treaty committees, while the best exemplar of the second case is the European Court of Human Rights.
Mechanisms also vary as to the degree of individual access to them. Under some conventions – e.g. the European Convention on Human Rights – individuals or states are permitted, subject to certain conditions, to take individual cases to the enforcement mechanisms; under most, however (e.g. the UN conventions), individual access is contingent on the acceptance of that right by the relevant state party, either by a declaration at the time of ratification or accession, or through ratification of or accession to an optional protocol to the convention. This is part of the evolution of international law over the last several decades. It has moved from a body of laws governing states to recognizing the importance of individuals and their rights within the international legal framework.
The Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights, and the International Covenant on Economic, Social and Cultural Rights are sometimes referred to as the "international bill of rights".

</doc>
<doc id="15080" url="http://en.wikipedia.org/wiki?curid=15080" title="Indian removal">
Indian removal

Indian removal was a 19th-century United States policy of Indian relocation to Indian Territory (what is now Oklahoma). The policy traced its direct origins to the administration of James Monroe, though it addressed conflicts between whites and Indians that had been occurring since the 17th century, and were getting worse by the early 19th century as white settlers were increasingly pushing west. The Indian Removal Act was the key act that enforced Indian removal, and was signed into law by President Andrew Jackson on May 26, 1830.
The Revolutionary background.
Some of the American Revolutionary thinkers and leaders viewed the American Indians not as a single people, but as nations in their own right, and developed early policies for the new United States to interact with Indian tribes.
Benjamin Franklin.
Benjamin Franklin, in his "Proposed Articles of Confederation" (presented to the Continental Congress on May 10, 1775) for the nation about to take birth, called for a "perpetual Alliance" with the Indians, especially with the Six Nations of the Iroquois Confederacy:
 Article XI. A perpetual Alliance offensive and defensive, is to be entered into as soon as may be with the Six Nations; their Limits to be ascertained and secured to them; their Land not to be encroached on, nor any private or Colony Purchases made of them hereafter to be held good; nor any Contract for Lands to be made but between the Great Council of the Indians at Onondaga and the General Congress. The Boundaries and Lands of all the other Indians shall also be ascertained and secured to them in the same manner; and Persons appointed to reside among them in proper Districts, who shall take care to prevent Injustice in the Trade with them, and be enabled at our general Expense by occasional small Supplies, to relieve their personal Wants and Distresses. And all Purchases from them shall be by the Congress for the General Advantage and Benefit of the United Colonies.
Thomas Jefferson.
Thomas Jefferson, in his Notes on the State of Virginia (1785), defended American Indian culture and marveled at how the tribes of Virginia "never submitted themselves to any laws, any coercive power, any shadow of government" due to their "moral sense of right and wrong". 
He would later write, "I believe the Indian then to be in body and mind equal to the whiteman". His desire was for the Native Americans to intermix with European Americans and to become one people. (p)
To achieve that end, President Jefferson would - in addition to offering U.S. citizenship to some of the Indian nations - propose lending credit to them for trade with the expectation they would be unable to honor their debts and thereby the United States would acquire their land.
George Washington.
President George Washington, while addressing the Seneca nation in 1790, publicly pledged to uphold their “just rights” and described the pre-Constitutional defrauding of the Indians out of their land as “evil”.
In March and April of 1792, Washington met with 50 tribal chiefs in Philadelphia – including the Iroquois – to discuss closer friendship between them and the United States.
Later that same year, Washington stressed the need for building peace, trust, and commerce with America's Indian neighbors:
I cannot dismiss the subject of Indian affairs without again recommending to your consideration the expediency of more adequate provision for giving energy to the laws throughout our interior frontier, and for restraining the commission of outrages upon the Indians; without which all pacific plans must prove nugatory. To enable, by competent rewards, the employment of qualified and trusty persons to reside among them, as agents, would also contribute to the preservation of peace and good neighbourhood. If, in addition to these expedients, an eligible plan could be devised for promoting civilization among the friendly tribes, and for carrying on trade with them, upon a scale equal to their wants, and under regulations calculated to protect them from imposition and extortion, its influence in cementing their interests with our’s [sic] could not but be considerable.
In 1795, in his Seventh Message to Congress, Washington expressed that if the US government wanted peace with the Indians, then it must give peace to them, and that if the US wanted raids by Indians to stop, then raids by American "frontier inhabitants" must also stop.
Early Congressional Acts.
The new Congress passed the Northwest Ordinance of 1787, which would, for years to come, serve broadly as a precedent for the manner in which the United States' territorial expansion would occur, called for the protection of Indians' "property, rights, and liberty":
 Article 3. Religion, morality, and knowledge, being necessary to good government and the happiness of mankind, schools and the means of education shall forever be encouraged. The utmost good faith shall always be observed towards the Indians; their lands and property shall never be taken from them without their consent; and, in their property, rights, and liberty, they shall never be invaded or disturbed, unless in just and lawful wars authorized by Congress; but laws founded in justice and humanity, shall from time to time be made for preventing wrongs being done to them, and for preserving peace and friendship with them.
The U.S. Constitution of 1787 (Article I, Section 8) calls for regulating commerce with the Indian tribes, and makes their importance to Congress equal to that of the states and foreign governments.
In 1790, Congress passed the Indian Nonintercourse Act (renewed and amended in 1793, 1796, 1799, 1802, and 1834) to protect and codify the Indians’ land rights.
Jeffersonian policy.
As president, Thomas Jefferson developed far reaching Indian policy that had two primary goals. The security of the fledgling United States was paramount so Jefferson wanted to assure the Native nations were tightly bound to the United States and not other foreign nations. Secondly he wanted "to civilize"(p) them into an agricultural or more urbanized lifestyles. These goals would be achieved through trade and treaties.
He encouraged American policy to allow Native Americans to remain east of the Mississippi as long as they became assimilated or "civilized". As President, Jefferson made sustained efforts to win the friendship and cooperation of many Native American nations. He repeatedly articulated his aspirations for a united nation of both Whites and Indians, such as the following from a letter to the Seneca spiritual leader Handsome Lake dated November 3, 1802:
Go on then, brother, in the great reformation you have undertaken... In all your enterprises for the good of your people, you may count with confidence on the aid and protection of the United States, and on the sincerity and zeal with which I am myself animated in the furthering of this humane work. You are our brethren of the same land; we wish your prosperity as brethren should do. Farewell."
Jefferson's personal nonsectarian religiosity appears to show in his references to the Great Spirit, as in the following letter to the Choctaw nation dated December 17, 1803:
I am glad, brothers, you are willing to go and visit some other parts of our country... we thank the Great Spirit who took care of you on the ocean, and brought you safe and in good health to the seat of our great Council; and we hope His care will accompany and protect you, on your journey and return home; and that He will preserve and prosper your nation in all its just pursuits."
President Jefferson also sought full U.S. citizenship for those Indian nations which desired it, including the Cherokee. In his Eighth Annual Message to Congress on November 8, 1808, he presented to the nation a vision of White and Indian unity:
With our Indian neighbors the public peace has been steadily maintained... And, generally, from a conviction that we consider them as part of ourselves, and cherish with sincerity their rights and interests, the attachment of the Indian tribes is gaining strength daily... and will amply requite us for the justice and friendship practiced towards them... [O]ne of the two great divisions of the Cherokee nation have now under consideration to solicit the citizenship of the United States, and to be identified with us in laws and government, in such progressive manner as we shall think best."
Years after the Jefferson presidency, in 1817 the U.S. government would again offer citizenship to the Cherokee who lived east of the Mississippi River, along with 640 acres per family.
As other writings illustrate, his general compassion for the Indians at times gave way to impatience with nations which responded unfavorably to his communications with them, and to his frustration with the limited success of his efforts. His intention was to change their lifestyle from hunter-gatherer to farming, largely through "the decrease of game rendering their subsistence by hunting insufficient". Jefferson expected that the switch to agriculture would make them dependent on White Americans for trade goods and therefore more likely to give up their land in exchange. In an 1803 letter to William Henry Harrison, Jefferson wrote:
When they withdraw themselves to the culture of a small piece of land, they will perceive how useless to them are their extensive forests, and will be willing to pare them off from time to time in exchange for necessaries for their farms and families. To promote this disposition to exchange lands, which they have to spare and we want, for necessaries, which we have to spare and they want, we shall push our trading uses, and be glad to see the good and influential individuals among them run in debt, because we observe that when these debts get beyond what the individuals can pay, they become willing to lop them off by a cession of lands. At our trading houses, too, we mean to sell so low as merely to repay us cost and charges, so as neither to lessen or enlarge our capital. This is what private traders cannot do, for they must gain; they will consequently retire from the competition, and we shall thus get clear of this pest without giving offence or umbrage to the Indians. In this way our settlements will gradually circumscribe and approach the Indians, and they will in time either incorporate with us as citizens of the United States, or remove beyond the Mississippi. The former is certainly the termination of their history most happy for themselves; but, in the whole course of this, it is essential to cultivate their love. As to their fear, we presume that our strength and their weakness is now so visible that they must see we have only to shut our hand to crush them, and that all our liberalities to them proceed from motives of pure humanity only. Should any tribe be foolhardy enough to take up the hatchet at any time, the seizing the whole country of that tribe, and driving them across the Mississippi, as the only condition of peace, would be an example to others, and a furtherance of our final consolidation. 
However, elsewhere in the same letter, Jefferson spoke of protecting the Indians from injustices perpetrated by Whites:
Our system is to live in perpetual peace with the Indians, to cultivate an affectionate attachment from them, by everything just and liberal which we can do for them within... reason, and by giving them effectual protection against wrongs from our own people.
Native American land was sometimes purchased, either via a treaty or under duress. The idea of land exchange, that is, Native Americans would give up their land east of the Mississippi in exchange for a similar amount of territory west of the river, was first proposed by Jefferson in 1803 and was first incorporated into treaties in 1817. The Indian Removal Act of 1830 incorporated this concept.
Calhoun's plan.
Under President James Monroe, Secretary of War John C. Calhoun devised the first plans for Indian removal. By late 1824, Monroe approved Calhoun's plans and in a special message to the Senate on January 27, 1825, requested the creation of the Arkansas Territory and Indian Territory. The Indians east of the Mississippi were to voluntarily exchange their lands for lands west of the river. The Senate accepted Monroe's request and asked Calhoun to draft a bill, which was killed in the House of Representatives by the Georgia delegation. President John Quincy Adams assumed the Calhoun–Monroe policy and was determined to remove the Indians by non-forceful means, but Georgia refused to submit to Adams' request and forced Adams to make a treaty with Creeks and Cherokees granting Georgia what it wanted. When Andrew Jackson became the president from the newly organized Democratic Party, he agreed that the Indians should be forced to exchange eastern lands for western lands.
Indian Removal Act.
Andrew Jackson became president of the United States in 1829, and with his inauguration the stance of the government towards the Indians, along with that of the Democratic Party, turned harsher. Jackson abandoned the policy of his predecessors of treating different Indian groups as separate nations. Instead, he aggressively pursued plans to move all Indian tribes living east of the Mississippi River to west of the Mississippi. At Jackson's request, the United States Congress opened a fierce debate on an Indian Removal Bill. The Senate passed the measure 28–19, the House 102–97. Jackson signed the legislation into law June 30, 1830.
In 1830, the majority of the "Five Civilized Tribes"—the Chickasaw, Choctaw, Creek, Seminole, and Cherokee—were living east of the Mississippi as they had for thousands of years. The Indian Removal Act of 1830 implemented the U.S. government policy towards the Indian populations, which called for moving Native American tribes living east of the Mississippi River to lands west of the river. While it did not authorize the forced removal of the indigenous tribes, it authorized the President to negotiate land exchange treaties with tribes located in lands of the United States.
Choctaw.
On September 27, 1830, the Choctaw signed the Treaty of Dancing Rabbit Creek and by concession, became the first Native American tribe to be removed. The agreement represented one of the largest transfers of land that was signed between the U.S. Government and Native Americans without being instigated by warfare. By the treaty, the Choctaw signed away their remaining traditional homelands, opening them up for European-American settlement in Mississippi Territory. When the Choctaw reached Little Rock, a Choctaw chief referred to the trek as a "trail of tears and death".
Alexis de Tocqueville, the French philosopher, witnessed the Choctaw removals while in Memphis, Tennessee, in 1831,
In the whole scene there was an air of ruin and destruction, something which betrayed a final and irrevocable adieu; one couldn't watch without feeling one's heart wrung. The Indians were tranquil, but sombre and taciturn. There was one who could speak English and of whom I asked why the Chactas were leaving their country. "To be free," he answered, could never get any other reason out of him. We ... watch the expulsion ... of one of the most celebrated and ancient American peoples.—Alexis de Tocqueville, "Democracy in America"
Cherokee.
While the Indian Removal Act made the move of the tribes voluntary, it was often abused by government officials. The best-known example is the Treaty of New Echota. It was negotiated and signed by a small faction of Cherokee tribal members, not the tribal leadership, on December 29, 1835. It resulted in the forced relocation of the tribe in 1838. An estimated 4,000 Cherokee died in the march, now known as the Trail of Tears. Missionary organizer Jeremiah Evarts urged the Cherokee Nation to take their case to the U.S. Supreme Court. The Marshall court ruled that while Native American tribes were sovereign nations ("Cherokee Nation v. Georgia", 1831), state laws had no force on tribal lands ("Worcester v. Georgia", 1832).
In spite of this acculturation, many white settlers and land speculators simply desired the land. Some claimed their presence was a threat to peace and security. Some U.S. states, like Georgia in 1830, passed a law which prohibited whites from living on Native American territory after March 31, 1831, without a license from the state. This law was written to justify removing white missionaries who were helping the Native Americans resist removal.
Seminole.
In 1835, the Seminole refused to leave their lands in Florida, leading to the Second Seminole War. Osceola led the Seminole in their fight against removal. Based in the Everglades of Florida, Osceola and his band used surprise attacks to defeat the U.S. Army in many battles. In 1837, Osceola was seized by deceit upon the orders of U.S. General Thomas Jesup when Osceola came under a flag of truce to negotiate peace. Osceola would die in prison of illness. The war would end up costing the U.S. over 1,500 deaths and cost the government $20 million. Some Seminole traveled deeper into the Everglades, while others moved west. Removal continued out west and numerous wars ensued over land.
Muscogee (Creek).
In the aftermath of the Treaty of Fort Jackson and the Treaty of Washington, the Muscogee were confined to a small strip of land in present-day east central Alabama. Following the Indian Removal Act, in 1832 the Creek National Council signed the Treaty of Cusseta, ceding their remaining lands east of the Mississippi to the U.S., and accepting relocation to the Indian Territory. Most Muscogee were removed to Indian Territory during the Trail of Tears in 1834, although some remained behind.
Friends and Brothers – By permission of the Great Spirit above, and the voice of the people, I have been made President of the United States, and now speak to you as your Father and friend, and request you to listen. Your warriors have known me long. You know I love my white and red children, and always speak with a straight, and not with a forked tongue; that I have always told you the truth ... Where you now are, you and my white children are too near to each other to live in harmony and peace. Your game is destroyed, and many of your people will not work and till the earth. Beyond the great River Mississippi, where apart of your nation has gone, your Father has provided a country large enough for all of you, and he advises you to remove to it. There your white brothers will not trouble you; they will have no claim to the land, and you can live upon it you and all your children, as long as the grass grows or the water runs, in peace and plenty. It will be yours forever. For the improvements in the country where you now live, and for all the stock which you cannot take with you, your Father will pay you a fair price ...—President Andrew Jackson addressing the Creek, 1829
Chickasaw.
Unlike other tribes who exchanged land grants, the Chickasaw were to receive mostly financial compensation of $3 million from the United States for their lands east of the Mississippi River.
In 1836, the Chickasaw had reached an agreement that purchased land from the previously removed Choctaw after a bitter five-year debate. They paid the Choctaw $530,000 for the westernmost part of Choctaw land. The first group of Chickasaw moved in 1837. The $3,000,000 that the U.S. owed the Chickasaw went unpaid for nearly 30 years.
Aftermath.
As a result, the five tribes were resettled in the new Indian Territory in modern-day Oklahoma and parts of Kansas. Some indigenous nations resisted forced migration more forcefully. Those few who stayed behind eventually formed tribal groups including the Eastern Band Cherokee, based in North Carolina, the Mississippi Band of Choctaw Indians, the Seminole Tribe of Florida, and the Creeks in Atmore, Alabama.
Southern Removals.
"Many figures have been rounded."
The North.
Tribes in the Old Northwest were far smaller and more fragmented than the Five Civilized Tribes, so the treaty and emigration process was more piecemeal. Bands of Shawnee, Ottawa, Potawatomi, Sauk, and Meskwaki (Fox) signed treaties and relocated to the Indian Territory. In 1832, a Sauk leader named Black Hawk led a band of Sauk and Fox back to their lands in Illinois. In the Black Hawk War, the U.S. Army and Illinois militia defeated Black Hawk and his warriors, resulting in the Sauk and Fox being relocated into what would become modern-day Iowa.
The Iroquois were also supposed to be part of Indian removal, and the Treaty of Buffalo Creek arranged for them to be removed to land in Wisconsin and Kansas. However, the land company that was to purchase the land for the territories reneged on their deal, and subsequent treaties in 1842 and 1857 gave back most of the Iroquois' reservations untouched. Only the Buffalo Creek Reservation was ever dissolved as part of the removal program; a small portion was purchased back over a century later to build a casino.

</doc>
<doc id="15081" url="http://en.wikipedia.org/wiki?curid=15081" title="Green Party (Ireland)">
Green Party (Ireland)

The Green Party (Irish: "Comhaontas Glas") is a green party in Ireland that operates in both the Republic of Ireland and Northern Ireland. It was founded as the Ecology Party of Ireland in 1981 by Dublin teacher Christopher Fettes. The party became the Green Alliance in 1983 and in 1987 was renamed to its current title in English. Its leader is Eamon Ryan, its deputy leader is Catherine Martin and its chairman is Roderic O'Gorman. 
Green Party candidates have been elected to all levels of representation; local, Dáil and European Parliament, and in 2007 the party gained its first representation in the Northern Ireland Assembly, the Green Party in Northern Ireland having become a region of the Republic's party the previous year.
The Greens became part of the Irish government for the first time following the Irish general election, 2007, having agreed upon a programme for government in coalition with Fianna Fáil and the Progressive Democrats. In the wake of the Irish financial crisis, the party lost a significant amount of its support and came under pressure to withdraw its support for the administration. In January 2011 the party withdrew from government, after passing legislation for European Union and International Monetary Fund financial support for the Republic's bank bailout, and a dispute with Fianna Fáil over the appointment of cabinet ministers. In the February 2011 election, the party suffered a wipeout, losing all six of its TDs. Following the 2011 Seanad Éireann election, the party no longer has any representatives in the Oireachtas. 
It has one representative in the Northern Ireland Assembly.
History.
The party's first electoral outing was when 7 candidates contested the November 1982 general election under the "Ecology Party" banner, winning 0.2% of the vote. Following a name-change, they contested the 1984 European Parliament elections, with their party founder winning 1.9% in the Dublin constituency. The following year they won their first election when Marcus Counihan was elected to Killarney Urban District Council during the 1985 Local Elections. The party nationally ran 34 candidates and won 0.6% of the vote. The party continued to struggle until the general election of 1989 when the again renamed party won its first seat in parliament, the Dáil, when Roger Garland was elected in Dublin South. In the 1994 European Parliament election Patricia McKenna topped the poll for the Dublin Constituency and Nuala Ahern won a seat in Leinster. They retained their seats in 1999 although the party lost 5 councillors in local elections held that year despite an increase in their vote. In the general election of 1997 the party gained a seat when John Gormley won a Dáil seat in Dublin South–East.
At the general election of 2002 that it made a breakthrough, getting 6 Teachtaí Dála (TDs) elected to the Dáil with 4% of the national vote. However, in the election to the European Parliament of June 2004, the party lost both of the European Parliament seats. In the 2004 local elections at county level it increased its number of councillors from 8 to 18 out of 883 and at town council level its number of councillors increased from 5 to 14 out of 744. While in government, the vast majority of its seats were lost at the 2009 council elections, including its entire traditional Dublin base, where - with the exception of a Town Council Seat in Balbriggan - it held no council seats in Dublin and only three County Council seats in total. In the 2014 local elections, the party gained nine seats for a total of twelve. The party was successful in the four Dublin area councils.
It has about fifteen hundred members.
Organisation.
The National Executive Committee is the organising committee of the party. It comprises the party leader Eamon Ryan, deputy leader Catherine Martin, Chair Roderic O'Gorman, Young Greens representative, Treasurer and ten members elected annually at the party convention.
Leadership.
The party did not have a national leader until 2001. At a special "Leadership Convention" in Kilkenny on 6 October 2001, Trevor Sargent was elected the first official leader of the Green Party. He was re-elected to this position in 2003 and again in 2005. The party's constitution requires that a leadership election be held within six months of a general election.
Sargent resigned the leadership in the wake of the general election to the 30th Dáil. During the campaign, Sargent had promised that he would not lead the party into Government with Fianna Fáil. In the election outcome the party retained 6 Dáil seats, making them the most likely partner for Fianna Fáil. Sargent and the party negotiated a coalition government and at the 12 June 2007 membership meeting to approve the agreement, he announced his resignation as leader.
In the subsequent leadership election, John Gormley became the new leader on 17 July 2007, defeating Patricia McKenna by 478 votes to 263. Mary White was subsequently elected as the deputy Leader. John Gormley served as Minister for the Environment, Heritage and Local Government from July 2007 until the Green Party's decision to exit Government in December 2010.
Following the election defeat of 2011, John Gormley announced his intention not to seek another term as Green Party leader. Eamon Ryan was elected as the new party leader, over party colleagues Phil Kearney and Cllr Malcolm Noonan in a postal ballot election of party members in May 2011. Monaghan based former councillor Catherine Martin defeated Down based Dr John Barry and former Senator Mark Dearey to the post of Deputy Leader on 11 June 2011 during the party's annual convention.
The Green Party had six seats in the Irish government but lost them all in the 2011 general election. Party Chairman Dan Boyle and Déirdre de Búrca were nominated by the Taoiseach to Seanad Éireann after the formation of the Fianna Fáil–PD–Green Party government in 2007 and Niall Ó Brolcháin elected in December 2009. De Búrca resigned in February 2010, and was replaced by Mark Dearey. Neither Dan Boyle or Niall O'Brolchain were re-elected to Seanad Éireann in the Seanad election of 2011, meaning the Green Party is currently without Oireachtas representation.
Irish and European politics.
The Green Party is an all-island party, with a region in each of the Republic of Ireland and Northern Ireland. The Green Party in Northern Ireland voted to become a region of the Green Party in Ireland in 2005 at its annual convention, and again in a postal ballot in March 2006.) Brian Wilson, formerly a councillor for the Alliance Party, won the Green Party's first seat in the Northern Ireland Assembly in the 2007 election. Steven Agnew held that seat in the 2011 election.
The Irish Green Party is a member of the European Green Party. Though it previously held a more eurosceptic stance than is usually articulated by most other green parties in Europe, in 2009 the party backed the Lisbon Treaty with support from two thirds of the party.
2007 Dáil election.
Although the party's share of first preference votes increased by some 22% from 3.84% to 4.69% nationally in the 2007 general election, held on 24 May 2007, the party failed to increase the number of TDs returned. Mary White won a seat for the first time in Carlow-Kilkenny; however, Dan Boyle lost his seat in Cork South–Central leaving the party with the same number of TDs as before.
Those elected were:
In government.
The Green Party entered government with Fianna Fáil and the Progressive Democrats on 14 June 2007, gaining two senior ministers John Gormley, Minister for the Environment, Heritage and Local Government and Eamon Ryan, Minister for Communications, Energy and Natural Resources. Trevor Sargent was named the junior minister for Minister of State for Food and Horticulture, however Sargent later resigned the position in 2010. On 23 March 2010, the Green Party gained two new junior ministries. Ciaran Cuffe was appointed as Minister for Horticulture, Sustainable Travel, Planning and Heritage. Mary White was appointed as Minister for Equality, Human Rights and Integration. 
The Green Party had approached the 2007 General Election in the Republic on an independent platform, ruling out no coalition partners while expressing its preference for an alternative to the incumbent coalition. The results of the election ruled out the possibility of a Fine Gael/Labour/Green government without support from a combination of the Progressive Democrats, Sinn Féin and various independents (77 seats) leaving it 7 seats short of a majority. Fine Gael ruled out a potential coalition arrangement with Sinn Féin opening the way for Green Party negotiations with Fianna Fáil.
Negotiations for government.
Before the negotiations began Ciarán Cuffe wrote on his blog that "a deal with Fianna Fáil would be a deal with the devil... and [the Green Party would be] decimated as a Party". The negotiations were undertaken by Dan Boyle, Donall Geoghegan (the party's general secretary) and the at that time party Chair John Gormley. The Green Party walked out after 6 days in what Donall Geoghegan later said was due to there not being "enough in [the deal] to allow [the Green Party] to continue". The negotiations restarted on 11 June with a draft programme for government being agreed one day later, which under party rules needed 66% of members to endorse it at a special convention.
On 13 June 2007, Green members in the Mansion House, Dublin, voted 86% in favour (441 to 67; with 2 spoilt votes) of entering coalition with Fianna Fáil. The following day, the six Green Party TDs voted for the re-election of Bertie Ahern as Taoiseach.
This was the first time the Green Party had entered government in Ireland.
Criticisms.
Before their entry into government, the Green Party were vocal supporters of the Shell to Sea movement, the campaign to reroute the M3 motorway away from Tara and (to a lesser extent) the campaign to end United States military use of Shannon airport. Since the Green Party entered government, there were no substantive changes in government policy on these issues, which meant that Eamon Ryan oversaw the Corrib gas project while he was in office. The Green Party made an inquiry into the irregularities surrounding the project (see Corrib gas controversy) a precondition of government at their last annual conference but changed their stance during post-election negotiations with Fianna Fáil. The County Mayo branch of the party still supports efforts to relocate the refinery to an alternative location.
2008 budget.
The 2008 budget, announced on 6 December 2007, did not include a carbon levy on fuels such as petrol, diesel and home heating oil, which the Green Party had sought before the election. A carbon levy was however introduced in the 2010 Budget. The 2008 budget did include a separate carbon budget announced by Gormley, which introduced new energy efficiency tax credit, a ban on incandescent bulbs from January 2009, a tax scheme incentivising commuters' purchases of bicycles and a new scale of vehicle registration tax based on carbon emissions.
Treaty of Lisbon.
In 2007, the Green Party launched an internal debate on the party's stance on the Treaty of Lisbon. At a special convention on 19 January 2008 to consider whether or not to support what would become the Twenty-eighth Amendment of the Constitution of Ireland, the party voted 63.5% in favour of supporting the Lisbon Treaty fell short of the party's two-third majority requirement for policy issues. As a result, the Green Party did not participate in the referendum debate, although individual members were involved on different sides 
Following the Irish Government's negotiation with EU member states of additional legal guarantees and assurances, and the subsequent adoption by Dáil and Seanad Éireann of the Twenty-eighth Amendment of the Constitution Bill (2009), the Green Party held another special convention meeting in Dublin on 18 July 2009 to decide its position on the second Lisbon referendum. At the meeting precisely two thirds of party members present voted to campaign for a Yes in the referendum. This was the first time in the party's history that it campaigned in favour of a European treaty.
Resignations in 2010.
In 2010, Déirdre de Búrca, one of two Green Party Senators nominated by Taoiseach Bertie Ahern in 2007, resigned from the party and her seat, in part due to Gormley's inability to secure her a job in the European Commission. On 23 February 2010, Trevor Sargent, one of six Green Party TDs, and former leader of the party from 2001 to 2007, resigned as Minister of State for Food and Horticulture due to allegations over contacting Gardaí about a criminal case involving a constituent.
Withdrawal from government.
On 23 January 2011, the Green Party met with Taoiseach Brian Cowen following his resignation as leader of coalition partner Fianna Fáil the previous afternoon. The Green Party then announced it was withdrawing from governing the country and took its place on the opposition benches with immediate effect. Green Party leader John Gormley said at a press conference announcing the withdrawal: For a very long time we in the Green Party have stood back in the hope that Fianna Fáil could resolve persistent doubts about their party leadership. A definitive resolution of this has not yet been possible. And our patience has reached an end.
The party had two ministers: Minister for the Environment, Heritage and Local Government John Gormley and Minister for Communications, Energy and Natural Resources Eamon Ryan. These were reassigned to Fianna Fáil ministers Éamon Ó Cuív and Pat Carey respectively. Green Ministers of State Ciarán Cuffe and Mary White also resigned from their roles.
Government record.
In almost four years in Government, from 2007–2011, the party were said to have succeeded in, among other areas, the passage of civil partnership legislation granting significant rights to same-sex couples, the introduction of major planning reform, a major increase in renewable energy output, and a nationwide scheme of home insulation retrofitting.
2011 elections.
The party suffered a wipeout at the 2011 general election in the Republic, with all of its six TDs losing their seats, including those of former Ministers John Gormley and Eamon Ryan. Three out of their six incumbent TDs lost their deposits. The party's share of the vote fell below 2%, meaning that they could not reclaim election expenses, and their lack of parliamentary representation led to the ending of state funding for the party. 
The candidates in the 2011 Seanad election were Dan Boyle and Niall Ó Brolcháin, but neither were elected, and as a result for the first time since 1989, the Greens have no representatives in the Oireachtas.
Eamon Ryan was elected as party leader on 27 May 2011, succeeding John Gormley. Catherine Martin, a former Carrickmacross town councillor, was later appointed deputy leader, while Ciaran Cuffe and Mark Dearey were also placed on the party's front bench.
2014 local and European elections.
In the 2014 European Election the party received 4.9% of the vote nationally (an increase of 3% on the 2009 result). Despite a very close race between Eamon Ryan and Nessa Childers for the final seat in Dublin, no Green candidate was elected.
In the 2014 local elections in the Republic the party received 1.6% of the vote nationally. 12 candidates were elected to County Councils, compared to 3 previously.

</doc>
<doc id="15085" url="http://en.wikipedia.org/wiki?curid=15085" title="Iconoclasm">
Iconoclasm

Iconoclasm is the destruction of religious icons and other images or monuments for religious or political motives. In time, the word, usually in the adjectival form, has also come to refer to aggressive statements or actions against any well-established status quo. It is a frequent component of major political or religious changes. The term does not generally encompass the specific destruction of images of a ruler after his death or overthrow ("damnatio memoriae"). The destruction of religious icons by a group with another religion or culture is not considered iconoclasm. 
People who engage in or support iconoclasm are called "iconoclasts", a term that has come to be applied figuratively to any individual who challenges "cherished beliefs or venerated institutions on the grounds that they are erroneous or pernicious". Conversely, one who reveres or venerates religious images is called (by iconoclasts) an "iconolater"; in a Byzantine context, such a person is called an "iconodule" or "iconophile".
Iconoclasm may be carried out by people of a different religion, but is often the result of sectarian disputes between factions of the same religion. In Christianity, iconoclasm has generally been motivated by people who adopt a literal interpretation of the Ten Commandments, which forbid the making and worshipping of "graven images or any likeness of anything". The Church Fathers identified Jews and Judaism with heresy. They saw deviations from Orthodox Christianity and opposition to the veneration of images as heresies that were essentially "Jewish in spirit". The degree of iconoclasm among Christian sects greatly varies. Islam, in general, tends to be more iconoclastic than Christianity, with Sunni Islam being more iconoclastic than Shia Islam.
Religious iconoclasm.
Byzantine era.
Although widespread use of Christian iconography only began as Christianity increasingly spread among gentiles after the legalization of Christianity by Roman Emperor Constantine (c. 312 AD), scattered expressions of opposition to the use of images were reported (e.g. Spanish Synod of Elvira). The period after the reign of Roman Emperor Justinian (527–565) evidently saw a huge increase in the use of images, both in volume and quality, and a gathering aniconic reaction.
In the Eastern Roman (Byzantine) Empire, government-led iconoclasm began with Byzantine Emperor Leo III, following what seems to have been a long period of rising opposition to the use or misuse of images. The religious conflict created political and economic divisions in Byzantine society. It was generally supported by the Eastern, poorer, non-Greek peoples of the Empire who had to deal frequently with raids from the new Muslim Empire. On the other hand, the wealthier Greeks of Constantinople, and also the peoples of the Balkan and Italian provinces, strongly opposed iconoclasm.
Within the Byzantine Empire the government had probably been adopting Christian images more frequently. One notable change came in 695, when Justinian II's government added a full-face image of Christ on the obverse of imperial gold coins. The change caused the Caliph Abd al-Malik to stop his earlier adoption of Byzantine coin types. He started a purely Islamic coinage with lettering only. A letter by the patriarch Germanus written before 726 to two Iconoclast bishops says that "now whole towns and multitudes of people are in considerable agitation over this matter" but there is little written evidence of the debate.
Protestant Reformation.
Some of the Protestant reformers, in particular Andreas Karlstadt, Huldrych Zwingli and John Calvin, encouraged the removal of religious images by invoking the Decalogue's prohibition of idolatry and the manufacture of graven (sculpted) images of God. As a result, individuals attacked statues and images, and others were lost during unauthorised iconoclastic riots. However, in most cases, civil authorities removed images in an orderly manner in the newly reformed Protestant cities and territories of Europe.
Significant iconoclastic riots took place in Zurich (in 1523), Copenhagen (1530), Münster (1534), Geneva (1535), Augsburg (1537), Scotland (1559), Rouen (1560) and Saintes and La Rochelle (1562). The Seventeen Provinces (now the Netherlands, Belgium and parts of Northern France) were disrupted by widespread Protestant iconoclasm in the summer of 1566. This is called the "Beeldenstorm" and began with the destruction of the statuary of the Monastery of Saint Lawrence in Steenvoorde after a "Hagenpreek", or field sermon, by Sebastiaan Matte.
Hundreds of other attacks included the sacking of the Monastery of Saint Anthony after a sermon by Jacob de Buysere. The "Beeldenstorm" marked the start of the revolution against the Spanish forces and the Catholic Church.
The Iconoclast belief was causing havoc throughout Europe, and in 1523, specifically due to the Swiss reformer Huldrych Zwingli, a vast amount of his followers viewed themselves as being involved in a spiritual community that in matters of faith should obey neither the visible Church nor lay authorities. According to author R.W Scribner: "Zwingli's attack on images, at the first debate, triggered iconoclastic incidents in Zurich and the villages under civic jurisdiction that the reformer was unwilling to condone." And due to this action of protest against authority, “Zwingli responded with a carefully reasoned treatise that men could not live in society without laws and constraint.”—Wallace, pp. 95
Muslim iconoclasm.
Within Muslim history, the act of removing idols from the Ka'ba in Mecca is considered by all believers to be of great symbolic and historical importance.
In general, Muslim societies have avoided the depiction of living beings (animals and humans) within such sacred spaces as mosques and madrasahs. This opposition to figural representation is not based on the Qur'an, but rather on various traditions contained within the Hadith. The prohibition of figuration has not always extended to the secular sphere, and a robust tradition of figural representation exists within Muslim art.
However, western authors have tended to perceive "a long, culturally determined, and unchanging tradition of violent iconoclastic acts" within Islamic society.
Recent events.
Certain Muslim denominations continue to pursue iconoclastic agendas. There has been much controversy within Islam over the recent and apparently on-going destruction of historic sites by Saudi Arabian authorities, prompted by the fear they could become the subject of "idolatry".
During the Tuareg rebellion of 2012, the radical Islamist militia Ansar Dine destroyed various Sufi shrines from the 15th and 16th centuries in the city of Timbuktu, Mali.
During the Bahraini urprising a large number of Shia mosques were destroyed by the Sunni government.
The Islamic State of Iraq and the Levant has carried out iconoclastic attacks such as the destruction of Shia mosques and shrines. Notable incidents include the Mosque of the Prophet Yunus (Jonah) and the Shrine to Seth in Mosul.
Political and revolutionary iconoclasm.
"Damnatio memoriae".
Revolutions and changes of regime, whether through uprising of the local population, foreign invasion, or a combination of both, are often accompanied by the public destruction of statues and monuments identified with the previous regime. This may also be known as "damnatio memoriae", the Ancient Roman practice of official obliteration of the memory of a specific individual. Stricter definitions of "iconoclasm" exclude both types of action, reserving the term for religious or more widely cultural destruction. In many cases, such as Revolutionary Russia or Ancient Egypt, this distinction can be hard to make. 
Several Roman emperors and other political figures subject to decrees of damnatio memoriae, included Sejanus, Publius Septimius Geta, and Domitian.
Iconoclasm in the French Revolution.
Throughout the radical phase of the French Revolution, iconoclasm was supported by members of the government as well as the citizenry. Numerous monuments, religious works, and other historically significant pieces were destroyed in an attempt to eradicate any memory of the Old Regime. At the same time, the republican government felt responsible to preserve these works for their historical, aesthetic, and cultural value. One way the republican government succeeded in their paradoxical mission of preserving and destroying symbols of the Old Regime was through the development of museums.
During the Revolution, a statue of King Louis XV in the Paris square which until then bore his name, was pulled down and destroyed. This was a prelude to the guillotining of his successor Louis XVI in the same site, renamed "Place de la Révolution" (at present Place de la Concorde).
The statue of Napoleon on the column at Place Vendôme, Paris was also the target of iconoclasm several times: destroyed after the Bourbon Restoration, restored by Louis-Philippe, destroyed during the Paris Commune and restored by Adolphe Thiers.
Demolition of Hindu temples.
During Muslim conquest.
Records from the campaign recorded in the Chach Nama record temple demolitions during the early 8th century, when the Umayyad governor of Damascus, Hajjaj, mobilized an expedition of 6,000 cavalry under Muhammad bin-Qasim in 712 CE.
The historian, Upendra Thakur records the persecution of Hindus and Buddhists:
... Muhammad triumphantly marched into the country, conquering Debal, Sehwan, Nerun, Brahmanadabad, Alor and Multan one after the other in quick succession, and in less than a year and a half, the far-flung Hindu kingdom was crushed ... There was a fearful outbreak of religious bigotry in several places and temples were wantonly desecrated. At Debal, the Nairun and Aror temples were demolished and converted into mosques.
Sultãn Sikandar Butshikan of Kashmir (AD 1389–1413) ordered the breaking of all "golden and silver images". The Tarikh-i-Firishta states: "After the emigration of the Bramins, Sikundur ordered all the temples in Kashmeer to be thrown down. Having broken all the images in Kashmeer, (Sikandar) acquired the title of ‘Destroyer of Idols’".
In 725 Junayad, the Arab governor of Sind, sent his armies to destroy the second Somanath temple. In 1024 AD, the temple was once again destroyed by Mahmud Ghazni who raided the temple from across the Thar Desert. The temple was rebuilt by the Gujjar Paramara King Bhoj of Malwa and the Solanki king Bhima of Gujarat (Anhilwara) or Patan between 1026 and 1042. The wooden structure was replaced by Kumarpal (r. 1143–72), who built the temple of stone.
Chinese "anti-foreignism".
During the Northern Expedition, in 1926 in Guangxi, Kuomintang Muslim General Bai Chongxi led his troops in destroying Buddhist temples and smashing idols, turning the temples into schools and Kuomintang party headquarters. It was reported that almost all of Buddhist monasteries in Guangxi were destroyed by Bai in this manner. The monks were removed.
Bai led a wave of anti-foreignism in Guangxi, attacking American, European, and other foreigners and missionaries, and generally making the province unsafe for foreigners. Westerners fled from the province, and some Chinese Christians were also attacked as imperialist agents.
The three goals of the movement were anti-foreignism, anti-imperialism, and anti-religion. Bai led the anti-religious movement against superstition. Huang Shaoxiong, also a Kuomintang member of the New Guangxi clique, supported Bai's campaign. Huang was not a Muslim, and the anti-religious campaign was agreed upon by all Guangxi Kuomintang members.
Other examples.
Other examples of political destruction of images include:

</doc>
<doc id="15086" url="http://en.wikipedia.org/wiki?curid=15086" title="IWW (disambiguation)">
IWW (disambiguation)

IWW may refer to:

</doc>
<doc id="15087" url="http://en.wikipedia.org/wiki?curid=15087" title="Imbolc">
Imbolc

Imbolc or Imbolg (pronounced ), also called (Saint) Brigid's Day (Irish: "Lá Fhéile Bríde", Scottish Gaelic: "Là Fhèill Brìghde", Manx: "Laa'l Breeshey"), is a Gaelic festival marking the beginning of spring. Most commonly it is held on 1 February, or about halfway between the winter solstice and the spring equinox. Historically, it was widely observed throughout Ireland, Scotland and the Isle of Man. It is one of the four Gaelic seasonal festivals—along with Beltane, Lughnasadh and Samhain—and corresponds to the Welsh "Gŵyl Fair y Canhwyllau". Christians observe it as the feast day of Saint Brigid, especially in Ireland.
Imbolc is mentioned in some of the earliest Irish literature and there is evidence it has been an important date since ancient times. It is believed that it was originally a pagan festival associated with the goddess Brigid and that it was Christianized as a festival of Saint Brigid, who is thought to be a Christianization of the goddess. At Imbolc, Brigid's crosses were made and a doll-like figure of Brigid, called a "Brídeóg", would be paraded from house-to-house. Brigid was said to visit one's home at Imbolc. To receive her blessings, people would make a bed for Brigid and leave her food and drink, while items of clothing would be left outside for her to bless. Brigid was also invoked to protect homes and livestock. Special feasts were had, holy wells were visited and it was also a time for divination.
Although many of its customs died out in the 20th century, it is still observed and in some places it has been revived as a cultural event. Since the latter 20th century, Celtic neopagans and Wiccans have observed Imbolc, or something based on it, as a religious holiday.
Etymology.
The etymology of Imbolc/Imbolg is unclear. The most common explanation is that is comes from the Old Irish "i mbolc" (Modern Irish "i mbolg"), meaning "in the belly", and refers to the pregnancy of ewes. This is pronounced . Another possible origin is the Old Irish "imb-fholc", "to wash/cleanse oneself", referring to a ritual cleansing. Eric P. Hamp derives it from a Proto-Indo-European root meaning both "milk" and "cleansing". Professor Alan Ward derives it from the Proto-Celtic "*embibolgon", "budding". The 10th century Cormac's Glossary derives it from "oimelc", "ewe milk", but many scholars see this as a folk etymology. Nevertheless, some Neopagans have adopted "Oimelc" as a name for the festival.
Since Imbolc is immediately followed (on 2 February) by Candlemas (Irish "Lá Fhéile Muire na gCoinneal" "feast day of Mary of the Candles", Welsh "Gŵyl Fair y Canhwyllau"), Irish "Imbolc" is sometimes translated into English as "Candlemas"; e.g. "iar n-imbulc, ba garb a ngeilt" translated as "after Candlemas, rough was their herding".
Prehistory.
The date of Imbolc is thought to have been significant in Ireland since the Neolithic period. This is based on the alignment of some Megalithic monuments. For example, at the Mound of the Hostages on the Hill of Tara, the inner chamber is aligned with the rising sun on the dates of Imbolc and Samhain.
Historic Imbolc customs.
In Gaelic Ireland, Imbolc was the "feis" or festival marking the beginning of spring, during which great feasts were held. It is attested in some of the earliest Old Irish literature, from the 10th century onward. It was one of four Gaelic seasonal festivals: Samhain (~1 November), Imbolc (~1 February), Beltane (~1 May) and Lughnasadh (~1 August). 
From the 18th century to the mid 20th century, many accounts of Imbolc/St Brigid's Day were recorded by folklorists and other writers. They tell us how it was celebrated then, and shed light on how it may have been celebrated in the past.
Imbolc has traditionally been celebrated on 1 February. However, because the day was deemed to begin and end at sunset, the celebrations would start on what is now 31 January. It has also been argued that the timing of the festival was originally more fluid and based on seasonal changes. It has been associated with the onset of the lambing season—which could vary by as much as two weeks before or after 1 February—and the blooming of blackthorn. 
The holiday was a festival of the hearth and home, and a celebration of the lengthening days and the early signs of spring. Celebrations often involved hearthfires, special foods, divination or watching for omens, candles or a bonfire if the weather permitted. Fire and purification were an important part of the festival. The lighting of candles and fires represented the return of warmth and the increasing power of the Sun over the coming months. A spring cleaning was also customary.
Holy wells were visited at Imbolc, and at the other Gaelic festivals of Beltane and Lughnasa. Visitors to holy wells would pray for health while walking 'sunwise' around the well. They would then leave offerings; typically coins or clooties (see clootie well). Water from the well was used to bless the home, family members, livestock and fields.
Donald Alexander Mackenzie also recorded that offerings were made "to earth and sea". The offering could be milk poured into the ground or porridge poured into the water, as a libation.
Brigid.
Imbolc is strongly associated with Saint Brigid (Old Irish: "Brigit", modern Irish: "Bríd", modern Scottish Gaelic: "Brìghde" or "Brìd", anglicised "Bridget"). Saint Brigid is thought to have been based on Brigid, a Gaelic goddess. The festival, which celebrates the onset of spring, is thought to be linked with Brigid in her role as a fertility goddess.
On Imbolc Eve, Brigid was said to visit virtuous households and bless the inhabitants. As Brigid represented the light half of the year, and the power that will bring people from the dark season of winter into spring, her presence was very important at this time of year. 
Families would have a special meal or supper on Imbolc Eve. This typically included food such as colcannon, sowans, dumplings, barmbrack and/or bannocks. Often, some of the food and drink would be set aside for Brigid. 
Brigid would be symbolically invited into the house and a bed would often be made for her. In the north of Ireland a family member, representing Brigid, would circle the home three times carrying rushes. They would then knock the door three times, asking to be let in. On the third attempt they are welcomed in, the meal is had, and the rushes are then made into a bed or crosses. In 18th century Mann, the custom was to stand at the door with a bundle of rushes and say "Brede, Brede, come to my house tonight. Open the door for Brede and let Brede come in". The rushes were then strewn on the floor as a carpet or bed for Brigid. In the 19th century, some old Manx women would make a bed for Brigid in the barn with food, ale, and a candle on a table. In the Hebrides in the late 18th century, a bed of hay would be made for Brigid and someone would then call out three times: "a Bhríd, a Bhríd, thig a sligh as gabh do leabaidh" ("Bríd Bríd, come in; thy bed is ready"). A white wand, usually made of birch, would be set by the bed. It represented the wand that Brigid was said to use to make the vegetation start growing again. In the 19th century, women in the Hebrides would dance while holding a large cloth and calling out "Bridean, Bridean, thig an nall 's dean do leabaidh" ("Bríd Bríd, come over and make your bed"). However, by this time the bed itself was rarely made.
Before going to bed, people would leave items of clothing or strips of cloth outside for Brigid to bless. Ashes from the fire would be raked smooth and, in the morning, they would look for some kind of mark on the ashes as a sign that Brigid had visited. The clothes or strips of cloth would be brought inside, and believed to now have powers of healing and protection.
In Ireland and Scotland, a representation of Brigid would be paraded around the community by girls and young women. Sometimes the representative was a girl, but usually it was a doll-like figure known as a "Brídeóg" (also called a 'Breedhoge' or 'Biddy'). It would be made from rushes or reeds and clad in bits of cloth, shells and/or flowers. In the Hebrides of Scotland, a bright shell or crystal called the "reul-iuil Bríde" (guiding star of Brigid) was set on its chest. The girls would carry it in procession while singing a hymn to Brigid. All wore white with their hair unbound as a symbol of purity and youth. They visited every house in the area, where they received either food or more decoration for the Brídeóg. Afterwards, they feasted in a house with the Brídeóg set in a place of honour, and put it to bed with lullabies. When the meal was done, the local young men humbly asked for admission, made obeisance to the Brídeóg, and joined the girls in dancing and merrymaking. In many parts, only unwed girls could carry the Brídeóg, but in some places both boys and girls carried it. In the late 17th century, Catholic families in the Hebrides would make a bed for the Brídeóg out of a basket. Up until the mid-20th century, children in Ireland still went house-to-house asking for pennies for "poor Biddy", or money for the poor. In County Kerry, men in white robes went from house to house singing.
In Ireland, Brigid's crosses ("pictured on the right") were made at Imbolc. A Brigid's cross usually consists of rushes woven into a square or equilateral cross, although three-armed crosses have also been recorded. They were often hung over doors, windows and stables to welcome Brigid and protect the buildings from fire and lightning. The crosses were generally left there until the next Imbolc. In western Connacht, people would make a "Crios Bríde" (Bríd's girdle); a great ring of rushes with a cross woven in the middle. Young boys would carry it around the village, inviting people to step through it and so be blessed.
Today, some people still make Brigid's crosses and Brídeógs or visit holy wells dedicated to St Brigid on 1 February.
Weather divination.
Imbolc was traditionally a time of weather divination, and the old tradition of watching to see if serpents or badgers came from their winter dens may be a forerunner of the North American Groundhog Day. A Scottish Gaelic proverb about the day is:
<poem>"Thig an nathair as an toll"
"Là donn Brìde,"
"Ged robh trì troighean dhen t-sneachd"
"Air leac an làir."
The serpent will come from the hole
On the brown Day of Bríde,
Though there should be three feet of snow
On the flat surface of the ground.</poem>
Imbolc was believed to be when the Cailleach—the divine hag of Gaelic tradition—gathers her firewood for the rest of the winter. Legend has it that if she wishes to make the winter last a good while longer, she will make sure the weather on Imbolc is bright and sunny, so she can gather plenty of firewood. Therefore, people would be relieved if Imbolc is a day of foul weather, as it means the Cailleach is asleep and winter is almost over. At Imbolc on the Isle of Man, where she is known as "Caillagh ny Groamagh", the Cailleach is said to take the form of a gigantic bird carrying sticks in her beak.
Neopaganism.
Imbolc or Imbolc-based festivals are held by some Neopagans. As there are many kinds of Neopaganism, their Imbolc celebrations can be very different despite the shared name. Some try to emulate the historic festival as much as possible. Other Neopagans base their celebrations on many sources, with historic accounts of Imbolc being only one of them.
Neopagans usually celebrate Imbolc on 1 February in the Northern Hemisphere and 1 August in the Southern Hemisphere. Some Neopagans celebrate it at the astronomical midpoint between the winter solstice and spring equinox (or the full moon nearest this point). In the Northern Hemisphere, this is usually on the 3rd or 4th of February. Other Neopagans celebrate Imbolc when the primroses, dandelions, and other spring flowers emerge.
Celtic Reconstructionist.
Celtic Reconstructionists strive to reconstruct the pre-Christian religions of the Celts. Their religious practices are based on research and historical accounts, but may be modified slightly to suit modern life. They avoid syncretism (i.e. combining practises from different cultures). They usually celebrate the festival when the first stirrings of spring are felt, or on the full moon nearest this. Many use traditional songs and rites from sources such as "The Silver Bough" and "The Carmina Gadelica". It is a time of honouring the Goddess Brigid, and many of her dedicants choose this time of year for rituals to her.
Wicca.
Wiccans celebrate something based on Imbolc as one of the eight "Sabbats" in their Wheel of the Year, following Midwinter and preceding Ostara. In Wicca, Imbolc is commonly associated with the goddess Brigid and as such it is sometimes seen as a "women's holiday" with specific rites only for female members of a coven. Among Dianic Wiccans, Imbolc is the traditional time for initiations.
External links.
Modern events

</doc>
<doc id="15088" url="http://en.wikipedia.org/wiki?curid=15088" title="Isaiah">
Isaiah

Isaiah ( or ;
Hebrew: יְשַׁעְיָהוּ,  "Yeshayahu",  "Yəšạʻyā́hû" ; Syriac: ܐܫܥܝܐ "Eshaya"; Greek: Ἠσαΐας, "Ēsaïās"; Arabic: إشعيا "Ishiya"; "Yah is salvation") was a prophet documented by the Biblical Book of Isaiah to have lived around the time of 8th-century BC Kingdom of Judah.
The exact relationship between the Book of Isaiah and any such historical Isaiah is complicated. One widespread view sees parts of the first half of the book (chapters 1–39) as originating with the historical prophet, interspersed with prose commentaries written in the time of King Josiah a hundred years later; with the remainder of the book dating from immediately before and immediately after the end of the exile in Babylon, almost two centuries after the time of the original prophet.
Jews and Christians consider the Book of Isaiah a part of their Biblical canon; he is the first listed (although not the earliest) of the "Nevi'im Aharonim", the latter prophets. 
Biography.
The first verse of the Book of Isaiah states that Isaiah prophesied during the reigns of Uzziah (or Azariah), Jotham, Ahaz, and Hezekiah, the kings of Judah (). Uzziah's reign was 52 years in the middle of the 8th century BC, and Isaiah must have begun his ministry a few years before Uzziah's death, probably in the 740s BC. Isaiah lived until the fourteenth year of Hezekiah's reign (who died 698 BC), and may have been contemporary for some years with Manasseh. Thus Isaiah may have prophesied for as long as 64 years.
Isaiah's wife was called "the prophetess" (), either because she was endowed with the prophetic gift, like Deborah () and Huldah (), or simply because she was the "wife of the prophet" (as he is named, for instance in ). The second interpretation, that it was simply an honorary title is likely. They had two sons, naming one Shear-Jashub, meaning "A remnant shall return" () and the younger, Maher-Shalal-Hash-Baz, meaning, "Spoil quickly, plunder speedily." () The book of Isaiah, along with the book of Jeremiah, is distinctive in the Hebrew bible for its direct portrayal of the "wrath of the Lord" as presented, for example, in Isaiah 9:19 stating, "Through the wrath of the Lord of hosts is the land darkened, and the people shall be as the fuel of the fire."
In early youth, Isaiah may have been moved by the invasion of Israel by the Assyrian monarch Tiglath-Pileser III (); and again, twenty years later, when he had already entered his office, by the invasion of Tiglath-Pileser and his career of conquest. Ahaz, king of Judah, at this crisis refused to co-operate with the kings of Israel and Syria in opposition to the Assyrians, and was on that account attacked and defeated by Rezin of Damascus and Pekah of Israel (; ). Humbled, Ahaz sided with Assyria and sought the aid of Tiglath-Pileser against Israel and Syria. The consequence was that Rezin and Pekah were conquered and many of the people carried captive to Assyria (, ; ).
Soon after this, Shalmaneser V determined to subdue the kingdom of Israel, Samaria was taken and destroyed (722 BC). So long as Ahaz reigned, the kingdom of Judah was unmolested by the Assyrian power; but on his accession to the throne, Hezekiah, who was encouraged to rebel "against the king of Assyria" (), entered into an alliance with the king of Egypt (). This led the king of Assyria to threaten the king of Judah, and at length to invade the land. Sennacherib (701 BC) led a powerful army into Judah. Hezekiah was reduced to despair, and submitted to the Assyrians (). But after a brief interval war broke out again. Again Sennacherib led an army into Judah, one detachment of which threatened Jerusalem (; ). Isaiah on that occasion encouraged Hezekiah to resist the Assyrians (), whereupon Sennacherib sent a threatening letter to Hezekiah, which he "spread before the LORD" ().
Then Isaiah the son of Amoz sent unto Hezekiah, saying: "Thus saith the LORD, the God of Israel: Whereas thou hast prayed to Me against Sennacherib king of Assyria,
this is the word which the LORD hath spoken concerning him: The virgin daughter of Zion hath despised thee and laughed thee to scorn; the daughter of Jerusalem hath shaken her head at thee.
Whom hast thou taunted and blasphemed? And against whom hast thou exalted thy voice? Yea, thou hast lifted up thine eyes on high, even against the Holy One of Israel!" ()
According to the account in 2 Kings 19 (and its derivative account in 2 Chronicles 32) the judgment of God now fell on the Assyrian army and wiped out 185,000 of its men. "Like Xerxes in Greece, Sennacherib never recovered from the shock of the disaster in Judah. He made no more expeditions against either the Southern Levant or Egypt."
The remaining years of Hezekiah's reign were peaceful (). Isaiah probably lived to its close, and possibly into the reign of Manasseh, but the time and manner of his death are not specified in either the Bible or recorded history. There is a tradition in Rabbinic literature that he suffered martyrdom by Manasseh.
In Christianity.
Gregory of Nyssa (c. 335–395), believed that the Prophet Isaiah "knew more perfectly than all others the mystery of the religion of the Gospel". Jerome (c. 342–420) also lauds the Prophet Isaiah, saying, "He was more of an Evangelist than a Prophet, because he described all of the Mysteries of the Church of Christ so vividly that you would assume he was not prophesying about the future, but rather was composing a history of past events."
In Islam.
Although Isaiah is not mentioned by name in the Quran or the Hadith, Muslim sources have accepted him as a prophet. Some Muslim scholars, such as Ibn Kathir and Kisa'i, reproduced Jewish traditions, transmitted through early Jewish converts to Islam, regarding Isaiah. Such Old Testament stories, which are not confirmed by the Quran or prophetic hadith, are referred to as Isra'iliyyah, and are not considered strong enough to be used as evidence in Islamic law. Isaiah is mentioned as a prophet in Ibn Kathir's "Story of Prophet Isaiah", and the modern writers Muhammad Asad and Abdullah Yusuf Ali accepted Isaiah as a true Hebrew prophet, who preached to the Israelites following the death of King David.
Isaiah is well known in Muslim exegesis and literature, notably for his predictions of the coming of Jesus and Muhammad. Isaiah's narrative in Muslim literature can be divided into three sections. The first establishes Isaiah as a prophet of Israel during the reign of Hezekiah; the second relates Isaiah's actions during the siege of Jerusalem by Sennacherib; and the third warns the nation of coming doom.
Muslim exegesis preserves a tradition parallel to the Hebrew Bible, which states that Hezekiah was king in Jerusalem during Isaiah's time. Hezekiah heard and obeyed Isaiah's advice, but could not quell the turbulence in Israel. This tradition, maintains that Hezekiah was a righteous man and that the turbulence worsened after him. After the death of the king, Isaiah told the people to not forsake God, and he warned Israel to cease from its persistent sin and disobedience. Muslim tradition maintains that the unrighteous of Israel in their anger sought to kill Isaiah. In a death that resembles that attributed to Isaiah in "Lives of the Prophets", Muslim exegesis recounts that Isaiah was martyred by Israelites by being sawed in half.
In the Baha'i Faith.
Isaiah is considered a lesser prophet in the Baha'i Faith. Abdul-Baha mentions prophecies by Isaiah which refer to a man called the Branch as applying to Baha'ullah.
Rabbinic literature.
According to the Rabbinic literature, Isaiah was a descendant of the royal house of Judah and Tamar (Sotah 10b). He was the son of Amoz ("not to be confused with Prophet Amos"), who was the brother of King Amaziah of Juda. (Talmud tractate Megillah 15a).

</doc>
<doc id="15089" url="http://en.wikipedia.org/wiki?curid=15089" title="Interpreted language">
Interpreted language

An interpreted language is a programming language for which most of its implementations execute instructions directly, without previously compiling a program into machine-language instructions. The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines already compiled into machine code.
The terms "interpreted language" and "compiled language" are not well defined because, in theory, any programming language can be either interpreted or compiled. In modern programing language implementation it is increasingly popular for a platform to provide both options.
Interpreted languages can also be contrasted with machine languages. Functionally, both execution and interpretation mean the same thing — fetching the next instruction/statement from the program and executing it. Although interpreted bytecode is additionally identical to machine code in form and has an assembler representation, the term "interpreted" is practically reserved for "software processed" languages (by virtual machine or emulator) on top of the native (i.e. hardware) processor.
In principle, programs in many languages may be compiled or interpreted, emulated or executed natively, so this designation is applied solely based on common implementation practice, rather than representing an essential property of a language. Akin to processor microcoding, many interpreters internally rely on just-in-time compilation.
Avoiding compilation, interpreted programs are easier to evolve during both development and execution (where they can morph themselves). On the other hand, since compilation implies translation into more machine-friendly format, interpreted programs run more slowly and less efficiently (i.e. waste considerably more energy). This is especially true for higher-level scripting languages, whose statements are complex to analyze compared to machine instruction.
Many languages have been implemented using both compilers and interpreters, including BASIC, C, Lisp, Pascal, and Python. Java and C# are compiled into bytecode, the virtual machine-friendly interpreted language. Lisp implementations can freely mix interpreted and compiled code.
Historical background of interpreted/compiled.
In the early days of computing, language design was heavily influenced by the decision to use compiling or interpreting as a mode of execution. For example, some compiled languages require that programs must explicitly state the data-type of a variable at the time it is declared or first used while some interpreted languages take advantage of the dynamic aspects of interpreting to make such declarations unnecessary. For example, Smalltalk (1980), which was designed to be interpreted at run-time, allows generic objects to dynamically interact with each other.
Initially, interpreted languages were compiled line-by-line; that is, each line was compiled as it was about to be executed, and if a loop or subroutine caused certain lines to be executed multiple times, they would be recompiled every time. This has become much less common. Most so-called interpreted languages use an intermediate representation, which combines compiling and interpreting. In this case, a compiler may output some form of bytecode or threaded code, which is then executed by a bytecode interpreter.
Examples include: 
The intermediate representation can be compiled once and for all (as in Java), each time before execution (as in Perl or Ruby), or each time a change in the source is detected before execution (as in Python).
Advantages of interpreting a language.
Interpreting a language gives implementations some additional flexibility over compiled implementations. Features that are often easier to implement in interpreters than in compilers include (but are not limited to):
Disadvantages of interpreted languages.
The main disadvantage of interpreting is a much slower speed of program execution compared to direct machine code execution on the host CPU. A technique used to improve performance is just-in-time compilation which converts frequently executed sequences of interpreted instruction to host machine code.
List of frequently used interpreted languages.
Languages usually compiled to a bytecode.
Many interpreted languages are first compiled to bytecode, which is normally interpreted by virtual machine exploiting some just-in-time compilation of bytecode to native code. However, sometimes, bytecode can also be compiled to a native binary using an AOT compiler) or executed natively, by hardware processor.

</doc>
<doc id="15095" url="http://en.wikipedia.org/wiki?curid=15095" title="Intifada">
Intifada

Intifada (انتفاضة "intifāḍah") is an Arabic word which literally means "flood" or "flooding", though it is popularly translated into English as "uprising", "resistance", or "rebellion". An intifada involves a large number of people, hence the word "flooding". It is often used as a term for popular resistance to oppression.
Intifada may be used to refer to these events:

</doc>
<doc id="15097" url="http://en.wikipedia.org/wiki?curid=15097" title="Ionosphere">
Ionosphere

The ionosphere is a region of Earth's upper atmosphere, from about 60 km to 600 km altitude, and includes the thermosphere and parts of the mesosphere and exosphere. It is distinguished because it is ionized by solar radiation. It plays an important part in atmospheric electricity and forms the inner edge of the magnetosphere. It has practical importance because, among other functions, it influences radio propagation to distant places on the Earth.
Geophysics.
The ionosphere is a shell of electrons and electrically charged atoms and molecules that surrounds the Earth, stretching from a height of about 50 km to more than 1000 km. It owes its existence primarily to ultraviolet radiation from the Sun.
The lowest part of the Earth's atmosphere, the troposphere extends from the surface to about 10 km. Above 10 km is the stratosphere, followed by the mesosphere. In the stratosphere incoming solar radiation creates the ozone layer. At heights of above 80 km, in the thermosphere, the atmosphere is so thin that free electrons can exist for short periods of time before they are captured by a nearby positive ion. The number of these free electrons is sufficient to affect radio propagation. This portion of the atmosphere is "ionized" and contains a plasma which is referred to as the ionosphere. In a plasma, the negative free electrons and the positive ions are attracted to each other by the electrostatic force, but they are too energetic to stay fixed together in an electrically neutral molecule.
Ultraviolet (UV), X-ray and shorter wavelengths of solar radiation are "ionizing," since photons at these frequencies contain sufficient energy to dislodge an electron from a neutral gas atom or molecule upon absorption. In this process the light electron obtains a high velocity so that the temperature of the created electronic gas is much higher (of the order of thousand K) than the one of ions and neutrals. The reverse process to ionization is recombination, in which a free electron is "captured" by a positive ion. Recombination occurs spontaneously, and causes the emission of a photon carrying away the energy produced upon recombination. As gas density increases at lower altitudes, the recombination process prevails, since the gas molecules and ions are closer together. The balance between these two processes determines the quantity of ionization present.
Ionization depends primarily on the Sun and its activity. The amount of ionization in the ionosphere varies greatly with the amount of radiation received from the Sun. Thus there is a diurnal (time of day) effect and a seasonal effect. The local winter hemisphere is tipped away from the Sun, thus there is less received solar radiation. The activity of the Sun is associated with the sunspot cycle, with more radiation occurring with more sunspots. Radiation received also varies with geographical location (polar, auroral zones, mid-latitudes, and equatorial regions). There are also mechanisms that disturb the ionosphere and decrease the ionization. There are disturbances such as solar flares and the associated release of charged particles into the solar wind which reaches the Earth and interacts with its geomagnetic field.
The ionospheric layers.
At night the F layer is the only layer of significant ionization present, while the ionization in the E and D layers is extremely low. During the day, the D and E layers become much more heavily ionized, as does the F layer, which develops an additional, weaker region of ionisation known as the F1 layer. The F2 layer persists by day and night and is the region mainly responsible for the refraction of radio waves.
D layer.
The D layer is the innermost layer, 60 km to 90 km above the surface of the Earth. Ionization here is due to Lyman series-alpha hydrogen radiation at a wavelength of 121.5 nanometre (nm) ionizing nitric oxide (NO). In addition, high solar activity can generate hard X-rays (wavelength < 1 nm) that ionize N2 and O2. Recombination is high in the D layer, so net ionization is low, and high-frequency (HF) radio waves are significantly damped within the D layer by collisions with electrons (about ten collisions every millisecond). This is the main reason for absorption of HF radio waves, particularly at 10 MHz and below, with progressively smaller absorption as the frequency gets higher. This effect peaks around noon and is reduced at night due to a decrease in the D layer's thickness; only a small part remains due to cosmic rays. A common example of the D layer in action is the disappearance of distant AM broadcast band stations in the daytime.
During solar proton events, ionization can reach unusually high levels in the D-region over high and polar latitudes. Such very rare events are known as Polar Cap Absorption (or PCA) events, because the increased ionization significantly enhances the absorption of radio signals passing through the region. In fact, absorption levels can increase by many tens of dB during intense events, which is enough to absorb most (if not all) transpolar HF radio signal transmissions. Such events typically last less than 24 to 48 hours.
E layer.
The E layer is the middle layer, 90 km to 150 km above the surface of the Earth. Ionization is due to soft X-ray (1–10 nm) and far ultraviolet (UV) solar radiation ionization of molecular oxygen (O₂). Normally, at oblique incidence, this layer can only reflect radio waves having frequencies lower than about 10 MHz and may contribute a bit to absorption on frequencies above. However, during intense Sporadic E events, the Es layer can reflect frequencies up to 50 MHz and higher. The vertical structure of the E layer is primarily determined by the competing effects of ionization and recombination. At night the E layer weakens because the primary source of ionization is no longer present. After sunset an increase in the height of the E layer maximum increases the range to which radio waves can travel by reflection from the layer.
This region is also known as the Kennelly–Heaviside layer or simply the Heaviside layer. Its existence was predicted in 1902 independently and almost simultaneously by the American electrical engineer Arthur Edwin Kennelly (1861–1939) and the British physicist Oliver Heaviside (1850–1925). However, it was not until 1924 that its existence was detected by Edward V. Appleton and Miles Barnett.
Es.
The Es layer (sporadic E-layer) is characterized by small, thin clouds of intense ionization, which can support reflection of radio waves, rarely up to 225 MHz. Sporadic-E events may last for just a few minutes to several hours. Sporadic E propagation makes VHF-operating radio amateurs very excited, as propagation paths that are generally unreachable can open up. There are multiple causes of sporadic-E that are still being pursued by researchers. This propagation occurs most frequently during the summer months when high signal levels may be reached. The skip distances are generally around 1640 km. Distances for one hop propagation can be anywhere from 900 km to 2500 km. Double-hop reception over 3500 km is possible.
F layer.
The F layer or region, also known as the Appleton-Barnett layer, extends from about 150 km to more than 500 km above the surface of Earth. It is the densest point of the ionosphere, which implies signals penetrating this layer will escape into space. At higher altitudes, the number of oxygen ions decreases and lighter ions such as hydrogen and helium become dominant; this layer is the topside ionosphere. There, extreme ultraviolet (UV, 10–100 nm) solar radiation ionizes atomic oxygen. The F layer consists of one layer at night, but during the day, a deformation often forms in the profile that is labeled F₁. The F₂ layer remains by day and night responsible for most skywave propagation of radio waves, facilitating high frequency (HF, or shortwave) radio communications over long distances.
From 1972 to 1975 NASA launched the AEROS and AEROS B satellites to study the F region.
Ionospheric model.
An ionospheric model is a mathematical description of the ionosphere as a function of location, altitude, day of year, phase of the sunspot cycle and geomagnetic activity. Geophysically, the state of the ionospheric plasma may be described by four parameters: "electron density, electron and ion temperature" and, since several species of ions are present, "ionic composition". Radio propagation depends uniquely on electron density.
Models are usually expressed as computer programs. The model may be based on basic physics of the interactions of the ions and electrons with the neutral atmosphere and sunlight, or it may be a statistical description based on a large number of observations or a combination of physics and observations. One of the most widely used models is the International Reference Ionosphere (IRI), which is based on data and specifies the four parameters just mentioned. The IRI is an international project sponsored by the Committee on Space Research (COSPAR) and the International Union of Radio Science (URSI). The major data sources are the worldwide network of ionosondes, the powerful incoherent scatter radars (Jicamarca, Arecibo, Millstone Hill, Malvern, St. Santin), the ISIS and Alouette topside sounders, and in situ instruments on several satellites and rockets. IRI is updated yearly. IRI is more accurate in describing the variation of the electron density from bottom of the ionosphere to the altitude of maximum density than in describing the total electron content (TEC) .Since 1999 this model is "International Standard" for the terrestrial ionosphere (standard TS16457).
Persistent anomalies to the idealized model.
Ionograms allow deducing, via computation, the true shape of the different layers. Nonhomogeneous structure of the electron/ion-plasma produces rough echo traces, seen predominantly at night and at higher latitudes, and during disturbed conditions.
Winter anomaly.
At mid-latitudes, the F2 layer daytime ion production is higher in the summer, as expected, since the Sun shines more directly on the Earth. However, there are seasonal changes in the molecular-to-atomic ratio of the neutral atmosphere that cause the summer ion loss rate to be even higher. The result is that the increase in the summertime loss overwhelms the increase in summertime production, and total F2 ionization is actually lower in the local summer months. This effect is known as the winter anomaly. The anomaly is always present in the northern hemisphere, but is usually absent in the southern hemisphere during periods of low solar activity.
Equatorial anomaly.
Within approximately ± 20 degrees of the "magnetic equator", is the "equatorial anomaly". It is the occurrence of a trough in the ionization in the F2 layer at the equator and crests at about 17 degrees in magnetic latitude. The Earth's magnetic field lines are horizontal at the magnetic equator. Solar heating and tidal oscillations in the lower ionosphere move plasma up and across the magnetic field lines. This sets up a sheet of electric current in the E region which, with the horizontal magnetic field, forces ionization up into the F layer, concentrating at ± 20 degrees from the magnetic equator. This phenomenon is known as the "equatorial fountain".
Equatorial electrojet.
The worldwide solar-driven wind results in the so-called Sq (solar quiet) current system in the E region of the Earth's ionosphere (ionospheric dynamo region) (100 km – 130 km altitude). Resulting from this current is an electrostatic field directed E-W (dawn-dusk) in the equatorial day side of the ionosphere. At the magnetic dip equator, where the geomagnetic field is horizontal, this electric field results in an enhanced eastward current flow within ± 3 degrees of the magnetic equator, known as the equatorial electrojet.
Ephemeral ionospheric perturbations.
X-rays: sudden ionospheric disturbances (SID).
When the Sun is active, strong solar flares can occur that will hit the sunlit side of Earth with hard X-rays. The X-rays will penetrate to the D-region, releasing electrons that will rapidly increase absorption, causing a high frequency (3–30 MHz) radio blackout. During this time very low frequency (3–30 kHz) signals will be reflected by the D layer instead of the E layer, where the increased atmospheric density will usually increase the absorption of the wave and thus dampen it. As soon as the X-rays end, the sudden ionospheric disturbance (SID) or radio black-out ends as the electrons in the D-region recombine rapidly and signal strengths return to normal.
Protons: polar cap absorption (PCA).
Associated with solar flares is a release of high-energy protons. These particles can hit the Earth within 15 minutes to 2 hours of the solar flare. The protons spiral around and down the magnetic field lines of the Earth and penetrate into the atmosphere near the magnetic poles increasing the ionization of the D and E layers. PCA's typically last anywhere from about an hour to several days, with an average of around 24 to 36 hours.
Geomagnetic storms.
A geomagnetic storm is a temporary intense disturbance of the Earth's magnetosphere.
Lightning.
Lightning can cause ionospheric perturbations in the D-region in one of two ways. The first is through VLF (very low frequency) radio waves launched into the magnetosphere. These so-called "whistler" mode waves can interact with radiation belt particles and cause them to precipitate onto the ionosphere, adding ionization to the D-region. These disturbances are called "lightning-induced electron precipitation" (LEP) events.
Additional ionization can also occur from direct heating/ionization as a result of huge motions of charge in lightning strikes. These events are called early/fast.
In 1925, C. T. R. Wilson proposed a mechanism by which electrical discharge from lightning storms could propagate upwards from clouds to the ionosphere. Around the same time, Robert Watson-Watt, working at the Radio Research Station in Slough, UK, suggested that the ionospheric sporadic E layer (Es) appeared to be enhanced as a result of lightning but that more work was needed. In 2005, C. Davis and C. Johnson, working at the Rutherford Appleton Laboratory in Oxfordshire, UK, demonstrated that the Es layer was indeed enhanced as a result of lightning activity. Their subsequent research has focused on the mechanism by which this process can occur.
Applications.
Radio communication.
DX communication, popular among amateur radio enthusiasts, is a term given to communication over great distances. Thanks to the property of ionized atmospheric gases to refract high frequency (HF, or shortwave) radio waves, the ionosphere can be utilized to "bounce" a transmitted signal down to ground. Transcontinental HF-connections rely on up to 5 bounces, or hops. Such communications played an important role during World War II. Karl Rawer's most sophisticated prediction method took account of several (zig-zag) paths, attenuation in the D-region and predicted the 11-year solar cycle by a method due to Wolfgang Gleißberg.
Mechanism of refraction.
When a radio wave reaches the ionosphere, the electric field in the wave forces the electrons in the ionosphere into oscillation at the same frequency as the radio wave. Some of the radio-frequency energy is given up to this resonant oscillation. The oscillating electrons will then either be lost to recombination or will re-radiate the original wave energy. Total refraction can occur when the collision frequency of the ionosphere is less than the radio frequency, and if the electron density in the ionosphere is great enough.
The critical frequency is the limiting frequency at or below which a radio wave is reflected by an ionospheric layer at vertical incidence. If the transmitted frequency is higher than the plasma frequency of the ionosphere, then the electrons cannot respond fast enough, and they are not able to re-radiate the signal. It is calculated as shown below:
where N = electron density per m3 and fcritical is in Hz.
The Maximum Usable Frequency (MUF) is defined as the upper frequency limit that can be used for transmission between two points at a specified time.
where formula_3 = angle of attack, the angle of the wave relative to the horizon, and sin is the sine function.
The cutoff frequency is the frequency below which a radio wave fails to penetrate a layer of the ionosphere at the incidence angle required for transmission between two specified points by refraction from the layer.
Other applications.
The open system electrodynamic tether, which uses the ionosphere, is being researched. The space tether uses plasma contactors and the ionosphere as parts of a circuit to extract energy from the Earth's magnetic field by electromagnetic induction.
Measurements.
Overview.
Scientists also are exploring the structure of the ionosphere by a wide variety of methods, including passive observations of optical and radio emissions generated in the ionosphere, bouncing radio waves of different frequencies from it, incoherent scatter radars such as the EISCAT, Sondre Stromfjord, Millstone Hill, Arecibo, and Jicamarca radars, coherent scatter radars such as the Super Dual Auroral Radar Network (SuperDARN) radars, and using special receivers to detect how the reflected waves have changed from the transmitted waves.
A variety of experiments, such as HAARP (High Frequency Active Auroral Research Program), involve high power radio transmitters to modify the properties of the ionosphere. These investigations focus on studying the properties and behavior of ionospheric plasma, with particular emphasis on being able to understand and use it to enhance communications and surveillance systems for both civilian and military purposes. HAARP was started in 1993 as a proposed twenty-year experiment, and is currently active near Gakona, Alaska.
The SuperDARN radar project researches the high- and mid-latitudes using coherent backscatter of radio waves in the 8 to 20 MHz range. Coherent backscatter is similar to Bragg scattering in crystals and involves the constructive interference of scattering from ionospheric density irregularities. The project involves more than 11 different countries and multiple radars in both hemispheres.
Scientists are also examining the ionosphere by the changes to radio waves, from satellites and stars, passing through it. The Arecibo radio telescope located in Puerto Rico, was originally intended to study Earth's ionosphere.
Ionograms.
Ionograms show the virtual heights and critical frequencies of the ionospheric layers and which are measured by an ionosonde. An ionosonde sweeps a range of frequencies, usually from 0.1 to 30 MHz, transmitting at vertical incidence to the ionosphere. As the frequency increases, each wave is refracted less by the ionization in the layer, and so each penetrates further before it is reflected. Eventually, a frequency is reached that enables the wave to penetrate the layer without being reflected. For ordinary mode waves, this occurs when the transmitted frequency just exceeds the peak plasma, or critical, frequency of the layer. Tracings of the reflected high frequency radio pulses are known as ionograms. Reduction rules are given in: "URSI Handbook of Ionogram Interpretation and Reduction", edited by William Roy Piggott and Karl Rawer, Elsevier Amsterdam, 1961 (translations into Chinese, French, Japanese and Russian are available).
Incoherent scatter radars.
Incoherent scatter radars operate above the critical frequencies. Therefore the technique allows to probe the ionosphere, unlike ionosondes, also above the electron density peaks. The thermal fluctuations of the electron density scattering the transmitted signals lack coherence, which gave the technique its name. Their power spectrum contains information not only on the density, but also on the ion and electron temperatures, ion masses and drift velocities.
Solar flux.
Solar flux is a measurement of the intensity of solar radio emissions at a frequency of 2800 MHz made using a radio telescope located in Dominion Radio Astrophysical Observatory, Penticton, British Columbia, Canada. Known also as the 10.7 cm flux (the wavelength of the radio signals at 2800 MHz), this solar radio emission has been shown to be proportional to sunspot activity. However, the level of the Sun's ultraviolet and X-ray emissions is primarily responsible for causing ionization in the Earth's upper atmosphere. We now have data from the GOES spacecraft that measures the background X-ray flux from the Sun, a parameter more closely related to the ionization levels in the ionosphere.
Ionospheres on other planets and Titan.
The atmosphere of Titan includes an ionosphere that ranges from about 1100 km to 1300 km in altitude and contains carbon compounds.
Planets with ionospheres (incomplete list): Venus,
Uranus.
History.
As early as 1839, the German mathematician and physicist Carl Friedrich Gauss postulated that an electrically conducting region of the atmosphere could account for observed variations of Earth's magnetic field. Sixty years later, Guglielmo Marconi received the first trans-Atlantic radio signal on December 12, 1901, in St. John's, Newfoundland (now in Canada) using a 152.4 m kite-supported antenna for reception. The transmitting station in Poldhu, Cornwall, used a spark-gap transmitter to produce a signal with a frequency of approximately 500 kHz and a power of 100 times more than any radio signal previously produced. The message received was three dits, the Morse code for the letter S. To reach Newfoundland the signal would have to bounce off the ionosphere twice. Dr. Jack Belrose has contested this, however, based on theoretical and experimental work. However, Marconi did achieve transatlantic wireless communications in Glace Bay, Nova Scotia, one year later.
In 1902, Oliver Heaviside proposed the existence of the "Kennelly–Heaviside layer" of the ionosphere which bears his name. Heaviside's proposal included means by which radio signals are transmitted around the Earth's curvature. Heaviside's proposal, coupled with Planck's law of black body radiation, may have hampered the growth of radio astronomy for the detection of electromagnetic waves from celestial bodies until 1932 (and the development of high-frequency radio transceivers). Also in 1902, Arthur Edwin Kennelly discovered some of the ionosphere's radio-electrical properties.
In 1912, the U.S. Congress imposed the Radio Act of 1912 on amateur radio operators, limiting their operations to frequencies above 1.5 MHz (wavelength 200 meters or smaller). The government thought those frequencies were useless. This led to the discovery of HF radio propagation via the ionosphere in 1923.
In 1926, Scottish physicist Robert Watson-Watt introduced the term "ionosphere" in a letter published only in 1969 in "Nature":
 We have in quite recent years seen the universal adoption of the term 'stratosphere'..and..the companion term 'troposphere'... The term 'ionosphere', for the region in which the main characteristic is large scale ionisation with considerable mean free paths, appears appropriate as an addition to this series.
Edward V. Appleton was awarded a Nobel Prize in 1947 for his confirmation in 1927 of the existence of the ionosphere. Lloyd Berkner first measured the height and density of the ionosphere. This permitted the first complete theory of short-wave radio propagation. Maurice V. Wilkes and J. A. Ratcliffe researched the topic of radio propagation of very long radio waves in the ionosphere. Vitaly Ginzburg has developed a theory of electromagnetic wave propagation in plasmas such as the ionosphere.
In 1962, the Canadian satellite Alouette 1 was launched to study the ionosphere. Following its success were Alouette 2 in 1965 and the two ISIS satellites in 1969 and 1971, further AEROS-A and -B in 1972 and 1975, all for measuring the ionosphere.
External links.
</dl>

</doc>
<doc id="15100" url="http://en.wikipedia.org/wiki?curid=15100" title="Interlingua">
Interlingua

Interlingua (; ISO 639 language codes "ia", "ina") is an international auxiliary language (IAL), developed between 1937 and 1951 by the International Auxiliary Language Association (IALA). It ranks among the top most widely used IALs (along with Esperanto and Ido), and is the most widely used naturalistic IAL: in other words, its vocabulary, grammar and other characteristics are derived from natural languages. Interlingua was developed to combine a simple, mostly regular grammar with a vocabulary common to the widest possible range of languages, making it unusually easy to learn, at least for those whose native languages were sources of Interlingua's vocabulary and grammar. Conversely, it is used as a rapid introduction to many natural languages.
Interlingua literature maintains that (written) Interlingua is comprehensible to the hundreds of millions of people who speak a Romance language, though it is actively spoken by only a few hundred.
The name Interlingua comes from the Latin words "inter", meaning between, and "lingua", meaning tongue or language. These morphemes are identical in Interlingua. Thus, Interlingua would be "between language", or "intermediary language".
Rationale.
The expansive movements of science, technology, trade, diplomacy, and the arts, combined with the historical dominance of the Greek and Latin languages have resulted in a large common vocabulary among European languages. With Interlingua, an objective procedure is used to extract and standardize the most widespread word or words for a concept found in a set of "control languages": English, French, Italian, Spanish and Portuguese, with German and Russian as secondary references. Words from any language are eligible for inclusion, so long as their internationality is shown by their presence in these control languages. Hence, Interlingua includes such diverse word forms as Japanese "geisha" and "samurai", Arabic "califa", Guugu Yimithirr "gangurru" (Interlingua: kanguru), and Finnish "sauna".
Interlingua combines this pre-existing vocabulary with a minimal grammar based on the control languages. People with a good knowledge of a Romance language, or a smattering of a Romance language plus a good knowledge of the "international scientific vocabulary" can frequently understand it immediately on reading or hearing it. Educated speakers of English also enjoy this easy comprehension. The immediate comprehension of Interlingua, in turn, makes it unusually easy to learn. Speakers of other languages can also learn to speak and write Interlingua in a short time, thanks to its simple grammar and regular word formation using a small number of roots and affixes.
Once learned, Interlingua can be used to learn other related languages quickly and easily, and in some studies, even to understand them immediately. Research with Swedish students has shown that, after learning Interlingua, they can translate elementary texts from Italian, Portuguese, and Spanish. In one 1974 study, an Interlingua class translated a Spanish text that students who had taken 150 hours of Spanish found too difficult to understand. Gopsill has suggested that Interlingua's freedom from irregularities allowed the students to grasp the mechanisms of language quickly.
Words in Interlingua retain their original form from the source language; they are altered as little as possible to fit Interlingua's phonotactics. Each word retains its original spelling, pronunciation, and meanings. For this reason, Interlingua is frequently termed a "naturalistic" IAL.
When compared to natural languages, Interlingua most resembles Spanish.
History.
The American heiress Alice Vanderbilt Morris (1874–1950) became interested in linguistics and the international auxiliary language movement in the early 1920s, and in 1924, Morris and her husband, Dave Hennen Morris, established the non-profit International Auxiliary Language Association (IALA) in New York City. Their aim was to place the study of IALs on a scientific basis. Morris developed the research program of IALA in consultation with Edward Sapir, William Edward Collinson, and Otto Jespersen.
International Auxiliary Language Association.
The IALA became a major supporter of mainstream American linguistics, funding, for example, numerous studies by Sapir, Collinson, and Morris Swadesh in the 1930s and 1940s. Alice Morris edited several of these studies and provided much of IALA's financial support. IALA also received support from such prestigious groups as the Carnegie Corporation, the Ford Foundation, the Research Corporation, and the Rockefeller Foundation.
In its early years, IALA concerned itself with three tasks: finding other organizations around the world with similar goals; building a library of books about languages and interlinguistics; and comparing extant IALs, including Esperanto, Esperanto II, Ido, Peano’s Interlingua (Latino sine flexione), Novial, and Interlingue (Occidental). In pursuit of the last goal, it conducted parallel studies of these languages, with comparative studies of national languages, under the direction of scholars at American and European universities. It also arranged conferences with proponents of these IALs, who debated features and goals of their respective languages. With a "concession rule" that required participants to make a certain number of concessions, early debates at IALA sometimes grew from heated to explosive.
At the Second International Interlanguage Congress, held in Geneva in 1931, IALA began to break new ground; 27 recognized linguists signed a testimonial of support for IALA's research program. An additional eight added their signatures at the third congress, convened in Rome in 1933. That same year, Professor Herbert N. Shenton and Dr. Edward L. Thorndike became influential in IALA's work by authoring key studies in the interlinguistic field.
The first steps towards the finalization of Interlingua were taken in 1937, when a committee of 24 eminent linguists from 19 universities published "Some Criteria for an International Language and Commentary". However, the outbreak of World War II in 1939 cut short the intended biannual meetings of the committee.
Development of a new language.
Originally, the association had not set out to create its own language. Its goal was to identify which auxiliary language already available was best suited for international communication, and how to promote it most effectively. However, after ten years of research, more and more members of IALA concluded that none of the existing interlanguages were up to the task. By 1937, the members had made the decision to create a new language, to the surprise of the world's interlanguage community.
To that point, much of the debate had been equivocal on the decision to use naturalistic (e.g., Peano’s Interlingua, Novial and Occidental) or systematic (e.g., Esperanto and Ido) words. During the war years, proponents of a naturalistic interlanguage won out. The first support was Dr. Thorndike's paper; the second was a concession by proponents of the systematic languages that thousands of words were already present in many – or even a majority – of the European languages. Their argument was that systematic derivation of words was a Procrustian bed, forcing the learner to unlearn and re-memorize a new derivation scheme when a usable vocabulary was already available. This finally convinced supporters of the systematic languages, and IALA from that point assumed the position that a naturalistic language would be best.
At the outbreak of World War II, IALA's research activities were moved from Liverpool to New York, where E. Clark Stillman established a new research staff. Stillman, with the assistance of Dr. Alexander Gode, developed a "prototyping" technique – an objective methodology for selecting and standardizing vocabulary based on a comparison of "control languages".
In 1943 Stillman left for war work and Gode became Acting Director of Research. IALA began to develop models of the proposed language, the first of which were presented in Morris's "General Report" in 1945.
From 1946 to 1948, renowned French linguist André Martinet was Director of Research. During this period IALA continued to develop models and conducted polling to determine the optimal form of the final language. In 1946, IALA sent an extensive survey to more than 3,000 language teachers and related professionals on three continents.
Four models were canvassed:
The results of the survey were striking. The two more schematic models were rejected – K overwhelmingly. Of the two naturalistic models, M received somewhat more support than P. IALA decided on a compromise between P and M, with certain elements of C.
Martinet took up a position at Columbia University in 1948, and Gode took on the last phase of Interlingua's development. The vocabulary and grammar of Interlingua were first presented in 1951, when IALA published the finalized "" and the 27,000-word "Interlingua–English Dictionary" (IED). In 1954, IALA published an introductory manual entitled "Interlingua a Prime Vista" ("Interlingua at First Sight").
Interestingly, the Interlingua presented by the IALA is very close to Peano’s Interlingua (Latino sine flexione), both in its grammar and especially in its vocabulary. Accordingly, the very name "Interlingua" was kept, yet a distinct abbreviation was adopted: IA instead of IL.
Success, decline, and resurgence.
An early practical application of Interlingua was the scientific newsletter "Spectroscopia Molecular", published from 1952 to 1980. In 1954, Interlingua was used at the Second World Cardiological Congress in Washington, D.C. for both written summaries and oral interpretation. Within a few years, it found similar use at nine further medical congresses. Between the mid-1950s and the late 1970s, some thirty scientific and especially medical journals provided article summaries in Interlingua. Science Service, the publisher of "Science Newsletter" at the time, published a monthly column in Interlingua from the early 1950s until Gode's death in 1970. In 1967, the powerful International Organization for Standardization, which normalizes terminology, voted almost unanimously to adopt Interlingua as the basis for its dictionaries.
The IALA closed its doors in 1953 but was not formally dissolved until 1956 or later. Its role in promoting Interlingua was largely taken on by Science Service, which hired Gode as head of its newly formed Interlingua Division. Hugh E. Blair, Gode's close friend and colleague, became his assistant. A successor organization, the Interlingua Institute, was founded in 1970 to promote Interlingua in the US and Canada. The new institute supported the work of other linguistic organizations, made considerable scholarly contributions and produced Interlingua summaries for scholarly and medical publications. One of its largest achievements was two immense volumes on phytopathology produced by the American Phytopathological Society in 1976 and 1977.
Interlingua had attracted many former adherents of other international-language projects, notably Occidental and Ido. The former Occidentalist Ric Berger founded The Union Mundial pro Interlingua (UMI) in 1955, and by the late 1950s, interest in Interlingua in Europe had already begun to overtake that in North America.
Beginning in the 1980s UMI has held international conferences every two years (typical attendance at the earlier meetings was 50 to 100) and launched a publishing programme that eventually produced over 100 volumes. Other Interlingua-language works were published by university presses in Sweden and Italy, and in the 1990s, Brazil and Switzerland. Several Scandinavian schools undertook projects that used Interlingua as a means of teaching the international scientific and intellectual vocabulary.
In 2000, the Interlingua Institute was dissolved amid funding disputes with the UMI; the American Interlingua Society, established the following year, succeeded the institute and responded to new interest emerging in Mexico.
In the Soviet bloc.
Interlingua was spoken and promoted in the Soviet bloc, despite attempts to suppress the language. In East Germany, government officials confiscated the letters and magazines that the UMI sent to Walter Raédler, the Interlingua representative there.
In Czechoslovakia, Július Tomin published his first article on Interlingua in the Slovak magazine "Príroda a spoločnosť" (Nature and Society) in 1971, after which he received several anonymous threatening letters. He went on to become the Czech Interlingua representative, teach Interlingua in the school system, and publish a series of articles and books.
Interlingua today.
Today, interest in Interlingua has expanded from the scientific community to the general public. Individuals, governments, and private companies use Interlingua for learning and instruction, travel, online publishing, and communication across language barriers. Interlingua is promoted internationally by the Union Mundial pro Interlingua. Periodicals and books are produced by many national organizations, such as the Societate American pro Interlingua, the Svenska Sällskapet för Interlingua, and the Union Brazilian pro Interlingua.
Samples.
From an essay by Alexander Gode:
Community.
It is not certain how many people have an active knowledge of Interlingua. As noted above, Interlingua is the most widely spoken naturalistic auxiliary language.
Interlingua's greatest advantage is that it is the most widely "understood" international auxiliary language by virtue of its naturalistic (as opposed to schematic) grammar and vocabulary, allowing those familiar with a Romance language, and educated speakers of English, to read and understand it without prior study.
Interlingua has active speakers on all continents, especially in South America and in Eastern and Northern Europe, most notably Scandinavia; also in Russia and Ukraine. In Africa, Interlingua has official representation in the Republic of the Congo. There are copious Interlingua web pages, including editions of Wikipedia and Wiktionary, and a number of periodicals, including "Panorama in Interlingua" from the Union Mundial pro Interlingua (UMI) and magazines of the national societies allied with it. There are several active mailing lists, and Interlingua is also in use in certain Usenet newsgroups, particularly in the europa.* hierarchy. Interlingua is presented on CDs, radio, and television. In recent years, samples of Interlingua have also been seen in music and anime.
Interlingua is taught in many high schools and universities, sometimes as a means of teaching other languages quickly, presenting interlinguistics, or introducing the international vocabulary. The University of Granada in Spain, for example, offers an Interlingua course in collaboration with the Centro de Formación Continua.
Every two years, the UMI organizes an international conference in a different country. In the year between, the Scandinavian Interlingua societies co-organize a conference in Sweden. National organizations such as the Union Brazilian pro Interlingua also organize regular conferences.
Phonology and orthography.
Phonology.
Interlingua is primarily a written language, and the pronunciation is not entirely settled. The sounds in parentheses are not used by all speakers.
Interlingua alphabet.
Interlingua uses the 26 letters of the ISO basic Latin alphabet with no diacritics. The alphabet, pronunciation in IPA & letter name in Interlingua are:
Orthography and pronunciation.
Interlingua has a largely phonemic orthography. For the most part, consonants are pronounced as in English, while the vowels are like Spanish. Double consonants are pronounced as single. Interlingua has five falling diphthongs, /ai/, /au/, /ei/, /eu/, and /oi/, although /ei/ and /oi/ are rare.
Stress.
The "general rule" is that stress falls on the vowel before the last consonant (e.g., "lingua", 'language', "esser", 'to be', "requirimento", 'requirement'), and where that is not possible, on the first vowel ("via", 'way', "io crea", 'I create'). There are a few exceptions, and the following rules account for most of them:
Speakers may pronounce all words according to the general rule mentioned above. For example, "kilometro" is acceptable, although "kilometro" is more common.
Loanwords.
Unassimilated foreign loanwords, or borrowed words, are pronounced and spelled as in their language of origin. Their spelling may contain diacritics, or accent marks. If the diacritics do not affect pronunciation, they are removed.
Phonotactics.
Interlingua has no explicitly defined phonotactics. However, the prototyping procedure for determining Interlingua words, which strives for internationality, should in general lead naturally to words that are easy for most learners to pronounce. In the process of forming new words, an ending cannot always be added without a modification of some kind in between. A good example is the plural "-s", which is always preceded by a vowel to prevent the occurrence of a hard-to-pronounce consonant cluster at the end. If the singular does not end in a vowel, the final "-s" becomes "-es."
Vocabulary.
Words in Interlingua may be taken from any language, as long as their internationality is verified by their presence in seven "control" languages: Spanish, Portuguese, Italian, French, and English, with German and Russian acting as secondary controls. These are the most widely spoken Romance, Germanic, and Slavic languages, respectively. Because of their close relationship, Spanish and Portuguese are treated as one unit. The largest number of Interlingua words are of Latin origin, with the Greek and Germanic languages providing the second and third largest number. The remainder of the vocabulary originates in Slavic and non-Indo-European languages.
Eligibility.
A word, that is a form with meaning, is eligible for the Interlingua vocabulary if it is verified by at least three of the four primary control languages. Either secondary control language can substitute for a primary language. Any word of Indo-European origin found in a control language can contribute to the eligibility of an international word. In some cases, the archaic or "potential" presence of a word can contribute to its eligibility.
A word can be potentially present in a language when a derivative is present, but the word itself is not. English "proximity", for example, gives support to Interlingua "proxime", meaning 'near, close'. This counts as long as one or more control languages actually have this basic root word, which the Romance languages all do. Potentiality also occurs when a concept is represented as a compound or derivative in a control language, the morphemes that make it up are themselves international, and the combination adequately conveys the meaning of the larger word. An example is Italian "fiammifero" (lit. flamebearer), meaning "match, lucifer", which leads to Interlingua "flammifero", or "match". This word is thus said to be potentially present in the other languages although they may represent the meaning with a single morpheme.
Words do not enter the Interlingua vocabulary solely because cognates exist in a sufficient number of languages. If their meanings have become different over time, they are considered different words for the purpose of Interlingua eligibility. If they still have one or more meanings in common, however, the word can enter Interlingua with this smaller set of meanings.
If this procedure did not produce an international word, the word for a concept was originally taken from Latin (see below). This only occurred with a few grammatical particles.
Form.
The form of an Interlingua word is considered an "international prototype" with respect to the other words. On the one hand, it should be neutral, free from characteristics peculiar to one language. On the other hand, it should maximally capture the characteristics common to all contributing languages. As a result, it can be transformed into any of the contributing variants using only these language-specific characteristics. If the word has any derivatives that occur in the source languages with appropriate parallel meanings, then their morphological connection must remain intact; for example, the Interlingua word for 'time' is spelled "tempore" and not "*tempus" or "*tempo" in order to match it with its derived adjectives, such as "temporal".
The language-specific characteristics are closely related to the sound laws of the individual languages; the resulting words are often close or even identical to the most recent form common to the contributing words. This sometimes corresponds with that of Vulgar Latin. At other times, it is much more recent or even contemporary. It is never older than the classical period.
An illustration.
The French "œil", Italian "occhio", Spanish "ojo", and Portuguese "olho" appear quite different, but they descend from a historical form "oculus". German "Auge", Dutch "oog" and English "eye" (cf. Czech and Polish "oko", Ukrainian "око" "(óko)") are related to this form in that all three descend from Proto-Indo-European "*okʷ". In addition, international derivatives like "ocular" and "oculista" occur in all of Interlingua's control languages. Each of these forms contributes to the eligibility of the Interlingua word. German and English base words do not influence the form of the Interlingua word, because their Indo-European connection is considered too remote. Instead, the remaining base words and especially the derivatives determine the form "oculo" found in Interlingua.
Notes on Interlingua vocabulary.
New words can be derived internally – that is, from existing Interlingua words – or extracted from the control languages in the manner of the original vocabulary. Internal word-building, though freer than in the control languages, is more limited than in schematic languages.
Originally, a word was taken from Latin if the usual procedure did not produce a sufficiently international word. More recently, modern alternatives have become generally accepted. For example, the southern Romance "comprar", meaning 'to buy', has replaced "emer", because the latter occurs only in derivatives in the control languages. Similarly, the modern form "troppo", 'too' or 'too much', has replaced "nimis", and "ma" 'but' has largely replaced "sed".
Grammar.
Interlingua has been developed to omit any grammatical feature that is absent from even one control language. Thus, Interlingua has no noun–adjective agreement by gender, case, or number (cf. Spanish and Portuguese "gatas negras" or Italian "gatte nere", 'black female cats'), because this is absent from English, and it has no progressive verb tenses (English "I am reading"), because they are absent from French. Conversely, Interlingua distinguishes singular nouns from plural nouns because all the control languages do.
The definite article "le" is invariable, as in English. Nouns have no grammatical gender. Plurals are formed by adding "-s", or "-es" after a final consonant. Personal pronouns take one form for the subject and one for the direct object and reflexive. In the third person, the reflexive is always "se". Most adverbs are derived regularly from adjectives by adding "-mente", or "-amente" after a "-c". An adverb can be formed from any adjective in this way.
Verbs take the same form for all persons ("io, tu, illa vive", 'I live', 'you live', 'she lives'). The indicative ("pare", 'appear', 'appears') is the same as the imperative ("pare!" 'appear!'), and there is no subjunctive. Three common verbs usually take short forms in the present tense: "es" for 'is', 'am', 'are;' "ha" for 'has', 'have;' and "va" for 'go', 'goes'. A few irregular verb forms are available, but rarely used.
There are four simple tenses (present, past, future, and conditional), three compound tenses (past, future, and conditional), and the passive voice. The compound structures employ an auxiliary plus the infinitive or the past participle (e.g., "Ille ha arrivate", 'He has arrived'). Simple and compound tenses can be combined in various ways to express more complex tenses (e.g., "Nos haberea morite", 'We would have died').
Word order is subject–verb–object, except that a direct object pronoun or reflexive pronoun comes before the verb ("Io les vide", 'I see them'). Adjectives may precede or follow the nouns they modify, but they most often follow it. The position of adverbs is flexible, though constrained by common sense.
The grammar of Interlingua has been described as similar to that of the Romance languages, but greatly simplified, primarily under the influence of English. More recently, Interlingua's grammar has been likened to the simple grammars of Japanese and particularly Chinese.
Criticisms and controversies.
Some opponents argue that, being based on a few European languages, Interlingua is best suited for speakers of European languages. Others contend that Interlingua has spelling irregularities that, while internationally recognizable in written form, increase the time needed to fully learn the language, especially for those unfamiliar with Indo-European languages. A related point of criticism is that Interlingua's credential as being Standard Average European is too weak outside the Romance languages. Some opponents see the Germanic, Slavic, and Celtic languages, in particular, as having little influence.
Proponents argue that Interlingua's source languages include not only Romance languages but English, German, and Russian as well. Moreover, the source languages are widely spoken internationally, and large numbers of their words also appear in other languages – still more when derivative forms and loan translations are included. Tests had shown that if a larger number of source languages were used, the results would be about the same. So, IALA selected a much simpler extraction procedure for Interlingua with little adverse effect on its internationality.
Flags and symbols.
As with Esperanto, there have been proposals for a flag of Interlingua; the proposal by Czech translator Karel Podrazil is recognized by multilingual sites. It consists of a white four-pointed star extending to the edges of the flag and dividing it into an upper blue and lower red half. The star is symbolic of the four cardinal directions, and the two halves symbolize Romance and non-Romance speakers of Interlingua who understand each other.
Another symbol of Interlingua is a globe surrounded by twelve stars on a black or blue background, echoing the twelve stars of the Flag of Europe (because the source languages of Interlingua are purely European). Novial [ Wikipedia] marks Interlingua with the Flag of the Europe itself.

</doc>
