<doc id="25412" url="http://en.wikipedia.org/wiki?curid=25412" title="Rock and roll">
Rock and roll

Rock and roll (often written as rock & roll or rock 'n' roll) is a genre of popular music that originated and evolved in the United States during the late 1940s and early 1950s, primarily from a combination of predominantly African-American genres such as blues, boogie woogie, jump blues, jazz, and gospel music, together with Western swing and country music. Though elements of rock and roll can be heard in blues records from the 1920s and in country records of the 1930s, the genre did not acquire its name until the 1950s.
The term "rock and roll" now has at least two different meanings, both in common usage: referring to the first wave of music that originated in the US in the 1950s and would later develop into the more encompassing international style known as "rock music", and as a term simply synonymous with the rock music and culture in the broad sense. For the purpose of differentiation, this article deals with the first definition.
In the earliest rock and roll styles of the late 1940s and early 1950s, either the piano or saxophone was often the lead instrument, but these were generally replaced or supplemented by guitar in the middle to late 1950s. The beat is essentially a blues rhythm with an accentuated backbeat, the latter almost always provided by a snare drum. Classic rock and roll is usually played with one or two electric guitars (one lead, one rhythm), a string bass or (after the mid-1950s) an electric bass guitar, and a drum kit. Beyond simply a musical style, rock and roll, as seen in movies and on television, influenced lifestyles, fashion, attitudes, and language. In addition, rock and roll may have contributed to the civil rights movement because both African-American and white American teens enjoyed the music. It went on to spawn various genres, often without the initially characteristic backbeat, that are now more commonly called simply "rock music" or "rock".
Terminology.
The term "rock and roll" now has at least two different meanings, both in common usage. The "American Heritage Dictionary" and the "Merriam-Webster Dictionary" both define rock and roll as synonymous with rock music. "Encyclopædia Britannica", on the other hand, regards it as the music that originated in the mid-1950s and later developed "into the more encompassing international style known as rock music".
The phrase "rocking and rolling" originally described the movement of a ship on the ocean, but was used by the early twentieth century, both to describe the spiritual fervor of black church rituals and as a sexual analogy. Various gospel, blues and swing recordings used the phrase before it became used more frequently – but still intermittently – in the 1940s, on recordings and in reviews of what became known as "rhythm and blues" music aimed at a black audience.
In 1934, the song "Rock and Roll" by Boswell Sisters appeared in the film "Transatlantic Merry-Go-Round". In 1942, "Billboard" magazine columnist Maurie Orodenker started to use the term "rock-and-roll" to describe upbeat recordings such as "Rock Me" by Sister Rosetta Tharpe. By 1943, the "Rock and Roll Inn" in South Merchantville, New Jersey, was established as a music venue. In 1951, Cleveland, Ohio disc jockey Alan Freed began playing this music style while popularizing the phrase to describe it.
Origins.
The origins of rock and roll have been fiercely debated by commentators and historians of music. There is general agreement that it arose in the Southern United States – a region which would produce most of the major early rock and roll acts – through the meeting of various influences that embodied a merging of the African musical tradition with European instrumentation. The migration of many former slaves and their descendants to major urban centers such as Memphis, New York City, Detroit, Chicago, Cleveland and Buffalo (See: Second Great Migration (African American)) meant that black and white residents were living in close proximity in larger numbers than ever before, and as a result heard each other's music and even began to emulate each other's fashions. Radio stations that made white and black forms of music available to both groups, the development and spread of the gramophone record, and African American musical styles such as jazz and swing which were taken up by white musicians, aided this process of "cultural collision".
The immediate roots of rock and roll lay in the rhythm and blues, then called "race music", and country music of the 1940s and 1950s. Particularly significant influences were jazz, blues, gospel, country, and folk. Commentators differ in their views of which of these forms were most important and the degree to which the new music was a re-branding of African American rhythm and blues for a white market, or a new hybrid of black and white forms.
In the 1930s jazz, and particularly swing, both in urban based dance bands and blues-influenced country swing, was among the first music to present African American sounds for a predominantly white audience. The 1940s saw the increased use of blaring horns (including saxophones), shouted lyrics and boogie woogie beats in jazz based music. During and immediately after World War II, with shortages of fuel and limitations on audiences and available personnel, large jazz bands were less economical and tended to be replaced by smaller combos, using guitars, bass and drums. In the same period, particularly on the West Coast and in the Midwest, the development of jump blues, with its guitar riffs, prominent beats and shouted lyrics, prefigured many later developments. In the documentary film Hail! Hail! Rock 'n' Roll, Keith Richards proposes that Chuck Berry developed his brand of rock and roll, by transposing the familiar two-note lead line of jump blues piano directly to the electric guitar, creating what is instantly recognizable as rock guitar. Similarly, country boogie and Chicago electric blues supplied many of the elements that would be seen as characteristic of rock and roll.
Rock and roll arrived at a time of considerable technological change, soon after the development of the electric guitar, amplifier and microphone, and the 45 rpm record. There were also changes in the record industry, with the rise of independent labels like Atlantic, Sun and Chess servicing niche audiences and a similar rise of radio stations that played their music. It was the realization that relatively affluent white teenagers were listening to this music that led to the development of what was to be defined as rock and roll as a distinct genre.
Because the development of rock and roll was an evolutionary process, no single record can be identified as unambiguously "the first" rock and roll record. Contenders for the title of "first rock and roll record" include Goree Carter's "" (1949); Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952; and "Rocket 88" by Jackie Brenston and his Delta Cats (backed by Ike Turner and his band The Kings of Rhythm), recorded by Sam Phillips for Sun Records in March 1951. In terms of its wide cultural impact across society in the US and elsewhere, Bill Haley's "Rock Around the Clock", recorded in April 1954 but not a commercial success until the following year, is generally recognized as an important milestone, but it was preceded by many recordings from earlier decades in which elements of rock and roll can be clearly discerned. Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent. Chuck Berry's 1955 classic "Maybellene" in particular features a distorted electric guitar solo with warm overtones created by his small valve amplifier. However, the use of distortion was predated by Guitar Slim, Willie Johnson of Howlin' Wolf's band, and Pat Hare; the latter two also made use of distorted power chords in the early 1950s. In addition, Bo Diddley introduced a new beat and unique electric guitar style, heavily influenced by African music and in turn influencing many later artists.
Early rock and roll.
Rockabilly.
"Rockabilly" usually (but not exclusively) refers to the type of rock and roll music which was played and recorded in the mid-1950s primarily by white singers such as Elvis Presley, Carl Perkins, Johnny Cash, and Jerry Lee Lewis, who drew mainly on the country roots of the music. Many other popular rock and roll singers of the time, such as Fats Domino and Little Richard, came out of the black rhythm and blues tradition, making the music attractive to white audiences, and are not usually classed as "rockabilly".
In July 1954, Elvis Presley recorded the regional hit "That's All Right" at Sam Phillips' Sun Studio in Memphis. Three months earlier, on April 12, 1954, Bill Haley & His Comets recorded "Rock Around the Clock". Although only a minor hit when first released, when used in the opening sequence of the movie "Blackboard Jungle" a year later, it set the rock and roll boom in motion. The song became one of the biggest hits in history, and frenzied teens flocked to see Haley and the Comets perform it, causing riots in some cities. "Rock Around the Clock" was a breakthrough for both the group and for all of rock and roll music. If everything that came before laid the groundwork, "Rock Around the Clock" introduced the music to a global audience.
In 1956, the arrival of rockabilly was underlined by the success of songs like "Folsom Prison Blues" by Johnny Cash, "Blue Suede Shoes" by Perkins and "Heartbreak Hotel" by Presley. For a few years it became the most commercially successful form of rock and roll. Later rockabilly acts, particularly performing songwriters like Buddy Holly, would be a major influence on British Invasion acts and particularly on the song writing of The Beatles and through them on the nature of later rock music.
Doo wop.
Doo wop was one of the most popular forms of 1950s rhythm and blues, often compared with rock and roll, with an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation. Its origins were in African American vocal groups of the 1930s and 40s, like the Ink Spots and the Mills Brothers, who had enjoyed considerable commercial success with arrangements based on close harmonies. They were followed by 1940s R&B vocal acts like The Orioles, The Ravens and The Clovers, who injected a strong element of traditional gospel and, increasingly, the energy of jump blues. By 1954, as rock and roll was beginning to emerge, a number of similar acts began to cross over from the R&B charts to mainstream success, often with added honking brass and saxophone, with The Crows, The Penguins, The El Dorados and The Turbans all scoring major hits. Despite the subsequent explosion in records from doo wop acts in the later 50s, many failed to chart or were one-hit wonders. Exceptions included The Platters, with songs including "The Great Pretender" (1955) and The Coasters with humorous songs like "Yakety Yak" (1958), both of which ranked among the most successful rock and roll acts of the era. Towards the end of the decade there were increasing numbers of white, particularly Italian American, singers taking up Doo Wop, creating all-white groups like The Mystics and Dion and the Belmonts and racially integrated groups like The Dell Vikings and The Impalas. Doo wop would be a major influence on vocal surf music, soul and early Merseybeat, including The Beatles.
Cover versions.
Many of the earliest white rock and roll hits were covers or partial re-writes of earlier black rhythm and blues or blues songs. Through the late 1940s and early 1950s, R&B music had been gaining a stronger beat and a wilder style, with artists such as Fats Domino and Johnny Otis speeding up the tempos and increasing the backbeat to great popularity on the juke joint circuit. Before the efforts of Freed and others, black music was taboo on many white-owned radio outlets, but artists and producers quickly recognized the potential of rock and roll. Most of Presley's early hits were covers of black rhythm and blues or blues songs, like "That's All Right" (a countrified arrangement of a blues number), "Baby Let's Play House", "Lawdy Miss Clawdy" and "Hound Dog".
Covers were customary in the music industry at the time; it was made particularly easy by the compulsory license provision of United States copyright law (still in effect). One of the first relevant successful covers was Wynonie Harris's transformation of Roy Brown's 1947 original jump blues hit "Good Rocking Tonight" into a more showy rocker and the Louis Prima rocker "Oh Babe" in 1950, as well as Amos Milburn's cover of what may have been the first white rock and roll record, Hardrock Gunter's "Birmingham Bounce" in 1949. The most notable trend, however, was white pop covers of black R&B numbers. The more familiar sound of these covers may have been more palatable to white audiences, there may have been an element of prejudice, but labels aimed at the white market also had much better distribution networks and were generally much more profitable. Famously, Pat Boone recorded sanitized versions of Little Richard songs. Later, as those songs became popular, the original artists' recordings received radio play as well.
The cover versions were not necessarily straightforward imitations. For example, Bill Haley's incompletely bowdlerized cover of "Shake, Rattle and Roll" transformed Big Joe Turner's humorous and racy tale of adult love into an energetic teen dance number, while Georgia Gibbs replaced Etta James's tough, sarcastic vocal in "Roll With Me, Henry" (covered as "Dance With Me, Henry") with a perkier vocal more appropriate for an audience unfamiliar with the song to which James's song was an answer, Hank Ballard's "Work With Me, Annie". Elvis' rock and roll version of "Hound Dog", taken mainly from a version recorded by pop band Freddie Bell and the Bellboys, was very different from the blues shouter that Big Mama Thornton had recorded four years earlier.
Decline.
Some commentators have suggested a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, The Big Bopper and Ritchie Valens in a plane crash (February 1959), the departure of Elvis for the army (March 1958), the retirement of Little Richard to become a preacher (October 1957), the scandal surrounding Jerry Lee Lewis' marriage to his thirteen-year-old cousin (May 1958), the arrest of Chuck Berry (December 1959), and the breaking of the Payola scandal implicating major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs (November 1959), gave a sense that the initial phase of rock and roll had come to an end.
There was also a process that has been described as the "feminisation" of rock and roll, with the charts beginning to be dominated by love ballads, often aimed at a female audience, and the rise of girl groups such as The Shirelles and The Crystals. Some music historians have pointed to important and innovative developments that built on rock and roll in this period, including multitrack recording, developed by Les Paul, the electronic treatment of sound by such innovators as Joe Meek, and the 'Wall of Sound' productions of Phil Spector, continued desegregation of the charts, the rise of surf music, garage rock and the Twist dance craze. Surf rock in particular, noted for the use of reverb-drenched guitars, became one of the most popular forms of American rock of the 60s.
British rock and roll.
In the 1950s, Britain was well placed to receive American rock and roll music and culture. It shared a common language, had been exposed to American culture through the stationing of troops in the country, and shared many social developments, including the emergence of distinct youth sub-cultures, which in Britain included the Teddy Boys and the rockers. Trad Jazz became popular, and many of its musicians were influenced by related American styles, including boogie woogie and the blues. The skiffle craze, led by Lonnie Donegan, utilised amateurish versions of American folk songs and encouraged many of the subsequent generation of rock and roll, folk, R&B and beat musicians to start performing. At the same time British audiences were beginning to encounter American rock and roll, initially through films including "Blackboard Jungle" (1955) and "Rock Around the Clock" (1955). Both movies contained the Bill Haley & His Comets hit "Rock Around the Clock", which first entered the British charts in early 1955 – four months before it reached the US pop charts – topped the British charts later that year and again in 1956, and helped identify rock and roll with teenage delinquency. American rock and roll acts such as Elvis Presley, Little Richard, Buddy Holly, Chuck Berry and Carl Perkins thereafter became major forces in the British charts.
The initial response of the British music industry was to attempt to produce copies of American records, recorded with session musicians and often fronted by teen idols. More grassroots British rock and rollers soon began to appear, including Wee Willie Harris and Tommy Steele. During this period American Rock and Roll remained dominant; however, in 1958 Britain produced its first "authentic" rock and roll song and star, when Cliff Richard reached number 2 in the charts with "Move It". At the same time, TV shows such as "Six-Five Special" and "Oh Boy!" promoted the careers of British rock and rollers like Marty Wilde and Adam Faith. Cliff Richard and his backing band, The Shadows, were the most successful home grown rock and roll based acts of the era. Other leading acts included Billy Fury, Joe Brown, and Johnny Kidd & The Pirates, whose 1960 hit song "Shakin' All Over" became a rock and roll standard.
As interest in rock and roll was beginning to subside in America in the late 1950s and early 1960s, it was taken up by groups in major British urban centres like Liverpool, Manchester, Birmingham, and London. About the same time, a British blues scene developed, initially led by purist blues followers such as Alexis Korner and Cyril Davies who were directly inspired by American musicians such as Robert Johnson, Muddy Waters and Howlin' Wolf. Many groups moved towards the beat music of rock and roll and rhythm and blues from skiffle, like the Quarrymen who became The Beatles, producing a form of rock and roll revivalism that carried them and many other groups to national success from about 1963 and to international success from 1964, known in America as the British Invasion. Groups that followed The Beatles included the beat-influenced Freddie and the Dreamers, The Searchers, Wayne Fontana and the Mindbenders, Herman's Hermits and The Dave Clark Five, and more directly blues-influenced groups including The Animals, The Kinks, The Rolling Stones, The Who and The Yardbirds. As the blues became an increasingly significant influence, leading to the creation of the blues rock of groups like The Moody Blues, Small Faces, The Move, Traffic and Cream, and developing into rock music, the influence of early rock and roll began to subside.
Cultural impact.
Rock and roll influenced lifestyles, fashion, attitudes, and language. In addition, rock and roll may have contributed to the civil rights movement because both African-American and white American teens enjoyed the music.
Many early rock and roll songs dealt with issues of cars, school, dating, and clothing. The lyrics of rock and roll songs described events and conflicts that most listeners could relate to through personal experience. Topics such as sex that had generally been considered taboo began to appear in rock and roll lyrics. This new music tried to break boundaries and express emotions that people were actually feeling but had not talked about. An awakening began to take place in American youth culture .
Race.
In the crossover of African American "race music" to a growing white youth audience, the popularization of rock and roll involved both black performers reaching a white audience and white performers appropriating African American music. Rock and roll appeared at a time when racial tensions in the United States were entering a new phase, with the beginnings of the civil rights movement for desegregation, leading to the Supreme Court ruling that abolished the policy of "separate but equal" in 1954, but leaving a policy which would be extremely difficult to enforce in parts of the United States. The coming together of white youth audiences and black music in rock and roll inevitably provoked strong white racist reactions within the US, with many whites condemning its breaking down of barriers based on color. Many observers saw rock and roll as heralding the way for desegregation, in creating a new form of music that encouraged racial cooperation and shared experience. Many authors have argued that early rock and roll was instrumental in the way both white and black teenagers identified themselves.
Teen culture.
Several rock historians have claimed that rock and roll was one of the first music genres to define an age group. It gave teenagers a sense of belonging, even when they were alone. Rock and roll is often identified with the emergence of teen culture among the first baby boomer generation, who had both greater relative affluence, leisure and who adopted rock and roll as part of a distinct sub-culture. This involved not just music, absorbed via radio, record buying, jukeboxes and TV programs like "American Bandstand", but it also extended to film, clothes, hair, cars and motorbikes, and distinctive language. The contrast between parental and youth culture exemplified by rock and roll was a recurring source of concern for older generations, who worried about juvenile delinquency and social rebellion, particularly as to a large extent rock and roll culture was shared by different racial and social groups.
In America, that concern was conveyed even in youth cultural artifacts such as comic books. In "There's No Romance in Rock and Roll" from "True Life Romance" (1956), a defiant teen dates a rock and roll-loving boy but drops him for one who likes traditional adult music— to her parents' relief. In Britain, where post-war prosperity was more limited, rock and roll culture became attached to the pre-existing Teddy Boy movement, largely working class in origins, and eventually to the longer lasting rockers. Rock and roll has been seen as reorienting popular music towards a teen market, often celebrating teen fashions, as in Carl Perkins' "Blue Suede Shoes" (1956) or Dion and the Belmonts' "A Teenager in Love" (1960).
Dance styles.
From its early 1950s beginning through the early 1960s, rock and roll music spawned new dance crazes. Teenagers found the syncopated backbeat rhythm especially suited to reviving Big Band era jitterbug dancing. "Sock hops", gym dances, and home basement dance parties became the rage, and American teens watched Dick Clark's "American Bandstand" to keep up on the latest dance and fashion styles. From the mid-1960s on, as "rock and roll" yielded gradually to "rock", later dance genres followed, starting with the twist, and leading up to funk, disco, house, techno, and hip hop.
References.
</dl>

</doc>
<doc id="25414" url="http://en.wikipedia.org/wiki?curid=25414" title="Religion">
Religion

A religion is an organized collection of beliefs, cultural systems, and world views that relate humanity to an order of existence. Many religions have narratives, symbols, and sacred histories that aim to explain the meaning of life, the origin of life, or the Universe. From their beliefs about the cosmos and human nature, people may derive morality, ethics, religious laws or a preferred lifestyle.
Many religions may have organized behaviors, clergy, a definition of what constitutes adherence or membership, holy places, and scriptures. The practice of a religion may include rituals, sermons, commemoration or veneration (of a deity, gods, or goddesses), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, music, art, dance, public service, or other aspects of human culture. Religions may also contain mythology.
The word "religion" is sometimes used interchangeably with "faith" or "set of duties"; however, in the words of Émile Durkheim, religion differs from private belief in that it is "something eminently social". A global 2012 poll reports 59% of the world's population as "religious" and 36% as not religious, including 13% who are atheists, with a 9% decrease in religious belief from 2005. On average, women are "more religious" than men. Some people follow multiple religions or multiple religious principles at the same time, regardless of whether or not the religious principles they follow traditionally allow for syncretism.
Etymology.
"Religion" (from O.Fr. "religion" "religious community," from L. "religionem" (nom. "religio") "respect for what is sacred, reverence for the gods," "obligation, the bond between man and the gods") is derived from the Latin "religiō", the ultimate origins of which are obscure. One possibility is an interpretation traced to Cicero, connecting "lego" "read", i.e. "re" (again) + "lego" in the sense of "choose", "go over again" or "consider carefully". Modern scholars such as Tom Harpur and Joseph Campbell favor the derivation from "ligare" "bind, connect", probably from a prefixed "re-ligare", i.e. "re" (again) + "ligare" or "to reconnect," which was made prominent by St. Augustine, following the interpretation of Lactantius. The medieval usage alternates with "order" in designating bonded communities like those of monastic orders: "we hear of the 'religion' of the Golden Fleece, of a knight 'of the religion of Avys'".
According to the philologist Max Müller, the root of the English word "religion", the Latin "religio", was originally used to mean only "reverence for God or the gods, careful pondering of divine things, piety" (which Cicero further derived to mean "diligence"). Max Müller characterized many other cultures around the world, including Egypt, Persia, and India, as having a similar power structure at this point in history. What is called ancient religion today, they would have only called "law".
Many languages have words that can be translated as "religion", but they may use them in a very different way, and some have no word for religion at all. For example, the Sanskrit word dharma, sometimes translated as "religion", also means law. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between "imperial law" and universal or "Buddha law", but these later became independent sources of power.
There is no precise equivalent of "religion" in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. One of its central concepts is "halakha", sometimes translated as "law"", which guides religious practice and belief and many aspects of daily life.
The use of other terms, such as obedience to God or Islam are likewise grounded in particular histories and vocabularies.
Definitions.
There are numerous definitions of religion and only a few are stated here. The typical dictionary definition of religion refers to a "belief in, or the worship of, a god or gods" or the "service and worship of God or the supernatural". However, writers and scholars have expanded upon the "belief in god" definitions as insufficient to capture the diversity of religious thought and experience.
Peter Mandaville and Paul James define religion as "a relatively-bounded system of beliefs, symbols and practices that addresses the nature of existence, and in which communion with others and Otherness is "lived" as if it both takes in and spiritually transcends socially-grounded ontologies of time, space, embodiment and knowing". This definition is intended, they write, to get away from the modernist dualisms or dichotomous understandings of immanence/transcendence, spirituality/materialism, and sacredness/secularity.
Edward Burnett Tylor defined religion as "the belief in spiritual beings". He argued, back in 1871, that narrowing the definition to mean the belief in a supreme deity or judgment after death or idolatry and so on, would exclude many peoples from the category of religious, and thus "has the fault of identifying religion rather with particular developments than with the deeper motive which underlies them". He also argued that the belief in spiritual beings exists in all known societies.
The anthropologist Clifford Geertz defined religion as a "system of symbols which acts to establish powerful, pervasive, and long-lasting moods and motivations in men by formulating conceptions of a general order of existence and clothing these conceptions with such an aura of factuality that the moods and motivations seem uniquely realistic." Alluding perhaps to Tylor's "deeper motive", Geertz remarked that "we have very little idea of how, in empirical terms, this particular miracle is accomplished. We just know that it is done, annually, weekly, daily, for some people almost hourly; and we have an enormous ethnographic literature to demonstrate it". The theologian Antoine Vergote also emphasized the "cultural reality" of religion, which he defined as "the entirety of the linguistic expressions, emotions and, actions and signs that refer to a supernatural being or supernatural beings"; he took the term "supernatural" simply to mean whatever transcends the powers of nature or human agency.
The sociologist Durkheim, in his seminal book The Elementary Forms of the Religious Life, defined religion as a "unified system of beliefs and practices relative to sacred things". By sacred things he meant things "set apart and forbidden—beliefs and practices which unite into one single moral community called a Church, all those who adhere to them". Sacred things are not, however, limited to gods or spirits. On the contrary, a sacred thing can be "a rock, a tree, a spring, a pebble, a piece of wood, a house, in a word, anything can be sacred". Religious beliefs, myths, dogmas and legends are the representations that express the nature of these sacred things, and the virtues and powers which are attributed to them.
In his book The Varieties of Religious Experience, the psychologist William James defined religion as "the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine". By the term "divine" James meant "any object that is god"like", whether it be a concrete deity or not" to which the individual feels impelled to respond with solemnity and gravity.
Echoes of James' and Durkheim's definitions are to be found in the writings of, for example, Frederick Ferré who defined religion as "one's way of valuing most comprehensively and intensively". Similarly, for the theologian Paul Tillich, faith is "the state of being ultimately concerned", which "is itself religion. Religion is the substance, the ground, and the depth of man's spiritual life." Friedrich Schleiermacher in the late 18th century defined religion as "das schlechthinnige Abhängigkeitsgefühl", commonly translated as "a feeling of absolute dependence". His contemporary Hegel disagreed thoroughly, defining religion as "the Divine Spirit becoming conscious of Himself through the finite spirit."
When religion is seen in terms of "sacred", "divine", intensive "valuing", or "ultimate concern", then it is possible to understand why scientific findings and philosophical criticisms (e.g. Richard Dawkins) do not necessarily disturb its adherents.
An increasing number of scholars have expressed reservations about ever defining the "essence" of religion. They observe that the way we use the concept today is a particularly modern construct that would not have been understood through much of history and in many cultures outside the West (or even in the West until after the Peace of Westphalia).
Theories.
Origins and development.
The origin of religion is uncertain. There are a number of theories regarding the subsequent origins of organized religious practices.
According to anthropologists John Monaghan and Peter Just, "Many of the great world religions appear to have begun as revitalization movements of some sort, as the vision of a charismatic prophet fires the imaginations of people seeking a more comprehensive answer to their problems than they feel is provided by everyday beliefs. Charismatic individuals have emerged at many times and places in the world. It seems that the key to long-term success – and many movements come and go with little long-term effect – has relatively little to do with the prophets, who appear with surprising regularity, but more to do with the development of a group of supporters who are able to institutionalize the movement."
The development of religion has taken different forms in different cultures. Some religions place an emphasis on belief, while others emphasize practice. Some religions focus on the subjective experience of the religious individual, while others consider the activities of the religious community to be most important. Some religions claim to be universal, believing their laws and cosmology to be binding for everyone, while others are intended to be practiced only by a closely defined or localized group. In many places religion has been associated with public institutions such as education, hospitals, the family, government, and political hierarchies.
Anthropologists John Monoghan and Peter Just state that, "it seems apparent that one thing religion or belief helps us do is deal with problems of human life that are significant, persistent, and intolerable. One important way in which religious beliefs accomplish this is by providing a set of ideas about how and why the world is put together that allows people to accommodate anxieties and deal with misfortune."
Social constructionism.
One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings. Among the main proponents of this theory of religion are Daniel Dubuisson, Timothy Fitzgerald, Talal Asad, and Jason Ānanda Josephson. The social constructionists argue that religion is a modern concept that developed from Christianity and was then applied inappropriately to non-Western cultures.
Daniel Dubuisson, a French anthropologist, says that the idea of religion has changed a lot over time and that one cannot fully understand its development by relying on consistent use of the term, which "tends to minimize or cancel out the role of history". "What the West and the history of religions in its wake have objectified under the name 'religion'", he says, " is ... something quite unique, which could be appropriate only to itself and its own history." He notes that St. Augustine's definition of "religio" differed from the way we used the modern word "religion".
Dubuisson prefers the term "cosmographic formation" to religion. Dubuisson says that, with the emergence of religion as a category separate from culture and society, there arose religious studies. The initial purpose of religious studies was to demonstrate the superiority of the "living" or "universal" European world view to the "dead" or "ethnic" religions scattered throughout the rest of the world, expanding the teleological project of Schleiermacher and Tiele to a worldwide ideal religiousness. Due to shifting theological currents, this was eventually supplanted by a liberal-ecumenical interest in searching for Western-style universal truths in every cultural tradition.
According to Fitzgerald, religion is not a universal feature of all cultures, but rather a particular idea that first developed in Europe under the influence of Christianity. Fitzgerald argues that from about the 4th century CE Western Europe and the rest of the world diverged. As Christianity became commonplace, the charismatic authority identified by Augustine, a quality we might today call "religiousness", exerted a commanding influence at the local level. As the Church lost its dominance during the Protestant Reformation and Christianity became closely tied to political structures, religion was recast as the basis of national sovereignty, and religious identity gradually became a less universal sense of spirituality and more divisive, locally defined, and tied to nationality. It was at this point that "religion" was dissociated with universal beliefs and moved closer to dogma in both meaning and practice. However there was not yet the idea of dogma as a personal choice, only of established churches. With the Enlightenment religion lost its attachment to nationality, says Fitzgerald, but rather than becoming a universal social attitude, it now became a personal feeling or emotion.
Asad argues that before the word "religion" came into common usage, Christianity was a "disciplina", a "rule" just like that of the Roman Empire. This idea can be found in the writings of St. Augustine (354–430). Christianity was then a power structure opposing and superseding human institutions, a literal Kingdom of Heaven. It was the discipline taught by one's family, school, church, and city authorities, rather than something calling one to self-discipline through symbols.
These ideas are developed by S. N. Balagangadhara. In the Age of Enlightenment, Balagangadhara says that the idea of Christianity as the purest expression of spirituality was supplanted by the concept of "religion" as a worldwide practice. This caused such ideas as religious freedom, a reexamination of classical philosophy as an alternative to Christian thought, and more radically Deism among intellectuals such as Voltaire. Much like Christianity, the idea of "religious freedom" was exported around the world as a civilizing technique, even to regions such as India that had never treated spirituality as a matter of political identity.
More recently, in "The Invention of Religion in Japan", Josephson has argued that while the concept of "religion" was Christian in its early formulation, non-Europeans (such as the Japanese) did not just acquiesce and passively accept the term's meaning. Instead they worked to interpret "religion" (and its boundaries) strategically to meet their own agendas and staged these new meanings for a global audience. In nineteenth century Japan, Buddhism was radically transformed from a pre-modern philosophy of natural law into a "religion," as Japanese leaders worked to address domestic and international political concerns. In summary, Josephson argues that the European encounter with other cultures has led to a partial de-Christianization of the category religion. Hence "religion" has come to refer to a confused collection of traditions with no possible coherent definition.
George Lindbeck, a Lutheran and a postliberal theologian (but not a social constructionist), says that religion does not refer to belief in "God" or a transcendent Absolute, but rather to "a kind of cultural and/or linguistic framework or medium that shapes the entirety of life and thought ... it is similar to an idiom that makes possible the description of realities, the formulation of beliefs, and the experiencing of inner attitudes, feelings, and sentiments."
Comparative religion.
Nicholas de Lange, Professor of Hebrew and Jewish Studies at Cambridge University, says that "The comparative study of religions is an academic discipline which has been developed within Christian theology faculties, and it has a tendency to force widely differing phenomena into a kind of strait-jacket cut to a Christian pattern. The problem is not only that other 'religions' may have little or nothing to say about questions which are of burning importance for Christianity, but that they may not even see themselves as religions in precisely the same way in which Christianity sees itself as a religion."
Types.
Categories.
Some scholars classify religions as either "universal religions" that seek worldwide acceptance and actively look for new converts, or "ethnic religions" that are identified with a particular ethnic group and do not seek converts. Others reject the distinction, pointing out that all religious practices, whatever their philosophical origin, are ethnic because they come from a particular culture.
In the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called "world religions." However, some recent scholarship has argued that not all types of religion are necessarily separated by mutually exclusive philosophies, and furthermore that the utility of ascribing a practice to a certain philosophy, or even calling a given practice religious, rather than cultural, political, or social in nature, is limited. The current state of psychological study about the nature of religiousness suggests that it is better to refer to religion as a largely invariant phenomenon that should be distinguished from cultural norms (i.e. "religions").
Some academics studying the subject have divided religions into three broad categories:
Interfaith cooperation.
Because religion continues to be recognized in Western thought as a universal impulse, many religious practitioners have aimed to band together in interfaith dialogue, cooperation, and religious peacebuilding. The first major dialogue was the Parliament of the World's Religions at the 1893 Chicago World's Fair, which remains notable even today both in affirming "universal values" and recognition of the diversity of practices among different cultures. The 20th century has been especially fruitful in use of interfaith dialogue as a means of solving ethnic, political, or even religious conflict, with Christian–Jewish reconciliation representing a complete reverse in the attitudes of many Christian communities towards Jews.
Recent interfaith initiatives include "A Common Word", launched in 2007 and focused on bringing Muslim and Christian leaders together, the "C1 World Dialogue", the "Common Ground" initiative between Islam and Buddhism, and a United Nations sponsored "World Interfaith Harmony Week".
Religious groups.
The list of still-active religious movements given here is an attempt to summarize the most important regional and philosophical influences on local communities, but it is by no means a complete description of every religious community, nor does it explain the most important elements of individual religiousness.
The five largest religious groups by world population, estimated to account for 5.8 billion people and 84% of the population, are Christianity, Islam, Buddhism, Hinduism (with the relative numbers for Buddhism and Hinduism dependent on the extent of syncretism) and traditional folk religion.
Abrahamic.
Abrahamic religions are monotheistic religions which believe they descend from Abraham.
Judaism is the oldest Abrahamic religion, originating in the people of ancient Israel and Judea. Judaism is based primarily on the Torah, a text which some Jews believe was handed down to the people of Israel through the prophet Moses. This along with the rest of the Hebrew Bible and the Talmud are the central texts of Judaism. The Jewish people were scattered after the destruction of the Temple in Jerusalem in 70 CE. Today there are about 13 million Jews, about 40 per cent living in Israel and 40 per cent in the United States.
Christianity is based on the life and teachings of Jesus of Nazareth (1st century) as presented in the New Testament. The Christian faith is essentially faith in Jesus as the Christ, the Son of God, and as Savior and Lord. Almost all Christians believe in the Trinity, which teaches the unity of Father, Son (Jesus Christ), and Holy Spirit as three persons in one Godhead. Most Christians can describe their faith with the Nicene Creed. As the religion of Byzantine Empire in the first millennium and of Western Europe during the time of colonization, Christianity has been propagated throughout the world. The main divisions of Christianity are, according to the number of adherents:
There are also smaller groups, including:
Islam is based on the Quran, one of the holy books considered by Muslims to be revealed by God, and on the teachings (hadith) of the Islamic prophet Muhammad, a major political and religious figure of the 7th century CE. Islam is the most widely practiced religion of Southeast Asia, North Africa, Western Asia, and Central Asia, while Muslim-majority countries also exist in parts of South Asia, Sub-Saharan Africa, and Southeast Europe. There are also several Islamic republics, including Iran, Pakistan, Mauritania, and Afghanistan.
Other denominations of Islam include Nation of Islam, Ibadi, Sufism, Quranism, Mahdavia, and non-denominational Muslims. Wahhabism is the dominant Muslim schools of thought in the Kingdom of Saudi Arabia.
The Bahá'í Faith is an Abrahamic religion founded in 19th century Iran and since then has spread worldwide. It teaches unity of all religious philosophies and accepts all of the prophets of Judaism, Christianity, and Islam as well as additional prophets including its founder Bahá'u'lláh.
Smaller regional Abrahamic groups also exist, including Samaritanism (primarily in Israel and the West Bank), the Rastafari movement (primarily in Jamaica), and Druze (primarily in Syria and Lebanon).
Iranian.
Iranian religions are ancient religions whose roots predate the Islamization of Greater Iran. Nowadays these religions are practiced only by minorities.
Zoroastrianism is based on the teachings of prophet Zoroaster in the 6th century BC. Zoroastrians worship the creator Ahura Mazda. In Zoroastrianism good and evil have distinct sources, with evil trying to destroy the creation of Mazda, and good trying to sustain it.
Mandaeism is a monotheistic religion with a strongly dualistic worldview. Mandaeans are sometime labeled as the "Last Gnostics".
Kurdish religions include the traditional beliefs of the Yazidi, Alevi, and Ahl-e Haqq. Sometimes these are labeled Yazdânism.
Indian.
Indian religions are practiced or were founded in the Indian subcontinent. They are sometimes classified as the "dharmic religions", as they all feature dharma, the specific law of reality and duties expected according to the religion.
Hinduism is a synecdoche describing the similar philosophies of Vaishnavism, Shaivism, and related groups practiced or founded in the Indian subcontinent. Concepts most of them share in common include karma, caste, reincarnation, mantras, yantras, and darśana. Hinduism is the most ancient of still-active religions, with origins perhaps as far back as prehistoric times. Hinduism is not a monolithic religion but a religious category containing dozens of separate philosophies amalgamated as Sanātana Dharma, which is the name by which Hinduism has been known throughout history by its followers.
Jainism, taught primarily by Parsva (9th century BCE) and Mahavira (6th century BCE), is an ancient Indian religion that prescribes a path of non-violence for all forms of living beings in this world. Jains are found mostly in India.
Buddhism was founded by Siddhattha Gotama in the 6th century BCE. Buddhists generally agree that Gotama aimed to help sentient beings end their suffering (dukkha) by understanding the true nature of phenomena, thereby escaping the cycle of suffering and rebirth (saṃsāra), that is, achieving nirvana.
Sikhism is a monotheistic religion founded on the teachings of Guru Nanak and ten successive Sikh gurus in 15th century Punjab. It is the fifth-largest organized religion in the world, with approximately 30 million Sikhs. Sikhs are expected to embody the qualities of a "Sant-Sipāhī"—a saint-soldier, have control over one's internal vices and be able to be constantly immersed in virtues clarified in the Guru Granth Sahib. The principal beliefs of Sikhi are faith in "Waheguru"—represented by the phrase "ik ōaṅkār", meaning one God, who prevails in everything, along with a praxis in which the Sikh is enjoined to engage in social reform through the pursuit of justice for all human beings.
East Asian religions.
East Asian religions (also known as Far Eastern religions or Taoic religions) consist of several religions of East Asia which make use of the concept of Tao (in Chinese) or Dō (in Japanese or Korean). They include:
African traditional.
African traditional religion encompasses the traditional religious beliefs of people in Africa. In north Africa, these religions have included traditional Berber religion, ancient Egyptian religion, and Waaq. West African religions include Akan religion, Dahomey (Fon) mythology, Efik mythology, Odinani of the Igbo people, Serer religion, and Yoruba religion, while Bushongo mythology, Mbuti (Pygmy) mythology, Lugbara mythology, Dinka religion, and Lotuko mythology come from central Africa. Southern African traditions include Akamba mythology, Masai mythology, Malagasy mythology, San religion, Lozi mythology, Tumbuka mythology, and Zulu mythology. Bantu mythology is found throughout central, southeast, and southern Africa.
There are also notable African diasporic religions practiced in the Americas, such as Santeria, Candomble, Vodun, Lucumi, Umbanda, and Macumba.
Indigenous and folk.
Indigenous religions or folk religions refers to a broad category of traditional religions that can be characterised by shamanism, animism and ancestor worship, where "traditional" means "indigenous, that which is aboriginal or foundational, handed down from generation to generation…". These are religions that are closely associated with a particular group of people, ethnicity or tribe; they often have no formal creeds or sacred texts. Some faiths are syncretic, fusing diverse religious beliefs and practices.
Folk religions are often omitted as a category in surveys even in countries where they are widely practiced, e.g. in China.
New religious movements.
Sociological classifications of religious movements suggest that within any given religious group, a community can resemble various types of structures, including "churches", "denominations", "sects", "cults", and "institutions".
Issues.
Economics.
While there has been much debate about how religion affects the economy of countries, in general there is a negative correlation between religiosity and the wealth of nations. In other words, the richer a nation is, the less religious it tends to be. However, sociologist and political economist Max Weber has argued that Protestant Christian countries are wealthier because of their Protestant work ethic.
Health.
Mayo Clinic researchers examined the association between religious involvement and spirituality, and physical health, mental health, health-related quality of life, and other health outcomes. The authors reported that: "Most studies have shown that religious involvement and spirituality are associated with better health outcomes, including greater longevity, coping skills, and health-related quality of life (even during terminal illness) and less anxiety, depression, and suicide."
The authors of a subsequent study concluded that the influence of religion on health is "largely beneficial", based on a review of related literature. According to academic James W. Jones, several studies have discovered "positive correlations between religious belief and practice and mental and physical health and longevity." 
An analysis of data from the 1998 US General Social Survey, whilst broadly confirming that religious activity was associated with better health and well-being, also suggested that the role of different dimensions of spirituality/religiosity in health is rather more complicated. The results suggested "that it may not be appropriate to generalize findings about the relationship between spirituality/religiosity and health from one form of spirituality/religiosity to another, across denominations, or to assume effects are uniform for men and women.
Violence.
Charles Selengut characterizes the phrase "religion and violence" as "jarring", asserting that "religion is thought to be opposed to violence and a force for peace and reconciliation. He acknowledges, however, that "the history and scriptures of the world's religions tell stories of violence and war as they speak of peace and love."
Hector Avalos argues that, because religions claim divine favor for themselves, over and against other groups, this sense of righteousness leads to violence because conflicting claims to superiority, based on unverifiable appeals to God, cannot be adjudicated objectively.
Critics of religion Christopher Hitchens and Richard Dawkins go further and argue that religions do tremendous harm to society by using violence to promote their goals, in ways that are endorsed and exploited by their leaders.
Regina Schwartz argues that all monotheistic religions are inherently violent because of an exclusivism that inevitably fosters violence against those that are considered outsiders. Lawrence Wechsler asserts that Schwartz isn't just arguing that Abrahamic religions have a violent legacy, but that the legacy is actually genocidal in nature.
Byron Bland asserts that one of the most prominent reasons for the "rise of the secular in Western thought" was the reaction against the religious violence of the 16th and 17th centuries. He asserts that "(t)he secular was a way of living with the religious differences that had produced so much horror. Under secularity, political entities have a warrant to make decisions independent from the need to enforce particular versions of religious orthodoxy. Indeed, they may run counter to certain strongly held beliefs if made in the interest of common welfare. Thus, one of the important goals of the secular is to limit violence."
Nonetheless, believers have used similar arguments when responding to atheists in these discussions, pointing to the widespread imprisonment and mass murder of individuals under atheist states in the twentieth century:
And who can deny that Stalin and Mao, not to mention Pol Pot and a host of others, all committed atrocities in the name of a Communist ideology that was explicitly atheistic? Who can dispute that they did their bloody deeds by claiming to be establishing a 'new man' and a religion-free utopia? These were mass murders performed with atheism as a central part of their ideological inspiration, they were not mass murders done by people who simply happened to be atheist.—Dinesh D'Souza
In response to such a line of argument, however, author Sam Harris writes:
The problem with fascism and communism, however, is not that they are too critical of religion; the problem is that they are too much like religions. Such regimes are dogmatic to the core and generally give rise to personality cults that are indistinguishable from cults of religious hero worship. Auschwitz, the gulag and the killing fields were not examples of what happens when human beings reject religious dogma; they are examples of political, racial and nationalistic dogma run amok. There is no society in human history that ever suffered because its people became too reasonable.—Sam Harris
Richard Dawkins has stated that Stalin's atrocities were influenced not by atheism but by dogmatic Marxism, and concludes that while Stalin and Mao happened to be atheists, they did not do their deeds in the name of atheism. On other occasions, Dawkins has replied to the argument that Adolf Hitler and Josef Stalin were antireligious with the response that Hitler and Stalin also grew moustaches, in an effort to show the argument as fallacious.
Instead, Dawkins argues in "The God Delusion" that "What matters is not whether Hitler and Stalin were atheists, but whether atheism systematically influences people to do bad things. There is not the smallest evidence that it does."
Dawkins adds that Hitler in fact, repeatedly affirmed a strong belief in Christianity, but that his atrocities were no more attributable to his theism than Stalin's or Mao's were to their atheism. In all three cases, he argues, the perpetrators' level of religiosity was incidental.
D'Souza responds that an individual need not explicitly invoke atheism in committing atrocities if it is already implied in his worldview, as is the case in Marxism.
Law.
The study of law and religion is a relatively new field, with several thousand scholars involved in law schools, and academic departments including political science, religion, and history since 1980. Scholars in the field are not only focused on strictly legal issues about religious freedom or non-establishment, but also study religions as they are qualified through judicial discourses or legal understanding of religious phenomena. Exponents look at canon law, natural law, and state law, often in a comparative perspective. Specialists have explored themes in western history regarding Christianity and justice and mercy, rule and equity, and discipline and love. Common topics of interest include marriage and the family and human rights. Outside of Christianity, scholars have looked at law and religion links in the Muslim Middle East and pagan Rome.
Studies have focused on secularization. In particular the issue of wearing religious symbols in public, such as headscarves that are banned in French schools, have received scholarly attention in the context of human rights and feminism.
Science.
Religious knowledge, according to religious practitioners, may be gained from religious leaders, sacred texts, scriptures, or personal revelation. Some religions view such knowledge as unlimited in scope and suitable to answer any question; others see religious knowledge as playing a more restricted role, often as a complement to knowledge gained through physical observation. Adherents to various religious faiths often maintain that religious knowledge obtained via sacred texts or revelation is absolute and infallible and thereby creates an accompanying religious cosmology, although the proof for such is often tautological and generally limited to the religious texts and revelations that form the foundation of their belief.
In contrast, the scientific method gains knowledge by testing hypotheses to develop theories through elucidation of facts or evaluation by experiments and thus only answers cosmological questions about the universe that can be observed and measured. It develops theories of the world which best fit physically observed evidence. All scientific knowledge is subject to later refinement, or even outright rejection, in the face of additional evidence. Scientific theories that have an overwhelming preponderance of favorable evidence are often treated as "de facto" verities in general parlance, such as the theories of general relativity and natural selection to explain respectively the mechanisms of gravity and evolution.
Regarding religion and science, Albert Einstein states (1940): "For science can only ascertain what is, but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action; it cannot justifiably speak of facts and relationships between facts…Now, even though the realms of religion and science in themselves are clearly marked off from each other, nevertheless there exist between the two strong reciprocal relationships and dependencies. Though religion may be that which determine the goals, it has, nevertheless, learned from science, in the broadest sense, what means will contribute to the attainment of the goals it has set up." 
Animal sacrifice.
Animal sacrifice is the ritual killing and offering of an animal to appease or maintain favour with a deity. Such forms of sacrifice are practised within many religions around the world and have appeared historically in almost all cultures.
Related forms of thought.
Superstition.
Superstition has been described as "the incorrect establishment of cause and effect" or a false conception of causation. Religion is more complex and includes social institutions and morality. But religions may include superstitions or make use of magical thinking. Adherents of one religion sometimes think of other religions as superstition.
Some atheists, deists, and skeptics regard religious belief as superstition.
Greek and Roman pagans, who saw their relations with the gods in political and social terms, scorned the man who constantly trembled with fear at the thought of the gods ("deisidaimonia"), as a slave might fear a cruel and capricious master. The Romans called such fear of the gods "superstitio". Early Christianity was outlawed as a "superstitio Iudaica", a "Jewish superstition", by Domitian in the 80s AD. In AD 425, when Rome had become Christian, Theodosius II outlawed pagan traditions as superstitious.
Ancient greek historian Polybius described "superstition" in Ancient Rome as an "instrumentum regni", an instrument of maintaining the cohesion of the Empire.
The Roman Catholic Church considers superstition to be sinful in the sense that it denotes a lack of trust in the divine providence of God and, as such, is a violation of the first of the Ten Commandments. The Catechism of the Catholic Church states that superstition "in some sense represents a perverse excess of religion" (para. #2110). "Superstition," it says, "is a deviation of religious feeling and of the practices this feeling imposes. It can even affect the worship we offer the true God, e.g., when one attributes an importance in some way magical to certain practices otherwise lawful or necessary. To attribute the efficacy of prayers or of sacramental signs to their mere external performance, apart from the interior dispositions that they demand is to fall into superstition. Cf. Matthew 23:16-22" (para. #2111)
Myth.
The word "myth" has several meanings.
Ancient polytheistic religions, such as those of Greece, Rome, and Scandinavia, are usually categorized under the heading of mythology. Religions of pre-industrial peoples, or cultures in development, are similarly called "myths" in the anthropology of religion. The term "myth" can be used pejoratively by both religious and non-religious people. By defining another person's religious stories and beliefs as mythology, one implies that they are less real or true than one's own religious stories and beliefs. Joseph Campbell remarked, "Mythology is often thought of as "other people's" religions, and religion can be defined as mis-interpreted mythology."
In sociology, however, the term "myth" has a non-pejorative meaning. There, "myth" is defined as a story that is important for the group whether or not it is objectively or provably true. Examples include the death and resurrection of Jesus, which, to Christians, explains the means by which they are freed from sin and is also ostensibly a historical event. But from a mythological outlook, whether or not the event actually occurred is unimportant. Instead, the symbolism of the death of an old "life" and the start of a new "life" is what is most significant. Religious believers may or may not accept such symbolic interpretations.
Secularism and irreligion.
The terms "atheist" (lack of belief in any gods) and "agnostic" (belief in the unknowability of the existence of gods), though specifically contrary to theistic (e.g. Christian, Jewish, and Muslim) religious teachings, do not by definition mean the opposite of "religious". There are religions (including Buddhism and Taoism), in fact, that classify some of their followers as agnostic, atheistic, or nontheistic. The true opposite of "religious" is the word "irreligious". Irreligion describes an absence of any religion; antireligion describes an active opposition or aversion toward religions in general.
As religion became a more personal matter in Western culture, discussions of society became more focused on political and scientific meaning, and religious attitudes (dominantly Christian) were increasingly seen as irrelevant for the needs of the European world. On the political side, Ludwig Feuerbach recast Christian beliefs in light of humanism, paving the way for Karl Marx's famous characterization of religion as "the opium of the people". Meanwhile, in the scientific community, T.H. Huxley in 1869 coined the term "agnostic," a term—subsequently adopted by such figures as Robert Ingersoll—that, while directly conflicting with and novel to Christian tradition, is accepted and even embraced in some other religions. Later, Bertrand Russell told the world "Why I Am Not a Christian", which influenced several later authors to discuss their breakaway from their own religious upbringings from Islam to Hinduism.
Some atheists also construct parody religions, for example, the Church of the SubGenius or the Flying Spaghetti Monster, which parodies the equal time argument employed by intelligent design Creationism. Parody religions may also be considered a post-modern approach to religion. For instance, in Discordianism, it may be hard to tell if even these "serious" followers are not just taking part in an even bigger joke. This joke, in turn, may be part of a greater path to enlightenment, and so on "ad infinitum".
Criticism of religion.
Religious criticism has a long history, going back at least as far as the 5th century BCE. During classical times, there were religious critics in ancient Greece, such as Diagoras "the atheist" of Melos, and in the 1st century BCE in Rome, with Titus Lucretius Carus's "De Rerum Natura".
During the Middle Ages and continuing into the Renaissance, potential critics of religion were persecuted and largely forced to remain silent. There were notable critics like Giordano Bruno, who was burned at the stake for disagreeing with religious authority.
In the 17th and 18th century with the Enlightenment, thinkers like David Hume and Voltaire criticized religion.
In the 19th century, Charles Darwin and the theory of evolution led to increased skepticism about religion. Thomas Huxley, Jeremy Bentham, Karl Marx, Charles Bradlaugh, Robert Ingersol, and Mark Twain were noted 19th-century and early-20th-century critics. In the 20th century, Bertrand Russell, Siegmund Freud, and others continued religious criticism.
Sam Harris, Daniel Dennett, Richard Dawkins, Victor J. Stenger, and the late Christopher Hitchens were active critics during the late 20th century and early 21st century.
Critics consider religion to be outdated, harmful to the individual (e.g. brainwashing of children, faith healing, female genital mutilation, circumcision), harmful to society (e.g. holy wars, terrorism, wasteful distribution of resources), to impede the progress of science, to exert social control, and to encourage immoral acts (e.g. blood sacrifice, discrimination against homosexuals and women, and certain forms of sexual violence such as marital rape). A major criticism of many religions is that they require beliefs that are irrational, unscientific, or unreasonable, because religious beliefs and traditions lack scientific or rational foundations.
Some modern-day critics, such as Bryan Caplan, hold that religion lacks utility in human society; they may regard religion as irrational. Nobel Peace Laureate Shirin Ebadi has spoken out against undemocratic Islamic countries justifying "oppressive acts" in the name of Islam.
Bibliography.
On religion definition:
Studies of religion in particular geographical areas:
</dl>

</doc>
<doc id="25417" url="http://en.wikipedia.org/wiki?curid=25417" title="Reed College">
Reed College

Reed College is a private liberal arts college located in southeast Portland in the U.S. state of Oregon. Founded in 1908, Reed is a residential college with a campus located in Portland's Eastmoreland neighborhood, featuring architecture based on the Tudor-Gothic style, and a forested canyon nature preserve at its center. Reed is known for its mandatory freshman humanities program, for its required senior-year thesis, as the only private undergraduate college with a primarily student-run nuclear reactor supporting its science programs, and for the unusually high proportion of graduates who go on to earn PhDs and other postgraduate degrees.
History.
The Reed Institute (the legal name of the college) was founded in 1908, and Reed College held its first classes in 1911. Reed is named for Oregon pioneers Simeon Gannett Reed (1830-1895) and Amanda Reed (died 1904). Simeon was an entrepreneur in trade on the Columbia River; in his will he suggested that his wife could "devote some portion of my estate to benevolent objects, or to the cultivation, illustration, or development of the fine arts in the city of Portland, or to some other suitable purpose, which shall be of permanent value and contribute to the beauty of the city and to the intelligence, prosperity, and happiness of the inhabitants". The first president of Reed (1910–1919) was William Trufant Foster, a former professor at Bates College and Bowdoin College in Maine.
Contrary to popular belief, the college did not grow out of student revolts and experimentation, but out of a desire to provide a "more flexible, individualized approach to a rigorous liberal arts education". Founded explicitly in reaction to the "prevailing model of East Coast, Ivy League education", the college's lack of varsity athletics, fraternities, and exclusive social clubs – as well as its coeducational, nonsectarian, and egalitarian status – gave way to an intensely academic and intellectual college whose purpose was to devote itself to "the life of the mind", that life being understood primarily as the academic life.
The college has a reputation for progressivism.
Distinguishing features.
According to sociologist Burton Clark, Reed is one of the most unusual institutions of higher learning in the United States, featuring a traditional liberal arts and natural sciences curriculum. It requires freshmen to take Humanities 110 – an intensive introduction to the Classics, covering ancient Greece and Rome as well as the Bible and ancient Jewish history. Its program in the sciences is likewise unusual – Reed's TRIGA research reactor makes it the only school in the United States to have a nuclear reactor operated entirely by undergraduates. Reed also requires all students to complete a thesis (a two-semester-long research project conducted under the guidance of professors) during the senior year as a prerequisite of graduation, and passing a junior qualifying exam at the end of the junior year is a prerequisite to beginning the thesis. Upon completion of the senior thesis, students must also pass an oral exam that may encompass questions not only about the thesis, but also about any course previously taken.
Reed maintains a 10:1 student-to-faculty ratio, and its small classes emphasize a "conference" style, in which the teacher often acts as a mediator for discussion rather than a lecturer. While large lecture-style classes exist, Reed emphasizes its smaller lab and conference sections.
Although letter grades are given to students, grades are de-emphasized at Reed. According to the school, "A conventional letter grade for each course is recorded for every student, but the registrar's office does not distribute grades to students, provided that work continues at satisfactory (C or higher) levels. Unsatisfactory grades are reported directly to the student and the student's adviser. Papers and exams are generally returned to students with lengthy comments but without grades affixed." There is no dean’s list or honor roll "per se", but students who maintain a GPA of 3.5 or above for an entire academic year receive academic commendations, which are noted on their transcripts, at the end of the spring semester. Many Reedies graduate without knowing either their cumulative GPA or their grades in individual classes. Reed also claims to have experienced very little grade inflation over the years, noting, for example, that only ten students graduated with a perfect 4.0 GPA in the period from 1983 to 2012. (Transcripts are accompanied by a card explaining Reed's relatively tough grading system, so as to not penalize students applying to graduate schools.) And though Reed does not award Latin honors to graduates, Reed does confer several awards for academic achievement at the time of commencement, including naming students to Phi Beta Kappa.
Reed has no fraternities or sororities, and few NCAA sports teams, although physical education classes (which range from kayaking to juggling) are required for graduation. Reed also has several intercollegiate athletic clubs, most notably the Rugby, Ultimate Frisbee, and Soccer teams.
What this means is that a community governed by an honor principle is a community not of rules and procedures but of virtue. As such, it is a community of unfreedom. There is no protected realm; one can never take refuge in, seek protection from, or hide behind a doctrine of rights. Anything that anyone does is, in principle, subject to evaluation. Was it a virtuous thing to do? Was it consistent with notions of honorableness? Does it contribute to the well-being of the community? Is it the kind of behavior that we value and wish to encourage? In the absence of rights, behavior that we do not wish to value and do not wish to encourage has absolutely no protection.
Peter J. Steinberger, Former Dean of the Faculty
Reed's ethical code is known as "The Honor Principle". First introduced as an agreement to promote ethical academic behavior, with the explicit end of relieving the faculty of the burden of policing student behavior, the Honor Principle was extended to cover all aspects of student life. While inspired by traditional honor systems, Reed's Honor Principle differs from these in that it is a guide for ethical standards themselves, not just their enforcement. Under the Honor Principle, there are no codified rules governing behavior. Rather, the onus is on students individually and as a community to define which behaviors are acceptable and which are not.
Discrete cases of grievance, known as "Honor Cases", are adjudicated by a Judicial Board, which consists of twelve full-time students. There is also an "Honor Council," which consists of students, faculty, and staff, designed to educate the community regarding the Honor Principle and mediate conflict between individuals.
Academic program.
Reed categorizes its academic program into five Divisions and the Humanities program. Overall, Reed offers five Humanities courses, twenty-six department majors, twelve interdisciplinary majors, six dual-degree programs with other colleges and universities, and programs for pre-medical and pre-veterinary students.
Humanities program.
Reed President Richard Scholz in 1922 called the educational program as a whole "an honest effort to disregard old historic rivalries and hostilities between the sciences and the arts, between professional and cultural subjects, and, ... the formal chronological cleavage between the graduate and the undergraduate attitude of mind". The Humanities program, which came into being in 1943 (as the union of two year-long courses, one in "world" literature, the other in "world" history) is one manifestation of this effort. One change to the program was the addition of a course in Chinese Civilization in 1995. The faculty has also recently approved several significant changes to the introductory syllabus. These changes include expanding the parameters of the course to include more material regarding urban and cultural environments.
Reed's Humanities program includes the mandatory freshman course "Introduction to Western Humanities" covering ancient Greek and Roman literature, history, art, religion, and philosophy. Sophomores, juniors, and seniors may take "Early Modern Europe" covering Renaissance thought and literature; "Modern Humanities" covering the Enlightenment, the French Revolution, the Industrial Revolution, and Modernism, and/or "Foundations of Chinese Civilization". There is also a Humanities Senior Symposium.
Interdisciplinary and dual-degree programs.
Reed also offers interdisciplinary programs in American studies, Environmental Studies, Biochemistry and Molecular Biology, Chemistry-Physics, Classics-Religion, Dance/Theatre, History-Literature, International and Comparative Policy Studies (ICPS), Literature-Theatre, Mathematics-Economics, and Mathematics-Physics.
Reed offers dual-degree programs in Computer Science (with University of Washington), Engineering (with Caltech, Columbia University, and Rensselaer Polytechnic Institute), Forestry or Environmental Management (with Duke University), and Fine Art (with the Pacific Northwest College of Art).
Admissions and student demographics.
Until the late 1990s, Reed accepted a larger percentage of total applicants than peer institutions – 76% in 1996. This led to high levels of attrition (drop-outs) during that period. Since then, the number of applicants for freshman admission has increased sharply. Since 2002, Reed's attrition rate has moved toward that of peer institutions, and the five-year graduation rate (76% for the 2004/05 entering class) now exceeds the national average.
The applicant pool for the class of 2013 was the third largest in Reed's history: 3,159 students applied and 1,225 were admitted, for an admission rate of 38.8%. The admitted class of 2013's average combined Math and Verbal SAT scores were 1407; the mean composite ACT score was 31; and the mean high school GPA was 4.034.
Reed's student body is 45% male and 55% female, and includes 22% minority students: 3% self-report as Black (including African-American, African, and Afro-Caribbean); 6% as Hispanic; 9% as Asian, 2% Native American, and 2% Mixed/Other. Minority numbers include some of the 7% international citizens (13% of freshmen did not self-report their ethnicity). In the class of 2010, 38% of students are from the United States's West Coast (California, Oregon, Washington), with the most coming from California.
Tuition and finances.
The total base cost for the 2012–13 academic year, including tuition, fees and room-and-board, is $55,920. "For the 2011–12 academic year, the average financial aid package – including grants, loans, and work opportunities – is approximately $35,990". In 2011–12 about half of students received financial aid from the college. In 2004 (the most recent data available), 1.4% of Reed graduates defaulted on their student loans – below the national Cohort Default Rate average of 5.1%.
Reed's endowment as of June 30, 2014, was $543 million. In the economic downturn that began in late 2007, Reed's total endowment had declined from $455 million in June 2007 to $311 million in June 2009. By the end of 2013, however, the endowment surpassed the $500 million mark.
Reputation.
Rankings.
In 1995 Reed College refused to participate in the "U.S. News & World Report" "best colleges" rankings, making it the first educational institution in the United States to refuse to participate in college rankings. According to Reed's Office of Admissions:
Reed College has actively questioned the methodology and usefulness of college rankings ever since the magazine's best-colleges list first appeared in 1983, despite the fact that the issue ranked Reed among the top ten national liberal arts colleges. Reed's concern intensified with disclosures in 1994 by the "Wall Street Journal" about institutions flagrantly manipulating data in order to move up in the rankings in "U.S. News" and other popular college guides. This led Reed's then-president Steven Koblik to inform the editors of "U.S. News" that he didn't find their project credible, and that the college would not be returning any of their surveys.
"Rolling Stone", in its October 16, 1997, issue, argued that Reed's rankings were artificially decreased by "U.S. News" after they stopped sending data to "U.S. News & World Report." Nicholas Thompson reiterated this judgment in an article in "The Washington Monthly" in 2000. Reed has also made the same claim. In discussing Reed's decision, former President Colin Diver wrote in an article for the November 2005 issue of the "Atlantic Monthly," "by far the most important consequence of sitting out the rankings game, however, is the freedom to pursue our own educational philosophy, not that of some news magazine." "U.S. News" maintains that their rankings are "a very legitimate tool for getting at a certain level of knowledge about colleges."
However, in 2005 Reed did submit statistics to The Princeton Review, and received first in Overall Undergraduate Academic Experience. In 2009, The Princeton Review ranked Reed number two in "Best Classroom Experience", number three in "Students Study the Most", and number five in "Birkenstock-Wearing, Tree-Hugging, Clove-Smoking Vegetarians." In 2006, "Newsweek" magazine named Reed as one of twenty-five "New Ivies", listing it among "the nation's elite colleges." In 2012, "Newsweek" ranked Reed the 15th "most rigorous" college in the nation.
Academic honors.
Reed has produced the second-highest number of Rhodes scholars for any liberal arts college—31—as well as over fifty Fulbright Scholars, over sixty Watson Fellows, and two MacArthur ("Genius") Award winners. A very high proportion of Reed graduates go on to earn PhDs, particularly in the sciences, history, political science, and philosophy. Reed is third in percentage of its graduates who go on to earn PhDs in all disciplines, after only Caltech and Harvey Mudd. In 1961, "Scientific American" declared that second only to Caltech, "This small college in Oregon has been far and away more productive of future scientists than any other institution in the U.S." Reed is first in this percentage in biology, second in chemistry and humanities, third in history, foreign languages, and political science, fourth in science and mathematics (fifth in physics and social sciences), sixth in anthropology, seventh in area and ethnic studies and linguistics, and eighth in English literature and medicine.
Reed's debating team, which had existed for only two years at the time, was awarded the first place sweepstakes trophy for Division II schools at the final tournament of the Northwest Forensics Conference in February 2004.
Loren Pope, former education editor for "The New York Times," writes about Reed in "Colleges That Change Lives," saying, "If you're a genuine intellectual, live the life of the mind, and want to learn for the sake of learning, the place most likely to empower you is not Harvard, Yale, Princeton, Chicago, or Stanford. It is the most intellectual college in the country—Reed in Portland, Oregon."
Drug use.
Since the 1960s, Reed has had a reputation for tolerating open drug use among its students. "The Insider's Guide to the Colleges", written by the staff of "Yale Daily News", notes an impression among students of institutional permissiveness: "According to students, the school does not bust students for drug or alcohol use unless they cause harm or embarrassment to another student." However, in 2010 the "Daily Beast" ranked the top 50 'Druggiest Colleges' in the United States, assessing "which campuses have both environments that may be permissive of drug use as well as a student body that partakes in illegal drug use." Reed College did not rank in the top 50, but comparable liberal arts institutions were listed, such as Williams (#9), Rollins (#20), and Gettysburg (#32).
In April, 2008, student Alex Lluch died of a heroin overdose in his on-campus dorm room. His death prompted revelations of several previous incidents, including the near-death heroin overdose of another student only months earlier. College President Colin Diver said "I don't honestly know" whether the drug death was an isolated incident or part of a larger problem. "When you say Reed," Diver said, "two words often come to mind. One is brains. One is drugs." Local reporter James Pitkin of the newspaper "Willamette Week" editorialized that "Reed College, a private school with one of the most prestigious academic programs in the U.S., is one of the last schools in the country where students enjoy almost unlimited freedom to experiment openly with drugs, with little or no hassles from authorities," though "Willamette Week" stated the following week concerning Pitkin's editorial: "As of press time, almost 500 responses, many expressing harsh criticism of "Willamette Week", had been posted on our website."
In March 2010, one more student died of drug-related causes in his off-campus residence. This led "The New York Times" to conclude that "Reed . . . has long been known almost as much for its unusually permissive atmosphere as for its impressively rigorous academics." Law enforcement authorities promised to take action, including sending undercover agents to Reed's annual Renn Fayre celebration.
Political.
Reed has a reputation for being politically left-wing. Whether in fact Reed's student body is more leftist than those of similar colleges is not difficult to determine, but Reed's academic tradition of open and passionate debate often spills into the off-campus political arena and, combined with a so called "tolerant" social environment, leads to radical leftism.
During the McCarthy era of the 1950s, then-President Duncan Ballantine fired Marxist philosopher Stanley Moore, a tenured professor, for his failure to cooperate with the House Un-American Activities Committee (HUAC) investigation. According to an article in the college's alumni magazine, "because of the decisive support expressed by Reed's faculty, students, and alumni for the three besieged teachers and for the principle of academic freedom, Reed College's experience with McCarthyism stands apart from that of most other American colleges and universities. Elsewhere in the academic world both tenured and nontenured professors with alleged or admitted communist party ties were fired with relatively little fuss or protest. At Reed, however, opposition to the political interrogations of the teachers was so strong that some believed the campus was in danger of closure." A statement of "regret" by the Reed administration and Board of Trustees was published in 1981, formally revising the judgment of the 1954 trustees. In 1993, then-President Steve Koblik invited Moore to visit the College, and in 1995 the last surviving member of the Board that fired Moore expressed his regret and apologized to him.
Campus.
The Reed College campus was established on a southeast Portland tract of land known in 1910 as Crystal Springs Farm, a part of the Ladd Estate, formed in the 1870s from original land claims. The college's grounds include 116 acre of contiguous land, including a wooded wetland known as Reed Canyon.
Portland architect A. E. Doyle developed a plan, never implemented in full, modeled on the University of Oxford's St. John's College. The original campus buildings (including the Library, the Old Dorm Block, and what is now the primary administration building, Eliot Hall) are brick Tudor Gothic buildings in a style similar to Ivy League campuses. In contrast, the science section of campus, including the physics, biology, and psychology (originally chemistry) buildings, were designed in the Modernist style. The Psychology Building, completed in 1949, was designed by Modernist architect Pietro Belluschi at the same time as his celebrated Equitable Building in downtown Portland.
The campus and buildings have undergone several phases of growth, and there are now 21 academic and administrative buildings and 18 residence halls. Since 2004, Reed's campus has expanded to include adjacent properties beyond its historic boundaries, such as the Birchwood Apartments complex and former medical administrative offices on either side of SE 28th Avenue, and the Parker House, across SE Woodstock from Prexy. At the same time the Willard House (donated to Reed in 1964), across from the college's main entrance at SE Woodstock and SE Reed College Place, was converted from faculty housing to administrative use. Reed announced on July 13, 2007, that it had purchased the Rivelli farm, a 1.5 acre tract of land south of the Garden House and west of Botsford Drive. Reed’s "immediate plans for the acquired property include housing a small number of students in the former Rivelli home during the 2007–08 academic year. Longer term, the college anticipates that it may seek to develop the northern portion of the property for additional student housing".
Residence halls.
Reed houses about 1,000 students in 18 residence halls on campus and several college-owned houses and apartment buildings on or adjacent to campus. Residence halls on campus range from the traditional (i.e., Gothic Old Dorm Block, referred to as "ODB") to the eclectic (e.g., Anna Mann, a Tudor-style cottage built in the 1920s by Reed's founding architect A. E. Doyle, originally used as a women's hall) language houses (Spanish, Russian, French, German, and Chinese), "temporary" housing, built in the 1960s (Cross Canyon – Chittick, Woodbridge, McKinley, Griffin), to more recently built dorms (Bragdon, Naito, Sullivan). There are also theme residence halls including everything from substance-free living to Arabic culture to a dorm for students interested in outdoors activities (hiking, climbing, bicycling, kayaking, skiing, etc.). The college's least-loved complex (as measured by applications to the College's housing lottery), MacNaughton and Foster-Scholz, is known on campus as "Asylum Block" because of its post-World War II modernist architecture and interior spaces dominated by long, straight corridors lined with identical doors, said by students to resemble that of an insane asylum. Until 2006, it was thought that these residence halls had been designed by architect Pietro Belluschi.
Under the 10-year Campus Master Plan adopted in 2006, Foster-Scholz is scheduled to be demolished and replaced, and MacNaughton to be remodeled. According to the master plan, "The College's goal is to provide housing on or adjacent to the campus that accommodates 75% of the [full-time] student population. At present, the College provides on-campus housing for 838 students".
In Spring 2007, the College broke ground on the construction of a new quadrangle called the Grove with four new residence halls (Aspen, Sequoia, Sitka, Bidwell) on the northwest side of the campus, which opened in Fall 2008. A new Spanish House residence was completed. Together, the five new residences added 142 new beds.
Reed Canyon.
The Reed College Canyon, a natural area and national wildlife preserve, bisects the campus, separating the academic buildings from many of the residence halls (the so-called "cross-canyon halls"). The canyon is filled by Crystal Creek Springs, a natural spring that drains into Johnson Creek.
Canyon Day, a tradition dating back to 1915, is held twice a year. On Canyon Day students and Reed neighbors join canyon crew workers to spend a day helping with restoration efforts.
A landmark of the campus, the Blue Bridge, spans the canyon. It appears on almost every viewbook that the college circulates. This bridge replaced the unique cantilevered bridge that served in that spot between 1959 and 1991, which "featured stressed plywood girders – the first time this construction had been used on a span of this size: a straight bridge 132 ft long and 15 ft high. It attracted great architectural interest during its lifetime". The Blue Bridge was originally known as the "Cross Canyon Bridge" until student Rain Lynham bought and installed black lights in 1998 as decoration for Renn Fayre, the traditional end of year, campus-wide party, of which she was the organizer that year. The new look proved so popular with students and faculty alike that the original white bulbs were never replaced.
A new pedestrian and bicycle bridge spanning the canyon was opened in Fall 2008. This bridge, dubbed the "Bouncy Bridge" or "Amber Bridge" by students, is 370 ft long, about a third longer than the Blue Bridge, and "connect[s] the new north campus quad to Gray Campus Center, the student union, the library, and academic buildings on the south side of campus".
Douglas F. Cooley Gallery.
Reed's Cooley Gallery is an internationally recognized contemporary art space located at the entrance to Reed's Hauser Library. It was established in 1988 as the result of a gift from Susan and Edward Cooley in honor of their late son. The Cooley Gallery has exhibited international artists such as Mona Hatoum, Al Held, David Reed and Gregory Crewdson as well as the contemporary art collection of Michael Ovitz. In pursuit of its mission to support the curriculum of the art, art history, and humanities programs at Reed, the gallery produces three or four exhibitions each year, along with lectures, colloquia, and artist visits. The gallery is currently under the directorship of Stephanie Snyder, who succeeded founding director Susan Fillin-Yeh in 2004.
Food services.
The cafeteria, known simply as "Commons", has a reputation for ecologically sustainable food services. The commons dining hall is operated by Bon Appétit, and food is purchased on an item-by-item basis. Suiting the student body, vegan and vegetarian dishes feature heavily on the menu. It is currently the only cafeteria on the small campus, with the exception of Caffe Paradiso, a small cafe on the other side of campus which also operated by board points.
The Reed College Co-ops are a theme community that reside in the Farm and Garden Houses, after many years on the first floor of MacNaughton Hall. These are the only campus dorms that are independent of the school's board plan. They traditionally throw an alternative "Thanksgiving" celebration that has sometimes included a square-dance. The Co-ops house students who purchase and prepare food together, sharing chores and conducting weekly, consensus-based meetings. It is a close community valuing sustainability, organic food, consensus-based decisions, self-government, music, and plants.
The Paradox ("Est. in the 80s") is a student-run cafe located on campus. In 2003 the Paradox opened a second cafe, dubbing it the "Paradox Lost" (an allusion to John Milton's "Paradise Lost"), at the southern end of the biology building, in the space commonly called the "Bio Fishbowl". The new north-campus dorms, which opened in Fall 2008, feature yet another small cafe, dubbed "Cafe Paradiso", thereby providing three coffee shops within a 116 acre campus. This third shop is not student-run, but is operated by Bon Appétit. Bon Appétit has a monopoly on the food services at Reed as they are the only ones who accept board points; written into their contract is the prohibition of food carts on campus.
Off-campus housing.
Reed also has off-campus housing. Many houses in the Woodstock and Eastmoreland Portland neighborhoods are traditionally rented to Reed students.
Icons and student life.
Griffin.
The official mascot of Reed is the griffin. In mythology, the griffin often pulled the chariot of the sun; in canto 32 of Dante's "Commedia" the griffin is associated with the Tree of Knowledge. The griffin was featured on the coat-of-arms of founder Simeon Reed and is now on the official seal of Reed College.
School color.
The official school color of Reed is Richmond Rose. Over the years, institutional memory of this fact has faded and the color appearing on the school's publications and merchandise has darkened to a shade of maroon. The most common examples of "Richmond Rose" are the satin tapes securing the degree certificate inside a Reed College diploma.
School song.
The school song, "Fair Reed," is sung to the tune of the 1912 popular song "Believe Me, if All Those Endearing Young Charms." It may be imitative of the Harvard anthem "Fair Harvard", which is also sung to the tune of "Believe Me, if All Those Endearing Young Charms". It was composed by former president William Trufant Foster shortly after Reed's founding, and is rarely heard today.
An unofficial Reed Alma Mater, "Epistemology Forever," sung to the tune of "The Battle Hymn of the Republic", has been sung by Reed students since the 1950s.
Student nickname.
Reed students and alumni referred to themselves as "Reedites" in the early years of the college. This term faded out in favor of the now ubiquitous "Reedie" after World War II. Around campus, prospective students are called "prospies".
Unofficial mottos and folklore.
An unofficial motto of Reed is "Communism, Atheism, Free Love", and can be found in the Reed College Bookstore on sweaters, T-shirts, etc. It was a label that the Reed community claimed from critics during the 1920s as a "tongue-in-cheek slogan" in reference to Reed's nonconformism. Reed's founding president William T. Foster's outspoken opposition against the entrance of the United States into World War I, as well as the college's support for feminism, its adherence to academic freedom (i.e., inviting a leader of the Socialist Party of America to speak on campus about the Russian Revolution’s potential impact on militarism, emancipation of women, and ending the persecution of Jews), and its nonsectarian status made the college a natural target for what was originally meant to be a pejorative slur.
The faux Reed Seal has changed over the years. In its original form the griffin was holding a hammer and sickle in its paws. Later versions had the griffin wearing boxing gloves.
One of the unofficial symbols of Reed is the Doyle Owl, a roughly 280 lb concrete statue that has been continuously stolen and re-stolen since about 1919. The original Doyle Owl (originally "House F Owl" after the dormitory named House F that later became Doyle dormitory) was a garden sculpture from the neighborhood stolen by House F residents as a prank (there is a photo of House F residents around the original owl that has been made into a T-shirt). The on-campus folklore of events surrounding the Doyle Owl is sufficiently large that, in 1983, a senior thesis was written on the topic of the Owl's oral history. The original Doyle Owl was destroyed many years ago; the current avatar is Doyle Owl number 13, plus or minus 11. At the present time only one Owl is being shown.
Paideia.
Each January, before the beginning of second-semester classes, the campus holds an interim period called Paideia (roughly drawn from the Greek, meaning 'education'). Originally conceived and approved by the faculty in 1968 for unstructured independent study or "UIS," Paideia ran for the full month of January from 1969–1981, supervised by a committee of faculty, staff and students. This festival of learning takes the form of classes and seminars put on by anyone who wishes to teach, including students, professors, staff members, and outside educators invited on-campus by members of the Reed Community. The classes are intended to be informal, yet intellectual activities free of the usual academic pressure endemic to Reed. Many such classes are explicitly trivial (one long-running tradition is to hold an underwater basket weaving class), while others are trivially academic (such as "Giant Concrete Gnome Construction", a class that, incidental to building monolithic gnomes, includes some content relating to the construction of pre-Christian monoliths). More structured classes (such as martial arts seminars and mini-classes on obscure academic topics), tournaments, and film festivals round out the schedule, which is different every year. The objective of Paideia is not only to learn new (possibly non-useful) things, but to turn the tables on students and encourage them to teach.
In his 2005 Stanford commencement lecture, Apple Inc. founder and Reed dropout Steve Jobs credited a Reed calligraphy class for his focus on choosing quality typefaces for the Macintosh. While the full calligraphy course is no longer taught at Reed, Paideia usually features a short course on the subject.
Renn Fayre.
Renn Fayre is an annual three-day celebration with a different theme each year. Born in the 1960s as an actual renaissance fair, it has long since lost all connection to anachronism and the Renaissance, although its name has persisted. The event is initiated by a procession of seniors throwing their thesis notes in a large bonfire after the completed theses are submitted.
Reed Arts Week.
Reed Arts Week is a week-long celebration of the arts at Reed. It features music, dance, film, creative writing, and the visual arts.
Student organizations.
According to Reed's website, each semester, a $130 student body fee "is collected from each full-time student by the business office, acting as agent for the student senate. The fee underwrites publication of the student newspaper and extracurricular activities, and partially supports the student union and ski cabin".
Student body funds (totaling roughly $370,000 annually) are distributed each semester to groups that place among the top 40 organizations in the semester's funding poll. The funding poll uses a voting system in which each organization provides a description that is ranked by each member of the student body with either 'top six', 'approve', 'no opinion', 'disapprove' or 'deep six.' These ranks are then tabulated by assigning numbers to each rank and summing across all voters. Afterwards, the top forty organizations present their budgets to the student body senate during Funding Circus. The following day the senate makes decisions about each budget in a process called Funding Hell.
The school's student-run newspaper, "The Reed College Quest "or simply the "Quest", has been published since 1913, and its radio station, KRRC has been broadcasting, with a few interruptions, since 1955.
Although some that partner with outside groups such as Oxfam or Planned Parenthood are more structured, most organizations are highly informal. There is no formal process for forming a student organization at Reed; a group of students (or a single student) announcing themselves as or just considering themselves a student organization is enough. Groups that want funding from the school's Student Activities office or Student Body Fees, however, must register with Student Activities or through the Student Senate. Other clubs at Reed are more controversial, such as the Master and Bation Club, a group dedicated to weekly gatherings focused on masturbation and sextoy sharing. The Reed archive of comic books and graphic novels, the MLLL (Comic Book Reading Room), is well into its fourth decade, and Beer Nation, the student group that organizes and manages various beer gardens throughout the year and during Renn Fayre, has existed for many years. Some organizations, such as the Motorized Couch Collective – dedicated to installing motors and wheels into furniture – have become more Reed myth than reality in recent years.
Reed has ample recreational facilities on campus, a ski cabin on Mount Hood, recreational clubs such as the Reed Outing Club (ROC), and Club Sports (with college-paid coaches), including ultimate frisbee, co-ed soccer, rugby, basketball, and squash.
Notable alumni.
Examples of alumni include John Alroy, John Backus, Mary Barnard, Yoram Bauman, Margaret Bechard, Daryl Bem, Louis T. Benezet, Walter Berns, Mei-mei Berssenbrugge, Lee Blessing, Arlene Blum, Robert Brightman, Kate Christensen, Jim Compton, Richard Crandall, Lamar Crowson, Richard J. Danzig, Suzan DelBene, Barry Hansen ("Dr. Demento"), David Eddings, Barbara Ehrenreich, Nancy Farmer, Janet Fitch, David H. French, Robert Friedland, Rose Friedman, Juliet Glass, Peter S. Goodman, Jonathan Grudin, Ted Robert Gurr, Peter Dobkin Hall, Richard L. Hanna, Ernest Haycox, David Hoggan, Dell Hymes, Maurice Isserman, Herbert Jasper, Dale W. Jorgenson, Don Kates, Gail M. Kelly, Laleh Khadivi, Daniel Kottke, Hope Lange, Berkeley Lent, Michael E. Levine, Hans A. Linde, Arthur H. Livermore, Jayne Loader, William D. McElroy, Dennis B. McGilvray, Willard Midgette, Tod H. Mikuriya, Bill Naito, Victor Nizet, Peter Norton, Daria O'Neill, Eric Overmyer, Keith Packard, Norman Packard, Emilio Pucci, Ray Raphael, Diane Ravitch, Ken Raymond, Aaron Rhodes, David Romtvedt, Eleanor Rosch, Diane Rosenbaum, Jay Rosenberg, Michael Rothschild, Vern Rutsala, Larry Sanger, Leslie Scalapino, Steven Shapin, Sydney Shoemaker, Susan Silas, Stephen C. Sillett, John Alexander Simpson, Kenneth J. Singleton, Robert Slavin, Stephen J. Smith, Gary Snyder, Norman Solomon, John Sperling, David Henry Sterry, Sumner Stone, Michael S. Teitelbaum, Sean Thackrey, Robert K. Thomas, Nicolaus Tideman, Igor Vamos, Bruce Voeller, Howard Vollum, Elizabeth Warnock Fernea, Lew Welch, Jon Westling, Philip Whalen, Howard Wolpe, and Allen W. Wood.
Among those who attended but did not graduate from Reed are James Beard, Ry Cooder, Katherine Dunn, Afshin Feiz, Steve Jobs, and Chris Langan.

</doc>
<doc id="25418" url="http://en.wikipedia.org/wiki?curid=25418" title="Proof by contradiction">
Proof by contradiction

In logic, proof by contradiction is a form of proof, and more specifically a form of indirect proof, that establishes the truth or validity of a proposition by showing that the proposition's being false would imply a contradiction. Proof by contradiction is also known as indirect proof, apagogical argument, proof by assuming the opposite, and reductio ad impossibilem. It is a particular kind of the more general form of argument known as "reductio ad absurdum".
G. H. Hardy described proof by contradiction as "one of a mathematician's finest weapons", saying "It is a far finer gambit than any chess gambit: a chess player may offer the sacrifice of a pawn or even a piece, but a mathematician offers the game."
Examples.
Irrationality of the square root of 2.
A classic proof by contradiction from mathematics is the proof that the square root of 2 is irrational. If it were rational, it could be expressed as a fraction "a"/"b" in lowest terms, where "a" and "b" are integers, at least one of which is odd. But if "a"/"b" = √2, then "a"2 = 2"b"2. Therefore "a"2 must be even. Because the square of an odd number is odd, that in turn implies that "a" is even. This means that "b" must be odd because a/b is in lowest terms.
On the other hand, if "a" is even, then "a"2 is a multiple of 4. If "a"2 is a multiple of 4 and "a"2 = 2"b"2, then 2"b"2 is a multiple of 4, and therefore "b"2 is even, and so is "b".
So "b" is odd and even, a contradiction. Therefore the initial assumption—that √2 can be expressed as a fraction—must be false.
The length of the hypotenuse.
The method of proof by contradiction has also been used to show that for any non-degenerate right triangle, the length of the hypotenuse is less than the sum of the lengths of the two remaining sides. The proof relies on the Pythagorean theorem. Letting "c" be the length of the hypotenuse and "a" and "b" the lengths of the legs, the claim is that "a" + "b" > "c".
The claim is negated to assume that "a" + "b" ≤ "c". Squaring both sides results in ("a" + "b")2 ≤ "c"2 or, equivalently, "a"2 + 2"ab" + "b"2 ≤ "c"2. A triangle is non-degenerate if each edge has positive length, so it may be assumed that "a" and "b" are greater than 0. Therefore, "a"2 + "b"2 < "a"2 + 2"ab" + "b"2 ≤ "c"2. The transitive relation may be reduced to "a"2 + "b"2 < "c"2. It is known from the Pythagorean theorem that "a"2 + "b"2 = "c"2. This results in a contradiction since strict inequality and equality are mutually exclusive. The latter was a result of the Pythagorean theorem and the former the assumption that "a" + "b" ≤ "c". The contradiction means that it is impossible for both to be true and it is known that the Pythagorean theorem holds. It follows that the assumption that "a" + "b" ≤ "c" must be false and hence "a" + "b" > "c", proving the claim.
No least positive rational number.
Consider the proposition, "P": "there is no smallest rational number greater than 0". In a proof by contradiction, we start by assuming the opposite, ¬"P": that there "is" a smallest rational number, say, "r".
Now "r"/2 is a rational number greater than 0 and smaller than "r".
But that contradicts our initial assumption, ¬"P", that "r" was the "smallest" rational number. So we can conclude that the original proposition, "P", must be true — "there is no smallest rational number greater than 0".
Other.
For other examples, see proof that the square root of 2 is not rational (where indirect proofs different from the above one can be found) and Cantor's diagonal argument.
In mathematical logic.
In mathematical logic, the proof by contradiction is represented as:
or
In the above, "P" is the proposition we wish to prove, and "S" is a set of statements, which are the premises—these could be, for example, the axioms of the theory we are working in, or earlier theorems we can build upon. We consider "P", or the negation of "P", in addition to "S"; if this leads to a logical contradiction "F", then we can conclude that the statements in "S" lead to the negation of "P", or "P" itself, respectively.
Note that the set-theoretic union, in some contexts closely related to logical disjunction (or), is used here for sets of statements in such a way that it is more related to logical conjunction (and).
A particular kind of indirect proof assumes that some object doesn't exist, and then proves that this would lead to a contradiction; thus, such an object must exist. Although it is quite freely used in mathematical proofs, not every school of mathematical thought accepts this kind of argument as universally valid. See further Nonconstructive proof.
Notation.
Proofs by contradiction sometimes end with the word "Contradiction!". Isaac Barrow and Baermann used the notation Q.E.A., for "quod est absurdum" ("which is absurd"), along the lines of Q.E.D., but this notation is rarely used today. A graphical symbol sometimes used for contradictions is a downwards zigzag arrow "lightning" symbol (U+21AF: ↯), for example in Davey and Priestley. Others sometimes used include a pair of opposing arrows (as formula_5 or formula_6), struck-out arrows (formula_7), a stylized form of hash (such as U+2A33: ⨳), or the "reference mark" (U+203B: ※). The "up tack" symbol (U+22A5: ⊥) used by philosophers and logicians (see contradiction) also appears, but is often avoided due to its usage for orthogonality.

</doc>
<doc id="25420" url="http://en.wikipedia.org/wiki?curid=25420" title="Reversible error">
Reversible error

In law, a reversible error is an error by the trier of law (judge) or the trier of fact (the jury or the judge if it is a bench trial) or malfeasance by one of the trying attorneys, which results in an unfair trial. It is to be distinguished from harmless errors that do not rise to a level that bring the validity of the judgment into question and thus do not lead to a reversal upon appeal.
A finding of reversible error requires that one or more of the appellant's "substantial rights" be affected, or the evidence in question be of such character as to have affected the outcome of the trial. (See e.g., "Montana Petroleum Tank Release Compensation Bd. v. Crumley's, Inc.", 174 P.3d 948, Mont.,2008) The criteria for determining what constitutes a "substantial right" is somewhat vague however, being that it varies from case to case, each presenting a slightly different interpretation of which rights are essential, or significant enough to warrant this sort of legal protection. Therefore, reversible errors resulting from the violation of an individual's "substantial right(s)" must be considered on an individual basis. 
Reversible errors include, but are not limited to:
If an appellate court determines that reversible error occurred, it may reverse the judgment of the lower court and order a new trial on such terms and conditions as are found to be just. 
Technically, attorney misconduct is not reversible error. Failure of the judge to remedy it during the trial is reversible error. In cases such as unfairly or illegally concealing evidence, there is no error on the part of the court but the court's decision may still be vacated and the matter returned for a new trial, because there is no other way for justice to be granted.

</doc>
<doc id="25421" url="http://en.wikipedia.org/wiki?curid=25421" title="Rapping">
Rapping

Rapping (or emceeing, MCing, spitting bars, or rhyming) is "spoken or chanted rhyming lyrics". The components of rapping include "content", "flow" (rhythm and rhyme), and "delivery". Rapping is distinct from spoken word poetry in that it is performed in time to a beat.
Rapping is often associated with and a primary ingredient of hip hop music, but the origins of the phenomenon can be said to predate hip hop culture by centuries. It can also be found in alternative rock such as that of Cake and the Red Hot Chili Peppers. Rapping is also used in Kwaito music, a genre that originated in Johannesburg, South Africa and is composed of hip hop elements.
Rapping can be delivered over a beat or without accompaniment. Stylistically, rap occupies a gray area between speech, prose, poetry, and singing. The word (meaning originally "to hit") as used to describe quick speech or repartee predates the musical form. The word had been used in British English since the 16th century, and specifically meaning "to say" since the 18th. It was part of the African American dialect of English in the 1960s meaning "to converse", and very soon after that in its present usage as a term denoting the musical style. Today, the terms "rap" and "rapping" are so closely associated with hip hop music that many use the terms interchangeably.
History.
Etymology and usage.
The English verb "rap" has various meanings, such as "to strike, especially with a quick, smart, or light blow", as well "to utter sharply or vigorously: to rap out a command." The Shorter Oxford English Dictionary gives a date of 1541 for the first recorded use of the word with the meaning "to utter (esp. an oath) sharply, vigorously, or suddenly". Wentworth and Flexner's "Dictionary of American Slang" gives the meaning "to speak to, recognize, or acknowledge acquaintance with someone", dated 1932, and a later meaning of "to converse, esp. in an open and frank manner". It is these meanings from which the musical form of "rapping" derives, and this definition may be from a shortening of repartee. A "rapper" refers to a performer who 'raps'.
By the late 1960s, when Hubert G. Brown changed his name to H. Rap Brown, "rap" was a slang term referring to an oration or speech, such as was common among the 'hip' crowd in the protest movements, but it did not come to be associated with a musical style for another decade.
"Rap" was used to describe talking on records as early as 1971, on Isaac Hayes' album "Black Moses" with track names such as "Ike's Rap," "Ike's Rap II," "Ike's Rap III," and so on. Hayes' "husky-voiced sexy spoken 'raps' became key components in his signature sound."
Del the Funky Homosapien similarly states that "rap" was used to refer to talking in a stylistic manner in the early 1970s: "I was born in '72... back then what rapping meant, basically, was you trying to convey something—you're trying to convince somebody. That's what rapping is, it's in the way you talk."
Roots.
Rapping can be traced back to its African roots. Centuries before hip hop music existed, the griots of West Africa were delivering stories rhythmically, over drums and sparse instrumentation. Such connections have been acknowledged by many modern artists, modern day "griots", spoken word artists, mainstream news sources, and academics.
Blues music, rooted in the work songs and spirituals of slavery and influenced greatly by West African musical traditions, was first played by blacks, and later by some whites, in the Mississippi Delta region of the United States around the time of the Emancipation Proclamation. Grammy-winning blues musician/historian Elijah Wald and others have argued that the blues were being rapped as early as the 1920s. Wald went so far as to call hip hop "the living blues." Jazz, which developed from the blues and other African-American and European musical traditions and originated around the beginning of the 20th century, has also influenced hip hop and has been cited as a precursor of hip hop. Not just jazz music and lyrics but also jazz poetry. According to John Sobol, the jazz musician and poet who wrote "Digitopia Blues", rap "bears a striking resemblance to the evolution of jazz both stylistically and formally."
One of the main influences on Hip Hop artists was James Brown. James Brown is credited for inventing funk music in the middle '60s. The characteristic funk drum beat is the most common rhythm used for rap music. Two of the earliest recordings which have a funk beat and lyrics which are rhymed in rhythm over this type of beat were released by comedian Pigmeat Markham, "Here Come the Judge" which was released in 1968 by the Chess label and in 1969 another song about running numbers called "Who Got The Number?".
Precursors also exist in non-African/African-American traditions, especially in vaudeville and musical theater. One such tradition is the patter song exemplified by Gilbert and Sullivan but that has origins in earlier Italian opera. "Rock Island" from Meridith Wilson's "The Music Man" is wholly spoken by an ensemble of travelling salesmen, as are most of the numbers for British actor Rex Harrison in the 1964 Lerner and Loewe musical My Fair Lady. Glenn Miller's "The Lady's in Love with You" and "The Little Man Who Wasn't There" (both 1939), each contain distinctly rap-like sequences set to a driving beat. In musical theater, the term "vamp" is identical to its meaning in jazz, gospel, and funk, and it fulfills the same function. Semi-spoken music has long been especially popular in British entertainment, and such examples as David Croft's theme to the 1970s' sitcom "Are You Being Served?" have elements indistinguishable from modern rap. In the realm of classical music, semi-spoken music was popular stylized by composer Arnold Schoenberg as Sprechstimme, and famously used in Ernst Toch's 1924 "Geographical Fugue" for spoken chorus and the final scene in Darius Milhaud's 1915 ballet "Les Choéphores". In the French chanson field, irrigated by a strong poetry tradition, such singer-songwriters as Léo Ferré or Serge Gainsbourg made their own use of spoken word over rock or symphonic music from the very beginning of the 1970s. Although these probably did not have a direct influence on rap's development in the African American cultural sphere, they paved the way for acceptance of spoken word music in the media market.
More directly related to the African American community were items like schoolyard chants and taunts, clapping games, jump-rope rhymes, some with unwritten folk histories going back hundreds of years across many nationalities. Sometimes these items contain racially offensive lyrics. A related area that is not strictly folklore is rhythmical cheering and cheerleading for military and sports.
Proto-rap.
Art forms such as spoken word jazz poetry and comedy records had an influence on the first rappers. Coke La Rock, often credited as hip-hop's first MC cites the Last Poets among his influences, as well as comedians such as The Wild Man Steve and Richard Pryor. Comedian Rudy Ray Moore released under the counter albums in the 1960s and 1970s such as "This Pussy Belongs To Me" (1970), which contained "raunchy, sexually explicit rhymes that often had to do with pimps, prostitutes, players, and hustlers", and which later led to him being called "The Godfather of Rap".
Gil Scott-Heron, a jazz poet/musician, has been cited as an influence on rappers such as Chuck D and KRS-One. Scott-Heron himself was influenced by Melvin Van Peebles, whose first album was 1968's "Brer Soul". Van Peebles describes his vocal style as "the old Southern style", which was influenced by singers he had heard growing up in South Chicago. Van Peebles also said that he was influenced by older forms of African-American music: "[...] people like Blind Lemon Jefferson and the field hollers. I was also influenced by spoken word song styles from Germany that I encountered when I lived in France."
During the mid-20th century, the musical culture of the Caribbean was constantly influenced by the concurrent changes in American music. As early as 1956, deejays were toasting (an African tradition of "rapped out" tales of heroism) over dubbed Jamaican beats. It was called "rap", expanding the word's earlier meaning in the African-American community—"to discuss or debate informally."
The early rapping of hip-hop developed out of announcements made over the microphone at parties, and later into more complex raps. Grandmaster Caz states: "The microphone was just used for making announcements, like when the next party was gonna be, or people's moms would come to the party looking for them, and you have to announce it on the mic. Different DJs started embellishing what they were saying. I would make an announcement this way, and somebody would hear that and they add a little bit to it. I'd hear it again and take it a little step further 'til it turned from lines to sentences to paragraphs to verses to rhymes."
One of the first rappers at the beginning of the hip hop period, at the end of the 1970s, was also hip hop's first DJ, Kool Herc. Herc, a Jamaican immigrant, started delivering simple raps at his parties, which some claim were inspired by the Jamaican tradition of toasting. However, Kool Herc himself denies this link (in the 1984 book "Hip Hop"), saying, "Jamaican toasting? Naw, naw. No connection there. I couldn't play reggae in the Bronx. People wouldn't accept it. The inspiration for rap is James Brown and the album "Hustler's Convention".". Herc also suggests he was too young while in Jamaica to get into sound system parties: "I couldn’t get in. Couldn’t get in. I was ten, eleven years old," and that while in Jamaica, he was listening to James Brown: "I was listening to American music in Jamaica and my favorite artist was James Brown. That's who inspired me. A lot of the records I played were by James Brown."
By the end of the 1970s, artists such as Kurtis Blow and The Sugarhill Gang were just starting to receive radio airplay and make an impact far outside of New York City, on a national scale. "Stranger in a Strange Land" on the 1971 album Leon Russell and the Shelter People and Blondie's inclusion of a rap section in their 1981 single, "Rapture", were some of the first songs with rap to top the U.S. "Billboard" Hot 100 charts.
Old-school hip hop.
Old school rap (1979–1984) was "easily identified by its relatively simple raps" according to AllMusic, "the emphasis was not on lyrical technique, but simply on good times", one notable exception being Melle Mel, who set the way for future rappers through his socio-political content and creative wordplay.
Golden age.
Golden age hip hop (the mid '80s to early '90s) was the time period where hip-hop lyricism went through its most drastic transformation – writer William Jelani Cobb says "in these golden years, a critical mass of mic prodigies were literally creating themselves and their art form at the same time" and Allmusic writes, "rhymers like PE's Chuck D, Big Daddy Kane, KRS-One, and Rakim basically invented the complex wordplay and lyrical kung-fu of later hip-hop”. The golden age is considered to have ended around '93–'94, marking the end of rap lyricism's most innovative period.
Flow.
"Flow" is defined as "the rhythms and rhymes" of a hip-hop song's lyrics and how they interact – the book "How to Rap" breaks flow down into rhyme, rhyme schemes, and rhythm (also known as cadence). 'Flow' is also sometimes used to refer to elements of the delivery (pitch, timbre, volume) as well, though often a distinction is made between the flow and the delivery.
Staying on the beat is central to rap's flow – many MCs note the importance of staying on-beat in "How to Rap" including Sean Price, Mighty Casey, Zion I, Vinnie Paz, Fredro Starr, Del The Funky Homosapien, Tech N9ne, People Under The Stairs, Twista, B-Real, Mr Lif, 2Mex, and Cage.
MCs stay on beat by stressing syllables in time to the four beats of the musical backdrop. Poetry scholar Derek Attridge describes how this works in his book "Poetic Rhythm" – "rap lyrics are written to be performed to an accompaniment that emphasizes the metrical structure of the verse". He says rap lyrics are made up of, "lines with four stressed beats, separated by other syllables that may vary in number and may include other stressed syllables. The strong beat of the accompaniment coincides with the stressed beats of the verse, and the rapper organizes the rhythms of the intervening syllables to provide variety and surprise".
The same technique is also noted in the book "How to Rap", where diagrams are used to show how the lyrics line up with the beat – "stressing a syllable on each of the four beats gives the lyrics the same underlying rhythmic pulse as the music and keeps them in rhythm... other syllables in the song may still be stressed, but the ones that fall in time with the four beats of a bar are the only ones that need to be emphasized in order to keep the lyrics in time with the music".
History of flow.
Old school flows were relatively basic and used only few syllables per bar, simple rhythmic patterns, and basic rhyming techniques and rhyme schemes.
Melle Mel is cited as an MC who epitomizes the old school flow – Kool Moe Dee says, "from 1970 to 1978 we rhymed one way [then] Melle Mel, in 1978, gave us the new cadence we would use from 1978 to 1986." he's the first emcee to explode in a new rhyme cadence, and change the way every emcee rhymed forever. Rakim, The Notorious B.I.G., and Eminem have flipped the flow, but Melle Mel's downbeat on the two, four, kick to snare cadence is still the rhyme foundation all emcees are building on".
Artists and critics often credit Rakim with creating the overall shift from the more simplistic old school flows to more complex flows near the beginning of hip hop's new school – Kool Moe Dee says, "any emcee that came after 1986 had to study Rakim just to know what to be able to do. Rakim, in 1986, gave us flow and that was the rhyme style from 1986 to 1994. from that point on, anybody emceeing was forced to focus on their flow". Kool Moe Dee explains that before Rakim, the term 'flow' wasn't widely used – "Rakim is basically the inventor of flow. We were not even using the word flow until Rakim came along. It was called rhyming, it was called cadence, but it wasn't called flow. Rakim created flow!" He adds that while Rakim upgraded and popularized the focus on flow, "he didn't invent the word".
Kool Moe Dee states that Biggie introduced a newer flow which "dominated from 1994 to 2002", and also says that Method Man was "one of the emcees from the early to mid-'90s that ushered in the era of flow... Rakim invented it, Big Daddy Kane, KRS-One, and Kool G Rap expanded it, but Biggie and Method Man made flow the single most important aspect of an emcee's game". He also cites Craig Mack as an artist who contributed to developing flow in the '90s.
Music scholar Adam Krims says, "the flow of MCs is one of the profoundest changes that separates out new-sounding from older-sounding music... it is widely recognized and remarked that rhythmic styles of many commercially successful MCs since roughly the beginning of the 1990s have progressively become faster and more 'complex'". He cites "members of the Wu-Tang Clan, Nas, AZ, Big Pun, and Ras Kass, just to name a few" as artists who exemplify this progression.
Kool Moe Dee adds, "in 2002 Eminem created the song that got the first Oscar in Hip-Hop history [Lose Yourself]... and I would have to say that his flow is the most dominant right now (2003)".
Styles.
There are many different styles of flow, with different terminology used by different people – stic.man of Dead Prez uses the following terms –
Alternatively, music scholar Adam Krims uses the following terms –
Rhyme.
MCs use many different rhyming techniques, including complex rhyme schemes, as Adam Krims points out – "the complexity... involves multiple rhymes in the same rhyme complex (i.e. section with consistently rhyming words), internal rhymes, [and] offbeat rhymes". There is also widespread use of multisyllabic rhymes, by artists such as Kool G Rap, Big Daddy Kane, Rakim, Big L, Nas and Eminem.
It has been noted that rap's use of rhyme is some of the most advanced in all forms of poetry – music scholar Adam Bradley notes, "rap rhymes so much and with such variety that it is now the largest and richest contemporary archive of rhymed words. It has done more than any other art form in recent history to expand rhyme's formal range and expressive possibilities".
In the book "How to Rap", Masta Ace explains how Rakim and Big Daddy Kane caused a shift in the way MCs rhymed: "Up until Rakim, everybody who you heard rhyme, the last word in the sentence was the rhyming [word], the connection word. Then Rakim showed us that you could put rhymes within a rhyme... now here comes Big Daddy Kane — instead of going three words, he's going multiple". "How to Rap" explains that "rhyme is often thought to be the most important factor in rap writing... rhyme is what gives rap lyrics their musicality.
Rhythm.
Many of the rhythmic techniques used in rapping come from percussive techniques and many rappers compare themselves to percussionists. "How to Rap 2" identifies all the rhythmic techniques used in rapping such as triplets, flams, 16th notes, 32nd notes, syncopation, extensive use of rests, and rhythmic techniques unique to rapping such as West Coast "lazy tails," coined by Shock G. Rapping has also been done in various time signatures, such as 3/4 time.
Since the 2000s, rapping has evolved into a style of rap that spills over the boundaries of the beat, closely resembling spoken English. Rappers like MF Doom and Eminem have exhibited this style, and since then, rapping has been difficult to notate. The American Hip Hop Group Crime Mob exhibited a new rap flow in songs like "Knuck If You Buck", heavily dependent on triplets. Rappers including Drake, Kanye West, Rick Ross, Young Jeezy and more have included this influence in their music. In 2014, an American Hip Hop collective from Atlanta, Migos, popularized this flow, and is commonly referred to as the "Migos Flow" (a term that retains contention within the Hip Hop community). 
Rap notation and flow diagrams.
The standard form of rap notation is the flow diagram, where rappers line-up their lyrics underneath "beat numbers". Different rappers have slightly different forms of flow diagram that they use: Del the Funky Homosapien says, "I'm just writing out the rhythm of the flow, basically. Even if it's just slashes to represent the beats, that's enough to give me a visual path.", Vinnie Paz states, "I've created my own sort of writing technique, like little marks and asterisks to show like a pause or emphasis on words in certain places.", and Aesop Rock says, "I have a system of maybe 10 little symbols that I use on paper that tell me to do something when I’m recording."
Hip-hop scholars also make use of the same flow diagrams: the books "How to Rap" and "How to Rap 2" use the diagrams to explain rap's triplets, flams, rests, rhyme schemes, runs of rhyme, and breaking rhyme patterns, among other techniques. Similar systems are used by PhD musicologists Adam Krims in his book "Rap Music and the Poetics of Identity" and Kyle Adams in his academic work on flow.
Because rap revolves around a strong 4/4 beat, with certain syllables said in time to the beat, all the notational systems have a similar structure: they all have the same 4 beat numbers at the top of the diagram, so that syllables can be written in-line with the beat numbers. This allows devices such as rests, "lazy tails", flams, and other rhythmic techniques to be shown, as well as illustrating where different rhyming words fall in relation to the music.
Delivery/performance.
To successfully deliver a rap, a rapper must also develop vocal presence, enunciation, and breath control. Vocal presence is the distinctiveness of a rapper's voice on record. Enunciation is essential to a flowing rap; some rappers choose also to exaggerate it for comic and artistic effect. Breath control, taking in air without interrupting one's delivery, is an important skill for a rapper to master, and a must for any MC. An MC with poor breath control cannot deliver difficult verses without making unintentional pauses.
Raps are sometimes delivered with melody. West Coast rapper Egyptian Lover was the first notable MC to deliver "sing-raps." Popular rappers such as 50 Cent and Ja Rule add a slight melody to their otherwise purely percussive raps whereas some rappers such as Cee-Lo Green are able to harmonize their raps with the beat. The Midwestern group Bone Thugs-n-Harmony was one of the first groups to achieve nationwide recognition for using the fast-paced, melodic and harmonic raps that are also practiced by Do or Die, another Midwestern group. Another rapper that harmonized his rhymes was Nate Dogg, a rapper part of the group 213. Rakim experimented not only with following the beat, but also with complementing the song's melody with his own voice, making his flow sound like that of an instrument (a saxophone in particular).
The ability to rap quickly and clearly is sometimes regarded as an important sign of skill. In certain hip hop subgenres such as chopped and screwed, slow-paced rapping is often considered optimal. The current record for fastest rapper is held by Spanish rapper Domingo Edjang Moreno, known by his alias Chojin, who rapped 921 syllables in one minute on December 23, 2008.
Emcees.
In the late 1970s, the term Emcee, MC or M.C. became an alternative title for a rapper, and for their role within hip hop music and culture. An MC uses rhyming verses, pre-written or ad lib ('freestyled'), to introduce the DJ with whom they work, to keep the crowd entertained or to glorify themselves. As hip hop progressed, the title MC acquired backronyms such as 'mike chanter' 'microphone controller', 'microphone checker', 'music commentator', and one who 'moves the crowd'. A recent neologistic acronym, gaining use, is 'mentor to child'. Some use this word interchangeably with the term "rapper", while for others the term denotes a superior level of skill and connection to the wider culture.
MC can often be used as a term of distinction; referring to an artist with good performance skills. As Kool G Rap notes, "masters of ceremony, where the word 'M.C.' comes from, means just keeping the party alive" [sic]. Many people in hiphop including DJ Premier and KRS-One feel that James Brown was the first MC. James Brown had the lyrics, moves, and soul that greatly influenced a lot of rappers in Hip-Hop, and arguably even started the first MC rhyme.
For some rappers, there was a distinction to the term, such as for MC Hammer who acquired the nickname "MC" for being a "Master of Ceremonies" which he used when he began performing at various clubs while on the road with the Oakland A's and eventually in the military (United States Navy). It was within the lyrics of a rap song called "This Wall" that Hammer first identified himself as M.C. Hammer and later marketed it on his debut album "Feel My Power".
Uncertainty over the acronym's expansion may be considered evidence for its ubiquity: the full term "Master of Ceremonies" is very rarely used in the hip hop scene. This confusion prompted the hip hop group A Tribe Called Quest to include this statement in the liner notes to their 1993 album "Midnight Marauders:
The use of the term MC when referring to a rhyming wordsmith originates from the dance halls of Jamaica. At each event, there would be a master of ceremonies who would introduce the different musical acts and would say a toast in style of a rhyme, directed at the audience and to the performers. He would also make announcements such as the schedule of other events or advertisements from local sponsors. The term MC continued to be used by the children of women who moved to New York City to work as maids in the 1970s. These MCs eventually created a new style of music called hip-hop based on the rhyming they used to do in Jamaica and the breakbeats used in records. MC has also recently been accepted to refer to all who engineer music.
Subject matter.
"Party rhymes", meant to pump up the crowd at a party, were nearly the exclusive focus of old school hip hop, and they remain a staple of hip hop music to this day. In addition to party raps, rappers also tend to make references to love and sex. Love raps were first popularized by Spoonie Gee of the Treacherous Three, and later, in the golden age of hip hop, Big Daddy Kane, Heavy D, and LL Cool J would continue this tradition.
Hip hop artists such as KRS-One, Hopsin, Public Enemy, Lupe Fiasco, Mos Def, Talib Kweli, Jay-Z, Nas, The Notorious B.I.G. (Biggie), and dead prez are known for their sociopolitical subject matter. Their West Coast counterparts include Emcee Lynx, The Coup, Paris, and Michael Franti. Tupac Shakur was also known for rapping about social issues such as police brutality, teenage pregnancy, and racism.
Other rappers take a less critical approach to urbanity, sometimes even embracing such aspects as crime. Schoolly D was the first notable MC to rap about crime. Early on KRS-One was accused of celebrating crime and a hedonistic lifestyle, but after the death of his DJ, Scott La Rock, KRS-One went on to speak out against violence in hip hop and has spent the majority of his career condemning violence and writing on issues of race and class. Ice-T was one of the first rappers to call himself a "playa" and discuss guns on record, but his theme tune to the 1988 film "Colors" contained warnings against joining gangs. Gangsta rap, made popular largely because of N.W.A, brought rapping about crime and the gangster lifestyle into the musical mainstream.
Materialism has also been a popular topic in hip-hop since at least the early 1990s, with rappers boasting about their own wealth and possessions, and name-dropping specific brands: liquor brands Cristal and Rémy Martin, car manufacturers Bentley and Mercedes-Benz and clothing brands Gucci and Versace have all been popular subjects for rappers.
Various politicians, journalists, and religious leaders have accused rappers of fostering a culture of violence and hedonism among hip hop listeners through their lyrics. However, there are also rappers whose messages may not be in conflict with these views, for example Christian hip hop. Others have praised the "political critique, innuendo and sarcasm" of hip hop music.
In contrast to the more hedonistic approach of gangsta rappers, some rappers have a spiritual or religious focus. Christian rap is currently the most commercially successful form of religious rap. With Christian rappers like Lecrae, Thi'sl and Hostyle Gospel winning national awards and making regular appearances on Television, Christian Hip Hop seem to have found its way in the hip hop family. Aside from Christianity, the Five Percent Nation, an Islamic esotericist religious/spiritual group, has been represented more than any religious group in popular hip hop. Artists such as Rakim, the members of the Wu-Tang Clan, Brand Nubian, X-Clan and Busta Rhymes have had success in spreading the theology of the Five Percenters.
Literary technique.
Rappers use the literary techniques of double entendres, alliteration, and other forms of wordplay that are also found in classical poetry. Similes and metaphors are used extensively in rap lyrics; rappers such as Fabolous and Lloyd Banks have written entire songs in which every line contains similes, whereas MCs like Rakim, GZA, and Jay-Z are known for the metaphorical content of their raps. Rappers such as Lupe Fiasco are known for the complexity of their songs that contain metaphors within extended metaphors.
Diction and dialect.
Many hip hop listeners believe that a rapper's lyrics are enhanced by a complex vocabulary. Kool Moe Dee claims that he appealed to older audiences by using a complex vocabulary in his raps. Rap is famous, however, for having its own vocabulary—from international hip hop slang to regional slang. Some artists, like the Wu-Tang Clan, develop an entire lexicon among their clique. African American Vernacular English has always had a significant effect on hip hop slang and vice versa. Certain regions have introduced their unique regional slang to hip hop culture, such as the Bay Area (Mac Dre, E-40), Houston (Chamillionaire, Paul Wall), Atlanta (Ludacris, Lil Jon, T.I.), and Kentucky (Nappy Roots). The Nation of Gods and Earths, aka The Five Percenters, has influenced mainstream hip hop slang with the introduction of phrases such as "word is bond" that have since lost much of their original spiritual meaning. Preference toward one or the other has much to do with the individual; GZA, for example, prides himself on being very visual and metaphorical but also succinct, whereas underground rapper MF DOOM is known for heaping similes upon similes. In still another variation, 2Pac was known for saying exactly what he meant, literally and clearly.
Freestyle and battle.
There are two kinds of freestyle rap: one is scripted (recitation), but having no particular overriding subject matter, the second typically referred to as "freestyling" or "spitting", is the improvisation of rapped lyrics. When freestyling, some rappers inadvertently reuse old lines, or even "cheat" by preparing segments or entire verses in advance. Therefore, freestyles with proven spontaneity are valued above generic, always usable lines. Rappers will often reference places or objects in their immediate setting, or specific (usually demeaning) characteristics of opponents, to prove their authenticity and originality.
Battle rapping, which can be freestyled, is the competition between two or more rappers in front of an audience. The tradition of insulting one's friends or acquaintances in rhyme goes back to the dozens, and was portrayed famously by Muhammad Ali in his boxing matches. The winner of a battle is decided by the crowd and/or preselected judges. According to Kool Moe Dee, a successful battle rap focuses on an opponent's weaknesses, rather than one's own strengths. Television shows such as MTV's "DFX" and BET's "106 and Park" host weekly freestyle battles live on the air. Battle rapping gained widespread public recognition outside of the African-American community with rapper Eminem's movie, "8 Mile."
The strongest battle rappers will generally perform their rap fully freestyled. This is the most effective form in a battle as the rapper can comment on the other person, whether it be what they look like, or how they talk, or what they wear. It also allows the rapper to reverse a line used to "diss" him or her if they are the second rapper to battle. This is known as a 'flip'. Jin The Emcee was considered 'World Champion' battle rapper in the mid-2000s.
Social impact.
Derivatives and influence.
Throughout hip hop's history, new musical styles and genres have developed that contain rapping. Entire genres, such as rap rock and its derivatives rapcore and rap metal (rock/metal/punk with rapped vocals), or hip house have resulted from the fusion of rap and other styles. Many popular music genres with a focus on percussion have contained rapping at some point; be it disco (DJ Hollywood), jazz (Gang Starr), new wave (Blondie), funk (Fatback Band), contemporary R&B (Mary J. Blige), reggaeton (Daddy Yankee), or even Japanese dance music (Soul'd Out). UK garage music has begun to focus increasingly on rappers in a new subgenre called grime, pioneered and popularized by the MC Dizzee Rascal. Increased popularity with the music has shown more UK rappers going to America as well as tour there, such as Sway DaSafo possibly signing with Akon's label Konvict. Hyphy is the latest of these spin-offs. It is typified by slowed-down atonal vocals with instrumentals that borrow heavily from the hip hop scene and lyrics centered on illegal street racing and car culture. Another Oakland, California group, Beltaine's Fire, has recently gained attention for their Celtic fusion sound which blends hip hop beats with Celtic melodies. Unlike the majority of hip hop artists, all their music is performed live without samples, synths, or drum machines, drawing comparisons to The Roots and Rage Against the Machine.
Bhangra, a widely popular style of music from Punjab, India has been mixed numerous times with reggae and hip hop music. The most popular song in this genre in the United States was "Mundian to Bach Ke" or "Beware the Boys" by Panjabi MC and Jay-Z. Although "Mundian To Bach Ke" had been released previously, the mixing with Jay-Z popularized the genre further.
Though the majority of rappers are male, there have been a number of female rap stars, including Iggy Azalea, Lauryn Hill, MC Lyte, Lil' Kim, Missy Elliott, Queen Latifah, Da Brat, Eve, Trina, Nicki Minaj, Khia, M.I.A., CL from 2NE1, Foxy Brown, and Lisa Lopes from TLC. There is also deaf rap artist Signmark.
References.
</dl>

</doc>
<doc id="25422" url="http://en.wikipedia.org/wiki?curid=25422" title="Deaths in 2003">
Deaths in 2003

The following is a list of notable deaths in 2003. Names are listed under the date of death and not the date it was announced. Names under each date are listed in alphabetical order by family name.
A typical entry lists information in the following sequence:

</doc>
<doc id="25423" url="http://en.wikipedia.org/wiki?curid=25423" title="Rock music">
Rock music

Rock music is a genre of popular music that originated as "rock and roll" in the United States in the 1950s, and developed into a range of different styles in the 1960s and later, particularly in the United Kingdom and the United States. It has its roots in 1940s' and 1950s' rock and roll, itself heavily influenced by rhythm and blues and country music. Rock music also drew strongly on a number of other genres such as blues and folk, and incorporated influences from jazz, classical and other musical sources.
Musically, rock has centered on the electric guitar, usually as part of a rock group with electric bass guitar and drums. Typically, rock is song-based music usually with a 4/4 time signature using a verse-chorus form, but the genre has become extremely diverse. Like pop music, lyrics often stress romantic love but also address a wide variety of other themes that are frequently social or political in emphasis. The dominance of rock by white, male musicians has been seen as one of the key factors shaping the themes explored in rock music. Rock places a higher degree of emphasis on musicianship, live performance, and an ideology of authenticity than pop music.
By the late 1960s, referred to as the "golden age" or "classic rock" period, a number of distinct rock music subgenres had emerged, including hybrids like blues rock, folk rock, country rock, raga rock, and jazz-rock fusion, many of which contributed to the development of psychedelic rock, which was influenced by the countercultural psychedelic scene. New genres that emerged from this scene included progressive rock, which extended the artistic elements; glam rock, which highlighted showmanship and visual style; and the diverse and enduring subgenre of heavy metal, which emphasized volume, power, and speed. In the second half of the 1970s, punk rock reacted against the perceived overblown, inauthentic and overly mainstream aspects of these genres to produce a stripped-down, energetic form of music valuing raw expression and often lyrically characterised by social and political critiques. Punk was an influence into the 1980s on the subsequent development of other subgenres, including new wave, post-punk and eventually the alternative rock movement. From the 1990s alternative rock began to dominate rock music and break through into the mainstream in the form of grunge, Britpop, and indie rock. Further fusion subgenres have since emerged, including pop punk, rap rock, and rap metal, as well as conscious attempts to revisit rock's history, including the garage rock/post-punk and synthpop revivals at the beginning of the new millennium.
Rock music has also embodied and served as the vehicle for cultural and social movements, leading to major sub-cultures including mods and rockers in the UK and the hippie counterculture that spread out from San Francisco in the US in the 1960s. Similarly, 1970s punk culture spawned the visually distinctive goth and emo subcultures. Inheriting the folk tradition of the protest song, rock music has been associated with political activism as well as changes in social attitudes to race, sex and drug use, and is often seen as an expression of youth revolt against adult consumerism and conformity.
Characteristics.
The sound of rock is traditionally centered on the electric guitar, which emerged in its modern form in the 1950s with the popularization of rock and roll. The sound of an electric guitar in rock music is typically supported by an electric bass guitar pioneered in jazz music in the same era, and percussion produced from a drum kit that combines drums and cymbals. This trio of instruments has often been complemented by the inclusion of others, particularly keyboards such as the piano, Hammond organ and synthesizers. A group of musicians performing rock music is termed a rock band or rock group and typically consists of between two and five members. Classically, a rock band takes the form of a quartet whose members cover one or more roles, including vocalist, lead guitarist, rhythm guitarist, bass guitarist, drummer and often that of keyboard player or other instrumentalist.
Rock music is traditionally built on a foundation of simple unsyncopated rhythms in a 4/4 meter, with a repetitive snare drum back beat on beats two and four. Melodies are often derived from older musical modes, including the Dorian and Mixolydian, as well as major and minor modes. Harmonies range from the common triad to parallel fourths and fifths and dissonant harmonic progressions. Rock songs from the mid-1960s onwards often used the verse-chorus structure derived from blues and folk music, but there has been considerable variation from this model. Critics have stressed the eclecticism and stylistic diversity of rock. Because of its complex history and tendency to borrow from other musical and cultural forms, it has been argued that "it is impossible to bind rock music to a rigidly delineated musical definition."
Unlike many earlier styles of popular music, rock lyrics have dealt with a wide range of themes in addition to romantic love: including sex, rebellion against "The Establishment", social concerns and life styles. These themes were inherited from a variety of sources, including the Tin Pan Alley pop tradition, folk music and rhythm and blues. Music journalist Robert Christgau characterizes rock lyrics as a "cool medium" with simple diction and repeated refrains, and asserts that rock's primary "function" "pertains to music, or, more generally, noise." The predominance of white, male and often middle class musicians in rock music has often been noted and rock has been seen as an appropriation of black musical forms for a young, white and largely male audience. As a result it has been seen as articulating the concerns of this group in both style and lyrics.
Since the term rock began to be used in preference to rock and roll from the late-1960s, it has often been contrasted with pop music, with which it has shared many characteristics, but from which it is often distanced by an emphasis on musicianship, live performance and a focus on serious and progressive themes as part of an ideology of authenticity that is frequently combined with an awareness of the genre's history and development. According to Simon Frith "rock was something more than pop, something more than rock and roll. Rock musicians combined an emphasis on skill and technique with the romantic concept of art as artistic expression, original and sincere". In the new millennium the term "rock" has sometimes been used as a blanket term including forms such as pop music, reggae music, soul music, and even hip hop, with which it has been influenced but often contrasted through much of its history.
Origins.
Rock and roll.
The foundations of rock music are in rock and roll, which originated in the United States during the late 1940s and early 1950s, and quickly spread to much of the rest of the world. Its immediate origins lay in a melding of various black musical genres of the time, including rhythm and blues and gospel music, with country and western. In 1951, Cleveland, Ohio disc jockey Alan Freed began playing rhythm and blues music for a multi-racial audience, and is credited with first using the phrase "rock and roll" to describe the music.
Debate surrounds which record should be considered the first rock and roll record. Contenders include Goree Carter's "" (1949); Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952; and "Rocket 88" by Jackie Brenston and his Delta Cats (in fact, Ike Turner and his band the Kings of Rhythm), recorded by Sam Phillips for Sun Records in 1951. Four years later, Bill Haley's "Rock Around the Clock" (1955) became the first rock and roll song to top "Billboard" magazine's main sales and airplay charts, and opened the door worldwide for this new wave of popular culture.
It has been argued that "That's All Right (Mama)" (1954), Elvis Presley's first single for Sun Records in Memphis, was the first rock and roll record, but, at the same time, Big Joe Turner's "Shake, Rattle & Roll", later covered by Haley, was already at the top of the Billboard R&B charts. Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent. Soon rock and roll was the major force in American record sales and crooners, such as Eddie Fisher, Perry Como, and Patti Page, who had dominated the previous decade of popular music, found their access to the pop charts significantly curtailed.
Rock and roll has been seen as leading to a number of distinct subgenres, including rockabilly, combining rock and roll with "hillbilly" country music, which was usually played and recorded in the mid-1950s by white singers such as Carl Perkins, Jerry Lee Lewis, Buddy Holly and with the greatest commercial success, Elvis Presley. In contrast doo wop placed an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation and had its origins in 1930s and '40s African American vocal groups. Acts like the Crows, the Penguins, the El Dorados and the Turbans all scored major hits, and groups like the Platters, with songs including "The Great Pretender" (1955), and the Coasters with humorous songs like "Yakety Yak" (1958), ranked among the most successful rock and roll acts of the period.
The era also saw the growth in popularity of the electric guitar, and the development of a specifically rock and roll style of playing through such exponents as Chuck Berry, Link Wray, and Scotty Moore. The use of distortion, pioneered by electric blues guitarists such as Guitar Slim, Willie Johnson and Pat Hare in the early 1950s, was popularized by Chuck Berry in the mid-1950s. The use of power chords, pioneered by Willie Johnson and Pat Hare in the early 1950s, was popularized by Link Wray in the late 1950s.
In the United Kingdom, the trad jazz and folk movements brought visiting blues music artists to Britain. Lonnie Donegan's 1955 hit "Rock Island Line" was a major influence and helped to develop the trend of skiffle music groups throughout the country, many of which, including John Lennon's Quarrymen, moved on to play rock and roll.
Commentators have traditionally perceived a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, The Big Bopper and Richie Valens in a plane crash, the departure of Elvis for the army, the retirement of Little Richard to become a preacher, prosecutions of Jerry Lee Lewis and Chuck Berry and the breaking of the payola scandal (which implicated major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs), gave a sense that the rock and roll era established at that point had come to an end.
"In-between years".
The period of the later 1950s and early 1960s, between the end of the initial period of innovation and what became known in the US as the "British Invasion", has traditionally been seen as an era of hiatus for rock and roll. More recently some authors have emphasised important innovations and trends in this period without which future developments would not have been possible. While early rock and roll, particularly through the advent of rockabilly, saw the greatest commercial success for male and white performers, in this era the genre was dominated by black and female artists. Rock and roll had not disappeared at the end of the 1950s and some of its energy can be seen in the Twist dance craze of the early 60s, mainly benefiting the career of Chubby Checker. Having died down in the late 1950s, doo wop enjoyed a revival in the same period, with hits for acts like the Marcels, the Capris, Maurice Williams and the Zodiacs, and Shep and the Limelights. The rise of girl groups like the Chantels, the Shirelles and the Crystals placed an emphasis on harmonies and polished production that was in contrast to earlier rock and roll. Some of the most significant girl group hits were products of the Brill Building Sound, named after the block in New York where many songwriters were based, which included the number 1 hit for the Shirelles "Will You Love Me Tomorrow" in 1960, penned by the partnership of Gerry Goffin and Carole King.
Cliff Richard had the first British rock and roll hit with "Move It", effectively ushering in the sound of British rock. At the start of the 1960s, his backing group the Shadows was the most successful group recording instrumentals. While rock 'n' roll was fading into lightweight pop and ballads, British rock groups at clubs and local dances, heavily influenced by blues-rock pioneers like Alexis Korner, were starting to play with an intensity and drive seldom found in white American acts.
Also significant was the advent of soul music as a major commercial force. Developing out of rhythm and blues with a re-injection of gospel music and pop, led by pioneers like Ray Charles and Sam Cooke from the mid-1950s, by the early 60s figures like Marvin Gaye, James Brown, Aretha Franklin, Curtis Mayfield and Stevie Wonder were dominating the R&B charts and breaking through into the main pop charts, helping to accelerate their desegregation, while Motown and Stax/Volt Records were becoming major forces in the record industry. All of these elements, including the close harmonies of doo wop and girl groups, the carefully crafted song-writing of the Brill Building Sound and the polished production values of soul, have been seen as influencing the Merseybeat sound, particularly the early work of The Beatles, and through them the form of later rock music. Some historians of music have also pointed to important and innovative technical developments that built on rock and roll in this period, including the electronic treatment of sound by such innovators as Joe Meek, and the elaborate production methods of the Wall of Sound pursued by Phil Spector.
Surf music.
The instrumental rock and roll of performers such as Duane Eddy, Link Wray and the Ventures was developed by Dick Dale, who added distinctive "wet" reverb, rapid alternate picking, and Middle Eastern and Mexican influences. He produced the regional hit "Let's Go Trippin'" in 1961 and launched the surf music craze, following up with songs like "Misirlou" (1962). Like Dale and his Del-Tones, most early surf bands were formed in Southern California, including the Bel-Airs, the Challengers, and Eddie & the Showmen. The Chantays scored a top ten national hit with "Pipeline" in 1963 and probably the best known surf tune was 1963's "Wipe Out", by the Surfaris, which hit number 2 and number 10 on the Billboard charts in 1965.
Groups which crossed over to this genre included the Astronauts, from Boulder, Colorado; the Trashmen, from Minneapolis, Minnesota, who had a number 4 hit with "Surfin Bird" in 1964; and the Rivieras from South Bend, Indiana, who reached number 5 in 1964 with "California Sun". The Atlantics, from Sydney, made a significant contribution to the genre, with their hit "Bombora" (1963). European instrumental bands around this time generally focused more on the more rock and roll style played by The Shadows, but the Dakotas, who were the British backing band for Merseybeat singer Billy J. Kramer, gained some attention as surf musicians with "Cruel Sea" (1963), which was later covered by American instrumental surf bands, including the Ventures.
Surf music achieved its greatest commercial success as vocal music, particularly the work of the Beach Boys, formed in 1961 in Southern California. Their early albums included both instrumental surf rock (among them covers of music by Dick Dale) and vocal songs, drawing on rock and roll and doo wop and the close harmonies of vocal pop acts like the Four Freshmen. Their first chart hit, "Surfin'" in 1962 reached the Billboard top 100 and helped make the surf music craze a national phenomenon. From 1963 the group began to leave surfing behind as subject matter as Brian Wilson became their major composer and producer, moving on to the more general themes of male adolescence including cars and girls in songs like "Fun, Fun, Fun" (1964) and "California Girls" (1965). Other vocal surf acts followed, including one-hit wonders like Ronny & the Daytonas with "G. T. O." (1964) and Rip Chords with "Hey Little Cobra", which both reached the top ten, but the only other act to achieve sustained success with the formula were Jan & Dean, who had a number 1 hit with "Surf City" (co-written with Brian Wilson) in 1963. The surf music craze and the careers of almost all surf acts was effectively ended by the arrival of the British Invasion from 1964. Only the Beach Boys were able to sustain a creative career into the mid-1960s, producing a string of hit singles and albums, including the highly regarded "Pet Sounds" in 1966, which made them, arguably, the only American rock or pop act that could rival The Beatles.
Golden age.
British Invasion.
By the end of 1962, what would become the British rock scene had started with beat groups like the Beatles, Gerry & the Pacemakers and the Searchers from Liverpool and Freddie and the Dreamers, Herman's Hermits and the Hollies from Manchester. They drew on a wide range of American influences including soul, rhythm and blues and surf music, initially reinterpreting standard American tunes and playing for dancers. Bands like the Animals from Newcastle and Them from Belfast, and particularly those from London like the Rolling Stones and the Yardbirds, were much more directly influenced by rhythm and blues and later blues music. Soon these groups were composing their own material, combining US forms of music and infusing it with a high energy beat. Beat bands tended towards "bouncy, irresistible melodies", while early British rhythm and blues acts tended towards less sexually innocent, more aggressive songs, often adopting an anti-establishment stance. There was, however, particularly in the early stages, considerable musical crossover between the two tendencies. By 1963, led by the Beatles, beat groups had begun to achieve national success in Britain, soon to be followed into the charts by the more rhythm and blues focused acts.
"I Want to Hold Your Hand" was the Beatles' first number 1 hit on the "Billboard" Hot 100, spending 7 weeks at the top and a total of 15 weeks on the chart. Their first appearance on "The Ed Sullivan Show" on 9 February 1964, drawing an estimated 73 million viewers (at the time a record for an American television program) is often considered a milestone in American pop culture. The Beatles went on to become the biggest selling rock band of all time and they were followed into the US charts by numerous British bands. During the next two years British acts dominated their own and the US charts with Peter and Gordon, the Animals, Manfred Mann, Petula Clark, Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits, the Rolling Stones, the Troggs, and Donovan all having one or more number 1 singles. Other major acts that were part of the invasion included the Kinks and the Dave Clark Five.
The British Invasion helped internationalize the production of rock and roll, opening the door for subsequent British (and Irish) performers to achieve international success. In America it arguably spelled the end of instrumental surf music, vocal girl groups and (for a time) the teen idols, that had dominated the American charts in the late 1950s and 60s. It dented the careers of established R&B acts like Fats Domino and Chubby Checker and even temporarily derailed the chart success of surviving rock and roll acts, including Elvis. The British Invasion also played a major part in the rise of a distinct genre of rock music, and cemented the primacy of the rock group, based on guitars and drums and producing their own material as singer-songwriters.
Garage rock.
Garage rock was a raw form of rock music, particularly prevalent in North America in the mid-1960s and so called because of the perception that it was rehearsed in a suburban family garage. Garage rock songs revolved around the traumas of high school life, with songs about "lying girls" being particularly common. The lyrics and delivery were more aggressive than was common at the time, often with growled or shouted vocals that dissolved into incoherent screaming. They ranged from crude one-chord music (like the Seeds) to near-studio musician quality (including the Knickerbockers, the Remains, and the Fifth Estate). There were also regional variations in many parts of the country with flourishing scenes particularly in California and Texas. The Pacific Northwest states of Washington and Oregon had perhaps the most defined regional sound.
The style had been evolving from regional scenes as early as 1958. "Tall Cool One" (1959) by The Wailers and "Louie Louie" by the Kingsmen (1963) are mainstream examples of the genre in its formative stages. By 1963, garage band singles were creeping into the national charts in greater numbers, including Paul Revere and the Raiders (Boise), the Trashmen (Minneapolis) and the Rivieras (South Bend, Indiana). Other influential garage bands, such as the Sonics (Tacoma, Washington), never reached the "Billboard" Hot 100. In this early period many bands were heavily influenced by surf rock and there was a cross-pollination between garage rock and frat rock, sometimes viewed as merely a subgenre of garage rock.
The British Invasion of 1964–66 greatly influenced garage bands, providing them with a national audience, leading many (often surf or hot rod groups) to adopt a British influence, and encouraging many more groups to form. Thousands of garage bands were extant in the US and Canada during the era and hundreds produced regional hits. Examples include: "The Witch" by Tacoma's the Sonics (1965), "Where You Gonna Go" by Detroit's Unrelated Segments (1967), "Girl I Got News for You" by Miami's Birdwatchers (1966) and "1–2–5" by Montreal's the Haunted. Despite scores of bands being signed to major or large regional labels, most were commercial failures. It is generally agreed that garage rock peaked both commercially and artistically around 1966. By 1968 the style largely disappeared from the national charts and at the local level as amateur musicians faced college, work or the draft. New styles had evolved to replace garage rock (including blues rock, progressive rock and country rock). In Detroit, garage rock's legacy remained alive into the early 1970s, with bands such as the MC5 and the Stooges, who employed a much more aggressive approach to the form. These bands began to be labelled punk rock and are now often seen as proto-punk or proto-hard rock.
Pop rock.
The term "pop" has been used since the early 20th century to refer to popular music in general, but from the mid-1950s it began to be used for a distinct genre, aimed at a youth market, often characterized as a softer alternative to rock and roll. In the aftermath of the British Invasion, from about 1967, it was increasingly used in opposition to the term rock music, to describe a form that was more commercial, ephemeral and accessible. In contrast rock music was seen as focusing on extended works, particularly albums, was often associated with particular sub-cultures (like the counterculture of the 1960s), placed an emphasis on artistic values and "authenticity", stressed live performance and instrumental or vocal virtuosity and was often seen as encapsulating progressive developments rather than simply reflecting existing trends.
Nevertheless much pop and rock music has been very similar in sound, instrumentation and even lyrical content. The terms "pop-rock" and "power pop" have been used to describe more commercially successful music that uses elements from, or the form of, rock music. Pop-rock has been defined as an "upbeat variety of rock music represented by artists such as Elton John, Paul McCartney, the Everly Brothers, Rod Stewart, Chicago, and Peter Frampton." The term "power pop" was coined by Pete Townshend of the Who in 1966, but not much used until it was applied to bands like Badfinger in the 1970s, who proved some of the most commercially successful of the period.
Blues rock.
Although the first impact of the British Invasion on American popular music was through beat and R&B based acts, the impetus was soon taken up by a second wave of bands that drew their inspiration more directly from American blues, including the Rolling Stones and the Yardbirds. British blues musicians of the late 1950s and early 60s had been inspired by the acoustic playing of figures such as Lead Belly, who was a major influence on the Skiffle craze, and Robert Johnson. Increasingly they adopted a loud amplified sound, often centered on the electric guitar, based on the Chicago blues, particularly after the tour of Britain by Muddy Waters in 1958, which prompted Cyril Davies and guitarist Alexis Korner to form the band Blues Incorporated. The band involved and inspired many of the figures of the subsequent British blues boom, including members of the Rolling Stones and Cream, combining blues standards and forms with rock instrumentation and emphasis.
The other key focus for British blues was around John Mayall who formed the Bluesbreakers, whose members included Eric Clapton (after his departure from The Yardbirds) and later Peter Green. Particularly significant was the release of "Blues Breakers with Eric Clapton (Beano)" album (1966), considered one of the seminal British blues recordings and the sound of which was much emulated in both Britain and the United States. Eric Clapton went on to form supergroups Cream, Blind Faith and Derek and the Dominos, followed by an extensive solo career that helped bring blues rock into the mainstream. Green, along with the Bluesbreaker's rhythm section Mick Fleetwood and John McVie, formed Peter Green's Fleetwood Mac, who enjoyed some of the greatest commercial success in the genre. In the late 60s Jeff Beck, also an alumnus of the Yardbirds, moved blues rock in the direction of heavy rock with his band, the Jeff Beck Group. The last Yardbirds guitarist was Jimmy Page, who went on to form "The New Yardbirds" which rapidly became Led Zeppelin. Many of the songs on their first three albums, and occasionally later in their careers, were expansions on traditional blues songs.
In America, blues rock had been pioneered in the early 1960s by guitarist Lonnie Mack, but the genre began to take off in the mid-60s as acts developed a sound similar to British blues musicians. Key acts included Paul Butterfield (whose band acted like Mayall's Bluesbreakers in Britain as a starting point for many successful musicians), Canned Heat, the early Jefferson Airplane, Janis Joplin, Johnny Winter, the J. Geils Band and Jimi Hendrix with his power trios, the Jimi Hendrix Experience and Band of Gypsys, whose guitar virtuosity and showmanship would be among the most emulated of the decade. Blues rock bands from the southern states, like the Allman Brothers Band, Lynyrd Skynyrd, and ZZ Top, incorporated country elements into their style to produce distinctive Southern rock.
Early blues rock bands often emulated jazz, playing long, involved improvisations, which would later be a major element of progressive rock. From about 1967 bands like Cream and the Jimi Hendrix Experience had begun to move away from purely blues-based music into psychedelia. By the 1970s, blues rock had become heavier and more riff-based, exemplified by the work of Led Zeppelin and Deep Purple, and the lines between blues rock and hard rock "were barely visible", as bands began recording rock-style albums. The genre was continued in the 1970s by figures such as George Thorogood and Pat Travers, but, particularly on the British scene (except perhaps for the advent of groups such as Status Quo and Foghat who moved towards a form of high energy and repetitive boogie rock), bands became focused on heavy metal innovation, and blues rock began to slip out of the mainstream.
Folk rock.
By the 1960s, the scene that had developed out of the American folk music revival had grown to a major movement, utilising traditional music and new compositions in a traditional style, usually on acoustic instruments. In America the genre was pioneered by figures such as Woody Guthrie and Pete Seeger and often identified with progressive or labor politics. In the early sixties figures such as Joan Baez and Bob Dylan had come to the fore in this movement as singer-songwriters. Dylan had begun to reach a mainstream audience with hits including "Blowin' in the Wind" (1963) and "Masters of War" (1963), which brought "protest songs" to a wider public, but, although beginning to influence each other, rock and folk music had remained largely separate genres, often with mutually exclusive audiences.
Early attempts to combine elements of folk and rock included the Animals' "House of the Rising Sun" (1964), which was the first commercially successful folk song to be recorded with rock and roll instrumentation and the Beatles "I'm a Loser" (1964), arguably the first Beatles song to be influenced directly by Dylan. The folk rock movement is usually thought to have taken off with The Byrds' recording of Dylan's "Mr. Tambourine Man" which topped the charts in 1965. With members who had been part of the cafe-based folk scene in Los Angeles, the Byrds adopted rock instrumentation, including drums and 12-string Rickenbacker guitars, which became a major element in the sound of the genre. Later that year Dylan adopted electric instruments, much to the outrage of many folk purists, with his "Like a Rolling Stone" becoming a US hit single. Folk rock particularly took off in California, where it led acts like The Mamas & the Papas and Crosby, Stills and Nash to move to electric instrumentation, and in New York, where it spawned performers including The Lovin' Spoonful and Simon and Garfunkel, with the latter's acoustic "The Sounds of Silence" (1965) being remixed with rock instruments to be the first of many hits.
These acts directly influenced British performers like Donovan and Fairport Convention. In 1969 Fairport Convention abandoned their mixture of American covers and Dylan-influenced songs to play traditional English folk music on electric instruments. This electric folk was taken up by bands including Pentangle, Steeleye Span and The Albion Band, which in turn prompted Irish groups like Horslips and Scottish acts like the JSD Band, Spencer's Feat and later Five Hand Reel, to use their traditional music to create a brand of Celtic rock in the early 1970s.
Folk rock reached its peak of commercial popularity in the period 1967–68, before many acts moved off in a variety of directions, including Dylan and the Byrds, who began to develop country rock. However, the hybridization of folk and rock has been seen as having a major influence on the development of rock music, bringing in elements of psychedelia, and helping to develop the ideas of the singer-songwriter, the protest song and concepts of "authenticity".
Psychedelic rock.
Psychedelic music's LSD-inspired vibe began in the folk scene, with the New York-based Holy Modal Rounders using the term in their 1964 recording of "Hesitation Blues". The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas, at the end of 1965; producing an album that made their direction clear, with "The Psychedelic Sounds of the 13th Floor Elevators" the following year. The Beatles introduced many of the major elements of the psychedelic sound to audiences in this period, with "I Feel Fine" using guitar feedback; in late 1965 the "Rubber Soul" album included the use of a sitar on "Norwegian Wood" and they employed backmasking on their 1966 single B-side "Rain" and other tracks that appeared on their "Revolver" album later that year.
Psychedelic rock particularly took off in California's emerging music scene as groups followed the Byrds from folk to folk rock from 1965. The psychedelic life style had already developed in San Francisco and particularly prominent products of the scene were the Grateful Dead, Country Joe and the Fish, the Great Society and Jefferson Airplane. The Byrds rapidly progressed from purely folk rock in 1966 with their single "Eight Miles High", widely taken to be a reference to drug use. In Britain, an influential band in the genre were The Yardbirds, who, with Jeff Beck as their guitarist, increasingly moved into psychedelic territory, adding up-tempo improvised "rave ups", Gregorian chant and world music influences to songs including "Still I'm Sad" (1965) and "Over Under Sideways Down" (1966). From 1966 the UK underground scene based in North London, supported new acts including Pink Floyd, Traffic and Soft Machine. The same year saw Donovan's folk-influenced hit album "Sunshine Superman", considered one of the first psychedelic pop records, as well as the débuts of blues rock bands Cream and the Jimi Hendrix Experience, whose extended guitar-heavy jams became a key feature of psychedelia.
Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release their definitive psychedelic statement in "Sgt. Pepper's Lonely Hearts Club Band", including the controversial track "Lucy in the Sky with Diamonds" and the Rolling Stones responded later that year with "Their Satanic Majesties Request". Pink Floyd produced what is usually seen as their best psychedelic work "The Piper at the Gates of Dawn". In America the Summer of Love was prefaced by the Human Be-In event and reached its peak at the Monterey Pop Festival, the latter helping to make major American stars of Jimi Hendrix and the Who, whose single "I Can See for Miles" delved into psychedelic territory. Key recordings included Jefferson Airplane's "Surrealistic Pillow" and the Doors' "Strange Days". These trends climaxed in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts, but by the end of the decade psychedelic rock was in retreat. Brian Wilson of the Beach Boys, Brian Jones of the Rolling Stones, Peter Green of Fleetwood Mac and Syd Barrett of Pink Floyd were early "acid casualties", the Jimi Hendrix Experience and Cream broke up before the end of the decade and many surviving acts moved away from psychedelia into more back-to-basics "roots rock", the wider experimentation of progressive rock, or riff-laden heavy rock.
Progression.
Roots rock.
Roots rock is the term now used to describe a move away from what some saw as the excesses of the psychedelic scene, to a more basic form of rock and roll that incorporated its original influences, particularly country and folk music, leading to the creation of country rock and Southern rock. In 1966 Bob Dylan went to Nashville to record the album "Blonde on Blonde". This, and subsequent more clearly country-influenced albums, have been seen as creating the genre of country folk, a route pursued by a number of, largely acoustic, folk musicians. Other acts that followed the back-to-basics trend were the Canadian group the Band and the California-based Creedence Clearwater Revival, both of which mixed basic rock and roll with folk, country and blues, to be among the most successful and influential bands of the late 1960s. The same movement saw the beginning of the recording careers of Californian solo artists like Ry Cooder, Bonnie Raitt and Lowell George, and influenced the work of established performers such as the Rolling Stones' "Beggar's Banquet" (1968) and the Beatles' "Let It Be" (1970).
In 1968 Gram Parsons recorded "Safe at Home" with the International Submarine Band, arguably the first true country-rock album. Later that year he joined the Byrds for "Sweetheart of the Rodeo" (1968), generally considered one of the most influential recordings in the genre. The Byrds continued in the same vein, but Parsons left to be joined by another ex-Byrds member Chris Hillman in forming the Flying Burrito Brothers who helped establish the respectability and parameters of the genre, before Parsons departed to pursue a solo career. Bands in California that adopted country rock included Hearts and Flowers, Poco and New Riders of the Purple Sage, the Beau Brummels and the Nitty Gritty Dirt Band. Some performers also enjoyed a renaissance by adopting country sounds, including: the Everly Brothers; one-time teen idol Rick Nelson who became the frontman for the Stone Canyon Band; former Monkee Mike Nesmith who formed the First National Band; and Neil Young. The Dillards were, unusually, a country act, who moved towards rock music. The greatest commercial success for country rock came in the 1970s, with artist including the Doobie Brothers, Emmylou Harris, Linda Ronstadt and the Eagles (made up of members of the Burritos, Poco and Stone Canyon Band), who emerged as one of the most successful rock acts of all time, producing albums that included "Hotel California" (1976).
The founders of Southern rock are usually thought to be the Allman Brothers Band, who developed a distinctive sound, largely derived from blues rock, but incorporating elements of boogie, soul, and country in the early 1970s. The most successful act to follow them were Lynyrd Skynyrd, who helped establish the "Good ol' boy" image of the subgenre and the general shape of 1970s' guitar rock. Their successors included the fusion/progressive instrumentalists Dixie Dregs, the more country-influenced Outlaws, jazz-leaning Wet Willie and (incorporating elements of R&B and gospel) the Ozark Mountain Daredevils. After the loss of original members of the Allmans and Lynyrd Skynyrd, the genre began to fade in popularity in the late 1970s, but was sustained the 1980s with acts like .38 Special, Molly Hatchet and the Marshall Tucker Band.
Progressive rock.
Progressive rock, a term sometimes used interchangeably with art rock, was an attempt to move beyond established musical formulas by experimenting with different instruments, song types, and forms. From the mid-1960s the Left Banke, the Beatles, the Rolling Stones and the Beach Boys, had pioneered the inclusion of harpsichords, wind and string sections on their recordings to produce a form of Baroque rock and can be heard in singles like Procol Harum's "A Whiter Shade of Pale" (1967), with its Bach-inspired introduction. The Moody Blues used a full orchestra on their album "Days of Future Passed" (1967) and subsequently created orchestral sounds with synthesisers. Classical orchestration, keyboards and synthesisers were a frequent edition to the established rock format of guitars, bass and drums in subsequent progressive rock.
Instrumentals were common, while songs with lyrics were sometimes conceptual, abstract, or based in fantasy and science fiction. The Pretty Things' "SF Sorrow" (1968), the Who's "Tommy" (1969) and the Kinks' "Arthur (Or the Decline and Fall of the British Empire)" (1969) introduced the format of rock operas and opened the door to concept albums, often telling an epic story or tackling a grand overarching theme. King Crimson's 1969 début album, "In the Court of the Crimson King", which mixed powerful guitar riffs and mellotron, with jazz and symphonic music, is often taken as the key recording in progressive rock, helping the widespread adoption of the genre in the early 1970s among existing blues-rock and psychedelic bands, as well as newly formed acts.
The vibrant Canterbury scene saw acts following Soft Machine from psychedelia, through jazz influences, toward more expansive hard rock, including Caravan, Hatfield and the North, Gong, and National Health. Greater commercial success was enjoyed by Pink Floyd, who also moved away from psychedelia after the departure of Syd Barrett in 1968, with "The Dark Side of the Moon" (1973), seen as a masterpiece of the genre, becoming one of the best-selling albums of all time. There was an emphasis on instrumental virtuosity, with Yes showcasing the skills of both guitarist Steve Howe and keyboard player Rick Wakeman, while Emerson, Lake & Palmer were a supergroup who produced some of the genre's most technically demanding work. Jethro Tull and Genesis both pursued very different, but distinctly English, brands of music. Renaissance, formed in 1969 by ex-Yardbirds Jim McCarty and Keith Relf, evolved into a high-concept band featuring the three-octave voice of Annie Haslam. Most British bands depended on a relatively small cult following, but a handful, including Pink Floyd, Genesis and Jethro Tull, managed to produce top ten singles at home and break the American market.
The American brand of prog rock varied from the eclectic and innovative Frank Zappa, Captain Beefheart and Blood, Sweat & Tears, to more pop rock orientated bands like Boston, Foreigner, Kansas, Journey and Styx. These, beside British bands Supertramp and ELO, all demonstrated a prog rock influence and while ranking among the most commercially successful acts of the 1970s, issuing in the era of "pomp" or "arena rock", which would last until the costs of complex shows (often with theatrical staging and special effects), would be replaced by more economical rock festivals as major live venues in the 1990s.
The instrumental strand of the genre resulted in albums like Mike Oldfield's "Tubular Bells" (1973), the first record, and worldwide hit, for the Virgin Records label, which became a mainstay of the genre. Instrumental rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can and Faust to circumvent the language barrier. Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent synth rock. With the advent of punk rock and technological changes in the late 1970s, progressive rock was increasingly dismissed as pretentious and overblown. Many bands broke up, but some, including Genesis, ELP, Yes, and Pink Floyd, regularly scored top ten albums with successful accompanying worldwide tours. Some bands which emerged in the aftermath of punk, such as Siouxsie and the Banshees, Ultravox and Simple Minds, showed the influence of prog, as well as their more usually recognized punk influences.
Jazz rock.
In the late 1960s jazz rock emerged as a distinct subgenre out of the blues rock, psychedelic and progressive rock scenes, mixing the power of rock with the musical complexity and improvisational elements of jazz. Many early US rock and roll musicians had begun in jazz and carried some of these elements into the new music. In Britain the subgenre of blues rock, and many of its leading figures, like Ginger Baker and Jack Bruce of Cream, had emerged from the British jazz scene. Often highlighted as the first true jazz-rock recording is the only album by the relatively obscure New York-based the Free Spirits with "Out of Sight and Sound" (1966). The first group of bands to self-consciously use the label were R&B oriented white rock bands that made use of jazzy horn sections, like Electric Flag, Blood, Sweat & Tears and Chicago, to become some of the most commercially successful acts of the later 1960s and early 1970s.
British acts to emerge in the same period from the blues scene, to make use of the tonal and improvisational aspects of jazz, included Nucleus and the Graham Bond and John Mayall spin-off Colosseum. From the psychedelic rock and the Canterbury scenes came Soft Machine, who, it has been suggested, produced one of the artistically successfully fusions of the two genres. Perhaps the most critically acclaimed fusion came from the jazz side of the equation, with Miles Davis, particularly influenced by the work of Hendrix, incorporating rock instrumentation into his sound for the album "Bitches Brew" (1970). It was a major influence on subsequent rock-influenced jazz artists, including Herbie Hancock, Chick Corea and Weather Report. The genre began to fade in the late 1970s, as a mellower form of fusion began to take its audience, but acts like Steely Dan, Frank Zappa and Joni Mitchell recorded significant jazz-influenced albums in this period, and it has continued to be a major influence on rock music.
Glam rock.
Glam rock emerged from the English psychedelic and art rock scenes of the late 1960s and can be seen as both an extension of and reaction against those trends. Musically diverse, varying between the simple rock and roll revivalism of figures like Alvin Stardust to the complex art rock of Roxy Music, and can be seen as much as a fashion as a musical subgenre. Visually it was a mesh of various styles, ranging from 1930s Hollywood glamor, through 1950s pin-up sex appeal, pre-war Cabaret theatrics, Victorian literary and symbolist styles, science fiction, to ancient and occult mysticism and mythology; manifesting itself in outrageous clothes, makeup, hairstyles, and platform-soled boots. Glam is most noted for its sexual and gender ambiguity and representations of androgyny, beside extensive use of theatrics. It was prefigured by the showmanship and gender-identity manipulation of American acts such as the Cockettes and Alice Cooper.
The origins of glam rock are associated with Marc Bolan, who had renamed his folk duo to T. Rex and taken up electric instruments by the end of the 1960s. Often cited as the moment of inception is his appearance on the UK TV programme "Top of the Pops" in December 1970 wearing glitter, to perform what would be his first number 1 single "Ride a White Swan". From 1971, already a minor star, David Bowie developed his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act. These performers were soon followed in the style by acts including Roxy Music, Sweet, Slade, Mott the Hoople, Mud and Alvin Stardust. While highly successful in the single charts in the UK, very few of these musicians were able to make a serious impact in the United States; Bowie was the major exception becoming an international superstar and prompting the adoption of glam styles among acts like Lou Reed, Iggy Pop, New York Dolls and Jobriath, often known as "glitter rock" and with a darker lyrical content than their British counterparts. In the UK the term glitter rock was most often used to refer to the extreme version of glam pursued by Gary Glitter and his support musicians the Glitter Band, who between them achieved eighteen top ten singles in the UK between 1972 and 1976. A second wave of glam rock acts, including Suzi Quatro, Roy Wood's Wizzard and Sparks, dominated the British single charts from about 1974 to 1976. Existing acts, some not usually considered central to the genre, also adopted glam styles, including Rod Stewart, Elton John, Queen and, for a time, even the Rolling Stones. It was also a direct influence on acts that rose to prominence later, including Kiss and Adam Ant, and less directly on the formation of gothic rock and glam metal as well as on punk rock, which helped end the fashion for glam from about 1976. Glam has since enjoyed sporadic modest revivals through bands such as Chainsaw Kittens, the Darkness and in R n' B crossover act Prince.
Soft rock, hard rock and early heavy metal.
From the late 1960s it became common to divide mainstream rock music into soft and hard rock. Soft rock was often derived from folk rock, using acoustic instruments and putting more emphasis on melody and harmonies. Major artists included Carole King, Cat Stevens and James Taylor. It reached its commercial peak in the mid- to late 70s with acts like Billy Joel, America and the reformed Fleetwood Mac, whose "Rumours" (1977) was the best-selling album of the decade. In contrast, hard rock was more often derived from blues-rock and was played louder and with more intensity. It often emphasised the electric guitar, both as a rhythm instrument using simple repetitive riffs and as a solo lead instrument, and was more likely to be used with distortion and other effects. Key acts included British Invasion bands like the Who and the Kinks, as well as psychedelic era performers like Cream, Jimi Hendrix and the Jeff Beck Group. Hard rock-influenced bands that enjoyed international success in the later 1970s included Queen, Thin Lizzy, Aerosmith and AC/DC.
From the late 1960s the term heavy metal began to be used to describe some hard rock played with even more volume and intensity, first as an adjective and by the early 1970s as a noun. The term was first used in music in Steppenwolf's "Born to Be Wild" (1967) and began to be associated with pioneer bands like San Francisco's Blue Cheer and Michigan's Grand Funk Railroad. By 1970 three key British bands had developed the characteristic sounds and styles which would help shape the subgenre. Led Zeppelin added elements of fantasy to their riff laden blues-rock, Deep Purple brought in symphonic and medieval interests from their progressive rock phrase and Black Sabbath introduced facets of the gothic and modal harmony, helping to produce a "darker" sound. These elements were taken up by a "second generation" of heavy metal bands into the late 1970s, including: Judas Priest, UFO, Motörhead and Rainbow from Britain; Kiss, Ted Nugent, and Blue Öyster Cult from the US; Rush from Canada and Scorpions from Germany, all marking the expansion in popularity of the subgenre. Despite a lack of airplay and very little presence on the singles charts, late-1970s heavy metal built a considerable following, particularly among adolescent working-class males in North America and Europe.
Christian rock.
Rock has been criticized by some Christian religious leaders, who have condemned it as immoral, anti-Christian and even demonic. However, Christian rock began to develop in the late 1960s, particularly out of the Jesus movement beginning in Southern California, and emerged as a subgenre in the 1970s with artists like Larry Norman, usually seen as the first major "star" of Christian rock. The genre has been particularly popular in the United States. Many Christian rock performers have ties to the contemporary Christian music scene, while other bands and artists are closely linked to independent music. Since the 1980s Christian rock performers have gained mainstream success, including figures such as the American gospel-to-pop crossover artist Amy Grant and the British singer Cliff Richard. While these artists were largely acceptable in Christian communities the adoption of heavy rock and glam metal styles by bands like Petra and Stryper, who achieved considerable mainstream success in the 1980s, was more controversial. From the 1990s there were increasing numbers of acts who attempted to avoid the Christian band label, preferring to be seen as groups who were also Christians, including P.O.D and Collective Soul.
Punk era.
Punk rock.
Punk rock was developed between 1974 and 1976 in the United States and the United Kingdom. Rooted in garage rock and other forms of what is now known as protopunk music, punk rock bands eschewed the perceived excesses of mainstream 1970s rock. They created fast, hard-edged music, typically with short songs, stripped-down instrumentation, and often political, anti-establishment lyrics. Punk embraces a DIY (do it yourself) ethic, with many bands self-producing their recordings and distributing them through informal channels.
By late 1976, acts such as the Ramones and Patti Smith, in New York City, and the Sex Pistols and the Clash, in London, were recognized as the vanguard of a new musical movement. The following year saw punk rock spreading around the world. Punk quickly, though briefly, became a major cultural phenomenon in the United Kingdom. For the most part, punk took root in local scenes that tended to reject association with the mainstream. An associated punk subculture emerged, expressing youthful rebellion and characterized by distinctive clothing styles and a variety of anti-authoritarian ideologies.
By the beginning of the 1980s, faster, more aggressive styles such as hardcore and Oi! had become the predominant mode of punk rock. This has resulted in several evolved strains of hardcore punk, such as D-beat (a distortion-heavy subgenre influenced by the UK band Discharge), anarcho-punk (such as Crass), grindcore (such as Napalm Death), and crust punk. Musicians identifying with or inspired by punk also pursued a broad range of other variations, giving rise to New wave, post-punk and the alternative rock movement.
New wave.
Although punk rock was a significant social and musical phenomenon, it achieved less in the way of record sales (being distributed by small specialty labels such as Stiff Records), or American radio airplay (as the radio scene continued to be dominated by mainstream formats such as disco and album-oriented rock). Punk rock had attracted devotees from the art and collegiate world and soon bands sporting a more literate, arty approach, such as Talking Heads, and Devo began to infiltrate the punk scene; in some quarters the description "new wave" began to be used to differentiate these less overtly punk bands. Record executives, who had been mostly mystified by the punk movement, recognized the potential of the more accessible new wave acts and began aggressively signing and marketing any band that could claim a remote connection to punk or new wave. Many of these bands, such as the Cars and the Go-Go's can be seen as pop bands marketed as new wave; other existing acts, including the Police, the Pretenders and Elvis Costello, used the new wave movement as the springboard for relatively long and critically successful careers, while "skinny tie" bands exemplified by the Knack, or the photogenic Blondie, began as punk acts and moved into more commercial territory.
Between 1979 and 1985, influenced by Kraftwerk, Yellow Magic Orchestra, David Bowie and Gary Numan, British new wave went in the direction of such New Romantics as Spandau Ballet, Ultravox, Japan, Duran Duran, A Flock of Seagulls, Culture Club, Talk Talk and the Eurythmics, sometimes using the synthesizer to replace all other instruments. This period coincided with the rise of MTV and led to a great deal of exposure for this brand of synthpop, creating what has been characterised as a second British Invasion. Some more traditional rock bands adapted to the video age and profited from MTV's airplay, most obviously Dire Straits, whose "Money for Nothing" gently poked fun at the station, despite the fact that it had helped make them international stars, but in general, guitar-oriented rock was commercially eclipsed.
Post-punk.
If hardcore most directly pursued the stripped down aesthetic of punk, and new wave came to represent its commercial wing, post-punk emerged in the later 1970s and early '80s as its more artistic and challenging side. Major influences beside punk bands were the Velvet Underground, the Who, Frank Zappa and Captain Beefheart, and the New York-based no wave scene which placed an emphasis on performance, including bands such as James Chance and the Contortions, DNA and Sonic Youth. Early contributors to the genre included the US bands Pere Ubu, Devo, the Residents and Talking Heads.
The first wave of British post-punk included Gang of Four, Siouxsie and the Banshees and Joy Division, who placed less emphasis on art than their US counterparts and more on the dark emotional qualities of their music. Bands like Siouxsie and the Banshees, Bauhaus, the Cure, and the Sisters of Mercy, moved increasingly in this direction to found Gothic rock, which had become the basis of a major sub-culture by the early 1980s. Similar emotional territory was pursued by Australian acts like the Birthday Party and Nick Cave. Members of Bauhaus and Joy Division explored new stylistic territory as Love and Rockets and New Order respectively. Another early post-punk movement was the industrial music developed by British bands Throbbing Gristle and Cabaret Voltaire, and New York-based Suicide, using a variety of electronic and sampling techniques that emulated the sound of industrial production and which would develop into a variety of forms of post-industrial music in the 1980s.
The second generation of British post-punk bands that broke through in the early 1980s, including the Fall, the Pop Group, the Mekons, Echo and the Bunnymen and the Teardrop Explodes, tended to move away from dark sonic landscapes. Arguably the most successful band to emerge from post-punk was Ireland's U2, who incorporated elements of religious imagery together with political commentary into their often anthemic music, and by the late 1980s had become one of the biggest bands in the world. Although many post-punk bands continued to record and perform, it declined as a movement in the mid-1980s as acts disbanded or moved off to explore other musical areas, but it has continued to influence the development of rock music and has been seen as a major element in the creation of the alternative rock movement.
New waves and genres in heavy metal.
Although many established bands continued to perform and record, heavy metal suffered a hiatus in the face of the punk movement in the mid-1970s. Part of the reaction saw the popularity of bands like Motörhead, who had adopted a punk sensibility, and Judas Priest, who created a stripped down sound, largely removing the remaining elements of blues music, from their 1978 album "Stained Class". This change of direction was compared to punk and in the late 1970s became known as the New Wave of British Heavy Metal (NWOBHM). These bands were soon followed by acts including Iron Maiden, Vardis, Diamond Head, Saxon, Def Leppard and Venom, many of which began to enjoy considerable success in the US. In the same period Eddie Van Halen established himself as a metal guitar virtuoso after his band's self-titled 1978 album. Randy Rhoads and Yngwie Malmsteen also became established virtuosos, associated with what would be known as the neoclassical metal style.
Inspired by NWOBHM and Van Halen's success, a metal scene began to develop in Southern California from the late 1970s, based on the clubs of L.A.'s Sunset Strip and including such bands as Quiet Riot, Ratt, Mötley Crüe, and W.A.S.P., who, along with similarly styled acts such as New York's Twisted Sister, incorporated the theatrics (and sometimes makeup) of glam rock acts like Alice Cooper and Kiss. The lyrics of these glam metal bands characteristically emphasized hedonism and wild behavior and musically were distinguished by rapid-fire shred guitar solos, anthemic choruses, and a relatively melodic, pop-oriented approach. The most commercially significant release of the era being "Slippery When Wet" (1986) by Bon Jovi from New Jersey, selling over 12 million copies in the US alone. The album has been credited with widening the audience for the subgenre, particularly by appealing to women as well as the traditional male dominated audience, and opening the door to MTV and commercial success for other bands at the end of the decade. By the mid-1980s bands were beginning to emerge from the L.A. scene that pursued a less glam image and a rawer sound, particularly Guns N' Roses, breaking through with the chart-topping "Appetite for Destruction" (1987), and Jane's Addiction, who emerged with their major label debut "Nothing's Shocking", the following year.
In the late 1980s metal fragmented into several subgenres, including thrash metal, which developed in the US from the style known as speed metal, under the influence of hardcore punk, with low-register guitar riffs typically overlaid by shredding leads. Lyrics often expressed nihilistic views or deal with social issues using visceral, gory language. It was popularised by the "Big Four of Thrash": Metallica, Anthrax, Megadeth, and Slayer. Death metal developed out of thrash, particularly influenced by the bands Venom and Slayer. Florida's Death and the Bay Area's Possessed emphasized lyrical elements of blasphemy, diabolism and millenarianism, with vocals usually delivered as guttural "death growls," high-pitched screaming, complemented by downtuned, highly distorted guitars and extremely fast double bass percussion. Black metal, again influenced by Venom and pioneered by Denmark's Mercyful Fate, Switzerland's Hellhammer and Celtic Frost, and Sweden's Bathory, had many similarities in sound to death metal, but was often intentionally lo-fi in production and placed greater emphasis on satanic and pagan themes. Bathory were particularly important in inspiring the further subgenres of Viking metal and folk metal. Power metal emerged in Europe in the late 1980s as a reaction to the harshness of death and black metal and was established by Germany's Helloween, who combined a melodic approach with thrash's speed and energy. England's DragonForce and Florida's Iced Earth have a sound indebted to NWOBHM, while acts such as Florida's Kamelot, Finland's Nightwish, Italy's Rhapsody of Fire, and Russia's Catharsis feature a keyboard-based "symphonic" sound, sometimes employing orchestras and opera singers. In contrast to other subgenres doom metal, influenced by Gothic rock, slowed down the music, with bands like England's Pagan Altar and Witchfinder General and the United States' Pentagram, Saint Vitus and Trouble, emphasizing melody, down-tuned guitars, a 'thicker' or 'heavier' sound and a sepulchral mood. American bands such as Queensrÿche and Dream Theater pioneered an often instrumentally challenging fusion of NWOBHM and progressive rock called progressive metal, with bands such as Symphony X combining aspects of power metal and classical music with the style, while Sweden's Opeth developed a unique style indebted to both death metal and atmospheric 70s prog rock.
Heartland rock.
American working-class oriented heartland rock, characterized by a straightforward musical style, and a concern with the lives of ordinary, blue-collar American people, developed in the second half of the 1970s. The term heartland rock was first used to describe Midwestern arena rock groups like Kansas, REO Speedwagon and Styx, but which came to be associated with a more socially concerned form of roots rock more directly influenced by folk, country and rock and roll. It has been seen as an American Midwest and Rust Belt counterpart to West Coast country rock and the Southern rock of the American South. Led by figures who had initially been identified with punk and New Wave, it was most strongly influenced by acts such as Bob Dylan, the Byrds, Creedence Clearwater Revival and Van Morrison, and the basic rock of 60s garage and the Rolling Stones.
Exemplified by the commercial success of singer songwriters Bruce Springsteen, Bob Seger, and Tom Petty, along with less widely known acts such as Southside Johnny and the Asbury Jukes and Joe Grushecky and the Houserockers, it was partly a reaction to post-industrial urban decline in the East and Mid-West, often dwelling on issues of social disintegration and isolation, beside a form of good-time rock and roll revivalism. The genre reached its commercial, artistic and influential peak in the mid-1980s, with Springsteen's "Born in the USA" (1984), topping the charts worldwide and spawning a series of top ten singles, together with the arrival of artists including John Mellencamp, Steve Earle and more gentle singer/songwriters such as Bruce Hornsby. It can also be heard as an influence on artists as diverse as Billy Joel, Kid Rock and the Killers.
Heartland rock faded away as a recognized genre by the early 1990s, as rock music in general, and blue collar and white working class themes in particular, lost influence with younger audiences, and as heartland's artists turned to more personal works. Many heartland rock artists continue to record today with critical and commercial success, most notably Bruce Springsteen, Tom Petty and John Mellencamp, although their works have become more personal and experimental and no longer fit easily into a single genre. Newer artists whose music would perhaps have been labelled heartland rock had it been released in the 1970s or 1980s, such as Missouri's Bottle Rockets and Illinois' Uncle Tupelo, often find themselves labeled alt-country.
Emergence of alternative rock.
The term alternative rock was coined in the early 1980s to describe rock artists who did not fit into the mainstream genres of the time. Bands dubbed "alternative" had no unified style, but were all seen as distinct from mainstream music. Alternative bands were linked by their collective debt to punk rock, through hardcore, New Wave or the post-punk movements. Important alternative rock bands of the 1980s in the US included R.E.M., Hüsker Dü, Jane's Addiction, Sonic Youth, and the Pixies, and in the UK the Cure, New Order, the Jesus and Mary Chain, and the Smiths. Artists were largely confined to independent record labels, building an extensive underground music scene based on college radio, fanzines, touring, and word-of-mouth. They rejected the dominant synthpop of the early 1980s, marking a return to group-based guitar rock.
Few of these early bands achieved mainstream success, although exceptions to this rule include R.E.M., the Smiths, and the Cure. Despite a general lack of spectacular album sales, the original alternative rock bands exerted a considerable influence on the generation of musicians who came of age in the 1980s and ended up breaking through to mainstream success in the 1990s. Styles of alternative rock in the U.S. during the 1980s included jangle pop, associated with the early recordings of R.E.M., which incorporated the ringing guitars of mid-1960s pop and rock, and college rock, used to describe alternative bands that began in the college circuit and college radio, including acts such as 10,000 Maniacs and the Feelies. In the UK Gothic rock was dominant in the early 1980s, but by the end of the decade indie or dream pop like Primal Scream, Bogshed, Half Man Half Biscuit and the Wedding Present, and what were dubbed shoegaze bands like My Bloody Valentine, Ride, Lush, Chapterhouse, and the Boo Radleys. Particularly vibrant was the Madchester scene, produced such bands as Happy Mondays, the Inspiral Carpets, and Stone Roses. The next decade would see the success of grunge in the United States and Britpop in the United Kingdom, bringing alternative rock into the mainstream.
Alternative.
Grunge.
Disaffected by commercialized and highly produced pop and rock in the mid-1980s, bands in Washington state (particularly in the Seattle area) formed a new style of rock which sharply contrasted with the mainstream music of the time. The developing genre came to be known as "grunge", a term descriptive of the dirty sound of the music and the unkempt appearance of most musicians, who actively rebelled against the over-groomed images of other artists. Grunge fused elements of hardcore punk and heavy metal into a single sound, and made heavy use of guitar distortion, fuzz and feedback. The lyrics were typically apathetic and angst-filled, and often concerned themes such as social alienation and entrapment, although it was also known for its dark humor and parodies of commercial rock.
Bands such as Green River, Soundgarden, the Melvins and Skin Yard pioneered the genre, with Mudhoney becoming the most successful by the end of the decade. However, grunge remained largely a local phenomenon until 1991, when Nirvana's "Nevermind" became a huge success thanks to the lead single "Smells Like Teen Spirit". "Nevermind" was more melodic than its predecessors, but the band refused to employ traditional corporate promotion and marketing mechanisms. During 1991 and 1992, other grunge albums such as Pearl Jam's "Ten", Soundgarden's "Badmotorfinger" and Alice in Chains' "Dirt", along with the "Temple of the Dog" album featuring members of Pearl Jam and Soundgarden, became among the 100 top-selling albums. Major record labels signed most of the remaining grunge bands in Seattle, while a second influx of acts moved to the city in the hope of success. However, with the death of Kurt Cobain and the subsequent break-up of Nirvana in 1994, touring problems for Pearl Jam and the departure of Alice in Chains' lead singer Layne Staley in 1996, the genre began to decline, partly to be overshadowed by Britpop and more commercial sounding post-grunge.
Britpop.
Britpop emerged from the British alternative rock scene of the early 1990s and was characterised by bands particularly influenced by British guitar music of the 1960s and 1970s. The Smiths were a major influence, as were bands of the Madchester scene, which had dissolved in the early 1990s. The movement has been seen partly as a reaction against various U.S. based, musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon and as a reassertion of a British rock identity. Britpop was varied in style, but often used catchy tunes and hooks, beside lyrics with particularly British concerns and the adoption of the iconography of the 1960s British Invasion, including the symbols of British identity previously utilised by the mods. It was launched around 1992 with releases by groups such as Suede and Blur, who were soon joined by others including Oasis, Pulp, Supergrass and Elastica, who produced a series of top ten albums and singles. For a while the contest between Blur and Oasis was built by the popular press into "The Battle of Britpop", initially won by Blur, but with Oasis achieving greater long-term and international success, directly influencing a third generation of Britpop bands, including The Boo Radleys, Ocean Colour Scene and Cast. Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement known as Cool Britannia. Although its more popular bands, particularly Blur and Oasis, were able to spread their commercial success overseas, especially to the United States, the movement had largely fallen apart by the end of the decade.
Post-grunge.
The term post-grunge was coined for the generation of bands that followed the emergence into the mainstream and subsequent hiatus of the Seattle grunge bands. Post-grunge bands emulated their attitudes and music, but with a more radio-friendly commercially oriented sound. Often they worked through the major labels and came to incorporate diverse influences from jangle pop, pop-punk, alternative metal or hard rock. The term post-grunge was meant to be pejorative, suggesting that they were simply musically derivative, or a cynical response to an "authentic" rock movement. From 1994, former Nirvana drummer Dave Grohl's new band, the Foo Fighters, helped popularize the genre and define its parameters.
Some post-grunge bands, like Candlebox, were from Seattle, but the subgenre was marked by a broadening of the geographical base of grunge, with bands like Los Angeles' Audioslave, and Georgia's Collective Soul and beyond the US to Australia's Silverchair and Britain's Bush, who all cemented post-grunge as one of the most commercially viable subgenres of the late 1990s. Although male bands predominated, female solo artist Alanis Morissette's 1995 album "Jagged Little Pill", labelled as post-grunge, also became a multi-platinum hit. Bands like Creed and Nickelback took post-grunge into the 21st century with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs, and were followed in this vein by new acts including Shinedown, Seether, 3 Doors Down and Puddle of Mudd.
Pop punk.
The origins of 1990s pop punk can be seen in the more song-oriented bands of the 1970s punk movement like the Buzzcocks and the Clash, commercially successful New Wave acts such as the Jam and the Undertones, and the more hardcore-influenced elements of alternative rock in the 1980s. Pop-punk tends to use power-pop melodies and chord changes with speedy punk tempos and loud guitars. Punk music provided the inspiration for some California-based bands on independent labels in the early 1990s, including Rancid, Pennywise, Weezer and Green Day. In 1994 Green Day moved to a major label and produced the album "Dookie", which found a new, largely teenage, audience and proved a surprise diamond-selling success, leading to a series of hit singles, including two number ones in the US. They were soon followed by the eponymous début from Weezer, which spawned three top ten singles in the US. This success opened the door for the multi-platinum sales of metallic punk band the Offspring with "Smash" (1994). This first wave of pop punk reached its commercial peak with Green Day's "Nimrod" (1997) and The Offspring's "Americana" (1998).
A second wave of pop punk was spearheaded by Blink-182, with their breakthrough album "Enema of the State" (1999), followed by bands such as Good Charlotte, Bowling for Soup and Sum 41, who made use of humour in their videos and had a more radio-friendly tone to their music, while retaining the speed, some of the attitude and even the look of 1970s punk. Later pop-punk bands, including Simple Plan, the All-American Rejects and Fall Out Boy, had a sound that has been described as closer to 1980s hardcore, while still achieving commercial success.
Indie rock.
In the 1980s the terms indie rock and alternative rock were used interchangeably. By the mid-1990s, as elements of the movement began to attract mainstream interest, particularly grunge and then Britpop, post-grunge and pop-punk, the term alternative began to lose its meaning. Those bands following the less commercial contours of the scene were increasingly referred to by the label indie. They characteristically attempted to retain control of their careers by releasing albums on their own or small independent labels, while relying on touring, word-of-mouth, and airplay on independent or college radio stations for promotion. Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands like the Cranberries and Superchunk, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco. It has been noted that indie rock has a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of feminist-informed Riot Grrrl music. Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown outside them.
By the end of the 1990s many recognisable subgenres, most with their origins in the late '80s alternative movement, were included under the umbrella of indie. Lo-fi eschewed polished recording techniques for a D.I.Y. ethos and was spearheaded by Beck, Sebadoh and Pavement. The work of Talk Talk and Slint helped inspire both post rock, an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika, as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez. Space rock looked back to progressive roots, with drone heavy and minimalist acts like Spacemen 3, the two bands created out of its split, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You Black Emperor! and Quickspace. In contrast, Sadcore emphasised pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters, while the revival of Baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian and Rufus Wainright.
Alternative metal, rap rock and nu metal.
Alternative metal emerged from the hardcore scene of alternative rock in the US in the later 1980s, but gained a wider audience after grunge broke into the mainstream in the early 1990s. Early alternative metal bands mixed a wide variety of genres with hardcore and heavy metal sensibilities, with acts like Jane's Addiction and Primus utilizing prog-rock, Soundgarden and Corrosion of Conformity using garage punk, the Jesus Lizard and Helmet mixing noise-rock, Ministry and Nine Inch Nails influenced by industrial music, Monster Magnet moving into psychedelia, Pantera, Sepultura and White Zombie creating groove metal, while Biohazard and Faith No More turned to hip hop and rap.
Hip hop had gained attention from rock acts in the early 1980s, including The Clash with "The Magnificent Seven" (1981) and Blondie with "Rapture" (1981). Early crossover acts included Run DMC and the Beastie Boys. Detroit rapper Esham became known for his "acid rap" style, which fused rapping with a sound that was often based in rock and heavy metal. Rappers who sampled rock songs included Ice-T, The Fat Boys, LL Cool J, Public Enemy and Whodini. The mixing of thrash metal and rap was pioneered by Anthrax on their 1987 comedy-influenced single "I'm the Man".
In 1990, Faith No More broke into the mainstream with their single "Epic", often seen as the first truly successful combination of heavy metal with rap. This paved the way for the success of existing bands like 24-7 Spyz and Living Colour, and new acts including Rage Against the Machine and Red Hot Chili Peppers, who all fused rock and hip hop among other influences. Among the first wave of performers to gain mainstream success as rap rock were 311, Bloodhound Gang, and Kid Rock. A more metallic sound - "nu metal" - was pursued by bands including Limp Bizkit, Korn and Slipknot. Later in the decade this style, which contained a mix of grunge, punk, metal, rap and turntable scratching, spawned a wave of successful bands like Linkin Park, P.O.D. and Staind, who were often classified as rap metal or nu metal, the first of which are the best-selling band of the genre.
In 2001, nu metal reached its peak with albums like Staind's "Break the Cycle", P.O.D's "Satellite", Slipknot's "Iowa" and Linkin Park's "Hybrid Theory". New bands also emerged like Disturbed, Godsmack and Papa Roach, whose major label début "Infest" became a platinum hit. Korn's long awaited fifth album "Untouchables", and Papa Roach's second album "Lovehatetragedy", did not sell as well as their previous releases, while nu metal bands were played more infrequently on rock radio stations and MTV began focusing on pop punk and emo. Since then, many bands have changed to a more conventional hard rock, heavy metal, or electronic music sound.
Post-Britpop.
From about 1997, as dissatisfaction grew with the concept of Cool Britannia, and Britpop as a movement began to dissolve, emerging bands began to avoid the Britpop label while still producing music derived from it. Many of these bands tended to mix elements of British traditional rock (or British trad rock), particularly the Beatles, Rolling Stones and Small Faces, with American influences, including post-grunge. Drawn from across the United Kingdom (with several important bands emerging from the north of England, Scotland, Wales and Northern Ireland), the themes of their music tended to be less parochially centered on British, English and London life and more introspective than had been the case with Britpop at its height. This, beside a greater willingness to engage with the American press and fans, may have helped some of them in achieving international success.
Post Britpop bands have been seen as presenting the image of the rock star as an ordinary person and their increasingly melodic music was criticised for being bland or derivative. Post Britpop bands like the Verve with "Urban Hymns" (1997), Radiohead from "OK Computer" (1997), Travis from "The Man Who" (1999), Stereophonics from "Performance and Cocktails" (1999), Feeder from "Echo Park" (2001) and particularly Coldplay from their debut album "Parachutes" (2000), achieved much wider international success than most of the Britpop groups that had preceded them, and were some of the most commercially successful acts of the late 1990s and early 2000s, arguably providing a launchpad for the subsequent garage rock or post-punk revival, which has also been seen as a reaction to their introspective brand of rock.
2000s–present.
Post-hardcore and emo.
Post-hardcore developed in the US, particularly in the Chicago and Washington, D.C areas, in the early to mid-1980s, with bands that were inspired by the do-it-yourself ethics and guitar-heavy music of hardcore punk, but influenced by post-punk, adopting longer song formats, more complex musical structures and sometimes more melodic vocal styles. Existing bands that moved on from hardcore included Fugazi. From the late 1980s they were followed by bands including Quicksand, Girls Against Boys and The Jesus Lizard. Bands that formed in the 1990s included Thursday, Thrice, Finch, and Poison the Well.
Emo also emerged from the hardcore scene in 1980s Washington, D.C., initially as "emocore", used as a term to describe bands who favored expressive vocals over the more common abrasive, barking style. The style was pioneered by bands Rites of Spring and Embrace, the last formed by Ian MacKaye, whose Dischord Records became a major centre for the emerging D.C. emo scene, releasing work by Rites of Spring, Dag Nasty, Nation of Ulysses and Fugazi. Fugazi emerged as the definitive early emo band, gaining a fanbase among alternative rock followers, not least for their overtly anti-commercial stance. The early emo scene operated as an underground, with short-lived bands releasing small-run vinyl records on tiny independent labels. The mid-'90s sound of emo was defined by bands like Jawbreaker and Sunny Day Real Estate who incorporated elements of grunge and more melodic rock. Only after the breakthrough of grunge and pop punk into the mainstream did emo come to wider attention with the success of Weezer's "Pinkerton" (1996) album, which utilised pop punk. Late 1990s bands drew on the work of Fugazi, SDRE, Jawbreaker and Weezer, including The Promise Ring, The Get Up Kids, Braid, Texas Is the Reason, Joan of Arc, Jets to Brazil and most successfully Jimmy Eat World, and by the end of the millennium it was one of the more popular indie styles in the US.
Emo broke into mainstream culture in the early 2000s with the platinum-selling success of Jimmy Eat World's "Bleed American" (2001) and Dashboard Confessional's "The Places You Have Come to Fear the Most" (2003). The new emo had a much more mainstream sound than in the 90s and a far greater appeal amongst adolescents than its earlier incarnations. At the same time, use of the term emo expanded beyond the musical genre, becoming associated with fashion, a hairstyle and any music that expressed emotion. The term emo has been applied by critics and journalists to a variety of artists, including multi-platinum acts such as Fall Out Boy and My Chemical Romance and disparate groups such as Paramore and Panic at the Disco, even when they protest the label. By 2003 post-hardcore bands had also caught the attention of major labels and began to enjoy mainstream success in the album charts. A number of these bands were seen as a more aggressive offshoot of emo and given the often vague label of screamo. Around this time, a new wave of post-hardcore bands began to emerge onto the scene that incorporated more pop punk and alternative rock styles into their music, including The Used, Hawthorne Heights, Senses Fail, From First to Last and Emery and Canadian bands Silverstein and Alexisonfire. British bands like Funeral For A Friend, The Blackout and Enter Shikari also made headway.
Garage rock/post-punk revival.
In the early 2000s, a new group of bands that played a stripped down and back-to-basics version of guitar rock, emerged into the mainstream. They were variously characterised as part of a garage rock, post-punk or new wave revival. Because the bands came from across the globe, cited diverse influences (from traditional blues, through New Wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed. There had been attempts to revive garage rock and elements of punk in the 1980s and 1990s and by 2000 scenes had grown up in several countries. The Detroit rock scene included the Von Bondies, Electric Six, the Dirtbombs and the Detroit Cobras and that of New York Radio 4, Yeah Yeah Yeahs and the Rapture. Elsewhere, other lesser-known acts such as Billy Childish and the Buff Medways from Britain, the (International) Noise Conspiracy from Sweden, the 5.6.7.8's from Japan, and the Oblivians from Memphis enjoyed underground, regional or national success.
The commercial breakthrough from these scenes was led by four bands: the Strokes, who emerged from the New York club scene with their début album "Is This It" (2001); the White Stripes, from Detroit, with their third album "White Blood Cells" (2001); the Hives from Sweden after their compilation album "Your New Favourite Band" (2001); and the Vines from Australia with "Highly Evolved" (2002). They were christened by the media as the "The" bands, and dubbed "The saviours of rock 'n' roll", leading to accusations of hype. A second wave of bands that gained international recognition due to the movement included Black Rebel Motorcycle Club, the Killers, Interpol and Kings of Leon from the US, the Libertines, Arctic Monkeys, Bloc Party, Editors, Franz Ferdinand and Placebo from the UK, Jet from Australia and the Datsuns and the D4 from New Zealand.
Contemporary heavy metal, metalcore and retro-metal.
By the new millennium Scandinavia had emerged as one of the areas producing innovative and successful bands, while Belgium, Holland and especially Germany were the most significant markets. Established continental metal bands that placed multiple albums in the top 20 of the German charts between 2003 and 2008, including Finnish band Children of Bodom, Norwegian act Dimmu Borgir, Germany's Blind Guardian and Sweden's HammerFall.
Metalcore, originally an American hybrid of thrash metal and hardcore punk, emerged as a commercial force in the mid-2000s. It was rooted in the crossover thrash style developed two decades earlier by bands such as Suicidal Tendencies, Dirty Rotten Imbeciles, and Stormtroopers of Death and remained an underground phenomenon through the 1990s; early bands include Earth Crisis, Converge, Hatebreed and Shai Hulud. Killswitch Engage's "The End of Heartache" and Shadows Fall's "The War Within" to debut at number 21 and number 20, respectively, on the "Billboard" album chart. Bullet for My Valentine, from Wales, broke into the top 5 in both the U.S. and British charts with "Scream Aim Fire" (2008). Metalcore bands have received prominent slots at Ozzfest and the Download Festival. Lamb of God, with a related blend of metal styles, reached number 2 on the "Billboard" charts in 2009 with "Wrath".
The success of these bands and others such as Trivium, who have released both metalcore and straight-ahead thrash albums, and Mastodon, who played in a progressive/sludge style, inspired claims of a metal revival in the United States, dubbed by some critics the "New Wave of American Heavy Metal". Its roots have been traced to the music of acts like Pantera, Biohazard and Machine Head, drawing on New York hardcore, thrash metal and punk, helping to inspire a move away from the nu metal of the early 2000s and a return to riffs and guitar solos.
The term "retro-metal" has been applied to such bands as Texas-based the Sword, California's High on Fire, Sweden's Witchcraft, and Australia's Wolfmother. The Sword's "Age of Winters" (2006) drew heavily on the work of Black Sabbath and Pentagram, while Witchcraft added elements of folk rock and psychedelic rock, and Wolfmother's self-titled 2005 debut album combined elements of the sounds of Deep Purple and Led Zeppelin.
Digital electronic rock.
In the 2000s, as computer technology became more accessible and music software advanced, it became possible to create high quality music using little more than a single laptop computer. This resulted in a massive increase in the amount of home-produced electronic music available to the general public via the expanding internet, and new forms of performance such as laptronica and live coding. These techniques also began to be used by existing bands, as with industrial rock act Nine Inch Nails' album "Year Zero" (2007), and by developing genres that mixed rock with digital techniques and sounds, including indie electronic, electroclash, dance-punk and new rave.
Indie electronic, which had begun in the early 1990s with bands like Stereolab and Disco Inferno, took off in the new millennium as the new digital technology developed, with acts including Broadcast from the UK, Justice from France, Lali Puna from Germany and The Postal Service, and Ratatat from the US, mixing a variety of indie sounds with electronic music, largely produced on small independent labels. The electroclash subgenre began in New York at the end of the 1990s, combining synth pop, techno, punk and performance art. It was pioneered by I-F with their track "Space Invaders Are Smoking Grass" (1998), and pursued by artists including Felix da Housecat, Peaches, Chicks on Speed, and Ladytron. It gained international attention at the beginning of the new millennium and spread to scenes in London and Berlin, but rapidly faded as a recognisable genre. Dance-punk, mixing post-punk sounds with disco and funk, had developed in the 1980s, but it was revived among some bands of the garage rock/post-punk revival in the early years of the new millennium, particularly among New York acts such as Liars, The Rapture and Radio 4, joined by dance-oriented acts who adopted rock sounds such as Out Hud. In Britain the combination of indie with dance-punk was dubbed new rave in publicity for Klaxons and the term was picked up and applied by the NME to bands including Trash Fashion, New Young Pony Club, Hadouken!, Late of the Pier, Test Icicles and Shitdisco, forming a scene with a similar visual aesthetic to earlier rave music.
Renewed interest in electronic music and nostalgia for the 1980s led to the beginnings of a synthpop revival, with acts including Adult and Fischerspooner. In 2003-4 it began to move into the mainstream with Ladytron, the Postal Service, Cut Copy, the Bravery and, with most commercial success, The Killers all producing records that incorporated vintage synthesizer sounds and styles which contrasted with the dominant sounds of post-grunge and nu-metal. The style was picked up by a large number of performers, particularly female solo artists, leading the British and other media to proclaim a new era of the female electropop star. Artists named included British acts Little Boots, La Roux and Ladyhawke. Male acts that emerged in the same period included Calvin Harris, Frankmusik, Hurts, Kaskade, LMFAO, and Owl City, whose single "Fireflies" (2009) reached the top of the Billboard chart.
Social impact.
Different subgenres of rock were adopted by, and became central to, the identity of a large number of sub-cultures. In the 1950s and 1960s, respectively, British youths adopted the Teddy Boy and Rockers subcultures, which revolved around US rock and roll. The counterculture of the 1960s was closely associated with psychedelic rock. The mid-1970s punk subculture began in the US, but it was given a distinctive look by British designer Vivienne Westwood, a look which spread worldwide. Out of the punk scene, the Goth and Emo subcultures grew, both of which presented distinctive visual styles.
When an international rock culture developed, it supplanted cinema as the major sources of fashion influence. Paradoxically, followers of rock music have often mistrusted the world of fashion, which has been seen as elevating image above substance. Rock fashions have been seen as combining elements of different cultures and periods, as well as expressing divergent views on sexuality and gender, and rock music in general has been noted and criticised for facilitating greater sexual freedom. Rock has also been associated with various forms of drug use, including the stimulants taken by some mods in the early to mid-1960s, through the LSD linked with psychedelic rock in the late 1960s and early 1970s; and sometimes to cannabis, cocaine and heroin, all of which have been eulogised in song.
Rock has been credited with changing attitudes to race by opening up African-American culture to white audiences; but at the same time, rock has been accused of appropriating and exploiting that culture. While rock music has absorbed many influences and introduced Western audiences to different musical traditions, the global spread of rock music has been interpreted as a form of cultural imperialism. Rock music inherited the folk tradition of protest song, making political statements on subjects such as war, religion, poverty, civil rights, justice and the environment. Political activism reached a mainstream peak with the "Do They Know It's Christmas?" single (1984) and Live Aid concert for Ethiopia in 1985, which, while successfully raising awareness of world poverty and funds for aid, have also been criticised (along with similar events), for providing a stage for self-aggrandisement and increased profits for the rock stars involved.
Since its early development rock music has been associated with rebellion against social and political norms, most obviously in early rock and roll's rejection of an adult-dominated culture, the counterculture's rejection of consumerism and conformity and punk's rejection of all forms of social convention, however, it can also be seen as providing a means of commercial exploitation of such ideas and of diverting youth away from political action.

</doc>
<doc id="25424" url="http://en.wikipedia.org/wiki?curid=25424" title="Retronym">
Retronym

A retronym is a type of neologism that provides a new name for something to differentiate the original from a more recent form or version.
Advances in technology are often responsible for retronym coinage. For example, the term "acoustic guitar" was coined at the advent of electric guitars and analog watches were thus named to distinguish them from digital watches.
Examples.
Sometimes retronyms serve to differentiate two similarly named people, as with U.S. President George Bush, who, after his son George W. Bush was elected president in 2000, was typically referred to as "George Bush, Sr.", "George H. W. Bush", "H. W.", or "Bush 41" (as he was the 41st U.S. President).
In the entertainment industry, this can manifest itself as calling a movie "Part 1" after sequels are released or by slightly altering the title (e.g., ' or "Indiana Jones and the Raiders of the Lost Ark") to emphasize its connection with the sequel(s), or by referring to a television series as "the original", as in '.. This is similar to World War One, which was called the Great War prior to World War Two.
The earliest razors with encased blades were called "safety razors" to distinguish them from what were then just called "razors". But the safety razor has since become the standard and the original razor generally called a "straight razor".
The first bicycles with two wheels of equal size were called "safety bicycles" because they were easier to handle than the then-dominant style that had one large wheel and one small wheel, which then became known as an "ordinary" bicycle. Since the end of the 19th century, most bicycles have been expected to have two equal sized wheels, and the other type has been renamed "penny-farthing" or "high-wheeler" bicycle.
In the UK (and increasingly elsewhere), the term 'real ale' has been used since the early 1970s to distinguish beers produced and served by traditional British methods from gas-pressurised beers that became more popular during the earlier part of the twentieth-century.
With the increasing discussion and legalization of same-sex marriage in the 20th and 21st centuries, terms such as "straight marriage" and "heterosexual marriage" arose to distinguish a set of unions which the term "marriage" had previously meant by default.
In Judaism, the Hebrew Bible is referred to as the Tanakh, an acronym for its three sections (the Torah or Pentateuch, the Nevi'im or Prophets, and the Ketuvim or Writings). In Christianity, the Bible includes an additional section, the New Testament, calling the Hebrew Bible the "Old Testament."
Evolution in usage.
The original use of an adjective to describe a particular variant of an object is typically compositional, as in "acoustic guitar", but gradually over time it becomes a collocation, a name or technical term in its own right with additional nuances, greater specificity, and general but implicit agreement on it as the appropriate term versus alternative descriptions of the original.
The main exceptions to this relate to ownership, such as a trademark owner adding words to an existing product name or brand to create differentiated names for new variants of a product, which thus enjoy the status of a name immediately upon release of the product range.
Word history.
The term "retronym" was coined by Frank Mankiewicz in 1980 and popularized by William Safire in "The New York Times".
In 2000 "The American Heritage Dictionary" (4th edition) became the first major dictionary to include the word "retronym".
In some instances "back-formation" can be a type of retronymy; for example, humans have always diagnosed diseases (to whatever limited extent that they understood physiology and medicine in each era), but the verb "to diagnose" as a back-formation from the noun "diagnosis" was not attested in written English until about 1859. It was thus a new name for an old action.

</doc>
<doc id="25427" url="http://en.wikipedia.org/wiki?curid=25427" title="Rush Limbaugh">
Rush Limbaugh

Rush Hudson Limbaugh III (, ; born January 12, 1951) is an American entertainer, radio talk show host, writer, and conservative political commentator. Since he was 16, Limbaugh has worked a series of disc jockey jobs. His talk show began in 1984 at Sacramento radio station KFBK, featuring his ongoing format of political commentary and listener calls. In 1988, Limbaugh began broadcasting his show nationally from radio station WABC in New York City. He currently lives in Palm Beach, Florida, where he broadcasts "The Rush Limbaugh Show", the highest-rated talk-radio program in the United States. "Talkers Magazine" in 2012 lists Limbaugh as the most-listened-to talk show host with a weekly audience of 15 million.
In the 1990s, Limbaugh's books "The Way Things Ought to Be" (1992) and "See, I Told You So" (1993) made "The New York Times" Best Seller list. Limbaugh frequently criticizes, in his books and on his show, what he regards as liberal policies and politicians, as well as what he perceives as a pervasive liberal bias in major U.S. media. Limbaugh is among the highest paid people in U.S. media, signing a contract in 2008 for $400 million through 2016.
Early life.
Rush Hudson Limbaugh III was born in Cape Girardeau, Missouri, the son of Mildred Carolyn "Millie" (née Armstrong) and Rush Hudson Limbaugh, Jr. His father was a lawyer and a U.S. fighter pilot who served in the China Burma India Theater of World War II. His mother was a native of Searcy, Arkansas. The name "Rush" was originally chosen for his grandfather to honor the maiden name of family member Edna Rush.
Limbaugh has German ancestry. His family has many lawyers, including his grandfather, father and brother David. His uncle, Stephen N. Limbaugh, Sr. is a federal judge in the United States District Court for the Eastern District of Missouri. His cousin, Stephen N. Limbaugh, Jr., is currently a judge in the same court, appointed by George W. Bush. Rush Limbaugh, Sr., Limbaugh's grandfather was a Missouri prosecutor, judge, special commissioner, member of the Missouri House of Representatives from 1930 until 1932, and longtime president of the Missouri Historical Society. The Federal Courthouse in Cape Girardeau is named for Limbaugh's grandfather, Rush Limbaugh, Sr.
Limbaugh began his career in radio as a teenager in 1967 in his hometown of Cape Girardeau, using the name Rusty Sharpe. Limbaugh graduated from Cape Girardeau, Missouri Central High School in 1969. He played football. Because of his parents' desire to see him attend college, he enrolled in Southeast Missouri State University but left the school after two semesters and one summer. According to his mother, "he flunked everything", and "he just didn't seem interested in anything except radio."
Limbaugh's biographer states that a large part of his life has been dedicated to gaining his father's respect and approval.
Professional career.
1970s.
After dropping out of college, Limbaugh moved to McKeesport, Pennsylvania. In 1972, he became a Top 40 music disc jockey on WIXZ, a small AM radio station that reached much of the Pittsburgh area. He started with an afternoon show and later did mornings, broadcasting under the name Jeff Christie. Limbaugh moved to Pittsburgh station KQV in 1973 as the evening disc jockey, succeeding Jim Quinn. He was fired in late-1974, when the station was sold to Taft Broadcasting. Limbaugh was reportedly told by management that he would never make it as on air talent, and should consider going into sales. Unable to find another job in local radio, Limbaugh moved back home to Cape Girardeau. He became a lifelong fan of the Pittsburgh Steelers from his time in the region.
For the rest of the decade Limbaugh took jobs at several radio stations, working in music radio, before settling in Kansas City. In 1979, he left radio and accepted a position as director of promotions with the Kansas City Royals baseball team. There he developed a close friendship with then-Royals star third baseman and future Hall of Famer George Brett; the two remain close friends.
1980s.
In 1984, Limbaugh returned to radio as a talk show host at KFBK in Sacramento, California, where he replaced Morton Downey, Jr. The repeal of the Fairness Doctrine—which had required that stations provide free air time for responses to any controversial opinions that were broadcast—by the FCC in 1987 meant stations could broadcast editorial commentary without having to present opposing views. Daniel Henninger wrote, in a "Wall Street Journal" editorial, "Ronald Reagan tore down this wall (the Fairness Doctrine) in 1987 ... and Rush Limbaugh was the first man to proclaim himself liberated from the East Germany of liberal media domination."
On August 1, 1988, after achieving success in Sacramento and drawing the attention of former ABC Radio President Edward McLaughlin, Limbaugh moved to New York City and began his national radio show. He debuted just weeks after the Democratic National Convention, and just weeks before the Republican National Convention. Limbaugh's radio home in New York City was the talk-formatted WABC, and this remains his flagship station (although Limbaugh now hosts his program from West Palm Beach).
1990s.
In December 1990, journalist Lewis Grossberger wrote in "The New York Times" that Limbaugh had "more listeners than any other talk show host" and described Limbaugh's style as "bouncing between earnest lecturer and political vaudevillian". Limbaugh's rising popularity coincided with the Persian Gulf War, and his support for the war effort and his relentless ridicule of peace activists. The program gained more popularity and was moved to stations with larger audiences, eventually being broadcast on over 650 radio stations nationwide.
In 1992, Democrat Bill Clinton was elected president of the United States. Limbaugh satirized the policies of Clinton and First Lady Hillary Rodham Clinton, as well as those of the Democratic Party. When the Republican Party won control of Congress in the 1994 midterm elections, the freshman Republican class awarded Limbaugh an honorary membership in their caucus. This event confirmed him as an influential figure on the national political scene.
2000s.
Limbaugh had publicized personal difficulties in the 2000s. In late 2001, he acknowledged that he had gone almost completely deaf, although he continued his show. He was able to regain much of his hearing with the help of a cochlear implant in 2001.
In 2003, Limbaugh had a brief stint as a pro football commentator with ESPN. He resigned a few weeks into the 2003 NFL season after making comments about the press coverage for quarterback Donovan McNabb that caused controversy and accusations of racism on the part of Limbaugh. His comment about McNabb was: "I don't think he's been that good from the get-go. I think what we've had here is a little social concern in the NFL. I think the media has been very desirous that a black quarterback do well. They're interested in black coaches and black quarterbacks doing well. I think there's a little hope invested in McNabb and he got a lot of credit for the performance of his team that he really didn't deserve. The defense carried this team." For example, a sportswriter construed the comment as racist against himself and other sportswriters. Another sports analyst wrote Limbaugh's viewpoint was shared by "many football fans and analysts" and "it is ... absurd to say that the sports media haven't overrated Donovan McNabb because he's black."
In 2003, Limbaugh stated that he was addicted to pain medication, and sought treatment. In April 2006, Limbaugh turned himself in to authorities, on a warrant issued by the state attorney's office, and was arrested "on a single charge of prescription fraud". His record was later expunged.
"The Rush Limbaugh Show".
Limbaugh's radio show airs for three hours each weekday beginning at noon Eastern Standard Time on both AM and FM radio. The program is also broadcast worldwide on the Armed Forces Radio Network.
Radio broadcasting shifted from AM to FM in the late 1970s because of the opportunity to broadcast music in stereo with better fidelity. Limbaugh's show was first nationally syndicated in August 1988, in a later stage of AM's decline. Limbaugh's popularity paved the way for other conservative talk radio programming to become commonplace on the AM radio. In March 2006, WBAL in Baltimore became the first major market radio station in the country to drop Limbaugh's nationally syndicated radio program. In 2007, "Talkers" magazine again named him No. 1 in its "Heavy Hundred" most important talk show hosts.
Limbaugh frequently mentions the EIB (Excellence In Broadcasting) Network, trademarked in 1990. In the beginning, his show was co-owned and first syndicated by Edward F. McLaughlin, former president of ABC who founded EFM Media in 1988, with Limbaugh's show as his first product. In 1997, McLaughlin sold EFM to Jacor Communications, which was ultimately bought up by Clear Channel Communications. Today, Limbaugh owns a majority of the show, which is syndicated by the Premiere Radio Networks.
According to a 2001 article in "U.S. News & World Report", Limbaugh had an eight-year contract, at the rate of $31.25 million a year. In 2007, Limbaugh earned $33 million. On July 2, 2008, Matt Drudge reported that Limbaugh signed a contract extension through 2016 that is worth over $400 million, breaking records for any broadcast. A November 2008 poll by Zogby International found that Rush Limbaugh was the most trusted news personality in the nation, garnering 12.5 percent of poll responses.
Television show.
Limbaugh had a syndicated half-hour television show from 1992 through 1996, produced by Roger Ailes. The show discussed many of the topics on his radio show, and was taped in front of an audience. Rush Limbaugh says he loves doing his radio show, but not a TV show.
Other media appearances.
Limbaugh's first television hosting experience came March 30, 1990, as a guest host on Pat Sajak's CBS late-night talk show, "The Pat Sajak Show". ACT UP activists in the audience heckled Limbaugh repeatedly; ultimately the entire studio audience was cleared. In 2001, Sajak said the incident was "legendary around CBS".
On December 17, 1993, Limbaugh appeared on the "Late Show with David Letterman". Limbaugh also guest-starred (as himself) on a 1994 episode of "Hearts Afire". He appeared in the 1995 Billy Crystal film "Forget Paris", and in 1998 on an episode of "The Drew Carey Show".
In 2007, Limbaugh made cameo appearances on Fox News Channel's short-lived "The 1/2 Hour News Hour" in a series of parodies portraying him as the future President of the United States. In the parodies, his vice president was fellow conservative pundit Ann Coulter. He also made a cameo in the "Family Guy" episode "Blue Harvest" that year. More recent "Family Guy" appearances have been: the 2010 episode "Excellence in Broadcasting", and the 2011 episode "It's a Trap!", a parody of "Return of the Jedi", in which Limbaugh can be heard on the radio claiming that, among other things, the "intergalactic liberal space media" was lying about climate change on the planet Hoth, and that Lando Calrissian's administrative position on Cloud City was a result of affirmative action.
Influence and legacy.
His persona has often been utilized as a template for a stereotypical conservative talk show host on TV shows and in movies, including in the "Columbo" episode "Butterfly in Shades of Grey" (1993), portrayed in the character Fielding Chase by William Shatner; in two episodes of "The Simpsons", "Sideshow Bob Roberts" (1994) and "You Kent Always Say What You Want" (2007) as Birch Barlow; and in the episode "Right On" (1994) on "Beavis and Butt-head".
As a result of his television program, Limbaugh became known for wearing distinctive neckties. In response to viewer interest, Limbaugh launched a series of ties designed primarily by his then-wife Marta. Sales of the ties reached over US$5 million in their initial sales year, but were later discontinued.
In January 2010, Chicago's Second City announced a new production, "Rush Limbaugh: The Musical", a musical parody of Limbaugh which incorporates a pastiche of several Broadway musicals. It follows in the footsteps of 2009's successful run of "Rod Blagojevich Superstar", which was written and developed by the same creative team.
On January 30, 2010, Limbaugh was a judge for the 2010 Miss America pageant in Las Vegas. In early 2011, Limbaugh was the subject of the third season of Golf Channel's "The Haney Project", in which instructor Hank Haney coached him in eight episodes.
Awards and recognition.
In 1992, Ronald Reagan sent Limbaugh a letter in which he thanked him "for all you're doing to promote Republican and conservative principles ... [and] you have become the Number One voice for conservatism in our Country."
Limbaugh was awarded the Marconi Radio Award for Syndicated Radio Personality of the Year in 2014 (given by the National Association of Broadcasters). He has won the award four times previously (in 1992, 1995, 2000, and 2005). He was inducted into the National Radio Hall of Fame in 1993. He was later inducted into the National Association of Broadcasters Hall of Fame in 1998.
In 2002, "Talkers Magazine" ranked him as the greatest radio talk show host of all time. Limbaugh is the highest-paid syndicated radio host.
On March 29, 2007, Limbaugh was awarded the inaugural William F. Buckley, Jr. Award for Media Excellence, by the Media Research Center, a conservative media analysis group.
On January 5, 2008, the conservative magazine "Human Events" announced Limbaugh as their 2007 Man of the Year.
On December 1, 2008, "TV Guide" reported that Limbaugh was selected as one of America's top ten most fascinating people of 2008 for a Barbara Walters ABC special that aired on December 4, 2008.
On February 28, 2009, following his self-described "first address to the Nation" lasting 90 minutes, carried live on CNN and Fox News and recorded for C-SPAN, Limbaugh received CPAC's "Defender of the Constitution Award", a document originally signed by Benjamin Franklin, given to someone "who has stood up for the First Amendment ... Rush Limbaugh is for America, exactly what Benjamin Franklin did for the Founding Fathers ... the only way we will be successful is if we listen to Rush Limbaugh".
Zev Chafets, whose book "Rush Limbaugh: An Army of One" was published May 25, 2010, wrote after the first primaries of the 2010 U.S. election season that Limbaugh was "the brains and the spirit behind" the Republican Party's "resurgence" in the wake of the 2008 election of President Barack Obama. In his May 20, 2010, "New York Times" op-ed column, Chafets pointed among others to Sen. Arlen Specter's defeat, after being labeled by Limbaugh "Republican in Name Only," and to Sarah Palin, whose "biggest current applause line—Republicans are not just the party of no, but the party of hell no—came courtesy of Mr. Limbaugh." More generally, Chafets wrote, Limbaugh has argued the party-of-no Ronald Reagan conservative course for the Republicans vigorously, notably since six weeks after the Obama inauguration, and has been fundamental to, and encouraging to, the more prominently noted Tea Party movement.
Rush Limbaugh was inducted into the Hall of Famous Missourians on May 14, 2012. A bronze bust of Limbaugh is now on display in the Missouri State Capitol building in Jefferson City. It is the only such bust with its own security camera to discourage vandalism.
The Children's Book Council named Limbaugh their 2014 Author of the Year for his book "Rush Revere and the Brave Pilgrims: Time-Travel Adventures with Exceptional Americans".
Works.
In 1992, Limbaugh published his first book, "The Way Things Ought To Be", followed by "See, I Told You So" in 1993. Both became number one on the "New York Times" Best Seller list, "The Way Things Ought to Be" remaining there for 24 weeks. The text of the first book was taped by Limbaugh, and transcribed and edited by "Wall Street Journal" writer John Fund. In the second book, Joseph Farah of "WorldNetDaily" is named as his collaborator.
In 2013, Limbaugh authored a children's book titled "Rush Revere and the Brave Pilgrims: Time-Travel with Exceptional Americans". The book was released on October 29, 2013. It won Limbaugh the Author of the Year Award at the Children's Choice Book Awards in 2014.
In 2014, Limbaugh authored his second children's book titled "Rush Revere and the First Patriots: Time-Travel with Exceptional Americans". For his new series, Limbaugh was selected as an author-of-the year finalist for the annual Children's and Teen Choice Book Awards.
In late 2014, Limbaugh authored his third children's book titled "Rush Revere and the American Revolution". Limbaugh and his wife, Kathryn, dedicated the third book to the U.S. military and their families.
Views.
In his first "New York Times" best seller, Limbaugh describes himself as conservative, and is critical of broadcasters in many media outlets for claiming to be objective. He has criticized political centrists, independents, and moderate conservatives, claiming they are responsible for Democrat Barack Obama's victory over Republican John McCain in the 2008 U.S. Presidential Election and inviting them to leave the Republican party. He calls for the adoption of core conservative philosophies in order to ensure the survival of the Republican party.
James Rainey of the "Los Angeles Times" quoted Limbaugh as saying after the 2008 election of Barack Obama as the 44th President of the United States that the Democrats will "take your 401(k), put it in the Social Security Trust Fund."
African-Americans.
Limbaugh has been noted for making controversial race-related statements with regard to African-Americans. He once opined that all newspaper composite pictures of wanted criminals resembled Jesse Jackson, and another time that "the NFL all too often looks like a game between the Bloods and the Crips without any weapons." While employed as what he describes as an "insult-radio" DJ, he used a derogatory racial stereotype to characterize a black caller he could not understand, telling the caller to "take that bone out of your nose and call me back." In March 2010, Limbaugh used the similarity of recently resigned Rep. Eric Massa's surname to the slavery-era African-American pronunciation of "master" to make a pun on the possibility that Gov. David Paterson, New York's first African-American governor, would pick Massa's replacement: "Let's assume you're right [caller]. So, David Paterson will become the massa who gets to appoint whoever gets to take Massa's place. So, for the first time in his life, Paterson's gonna be a massa. Interesting, interesting."
Limbaugh has asserted that African-Americans, in contrast with other minority groups, are "left behind" socially because they have been systematically trained from a young age to hate America because of the welfare state.
Capital punishment.
Limbaugh supports capital punishment, saying "the only thing cruel about the death penalty is last-minute stays."
Drug abuse.
Limbaugh has been an outspoken critic of what he sees as leniency towards criminal drug use in America. On his television show in October 5, 1995, Limbaugh stated, "too many whites are getting away with drug use" and illegal drug trafficking. Limbaugh proposed that the racial disparity in drug enforcement could be fixed if authorities increased detection efforts, conviction rates, and jail time for whites involved in illegal drugs.
Environmental issues.
Limbaugh is critical of environmentalism and climate science. He has disputed claims of anthropogenic global warming, and the relationship between CFCs and depletion of the ozone layer, saying the scientific evidence does not support them. Limbaugh has argued against the scientific consensus on climate change saying it is "just a bunch of scientists organized around a political proposition." Limbaugh has used the term "environmentalist wacko" when referring to left-leaning environmental advocates. As a rhetorical device, he has also used the term to refer to more mainstream climate scientists and other environmental scientists and advocates with whom he disagrees.
Limbaugh has written that "there are more acres of forestland in America today than when Columbus discovered the continent in 1492," a claim that is disputed by the United States Forest Service and the American Forestry Association, which state that the precolonial forests have been reduced by about 24 percent or nearly 300 million acres.
Feminism.
Limbaugh is critical of feminism, saying that: "Feminism was established so as to allow unattractive women easier access to the mainstream of society." He also popularized the term "feminazi", referring to about two dozen feminists "to whom the most important thing in life is ensuring that as many abortions as possible occur." He credited his friend Tom Hazlett, a professor of law and economics at George Mason University, with coining the term.
Iraq prisoner abuse.
On the Abu Ghraib torture and prisoner abuse scandal, Limbaugh said, "This is no different than what happens at the Skull and Bones initiation ... And we're going to ruin people's lives over it and we're going to hamper our military effort, and then we are going to really hammer them because they had a good time."
Obama's policies.
On January 16, 2009, Limbaugh commented on the (then-upcoming) Obama presidency, "I hope he fails." Limbaugh later said that he wants to see Obama's "policies" fail, not the man himself. Speaking of Obama, Limbaugh said, "He's my president, he's a human being, and his ideas and policies are what count for me."
Use of entertainment props.
Limbaugh utilizes props to introduce his monologues on various topics. On his radio show, news about the homeless has often been preceded with the Clarence "Frogman" Henry song "Ain't Got No Home." For a time, Dionne Warwick's song "I Know I'll Never Love This Way Again" preceded reports about people with AIDS. These later became "condom updates" preceded by Fifth Dimension's song, "Up, Up and Away". For two weeks in 1989, on his Sacramento radio show, Limbaugh performed "caller abortions" where he would end a call suddenly to the sounds of a vacuum cleaner and a scream. He would then deny that he had "hung up" on the caller, which he had promised not to do. Limbaugh claims that he used this gag to illustrate "the tragedy of abortion" as well as to highlight the question of whether abortion constitutes murder. During the Clinton administration, while filming his television program, Limbaugh referred to media coverage of Socks, the Clintons' cat. He then stated, "But did you know there is also a White House dog?" and a picture of Chelsea Clinton was shown. When questioned about it, Limbaugh claimed that it was an accident and that without his permission some technician had put up the picture of Chelsea.
Claims of inaccuracy.
Some groups and individuals have criticized Limbaugh's accuracy. The July–August 1994 issue of "Extra!", a publication of the progressive group Fairness and Accuracy in Reporting (FAIR), alleges 50 different inaccuracies and distortions in Limbaugh's commentary. Others have since joined FAIR in questioning Limbaugh's facts. Comedian Al Franken, who later became a Senator, wrote a satirical book ("Rush Limbaugh Is a Big Fat Idiot and Other Observations") in which he accused Limbaugh of distorting facts to serve his own political biases.
Professor Ray Perkins has written a book titled "Logic and Mr. Limbaugh: A Dittohead's Guide To Fallacious Reading", which takes "50 examples of logical reasoning from Rush's statements, identifies the logical arguments, and points out fallacies."
Limbaugh has been criticized for inaccuracies by the Environmental Defense Fund. A defense fund report authored by Princeton University endowed geoscience professor Michael Oppenheimer and professor of biology David Wilcove lists 14 significant scientific facts that, the authors allege, Limbaugh misrepresented in his book "The Way Things Ought to Be". The authors conclude that "Rush Limbaugh ... allows his political bias to distort the truth about a whole range of important scientific issues."
On October 14, 2011, Limbaugh questioned the U.S. military initiative against Joseph Kony and his Lord's Resistance Army (LRA), based on the assumption that they were Christians. "They are fighting the Muslims in Sudan. And Obama has sent troops, United States troops to remove them from the battlefield, which means kill them." Upon learning about the accusations leveled against Kony, which included kidnapping whole schools of young children for use as child soldiers, Limbaugh stated that he would research the group. The show's written transcript on his website was not changed.
Charitable work.
Leukemia and lymphoma telethon.
Limbaugh holds an annual fundraising telethon called the "EIB Cure-a-Thon" for the Leukemia & Lymphoma Society. In 2006, the EIB Cure-a-Thon conducted its 16th annual telethon, raising $1.7 million, totaling over $15 million since the first cure-a-thon. According to Leukemia and Lymphoma Society annual reports, Limbaugh personally contributed between $100,000 and $499,999 from 2000–2005 and 2007, and Limbaugh said that he contributed around $250,000 in 2003, 2004 and 2005. NewsMax reported Limbaugh donated $250,000 in 2006, and the Society's 2006 annual report placed him in the $500,000 to $999,999 category. Limbaugh donated $320,000 during the 2007 Cure-a-Thon, which the Leukemia and Lymphoma Society reported had raised $3.1 million. On his radio program April 18, 2008, Limbaugh pledged $400,000 to the Leukemia and Lymphoma Society after being challenged by two listeners to increase his initial pledge of $300,000.
Marine Corps–Law Enforcement Foundation.
Limbaugh conducts an annual drive to help the Marine Corps–Law Enforcement Foundation collect contributions to provide scholarships for children of Marines and law enforcement officers/agents who have died in the line of duty. The foundation was the beneficiary of a record $2.1 million eBay auction in October 2007 after Limbaugh listed for sale a letter critical of him signed by 41 Democratic senators and pledged to match the selling price.
With the recent founding of his and his wife's company, Two if by Tea, they have pledged to donate at least $100,000 to the MC-LEF beginning in June 2011.
Personal life.
Limbaugh has had four marriages, three divorces, and no children. He was first married at the age of 26 to Roxy Maxine McNeely, a sales secretary at radio station WHB in Kansas City, Missouri. They were married at the Centenary United Methodist Church in Limbaugh's hometown of Cape Girardeau, Missouri on September 24, 1977. McNeely filed for divorce in March 1980, citing "incompatibility." They were formally divorced on July 10, 1980.
In 1983, Limbaugh married Michelle Sixta, a college student and usherette at the Kansas City Royals Stadium Club. They were divorced in 1990, and she remarried the following year.
On May 27, 1994, Limbaugh married Marta Fitzgerald, a 35-year-old aerobics instructor whom he met on the online service CompuServe in 1990. They were married at the house of U.S. Supreme Court Justice Clarence Thomas, who officiated. They were separated on June 11, 2004. Limbaugh announced his divorce on the air. The divorce was finalized in December 2004. In September 2004, Limbaugh became romantically involved with then-TV personality Daryn Kagan, and they broke up in February 2006.
Limbaugh has lived in Palm Beach, Florida since 1996. A friend recalls that Limbaugh "fell in love with Palm Beach...after visiting her over Memorial Day weekend in 1995." Unlike New York, Florida does not tax income, the stated reason Limbaugh moved his residence and established his "Southern Command".
On December 30, 2009, while vacationing in Honolulu, Hawaii, Limbaugh was admitted to Queen's Medical Center with intense chest pains. His doctors attributed the pain to angina pectoris.
He dated Kathryn Rogers, a party planner from Florida, for three years before he married her on June 5, 2010. During the wedding reception after the ceremony, Elton John entertained the wedding guests for a reported $1 million fee; however, Limbaugh himself denied that the $1 million figure was accurate on his September 7, 2010, radio show.
Through a holding company, KARHL Holdings (KARHL meaning "Kathryn and Rush Hudson Limbaugh"), Limbaugh launched a line of bottled iced tea beverages, entitled "Two if by Tea" a play on the line from Henry Wadsworth Longfellow's "Paul Revere's Ride "one if by land, two if by sea". KARHL Holdings features a Rush Revere website where children can send notes to Liberty, the time-traveling, talking horse.
Prescription drug addiction.
On October 3, 2003, the "National Enquirer" reported that Limbaugh was being investigated for illegally obtaining the prescription drugs oxycodone and hydrocodone. Other news outlets quickly confirmed the investigation. He admitted to listeners on his radio show on October 10, 2003, that he was addicted to prescription painkillers and stated that he would enter inpatient treatment for 30 days, immediately after the broadcast. Limbaugh stated his addiction to painkillers resulted from several years of severe back pain heightened by a botched surgery intended to correct those problems.
A subsequent investigation into whether Limbaugh had violated Florida's doctor shopping laws was launched by the Palm Beach State Attorney, which raised privacy issues when investigators seized Limbaugh's private medical records looking for evidence of crimes. Roy Black, one of Limbaugh's attorneys, stated that "Rush Limbaugh was singled out for prosecution because of who he is. We believe the state attorney's office is applying a double standard." On November 9, 2005, following two years of investigations, Assistant State Attorney James L. Martz requested that the court set aside Limbaugh's doctor–patient confidentiality rights and allow the state to question his physicians. Limbaugh's attorney opposed the prosecutor's efforts to interview his doctors on the basis of patient privacy rights, and argued that the prosecutor had violated Limbaugh's Fourth Amendment rights by illegally seizing his medical records. The American Civil Liberties Union issued a statement in agreement and filed an amicus curiae brief in support of Limbaugh. On December 12, 2005, Judge David F. Crow delivered a ruling prohibiting the State of Florida from questioning Limbaugh's physicians about "the medical condition of the patient and any information disclosed to the health care practitioner by the patient in the course of the care and treatment of the patient."
On April 28, 2006, a warrant was issued for his arrest on the charge of doctor shopping. According to Teri Barbera, spokeswoman for the sheriff, during his arrest, Limbaugh was booked, photographed, and fingerprinted, but not handcuffed. He was then released after about an hour on $3,000 bail. After his surrender, he filed a "not guilty" plea to the charge. Prosecutors explained that the charges were brought after they discovered he received about 2,000 painkillers, prescribed by four doctors in six months, at a pharmacy near his Palm Beach mansion. In 2009, after 3 years of prolonged discussion regarding a settlement, prosecutors agreed to drop the charge if Limbaugh paid $30,000 to defray the cost of the investigation, completed an 18-month therapy regimen with his physician, submitted to random drug testing, and gave up his right to own a firearm for eighteen months. Limbaugh agreed to the settlement, though he continued to maintain his innocence of doctor shopping and asserted that the state's offer resulted from a lack of evidence supporting the charge.
Before his addiction became known, Limbaugh had condemned illegal drug use on his television program, stating that "Drug use, some might say, is destroying this country... And so if people are violating the law by doing drugs, they ought to be accused and they ought to be convicted and they ought to be sent up."
In June 2006, Limbaugh was detained by drug enforcement agents at Palm Beach International Airport. Customs officials confiscated Viagra from Limbaugh's luggage as he was returning from the Dominican Republic. The prescription was not in Limbaugh's name. After he was released with no charges filed, Limbaugh joked about the incident on his radio show, claiming that he got the Viagra at the Clinton Library and was told they were blue M&M's. He also stated that "I had a great time in the Dominican Republic. Wish I could tell you about it."
Cigar aficionado.
In the early 1990s, when the cigar boom was gaining momentum, Limbaugh was seen frequently with a cigar in hand and by the end of the 1990s, cigars had become Limbaugh's staple in many public appearances. Often starting segments of his show with the phrase, "Amid billowing clouds of fragrant and aromatic first-, second-, and sometimes third-hand premium cigar smoke" as well as mentioning a story print-out in his "formerly nicotine-stained fingers", cigars became a common topic of discussion. In the spring of 1994, Limbaugh appeared on the cover of the magazine "Cigar Aficionado" and shared the story of his conversion to cigars. He has since been a frequent participant in many events such as "The Big Smoke", hosted throughout the year by the magazine. Limbaugh has participated in many charity cigar auctions hosted by the magazine, and is known to talk frequently with his listeners about his and their cigar interests, preferences and recommendations. "I think cigars are just a tremendous addition to the enjoyment of life." He is also frequently seen in his studio smoking a cigar during his show.
Deafness.
Rush Limbaugh has described himself as being "100 percent, totally deaf". In 2001, Limbaugh announced that he had lost most of his ability to hear: "I cannot hear television. I cannot hear music. I am, for all practical purposes, deaf – and it's happened in three months." He said that the condition was not genetic. On December 19, 2001, doctors at the House Ear Clinic in Los Angeles were able to successfully restore a measure of his hearing through cochlear implant surgery. Limbaugh received a Clarion CII Bionic Ear.
When questioned whether Limbaugh's sudden hearing loss was caused by his addiction to opiates, his cochlear implant doctor, otolaryngologist Jennifer Derebery, said that it was possible but that there is no way to know for sure without performing tests that would destroy Limbaugh's hearing completely. "We don't know why some people, but apparently not most, who take large doses may lose their hearing".
In 2005, Limbaugh was forced to undergo "tuning" due to an "eye twitch", an apparent side-effect of cochlear implants.
On April 8, 2014, on his radio program, Limbaugh announced his decision to 'go bilateral'. "I'm going to get an implant on the right side", he said. After bilateral tuning, there was 100% improvement. "Coming from total deafness, it is miraculous! How can you not believe in God?" Limbaugh said in his national daily broadcast.
Controversies and criticisms.
Michael J. Fox.
In October 2006 Limbaugh said Michael J. Fox, who suffers from Parkinson's disease, had exaggerated the effects of his affliction in political TV ad advocating for funding of stem cell research. Limbaugh said that Fox in the ad had been "shameless" in "moving all around and shaking", and Fox had not taken "his medication or he's acting, one of the two". Fox said "the irony of it is I was too medicated", adding that there was no way to predict how his symptoms would manifest. Limbaugh said he would apologize to Fox "bigley and hugely...if I am wrong in characterizing his behavior on this commercial as an act". In 2012, Fox said Limbaugh in 2006 had acted on "bullying instincts" when "he said I faked it. I didn't fake it", and said Limbaugh's goal was to have him marginalized and shut down for his stem cell stance.
Phony soldiers.
Media Matters' reported radio talk show host Rush Limbaugh in 2007 saying that Iraq War veterans opposed to the war as "the phony soldiers". Limbaugh later said that he was speaking of Jesse MacBeth, a soldier who falsely claimed to have been decorated for valor but, in fact, had never seen combat. Limbaugh said Media Matters was trying to smear him with out of context and selectively edited comments. After Limbaugh published what he said was the entire transcript of phony soldiers discussion, Media Matters said that over a minute and 30 seconds of the transcript was omitted without "notation or ellipsis to indicate that there is, in fact, a break in the transcript." Limbaugh said during the minute and a half gap Media Matters had pointed out, he was waiting for relevant ABC news copy on the topic, and the transcript and audio edits were "for space and relevance reasons, not to hide anything." Senator Harry Reid and 41 Democrats, including Hillary Clinton, signed a letter asking the CEO of Clear Channel to denounce Limbaugh. Instead, he gave the letter to Limbaugh to auction. It raised over $2 million for the Marine-Corps Law Enforcement Foundation.
Sandra Fluke.
On February 29, 2012, Rush Limbaugh while talking about contraceptive mandates, included remarks about law student Sandra Fluke as a "slut" and "prostitute". Limbaugh was commenting on Fluke's speech the previous week to House Democrats in support of mandating insurance coverage for contraceptives. Limbaugh made numerous similar statements over the next two days, leading to the loss of 45 to "more than 100" local and national sponsors and Limbaugh's apology on his show for some of his comments.

</doc>
<doc id="25428" url="http://en.wikipedia.org/wiki?curid=25428" title="Roman Polanski">
Roman Polanski

Roman Polanski (born Rajmund Roman Thierry Polański; 18 August 1933) is a Polish and, since 1976, naturalized-French film director, producer, writer, and actor. Having made films in Poland, the United Kingdom, France and the United States, he is considered one of the few "truly international filmmakers." Polanski's films have inspired diverse directors, including the Coen brothers, Wes Anderson, David Fincher, Atom Egoyan, Darren Aronofsky, Park Chan-wook, Sean Durkin, Abel Ferrara, and Wes Craven.
Born in Paris to Polish parents, he moved with his family back to Poland (Second Polish Republic) in 1937, shortly before the outbreak of World War II. He survived the Holocaust, was educated in Poland (People's Republic of Poland), and became a director of both art house and commercial films. Polanski's first feature-length film, "Knife in the Water" (1962), made in Poland, was nominated for a United States Academy Award for Best Foreign Language Film but was beaten by Federico Fellini's "8½". He has since received five more Oscar nominations, along with two Baftas, four Césars, a Golden Globe Award and the Palme d'Or of the Cannes Film Festival in France. In the United Kingdom he directed three films, beginning with "Repulsion" (1965). In 1968 he moved to the United States and cemented his status by directing the horror film "Rosemary's Baby" (1968), for which Ruth Gordon won an Academy Award as Best Supporting Actress.
In 1969, Polanski's pregnant wife, Sharon Tate, was murdered by members of the Manson Family while staying at Polanski's Benedict Canyon home above Los Angeles. Following Tate's death, Polanski returned to Europe and spent much of his time in Paris and Gstaad, but did not direct another film until "Macbeth" (1971) in England. The following year he went to Italy to make "What?" (1973) and subsequently spent the next five years living near Rome. However, he travelled to Hollywood to direct "Chinatown" (1974). The film was nominated for eleven Academy Awards, and was a critical and box-office success. Polanski's next film, "The Tenant" (1976), was shot in France, and completed the "Apartment Trilogy", following "Repulsion" and "Rosemary's Baby".
In 1977, after a photo shoot in Los Angeles, Polanski was arrested for the rape of a 13-year-old girl and pleaded guilty to the charge of unlawful sex with a minor. To avoid sentencing, Polanski fled to his home in London, eventually settling in France. More than 32 years later, in September 2009, he was temporarily arrested by Swiss police at the request of United States authorities, who unsuccessfully asked for his extradition. During an interview for a later film documentary, he offered his apology to the woman, and later said that he had regretted that episode for the last 33 years.
Polanski continued to make films such as "The Pianist" (2002), a WWII true story drama about Jewish-Polish musician Władysław Szpilman. The film won three Academy Awards including Best Director, along with numerous international awards. He also directed other films, including "Oliver Twist" (2005), a story which parallels his own life as a "young boy attempting to triumph over adversity". In 2009 at age 76 he was due to be awarded a lifetime-achievement award at the Zurich Film Festival but was arrested and held in a detention center for 67 days upon arrival in Zurich by the authorities in Switzerland at the request of United States authorities. After posting $4.5 million bail, he was allowed to live at his chalet in Zurich under house arrest, wearing an ankle monitor, for nine months after which he was freed. Due to Polanski's arrest, post-production for the film he was working on, "The Ghost Writer", was briefly put on hold, but he resumed and completed work during house arrest at his Swiss villa. He was unable to participate in the film's world premiere at the Berlinale festival on February 12, 2010. He was awarded Best Director at the "The Ghost Writer" (2010) at the 23rd European Film Awards that year but was unable to receive it due to his retention by the Swiss authorities. In 2011, he was able to travel to Zurich from his home in France to receive the award. as well as Best Director at the 60th Berlin International Film Festival, and "Carnage" (2011), a comedy-drama starring Jodie Foster and Kate Winslet.
Early life.
Polanski was born as Rajmund Roman Thierry Polański in Paris, France on 18 August 1933, the son of Bula (née Katz-Przedborska) and Ryszard Polański, a painter and manufacturer of sculptures, who had changed his family name from Liebling. His mother had a daughter, Annette, by her previous husband. Annette managed to survive Auschwitz, where her mother died, and left Poland forever for France. Polański's Polish-born father was Jewish; Polański's Russian-born mother had been raised Roman Catholic, and was of half Jewish ancestry. Polański's parents were both agnostics. Polański, influenced by his education in the People's Republic of Poland, said "I'm an atheist" in an interview about his film, "Rosemary's Baby".
World War II.
The Polański family moved back to the Polish city of Kraków in 1936, and were living there when World War II began with the invasion of Poland. Kraków was soon occupied by the German forces, and Nazi racial purity laws made the Polańskis targets of persecution, forcing them into the Kraków Ghetto, along with thousands of the city's Jews. Around the age of five, he attended primary school for only a few weeks, until "all the Jewish children were abruptly expelled," writes biographer Christopher Sandford. That initiative was soon followed by requiring all Jewish children over the age of twelve to wear white armbands with a blue Star of David imprinted for visual identification. After he was expelled, he would not be allowed to enter another classroom for the next six years.:18 Polanski then witnessed both the ghettoization of Kraków's Jews into a compact area of the city, and the subsequent deportation of all the ghetto's Jews to concentration camps, including watching as his father was taken away. He remembers from age six, one of his first experiences of the terrors to follow:
I had just been visiting my grandmother ... when I received a foretaste of things to come. At first I didn't know what was happening. I simply saw people scattering in all directions. Then I realized why the street had emptied so quickly. Some women were being herded along it by German soldiers. Instead of running away like the rest, I felt compelled to watch.
One older woman at the rear of the column couldn't keep up. A German officer kept prodding her back into line, but she fell down on all fours, ... Suddenly a pistol appeared in the officer's hand. There was a loud bang, and blood came welling out of her back. I ran straight into the nearest building, squeezed into a smelly recess beneath some wooden stairs, and didn't come out for hours. I developed a strange habit: clenching my fists so hard that my palms became permanently calloused. I also woke up one morning to find that I had wet my bed.
His father was transferred, along with thousands of other Jews, to Mauthausen, a group of 49 German concentration camps in Austria. His mother was taken to Auschwitz and was killed soon after arriving. The forced exodus took place immediately after the German liquidation of the Kraków ghetto, a true-life backdrop to Polanski's film, "The Pianist" (2002). Polanski, who was then hiding from the Germans, remembered seeing his father being marched off with a long line of people. Polanski tried getting closer to his father to ask him what was happening, and managed to get within a few yards. His father saw him, but afraid his son might be spotted by the German soldiers, whispered (in Polish), "Get lost!":24 Polański escaped the Kraków Ghetto in 1943 and survived by assuming the name Romek Wilk, with the help of some Polish Roman Catholic families including Mrs Sermak who promised his father to shelter him.:21 He attended church, learned to recite Catholic prayers by heart, and behaved outwardly as a Roman Catholic, although he was never baptized. His efforts to blend into a Catholic household failed miserably at least once, when the parish priest visiting the family posed questions to him one-on-one about the catechism: "You aren't one of us", he said.
As he roamed the countryside trying to survive in a Poland now occupied by German troops, he witnessed many horrors, such as being "forced to take part in a cruel and sadistic game in which German soldiers took shots at him for target practice." Author Ian Freer concludes that his constant childhood fears and dread of violence have contributed to the "tangible atmospheres he conjures up on film."
By the time the war ended in 1945, a fifth of the Polish population had been killed, with the vast majority of the victims being civilians. Of those deaths, 3 million were of Polish Jews, 90% of the country's Jewish population. According to Sandford, Polanski would use the memory of his mother, her dress and makeup style, as a physical model for Faye Dunaway's character in his film "Chinatown" (1974).:13
After the war.
After the war he was reunited with his father and moved back to Kraków. His father remarried 21 December 1946 to Wanda Zajączkowska (a woman Polanski had never liked) and died of cancer in 1984. Time repaired the family contacts; Polanski visited them in Kraków, and relatives visited him in Hollywood and Paris. Polanski recalls the villages and families he lived with as relatively primitive by European standards:
They were really simple Catholic peasants. This Polish village was like the English village in "Tess." Very primitive. No electricity. The kids with whom I lived didn't know about electricity ... they wouldn't believe me when I told them it was enough to turn on a switch!
He stated that "you must live in a Communist country to really understand how bad it can be. Then you will appreciate capitalism." He also remembered events at the war's end and his reintroduction to mainstream society when he was 12, forming friendships with other children, such as Roma Ligocka, Ryszard Horowitz and his family.
Introduction to movies.
Polanski's fascination with cinema began very early, when he was around age four or five. He recalls this period in an interview:
Even as a child, I always loved cinema and was thrilled when my parents would take me before the war. Then we were put into the ghetto in Krakòw and there was no cinema, but the Germans often showed newsreels to the people outside the ghetto, on a screen in the market place. And there was one particular corner where you could see the screen through the barbed wire. I remember watching with fascination, although all they were showing was the German army and German tanks, with occasional anti-Jewish slogans inserted on cards.
After the war, he watched films, either at school or at a local cinema, using whatever pocket money he had. Polanski writes, "Most of this went on the movies, but movie seats were dirt cheap, so a little went a long way. I lapped up every kind of film." As time went on, movies became more than an escape into entertainment, as he explains:
Movies were becoming an absolute obsession with me. I was enthralled by everything connected with the cinema — not just the movies themselves but the aura that surrounded them. I loved the luminous rectangle of the screen, the sight of the beam slicing through the darkness from the projection booth, the miraculous synchronization of sound and vision, even the dusty smell of the tip-up seats. More than anything else though, I was fascinated by the actual mechanics of the process.
Early career in Poland.
Polanski attended the National Film School in Łódź, the third-largest city in Poland. In the 1950s Polanski took up acting, appearing in Andrzej Wajda's "Pokolenie" ("A Generation", 1954) and in the same year in Silik Sternfeld's "Zaczarowany rower" ("Enchanted Bicycle" or "Magical Bicycle"). Polanski's directorial debut was also in 1955 with a short film "Rower" ("Bicycle"). "Rower" is a semi-autobiographical feature film, believed to be lost, which also starred Polanski. It refers to his real-life violent altercation with a notorious Kraków felon, Janusz Dziuba, who arranged to sell Polanski a bicycle, but instead beat him badly and stole his money. In real life the offender was arrested while fleeing after fracturing Polanski's skull, and executed for three murders, out of eight prior such assaults which he had committed. Several other short films made during his study at Łódź gained him considerable recognition, particularly "Two Men and a Wardrobe" (1958) and "When Angels Fall" (1959). He graduated in 1959.
Film director.
1960s.
Polanski's first feature-length film, "Knife in the Water", was also one of the first significant Polish films after the Second World War that did not have a war theme. Scripted by Jerzy Skolimowski, Jakub Goldberg, and Polanski, "Knife in the Water" is about a wealthy, unhappily married couple who decide to take a mysterious hitchhiker with them on a weekend boating excursion. A dark and unsettling work, Polanski's debut feature subtly evinces a profound pessimism about human relationships with regard to the psychological dynamics and moral consequences of status envy and sexual jealousy. "Knife in the Water" was a major commercial success in the West and gave Polanski an international reputation. The film also earned its director his first Academy Award nomination (Best Foreign Language Film) in 1963. Leon Niemczyk, who played Andrzej, was the only professional actor in the film. Jolanta Umecka, who played Krystyna, was discovered by Polanski at a swimming pool.
Polanski left then-communist Poland and moved to France, where he had already made two notable short films in 1961: "The Fat and the Lean" and "Mammals". While in France, Polanski contributed one segment ("La rivière de diamants") to the French-produced omnibus film, "Les plus belles escroqueries du monde" (English title: "The Beautiful Swindlers") in 1964. However, Polanski found that in the early 1960s the French film industry was xenophobic and generally unwilling to support a rising filmmaker of foreign origin.
Polanski made three feature films in England, based on original scripts written by himself and Gérard Brach, a frequent collaborator. "Repulsion" (1965) is a psychological horror film focusing on a young Belgian woman named Carol (Catherine Deneuve), who is living in London with her older sister (Yvonne Furneaux). The film's themes, situations, visual motifs, and effects clearly reflect the influence of early surrealist cinema as well as horror movies of the 1950s — particularly Luis Buñuel's "Un chien Andalou", Jean Cocteau's "The Blood of a Poet", Henri-Georges Clouzot's "Diabolique" and Alfred Hitchcock's "Psycho".
"Cul-de-sac" (1966) is a bleak nihilist tragicomedy filmed on location in Northumberland. The tone and premise of the film owe a great deal to Samuel Beckett's "Waiting for Godot", along with aspects of Harold Pinter's "The Birthday Party".
"The Fearless Vampire Killers" (1967) (known by its original title, "Dance of the Vampires" in most countries outside the United States) is a parody of vampire films. The plot concerns a buffoonish professor and his clumsy assistant, Alfred (played by Polanski), who are traveling through Transylvania in search of vampires. The ironic and macabre ending is considered classic Polanski. "The Fearless Vampire Killers" was Polanski's first feature to be photographed in color with the use of Panavision lenses, and included a striking visual style with snow-covered, fairy-tale landscapes, similar to the work of Soviet fantasy filmmakers. In addition, the richly textured color schemes of the settings evoke the magical, kaleidoscopic paintings of the great Belarussian-Jewish artist Marc Chagall, who provides the namesake for the innkeeper in the film. The film was written for Jack MacGowran, who played the lead role of Professor Abronsius.
Polanski met Sharon Tate while the film was being made, where she played the role of the local innkeeper's daughter. They were married in London on 20 January 1968.
Paramount studio head Robert Evans brought Polanski to America ostensibly to direct the film "Downhill Racer", but told Polanski that he really wanted to him to read the horror novel "Rosemary's Baby" by Ira Levin to see if a film could be made out of it. Polanski read it non-stop through the night and the following morning decided he wanted to write as well as direct it. He wrote the 272-page screenplay for the film in slightly longer than three weeks. The film, "Rosemary's Baby" (1968), was a box-office success and became his first Hollywood production, thereby establishing his reputation as a major commercial filmmaker. The film, a horror-thriller set in trendy Manhattan, is about Rosemary Woodhouse (Mia Farrow), a young housewife who is impregnated by the devil. Polanski's screenplay adaptation earned him a second Academy Award nomination.
On 9 August 1969, while Polanski was working in London, his wife, Sharon Tate, and four other people were murdered at the Polanskis' residence in Los Angeles.
1970s.
Polanski adapted "Macbeth" into a screenplay with the Shakespeare expert Kenneth Tynan. Jon Finch and Francesca Annis played the main characters. Hugh Hefner and Playboy Productions funded the 1971 film, which opened in New York and was screened in Playboy Theater. Hefner was credited as executive producer, and the film was listed as a "Playboy Production". It was controversial because of Lady Macbeth's being nude in a scene, and received an X rating because of its graphic violence and nudity. In his autobiography, Polanski wrote that he wanted to be true to the violent nature of the work, and that he had been aware that his first project following Tate's murder, would be subject to scrutiny and probable criticism regardless of the subject matter; if he had made a comedy he would have been perceived as callous.
Written by Polanski and previous collaborator Gérard Brach, "What?" (1973) is a mordant absurdist comedy loosely based on the themes of "Alice in Wonderland" and Henry James. The film is a rambling shaggy dog story about the sexual indignities that befall a winsome young American hippie woman hitchhiking through Europe.
Polanski returned to Hollywood in 1973 to direct "Chinatown" (1974) for Paramount Pictures. The film is widely considered to be one of the finest American crime movies and was nominated for 11 Academy Awards. The stars, Jack Nicholson and Faye Dunaway, both received Oscar nominations, and the script by Robert Towne won for Best Original Screenplay. Polanski appears in a cameo role.
Polanski returned to Paris for his next film, "The Tenant" (1976), which was based on a 1964 novel by Roland Topor, a French writer of Polish-Jewish origin. In addition to directing the film, Polanski also played a leading role of a timid Polish immigrant living in Paris. Together with "Repulsion" and "Rosemary's Baby", "The Tenant" can be seen as the third installment in a loose trilogy of films called the "Apartment Trilogy" that explore the themes of social alienation and psychic and emotional breakdown.
In 1978, Polanski became a fugitive from American justice and could no longer work in countries where he might face arrest or extradition.
He dedicated his next film, "Tess" (1979), to the memory of his late wife, Sharon Tate. It was Tate who suggested to Polanski that he read "Tess of the d'Urbervilles", as she felt it might make a good film. Nastassja Kinski appeared in the title role opposite Peter Firth and Leigh Lawson.
"Tess" was shot in the north of France instead of Hardy's England and became the most expensive film made in France up to that time. Ultimately, it proved a financial success and was well received by both critics and the public. Polanski won France's César Awards for Best Picture and Best Director and received his fourth Academy Award nomination (and his second nomination for Best Director). The film received three Oscars: best cinematography, best art direction, best costume design, and was nominated for best picture.
1980s.
In 1981, Polanski directed and co-starred (as Mozart) in a stage production of Peter Shaffer's play "Amadeus", first in Warsaw, then in Paris. The play was again directed by Polanski, in Milan, in 1999.
Nearly seven years passed before Polanski's next film, "Pirates", a lavish period piece starring Walter Matthau as Captain Red, which the director intended as an homage to the beloved Errol Flynn swashbucklers of his childhood. Captain Red's henchman, Jean Baptiste, was played by Cris Campion. The film is about a rebellion the two lead on a ship called the "Neptune", in the seventeenth century. The screenplay was written by Polanski, Gérard Brach, and John Brownjohn. The film was shot on location in Tunisia, using a full sized pirate vessel constructed for the production. It was a financial and critical failure, recovering a small fraction of its production budget and garnering a single Academy Award nomination.
"Frantic" (1988) was a Hitchcockian suspense-thriller starring Harrison Ford and the actress/model Emmanuelle Seigner, who later became Polanski's wife. The film follows an ordinary tourist in Paris whose wife is kidnapped. He attempts, hopelessly, to go through the Byzantine bureaucratic channels to deal with her disappearance, but finally takes matters into his own hands.
1990s.
Polanski followed this with the dark psycho-sexual film "Bitter Moon" (1992), followed by a film of the acclaimed play "Death and the Maiden" (1994) starring Sigourney Weaver.
"The Ninth Gate" is a thriller based on the novel "El Club Dumas" by Arturo Perez-Reverte and starring Johnny Depp. The movie's plot is based on the idea that an ancient text called "The Nine Gates of the Kingdom of Shadow", authored by Aristide Torchia along with Lucifer, is the key to raising Satan.
In 1997, Polanski directed a stage version of his 1967 film "The Fearless Vampire Killers", which debuted in Vienna followed by successful runs in Stuttgart, Hamburg, Berlin, and Budapest. On 11 March 1998, Polanski was elected a member of the Académie des Beaux-Arts.
2000s.
In 2001, Polanski filmed "The Pianist", an adaptation of the WWII autobiography of the same name by Polish-Jewish musician Władysław Szpilman. Szpilman's experiences as a persecuted Jew in Poland during WWII were reminiscent of those of Polanski and his family. While Szpilman and Polanski escaped the concentration camps, their families did not, eventually perishing.
When Warsaw, Poland was chosen for the 2002 premiere of "The Pianist", "the country exploded with pride." According to reports, numerous former communists came to the screening and "agreed that it was a fantastic film."
In May 2002, the film won the "Palme d'Or" (Golden Palm) award at the Cannes Film Festival, as well as Césars for Best Film and Best Director, and later the 2002 Academy Award for Directing. Because he would have been arrested in the United States, Polanski did not attend the Academy Awards ceremony in Hollywood. After the announcement of the Best Director Award, Polanski received a standing ovation from most of those present in the theater. Actor Harrison Ford accepted the award for Polanski, and then presented the Oscar to him at the Deauville Film Festival five months later in a public ceremony. Polanski later received the Crystal Globe award for outstanding artistic contribution to world cinema at the Karlovy Vary International Film Festival in 2004.
"Oliver Twist" is an adaptation of Dickens's classic, written by "The Pianist"'s Ronald Harwood and shot in Prague. Polanski said in interviews that he made the film as something he could show his children, and that the life of the young scavenger mirrored his own life, fending for himself in World War II Poland.
2010s.
"The Ghost Writer", a thriller focusing on a ghostwriter working on the memoirs of a character based loosely on former British prime minister Tony Blair, swept the European Film Awards in 2010, winning six awards, including best movie, director, actor and screenplay. When it premiered at the 60th Berlinale in February 2010, Polanski won a Silver Bear for Best Director, and in February 2011, it won four César Awards, France's version of the Academy Awards.
The film is based on a novel by British writer Robert Harris. Harris and Polanski had previously worked for many months on a film of Harris's earlier novel "Pompeii", a novel that was actually inspired by Polanski's "Chinatown". They had completed a script for "Pompeii" and were nearing production when the film was cancelled due to a looming actors' strike in September 2007. After that film fell apart, they moved on to Harris's novel, The Ghost, and adapted it for the screen together.
The cast includes Ewan McGregor as the writer and Pierce Brosnan as former British Prime Minister Adam Lang. The film was shot on locations in Germany.
In the United States, film critic Roger Ebert included it in his top 10 pick for 2010, and states that "this movie is the work of a man who knows how to direct a thriller. Smooth, calm, confident, it builds suspense instead of depending on shock and action." Co-star Ewan McGregor agrees, saying about Polanski that "he's a legend... I've never examined a director and the way that they work, so much before. He's brilliant, just brilliant, and absolutely warrants his reputation as a great director."
Polanski shot "Carnage" in February/March 2011. The film is a screen version of Yasmina Reza's play "God of Carnage", a comedy about the relationship between two couples after their children get in a fight at school and the selfishness of everyone, which eventually leads to chaos. It stars Kate Winslet, Jodie Foster, Christoph Waltz and John C. Reilly. Though set in New York, it was shot in Paris. The film had its world premiere on 9 September 2011 at the Venice Film Festival and was released in the United States by Sony Pictures Classics on 16 December 2011.
Co-stars Jodie Foster and Kate Winslet commented about Polanski's directing style. According to Foster, "He has a very, very definitive style about how he likes it done. He decides everything. He decided every lens. Every prop. Everything. It's all him." Winslet adds that "Roman is one of the most extraordinary men I've ever met. The guy is 77 years old. He has an effervescent quality to him. He's very joyful about his work, which is infectious. He likes to have a small crew, to the point that, when I walked on the set, my thought was, 'My God, this is it?'” Also noting that style of directing, New York Film Festival director Richard Pena, during the American premiere of the film, called Polanski "a poet of small spaces... in just a couple of rooms he can conjure up an entire world, an entire society."
His latest film is an adaptation of the award-winning play "Venus in Fur", starring his wife Emmanuelle Seigner and Mathieu Amalric. Polanski worked with the play's author, David Ives on the screenplay. The film was shot from December 2012 to February 2013 in French and is Polanski's first non-English language feature film in forty years. The film premiered in competition at the 2013 Cannes Film Festival on 25 May 2013.
Polanski is currently preparing to shoot "D", a film about the notorious Dreyfus affair in the 19th century, in which one of the few Jewish members of the French Army's general staff was wrongly convicted of passing military secrets to the German Empire and sent to Devil's Island, only to be acquitted 12 years later. The film is written by Robert Harris, who is working with Polanski for the third time. The film is scheduled to begin production in Warsaw in July, 2015.
Marriages and relationships.
Barbara Kwiatkowska-Lass.
Polanski's first wife, Barbara Lass (née Kwiatkowska), was a Polish actress who also starred in Polanski's 1959 "When Angels Fall". The couple were married in 1959 and divorced in 1961.
Sharon Tate.
Polanski met rising actress Sharon Tate while filming "The Fearless Vampire Killers", and during the production the two of them began dating. On 20 January 1968, Polanski married Tate in London.
In August 1969, while Polanski was in Europe working on a film, Tate was murdered along with four of their friends at their home in Los Angeles by members of Charles Manson's "family," a group of young, gullible, and mostly female followers. Tate was pregnant at the time of her murder.
Manson, along with members of his "family", was arrested in late 1969, and eventually tried and found guilty in 1971 of 27 counts, including first-degree murder, an event now called the Manson murders. Because at the time it was one of the most "horrific crimes in modern history," the crime and trial of Manson and his followers became a media sensation, leading to movies, documentaries and bestselling books.
Polanski has said that his absence on the night of the murders is the greatest regret of his life. In his autobiography, he wrote, "Sharon's death is the only watershed in my life that really matters", and commented that her murder changed his personality from a "boundless, untroubled sea of expectations and optimism" to one of "ingrained pessimism ... eternal dissatisfaction with life". In his autobiography, Polanski described his brief time with Tate as the best years of his life.
Polanski was also left with a very negative impression of the press, which he felt was interested in sensationalizing the lives of the victims, and indirectly himself, to attract readers. He was shocked by the lack of sympathy expressed in various news stories:
I had long known that it was impossible for a journalist to convey 100 percent of the truth, but I didn't realize to what extent the truth is distorted, both by the intentions of the journalist and by neglect. I don't mean just the interpretations of what happened; I also mean the facts. The reporting about Sharon and the murders was virtually criminal. Reading the papers, I could not believe my eyes. I could not believe my eyes! They blamed the victims for their own murders. I really despise the press. I didn't always. The press made me despise it.
Among the media-generated sensationalism were rumors that claimed Tate and her visitors were taking drugs, despite the coroner announcing that no traces of drugs or nicotine were found after Tate's autopsy. For years afterward, notes Sandford, "reporters openly speculated about the Polanskis' home life" and their personalities in order to create more media gossip about the private lives of Hollywood celebrities.:2
Nastassja Kinski.
In 1976, Polanski started a romantic relationship with Nastassja Kinski, who starred in "Tess". She was between 15 and 17 years old at the time and he was 43. Their relationship ended at the completion of filming. In an interview with David Letterman in 1982, she described their relationship and gave her opinion about his sexual assault case, claiming it was "ridiculous" and his residence in France was "a loss for America."
Emmanuelle Seigner.
In 1989, Polanski married French actress Emmanuelle Seigner, 33 years his junior. They have two children, daughter Morgane and son Elvis. Polanski and his children speak Polish at home.
Legal history.
Sexual abuse case.
On 11 March 1977, Polanski, then 43 years old, was arrested in Los Angeles for the sexual assault of 13-year-old Samantha Geimer during a photo shoot for French "Vogue" magazine. Polanski was indicted on six counts of criminal behavior, including rape. At his arraignment he pleaded not guilty to all charges. Many executives in Hollywood came to his defense.
Geimer's attorney next arranged a plea bargain in which five of the six charges would be dismissed, and Polanski accepted. Because Polanski fled the country before final sentencing, the charges were not dismissed and still remain pending.
As a result of the plea bargain, Polanski pleaded guilty to the charge of "Unlawful Sexual Intercourse with a minor," and was ordered to undergo 90 days of psychiatric evaluation at California Institution for Men at Chino. On release from prison after 42 days, Polanski understood that at the final sentencing he would be put on probation. However, he learned that the judge was planning to renege on his promise of no further jail time, and might even deport him. Polanski's attorney suggested that despite the fact that the prosecuting attorneys recommended probation, "the judge could no longer be trusted..." and the judge's representations were "worthless".
Upon learning of the judge's plans, Polanski fled to France on 1 February 1978, just hours before sentencing. As a French citizen, he has been protected from extradition and has lived mostly in France since then.
In an interview with Larry King, Geimer said that the police and media had been slow at the time of the assault to believe her account, which she attributed to the climate of the era. In 1988 she sued Polanski. Among other things, the suit alleged sexual assault, false imprisonment, seduction of a minor, and intentional infliction of emotional distress. In 1993 Polanski agreed to settle with Geimer. In August 1996 Polanski still owed her $604,416; Geimer and her lawyers later confirmed that the settlement was completed.
On 26 September 2009, Polanski was arrested while in Switzerland at the request of United States authorities. The arrest brought renewed attention to the case and stirred controversy, particularly in the United States and Europe. Polanski was defended by many prominent individuals, including Hollywood celebrities and European artists and politicians, who called for his release. American public opinion was reported to run against him, however, and polls in France and Poland showed strong majorities favored his extradition to the United States.
Polanski was jailed near Zürich for two months, then put under house arrest at his home in Gstaad while awaiting decision of appeals fighting extradition. On 12 July 2010 the Swiss rejected the United States request, declared him a "free man" and released him from custody. Polanski remains the subject of an Interpol red notice issued in 2005 at the request of the United States.
During a television interview on 10 March 2011, Geimer blamed the media, reporters, the court, and the judge for causing "way more damage to [her] and [her] family than anything Roman Polanski has ever done," and stated that the judge was using her and a noted celebrity for his own personal gain from the media exposure.
In January 2014, newly uncovered emails by a Los Angeles County Superior Court judge from 2008, indicated that if Polanski returned to the United States for a hearing, he might be compelled to free him because of the conduct by the judge who originally handled the case. The emails which were disclosed were related to a 2008 documentary film by Marina Zenovich. In late October 2014, Polanski was questioned by prosecutors in Kraków.
Documentary films.
In 2008 the documentary film by Marina Zenovich, "", was released in Europe and the United States where it won numerous awards. The film focuses on the judge in the case and the possible reasons why he changed his mind. It includes interviews with people involved in the case, including the victim, Geimer, and the prosecutor, Roger Gunson. Geimer said that the judge "didn't care what happened" to her or Polanski, but "was orchestrating some little show," while Gunson added, "I'm not surprised that Polanski left under those circumstances, ... it was going to be a real circus."
Former DA David Wells, whose statements were the most damning against Polanski, and who said he advised the judge to imprison Polanski, admitted that he lied about those statements, and said that to the press to "play up" his own role.
In December 2009, a California appellate court discussed the film's allegations as it denied Polanski's request to have the case dismissed. While saying they "deeply concerned" the court, and were "in many cases supported by considerable evidence," it also found that “(e)ven in light of our fundamental concern about the misconduct ... flight was not Polanski’s only option. It was not even his best option." It said dismissal of the case, which would erase Polanski's guilty plea, wouldn't be an "appropriate result," and that he still had other legal options.
In September 2011, the documentary film "" had its world premiere in Zürich, Switzerland. During an interview in the film, he offers his apology to Geimer: "She is a double victim: My victim, and a victim of the press." On this occasion, he collected the lifetime achievement award he was to receive at the time of his arrest two years earlier.
"Vanity Fair" libel case.
In 2004, Polanski sued "Vanity Fair" magazine in London for libel. A 2002 article in the magazine claimed that Polanski promised he would "make another Sharon Tate out of you" in an attempt to seduce a Scandinavian model while he was travelling to Tate's funeral. He received supporting testimony from Mia Farrow, and "Vanity Fair" "was unable to prove that the incident occurred." Polanski was awarded £50,000 in damages plus some of his legal costs.
Awards and nominations.
Other awards.
New York Film Critics Circle Awards
Venice Film Festival
References.
Bibliography.
</dl>

</doc>
<doc id="25431" url="http://en.wikipedia.org/wiki?curid=25431" title="Russian language">
Russian language

Russian (ру́сский язы́к, "russky yazyk", pronounced ]) is an East Slavic language and an official language in Russia, Belarus, Kazakhstan, and Kyrgyzstan. It is an unofficial but widely spoken language in Ukraine, Moldova, Latvia, Estonia, and to a lesser extent, the other countries that were once constituent republics of the Soviet Union and former participants of the Eastern Bloc. Russian belongs to the family of Indo-European languages and is one of the three living members of the East Slavic languages. Written examples of Old East Slavonic are attested from the 10th century onwards.
It is the most geographically widespread language of Eurasia and the most widely spoken of the Slavic languages. It is also the largest native language in Europe, with 144 million native speakers in Russia, Ukraine and Belarus. Russian is the eighth most spoken language in the world by number of native speakers and the seventh by total number of speakers. The language is one of the six official languages of the United Nations.
Russian distinguishes between consonant phonemes with palatal secondary articulation and those without, the so-called "soft" and "hard" sounds. This distinction is found between pairs of almost all consonants and is one of the most distinguishing features of the language. Another important aspect is the reduction of unstressed vowels. Stress, which is unpredictable, is not normally indicated orthographically though an optional acute accent (, "znak udareniya") may be used to mark stress, such as to distinguish between homographic words, for example (zamok, meaning "lock") and (zamok, meaning "castle"), or to indicate the proper pronunciation of uncommon words or names.
Classification.
Russian is a Slavic language of the Indo-European family. It is a lineal descendant of the language used in Kievan Rus'. From the point of view of the spoken language, its closest relatives are Ukrainian, Belarusian, and Rusyn, the other three languages in the East Slavic group. In many places in eastern and southern Ukraine and throughout Belarus, these languages are spoken interchangeably, and in certain areas traditional bilingualism resulted in language mixtures, e.g. Surzhyk in eastern Ukraine and Trasianka in Belarus. An East Slavic Old Novgorod dialect, although vanished during the 15th or 16th century, is sometimes considered to have played a significant role in the formation of modern Russian. Also Russian has notable lexical similarities with Bulgarian due to a common Church Slavonic influence on both languages, as well as because of later interaction in the 19th–20th centuries, although Bulgarian grammar differs markedly from Russian. In the 19th century, the language was often called "Great Russian" to distinguish it from Belarusian, then called "White Russian" and Ukrainian, then called "Little Russian".
The vocabulary (mainly abstract and literary words), principles of word formations, and, to some extent, inflections and literary style of Russian have been also influenced by Church Slavonic, a developed and partly russified form of the South Slavic Old Church Slavonic language used by the Russian Orthodox Church. However, the East Slavic forms have tended to be used exclusively in the various dialects that are experiencing a rapid decline. In some cases, both the East Slavic and the Church Slavonic forms are in use, with many different meanings. "For details, see Russian phonology and History of the Russian language."
Over the course of centuries, the vocabulary and literary style of Russian have also been influenced by Western and Central European languages such as Greek, Latin, Polish, Dutch, German, French, Italian and English, and to a lesser extent the languages to the south and the east: Uralic, Turkic, Persian, Arabic, as well as Hebrew,
According to the Defense Language Institute in Monterey, California, Russian is classified as a level III language in terms of learning difficulty for native English speakers, requiring approximately 1,100 hours of immersion instruction to achieve intermediate fluency. It is also regarded by the United States Intelligence Community as a "hard target" language, due to both its difficulty to master for English speakers and its critical role in American world policy.
Standard Russian.
The standard well-known form of Russian is generally called the "modern Russian literary language" (современный русский литературный язык). It arose in the beginning of the 18th century with the modernization reforms of the Russian state under the rule of Peter the Great, and developed from the Moscow (Middle or Central Russian) dialect substratum under the influence of some of the previous century's Russian chancellery language.
Mikhail Lomonosov first compiled a normalizing grammar book in 1755; in 1783 the Russian Academy's first explanatory Russian dictionary appeared. During the end of the 18th and 19th centuries, during a period known as the "Golden Age", the grammar, vocabulary and pronunciation of the Russian language was stabilized and standardized, and it became the nationwide literary language; meanwhile, Russia's world-famous literature flourished.
Until the 20th century, the language's spoken form was the language of only the upper noble classes and urban population, as Russian peasants from the countryside continued to speak in their own dialects. By the mid-20th century, such dialects were forced out with the introduction of the compulsory education system that was established by the Soviet government. Despite the formalization of Standard Russian, some nonstandard dialectal features (such as fricative [ɣ] in Southern Russian dialects) are still observed in colloquial speech.
Geographic distribution.
During the Soviet period, the policy toward the languages of the various other ethnic groups fluctuated in practice. Though each of the constituent republics had its own official language, the unifying role and superior status was reserved for Russian, although it was declared the official language only in 1990. Following the break-up of the USSR in 1991, several of the newly independent states have encouraged their native languages, which has partly reversed the privileged status of Russian, though its role as the language of post-Soviet national discourse throughout the region has continued.
In 2010, there were 259.8 million speakers of Russian in the world: in Russia - 137.5, in the CIS and Baltic countries - 93.7, in Eastern Europe and the Balkans - 12.9, Western Europe - 7.3, Asia - 2.7, Middle East and North Africa - 1.3, Sub-Saharan Africa - 0.1, Latin America - 0.2, USA, Canada, Australia and New Zealand - 4.1. Thus, the Russian language is the 6th largest in the world by number of native speakers, after English, Chinese, Hindi/Urdu, Spanish and Arabic.
According to the census of 2010 in Russia Russian language skills were indicated by 138 million people (99.4% population), while according to the 2002 census - 142.6 million people (99.2% population). Among the urban residents 101 million people (99.8% population) had Russian language skills, while in rural areas - 37 million people (98.7% population).
In Latvia its official recognition and legality in the classroom have been a topic of considerable debate in a country where more than one-third of the population is Russian-speaking (see Russians in Latvia). Similarly, in Estonia, ethnic Russians constitute 25.5% of the country's current population and 58.6% of the native Estonian population is also able to speak Russian. In all, 67.8% of Estonia's population can speak Russian. Command of Russian language, however, is rapidly decreasing among younger Estonians (primarily being replaced by the command of English). For example, if 53% of ethnic Estonians between 15 and 19 claim to speak some Russian, then among the 10–14 year old group, command of Russian has fallen to 19% (which is about one-third the percentage of those who claim to have command of English in the same age group).
In Kazakhstan and Kyrgyzstan, Russian remains a co-official language with Kazakh and Kyrgyz, respectively. Large Russian-speaking communities still exist in northern Kazakhstan, and ethnic Russians comprise 25.6% of Kazakhstan's population.
Those who speak Russian as a mother or secondary language in Lithuania represent approximately 60% of the population of Lithuania. Also, more than half of the population of the Baltic states speak Russian either as a foreign language or as a mother tongue. As the Grand Duchy of Finland was part of the Russian Empire from 1809 to 1918, a number of Russian speakers have remained in Finland. There are 33,400 Russian-speaking Finns, amounting to 0.6% of the population. Five thousand (0.1%) of them are late 19th century and 20th century immigrants or their descendants, and the remaining majority are recent immigrants who moved there in the 1990s and later.
In the 20th century, Russian was mandatorily taught in the schools of the members of the old Warsaw Pact and in other countries that used to be satellites of the USSR. In particular, these countries include Poland, Bulgaria, the Czech Republic, Slovakia, Hungary, Albania, former East Germany and Cuba. However, younger generations are usually not fluent in it, because Russian is no longer mandatory in the school system. According to the Eurobarometer 2005 survey, though, fluency in Russian remains fairly high (20–40%) in some countries, in particular those where the people speak a Slavic language and thereby have an edge in learning Russian (namely, Poland, Czech Republic, Slovakia, and Bulgaria). In 2005, it was the most widely taught foreign language in Mongolia, and was compulsory in Year 7 onward as a second foreign language in 2006. Russian is also spoken in Israel by at least 750,000 ethnic Jewish immigrants from the former Soviet Union, according to the 1999 census. The Israeli press and websites regularly publish material in Russian. Russian is also spoken as a second language by a small number of people in Afghanistan (Awde and Sarwan, 2003).
The language was first introduced in North America when Russian explorers voyaged into Alaska and claimed it for Russia during the 1700s. Although most colonists left after the United States bought the land in 1867, a handful stayed and preserved the Russian language in this region to this day, although only a few elderly speakers of this unique dialect are left. Sizable Russian-speaking communities also exist in North America, especially in large urban centers of the U.S. and Canada, such as New York City, Philadelphia, Boston, Los Angeles, Nashville, San Francisco, Seattle, Spokane, Toronto, Baltimore, Miami, Chicago, Denver and Cleveland. In a number of locations they issue their own newspapers, and live in ethnic enclaves (especially the generation of immigrants who started arriving in the early 1960s). Only about 25% of them are ethnic Russians, however. Before the dissolution of the Soviet Union, the overwhelming majority of Russophones in Brighton Beach, Brooklyn in New York City were Russian-speaking Jews. Afterward, the influx from the countries of the former Soviet Union changed the statistics somewhat, with ethnic Russians and Ukrainians immigrating along with some more Russian Jews and Central Asians. According to the United States Census, in 2007 Russian was the primary language spoken in the homes of over 850,000 individuals living in the United States.
Significant Russian-speaking groups also exist in Western Europe. These have been fed by several waves of immigrants since the beginning of the 20th century, each with its own flavor of language. The United Kingdom, Spain, Portugal, France, Italy, Belgium, Greece, Brazil, Norway, and Austria have significant Russian-speaking communities. Germany has the highest Russian-speaking population outside the former Soviet Union with approximately 3 million people. They are split into three groups, from largest to smallest: Russian-speaking ethnic Germans ("Aussiedler"), ethnic Russians, and Jews. Australian cities Melbourne and Sydney also have Russian-speaking populations, with the most Russians living in southeast Melbourne, particularly the suburbs of Carnegie and Caulfield. Two-thirds of them are actually Russian-speaking descendants of Germans, Greeks, Jews, Azerbaijanis, Armenians or Ukrainians, who either repatriated after the USSR collapsed, or are just looking for temporary employment.
According to the 2011 Census of Ireland, there were 21,639 people in the nation who use Russian as a home language. However, of this only 13% were Russian nationals. 20% held Irish citizenship, while 27% and 14% were holding the passports of Latvia and Lithuania respectively. Some are Russian speakers from Latvia and Lithuania who were unable to obtain Latvian or Lithuanian citizenship. There were 20,984 Russian speakers in Cyprus according to the Census of 2011, accounting for 2.5% of the population. The Russian language in the world is reduced due to the decrease in the number of Russians in the world and diminution of the total population in Russia (where Russian is an official language). The collapse of the Soviet Union and reduction in influence of Russia also has reduced the popularity of the Russian language in the rest of the world.
Russians in China form one of the 56 ethnic groups officially recognized by mainland China.
According to figures published in 2006 in the journal "" research deputy director of Research Center for Sociological Research of the Ministry of Education and Science (Russia) Arefyev A. L., the Russian language is gradually losing its position in the world in general, and in Russia in particular. In 2012, A. L. Arefyev published a new study "Russian language at the turn of the 20th-21st centuries", in which he confirmed his conclusion about the trend of further weakening of the Russian language in all regions of the world (findings published in 2013 in the journal ""). In the countries of the former Soviet Union the Russian language is gradually being replaced by local languages. Currently the number speakers of Russian language in the world depends on the number of Russians in the world (as the main sources distribution Russian language) and total population Russia (where Russian is an official language).
Official status.
Russian is the official language of Russia, although it shares the official status at regional level with other languages in the numerous ethnic autonomies within Russia, such as Chuvashia, Bashkortostan, Tatarstan, and Yakutia. It is also a co-official language of Belarus, Kazakhstan, Kyrgyzstan, and a co-official language of the unrecognized country of Transnistria and partially recognized countries of South Ossetia and Abkhazia. In Ukraine, Russian lacks the status of a state language, but still enjoys an extensive protection as a regional and minority language with some official functions. The Constitution of Ukraine guarantees "free development, use and protection" of the Russian language. Russian is one of the six official languages of the United Nations. Education in Russian is still a popular choice for both Russian as a second language (RSL) and native speakers in Russia as well as many of the former Soviet republics. Russian is still seen as an important language for children to learn in most of the former Soviet republics.
While 94% of school students in Russia receive their education primarily in Russian, this number is lower in other former Soviet countries: 75% in Belarus, 41% in Kazakhstan and Uzbekistan, 20% in Ukraine, 23% in Kyrgyzstan, 21% in Moldova, 7% in Azerbaijan, 5% in Georgia, and 2% in Armenia and Tajikistan. The percentage of ethnic Russians is 81% in Russia (2010 census) 10% in Belarus, 36% in Kazakhstan, 17% in Ukraine, 9% in Kyrgyzstan, 6% in Moldova, 2% in Azerbaijan, 1.5% in Georgia and less than 1% in both Armenia and Tajikistan.
Russian-language schooling is also available in Latvia, Estonia and Lithuania. However, due to recent high school reforms in Latvia (whereby the government pays a substantial sum to a school to teach in the national language), the number of subjects taught in Russian has been reduced in the country. The language has a co-official status alongside Romanian in the autonomies of Gagauzia and Transnistria in Moldova. According to a poll by FOM-Ukraine, Russian is the most widely spoken language in Ukraine understood by everyone.
The Russian language is also one of two official languages aboard the International Space Station - NASA astronauts who serve alongside Russian cosmonauts usually take Russian language courses. This practice goes back to the Apollo-Soyuz mission, which first flew in 1975.
Russian as an international language.
Russian is one of the official languages (or has similar status and interpretation must be provided into Russian) of the United Nations, International Atomic Energy Agency, World Health Organization, International Civil Aviation Organization, UNESCO, World Intellectual Property Organization, International Telecommunication Union, World Meteorological Organization, Food and Agriculture Organization, International Fund for Agricultural Development, International Criminal Court, International Monetary Fund, International Olympic Committee, Universal Postal Union, World Bank, Commonwealth of Independent States, Organization for Security and Co-operation in Europe, Shanghai Cooperation Organisation, Eurasian Economic Community, Collective Security Treaty Organization, Antarctic Treaty Secretariat, International Organization for Standardization, GUAM Organization for Democracy and Economic Development, International Mathematical Olympiad.
In March 2013 it was announced that Russian is now the second-most used language on the Internet after English. People use the Russian language on 5.9% of all websites, slightly ahead of German and far behind English (54.7%). Russian is used not only on 89.8% of .ru sites, but also on 88.7% of sites with the former Soviet Union domain .su. The websites of former Soviet Union nations also use high levels of Russian: 79.0% in Ukraine, 86.9% in Belarus, 84.0% in Kazakhstan, 79.6% in Uzbekistan, 75.9% in Kyrgyzstan and 81.8% in Tajikistan. However, Russian is the sixth-most used language on the top 1,000 sites, behind English, Chinese, French, German and Japanese.
Dialects.
Russian is a rather homogeneous language, in terms of dialectal variation, due to the early political centralization under the Moscow rule, compulsory education, mass migration from rural to urban areas in the 20th century, as well as other factors. The standard language is used in written and spoken form almost everywhere in the country, from Kaliningrad and Saint Petersburg in the West to Vladivostok and Petropavlovsk-Kamchatsky in the East, notwithstanding the enormous distance in between.
Despite leveling after 1900, especially in matters of vocabulary and phonetics, a number of dialects still exist in Russia. Some linguists divide the dialects of Russian into two primary regional groupings, "Northern" and "Southern", with Moscow lying on the zone of transition between the two. Others divide the language into three groupings, Northern, Central (or Middle) and Southern, with Moscow lying in the Central region. All dialects also divided in two main chronological categories: the dialects of "primary formation" (the territory of the Eastern Rus' or Muscovy, roughly consists of the modern Central and Northwestern Federal districts); and "secondary formation" (other territory). Dialectology within Russia recognizes dozens of smaller-scale variants. The dialects often show distinct and non-standard features of pronunciation and intonation, vocabulary and grammar. Some of these are relics of ancient usage now completely discarded by the standard language.
The Northern Russian dialects and those spoken along the Volga River typically pronounce unstressed /o/ clearly (the phenomenon called okanye/оканье). Besides the absence of vowel reduction, some dialects have high or diphthongal /e~i̯ɛ/ in the place of Proto-Slavic *"ě" and /o~u̯ɔ/ in stressed closed syllables (as in Ukrainian) instead of Standard Russian /e/ and /o/. An interesting morphological feature is a post-posed definite article "-to, -ta, -te" similarly to that existing in Bulgarian and Macedonian.
In the Southern Russian dialects, instances of unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable are not reduced to [ɪ] (as occurs in the Moscow dialect), being instead pronounced [a] in such positions (e.g. несли is pronounced [nʲaˈslʲi], not [nʲɪsˈlʲi]) – this is called yakanye/яканье.
Consonants include a fricative /ɣ/, a semivowel /w~u̯/ and /x~xv~xw/, whereas the Standard and Northern dialects have the consonants /ɡ/, /v/, and final /l/ and /f/, respectively.
The morphology features a palatalized final /tʲ/ in 3rd person forms of verbs (this is unpalatalized in the Standard and Northern dialects). Some of these features such as akanye/yakanye, a debuccalized or lenited /ɡ/, a semivowel /w~u̯/ and palatalized final /tʲ/ in 3rd person forms of verbs are also present in modern Belarusian and some dialects of Ukrainian (Eastern Polesian), indicating a linguistic continuum.
The city of Veliky Novgorod has historically displayed a feature called chokanye/tsokanye (чоканье/цоканье), where /tɕ/ and /ts/ were switched or merged. So, цапля ('heron') has been recorded as 'чапля'. Also, the second palatalization of velars did not occur there, so the so-called ě² (from the Proto-Slavic diphthong *ai) did not cause /k, ɡ, x/ to shift to /ts, dz, s/; therefore, where Standard Russian has цепь ('chain'), the form кепь [kʲepʲ] is attested in earlier texts.
Among the first to study Russian dialects was Lomonosov in the 18th century. In the 19th, Vladimir Dal compiled the first dictionary that included dialectal vocabulary. Detailed mapping of Russian dialects began at the turn of the 20th century. In modern times, the monumental "Dialectological Atlas of the Russian Language" ("Диалектологический атлас русского языка" ]), was published in three folio volumes 1986–1989, after four decades of preparatory work.
Alphabet.
Russian is written using a Cyrillic alphabet. The Russian alphabet consists of 33 letters. The following table gives their upper case forms, along with values for each letter's typical sound:
Older letters of the Russian alphabet include ⟨ѣ⟩, which merged to ⟨е⟩ (/je/ or /ʲe/); ⟨і⟩ and ⟨ѵ⟩, which both merged to ⟨и⟩ (/i/); ⟨ѳ⟩, which merged to ⟨ф⟩ (/f/); ⟨ѫ⟩, which merged to ⟨у⟩ (/u/); ⟨ѭ⟩, which merged to ⟨ю⟩ (/ju/ or /ʲu/); and ⟨ѧ/⟨ѩ⟩⟩, which later were graphically reshaped into ⟨я⟩ and merged phonetically to /ja/ or /ʲa/. While these older letters have been abandoned at one time or another, they may be used in this and related articles. The yers ⟨ъ⟩ and ⟨ь⟩ originally indicated the pronunciation of "ultra-short" or "reduced" /ŭ/, /ĭ/.
Transliteration.
Because of many technical restrictions in computing and also because of the unavailability of Cyrillic keyboards abroad, Russian is often transliterated using the Latin alphabet. For example, мороз ('frost') is transliterated "moroz", and мышь ('mouse'), "mysh" or "myš"'. Once commonly used by the majority of those living outside Russia, transliteration is being used less frequently by Russian-speaking typists in favor of the extension of Unicode character encoding, which fully incorporates the Russian alphabet. Free programs leveraging this Unicode extension are available which allow users to type Russian characters, even on Western 'QWERTY' keyboards.
Computing.
The Russian alphabet has many systems of character encoding. KOI8-R was designed by the Soviet government and was intended to serve as the standard encoding. This encoding was and still is widely used in UNIX-like operating systems. Nevertheless, the spread of MS-DOS and OS/2 (IBM866), traditional Macintosh (ISO/IEC 8859-5) and Microsoft Windows (CP1251) created chaos and ended by establishing different encodings as de facto standards, with Windows-1251 becoming a de facto standard in Russian Internet and e-mail communication during the period of roughly 1995–2005.
All the obsolete 8-bit encodings are rarely used in the communication protocols and text-exchange data formats, being mostly replaced with UTF-8. A number of encoding conversion applications were developed. "iconv" is an example that is supported by most versions of Linux, Macintosh and some other operating systems; but converters are rarely needed unless accessing texts created more than a few years ago.
In addition to the modern Russian alphabet, Unicode (and thus UTF-8) encodes the Early Cyrillic alphabet (which is very similar to the Greek alphabet), as well as all other Slavic and non-Slavic but Cyrillic-based alphabets.
Orthography.
Russian spelling is reasonably phonemic in practice. It is in fact a balance among phonemics, morphology, etymology, and grammar; and, like that of most living languages, has its share of inconsistencies and controversial points. A number of rigid spelling rules introduced between the 1880s and 1910s have been responsible for the former whilst trying to eliminate the latter.
The current spelling follows the major reform of 1918, and the final codification of 1956. An update proposed in the late 1990s has met a hostile reception, and has not been formally adopted. The punctuation, originally based on Byzantine Greek, was in the 17th and 18th centuries reformulated on the French and German models.
According to the Institute of Russian Language of the Russian Academy of Sciences, an optional acute accent (знак ударения) may, and sometimes should, be used to mark stress. For example, it is used to distinguish between otherwise identical words, especially when context does not make it obvious: замо́к/за́мок (lock/castle), сто́ящий/стоя́щий (worthwhile/standing), чудно́/чу́дно (this is odd/this is marvelous), молоде́ц/мо́лодец (attaboy/fine young man), узна́ю/узнаю́ (I shall learn it/I recognize it), отреза́ть/отре́зать (to be cutting/to have cut); to indicate the proper pronunciation of uncommon words, especially personal and family names (афе́ра, гу́ру, Гарси́я, Оле́ша, Фе́рми), and to show which is the stressed word in a sentence (Ты́ съел печенье?/Ты съе́л печенье?/Ты съел пече́нье? – Was it "you" who ate the cookie?/Did you "eat" the cookie?/Was it the "cookie" that you ate?). Stress marks are mandatory in lexical dictionaries and books for children or Russian learners.
Phonology.
The phonological system of Russian is inherited from Common Slavonic; it underwent considerable modification in the early historical period before being largely settled around the year 1400.
The language possesses five vowels (or six, under the St. Petersburg Phonological School), which are written with different letters depending on whether or not the preceding consonant is palatalized. The consonants typically come in plain vs. palatalized pairs, which are traditionally called "hard" and "soft." (The "hard" consonants are often velarized, especially before front vowels, as in Irish). The standard language, based on the Moscow dialect, possesses heavy stress and moderate variation in pitch. Stressed vowels are somewhat lengthened, while unstressed vowels tend to be reduced to near-close vowels or an unclear schwa. (See also: vowel reduction in Russian.)
The Russian syllable structure can be quite complex with both initial and final consonant clusters of up to 4 consecutive sounds. Using a formula with V standing for the nucleus (vowel) and C for each consonant the structure can be described as follows:
Clusters of four consonants are not very common, however, especially within a morpheme. Examples: взгляд (/vzɡlʲat/, "glance"), государство ([gəsʊˈdarstvə], 'state'), строительство ([strɐˈitʲɪlʲstvə], 'construction').
Consonants.
Russian is notable for its distinction based on palatalization of most of the consonants. While /k/, /ɡ/, /x/ do have palatalized allophones [kʲ, ɡʲ, xʲ], only /kʲ/ might be considered a phoneme, though it is marginal and generally not considered distinctive (the only native minimal pair which argues for /kʲ/ to be a separate phoneme is "это ткёт" ([ˈɛtə tkʲɵt], 'it weaves')/"этот кот" ([ˈɛtət kot], 'this cat')). Palatalization means that the center of the tongue is raised during and after the articulation of the consonant. In the case of /tʲ/ and /dʲ/, the tongue is raised enough to produce slight frication (affricate sounds). These sounds: /t, d, ts, s, z, n and rʲ/ are dental, that is pronounced with the tip of the tongue against the teeth rather than against the alveolar ridge.
Grammar.
Russian has preserved an Indo-European synthetic-inflectional structure, although considerable levelling has taken place.
Russian grammar encompasses:
The spoken language has been influenced by the literary one but continues to preserve characteristic forms. The dialects show various non-standard grammatical features, some of which are archaisms or descendants of old forms since discarded by the literary language.
Vocabulary.
Number of words in Russian.
See History of the Russian language for an account of the successive foreign influences on Russian.
The total number of words in Russian is difficult to ascertain because of the ability to agglutinate and create manifold compounds, diminutives, etc. (see Word Formation under Russian grammar). The number of listed words or entries in some of the major dictionaries published during the past two centuries, and the total vocabulary of Alexander Pushkin (who is credited with greatly augmenting and codifying literary Russian), are as follows:
Note: The above numbers do not properly show the real quantity of words in Russian, as Russian dictionaries do not have a goal to collect all words of the language, but to establish normalized vocabulary of standard neutral style. They do not contain special technical and scientific terms, many lexical derivatives, colloquial and dialectical words, and slang.
Proverbs and sayings.
The Russian language is replete with many hundreds of proverbs (пословица [pɐˈslovʲɪtsə]) and sayings (поговоркa [pəɡɐˈvorkə]). These were already tabulated by the 17th century and collected and studied in the 19th and 20th centuries, with folk tales being especially fertile sources.
History and examples.
The history of Russian language may be divided into the following periods.
Judging by the historical records, by approximately 1000 AD the predominant ethnic group over much of modern European Russia, Ukraine and Belarus was the Eastern branch of the Slavs, speaking a closely related group of dialects. The political unification of this region into Kievan Rus' in about 880, from which modern Russia, Ukraine and Belarus trace their origins, established Old East Slavic as a literary and commercial language. It was soon followed by the adoption of Christianity in 988 and the introduction of the South Slavic Old Church Slavonic as the liturgical and official language. Borrowings and calques from Byzantine Greek began to enter the Old East Slavic and spoken dialects at this time, which in their turn modified the Old Church Slavonic as well.
Dialectal differentiation accelerated after the breakup of Kievan Rus' in approximately 1100. On the territories of modern Belarus and Ukraine emerged Ruthenian and in modern Russia medieval Russian. They became distinct since the 13th century, i.e. following the division of that land between the Grand Duchy of Lithuania, Poland and Hungary in the west and independent Novgorod and Pskov feudal republics plus numerous small duchies (which came to be vassals of the Tatars) in the east.
The official language in Moscow and Novgorod, and later, in the growing Muscovy, was Church Slavonic, which evolved from Old Church Slavonic and remained the literary language for centuries, until the Petrine age, when its usage became limited to biblical and liturgical texts. Russian developed under a strong influence of Church Slavonic until the close of the 17th century; afterward the influence reversed, leading to corruption of liturgical texts.
The political reforms of Peter the Great (Пётр Вели́кий, "Pyótr Velíkiy") were accompanied by a reform of the alphabet, and achieved their goal of secularization and Westernization. Blocks of specialized vocabulary were adopted from the languages of Western Europe. By 1800, a significant portion of the gentry spoke French daily, and German sometimes. Many Russian novels of the 19th century, e.g. Leo Tolstoy's (Лев Толсто́й) "War and Peace", contain entire paragraphs and even pages in French with no translation given, with an assumption that educated readers would not need one.
The modern literary language is usually considered to date from the time of Alexander Pushkin (Алекса́ндр Пу́шкин) in the first third of the 19th century. Pushkin revolutionized Russian literature by rejecting archaic grammar and vocabulary (so-called "высо́кий стиль" — "high style") in favor of grammar and vocabulary found in the spoken language of the time. Even modern readers of younger age may only experience slight difficulties understanding some words in Pushkin's texts, since relatively few words used by Pushkin have become archaic or changed meaning. In fact, many expressions used by Russian writers of the early 19th century, in particular Pushkin, Mikhail Lermontov (Михаи́л Ле́рмонтов), Nikolai Gogol (Никола́й Го́голь), Aleksander Griboyedov (Алекса́ндр Грибое́дов), became proverbs or sayings which can be frequently found even in modern Russian colloquial speech.
Зи́мний ве́чер ]
Бу́ря мгло́ю не́бо кро́ет, [ˈburʲə ˈmɡloju ˈnʲɛbə ˈkroɪt]
Ви́хри сне́жные крутя́; [ˈvʲixrʲɪ ˈsʲnʲɛʐnɨɪ krʊˈtʲa]
То, как зверь, она́ заво́ет, [ˈto kaɡ zvʲerʲ ɐˈna zɐˈvoɪt]
То запла́чет, как дитя́, [ˈto zɐˈplatɕɪt, kaɡ dʲɪˈtʲa]
То по кро́вле обветша́лой [ˈto pɐˈkrovlʲɪ ɐbvʲɪˈtʂaləj]
Вдруг соло́мой зашуми́т, [ˈvdruk sɐˈloməj zəʂʊˈmʲit]
То, как пу́тник запозда́лый, [ˈto ˈkak ˈputʲnʲɪɡ zəpɐˈzdɑlɨj]
К нам в око́шко застучи́т. [ˈknam vɐˈkoʂkə zəstʊˈtɕit]
The political upheavals of the early 20th century and the wholesale changes of political ideology gave written Russian its modern appearance after the spelling reform of 1918. Political circumstances and Soviet accomplishments in military, scientific and technological matters (especially cosmonautics), gave Russian a worldwide prestige, especially during the mid-20th century.

</doc>
<doc id="25432" url="http://en.wikipedia.org/wiki?curid=25432" title="Rush (band)">
Rush (band)

Rush is a Canadian rock band formed in August 1968 in the Willowdale neighbourhood of Toronto, Ontario. The band is composed of bassist, keyboardist, and lead vocalist Geddy Lee; guitarist and backing vocalist Alex Lifeson; and drummer, percussionist, and lyricist Neil Peart. The band and its membership went through several reconfigurations between 1968 and 1974, achieving its current form when Peart replaced original drummer John Rutsey in July 1974, two weeks before the group's first United States tour.
Since the release of the band's self-titled debut album in March 1974, Rush has become known for its musicianship, complex compositions, and eclectic lyrical motifs drawing heavily on science fiction, fantasy, history, and philosophy. Rush's music style has changed over the years, from a blues-inspired hard rock beginning, later moving into progressive rock, and including a period with heavy use of synthesizers. Its musical style returned to a more guitar-oriented sound in 1989. The band's latest studio album, "Clockwork Angels" (2012) won the Album Of The Year Award from Progressive Music Awards. The supporting tour ran from September 2012 to August 2013.
According to the RIAA Rush ranks 80th with sales of 25 million units in the United States. Although total worldwide album sales are not calculated by any single entity, several industry sources estimated Rush's total worldwide album sales at over 40 million units as of 2004. The group has been awarded 24 gold, 14 platinum, and 3 multi-platinum albums.
Rush has received seven Grammy award nominations, although they have never won a Grammy. The band, however, has won several Juno Awards, was inducted into the Canadian Music Hall of Fame in 1994, and inducted into the Rock and Roll Hall of Fame in 2013. Over their careers, the members of Rush have been acknowledged as some of the most proficient players on their respective instruments, with each band member winning numerous awards in magazine readers' polls. Rush plans to stop large-scale touring at the end of 2015.
History.
Early years (1968–76).
The original line-up was formed in the neighbourhood of Willowdale in Toronto, Ontario, by Lifeson, bassist and front man Jeff Jones, and drummer John Rutsey. Within a couple of weeks of forming, and before their second performance, bassist and lead vocalist Jones left the band and was replaced by Geddy Lee, a schoolmate of Lifeson. After several line-up reformations, Rush's official incarnation was formed in May 1971 consisting of Lee, Lifeson, and Rutsey. The band was managed by local Toronto resident Ray Danniels, a frequent attendee of Rush's early shows.
After gaining stability in the line-up and honing their skills on the local bar/high school dance circuit, the band members came to release their first single "Not Fade Away", a cover of the Buddy Holly song, in 1973. Side B contained an original composition, "You Can't Fight It", credited to Lee and Rutsey. The single generated little reaction (#99 RPM Magazine charts) and, because of record company indifference, the band formed their own independent record label, "Moon Records". With the aid of Danniels and the newly enlisted engineer Terry Brown, the band released its self-titled debut album in 1974, which was considered highly derivative of Led Zeppelin. "Rush" had limited local popularity until the album was picked up by WMMS, a radio station in Cleveland, Ohio. Donna Halper, a music director and DJ working at the station, selected "Working Man" for her regular play list. The song's blue collar theme resonated with hard rock fans, and this newfound popularity led to the album being re-released by Mercury Records in the U.S.
Immediately after the release of the debut album in 1974, Rutsey was forced to leave the band due to health difficulties (stemming from diabetes) and his general distaste for touring. His last performance with the band was on July 25, 1974 at Centennial Hall in London, Ontario. Rush held auditions for a new drummer and eventually selected Neil Peart as Rutsey's replacement. Peart officially joined the band on July 29, 1974, two weeks before the group's first U.S. tour. They performed their first concert together, opening for Uriah Heep and Manfred Mann with an attendance of over 11,000 people at the Civic Arena in Pittsburgh, Pennsylvania on August 14. In addition to becoming the band's drummer, Peart assumed the role of principal lyricist from Lee, who had very little interest in writing, despite having penned the lyrics of the band's first album. Instead, Lee, along with Lifeson, focused primarily on the instrumental aspects of Rush. "Fly by Night" (1975), Rush's first album after recruiting Peart, saw the inclusion of the band's first epic mini-tale "By-Tor and the Snow Dog", replete with complex arrangements and multi-section format. Lyrical themes also underwent dramatic changes after the addition of Peart because of his love for fantasy and science-fiction literature. Despite these many differences, however, some of the music and songs still closely mirrored the blues style found on Rush's debut.
Following quickly on the heels of "Fly By Night", the band released "Caress of Steel" (1975), a five-track album featuring two extended multi-chapter songs, "The Necromancer" and "The Fountain of Lamneth". Some critics said "Caress of Steel" was unfocused and an audacious move for the band because of the placement of two back-to-back protracted songs, as well as a heavier reliance on atmospherics and story-telling, a large deviation from "Fly by Night". Intended to be the band's first "break-through" album, "Caress of Steel" sold below expectations and the promotional tour consisted of smaller venues, which led to the moniker the "Down the Tubes Tour". In light of these events, Rush's record label attempted to pressure the members into moulding their next album in a more commercially friendly and accessible fashion; however, the band ignored the requests and developed their next album, "2112" with a 20-minute title track divided into seven sections. Despite this, the album was the band's first taste of commercial success and their first platinum album in Canada. The supporting tour for the album culminated in a three-night stand at Massey Hall in Toronto, which the band recorded for the release of their first live album titled "All the World's a Stage". AllMusic critic Greg Prato notes that the album demarcates the boundary between the band's early years and the next era of their music.
Mainstream success (1977–81).
After "2112", Rush retreated to the United Kingdom to record "A Farewell to Kings" (1977) and "Hemispheres" (1978) at Rockfield Studios in Wales. These albums saw the band members expanding the use of progressive elements in their music. "As our tastes got more obscure," Geddy Lee said in a recent interview, "we discovered more progressive rock-based bands like Yes, Van der Graaf Generator and King Crimson, and we were very inspired by those bands. They made us want to make our music more interesting and more complex and we tried to blend that with our own personalities to see what we could come up with that was indisputably us." Trademarks such as increased synthesizer usage, lengthy songs reminiscent of miniature concept albums, and highly dynamic playing featuring complex time signature changes became a staple of Rush's compositions. To achieve a broader, more progressive palette of sound, Alex Lifeson began to experiment with classical and twelve-string guitars, and Geddy Lee added bass-pedal synthesizers and Minimoog. Likewise, Peart's percussion became diversified in the form of triangles, glockenspiel, wood blocks, cowbells, timpani, gong, and chimes. Beyond instrument additions, the band kept in stride with the progressive rock movement by continuing to compose long, conceptual songs with science fiction and fantasy overtones. As the new decade approached, however, Rush gradually began to dispose of its older styles of music in favour of shorter, and sometimes softer, arrangements. The lyrics up to this point (most of them written by Peart) were heavily influenced by classical poetry, fantasy literature, science fiction, and the writings of novelist Ayn Rand, as exhibited most prominently by their 1975 song "Anthem" from "Fly By Night" and a specifically acknowledged derivation in "2112" (1976).
"Permanent Waves" (1980) dramatically shifted Rush's style of music via the introduction of reggae and new wave elements. Although a hard rock style was still evident, more and more synthesizers were introduced. Moreover, because of the limited airplay Rush's previous extended-length songs received, "Permanent Waves" included shorter, more radio-friendly songs such as "The Spirit of Radio" and "Freewill", two songs that helped "Permanent Waves" become Rush's first U.S. Top 5 album; both songs continue to make appearances on classic rock radio stations in Canada and the United States to this day. Meanwhile, Peart's lyrics shifted toward an expository tone with subject matter that dwelled less on fantastical or allegorical story-telling and more heavily on topics that explored humanistic, social and emotional elements. Rush joined with fellow Toronto-based rock band Max Webster on July 28, 1980 to record "Battle Scar" for their 1980 release, "Universal Juveniles". While on tour together following the release, both bands would join between sets to play "Battle Scar". The song acted as both a transition from Max Webster to Rush, as well as a warm-up for Peart. In addition, Max Webster lyricist Pye Dubois offered the band lyrics to a song he had written. The band accepted; the song went on, after reworking by Peart, to become "Tom Sawyer".
Rush's popularity reached its pinnacle with the release of "Moving Pictures" in 1981. "Moving Pictures" essentially continued where "Permanent Waves" left off, extending the trend of highly accessible and commercially friendly progressive rock that helped thrust them into the spotlight. The lead track, "Tom Sawyer", is probably the band's best-known song with "Limelight" also receiving satisfactory responses from listeners and radio stations. "Moving Pictures" was Rush's last album to feature an extended song, the eleven-minute "The Camera Eye". The song also contained the band's heaviest usage of synthesizers up to that point, hinting that Rush's music was shifting direction once more. "Moving Pictures" reached No.3 on the "Billboard" 200 album chart and has been certified quadruple platinum by the Recording Industry Association of America.
Following the success of "Moving Pictures" and the completion of another four studio albums, Rush released a second live recording, "Exit...Stage Left", in 1981.
Synthesizer period (1982–89).
The band underwent another radical stylistic transmutation with the recording of "Signals" in 1982. While Lee's synthesizers had been featured instruments ever since the late '70s, keyboards were suddenly shifted from the contrapuntal background to the melodic front-lines in songs like "Countdown" and the lead-off track "Subdivisions". Both feature prominent lead synthesizer lines with minimalistic guitar chords and solos. Other previously unused instrument additions were seen in the song "Losing It," featuring collaborator Ben Mink on electric violin.
"Signals" also represented a drastic stylistic transformation apart from instrumental changes. The album contained Rush's only U.S. top-40 pop hit, "New World Man", while other more experimental songs such as "Digital Man", "The Weapon", and "Chemistry" expanded the band's use of ska, reggae, and funk. Although the band members consciously decided to move in this overall direction, creative differences between the band and long-time producer Terry Brown began to emerge. The band felt dissatisfied with Brown's studio treatment of "Signals", while Brown was becoming more uncomfortable with the increased use of synthesizers in the music. Ultimately, Rush and Brown parted ways in 1983, and the experimentation with new electronic instruments and varying musical styles would come into further play on their next studio album.
The style and production of "Signals" were augmented and taken to new heights on "Grace Under Pressure" (1984). It was Peart who named the album, as he borrowed the words of Ernest Hemingway to describe what the band had to go through after making the decision to leave Terry Brown. Producer Steve Lillywhite, who gained fame with successful productions of Simple Minds and U2, was enlisted to produce "Grace Under Pressure". He backed out at the last moment, however, much to the ire of Lee, Lifeson and Peart. Lee said "Steve Lillywhite is really not a man of his word...after agreeing to do our record, he got an offer from Simple Minds, changed his mind, blew us off... so it put us in a horrible position." Rush eventually hired Peter Henderson to co-produce and engineer the album instead.
Musically, although Lee's use of sequencers and synthesizers remained the band's cornerstone, his focus on new technology was complemented by Peart's adaptation of Simmons electronic drums and percussion. Lifeson's contributions on the album were decidedly enhanced to act as an overreaction to the minimalistic role he played on "Signals". Still, many of his trademark guitar textures remained intact in the form of open reggae chords and funk and new-wave rhythms.
With new producer Peter Collins, the band released "Power Windows" (1985) and "Hold Your Fire" (1987). The music on these two albums gives far more emphasis and prominence to Lee's multi-layered synthesizer work. While fans and critics took notice of Lifeson's diminished guitar work, his presence was still palpable. Lifeson, like many guitarists in the late 1980s, experimented with processors that reduced his instrument to echoey chord bursts and razor-thin leads. "Hold Your Fire" represents both a modest extension of the guitar stylings found on "Power Windows", and, according to Allmusic critic Eduardo Rivadavia, the culmination of this era of Rush. Whereas the previous five Rush albums sold platinum or better, "Hold Your Fire" only went gold in November 1987, although it managed to peak at number 13 on the "Billboard" 200.
A third live album and video, "A Show of Hands" (1989), was also released by Anthem and Mercury following the Power Windows and Hold Your Fire tours, demonstrating the aspects of Rush in the '80s. "A Show of Hands" met with strong fan approval, but "Rolling Stone" critic Michael Azerrad dismissed it as "musical muscle" with 1.5 stars, claiming Rush fans viewed their favourite power trio as "the holy trinity". Nevertheless, "A Show of Hands" managed to surpass the gold album mark, reaching number 21 on the "Billboard" 200. At this point, the group decided to change international record labels from Mercury to Atlantic. After Rush's departure in 1989, Mercury released a double platinum two-volume compilation of their Rush catalogue, "Chronicles" (1990).
Return to guitar-oriented sound (1989–97).
Rush started to deviate from its 1980s style with the albums "Presto" and "Roll the Bones". Produced by record engineer and musician Rupert Hine, these two albums saw Rush shedding much of its keyboard-saturated sound. Beginning with "Presto" (1989), the band opted for arrangements that were notably more guitar-centric than the previous two studio albums. Although synthesizers were still used in many songs, the instrument was no longer featured as the centrepiece of Rush's compositions. Continuing this trend, "Roll the Bones" (1991) extended the use of the standard three-instrument approach with even less focus on synthesizers than its predecessor. While musically these albums do not deviate significantly from a general pop-rock sound, Rush incorporated traces of other musical styles. "Roll the Bones", for instance, exhibits funk and hip hop elements, and the instrumental track "Where's My Thing?" features several jazz components. This return to three-piece instrumentation helped pave the way for future albums in the mid-90s, which would adopt a more straightforward rock formula.
The transition from synthesizers to more guitar-oriented and organic instrumentation continued with "Counterparts" (1993) and its follow-up, "Test for Echo" (1996), again both produced in collaboration with Peter Collins. Musically, "Counterparts" and "Test For Echo" are two of Rush's most guitar-driven albums. Musically, "Test For Echo" still retained much of the progressive hard rock/alternative style already charted on the previous record with Lifeson and Lee's playing remaining more or less unchanged; however, a distinct modification in technique became apparent in Peart's playing from his jazz and swing training under the tutelage of jazz instructor Freddie Gruber during the interim between "Counterparts" and "Test For Echo". In October 1996, in support of "Test For Echo", the band embarked on a North American tour, the band's first without an opening act and dubbed "An Evening with Rush". The tour was broken up into two segments spanning October through December 1996 and May through July 1997 with the band taking a respite between tour legs.
Hiatus and comeback (1997–2005).
After wrapping up the tour promoting "Test for Echo" in 1997, the band entered a five-year hiatus primarily due to personal tragedies in Peart's life. Peart's daughter Selena died in an car accident in August 1997, followed by the death of his wife Jacqueline from cancer in June 1998. Peart took a hiatus to mourn and reflect, during which he travelled extensively throughout North America on his BMW motorcycle, covering 88,000 km. At some point in his journey, Peart decided to return to the band. Peart wrote "" as a chronicle of his geographical and emotional journey. In this book he writes of how he had told his bandmates at Selena's funeral, "consider me retired." On November 10, 1998 a triple CD live album entitled "Different Stages" was released, dedicated to the memory of Selena and Jacqueline. Mixed by producer Paul Northfield and engineered by Terry Brown, it contained three discs packed with recorded performances from the band's "Counterparts", "Test For Echo", and "A Farewell to Kings" tours, marking the fourth officially released live album by the band.
After a time of grief and recovery, and while visiting long-time Rush photographer Andrew MacNaughtan in Los Angeles, Peart was introduced to his future wife, photographer Carrie Nuttall. Peart married Nuttall on September 9, 2000. In early 2001 he announced to his band mates that he was ready to once again enter the studio and get back into the business of making music. With the help of producer Paul Northfield the band returned in May 2002 with "Vapor Trails", written and recorded in Toronto. To herald the band's comeback, the single and lead track from the album, "One Little Victory" was designed to grab the attention of listeners with its rapid guitar and drum tempos. "Vapor Trails" marked the first studio recording not to include a single synthesizer, organ or keyboard part since the early 1970s. While the album is almost completely guitar-driven, it is mostly devoid of any conventional sounding guitar solos, a conscious decision made by Lifeson during the writing process. According to the band, the entire developmental process for "Vapor Trails" was extremely taxing and took approximately 14 months to finish, by far the longest the band had ever spent writing and recording a studio album. The album was supported by the band's first tour in six years, including first-ever concerts in Mexico City and Brazil, where they played to some of the largest crowds of their career.
A triple CD live album and dual DVD, "Rush in Rio", was released in late October 2003 featuring an entire concert performance recorded on the last night of their Vapor Trails Tour, November 23, 2002, at Maracanã Stadium in Rio de Janeiro, Brazil. To celebrate their 30th anniversary, June 2004 saw the release of "Feedback", a studio EP recorded in suburban Toronto featuring eight covers of such artists as Cream, The Who and The Yardbirds, bands that the members of Rush cite as inspiration around the time of their inception. To help support "Feedback" and continue celebrating their 30 year anniversary as a band, Rush hit the road again for their in the summer of 2004 playing dates in the United States, Canada, the United Kingdom, Germany, Italy, Sweden, the Czech Republic, and the Netherlands. On September 24, 2004 a Frankfurt, Germany concert was recorded at The Festhalle for DVD (titled "R30: Live in Frankfurt"), which was released November 22, 2005; a complete version of the R30 Frankfurt set (the original DVD release omitted eight songs) was released on Blu-ray on December 8, 2009.
Snakes & Arrows (2006–09).
During promotional interviews for the "R30 Live In Frankfurt" DVD, the band members revealed their intention to begin writing new material in early 2006. While in Toronto, Lifeson and Lee began the songwriting process in January 2006. During this time, Peart simultaneously assumed his role of lyric writing while residing in Southern California. The following September, Rush chose to hire American producer Nick Raskulinecz to co-produce the album. The band officially entered Allaire Studios, in Shokan, New York in November 2006 in order to record the bulk of the material. Taking the band five weeks, the sessions ended in December. On February 14, 2007, an announcement was made on the official Rush web site that the title of the new album would be "Snakes & Arrows". The first single, entitled "Far Cry", was released to North American radio stations on March 12, 2007 and reached No.2 on the Mediabase Mainstream and Radio and Records Charts.
The Rush website, newly redesigned on March 12 to support the new album, also announced that the band would embark on a tour to begin in the summer. "Snakes & Arrows" was released May 1, 2007 in North America, where it debuted at No.3 in the "Billboard" 200 with approximately 93,000 units sold in its first week. It would go on to sell an estimated 611,000 copies worldwide. To coincide with the Atlantic ocean hurricane season, "Spindrift" was released as the official second radio single on June 1, 2007, whereas "The Larger Bowl (A Pantoum)" saw single status on June 25, 2007. "The Larger Bowl" positioned within the top 20 of the Mainstream Rock and Media Base Mainstream charts, however, "Spindrift" failed to appear on any commercial chart. The planned intercontinental tour in support of "Snakes & Arrows" began on June 13, 2007 in Atlanta, Georgia, coming to a close on October 29, 2007 at Hartwall Arena in Helsinki, Finland.
The 2008 portion of the tour started on April 11, 2008 in San Juan, Puerto Rico at José Miguel Agrelot Coliseum and culminated on July 24, 2008 in Noblesville, Indiana at the Verizon Wireless Music Center. On April 15, the band released "Snakes & Arrows Live", a double live album documenting the first leg of the tour. Those same performances featured on "Snakes & Arrows Live" filmed at the Ahoy arena in Rotterdam, Netherlands on October 16 and 17, 2007 were released November 24 as a DVD and Blu-ray set. The video also includes footage from the 2008 portion of the tour, recorded at Verizon Wireless Amphitheater in Atlanta.
As the band neared the conclusion of their "Snakes & Arrows" tour, they announced their first appearance on American television in over 30 years. Rush was interviewed by Stephen Colbert, and they performed "Tom Sawyer" on "The Colbert Report" on July 16, 2008. Continuing to ride what one movie reviewer has called a "pop cultural wave," they also appeared at a live show in April 2009 for the comedy film "I Love You, Man".
Time Machine Tour, and Clockwork Angels (2009–2013).
On February 16, 2009, Lifeson remarked that the band may begin working on a new album in the Fall of 2009 with American producer Nick Raskulinecz once again producing. On March 19, 2010, the CBC posted a video interview with Lee and Lifeson where they discussed Rush's induction into the Canadian Songwriters Hall of Fame on March 28, 2010, at the Toronto Centre for the Arts' George Weston Recital Hall. The band was recognized for the songs "Limelight", "Closer to the Heart", "The Spirit of Radio", "Tom Sawyer" and "Subdivisions". In addition to discussing their induction, Lee and Lifeson touched on future material. During the interview, Lee was quoted as saying "... Just about a month and a half ago we had no songs. And now we've been writing and now we've got about 6 songs that we just love..." On March 26, 2010, in an interview with The Globe and Mail, Lifeson reconfirmed that the band had already written a half-dozen songs and that there was the potential for two supporting tours, one planned for Summer 2010 and a more extensive tour planned for Summer 2011. While still uncertain of exactly how and when the new material would be released, at the time he projected a tentative Spring 2011 release date. Soon after, Peart confirmed that Nick Raskulinecz had returned as co-producer.
In April 2010, Rush entered Blackbird Studios in Nashville with Raskulinecz to record "Caravan" and "BU2B", two new songs to be featured on the band's studio album "Clockwork Angels". Mixing was done by record engineer Richard Chycki at the Sound Kitchen in Franklin, Tennessee. "Caravan" was released June 1 to radio stations and made available for digital download at this time along with "BU2B". On April 8, both the official Rush website and PR Newswire announced that the band would embark on the Rush Time Machine Tour, confirming Lifeson's earlier predictions from March. The first leg of the tour began on June 29 in Albuquerque, New Mexico, and finished October 17 in Santiago, Chile, at the National Stadium. It featured the album "Moving Pictures" played in its entirety, as well as "Caravan" and "BU2B". It was suggested that Rush would return to the studio after the completion of the Time Machine Tour with plans to release "Clockwork Angels" in 2011. Nonetheless, Rush announced on November 19, 2010, that they would extend the Time Machine Tour. The second leg began on March 30, 2011, in Fort Lauderdale, and came to an end on July 2, 2011, in George, Washington. On November 8, 2011, the band released "", a concert DVD, Blu-ray and double CD documenting the April 15, 2011, concert at the Quicken Loans Arena in Cleveland, Ohio. Confirming an announcement from Richard Chycki via Twitter on December 20, Rush entered Revolution Recording studios in Toronto, Ontario, following completion of the tour's second leg, to finalize the recording of "Clockwork Angels." The second single, "Headlong Flight," was released April 19, 2012, to radio stations and made available for listening via online streaming. Peart and author Kevin J. Anderson collaborated on a novelization of "Clockwork Angels" that was released in September 2012.
"Clockwork Angels" was released in the United States and Canada on June 12, 2012, and its supporting Clockwork Angels Tour began on September 7, 2012. As of August 31, 2011, Rush switched their American distribution from Atlantic Records over to the Warner Brothers majority-owned metal label, Roadrunner Records. Roadrunner is handling American distribution of "Time Machine 2011: Live in Cleveland" and "Clockwork Angels". Anthem/Universal Music will continue to release their music in Canada. On April 18, 2013, Rush was inducted into the Rock and Roll Hall of Fame.
During Rush's European leg of the "Clockwork Angels Tour", the June 8, 2013 show at the Sweden Rock Festival was the group's first festival appearance in 30 years. The band's performances on November 25, 2012 in Phoenix, Arizona and November 28, 2012 in Dallas, Texas were recorded to make a live CD/DVD/Blu-ray that was released on November 19, 2013.
On November 18, 2013 guitarist Alex Lifeson said that the band has committed to taking a year off, following the completion of the world tour in support of Clockwork Angels. "We've committed to taking about a year off," Lifeson says. "We all agreed when we finished this ('Clockwork Angels') tour (in early August) we were going to take this time off and we weren't going to talk about band stuff or make any plans. We committed to a year, so that's going to take us through to the end of next summer, for sure. That's the minimum. We haven't stopped or quit. Right now we're just relaxing. We're taking it easy and just enjoying our current employment."
In September 2014, the "R40" box set was announced, which will commemorate the fortieth anniversary of the release of the band's self-titled debut album. The set will include five previously released live video albums, as well as various previously unreleased footage from across the band's career.
R40 Tour and tour reduction (2015).
On January 22, 2015, the band officially announced the Rush R40 Tour, celebrating the fortieth anniversary of drummer Neil Peart's membership in the band. The R40 Live Tour is scheduled for performances in more than 30 North American cities, starting May 8 in Tulsa, Oklahoma, and ending with an August 1 arena show at The Forum (Inglewood) in Los Angeles.
On April 29, 2015 Alex Lifeson officially announced that this would be the final large-scale Rush tour due to Peart's chronic tendonitis. However, he did not rule out future projects with the band, including smaller tours and limited performances, and even stated that he would like to work on soundtracks with Geddy Lee.
Musical style and influences.
Rush's musical style has changed substantially over the years. Its debut album was strongly influenced by British blues-based hard rock: an amalgam of sounds and styles from such rock bands as Black Sabbath, the Who, Cream and Led Zeppelin. Rush became increasingly influenced by bands of the British progressive rock movement - especially Genesis (Peter Gabriel era) and Jethro Tull, as Geddy Lee is a fan of both bands. In the tradition of progressive rock, Rush wrote protracted songs with irregular and multiple time signatures combined with fantasy/science fiction-inspired lyrics; however, they did not soften their sound. In the 1980s, Rush successfully merged their hard progressive sound with the trends of this period, experimenting with new wave, reggae and pop rock. This period included the band's most extensive use of instruments such as synthesizers, sequencers, and electronic percussion. With the approach of the early '90s and Rush's characteristic sound still intact, the band transformed their style once again to harmonize with the alternative rock movement.
Reputation.
More than 40 years of activity has provided Rush with the opportunity for musical diversity across their discography. As with many bands known for experimentation, changes have inevitably resulted in dissent among critics and fans. The bulk of the band's music has always included synthetic instruments, and this has been a source of contention among fans and critics, especially the band's heavy reliance on synthesizers and keyboards during the 1980s, particularly on albums "Grace Under Pressure", "Power Windows", and "Hold Your Fire".
The members of Rush have noted that people "either love Rush or hate Rush", resulting in strong detractors and an intensely loyal fan base. A July 2008 "Rolling Stone" article stated that "Rush fans are the Trekkies/trekkers of rock". They have been cited as an influence by various musical artists, including Anthrax, Foo Fighters, Metallica, Primus, Rage Against the Machine, and The Smashing Pumpkins, as well as progressive metal bands such as Prototype, Dream Theater Puya and Symphony X. Trent Reznor considers Rush to be one of his favourite bands in the 2010 documentary "" and has particularly cited the album "Signals" as a major influence on how to incorporate keyboards and synthesizers into hard rock.
Rush was eligible for nomination into the Rock and Roll Hall of Fame beginning in 1998; the band was nominated for entry in 2012 and their induction was announced on December 11, 2012. A reason for their previous exclusion may have been their genre. USA Today writer Edna Gunderson criticized the Hall of Fame for excluding some genres, including progressive rock. Supporters cited the band's accomplishments including longevity, proficiency, and influence, as well as commercial sales figures and RIAA certifications. In the years before induction, Lifeson expressed his indifference toward the perceived slight saying "I couldn't care less, look who's up for induction, it's a joke".
On April 24, 2010, the documentary "", directed by Scot McFadyen and Sam Dunn, premiered at the Tribeca Film Festival. It went on to receive the Tribeca Film Festival Audience Award. The film explores the band's influence on popular music and the reasons why that influence has been underrepresented over the years. This is done via interviews with popular musicians, music industry professionals, and the band members themselves.
On June 25, 2010, Rush received a star on the Hollywood Walk of Fame, located at 6752 Hollywood Boulevard. Critical acclaim continued to mount for Rush in 2010 when, on September 28, Classic Rock Magazine announced Rush would be that year's Living Legends awarded at the Marshall Classic Rock Roll of Honour Awards in the UK. The award was presented November 10, 2010. On September 29, Billboard.com announced that Rush would also receive the 2010 Legends of Live award for significant and lasting contributions to live music and the art of performing live and reaching fans through the concert experience. The award was presented at the Billboard Touring Awards on November 4, 2010.
The three band members were made Officers of the Order of Canada in 1996. In May 2012, the band received the Governor General's Performing Arts Award for Lifetime Artistic Achievement at a ceremony at Rideau Hall followed by a gala at the National Arts Centre celebrating the award recipients the following day.
Geddy Lee.
Geddy Lee's high-register vocal style has always been a signature of the band – and sometimes a focal point for criticism, especially during the early years of Rush's career when Lee's vocals were high-pitched, with a strong likeness to other singers like Robert Plant of Led Zeppelin. A review in the "New York Times" opined that Lee's voice "suggests a munchkin giving a sermon." Although his voice has softened over the years, it is often described as a "wail". His instrumental abilities, on the other hand, are rarely criticized. He has cited Jeff Berlin, Jack Casady, John Entwistle, Jack Bruce and Chris Squire as the bassists who had the biggest impact on his playing style. An award-winning musician, Lee's style, technique, and ability on the bass guitar have proven influential in the rock and heavy metal genres, inspiring such players as Steve Harris of Iron Maiden, John Myung of Dream Theater, Les Claypool of Primus, and Cliff Burton of Metallica among others. Lee is notable for his ability to operate various pieces of instrumentation simultaneously. This is most evident during live shows when Lee must play bass, supply lead vocals, manipulate keyboards, and trigger pedals during the course of a performance, as in the song "Tom Sawyer". Because of this, he is required to remain in one place during songs containing complex instrumentation. Lifeson and Peart are, to a lesser extent, responsible for similar actions during live shows.
Alex Lifeson.
Lifeson as a guitarist is best known for his signature riffing, electronic effects and processing, unorthodox chord structures, and a copious arsenal of equipment used over the years.
During his adolescent years, he was influenced by Jimi Hendrix, Pete Townshend, Jeff Beck, Eric Clapton and Jimmy Page. For versatility, Lifeson was known to incorporate touches of Spanish and classical music into Rush's guitar-driven sound during the 1970s. Taking a backseat to Lee's keyboards in the 1980s, Lifeson's guitar returned to the forefront in the 1990s, and especially on "Vapor Trails" (2002). During live performances, he is still responsible for cuing various guitar effects, the use of bass-pedal synthesizers and backing vocals.
Neil Peart.
Peart has been voted the greatest rock drummer by music fans, critics and fellow musicians, according to Drummerworld. He is also regarded as one of the finest practitioners of the in-concert drum solo. Initially inspired by Keith Moon, Peart absorbed the influence of other rock drummers from the 1960s and 1970s such as Ginger Baker, Carmine Appice, and John Bonham. Incorporation of unusual instruments (for rock drummers of the time) such as the glockenspiel and tubular bells, along with several standard kit elements, helped create a highly varied setup. Continually modified to this day, Peart's drumkit offers an enormous array of percussion instruments for sonic diversity. For two decades Peart honed his technique; each new Rush album introduced an expanded percussive vocabulary. In the 1990s, he reinvented his style with the help of drum coach Freddie Gruber.
Peart also serves as Rush's primary lyricist, attracting much attention over the years for his eclectic style. Known for penning concept suites and songs inspired by literature, music fan opinions of his writing have varied greatly, running the gamut from cerebral and insightful to pretentious and preachy. During the band's early years, Peart's lyrics were largely fantasy/science fiction-focused, though since 1980 he has focused more on social, emotional, and humanitarian issues. In 2007, he was placed second on "Blender" magazine's list of the "40 Worst Lyricists In Rock". Allmusic, however has called Peart "one of rock's most accomplished lyricists", Gibson.com describes Rush's lyrics as "great", and others believe the lyrics are "brilliant".
Sales.
Rush has released 24 gold records and 14 platinum records (including 3 multi-platinum), placing them third behind The Beatles and The Rolling Stones for the most consecutive gold or platinum studio albums by a rock band. As of 2005, Rush had sold about 25 million albums in the United States (ranking them 79th among recording acts) and 40 million worldwide. As of 2012, "Moving Pictures" was the band's highest-selling album (4.4 million units).
Despite dropping out of the public eye for five years after the gold-selling "Test for Echo" (which peaked at No. 5 on the "Billboard" 200 chart) and the band being relegated almost solely to classic rock stations in the U.S., "Vapor Trails" reached No. 6 on the "Billboard" 200 in its first week of release in 2002 with 108,000 albums sold. It has sold about 343,000 units to date. The subsequent "Vapor Trails" tour grossed over $24 million and included the largest audience ever to see a headlining Rush show: 60,000 fans in São Paulo, Brazil. Nevertheless, "Vapor Trails" remains their first album not to achieve at least gold status in the United States.
Rush's triple CD live album, "Rush in Rio" (2003), was certified gold, marking the fourth decade in which a Rush album had been released and certified at least gold. In 2004, "Feedback" cracked the top 20 on the "Billboard" 200 and received radio airplay. The band's 2007 album, "Snakes & Arrows", debuted at number 3 (just one position shy of Rush's highest peaking albums, "Counterparts" (1993) and "Clockwork Angels" (2012), which both debuted at number 2) on the "Billboard" 200, selling about 93,000 its first week of release. This marks the 13th studio album to appear in the Top 20 and the band's 27th album to appear on the chart. The album also debuted at number 1 on the Billboard's Top Rock Albums chart, and, when the album was released on the MVI format a month later, peaked at number 1 on the Top Internet Albums chart.
The tours in support of "Snakes & Arrows" in 2007 and 2008 accrued $21 million and $18.3 million, respectively, earning Rush the number 6 and 8 spots among the summers' rock concerts.
Live performances.
The members of Rush share a strong work ethic, desiring to accurately recreate songs from their albums when playing live performances. To achieve this goal, beginning in the late 1980s, Rush has included a capacious rack of digital samplers in their concert equipment to recreate the sounds of non-traditional instruments, accompaniments, vocal harmonies, and other sound "events" in real-time to match the sounds on the studio versions of the songs. In live performances, the band members share duties throughout most songs. Each member has one or more MIDI controllers, which are loaded with different sounds for each song, and use available limbs to trigger the sounds while simultaneously playing their primary instrument(s). It is with this technology that the group is able to present their arrangements in a live setting with the level of complexity and fidelity fans have come to expect, and without the need to resort to the use of backing tracks or employing an additional band member. The band members' coordinated use of pedal keyboards and other electronic triggers to "play" sampled instruments and audio events is subtly visible in their live performances, especially so on , their 2005 concert DVD.
A staple of Rush's concerts is a Neil Peart drum solo. Peart's drum solos include a basic framework of routines connected by sections of improvization, making each performance unique. Each successive tour sees the solo more advanced, with some routines dropped in favour of newer, more complex ones. Since the mid-1980s, Peart has used MIDI trigger pads to trigger sounds sampled from various pieces of acoustic percussion that would otherwise consume far too much stage area, such as a marimba, harp, temple blocks, triangles, glockenspiel, orchestra bells, tubular bells, and vibraslap as well as other, more esoteric percussion.
Philanthropy.
Rush actively participates in philanthropic causes. The band was one of several hometown favourites to play Molson Canadian Rocks for Toronto, also dubbed SARStock, at Downsview Park in Toronto on July 30, 2003, with an attendance of over half a million people. The concert was intended to benefit the Toronto economy after the SARS outbreaks earlier in the year. The band has also sustained an interest in promoting human rights. They donated $100,000 to the Canadian Museum for Human Rights after a concert they held in Winnipeg on May 24, 2008. Rush continues to sell T-shirts and donate the proceeds to the museum.
On July 24, 2013, Rush performed a benefit concert in Red Deer, Alberta, at the Enmax Centrium with all proceeds going to the Canadian Red Cross to help victims of the 2013 flooding that devastated many regions of southern Alberta. The original venue for the show, the Scotiabank Saddledome, was heavily damaged from the flooding and was unavailable for the concert date as originally planned.
The individual members of Rush have also been a part of philanthropic causes. Hughes & Kettner zenTera and TriAmp electronics have been endorsed and used by Lifeson for many years. A custom signature amplifier was engineered by Lifeson and released in April 2005 with the stipulation that UNICEF will receive a donation in the amount of $50 for every Alex Lifeson Signature TriAmp sold. Lee, a longtime fan of baseball, donated 200 baseballs signed by famous Negro League players, including Willie Mays, Hank Aaron and Josh Gibson, to the Negro Leagues Baseball Museum in June 2008. In late 2009, Geddy Lee and Alex Lifeson launched an auction for their initiative "Grapes Under Pressure", in support of the cause "Grapes for Humanity". The auction consisted of items from the band such as signed guitars, cymbals and basses, as well as autographs on all items by the band members. There were also autographs by band members from Depeche Mode, Tool, the Fray, Judas Priest, Pearl Jam and more, as well as signatures from Ricky, Julian and Bubbles from "" on a rare Epiphone guitar.
The band is featured on the album "Songs for Tibet", appearing with other celebrities as an initiative to support Tibet and the current Dalai Lama Tenzin Gyatso. The album was made downloadable on August 5, 2008 via iTunes and was released commercially August 12, 2008.
Rush has also been a big supporter of Little Kids Rock, a national nonprofit that works to restore and revitalize music education programs in disadvantaged U.S. public schools. They teamed up with Musician's Friend and Sabian to help Little Kids Rock provide percussion to public schools nationwide. They donated $500 of proceeds for every Neil Peart Paragon Cymbal Pack they sold, each of which came with a free splash cymbal personalized, autographed and dated by the Rush drummer himself. The cause-based marketing initiative raised over $50,000 for Little Kids Rock.
Further reading.
Books.
</dl>
Scholarly articles.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Rush (band)" article dated 2009-01-18, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="25433" url="http://en.wikipedia.org/wiki?curid=25433" title="Ronald Reagan">
Ronald Reagan

Ronald Wilson Reagan (; February 6, 1911 – June 5, 2004) was an American politician and conservative spokesman who served as the 40th President of the United States from 1981 to 1989. Prior to his presidency, he served as the 33rd Governor of California from 1967 to 1975, following a career as an actor and union leader in Hollywood.
Raised in a poor family in small towns in Northern Illinois, he graduated from Eureka College in 1932 and worked as a sports announcer on regional radio stations. Moving to Hollywood in 1937, he became a "B-move" film actor, starring in a few major productions. Reagan was twice elected twice as President of the Screen Actors Guild, where he tried to root out Communist influence. In the 1950s he was a spokesman for General Electric. Originally a liberal Democrat, he became conservative Republican in 1962.. In 1964, Reagan's speech, "A Time for Choosing", in support of Barry Goldwater's presidential campaign, earned him national attention. Building a network of supporters, he was elected governor of California in 1966. As governor, Reagan turned a state budget deficit to a surplus, ordered National Guard troops in during a period of protest movements in 1969, and was re-elected in 1970. He twice ran unsuccessfully for the Republican nominations in 1968 and 1976; four years later, he would win the nomination outright, going on to be elected the oldest President, defeating incumbent Jimmy Carter in 1980.
Entering the Presidency in 1981, Reagan implemented sweeping new political and economic initiatives. His supply-side economic policies, dubbed "Reaganomics", advocated tax rate reduction to spur economic growth, control of the money supply to curb inflation, economic deregulation, and reduced government spending. In his first term he survived an assassination attempt, escalated the War on Drugs, and fought public-sector labor. His economic policies saw a reduction of inflation from 12.5% to 4.4%, and an average annual growth of GDP of 7.91%; while Reagan did enact cuts in domestic spending, military spending increased federal outlays overall, even after adjustment for inflation. During his reelection bid, Reagan campaigned on the notion that it was "Morning in America", winning a landslide in 1984 with the largest electoral college victory in history. Foreign affairs dominated his second term, including ending of the Cold War, the bombing of Libya, and the revelation of the Iran–Contra affair. Publicly describing the Soviet Union as an "evil empire", he transitioned Cold War policy from détente to rollback, by escalating an arms race with the USSR while engaging in talks with Soviet General Secretary Mikhail Gorbachev, which culminated in the INF Treaty, shrinking both countries' nuclear arsenals. During his famous speech at the Brandenburg Gate, President Reagan challenged Soviet leader Mikhail Gorbachev to "tear down this wall!" Shortly after the end of his term, the Berlin Wall fell and the Soviet Union collapsed soon thereafter.
Leaving office in 1989, Reagan held an approval rating of sixty-eight percent, matching those of Franklin D. Roosevelt, and later Bill Clinton, as the highest ratings for departing presidents in the modern era. While having planned an active post-presidency, in 1994 Reagan disclosed his diagnosis with Alzheimer's disease earlier that year, appearing publicly for the last time at the funeral of Richard Nixon; he died ten years later at the age of 93. An icon among Republicans, he ranks highly in public and critical opinion of U.S. Presidents, and his tenure constituted a realignment toward conservative policies in the United States.
Early life.
Ronald Wilson Reagan was born in an apartment on the second floor of a commercial building in Tampico, Illinois on February 6, 1911, the son of Nelle Clyde (Wilson) and John Edward "Jack" Reagan. Reagan's father was a salesman and a storyteller, the grandson of Irish Catholic immigrants from County Tipperary, while his mother was of half Scots and half English descent (Reagan's maternal grandmother was born in Surrey, England). Reagan had one older brother, Neil
(1908–96), who became an advertising executive. As a boy, Reagan's father nicknamed his son "Dutch", due to his "fat little Dutchman"-like appearance, and his "Dutchboy" haircut; the nickname stuck with him throughout his youth. Reagan's family briefly lived in several towns and cities in Illinois, including Monmouth, Galesburg, and Chicago, in 1919, they returned to Tampico and lived above the H.C. Pitney Variety Store until finally settling in Dixon. After his election as president, residing in the upstairs White House private quarters, Reagan would quip that he was "living above the store again".
According to Paul Kengor, author of "God and Ronald Reagan", Reagan had a particularly strong faith in the goodness of people, which stemmed from the optimistic faith of his mother, Nelle, and the Disciples of Christ faith, which he was baptized into in 1922. For the time, Reagan was unusual in his opposition to racial discrimination, and recalled a time in Dixon when the local inn would not allow black people to stay there. Reagan brought them back to his house, where his mother invited them to stay the night and have breakfast the next morning.
After the closure of the Pitney Store in late 1920, the Reagans moved to Dixon; the midwestern "small universe" had a lasting impression on Reagan. He attended Dixon High School, where he developed interests in acting, sports, and storytelling.. His first job was as a lifeguard at the Rock River in Lowell Park, near Dixon, in 1927. Over a six-year period, Reagan reportedly performed 77 rescues as a lifeguard. Reagan attended Eureka College, a Disciples-oriented liberal arts school, where he became a member of the Tau Kappa Epsilon fraternity, a cheerleader, and studied economics and sociology. He developed a reputation as a jack of all trades, excelling in campus politics, sports and theater. He was a member of the football team and captain of the swim team. He was elected student body president and led a student revolt against the college president after he tried to cut back the faculty.
Entertainment career.
Radio and film.
After graduating from Eureka in 1932, Reagan drove himself to Iowa, where he held jobs as an announcer at several stations. He moved to WHO radio in Des Moines as an announcer for Chicago Cubs baseball games. His specialty was creating play-by-play accounts of games using as his source only basic descriptions that the station received by wire as the games were in progress.
While traveling with the Cubs in California, Reagan took a screen test in 1937 that led to a seven-year contract with Warner Brothers studios.. He spent the first few years of his Hollywood career in the "B film" unit, where, Reagan joked, the producers "didn't want them good, they wanted them Thursday". 
His first screen credit was the starring role in the 1937 movie "Love Is on the Air", and by the end of 1939 he had already appeared in 19 films, including "Dark Victory" with Bette Davis and Humphrey Bogart. Before the film "Santa Fe Trail" with Errol Flynn in 1940, he played the role of George "The Gipper" Gipp in the film "Knute Rockne, All American"; from it, he acquired the lifelong nickname "the Gipper". In 1941 exhibitors voted him the fifth most popular star from the younger generation in Hollywood.
Reagan's favorite acting role was as a double amputee in 1942's "Kings Row", in which he recites the line, "Where's the rest of me?", later used as the title of his 1965 autobiography. Many film critics considered "Kings Row" to be his best movie, though the film was condemned by "New York Times" critic Bosley Crowther.
Although Reagan called "Kings Row" the film that "made me a star", he was unable to capitalize on his success because he was ordered to active duty with the U.S. Army at San Francisco two months after its release, and never regained "star" status in motion pictures. In the post-war era, after being separated from almost four years of World War II stateside service with the 1st Motion Picture Unit in December 1945, Reagan co-starred in such films as, "The Voice of the Turtle", "John Loves Mary", "The Hasty Heart", "Bedtime for Bonzo", "Cattle Queen of Montana", "Tennessee's Partner", "Hellcats of the Navy" (the only film in which he appears with Nancy Reagan) and the 1964 remake "The Killers" (his final film and the only one in which he played a villain). Throughout his film career, his mother often answered much of his fan mail.
Military service.
After completing fourteen home-study Army Extension Courses, Reagan enlisted in the Army Enlisted Reserve and was commissioned a second lieutenant in the Officers Reserve Corps of the cavalry on May 25, 1937.
Reagan was ordered to active duty for the first time on April 18, 1942. Due to his poor eyesight, he was classified for limited service only, which excluded him from serving overseas. His first assignment was at the San Francisco Port of Embarkation at Fort Mason, California, as a liaison officer of the Port and Transportation Office. Upon the approval of the Army Air Force (AAF), he applied for a transfer from the cavalry to the AAF on May 15, 1942, and was assigned to AAF Public Relations and subsequently to the First Motion Picture Unit (officially, the "18th Army Air Force Base Unit") in Culver City, California. On January 14, 1943, he was promoted to first lieutenant and was sent to the Provisional Task Force Show Unit of "This Is The Army" at Burbank, California. He returned to the First Motion Picture Unit after completing this duty and was promoted to captain on July 22, 1943.
In January 1944, Reagan was ordered to temporary duty in New York City to participate in the opening of the Sixth War Loan Drive. He was reassigned to the First Motion Picture Unit on November 14, 1944, where he remained until the end of World War II. He was recommended for promotion to major on February 2, 1945, but this recommendation was disapproved on July 17 of that year. While with the First Motion Picture Unit in 1945, he was indirectly involved in discovering actress Marilyn Monroe. He returned to Fort MacArthur, California, where he was separated from active duty on December 9, 1945. By the end of the war, his units had produced some 400 training films for the AAF.
Reagan never left the United States during the war, though he kept a film reel, obtained while in the service, depicting the liberation of Auschwitz, as he believed that someday doubts would arise as to whether the Holocaust had occurred. It has been alleged that he was overheard telling Israeli foreign minister Yitzhak Shamir in 1983 that he had filmed that footage himself and helped liberate Auschwitz, though this purported conversation was disputed by Secretary of State George Shultz.
SAG president.
Reagan was first elected to the Board of Directors of the Screen Actors Guild in 1941, serving as an alternate. After World War II, he resumed service and became 3rd vice-president in 1946. The adoption of conflict-of-interest bylaws in 1947 led the SAG president and six board members to resign; Reagan was nominated in a special election for the position of president and subsequently elected. He was subsequently chosen by the membership to serve seven additional one-year terms, from 1947 to 1952 and in 1959. Reagan led SAG through eventful years that were marked by labor-management disputes, the Taft-Hartley Act, House Committee on Un-American Activities (HUAC) hearings and the Hollywood blacklist era.
Secret FBI informant in Hollywood.
During the late 1940s, Reagan and his wife provided the FBI with names of actors within the motion picture industry whom they believed to be communist sympathizers, though he expressed reservations; he said "Do they expect us to constitute ourselves as a little FBI of our own and determine just who is a Commie and who isn't?".
Reagan testified before the House Un-American Activities Committee on the subject as well. A fervent anti-communist, he reaffirmed his commitment to democratic principles, stating, "I never as a citizen want to see our country become urged, by either fear or resentment of this group, that we ever compromise with any of our democratic principles through that fear or resentment."
Television.
Though an early critic of television, Reagan landed fewer film roles in the late 1950s and decided to join the medium. He was hired as the host of "General Electric Theater", a series of weekly dramas that became very popular. His contract required him to tour GE plants sixteen weeks out of the year, often demanding of him fourteen speeches per day. He earned approximately $125,000 per year (about $1.07 million in 2010 dollars) in this role. His final work as a professional actor was as host and performer from 1964 to 1965 on the television series "Death Valley Days". Reagan and Nancy Davis appeared together several times, including an episode of "General Electric Theater" in 1958 called "A Turkey for the President".
Marriages and children.
In 1938, Reagan co-starred in the film "Brother Rat" with actress Jane Wyman (1917–2007). They were engaged at the Chicago Theatre, and married on January 26, 1940, at the Wee Kirk o' the Heather church in Glendale, California. Together they had two biological children, Maureen (1941–2001) and Christine (who was born in 1947 but only lived one day), and adopted a third, Michael (born 1945). After arguments about Reagan's political ambitions, Wyman filed for divorce in 1948, citing a distraction due to her husband's Screen Actors Guild union duties; the divorce was finalized in 1949. He is the only US president to have been divorced. Reagan and Wyman continued to be friends until his death, with Wyman voting for Reagan in both of his runs and, upon his death, saying "America has lost a great president and a great, kind, and gentle man."
Reagan met actress Nancy Davis (born 1921) in 1949 after she contacted him in his capacity as president of the Screen Actors Guild to help her with issues regarding her name appearing on a Communist blacklist in Hollywood. She had been mistaken for another Nancy Davis. She described their meeting by saying, "I don't know if it was exactly love at first sight, but it was pretty close." They were engaged at Chasen's restaurant in Los Angeles and were married on March 4, 1952, at the Little Brown Church in the San Fernando Valley. Actor William Holden served as best man at the ceremony. They had two children: Patti (born October 21, 1952) and Ron (born May 20, 1958).
Observers described the Reagans' relationship as close, authentic and intimate. During his presidency they were reported to frequently display their affection for one another; one press secretary said, "They never took each other for granted. They never stopped courting." He often called her "Mommy" and she called him "Ronnie". He once wrote to her, "Whatever I treasure and enjoy ... all would be without meaning if I didn't have you." When he was in the hospital in 1981, she slept with one of his shirts to be comforted by his scent. In a letter to U.S. citizens written in 1994, Reagan wrote "I have recently been told that I am one of the millions of Americans who will be afflicted with Alzheimer's disease... I only wish there was some way I could spare Nancy from this painful experience", and in 1998, while Reagan was stricken by Alzheimer's, Nancy told "Vanity Fair", "Our relationship is very special. We were very much in love and still are. When I say my life began with Ronnie, well, it's true. It did. I can't imagine life without him."
Early political career.
Reagan began his political career as a Democrat and, in December 1945, was prevented from leading an anti-nuclear rally in Hollywood by pressure from the Warner Bros. studio. He would later make nuclear weapons a key point of his presidency, specifically his opposition to mutually assured destruction, building on previous efforts to limit the spread of nuclear weapons to a new focus to reduce the numbers and types of them. In the 1948 election, Reagan strongly supported Harry S. Truman, appearing on stage with him during a campaign speech in Los Angeles. However, in the early 1950s, as his relationship with Republican actress Nancy Davis grew, he shifted to the right and, while remaining a Democrat, endorsed the presidential candidacies of Dwight D. Eisenhower in 1952 and 1956 as well as Richard Nixon in 1960. The last time Reagan actively supported a Democratic candidate was in 1950 when he helped Helen Gahagan Douglas in her unsuccessful Senate campaign against Richard Nixon.
After being hired in 1954 to host the "General Electric Theater", a TV drama series, and even more important crisscrossed the country giving talks to over 200,000 GE employees. Reagan already embraced the conservative views of the sponsoring company's officials. His many speeches—which he wrote himself—were non-partisan but carried a conservative, pro-business message; he was influenced by Lemuel Boulware, a senior GE executive. Boulware, known for his tough stance against unions and his innovative strategies to win over workers, championed the core tenets of modern American conservatism: free markets, anticommunism, lower taxes, and limited government. Eager for a larger stage, but not allowed to enter politics by GE, he quit and formally registered as a Republican. He often said "I didn't leave the Democratic Party. The party left me".
When legislation that would become Medicare was introduced in 1961, Reagan created a recording for the American Medical Association warning that such legislation would mean the end of freedom in America. Reagan said that if his listeners did not write letters to prevent it, "we will awake to find that we have socialism. And if you don't do this, and if I don't do it, one of these days, you and I are going to spend our sunset years telling our children, and our children's children, what it once was like in America when men were free." He also joined the National Rifle Association and would become a lifetime member.
Reagan gained national attention in his speeches for conservative presidential contender Barry Goldwater in 1964. Speaking for Goldwater, Reagan stressed his belief in the importance of smaller government. Consolidating themes he had developed in talks for GE, he argued in "A Time for Choosing" (October 27, 1964): "The Founding Fathers knew a government can't control the economy without controlling people. And they knew when a government sets out to do that, it must use force and coercion to achieve its purpose. So we have come to a time for choosing." He went on: "You and I are told we must choose between a left or right, but I suggest there is no such thing as a left or right. There is only an up or down. Up to man's age-old dream – the maximum of individual freedom consistent with order – or down to the ant heap of totalitarianism." This "A Time for Choosing" speech, which later became known as "The Speech", raised $1 million for Goldwater's campaign and was the key event that launched Reagan's political career.
Governor of California, 1967–75.
California Republicans were impressed with Reagan's political views and charisma after his "Time for Choosing" speech, he announced in late 1965, his campaign for Governor of California in 1966. He defeated former San Francisco mayor George Christopher in the GOP primary. In Reagan's campaign, he emphasized two main themes: "to send the welfare bums back to work", and, in reference to burgeoning anti-war and anti-establishment student protests at the University of California at Berkeley, "to clean up the mess at Berkeley". Ronald Reagan accomplished in 1966 what US Senator William F. Knowland in 1958 and former Vice-President Richard M. Nixon in 1962 had tried: he was elected, defeating two-term governor Edmund G. "Pat" Brown, and was sworn in on January 2, 1967. In his first term, he froze government hiring and approved tax hikes to balance the budget.
Shortly after the beginning of his term, Reagan tested the presidential waters in 1968 as part of a "Stop Nixon" movement, hoping to cut into Nixon's Southern support and be a compromise candidate if neither Nixon nor second-place Nelson Rockefeller received enough delegates to win on the first ballot at the Republican convention. However, by the time of the convention Nixon had 692 delegate votes, 25 more than he needed to secure the nomination, followed by Rockefeller with Reagan in third place.
Reagan was involved in high-profile conflicts with the protest movements of the era. On May 15, 1969, during the at UC Berkeley, Reagan sent the California Highway Patrol and other officers to quell the protests, in an incident that became known as "Bloody Thursday", resulting in the death of student James Rector and the blinding of carpenter Alan Blanchard. Reagan then called out 2,200 state National Guard troops to occupy the city of Berkeley for two weeks to crack down on the protesters. A year after "Bloody Thursday", Reagan responded to questions about campus protest movements saying, "If it takes a bloodbath, let's get it over with. No more appeasement." When the Symbionese Liberation Army kidnapped Patty Hearst in Berkeley and demanded the distribution of food to the poor, Reagan joked to a group of political aides about a botulism outbreak contaminating the food. Conversely, in that one afternoon, "Bloody Thursday", 111 police officers were injured, including one C.H.P. officer who was knifed in the chest. After calling in the National Guard, the Guard remained in Berkeley for 17 days, camping in People's Park, and demonstrations subsided as the University removed cordoned-off fencing and placed all development plans for People's Park on hold.
Early in 1967, the national debate on abortion was beginning. Democratic California state senator Anthony Beilenson introduced the "Therapeutic Abortion Act", in an effort to reduce the number of "back-room abortions" performed in California. The State Legislature sent the bill to Reagan's desk where, after many days of indecision, he signed it. About two million abortions would be performed as a result, most because of a provision in the bill allowing abortions for the well-being of the mother. Reagan had been in office for only four months when he signed the bill, and stated that had he been more experienced as governor, he would not have signed it. After he recognized what he called the "consequences" of the bill, he announced that he was pro-life. He maintained that position later in his political career, writing extensively about abortion.
Despite an unsuccessful attempt to recall him in 1968, Reagan was re-elected in 1970, defeating "Big Daddy" Jesse Unruh. He chose not to seek a third term in the following election cycle. One of Reagan's greatest frustrations in office concerned capital punishment, which he strongly supported. His efforts to enforce the state's laws in this area were thwarted when the Supreme Court of California issued its "People v. Anderson" decision, which invalidated all death sentences issued in California before 1972, though the decision was later overturned by a constitutional amendment. The only execution during Reagan's governorship was on April 12, 1967, when Aaron Mitchell's sentence was carried out by the state in San Quentin's gas chamber.
In 1969, Reagan, as Governor, signed the "Family Law Act", an amalgam of two bills which had been written and revised by the California state legislature for over two years and became the first no-fault divorce legislation in the United States.
Reagan's terms as governor helped to shape the policies he would pursue in his later political career as president. By campaigning on a platform of sending "the welfare bums back to work", he spoke out against the idea of the welfare state. He also strongly advocated the Republican ideal of less government regulation of the economy, including that of undue federal taxation.
Reagan did not seek re-election to a third term as governor in 1974 and was succeeded by Democratic California Secretary of State Jerry Brown on January 6, 1975.
1976 presidential campaign.
In 1976, Reagan challenged incumbent President Gerald Ford in a bid to become the Republican Party's candidate for president. Reagan soon established himself as the conservative candidate with the support of like-minded organizations such as the American Conservative Union which became key components of his political base, while President Ford was considered a more moderate Republican.
Reagan's campaign relied on a strategy crafted by campaign manager John Sears of winning a few primaries early to damage the inevitability of Ford's likely nomination. Reagan won North Carolina, Texas, and California, but the strategy failed, as he ended up losing New Hampshire, Florida, and his native Illinois. The Texas campaign lent renewed hope to Reagan, when he swept all ninety-six delegates chosen in the May 1 primary, with four more awaiting at the state convention. Much of the credit for that victory came from the work of three co-chairmen, including Ernest Angelo, the mayor of Midland, and Ray Barnhart of Houston, whom President Reagan would appoint in 1981 as director of the Federal Highway Administration.
However, as the GOP convention neared, Ford appeared close to victory. Acknowledging his party's moderate wing, Reagan chose moderate Senator Richard Schweiker of Pennsylvania as his running mate if nominated. Nonetheless, Ford prevailed with 1,187 delegates to Reagan's 1,070. Ford would go on to lose the 1976 Presidential election to the Democrat Jimmy Carter.
Reagan's concession speech emphasized the dangers of nuclear war and the threat posed by the Soviet Union. Though he lost the nomination, he received 307 write-in votes in New Hampshire, 388 votes as an Independent on Wyoming's ballot, and a single electoral vote from a faithless elector in the November election from the state of Washington, which Ford had won over Democratic challenger Jimmy Carter.
After the campaign, Reagan remained in the public debate with the Ronald Reagan Radio Commentary series and his political action committee, Citizens for the Republic, which was later revived in Alexandria, Virginia in 2009 by the Reagan biographer Craig Shirley.
1980 presidential campaign.
The 1980 presidential campaign between Reagan and incumbent President Jimmy Carter was conducted during domestic concerns and the ongoing Iran hostage crisis. His campaign stressed some of his fundamental principles: lower taxes to stimulate the economy, less government interference in people's lives, states' rights, and a strong national defense.
Reagan launched his campaign by declaring "I believe in states' rights." After receiving the Republican nomination, Reagan selected one of his primary opponents, George H.W. Bush, to be his running mate. His showing in the October televised debate boosted his campaign. Reagan won the election, carrying 44 states with 489 electoral votes to 49 electoral votes for Carter (representing six states and Washington, D.C.). Reagan received 50.7% of the popular vote while Carter took 41%, and Independent John B. Anderson (a liberal Republican) received 6.7%. Republicans captured the Senate for the first time since 1952, and gained 34 House seats, but the Democrats retained a majority.
Presidency, 1981–89.
During his Presidency, Reagan pursued policies that reflected his personal belief in individual freedom, brought changes domestically, both to the U.S. economy and expanded military, and contributed to the end of the Cold War. Termed the Reagan Revolution, his presidency would reinvigorate American morale, reinvigorate the American economy and reduce American reliance upon government. As president, Reagan kept a diary in which he commented on daily occurrences of his presidency and his views on the issues of the day. The diaries were published in May 2007 in the bestselling book, "The Reagan Diaries".
First term, 1981–85.
To date, Reagan is the oldest person elected to the office of the presidency (at 69) and the oldest president at the time of inauguration (at 69 years, 341 days). In his first inaugural address on January 20, 1981, which Reagan himself wrote, he addressed the country's economic malaise arguing: "In this present crisis, government is not the solution to our problems; government is the problem".
Prayer in schools and a moment of silence.
In 1981, Reagan became the first president to propose a constitutional amendment on school prayer. The school prayer had previously been banned by the supreme court in 1962, and Reagan's election reflected an opposition to the courts decision. Reagan's 1981 proposed amendment stated: "Nothing in this Constitution shall be construed to prohibit individual or group prayer in public schools or other public institutions. No person shall be required by the United States or by any state to participate in prayer." In a message to Congress, Reagan said that his proposed amendment would "restore the simple freedom of our citizens to offer prayer in public schools and institutions." In a nationally televised speech the following day, Rabbi Menachem M. Schneerson lauded Reagan's speech and said the moment of silence would "ensure that children grow up to be decent and upright." In 1984, Reagan again raised the issue, asking Congress "why can't [the] freedom to acknowledge God be enjoyed again by children in every schoolroom across this land?" In 1985, Reagan expressed his disappointment that the Supreme Court ruling still bans a moment of silence for public-school, and said he had "an uphill battle." In 1987 Reagan again renewed his call for Congress to support voluntary prayer in schools and end "the expulsion of God from America's classrooms." During his term in office, Reagan campaigned vigorously to restore prayer to the schools, first as a moment of prayer and later as a Moment of Silence.
Assassination attempt.
On March 30, 1981, only 69 days into the new administration, Reagan, his press secretary James Brady, Washington police officer Thomas Delahanty, and Secret Service agent Timothy McCarthy were struck by gunfire from would-be assassin John Hinckley Jr., outside the Washington Hilton Hotel. Although "close to death" upon arrival at George Washington University Hospital, Reagan was stabilized in the emergency room, then underwent emergency exploratory surgery. He recovered and was released from the hospital on April 11, becoming the first serving U.S. President to survive being shot in an assassination attempt. The attempt had great influence on Reagan's popularity; polls indicated his approval rating to be around 73%. Reagan believed that God had spared his life so that he might go on to fulfill a greater purpose.
Air traffic controllers' strike.
In summer 1981 PATCO, the union of federal air traffic controllers went on strike, violating a federal law prohibiting government unions from striking. Declaring the situation an emergency as described in the 1947 Taft–Hartley Act, Reagan stated that if the air traffic controllers "do not report for work within 48 hours, they have forfeited their jobs and will be terminated". They did not return and on August 5, Reagan fired 11,345 striking air traffic controllers who had ignored his order, and used supervisors and military controllers to handle the nation's commercial air traffic until new controllers could be hired and trained. A leading reference work on public administration concluded, "The firing of PATCO employees not only demonstrated a clear resolve by the president to take control of the bureaucracy, but it also sent a clear message to the private sector that unions no longer needed to be feared".
"Reaganomics" and the economy.
During Jimmy Carter's last year in office (1980), inflation averaged 12.5%, compared with 4.4% during Reagan's last year in office (1988). During Reagan's administration, the unemployment rate declined from 7.5% to 5.4%, with the rate reaching highs of 10.8% in 1982 and 10.4% in 1983, averaging 7.5% over the eight years, and GDP growth average 7.9% with a high of 12.2% growth in 1982.
Reagan implemented policies based on supply-side economics, advocating a "laissez-faire" philosophy and free-market fiscal policy, seeking to stimulate the economy with large, across-the-board tax cuts. He also supported returning the United States to some sort of gold standard, and successfully urged Congress to establish the U.S. Gold Commission to study how one could be implemented. Citing the economic theories of Arthur Laffer, Reagan promoted the proposed tax cuts as potentially stimulating the economy enough to expand the tax base, offsetting the revenue loss due to reduced rates of taxation, a theory that entered political discussion as the Laffer curve. Reaganomics was the subject of debate with supporters pointing to improvements in certain key economic indicators as evidence of success, and critics pointing to large increases in federal budget deficits and the national debt. His policy of "peace through strength" resulted in a record peacetime defense buildup including a 40% real increase in defense spending between 1981 and 1985.
During Reagan's presidency, federal income tax rates were lowered significantly with the signing of the Economic Recovery Tax Act of 1981 which lowered the top marginal tax bracket from 70% to 50% and the lowest bracket from 14% to 11%, however other tax increases passed by Congress and signed by Reagan, ensured that tax revenues over his two terms were 18.2% of GDP as compared to 18.1% over the 40-year period 1970–2010. Then, in 1982 the Job Training Partnership Act of 1982 was signed into law, initiating one of the United States' first public-private partnerships and a major part of the president's job creation program. Reagan's Assistant Secretary of Labor and Chief of Staff, Al Angrisani, was a primary architect of the bill.
Conversely, Congress passed and Reagan signed into law tax increases of some nature in every year from 1981 to 1987 to continue funding such government programs as Tax Equity and Fiscal Responsibility Act of 1982 (TEFRA), Social Security, and the Deficit Reduction Act of 1984 (DEFRA). Despite the fact that TEFRA was the "largest peacetime tax increase in American history", gross domestic product (GDP) growth recovered strongly after the early 1980s recession ended in 1982, and grew during his eight years in office at an annual rate of 7.91% per year, with a high of 12.2% growth in 1981. Unemployment peaked at 10.8% monthly rate in December 1982—higher than any time since the Great Depression—then dropped during the rest of Reagan's presidency. Sixteen million new jobs were created, while inflation significantly decreased. The Tax Reform Act of 1986, another bipartisan effort championed by Reagan, simplified the tax code by reducing the number of tax brackets to four and slashing a number of tax breaks. The top rate was dropped to 28%, but capital gains taxes were increased on those with the highest incomes from 20% to 28%. The increase of the lowest tax bracket from 11% to 15% was more than offset by expansion of the personal exemption, standard deduction, and earned income tax credit. The net result was the removal of six million poor Americans from the income tax roll and a reduction of income tax liability at all income levels.
The net effect of all Reagan-era tax bills was a 1% decrease in government revenues when compared to Treasury Department revenue estimates from the Administration's first post-enactment January budgets. However, federal income tax receipts increased from 1980 to 1989, rising from $308.7 billion to $549 billion or an average annual rate of 8.2% (2.5% attributed to higher Social Security receipts), and federal outlays grew at an annual rate of 7.1%.
Reagan's policies proposed that economic growth would occur when marginal tax rates were low enough to spur investment, which would then lead to increased economic growth, higher employment and wages. Critics labeled this "trickle-down economics"—the belief that tax policies that benefit the wealthy will create a "trickle-down" effect to the poor. Questions arose whether Reagan's policies benefited the wealthy more than those living in poverty, and many poor and minority citizens viewed Reagan as indifferent to their struggles. These views were exacerbated by the fact that Reagan's economic regimen included freezing the minimum wage at $3.35 an hour, slashing federal assistance to local governments by 60%, cutting the budget for public housing and Section 8 rent subsidies in half, and eliminating the antipoverty Community Development Block Grant program. The widening gap between the rich and poor had already begun during the 1970s before Reagan's economic policies took effect. Along with Reagan's 1981 cut in the top regular tax rate on unearned income, he reduced the maximum capital gains rate to only 20%. Reagan later set tax rates on capital gains at the same level as the rates on ordinary income like salaries and wages, with both topping out at 28%. Reagan is viewed as an antitax hero despite raising taxes eleven times over the course of his presidency, all in the name of fiscal responsibility. According to Paul Krugman, "Over all, the 1982 tax increase undid about a third of the 1981 cut; as a share of GDP, the increase was substantially larger than Mr. Clinton's 1993 tax increase". According to historian and domestic policy adviser Bruce Bartlett, Reagan's tax increases over the course of his presidency took back half of the 1981 tax cut.
Further following his opposition to government intervention, Reagan cut the budgets of non-military programs including Medicaid, food stamps, federal education programs and the EPA. While he protected entitlement programs, such as Social Security and Medicare, his administration attempted to purge many people with disabilities from the Social Security disability rolls.
The administration's stance toward the Savings and Loan industry contributed to the savings and loan crisis. It is also suggested, by a minority of Reaganomics critics, that the policies partially influenced the stock market crash of 1987, but there is no consensus regarding a single source for the crash. In order to cover newly spawned federal budget deficits, the United States borrowed heavily both domestically and abroad, raising the national debt from $997 billion to $2.85 trillion. Reagan described the new debt as the "greatest disappointment" of his presidency.
He reappointed Paul Volcker as Chairman of the Federal Reserve, and in 1987 he appointed monetarist Alan Greenspan to succeed him. Reagan ended the price controls on domestic oil which had contributed to energy crises in the early 1970s. The price of oil subsequently dropped, and the 1980s did not see the fuel shortages that the 1970s had. Reagan also fulfilled a 1980 campaign promise to repeal the windfall profit tax in 1988, which had previously increased dependence on foreign oil. Some economists, such as Nobel Prize winners Milton Friedman and Robert A. Mundell, argue that Reagan's tax policies invigorated America's economy and contributed to the economic boom of the 1990s. Other economists, such as Nobel Prize winner Robert Solow, argue that the deficits were a major reason why Reagan's successor, George H. W. Bush, reneged on a and raised taxes.
During Reagan's presidency, a program was initiated within the U.S. intelligence community to ensure America's economic strength. The program, Project Socrates, developed and demonstrated the means required for the United States to generate and lead the next evolutionary leap in technology acquisition and utilization for a competitive advantage—automated innovation. To ensure that the United States acquired the maximum benefit from automated innovation, Reagan, during his second term, had an executive order drafted to create a new federal agency to implement the Project Socrates results on a nationwide basis. However, Reagan's term came to end before the executive order could be coordinated and signed, and the incoming Bush administration, labeling Project Socrates as "industrial policy", had it terminated.
Escalation of the Cold War.
Reagan escalated the Cold War, accelerating a reversal from the policy of détente which began in 1979 after the Soviet war in Afghanistan. Reagan ordered a massive buildup of the United States Armed Forces and implemented new policies towards the Soviet Union: reviving the B-1 Lancer program that had been canceled by the Carter administration, and producing the MX missile. In response to Soviet deployment of the SS-20, Reagan oversaw NATO's deployment of the Pershing missile in West Germany.
Together with the United Kingdom's prime minister Margaret Thatcher, Reagan denounced the Soviet Union in ideological terms. In a famous address on June 8, 1982, to the British Parliament in the Royal Gallery of the Palace of Westminster, Reagan said, "the forward march of freedom and democracy will leave Marxism–Leninism on the ash heap of history". On March 3, 1983, he predicted that communism would collapse, stating, "Communism is another sad, bizarre chapter in human history whose last pages even now are being written". In a speech to the National Association of Evangelicals on March 8, 1983, Reagan called the Soviet Union "an evil empire".
After Soviet fighters downed Korean Air Lines Flight 007 near Moneron Island on September 1, 1983, carrying 269 people, including Georgia congressman Larry McDonald, Reagan labeled the act a "massacre" and declared that the Soviets had turned "against the world and the moral precepts which guide human relations among people everywhere". The Reagan administration responded to the incident by suspending all Soviet passenger air service to the United States, and dropped several agreements being negotiated with the Soviets, wounding them financially. As result of the shootdown, and the cause of KAL 007's going astray thought to be inadequacies related to its navigational system, Reagan announced on September 16, 1983, that the Global Positioning System would be made available for civilian use, free of charge, once completed in order to avert similar navigational errors in future.
Under a policy that came to be known as the Reagan Doctrine, Reagan and his administration also provided overt and covert aid to anti-communist resistance movements in an effort to "rollback" Soviet-backed communist governments in Africa, Asia, and Latin America. Reagan deployed the CIA's Special Activities Division to Afghanistan and Pakistan. They were instrumental in training, equipping and leading Mujaheddin forces against the Soviet Army. President Reagan's Covert Action program has been given credit for assisting in ending the Soviet occupation of Afghanistan, though some of the United States funded armaments introduced then would later pose a threat to U.S. troops in the 2000s (decade) war in Afghanistan. However, in a break from the Carter policy of arming Taiwan under the Taiwan Relations Act, Reagan also agreed with the communist government in China to reduce the sale of arms to Taiwan.
In March 1983, Reagan introduced the Strategic Defense Initiative, a defense project that would have used ground- and space-based systems to protect the United States from attack by strategic nuclear ballistic missiles. Reagan believed that this defense shield could make nuclear war impossible, but disbelief that the technology could ever work led opponents to dub SDI "Star Wars" and argue that the technological objective was unattainable. The Soviets became concerned about the possible effects SDI would have; leader Yuri Andropov said it would put "the entire world in jeopardy". For those reasons, David Gergen, former aide to President Reagan, believes that in retrospect, SDI hastened the end of the Cold War.
Critics labeled Reagan's foreign policies as aggressive, imperialistic, and chided them as "warmongering", though they were supported by leading American conservatives who argued that they were necessary to protect U.S. security interests. The Reagan administration also backed anti-communist leaders accused of severe human rights violations, such as Efraín Ríos Montt of Guatemala.
Lebanese Civil War, 1983.
With the approval of Congress, Reagan in 1983 sent forces to Lebanon to reduce the threat of the Lebanese Civil War. The American peacekeeping forces in Beirut, a part of a multinational force during the Lebanese Civil War, were attacked on October 23, 1983. The Beirut barracks bombing killed 241 American servicemen and wounded more than 60 others by a suicide truck bomber. Reagan sent in USS New Jersey (BB-62)battleship to shell Syrian positions in Lebanon. He then withdrew all the marines from Lebanon.
Operation Urgent Fury (Grenada), 1983.
On October 25, 1983, Reagan ordered U.S. forces to invade Grenada, code named Operation Urgent Fury, where a 1979 "coup d'état" had established an independent non-aligned Marxist–Leninist government. A formal appeal from the Organization of Eastern Caribbean States (OECS) led to the intervention of U.S. forces; President Reagan also cited an allegedly regional threat posed by a Soviet-Cuban military build-up in the Caribbean and concern for the safety of several hundred American medical students at St. George's University as adequate reasons to invade. "Operation Urgent Fury" was the first major military operation conducted by U.S. forces since the Vietnam War, several days of fighting commenced, resulting in a U.S. victory, with 19 American fatalities and 116 wounded American soldiers. In mid-December, after a new government was appointed by the Governor-General, U.S. forces withdrew.
1984 presidential campaign.
Reagan accepted the Republican nomination in Dallas, Texas. He proclaimed that it was "morning again in America", regarding the recovering economy and the dominating performance by the U.S. athletes at the 1984 Summer Olympics, among other things. He became the first American president to open an Olympic Games held in the United States.
Reagan's opponent in the 1984 presidential election was former Vice President Walter Mondale. With questions about Reagan's age, and a weak performance in the first presidential debate, his ability to perform the duties of president for another term was questioned. His apparent confused and forgetful behavior was evident to his supporters; they had previously known him clever and witty. Rumors began to circulate that he had Alzheimer's disease. Reagan rebounded in the second debate, and confronted questions about his age, quipping, "I will not make age an issue of this campaign. I am not going to exploit, for political purposes, my opponent's youth and inexperience", which generated applause and laughter, even from Mondale himself.
That November, Reagan was re-elected, winning 49 of 50 states. The president's overwhelming victory saw Mondale carry only his home state of Minnesota (by 3,800 votes) and the District of Columbia. Reagan won a record 525 electoral votes, the most of any candidate in United States history, and received 58.8% of the popular vote to Mondale's 40.6%.
Second term, 1985–89.
Reagan was sworn in as president for the second time on January 20, 1985, in a private ceremony at the White House. Because January 20 fell on a Sunday, a public celebration was not held but took place in the Capitol Rotunda the following day. January 21 was one of the coldest days on record in Washington, D.C.; due to poor weather, inaugural celebrations were held inside the Capitol. In the coming weeks he shook up his staff somewhat, moving White House Chief of Staff James Baker to Secretary of the Treasury and naming Treasury Secretary Donald Regan, a former Merrill Lynch officer, Chief of Staff.
In 1985, Reagan visited a German military cemetery in Bitburg to lay a wreath with West German Chancellor Helmut Kohl. It was determined that the cemetery held the graves of forty-nine members of the Waffen-SS. Reagan issued a statement that called the Nazi soldiers buried in that cemetery as themselves "victims", a designation which ignited a stir over whether Reagan had equated the SS men to victims of the Holocaust; Pat Buchanan, Reagan's Director of Communications, argued that the president did not equate the SS members with the actual Holocaust. Now strongly urged to cancel the visit, the president responded that it would be wrong to back down on a promise he had made to Chancellor Kohl. He ultimately attended the ceremony where two military generals laid a wreath.
The disintegration of the Space Shuttle Challenger on January 28, 1986, proved a pivotal moment in Reagan's presidency. All seven astronauts aboard were killed. On the night of the disaster, Reagan delivered a speech, written by Peggy Noonan, in which he said:
The future doesn't belong to the fainthearted; it belongs to the brave...We will never forget them, nor the last time we saw them, this morning, as they prepared for their journey and waved goodbye and 'slipped the surly bonds of Earth' to 'touch the face of God.'
War on Drugs.
Reagan announced a War on Drugs in 1982, in response to concerns about the increasing crack epidemic. Though Nixon had previously declared a war on drugs, Reagan advocated more militant policies.
He said that "drugs were menacing our society" and promised to fight for drug-free schools and workplaces, expanded drug treatment, stronger law enforcement and drug interdiction efforts, and greater public awareness.
In 1986, Reagan signed a drug enforcement bill that budgeted $1.7 billion to fund the War on Drugs and specified a mandatory minimum penalty for drug offenses. The bill was criticized for promoting significant racial disparities in the prison population and critics also charged that the policies did little to reduce the availability of drugs on the street, while resulting in a great financial burden for America. Defenders of the effort point to success in reducing rates of adolescent drug use. First Lady Nancy Reagan made the War on Drugs her main priority by founding the "Just Say No" drug awareness campaign, which aimed to discourage children and teenagers from engaging in recreational drug use by offering various ways of saying "no". Nancy Reagan traveled to 65 cities in 33 states, raising awareness about the dangers of drugs including alcohol.
Libya bombing.
Relations between Libya and the United States under President Reagan were continually contentious, beginning with the Gulf of Sidra incident in 1981; by 1982, Libyan leader Muammar Gaddafi was considered by the CIA to be, along with USSR leader Leonid Brezhnev and Cuban leader Fidel Castro, part of a group known as the "unholy trinity" and was also labeled as "our international public enemy number one" by a CIA official. These tensions were later revived in early April 1986, when a bomb exploded in a Berlin discothèque, resulting in the injury of 63 American military personnel and death of one serviceman. Stating that there was "irrefutable proof" that Libya had directed the "terrorist bombing", Reagan authorized the use of force against the country. In the late evening of April 15, 1986, the United States launched a series of air strikes on ground targets in Libya.
The UK Prime Minister Margaret Thatcher allowed the U.S. Air Force to use Britain's air bases to launch the attack, on the justification that the UK was supporting America's right to self-defense under Article 51 of the United Nations Charter. The attack was designed to halt Gaddafi's "ability to export terrorism", offering him "incentives and reasons to alter his criminal behavior". The president addressed the nation from the Oval Office after the attacks had commenced, stating, "When our citizens are attacked or abused anywhere in the world on the direct orders of hostile regimes, we will respond so long as I'm in this office". The attack was condemned by many countries. By a vote of 79 in favor to 28 against with 33 abstentions, the United Nations General Assembly adopted resolution 41/38 which "condemns the military attack perpetrated against the Socialist People's Libyan Arab Jamahiriya on April 15, 1986, which constitutes a violation of the Charter of the United Nations and of international law".
Immigration.
Reagan signed the Immigration Reform and Control Act in 1986. The act made it illegal to knowingly hire or recruit illegal immigrants, required employers to attest to their employees' immigration status, and granted amnesty to approximately three million illegal immigrants who entered the United States before January 1, 1982, and had lived in the country continuously. Critics argue that the employer sanctions were without teeth and failed to stem illegal immigration. Upon signing the act at a ceremony held beside the newly refurbished Statue of Liberty, Reagan said, "The legalization provisions in this act will go far to improve the lives of a class of individuals who now must hide in the shadows, without access to many of the benefits of a free and open society. Very soon many of these men and women will be able to step into the sunlight and, ultimately, if they choose, they may become Americans." Reagan also said, "The employer sanctions program is the keystone and major element. It will remove the incentive for illegal immigration by eliminating the job opportunities which draw illegal aliens here."
Iran–Contra affair.
In 1986, a scandal shook the administration stemming from the use of proceeds from covert arms sales to Iran to fund the Contras in Nicaragua, which had been specifically outlawed by an act of Congress. The Iran–Contra affair became the largest political scandal in the United States during the 1980s. The International Court of Justice, whose jurisdiction to decide the case was disputed by the United States, ruled that the United States had violated international law and breached treaties in Nicaragua in various ways (see "Nicaragua v. United States").
President Reagan professed ignorance of the plot's existence. He appointed two Republicans and one Democrat (John Tower, Brent Scowcroft and Edmund Muskie, known as the "Tower Commission") to investigate the scandal. The commission could not find direct evidence that Reagan had prior knowledge of the program, but criticized him heavily for his disengagement from managing his staff, making the diversion of funds possible. A separate report by Congress concluded that "If the president did not know what his national security advisers were doing, he should have". Reagan's popularity declined from 67% to 46% in less than a week, the greatest and quickest decline ever for a president. The scandal resulted in fourteen indictments within Reagan's staff, and eleven convictions.
Many Central Americans criticize Reagan for his support of the Contras, calling him an anti-communist zealot, blinded to human rights abuses, while others say he "saved Central America". Daniel Ortega, Sandinistan and president of Nicaragua, said that he hoped God would forgive Reagan for his "dirty war against Nicaragua".
End of the Cold War.
Until the early 1980s, the United States had relied on the qualitative superiority of its weapons to essentially frighten the Soviets, but the gap had been narrowed. Although the Soviet Union did not accelerate military spending after President Reagan's military buildup, their large military expenses, in combination with collectivized agriculture and inefficient planned manufacturing, were a heavy burden for the Soviet economy. At the same time, Saudi Arabia increased oil production, which resulted in a drop of oil prices in 1985 to one-third of the previous level; oil was the main source of Soviet export revenues. These factors contributed to a stagnant Soviet economy during Gorbachev's tenure.
Reagan recognized the change in the direction of the Soviet leadership with Mikhail Gorbachev, and shifted to diplomacy, with a view to encourage the Soviet leader to pursue substantial arms agreements. Reagan's personal mission was to achieve "a world free of nuclear weapons", which he regarded as "totally irrational, totally inhumane, good for nothing but killing, possibly destructive of life on earth and civilization". He was able to start discussions on nuclear disarmament with General Secretary Gorbachev. Gorbachev and Reagan held four summit conferences between 1985 and 1988: the first in Geneva, Switzerland, the second in Reykjavík, Iceland, the third in Washington, D.C., and the fourth in Moscow. Reagan believed that if he could persuade the Soviets to allow for more democracy and free speech, this would lead to reform and the end of Communism.
Speaking at the Berlin Wall on June 12, 1987, Reagan challenged Gorbachev to go further, saying:
Before Gorbachev's visit to Washington, D.C., for the third summit in 1987, the Soviet leader announced his intention to pursue significant arms agreements. The timing of the announcement led Western diplomats to contend that Gorbachev was offering major concessions to the United States on the levels of conventional forces, nuclear weapons, and policy in Eastern Europe. He and Reagan signed the Intermediate-Range Nuclear Forces (INF) Treaty at the White House, which eliminated an entire class of nuclear weapons. The two leaders laid the framework for the Strategic Arms Reduction Treaty, or START I; Reagan insisted that the name of the treaty be changed from Strategic Arms Limitation Talks to Strategic Arms Reduction Talks.
When Reagan visited Moscow for the fourth summit in 1988, he was viewed as a celebrity by the Soviets. A journalist asked the president if he still considered the Soviet Union the evil empire. "No", he replied, "I was talking about another time, another era". At Gorbachev's request, Reagan gave a speech on free markets at the Moscow State University. In his autobiography, "An American Life", Reagan expressed his optimism about the new direction that they charted and his warm feelings for Gorbachev. In November 1989, ten months after Reagan left office, the Berlin Wall was torn down, the Cold War was officially declared over at the Malta Summit on December 3, 1989, and two years later, the Soviet Union collapsed.
Health.
Early in his presidency, Reagan started wearing a custom, technologically advanced hearing aid, first in his right ear and later in his left as well. His decision to go public in 1983 regarding his wearing the small, audio-amplifying device boosted their sales.
On July 13, 1985, Reagan underwent surgery at Bethesda Naval Hospital to remove cancerous polyps from his colon. He relinquished presidential power to the Vice President for eight hours in a similar procedure as outlined in the 25th Amendment, which he specifically avoided invoking. The surgery lasted just under three hours and was successful. Reagan resumed the powers of the presidency later that day. In August of that year, he underwent an operation to remove skin cancer cells from his nose. In October, additional skin cancer cells were detected on his nose and removed.
In January 1987, Reagan underwent surgery for an enlarged prostate which caused further worries about his health. No cancerous growths were found, however, and he was not sedated during the operation. In July of that year, aged 76, he underwent a third skin cancer operation on his nose.
Judiciary.
During his 1980 campaign, Reagan pledged that, if given the opportunity, he would appoint the first female Supreme Court Justice. That opportunity came in his first year in office when he nominated Sandra Day O'Connor to fill the vacancy created by the retirement of Justice Potter Stewart. In his second term, Reagan elevated William Rehnquist to succeed Warren Burger as Chief Justice, and named Antonin Scalia to fill the vacant seat. Reagan nominated conservative jurist Robert Bork to the high court in 1987. Senator Ted Kennedy, a Democrat of Massachusetts, strongly condemned Bork, and great controversy ensued. Bork's nomination was rejected 58–42. Reagan then nominated Douglas Ginsburg, but Ginsburg withdrew his name from consideration after coming under fire for his cannabis use. Anthony Kennedy was eventually confirmed in his place. Along with his three Supreme Court appointments, Reagan appointed 83 judges to the United States Courts of Appeals, and 290 judges to the United States district courts.
Reagan also nominated Vaughn R. Walker, who would later be revealed to be the earliest known gay federal judge, to the United States District Court for the Central District of California. However, the nomination stalled in the Senate, and Walker was not confirmed until he was renominated by Reagan's successor, George H. W. Bush.
Early in his tenure, Reagan appointed Clarence M. Pendleton Jr., of San Diego as the first African American to chair the United States Commission on Civil Rights. Pendleton tried to steer the commission into a conservative direction in line with Reagan's views on social and civil rights policy during his time as tenure from 1981 until his sudden death in 1988. Pendleton soon aroused the ire of many civil rights advocates and feminists when he ridiculed the comparable worth proposal as being "Looney Tunes".
In 1984, Reagan commuted the 18-year sentence of former Louisiana Commissioner of Agriculture and Forestry Gil Dozier, a Democrat from Baton Rouge, to the time served for violations of both the Hobbs and the Racketeer Influenced and Corrupt Organizations acts. On September 23, 1980, the United States District Court for the Middle District of Louisiana convicted Dozier of extortion and racketeering when he pushed companies doing business with his department to make campaign contributions on his behalf. Reagan determined that the 18-year sentence was excessive compared to what other political figures in similar circumstances had been receiving.
Post-presidential years, 1989–2004.
After leaving office in 1989, the Reagans purchased a home in Bel Air, Los Angeles in addition to the Reagan Ranch in Santa Barbara. They regularly attended Bel Air Presbyterian Church and occasionally made appearances on behalf of the Republican Party; Reagan delivered a well-received speech at the 1992 Republican National Convention. Previously on November 4, 1991, the Ronald Reagan Presidential Library was dedicated and opened to the public. At the dedication ceremonies, five presidents were in attendance, as well as six first ladies, marking the first time that five presidents were gathered in the same location. Reagan continued publicly to speak in favor of a line-item veto; the Brady Bill; a constitutional amendment requiring a balanced budget; and the repeal of the 22nd Amendment, which prohibits anyone from serving more than two terms as president. In 1992 Reagan established the Ronald Reagan Freedom Award with the newly formed Ronald Reagan Presidential Foundation. His final public speech was on February 3, 1994, during a tribute to him in Washington, D.C., and his last major public appearance was at the funeral of Richard Nixon on April 27, 1994.
Alzheimer's disease.
Announcement and reaction.
In August 1994, at the age of 83, Reagan was diagnosed with Alzheimer's disease, an incurable neurological disorder which destroys brain cells and ultimately causes death. In November, he informed the nation through a handwritten letter, writing in part:
I have recently been told that I am one of the millions of Americans who will be afflicted with Alzheimer's Disease... At the moment I feel just fine. I intend to live the remainder of the years God gives me on this earth doing the things I have always done... I now begin the journey that will lead me into the sunset of my life. I know that for America there will always be a bright dawn ahead. Thank you, my friends. May God always bless you.
After his diagnosis, letters of support from well-wishers poured into his California home.
But there was also speculation over how long Reagan had demonstrated symptoms of mental degeneration. Former CBS White House correspondent Lesley Stahl recounted that, in her final meeting with the president in 1986, Reagan did not seem to know who Stahl was, and that she came close to reporting that Reagan was senile, but at the end of the meeting, Reagan had regained his alertness. However, Dr. Lawrence K. Altman, a physician employed as a reporter for the "New York Times", noted that "the line between mere forgetfulness and the beginning of Alzheimer's can be fuzzy", and all four of Reagan's White House doctors said that they saw no evidence of Alzheimer's while he was president. Dr. John E. Hutton, Reagan's primary physician from 1984 to 1989, said the president "absolutely" did not "show any signs of dementia or Alzheimer's". Reagan did experience occasional memory lapses, though, especially with names. Once, while meeting with Japanese Prime Minister Yasuhiro Nakasone, he repeatedly referred to Vice President Bush as "Prime Minister Bush". Reagan's doctors, however, note that he only began exhibiting overt symptoms of the illness in late 1992<ref name="NYT_2004/06/15"></ref> or 1993, several years after he had left office. His former Chief of Staff James Baker considered "ludicrous" the idea that Reagan slept during cabinet meetings. Other staff members, former aides, and friends said they saw no indication of Alzheimer's while he was President.
Complicating the picture, Reagan suffered an episode of head trauma in July 1989, five years before his diagnosis. After being thrown from a horse in Mexico, a subdural hematoma was found and surgically treated later in the year. Nancy Reagan, citing what doctors told her, asserts that her husband's 1989 fall hastened the onset of Alzheimer's disease, although acute brain injury has not been conclusively proven to accelerate Alzheimer's or dementia. Reagan's one-time physician Daniel Ruge has said it is possible, but not certain, that the horse accident affected the course of Reagan's memory.
Progression.
As the years went on, the disease slowly destroyed Reagan's mental capacity. He was only able to recognize a few people, including his wife, Nancy. He remained active, however; he took walks through parks near his home and on beaches, played golf regularly, and until 1999 he often went to his office in nearby Century City.
Reagan suffered a fall at his Bel Air home on January 13, 2001, resulting in a broken hip. The fracture was repaired the following day and the 89-year-old Reagan returned home later that week, although he faced difficult physical therapy at home. On February 6, 2001, Reagan reached the age of 90, becoming the third former president to do so (the other two being John Adams and Herbert Hoover, with Gerald Ford, George H.W. Bush and Jimmy Carter later reaching 90). Reagan's public appearances became much less frequent with the progression of the disease, and as a result, his family decided that he would live in quiet semi-isolation with his wife Nancy. Nancy Reagan told CNN's Larry King in 2001 that very few visitors were allowed to see her husband because she felt that "Ronnie would want people to remember him as he was". After her husband's diagnosis and death, Nancy Reagan became a stem-cell research advocate, urging Congress and President George W. Bush to support federal funding for embryonic stem-cell research, something Bush opposed. In 2009, she praised President Barack Obama for lifting restrictions on such research. Mrs. Reagan has said that she believes that it could lead to a cure for Alzheimer's.
Death.
Reagan died of pneumonia, complicated by Alzheimer's disease at his home in Bel Air, California, on the afternoon of June 5, 2004. A short time after his death, Nancy Reagan released a statement saying, "My family and I would like the world to know that President Ronald Reagan has died after 10 years of Alzheimer's disease at 93 years of age. We appreciate everyone's prayers." President George W. Bush declared June 11 a National Day of Mourning, and came in from around the world. Reagan's body was taken to the Kingsley and Gates Funeral Home in Santa Monica, California later in the day, where well-wishers paid tribute by laying flowers and American flags in the grass. On June 7, his body was removed and taken to the Ronald Reagan Presidential Library, where a brief family funeral was held conducted by Pastor Michael Wenning. His body lay in repose in the Library lobby until June 9; over 100,000 people viewed the coffin.
On June 9, Reagan's body was flown to Washington, D.C. where he became the tenth United States president to lie in state; in thirty-four hours, 104,684 people filed past the coffin.
On June 11, a state funeral was conducted in the Washington National Cathedral, and presided over by President George W. Bush. Eulogies were given by former British Prime Minister Margaret Thatcher, former Canadian Prime Minister Brian Mulroney, and both President George H.W. Bush and President George W. Bush. Also in attendance were Mikhail Gorbachev, and many world leaders, including British Prime Minister Tony Blair, Prince Charles, representing his mother Queen Elizabeth II, German Chancellor Gerhard Schröder, Italian Prime Minister Silvio Berlusconi, and interim presidents Hamid Karzai of Afghanistan, and Ghazi al-Yawer of Iraq.
After the funeral, the Reagan entourage was flown back to the Ronald W. Reagan Presidential Library in Simi Valley, California, where another service was held, and President Reagan was interred. At the time of his death, Reagan was the longest-lived president in U.S. history, having lived 93 years and 120 days (2 years, 8 months, and 23 days longer than John Adams, whose record he surpassed). He is now the second longest-lived president, just 45 days fewer than Gerald Ford. He was the first United States president to die in the 21st century, and his was the first state funeral in the United States since that of President Lyndon B. Johnson in 1973.
His burial site is inscribed with the words he delivered at the opening of the Ronald Reagan Presidential Library: "I know in my heart that man is good, that what is right will always eventually triumph and that there is purpose and worth to each and every life."
Legacy.
Since Reagan left office in 1989, substantial debate has occurred among scholars, historians, and the general public surrounding his legacy. Supporters have pointed to a more efficient and prosperous economy as a result of Reagan's economic policies, foreign policy triumphs including a peaceful end to the Cold War, and a restoration of American pride and morale. Proponents also argue Reagan restored faith in the American Dream with his unabated and passionate love for the United States, after a decline in American confidence and self-respect under Jimmy Carter's perceived weak leadership, particularly during the Iranian hostage crisis, as well as his gloomy, dreary outlook for the future of the United States during the 1980 election. Critics contend that Reagan's economic policies resulted in rising budget deficits, a wider gap in wealth, and an increase in homelessness and that the Iran-Contra affair lowered American credibility.
Opinions of Reagan's legacy among the country's leading policy makers and journalists differ as well. Edwin Feulner, President of The Heritage Foundation, said that Reagan "helped create a safer, freer world" and said of his economic policies: "He took an America suffering from 'malaise'... and made its citizens believe again in their destiny." However, Mark Weisbrot, co-Director of the Center for Economic and Policy Research, contended that Reagan's "economic policies were mostly a failure" while Howard Kurtz of "The Washington Post" opined that Reagan was "a far more controversial figure in his time than the largely gushing obits on television would suggest."
Despite the continuing debate surrounding his legacy, many conservative and liberal scholars agree that Reagan has been the most influential president since Franklin D. Roosevelt, leaving his imprint on American politics, diplomacy, culture, and economics through his effective communication, dedicated patriotism and pragmatic compromising. Since he left office, historians have reached a consensus, as summarized by British historian M. J. Heale, who finds that scholars now concur that Reagan rehabilitated conservatism, turned the nation to the right, practiced a considerably pragmatic conservatism that balanced ideology and the constraints of politics, revived faith in the presidency and in American exceptionalism, and contributed to victory in the Cold War.
Cold War.
The Cold War was a major political, economic and military endeavor for over four decades, but the confrontation between the two superpowers had decreased dramatically by the end of Reagan's presidency. The significance of Reagan's role in ending the Cold War has spurred contentious and opinionated debate. That Reagan played a role in contributing to the downfall of the Soviet Union is agreed, but the extent of this role is continuously debated, with many believing that Reagan's defense policies, economic policies, military policies and hard line rhetoric against the Soviet Union and Communism, as well as summits with General Secretary Gorbachev played a significant part in ending the Cold War.
He was first among post–World War II presidents to put into practice the concept that the Soviet Union could be defeated rather than simply negotiated with, a post-Détente strategy, a conviction that was vindicated by Gennadi Gerasimov, the Foreign Ministry spokesman under Gorbachev, who said that the Strategic Defense Initiative was "very successful blackmail. ... The Soviet economy couldn't endure such competition." Reagan's aggressive rhetoric toward the USSR had mixed effects; Jeffery W. Knopf observes that being labeled "evil" probably made no difference to the Soviets but gave encouragement to the East-European citizens opposed to communism.
General Secretary Gorbachev said of his former rival's Cold War role: "[He was] a man who was instrumental in bringing about the end of the Cold War", and deemed him "a great President". Gorbachev does not acknowledge a win or loss in the war, but rather a peaceful end; he said he was not intimidated by Reagan's harsh rhetoric. Margaret Thatcher, former Prime Minister of the United Kingdom, said of Reagan, "he warned that the Soviet Union had an insatiable drive for military power... but he also sensed it was being eaten away by systemic failures impossible to reform." She later said, "Ronald Reagan had a higher claim than any other leader to have won the Cold War for liberty and he did it without a shot being fired." Said Brian Mulroney, former Prime Minister of Canada: "He enters history as a strong and dramatic player [in the Cold War]." Former President Lech Wałęsa of Poland acknowledged, "Reagan was one of the world leaders who made a major contribution to communism's collapse." That Reagan had little or no effect in ending the Cold War is argued with equal weight; that Communism's internal weakness had become apparent, and the Soviet Union would have collapsed in the end regardless of who was in power. President Harry Truman's policy of containment is also regarded as a force behind the fall of the U.S.S.R., and the Soviet invasion of Afghanistan undermined the Soviet system itself.
Domestic and political legacy.
Ronald Reagan reshaped the Republican party, led the modern conservative movement, and altered the political dynamic of the United States. More men voted Republican under Reagan, and Reagan tapped into religious voters. The so-called "Reagan Democrats" were a result of his presidency.
After leaving office, Reagan became an iconic influence within the Republican party. His policies and beliefs have been frequently invoked by Republican presidential candidates since 1989. The 2008 Republican presidential candidates were no exception, for they aimed to liken themselves to him during the primary debates, even imitating his campaign strategies. Republican nominee John McCain frequently said that he came to office as "a foot soldier in the Reagan Revolution". Reagan's most famous statement regarding the role of smaller government was that "Government is not a solution to our problem, government is the problem."
Cultural and political image.
According to columnist Chuck Raasch, "Reagan transformed the American presidency in ways that only a few have been able to". He redefined the political agenda of the times, advocating lower taxes, a conservative economic philosophy, and a stronger military. His role in the Cold War further enhanced his image as a different kind of leader. Reagan's "avuncular style, optimism, and plain-folks demeanor" also helped him turn "government-bashing into an art form".
As a sitting president, Reagan did not have the highest approval ratings, but his popularity has increased since 1989. Gallup polls in 2001 and 2007 ranked him number one or number two when correspondents were asked for the greatest president in history. Reagan ranked third of post–World War II presidents in a 2007 Rasmussen Reports poll, fifth in an ABC 2000 poll, ninth in another 2007 Rasmussen poll, and eighth in a late 2008 poll by United Kingdom newspaper "The Times". In a Siena College survey of over 200 historians, however, Reagan ranked sixteenth out of 42. While the debate about Reagan's legacy is ongoing, the 2009 Annual "C-SPAN Survey of Presidential Leaders" ranked Reagan the 10th greatest president. The survey of leading historians rated Reagan number 11 in 2000.
In 2011, the Institute for the Study of the Americas released the first ever UK academic survey to rate U.S. presidents. This poll of UK specialists in U.S. history and politics placed Reagan as the 8th greatest U.S. president.
Reagan's ability to connect with Americans earned him the laudatory moniker "The Great Communicator". Of it, Reagan said, "I won the nickname the great communicator. But I never thought it was my style that made a difference—it was the content. I wasn't a great communicator, but I communicated great things." His age and soft-spoken speech gave him a warm grandfatherly image.
Reagan also earned the nickname "the Teflon President", in that public perceptions of him were not tarnished by the controversies that arose during his administration. According to Congresswoman Patricia Schroeder, who coined the phrase, and reporter Howard Kurtz, the epithet referred to Reagan's ability to "do almost anything wrong and not get blamed for it".
Public reaction to Reagan was always mixed; the oldest president was supported by young voters, and began an alliance that shifted many of them to the Republican party. Reagan did not fare well with minority groups, especially African-Americans. This was largely due to his opposition to affirmative action policies. However, his support of Israel throughout his presidency earned him support from many Jews. He emphasized family values in his campaigns and during his presidency, although he was the first president to have been divorced. The combination of Reagan's speaking style, unabashed patriotism, negotiation skills, as well as his savvy use of the media, played an important role in defining the 1980s and his future legacy.
Reagan was known to joke frequently during his lifetime, displayed humor throughout his presidency, and was famous for his storytelling. His numerous jokes and one-liners have been labeled "classic quips" and "legendary". Among the most notable of his jokes was one regarding the Cold War. As a microphone test in preparation for his weekly radio address in August 1984, Reagan made the following joke: "My fellow Americans, I'm pleased to tell you today that I've signed legislation that will outlaw Russia forever. We begin bombing in five minutes." Former aide David Gergen commented, "It was that humor... that I think endeared people to Reagan."
Honors.
Reagan received a number of awards in his pre- and post-presidential years. After his election as president, Reagan received a lifetime gold membership in the Screen Actors Guild, was inducted into the National Speakers Association Speaker Hall of Fame and received the United States Military Academy's Sylvanus Thayer Award.
In 1989, Reagan was made an Honorary Knight Grand Cross of the Order of the Bath, one of the highest British orders (this entitled him to the use of the post-nominal letters "GCB" but, as a foreign national, not to be known as "Sir Ronald Reagan"); only two American presidents have received this honor, Reagan and George H.W. Bush. Reagan was also named an honorary Fellow of Keble College, Oxford. Japan awarded him the Grand Cordon of the Order of the Chrysanthemum in 1989; he was the second American president to receive the order and the first to have it given to him for personal reasons (Dwight D. Eisenhower received it as a commemoration of U.S.-Japanese relations).
On January 18, 1993, Reagan's former Vice-President and sitting President George H. W. Bush awarded him the Presidential Medal of Freedom (awarded with distinction), the highest honor that the United States can bestow. Reagan was also awarded the Republican Senatorial Medal of Freedom, the highest honor bestowed by Republican members of the Senate.
On Reagan's 87th birthday, in 1998, Washington National Airport was renamed Ronald Reagan Washington National Airport by a bill signed into law by President Bill Clinton. That year, the Ronald Reagan Building and International Trade Center was dedicated in Washington, D.C. He was among 18 included in Gallup's List of Widely Admired People of the 20th century, from a poll conducted in the U.S. in 1999; two years later, USS "Ronald Reagan" was christened by Nancy Reagan and the United States Navy. It is one of few Navy ships christened in honor of a living person and the first aircraft carrier to be named in honor of a living former president.
In 1998 the U.S. Navy Memorial Foundation awarded Reagan its Naval Heritage award for his support of the U S Navy and military in both his film career and while he served as President.
Congress authorized the creation of the Ronald Reagan Boyhood Home National Historic Site in Dixon, Illinois in 2002, pending federal purchase of the property. On May 16 of that year, Nancy Reagan accepted the Congressional Gold Medal, the highest civilian honor bestowed by Congress, on behalf of the president and herself.
After Reagan's death, the United States Postal Service issued a President Ronald Reagan commemorative postage stamp in 2005. Later in the year, CNN, along with the editors of "Time" magazine, named him the "most fascinating person" of the network's first 25 years; "Time" listed Reagan one of the 100 Most Important People of the 20th century as well. The Discovery Channel asked its viewers to vote for The Greatest American in June 2005; Reagan placed in first place, ahead of Lincoln and Martin Luther King Jr.
In 2006, Reagan was inducted into the California Hall of Fame, located at The California Museum for History, Women, and the Arts. Every year since 2002, California Governors Gray Davis and Arnold Schwarzenegger have proclaimed February 6 "Ronald Reagan Day" in the state of California in honor of their most famous predecessor. In 2010, Schwarzenegger signed Senate Bill 944, authored by Senator George Runner, to make every February 6 Ronald Reagan Day in California.
In 2007, Polish President Lech Kaczyński posthumously conferred on Reagan the highest Polish distinction, the Order of the White Eagle, saying that Reagan had inspired the Polish people to work for change and helped to unseat the repressive communist regime; Kaczyński said it "would not have been possible if it was not for the tough-mindedness, determination, and feeling of mission of President Ronald Reagan". Reagan backed the nation of Poland throughout his presidency, supporting the anti-communist Solidarity movement, along with Pope John Paul II; the Ronald Reagan Park, a public facility in Gdańsk, was named in his honor.
On June 3, 2009, Nancy Reagan unveiled a statue of her late husband in the United States Capitol rotunda. The statue represents the state of California in the National Statuary Hall Collection. After Reagan's death, both major American political parties agreed to erect a statue of Reagan in the place of that of Thomas Starr King. The day before, President Obama signed the Ronald Reagan Centennial Commission Act into law, establishing a commission to plan activities to mark the upcoming centenary of Reagan's birth.
Independence Day 2011 saw the unveiling of another statue to Reagan this time in the British capital of London, outside the American Embassy, Grosvenor Square. The unveiling was supposed to be attended by Reagan's wife Nancy, but she did not attend; former Secretary of State Condoleezza Rice took her place and read a statement on her behalf; further to the former First Lady's absence President Reagan's friend and the British Prime Minister during Reagan's presidency Baroness Thatcher was also unable to attend due to frail health.
References.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Ronald Reagan" article dated 2012-12-17, and does not reflect subsequent edits to the article. ()
More spoken articles
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="25440" url="http://en.wikipedia.org/wiki?curid=25440" title="Robert J. Flaherty">
Robert J. Flaherty

Robert Joseph Flaherty, FRGS (; February 16, 1884 – July 23, 1951) was an American filmmaker who directed and produced the first commercially successful feature length documentary film, "Nanook of the North" (1922). The film made his reputation and nothing in his later life fully equaled its success, although he continued the development of this new genre of docufiction, e.g. with "Moana" (1926), set in the South Seas, and "Man of Aran" (1934), filmed in Ireland's Aran Islands.
He is considered the "Father" of both documentary and ethnographic film.
Flaherty was married to writer Frances H. Flaherty from 1914 until his death in 1951. Frances worked on several of her husband's films, and received an Academy Award nomination for Best Original Story for "Louisiana Story" (1948).
Early life.
Flaherty was one of seven children born to prospector Robert Henry Flaherty (an Irish Protestant) and Susan Klockner (a German Roman Catholic); he was sent to Upper Canada College in Toronto for his education. Flaherty began his career as a prospector in the Hudson Bay region of Canada, working for a railroad company.
Flaherty wrote for the "Geographical Review". In 1909 he shared stories about information he was told by an Inuit man, named Wetallok. Flaherty said he met Wetalltok while visiting the Hudson Bay. Flaherty was visiting the area in search of iron ore. In his story, Flaherty published a detailed map of the Inuit region and shared information about the bay that Wetallok had told him. His writing about Wetallok would go on to be published in Flaherty's book, "My Eskimo Friends: “Nanook of the North"."
"Nanook of the North".
In 1913, on his expedition to prospect the Belcher Islands, his boss, Sir William Mackenzie, suggested that he take a motion picture camera along. Flaherty brought with him a Bell and Howell hand cranked motion picture camera. He was particularly intrigued by the life of the Inuit people, and spent so much time filming them that he had begun to neglect his real work. When Flaherty returned to Toronto with 30,000 feet of film, the nitrate film stock was ignited in a fire started from his cigarette, in his editing room. His film was destroyed and he received burns on his hands. Although his editing print was saved and shown several times, Flaherty wasn't satisfied with the results. "It was utterly inept, simply a scene of this or that, no relation, no thread of story or continuity whatever, and it must have bored the audience to distraction. Certainly it bored me."
Flaherty was determined to make a new film, one following a life of a typical Inuit and his family. In 1920, Flaherty secured funds from Revillon Frères, a French fur trade company to shoot what was to become "Nanook of the North". On 15 August 1920, Flaherty arrived in Port Harrison, Quebec to shoot his film. He brought with him two Akeley motion-picture cameras which the Inuit referred to as "the aggie". Flaherty also brought full developing, printing, and projection equipment to show the Inuit his film, while he was still in the process of filming. Flaherty lived in an attached cabin to the Revillon Frères trading post.
In making "Nanook", Flaherty cast various locals in parts in the film, in the way that one would cast actors in a work of fiction. With the aim of showing traditional Inuit life, Flaherty also staged some scenes, including the ending, where Allakariallak (who acts the part of Nanook) and his screen family were supposedly at risk of dying if they could not find or build shelter quickly enough. The half-igloo had been built beforehand, with a side cut away for light so that Flaherty's camera could get a good shot. Additionally, Flaherty insisted that the Inuit not use rifles to hunt, though their use had by that time become common. He also pretended at one point that he could not hear the hunters' pleas for help, instead continuing to film their struggle and putting them in greater danger.
Melanie McGrath, a writer, writes that, while living in Northern Quebec for the year of filming "Nanook", Flaherty had an affair with his lead actress, the young Inuit woman who played Nanook's wife. A few months after he left, she gave birth to his son, Josephie (December 25, 1921 – 1984), whom he never acknowledged. Josephie was one of the Inuit who were relocated in the 1950s to very difficult living conditions in Resolute and Grise Fiord, in the extreme North (see High Arctic relocation). According to McGrath, Flaherty knew of his son's difficulties, but took no action. Corroboration of McGrath's account is not readily available and Flaherty himself never discussed the matter.
"Nanook" began a series of films that Flaherty was to make on the same theme; humanity against the elements. Others included "Moana: A Romance of the Golden Age" set in Samoa and "Man of Aran"set in the Aran Islands of Ireland. All these films employ the same rhetorical devices; the dangers of nature and the struggle of the communities to eke out an existence.
Hollywood.
"Nanook of the North" (1922) was a successful film, and Flaherty was in great demand afterwards. On a contract with Paramount to produce another film on the order of "Nanook", Flaherty went to Samoa to film "Moana" (1926). Flaherty shot "Moana" in Safune on the island of Savai'i where he lived with his wife and family for more than a year. The studio heads repeatedly asked for daily rushes but Flaherty had nothing to show because he had not filmed anything yet — his approach was to try to live with the community, becoming familiar with their way of life before writing a story about it to film. Flaherty was also concerned that there was no inherent conflict in the islanders' way of life, providing further incentive not to shoot anything. Eventually he decided to build the film around the ritual of a boy's entry to manhood. Flaherty was in Samoa from April 1923 until December 1924, with the film completed in December 1925 and released the following month. The film, on its release, was not as successful as "Nanook of the North" domestically, but it did very well in Europe, inspiring John Grierson to coin the word "documentary."
Before the release of "Moana", Flaherty made two short films in New York City with private backing, "The Pottery Maker" (1925) and "The Twenty-Four Dollar Island" (1927). Irving Thalberg of Metro-Goldwyn-Mayer invited Flaherty to film "White Shadows in the South Seas" (1928) in collaboration with W. S. Van Dyke, but their talents proved an uncomfortable fit, and Flaherty resigned from the production. Moving to Fox Film Corporation, Flaherty spent eight months working on the Native American documentary "Acoma the Sky City" (1929) but the production was shut down, and subsequently Flaherty's footage was lost in a studio vault fire. Flaherty then agreed to collaborate with F. W. Murnau on another South Seas picture, "Tabu" (1931). However, this combination proved even more volatile, and while Flaherty did contribute significantly to the story, the finished film, originally released by Paramount Pictures, is essentially Murnau's.
Britain.
After "Tabu", Flaherty was considered finished in Hollywood, and Frances Flaherty contacted John Grierson of the Empire Marketing Board Film Unit in London, who assigned Flaherty to the documentary "Industrial Britain" (1931). By comparison to Grierson and his unit, Flaherty's habitual working methods involved shooting relatively large amounts of film in relation to the planned length of the eventual finished movie, and the ensuing cost overruns obliged Grierson to take Flaherty off the project, which was edited by other hands into three shorter films.
Flaherty's career in Britain ended when producer Alexander Korda removed him from the production "Elephant Boy" (1937), re-editing it into a commercial entertainment picture.
Ireland.
Producer Michael Balcon took Flaherty on to direct "Man of Aran" (1934), which portrayed the harsh traditional lifestyle of the occupants of the isolated Aran Islands off the west coast of Ireland. "Man of Aran" was a major critical success, and for decades was considered in some circles an even greater achievement than "Nanook". As with "Nanook", "Man of Aran" showed human beings' efforts to survive under extreme conditions: in this case, an island whose soils were so thin that the inhabitants carried seaweed up from the sea to construct fields for cultivation. As with "Nanook", too, Flaherty cast locals in the various fictionalized roles, and made use of dramatic recreation of anachronistic behaviors: in this case, a sequence showing the hunting of sharks from small boats with harpoons, which the islanders had by then not practiced for several decades. He also staged the film's climactic sequence, in which three men in a small boat strive to row back to shore through perilously high, rock-infested seas.
Last years.
Back in the United States, Pare Lorentz of the United States Film Service hired Flaherty to film a documentary about US agriculture: a project which became "The Land". Flaherty and his wife covered some 100,000 miles, shooting 25,000 feet of film, and captured a series of striking images of rural America. Among the themes raised by Flaherty's footage were the challenge of the erosion of agricultural land and the Dust Bowl (as well as the beginning of effective responses via improved soil conservation practices), mechanization and rural unemployment, and large-scale migration from the Great Plains to California. In the latter context, Flaherty highlighted competition for agricultural jobs between native-born Americans and migrants from Mexico and the Philippines.
The film encountered a series of obstacles. After production had begun, Congress abolished the United States Film Service, and the project was shunted to the US Department of Agriculture (USDA). With US entry to World War 2 approaching, USDA officials (and the film's editor Helen van Dongen) attempted to reconcile Flaherty's footage with rapidly changing official messages (including a reversal of concern from pre-war rural unemployment to wartime labor shortages). Following the attack on Pearl Harbor, officials grew apprehensive that the film could project an unduly negative image of the US internationally, and although a prestige opening was held at the Museum of Modern Art in 1942, the film was never authorized for general release.
"Louisiana Story" (1948) was a Flaherty documentary shot by himself and Richard Leacock, this one about the installation of an oil rig in a Louisiana swamp. The film stresses the oil rig's peaceful and unproblematic coexistence with the surrounding environment, and was in fact funded by Standard Oil, a petroleum company. The main character of the film is a Cajun boy. The poetry of childhood and nature, some critics would argue, is used to make the exploration of oil look beautiful. Virgil Thomson composed the music for the film.
Flaherty was one of the directors of "" (1950), which won the Academy Award for Best Documentary Feature. The film was a re-edited version of the German/Swiss film originally titled "Michelangelo: Life of a Titan" (1938), directed by Curt Oertel. The re-edited version put a new English narration by Fredric March and musical score onto a shorter edit of the existing film. The new credits include Flaherty, Oertel, and Richard Lyford as directors and Ralph Alswang, Flaherty, and Robert Snyder as producers. The film was edited by Richard Lyford.
Legacy.
Flaherty is considered a pioneer of documentary film. He was one of the first to combine documentary subjects with a fiction-film-like narrative and poetic treatment.
Flaherty Island, one of the Belcher Islands in Hudson Bay, is named in his honor.
The Flaherty Seminar is an annual international forum for independent filmmakers and film-lovers, held in rural upstate New York. The festival was founded in Flaherty's honor by his widow in 1955.
Flaherty's contribution to the advent of the documentary is scrutinised in the 2010 British Universities Film & Video Council award-winning and FOCAL International award-nominated documentary '", written by Professor Brian Winston of University of Lincoln, UK, and directed by Mac Dara Ó Curraidhín. The film explores the nature of 'controlled actuality' and sheds new light on thinking about Flaherty. The argument is made that the impact of Flaherty's films on the indigenous peoples portrayed changes over time, as the films become valuable records for subsequent generations of now-lost ways of life. The film's title derives from Flaherty's own statement that he had been accused, in the staged climactic sequence of "Man of Aran", of "trying to drown a boatload of wild Irishmen".
Further reading.
</dl>

</doc>
<doc id="25441" url="http://en.wikipedia.org/wiki?curid=25441" title="Rolling Stone">
Rolling Stone

Rolling Stone is a fortnightly magazine that focuses on popular culture. It was founded in San Francisco in 1967 by Jann Wenner, who is still the magazine's editor-in-chief, and music critic Ralph J. Gleason. The magazine was first known for its musical coverage and for political reporting by Hunter S. Thompson. In the 1990s, the magazine changed its format to appeal to a younger readership interested in youth-oriented television shows, film actors, and popular music. In recent years, the magazine has resumed its traditional mix of content.
History.
"Rolling Stone Magazine" was founded in San Francisco in 1967 by Jann Wenner. To get the magazine off the ground, Wenner borrowed $7,500 from his own family and from the parents of his soon-to-be wife, Michell Palmer. The first issue carried a cover date of November 9, 1967 and was in newspaper format with a lead article on the Monterey Pop Festival. The cover price was 35¢ (equivalent to $<br>{Inflation} - Amount must not have "" prefix: 0.35.   today).
Wenner stated in the first issue that the title of the magazine referred to the 1950 blues song, "Rollin' Stone", recorded by Muddy Waters, the rock and roll band The Rolling Stones, and Bob Dylan's hit single "Like a Rolling Stone". "Rolling Stone" initially identified with and reported the hippie counterculture of the era. However, the magazine distanced itself from the underground newspapers of the time, such as "Berkeley Barb", embracing more traditional journalistic standards and avoiding the radical politics of the underground press. In the very first edition of the magazine, Wenner wrote that "Rolling Stone" "is not just about the music, but about the things and attitudes that music embraces."
In the 1970s, "Rolling Stone" began to make a mark with its political coverage, with the likes of gonzo journalist Hunter S. Thompson writing for the magazine's political section. Thompson first published his most famous work "Fear and Loathing in Las Vegas" within the pages of "Rolling Stone", where he remained a contributing editor until his death in 2005. In the 1970s, the magazine also helped launch the careers of many prominent authors, including Cameron Crowe, Lester Bangs, Joe Klein, Joe Eszterhas, Patti Smith and P. J. O'Rourke. It was at this point that the magazine ran some of its most famous stories, including that of the Patty Hearst abduction odyssey. One interviewer, speaking for a large number of his peers, said that he bought his first copy of the magazine upon initial arrival on his college campus, describing it as a "rite of passage".
In 1977, the magazine moved its headquarters from San Francisco to New York City. Editor Jann Wenner said San Francisco had become "a cultural backwater."
During the 1980s, the magazine began to shift towards being a general "entertainment" magazine. Music was still a dominant topic, but there was increasing coverage of celebrities in television, films and the pop culture of the day. The magazine also initiated its annual "Hot Issue" during this time.
"Rolling Stone" was initially known for its musical coverage and for Thompson's political reporting. In the 1990s, the magazine changed its format to appeal to a younger readership interested in youth-oriented television shows, film actors and popular music. This led to criticism that the magazine was emphasizing style over substance. In recent years, the magazine has resumed its traditional mix of content, including in-depth political stories. It has also expanded content to include coverage of financial and banking issues. As a result, the magazine has seen its circulation increase and its reporters invited as experts to network television programs of note.
The printed format has gone through several changes. The first publications, in 1967–72, were in folded tabloid newspaper format, with no staples, black ink text, and a single color highlight that changed each edition. From 1973 onwards, editions were produced on a four-color press with a different newsprint paper size. In 1979, the bar code appeared. In 1980, it became a gloss-paper, large format (10″×12″) magazine. As of edition of October 30, 2008, "Rolling Stone" has had a smaller, standard-format magazine size.
2000s.
After years of declining readership, the magazine experienced a major resurgence of interest and relevance with the work of two young journalists in the late 2000s, Michael Hastings and Matt Taibbi.
In 2005, Dana Leslie Fields, former publisher of "Rolling Stone", who had worked at the magazine for 17 years, was an inaugural inductee into the Magazine Hall of Fame.
In 2009, Taibbi unleashed an acclaimed series of scathing reports on the financial meltdown of the time. He famously described Goldman Sachs as "a great vampire squid."
Bigger headlines came at the end of June 2010. "Rolling Stone" caused a controversy in the White House by publishing in the July issue an article by journalist Michael Hastings entitled, "The Runaway General", quoting criticism of General Stanley A. McChrystal, commander of the International Security Assistance Force and U.S. Forces-Afghanistan commander, about Vice President Joe Biden and other Administration members of the White House. McChrystal resigned from his position shortly after his statements went public.
In 2010, Taibbi documented illegal and fraudulent actions by banks in the foreclosure courts, after traveling to Jacksonville, Florida and sitting in on hearings in the courtroom. His article, "Invasion of the Home Snatchers" also documented attempts by the judge to intimidate a homeowner fighting foreclosure and the attorney Taibbi accompanied into the court.
In January 2012, the magazine ran exclusive excerpts from Hastings' book just prior to publication. The book, "The Operators: The Wild and Terrifying Inside Story of America’s War in Afghanistan", provided a much more expansive look at McChrystal and the culture of senior American military and how they become embroiled in such wars. It has been described as a boozy, sexy account of the misadventures of America's "most notorious killers". The book reached Amazon's bestseller list in the first 48 hours of release, and it received generally favorable reviews. Salon's Glenn Greenwald described it as "superb," "brave" and "eye-opening."
In 2012, Taibbi, through his coverage of the LIBOR scandal, emerged as an expert on that topic, which lead to media appearances outside "Rolling Stone".
On November 9, 2012, the magazine published its first Spanish-language section on Latino music and culture, in the issue dated November 22.
Website.
"Rolling Stone"'s website features selected current articles, reviews, blogs, MP3s and other features, such as searchable and free encyclopedic articles about artists, with images and sometimes sound clips of their work. The articles and reviews are sometimes in a revised form of the published versions. The website also carries political and cultural articles and entries selected from the magazine's archives. 
The site at one time had an extensive message-board forum. By the late 1990s, this had developed into a thriving community, with a large number of regular members and contributors worldwide. However, the site was also plagued with numerous Internet trolls and malicious code-hackers, who vandalized the forum substantially. The magazine abruptly deleted the forum in May 2004, then began a new, much more limited message board community on their site in late 2005, only to remove it again in 2006. In March 2008, the website started a new message board section once again, then deleted it in April 2010. 
"Rolling Stone" devotes one of its table of contents pages to promoting material currently appearing on its website, listing detailed links to the items. The magazine also has a page at MySpace, Facebook and Twitter. 
On April 19, 2010, the website was updated drastically and now features the complete archives of "Rolling Stone". The archive was first launched under a for-pay model, but has since transitioned to a free-with-print-subscription model. In the spring of 2012, "Rolling Stone" launched a federated search feature which searches both the website and the archive.
Restaurant.
In December 2009, the "Los Angeles Times" reported that the owners of "Rolling Stone" magazine planned to open a "Rolling Stone" restaurant in the Hollywood & Highland Center in Hollywood in the spring of 2010. The expectation was that the restaurant could become the first of a national chain if it was successful. As of November 2010, the "soft opening" of the restaurant was planned for December 2010. In 2011, the restaurant was open for lunch and dinner as well as a full night club downstairs on the weekends. However, the restaurant closed around February 2013.
Criticism.
One major criticism of "Rolling Stone" involves its generational bias toward the 1960s and 1970s. One critic referred to the "Rolling Stone" list of the "99 Greatest Songs" as an example of "unrepentant rockist fogeyism". In further response to this issue, rock critic Jim DeRogatis, a former "Rolling Stone" editor, published a thorough critique of the magazine's lists in a book called "Kill Your Idols: A New Generation of Rock Writers Reconsiders the Classics" (ISBN 1-56980-276-9), which featured differing opinions from many younger critics.
"Rolling Stone" magazine has been criticized for reconsidering many classic albums that it had previously dismissed, and for frequent use of the 3.5 star rating. For example, Led Zeppelin was largely written off by "Rolling Stone" magazine critics during the band's most active years in the 1970s, but by 2006, a cover story on Led Zeppelin honored them as "the Heaviest Band of All Time". A critic for "Slate" magazine described a conference at which 1984's "The Rolling Stone Record Guide" was scrutinized. As he described it, "The guide virtually ignored hip-hop and ruthlessly panned heavy metal, the two genres that within a few years would dominate the pop charts. In an auditorium packed with music journalists, you could detect more than a few anxious titters: How many of us will want our record reviews read back to us 20 years hence?"
The hiring of former "FHM" editor Ed Needham further enraged critics who alleged that "Rolling Stone" had lost its credibility.
The 2003 "Rolling Stone's 100 Greatest Guitarists of all Time" article, which named only two female musicians, resulted in "Venus Zine" answering with their own list entitled, "The Greatest Female Guitarists of All Time".
Conservative columnist Jonah Goldberg stated that "Rolling Stone" had "essentially become the house organ of the Democratic National Committee." "Rolling Stone" editor Jann Wenner has made all of his political donations to Democrats.
"Rolling Stone"'s film critic, Peter Travers, has been criticized for his high number of repetitively used blurbs.
The August 2013 "Rolling Stone" cover, featuring convicted Boston Marathon bomber Dzhokhar Tsarnaev, drew widespread criticism that the magazine was "glamorizing terrorism" and that the cover was a "slap in the face to the great city of Boston." The online edition of the article was accompanied by a short editorial stating that the story "falls within the traditions of journalism and Rolling Stone's long-standing commitment to serious and thoughtful coverage of the most important political and cultural issues of our day". The controversial cover photograph that was used by "Rolling Stone" had previously featured on the front page of "The New York Times" on May 5, 2013.
In response to the outcry, New England-based CVS Pharmacy and Tedeschi Food Shops banned their stores from carrying the issue. Also refusing to sell the issue were Walgreens, Rite-Aid, Roche Bros., Kmart, H-E-B, Walmart, 7-Eleven, Hy-Vee, Rutter's Farm, United Supermarkets, Cumberland Farms, Market Basket, Shaw's and Stop & Shop. Boston mayor Thomas Menino sent a letter to "Rolling Stone" publisher Jann Wenner, calling the cover "ill conceived, at best ...[it] reaffirms a message that destruction gains fame for killers and their 'causes'." Menino also wrote, "To respond to you in anger is to feed into your obvious market strategy", and that Wenner could have written about the survivors or the people who came to help after the bombings. In conclusion he wrote, "The survivors of the Boston Marathon deserve "Rolling Stone" cover stories, though I no longer feel that "Rolling Stone "deserves them."
UVA rape story.
On December 5, 2014, "Rolling Stone"'s managing editor, Will Dana, apologized for a story that was run about an alleged gang rape on the campus of the University of Virginia. Separate inquiries by Phi Kappa Psi, the fraternity accused by "Rolling Stone" of facilitating the rape, and "The Washington Post" revealed major errors and discrepancies in the report. Reporter Sabrina Erdely's story was subject to intense media criticism. "The Washington Post" and "Boston Herald" issued calls for magazine staff involved in the report to be fired. "Rolling Stone" subsequently issued three apologies for the story. Some suggested that legal action against the magazine by persons accused of the rape might result.
"Rolling Stone" commissioned an outside investigation of the story and its problems by the Dean of the Columbia School of Journalism. The report uncovered complete journalistic failure in the UVA story and institutional problems with reporting at "Rolling Stone". "Rolling Stone" retracted the story on April 5, 2015. On April 6, 2015, following the investigation and retraction of the story, Phi Kappa Psi announced plans to pursue all available legal action against "Rolling Stone", including claims of defamation.
On 12 May 2015, UVA associate dean Nicole Eramo, chief administrator for handling sexual assault issues at the school, filed a $7.5 million defmation lawsuit in Charlottesville Circuit Court against "Rolling Stone" and Erdely, claiming damage to her reputation and emotional distress. Said the filing, ""Rolling Stone" and Erdely’s highly defamatory and false statements about Dean Eramo were not the result of an innocent mistake. They were the result of a wanton journalist who was more concerned with writing an article that fulfilled her preconceived narrative about the victimization of women on American college campuses, and a malicious publisher who was more concerned about selling magazines to boost the economic bottom line for its faltering magazine, than they were about discovering the truth or actual facts."
Covers.
Some artists have graced the cover many times, some of these pictures going on to become iconic. The Beatles, for example, have appeared on the cover more than thirty times, either individually or as a band. The first ten issues featured, in order of appearance, the following:

</doc>
<doc id="25445" url="http://en.wikipedia.org/wiki?curid=25445" title="Romania">
Romania

Romania ( ; Romanian: "România" ]), is a unitary semi-presidential republic located in Southeastern-Central Europe, bordering the Black Sea, between Bulgaria and Ukraine. It also borders Hungary, Serbia, and Moldova. It covers 238391 km2 and has a temperate-continental climate. With its 19.9 million inhabitants, it is the seventh most populous member of the European Union. Its capital and largest city, Bucharest, is the sixth largest city in the EU.
The Danube River, which is Europe's second longest river after the Volga, rises in Germany and flows southeastwards for a distance of 2857 km course through ten countries before emptying in Romania's Danube Delta. Some of its 1075 km length bordering the country drains the whole of it. The Carpathian Mountains (the tallest peak is Moldoveanu at 2544 m, 8346 ft) cross Romania from the north to the southwest.
Modern Romania emerged within the territories of the ancient Roman province of Dacia, and was formed in 1859 through a personal union of the principalities of Moldavia and Wallachia. The new state, officially named "Romania" since 1866, gained independence from the Ottoman Empire in 1877. At the end of World War I, Transylvania, Bukovina and Bessarabia united with the sovereign Kingdom of Romania. At the end of World War II, territories which today roughly correspond to the Republic of Moldova were occupied by the Soviet Union, and a few years later Romania became a socialist republic and member of the Warsaw Pact. After the 1989 Revolution, Romania began a transition back towards democracy and a capitalist market economy.
Following rapid economic growth in the 2000s, Romania has an economy predominantly based on services, and is a producer and net exporter of machines and electric energy, featuring companies like Automobile Dacia and OMV Petrom. Living standards have improved, and currently, Romania is an upper-middle income country with a high Human Development Index. It has been a member of NATO since 2004, and part of the European Union since 2007. Around 90% of the population identify themselves as Eastern Orthodox Christians, and are native speakers of Romanian, a Romance language. With a rich cultural history, Romania has been the home of influential artists, musicians, inventors and sportsmen, and features a variety of tourist attractions.
Etymology.
"Romania" derives from the Latin "romanus", meaning "citizen of Rome". The first known use of the appellation was attested in the 16th-century by Italian humanists travelling in Transylvania, Moldavia, and Wallachia.
The oldest surviving document written in Romanian, a 1521 letter known as the "Letter of Neacșu from Câmpulung", is also notable for including the first documented occurrence of the country's name: Wallachia is mentioned as "Țeara Rumânească" ("The Romanian Land", "țeara" from the Latin "terra", "land"; current spelling: "Țara Românească").
Two spelling forms: "român" and "rumân" were used interchangeably until sociolinguistic developments in the late 17th century led to semantic differentiation of the two forms: "rumân" came to mean "bondsman", while "român" retained the original ethnolinguistic meaning. After the abolition of serfdom in 1746, the word "rumân" gradually fell out of use and the spelling stabilised to the form "român". Tudor Vladimirescu, a revolutionary leader of the early 19th century, used the term "Rumânia" to refer exclusively to the principality of Wallachia."
The use of the name "Romania" to refer to the common homeland of all Romanians—its modern-day meaning—is first documented in the early 19th century. The name has been officially in use since 11 December 1861. English-language sources still used the terms "Rumania" or "Roumania", derived from the French spelling "Roumanie" and/or the Greek Ρουμανία, as recently as World War II, but the name has since been replaced with the official spelling "Romania". However, Time magazine used Rumania in an article as late as the April 2, 1973 edition.
History.
Early history.
The human remains found in Peștera cu Oase ("The Cave of the Bones"), radiocarbon dated as being from circa 40,000 years ago, represent the oldest known "Homo sapiens" in Europe. The Neolithic-Age Cucuteni area in northeastern Romania was the western region of the earliest European civilization, known as the Cucuteni-Trypillian culture. Also the earliest known salt works in the world is at Poiana Slatinei, near the village of Lunca in Romania; it was first used in the early Neolithic, around 6050 BC, by the Starčevo culture, and later by the Cucuteni-Trypillian culture in the Precucuteni period. Evidence from this and other sites indicates that the Cucuteni-Trypillian culture extracted salt from salt-laden spring water through the process of briquetage.
Prior to the Roman conquest of Dacia, the territories between Danube and Dniester rivers were inhabited by various Thracian peoples, including the Dacians and the Getae. Herodotus, in his work "Histories", notes the religious difference between the Getae and other Thracians, however, according to Strabo, the Dacians and the Getae spoke the same language. Dio Cassius draws attention to the cultural similarities between the two people. There is a scholarly dispute whether the Dacians and the Getae were the same people.
Roman incursions under Emperor Trajan between 101–102 AD and 105–106 AD led to result that about half of the Dacian kingdom became a province of the Roman Empire called "Dacia Felix". The Roman rule lasted 165 years. During this period the province was fully integrated to the Roman Empire and a sizeable part of the population was newcomers from other provinces. The Roman colonists introduced the Latin language. According to followers of the continuity theory, the intense Romanization gave birth to the Proto-Romanian language. The province was rich of ore deposits (especially gold and silver in places like Alburnus Maior). Roman troops pulled out of Dacia around 271 AD. The territory was later invaded by various migrating peoples.
Burebista, Decebalus and Trajan have often been considered as the Romanians' forefathers in Romanian historiography.
Middle Ages.
In the Middle Ages, Romanians lived in three Romanian principalities: Wallachia (Romanian: "Țara Românească" – "The Romanian Land"), Moldavia (Romanian: "Moldova") and in Transylvania. The existence of independent Romanian voivodeships in Transylvania as early as the 9th century is mentioned in "Gesta Hungarorum", but by the 11th century, Transylvania had become a largely autonomous part of the Kingdom of Hungary. In the other parts, many small local states with varying degrees of independence developed, but only under Basarab I and Bogdan I the larger principalities of Wallachia and Moldavia would emerge in the 14th century to fight the threat of the Ottoman Empire.
By 1541, as with the entire Balkan peninsula and most of Hungary, Moldavia, Wallachia, and Transylvania were under Ottoman suzerainty, preserving partial or full internal autonomy until the mid-19th century (Transylvania until 1711). This period featured several prominent rulers such as: Stephen the Great, Vasile Lupu, Alexander the Good and Dimitrie Cantemir in Moldavia; Vlad the Impaler, Mircea the Elder, Matei Basarab, Neagoe Basarab and Constantin Brâncoveanu in Wallachia; and Gabriel Bethlen in the Principality of Transylvania, and eventually John Hunyadi and Matthias Corvinus in Transylvania (then part of the Kingdom of Hungary). In 1600, the three principalities were ruled simultaneously by the Wallachian prince Michael the Brave ("Mihai Viteazul"), which was considered in later periods as the precursor of a modern Romania and became a point of reference for nationalists, as well as a catalyst for achieving a single Romanian state.
Independence and monarchy.
During the period of the Austro-Hungarian rule in Transylvania and of Ottoman suzerainty over Wallachia and Moldavia, most Romanians were given few rights in a territory where they formed the majority of the population. Nationalistic themes became principal during the Wallachian uprising of 1821, and the 1848 revolutions in Wallachia and Moldavia. The flag adopted for Wallachia by the revolutionaries was a blue-yellow-red horizontal tricolour (with blue above, in line with the meaning "Liberty, Justice, Fraternity"), while Romanian students in Paris hailed the new government with the same flag "as a symbol of union between Moldavians and Wallachians". The same flag, with the tricolour being mounted vertically, would later be officially adopted as the national flag of Romania.
After the failed 1848 revolutions not all the Great Powers supported the Romanians' expressed desire to officially unite in a single state. But in the aftermath of the Crimean War, the electors in both Moldavia and Wallachia voted in 1859 for the same leader, Alexandru Ioan Cuza, as "Domnitor" (prince in Romanian), and the two principalities became a personal union formally under the suzerainty of the Ottoman Empire. Following "coup d'état" in 1866, Cuza was exiled and replaced with Prince Carol I of Romania of the House of Hohenzollern-Sigmaringen. During the 1877–1878 Russo-Turkish War Romania fought on the Russian side, and in the aftermath, it was recognized as an independent state both by the Ottoman Empire and the Great Powers by the Treaty of San Stefano and the Treaty of Berlin. The new Kingdom of Romania underwent a period of stability and progress until 1914, and also acquired Southern Dobruja from Bulgaria after the Second Balkan War.
World Wars and Greater Romania.
Romania remained neutral for the first two years of World War I. Following the secret Treaty of Bucharest, according to which Romania would acquire territories with a majority of Romanian population from Austria-Hungary, it joined the Entente Powers and declared war on 27 August 1916. The Romanian military campaign began disastrously for Romania as the Central Powers occupied two-thirds of the country within months, before reaching a stalemate in 1917. Total military and civilian losses from 1916 to 1918, within contemporary borders, were estimated at 748,000. After the war, the transfer of Bukovina from Austria was acknowledged by the 1919 Treaty of Saint Germain, of Banat and Transylvania from Hungary by the 1920 Treaty of Trianon, and of Bessarabia from Russian rule by the 1920 Treaty of Paris.
The following interwar period is referred as Greater Romania, as the country achieved its greatest territorial extent at that time (almost 300000 km2). The application of radical agricultural reforms and the passing of a new constitution created a democratic framework and allowed for quick economic growth. With oil production of 7.2 million tons in 1937, Romania ranked second in Europe and seventh in the world. and was Europe's second-largest food producer. However, the early 1930s were marked by social unrest, high unemployment, and strikes, as there were over 25 separate governments throughout the decade. On several occasions in the last few years before World War II, the democratic parties were squeezed between conflicts with the fascist and chauvinistic Iron Guard and the authoritarian tendencies of king Carol II.
The Antonescu fascist regime played a major role in the The Holocaust in Romania, and copied the Nazi policies of oppression and genocide of Jews and Gypsies, mainly in the Eastern territories reoccupied by the Romanians from the Soviet Union. In total between 280,000 and 380,000 Jews in Romania (including Bessarabia, Bukovina and the Transnistria Governorate) were murdered during the war
and at least 11,000 Romanian Gypsies ("Roma") were also killed. Ion Antonescu was convicted of war crimes and executed in the end. 9 October is now the National Day of Commemorating the Holocaust in Romania.
During World War II, Romania tried again to remain neutral, but on 28 June 1940, it received a Soviet ultimatum with an implied threat of invasion in the event of non-compliance. Again foreign powers created heavy pressure on Romania, by means of the Soviet-Nazi Ribbentrop-Molotov Pact of non-aggression from 23 August 1939. As a result of it the Romanian government and the army were forced to retreat from Bessarabia as well as from northern Bukovina in order to avoid war with the Soviet Union. The king was compelled to abdicate and appointed general Ion Antonescu as the new Prime-Minister with full powers in ruling the state by royal decree. Romania was prompted to join the Axis military campaign. Thereafter, southern Dobruja was ceded to Bulgaria, while Hungary received Northern Transylvania as result of an Axis powers' arbitration. Romanian contribution to Operation Barbarossa was enormous, with the Romanian Army of over 1.2 million men in the summer of 1944, fighting in numbers second only to Nazi Germany.
Romania was the main source of oil for the Third Reich, and thus became the target of intense bombing by the Allies. Growing discontent among the population eventually peaked in August 1944 with King Michael's Coup, and the country switched sides to join the Allies. It is estimated that the coup shortened the war by as much as six months. Even though the Romanian Army had suffered 170,000 casualties after switching sides, Romania's role in the defeat of Nazi Germany was not recognized by the Paris Peace Conference of 1947, as the Soviet Union annexed Bessarabia and other territories corresponding roughly to present-day Republic of Moldova.
Communism.
During the Soviet occupation of Romania, the Communist-dominated government called for new elections in 1946, which were fraudulently won, with a fabricated 70% majority of the vote. Thus they rapidly established themselves as the dominant political force, and in 1947, forced King Michael I to abdicate and leave the country, and proclaimed Romania a people's republic. Romania remained under the direct military occupation and economic control of the USSR until the late 1950s. During this period, Romania's vast natural resources were continuously drained by mixed Soviet-Romanian companies (SovRoms) set up for unilateral exploitative purposes.
In 1948, the state began to nationalize private firms and to collectivize agriculture. Until the early 1960s, the government severely curtailed political liberties and vigorously suppressed any dissent with the help of the Securitate (the Romanian secret police). During this period the regime launched several campaigns of purges in which numerous "enemies of the state" and "parasite elements" were targeted for different forms of punishment, such as deportation, internal exile and internment in forced labour camps and prisons, sometimes for life, as well as extrajudicial killing. Nevertheless, anti-communist resistance was one of the most long-lasting in the Eastern Bloc. A 2006 Commission estimated the number of direct victims of the communist repression at two million people.
In 1965, Nicolae Ceaușescu came to power and started to conduct the foreign policy more independently from the Soviet Union. Thus, communist Romania was the only Warsaw Pact country who refused to participate at the Soviet-led 1968 invasion of Czechoslovakia (Ceaușescu even publicly condemned the action as "a big mistake, [and] a serious danger to peace in Europe and to the fate of communism in the world"); it was also the only communist state to maintain diplomatic relations with Israel after the 1967 Six-Day War; and established diplomatic relations with West Germany the same year. At the same time, close ties with the Arab countries (and the PLO) allowed Romania to play a key role in the Israel–Egypt and Israel–PLO peace talks. As Romania's foreign debt sharply increased between 1977 and 1981 (from US$3 billion to $10 billion), the influence of international financial organizations (such as the IMF and the World Bank) grew, gradually conflicting with Ceaușescu's autocratic rule. The latter eventually initiated a policy of total reimbursement of the foreign debt by imposing austerity steps that impoverished the population and exhausted the economy. At the same time, Ceaușescu greatly extended the authority of the Securitate secret police and imposed a severe cult of personality, which led to a dramatic decrease in the dictator's popularity and culminated in his overthrow and eventual execution, together with his wife, in the violent Romanian Revolution of December 1989.
Democracy.
After the revolution, the National Salvation Front (NSF), led by Ion Iliescu, took partial multi-party democratic and free market measures. In April 1990 a sit-in protest contesting the results of the elections and accusing the NSF, including Iliescu, of being made up of former Communists and members of the Securitate, rapidly grew to become what was called the Golaniad. The peaceful demonstrations degenerated into violence, prompting the intervention of coal miners summoned by Iliescu. This episode has been documented widely by both local and foreign media, and is remembered as the June 1990 Mineriad.
The subsequent disintegration of the Front produced several political parties including the Social Democratic Party, and the Democratic Party. The former governed Romania from 1990 until 1996 through several coalitions and governments with Ion Iliescu as head of state. Since then there have been several democratic changes of government: in 1996 Emil Constantinescu was elected president, in 2000 Iliescu returned to power, while Traian Băsescu was elected in 2004 and narrowly re-elected in 2009.
After the Cold War Romania developed closer ties with Western Europe and the United States, eventually joining NATO in 2004, and hosting the 2008 summit in Bucharest. The country applied in June 1993 for membership in the European Union and became an Associated State of the EU in 1995, an Acceding Country in 2004, and a full member on 1 January 2007. Following the "free travel agreement" with the EU and the economic instability throughout the 1990s, a large number of Romanians emigrated to North America and Western Europe, with particularly large communities in Italy and Spain. Currently, the Romanian diaspora is estimated at over two million people.
During the 2000s, Romania enjoyed one of the highest economic growth rates in Europe and has been referred at times as "the Tiger of Eastern Europe". This has been accompanied by a significant improvement in living standards as the country successfully reduced internal poverty and established a functional democratic state. However, Romania's development suffered a major setback during the late-2000s recession leading to a large gross domestic product contraction and budget deficit in 2009. This led to Romania borrowing from the International Monetary Fund. Worsening economic conditions led to unrest and triggered a political crisis in 2012. Romania still faces issues related to infrastructure, medical services, education, and corruption. Eventually, The Economist wrote at the end of 2013 that Romania is again booming with an economic growth at 4.1%, wages rising fast and lower unemployment than in Britain. The economy is accelerated by a government in the midst of liberalising, which is opening up new sectors (most notably, energy and telecoms) to competition and investment.
Geography.
With an area of 238391 km2, Romania is the largest country in Southeastern Europe and the twelfth-largest in Europe. It lies between latitudes 43° and 49° N, and longitudes 20° and 30° E. The terrain is distributed roughly equally between mountains, hills and plains. The Carpathian Mountains dominate the centre of Romania, with 14 mountain ranges reaching above 2000 m, and the highest point at Moldoveanu Peak (2544 m). They are surrounded by the Moldavian and Transylvanian plateaus and Carpathian Basin and Wallachian plains. The Danube river forms a large part of the border with Serbia and Bulgaria and flows into the Black Sea forming the Danube Delta, the second largest and best preserved delta in Europe, and also a biosphere reserve and a biodiversity World Heritage Site.
Owing to its distance from open sea and position on the Southeastern portion of the European continent, Romania has a climate that is temperate and continental, with four distinct seasons. The average annual temperature is 11 °C in the south and 8 °C in the north. In summer, average maximum temperatures in Bucharest rise to 28 °C, and temperatures over 35 °C are fairly common in the lower-lying areas of the country. In winter, the average maximum temperature is below 2 °C. Precipitation is average, with over 750 mm per year only on the highest western mountains, while around Bucharest it drops to around 600 mm.
A high percentage (47% of the land area) of the country is covered with natural and semi-natural ecosystems. Romania has one of the largest areas of undisturbed forest in Europe covering almost 27% of the territory. The fauna consists of 33,792 species of animals, 33,085 invertebrate and 707 vertebrate, with almost 400 unique species of mammals, birds, reptiles and amphibians, including about 50% of Europe's (excluding Russia) brown bears and 20% of its wolves. Some 3,700 plant species have been identified in the country, from which to date 23 have been declared natural monuments, 74 missing, 39 endangered, 171 vulnerable and 1,253 rare. There are almost 10000 km2 (about 5% of the total area) of protected areas in Romania covering 13 national parks and three biosphere reserves. The Danube Delta, at 5800 km2, is the largest continuous marshland in Europe, and supports 1,688 different plant species alone.
Governance.
The Constitution of Romania is based on the Constitution of France's Fifth Republic and was approved in a national referendum on 8 December 1991, and amended in October 2003 to bring it into conformity with the EU legislation. The country is governed on the basis of multi-party democratic system and of the segregation of the legislative, executive and judicial powers. It is a semi-presidential republic where executive functions are held by both government and the president. The latter is elected by popular vote for a maximum of two terms of five years and appoints the prime minister, who in turn appoints the Council of Ministers. The legislative branch of the government, collectively known as the Parliament (residing at the Palace of the Parliament), consists of two chambers (Senate and Chamber of Deputies) whose members are elected every four years by simple plurality.
The justice system is independent of the other branches of government, and is made up of a hierarchical system of courts culminating in the High Court of Cassation and Justice, which is the supreme court of Romania. There are also courts of appeal, county courts and local courts. The Romanian judicial system is strongly influenced by the French model, considering that it is based on civil law and is inquisitorial in nature. The Constitutional Court ("Curtea Constituțională") is responsible for judging the compliance of laws and other state regulations to the constitution, which is the fundamental law of the country and can only be amended through a public referendum. The 2007 entry into the EU has been a significant influence on its domestic policy, and including judicial reforms, increased judicial cooperation with other member states, and measures to combat corruption. Nevertheless, a 2013 report by Ernst & Young described Romania as corruption speaking about the EU countries, on par with Spain and Italy.
Foreign relations.
Since December 1989, Romania has pursued a policy of strengthening relations with the West in general, more specifically with the United States and the European Union. It joined the North Atlantic Treaty Organization (NATO) on 29 March 2004, the European Union (EU) on 1 January 2007, while it had joined the International Monetary Fund and the World Bank in 1972, and is a founding member of the World Trade Organization.
The current government has stated its goal of strengthening ties with and helping other countries (in particular Moldova, Ukraine and Georgia) with the process of integration with the rest of the West. Romania has also made clear since the late 1990s that it supports NATO and EU membership for the democratic former Soviet republics in Eastern Europe and the Caucasus. Romania also declared its public support for Turkey, and Croatia joining the European Union. Because it has a large Hungarian minority, Romania has also developed strong relations with Hungary. Romania opted on 1 January 2007, to adhere the Schengen Area, and its bid to join was approved by the European Parliament in June 2011, but was rejected by the EU Council in September 2011.
In December 2005, President Traian Băsescu and United States Secretary of State Condoleezza Rice signed an agreement that would allow a U.S. military presence at several Romanian facilities primarily in the eastern part of the country. In May 2009, Hillary Clinton declared US Secretary of State that "Romania is one of the most trustworthy and respectable partners of the USA."
Relations with Moldova are a special case, considering that the two countries share the same language and a common history. A movement for unification of Romania and Moldova appeared in the early 1990s after both countries achieved emancipation from communist rule, but lost ground in the mid-1990s when a new Moldovan government pursued an agenda towards preserving a Moldovan republic independent of Romania. Romania remains interested in Moldovan affairs and has officially rejected the Molotov–Ribbentrop Pact, and after the 2009 protests in Moldova and subsequent removal of Communists from power, relations between the two countries have improved considerably.
Military.
The Romanian Armed Forces consist of Land, Air, and Naval Forces, and are led by a Commander-in-chief under the supervision of the Ministry of Defense, and by the president as the Supreme Commander during wartime. The Armed Forces consist of approximately 15,000 civilians and 75,000 are military personnel—45,800 for land, 13,250 for air, 6,800 for naval forces, and 8,800 in other fields. The total defence spending in 2007 accounted for 2.05% of total national GDP, or approximately US$2.9 billion, with a total of $11 billion spent between 2006 and 2011 for modernization and acquisition of new equipment.
The Land Forces have overhauled their equipment in the past few years, and are actively participating in the War in Afghanistan. The Air Force currently operates modernized Soviet MiG-21 Lancer fighters which are due to be replaced by twelve F-16s, recently purchased. The Air Force purchased seven new C-27J Spartan tactical airlifters, while the Naval Forces acquired two modernized Type 22 frigates from the Royal Navy. Romanian troops participated in the occupation of Iraq, reaching a peak of 730 soldiers before being slowly drawn down to 350 soldiers. Romania terminated its mission in Iraq and withdrew its last troops on 24 July 2009, among the last countries to do so. Romania currently has some 1,900 troops deployed in Afghanistan. The Regele Ferdinand frigate participated in the 2011 military intervention in Libya.
In December 2011, the Romanian Senate unanimously adopted the draft law ratifying the Romania-United States agreement signed in September of the same year that would allow the establishment and operation of a US land-based ballistic missile defence system in Romania as part of NATO's efforts to build a continental missile shield.
Administrative divisions.
Romania is divided into 41 counties and the municipality of Bucharest. Each county is administered by a county council, responsible for local affairs, as well as a prefect responsible for the administration of national affairs at the county level. The prefect is appointed by the central government but cannot be a member of any political party. Each county is further subdivided into cities and communes, which have their own mayor and local council. There are a total of 319 cities and 2,686 communes in Romania. A total of 103 of the larger cities have municipality statuses, which gives them greater administrative power over local affairs. The municipality of Bucharest is a special case as it enjoys a status on par to that of a county. It is further divided into six sectors and has a prefect, a general mayor, and a general city council.
The NUTS-3 (Nomenclature of Territorial Units for Statistics) level divisions of European Union reflect Romania's administrative-territorial structure, and correspond to the 41 counties plus Bucharest. The cities and communes correspond to the NUTS-5 level divisions, but there are no current NUTS-4 level divisions. The NUTS-1 (four macroregions) and NUTS-2 (eight development regions) divisions exist but have no administrative capacity, and are instead used for coordinating regional development projects and statistical purposes.
Economy.
In 2013, Romania had a GDP (PPP) of around $386 billion and a GDP per capita (PPP) of $19,397. According to CIA's The World Factbook, Romania is an upper-middle income country economy. According to Eurostat, Romania's GDP per capita (PPS) was at 55% of the EU average in 2013, an increase from 42% in 2007 (the year of Romania's accession to the EU).
After 1989 the country experienced a decade of economic instability and decline, led in part by an obsolete industrial base and a lack of structural reform. From 2000 onwards, however, the Romanian economy was transformed into one of relative macroeconomic stability, characterised by high growth, low unemployment and declining inflation. In 2006, according to the Romanian Statistics Office, GDP growth in real terms was recorded at 7.7%, one of the highest rates in Europe. However, a recession following the global financial crisis of 2008–2009 forced the government was forced to borrow externally, including an IMF €20bn bailout program. GDP has been growing by over 2% each year since. According to IMF, the GDP per capita purchasing power parity grew from $14,875 in 2007 to an estimated $19,397 in 2014. Romania still has one of the lowest net average monthly wage in the EU of €540 in 2012, and an inflation of 3.7% in 2013. Unemployment in Romania was at 7% in 2012, which is very low compared to other EU countries.
Industrial output growth reached 6.5% year-on-year in February 2013, the highest in the EU-27. The largest local companies include carmaker Automobile Dacia, Petrom, Rompetrol, Ford Romania, Electrica, Romgaz, RCS & RDS and Banca Transilvania. Exports have increased substantially in the past few years, with a 13% annual rise in exports in 2010. Romania's main exports are cars, software, clothing and textiles, industrial machinery, electrical and electronic equipment, metallurgic products, raw materials, military equipment, pharmaceuticals, fine chemicals, and agricultural products (fruits, vegetables, and flowers). Trade is mostly centred on the member states of the European Union, with Germany and Italy being the country's single largest trading partners. The account balance in 2012 was estimated to be −4.52% of the GDP.
After a series of privatizations and reforms in the late 1990s and 2000s, government intervention in the Romanian economy is somewhat lower than in other European economies. In 2005, the government replaced Romania's progressive tax system with a flat tax of 16% for both personal income and corporate profit, among the lowest rates in the European Union. The economy is predominantly based on services, which account for 51% of GDP, even though industry and agriculture also have significant contributions, making up 36% and 13% of GDP, respectively. Additionally, 30% of the Romanian population was employed in 2006 in agriculture and primary production, one of the highest rates in Europe.
Since 2000, Romania has attracted increasing amounts of foreign investment, becoming the single largest investment destination in Southeastern and Central Europe. Foreign direct investment was valued at €8.3 billion in 2006. According to a 2011 World Bank report, Romania currently ranks 72nd out of 175 economies in the ease of doing business, scoring lower than other countries in the region such as the Czech Republic. Additionally, a study in 2006 judged it to be the world's second-fastest economic reformer (after Georgia).
Since 1867 the official currency has been "leu" (Romanian leu), and following a denomination in 2005, it has been valued at €0.2–0.3. After joining the EU in 2007, Romania is expected to adopt the euro sometime around 2020.
Infrastructure.
According to the "CIA Factbook", Romania's total road network was estimated in 2009 at 81713 km (excluding urban areas), out of which 66632 km was paved roads. There are plans to build a 2262.7 km long motorway system, consisting of six main motorways and six bypass motorways, but as of December 2014, 694.5 km have been laid down, with 240 km under construction or in tendering. The World Bank estimates the railway network at 22298 km of track, the fourth-largest railroad network in Europe. Rail transport experienced a dramatic decline after 1989, and was estimated at 99 million passenger journeys in 2004; but has experienced a recent (2013) revival due to infrastructure improvements and partial privatization of lines, accounting for 45% of all passenger and freight movements in the country. Bucharest Metro, the only underground railway system, was opened in 1979 and measures 61.41 km with an average ridership in 2007 of 600,000 passengers during the workweek. There are sixteen international commercial airports in service today, with five of them (Henri Coandă International Airport, Aurel Vlaicu International Airport, Timișoara International Airport, Constanta International Airport and Sibiu International Airport) being capable of handling wide-body aircraft. Over 7.6 million passengers flew through Bucharest's Henri Coandă International Airport in 2013.
Romania is a net exporter of electrical energy and is 46th worldwide in terms of consumption of electric energy. Around a third of the produced energy comes from renewable sources, mostly as hydroelectric power. In 2010, the main sources were coal (36%), hydroelectric (33%), nuclear (19%), and hydrocarbons (11%). It has one of the largest refining capacities in Eastern Europe, even though oil- and natural gas production has been decreasing for more than a decade. With one of the largest reserves of crude oil and shale gas in Europe, it is among the most energy-independent countries in the European Union, and is looking to further expand its nuclear power plant at Cernavodă.
There were almost 18,3 million connections to the Internet in June 2014. According to Bloomberg, in 2013 Romania ranked 5th in the world and 2nd in Europe in terms of internet connection speed, with Timișoara ranked among the highest in the world.
Tourism.
Tourism is a significant contributor to the Romanian economy, generating around 5% of GDP. According to the World Travel and Tourism Council, Romania was estimated to have the fourth fastest growing travel and tourism total demand in the world, with an estimated potential growth of 8% per year from 2007 to 2016. The number of tourist has been rising, reaching 3.5 million in the first half of 2014. Tourism in Romania attracted €400 million in investments in 2005.
More than 60% of the foreign visitors in 2007 were from other EU countries. Popular summer attractions of Mamaia and other Black Sea Resorts attracted 1.3 million tourists in 2009. Most popular skiing resorts are along the Valea Prahovei and in Poiana Brașov. Castles in Transylvanian cities such as Sibiu, Brașov, and Sighișoara. Rural tourism, focusing on folklore and traditions, has become an important alternative, and is targeted to promote such sites as Bran and its Dracula's Castle, the Painted churches of Northern Moldavia, and the Wooden churches of Maramureș. Other attractions include Danube Delta, and Sculptural Ensemble of Constantin Brâncuși at Târgu Jiu.
Science and technology.
Historically, Romanian researchers and inventors have made notable contributions to several fields. In the history of flight, Traian Vuia made the first airplane to take off on its own power and Aurel Vlaicu built and flew some of the earliest successful aircraft, while Henri Coandă discovered the Coandă effect of fluidics. Victor Babeș discovered more than 50 types of bacteria; biologist Nicolae Paulescu discovered insulin, while Emil Palade, received the Nobel Prize for his contributions to cell biology. Lazăr Edeleanu was the first chemist to synthesize amphetamine, while Costin Nenițescu developed numerous new classes of compounds in organic chemistry. Notable mathematicians include Spiru Haret, Grigore Moisil, and Ștefan Odobleja; physicists and inventors: Șerban Țițeica, Alexandru Proca, and Ștefan Procopiu.
During the 1990s and 2000s, the development of research was hampered by several factors, including corruption, low funding and a considerable brain drain. However, since the country's accession to the European Union, this has begun to change. After being slashed by 50% in 2009 because of the global recession, R&D spending was increased by 44% in 2010 and now stands at $0.5 billion (1.5 billion lei). In January 2011, the Parliament also passed a law that enforces "strict quality control on universities and introduces tough rules for funding evaluation and peer review". The country has joined or is about to join several major international organizations such as CERN and the European Space Agency. Overall, the situation has been characterized as "rapidly improving", albeit from a low base.
The nuclear physics facility of the European Union's proposed Extreme Light Infrastructure (ELI) laser will be built in Romania. In early 2012, Romania launched its first satellite from the Centre Spatial Guyanais in French Guyana. Starting December 2014, Romania is a co-owner of the International Space Station.
Demographics.
According to the 2011 census, Romania's population is 20,121,641. Like other countries in the region, its population is expected to gradually decline in the coming years as a result of sub-replacement fertility rates and negative net migration rate. In October 2011, Romanians made up 88.9% of the population. The largest ethnic minorities are the Hungarians, 6.5% of the population, and Roma, 3.3% of the population. Hungarians constitute a majority in the counties of Harghita and Covasna. Other minorities include Ukrainians, Germans, Turks, Lipovans, and Tatars. In 1930, there were 745,421 Germans in Romania, but only about 36,000 remain today. As of 2009, there were also approximately 133,000 immigrants living in Romania, primarily from Moldova and China.
The total fertility rate (TFR) in 2013 was estimated at 1.31 children born per woman, which is below the replacement rate of 2.1, and one of the lowest in the world. In 2012, 31% of births were to unmarried women. The birth rate (9.49‰, 2012) is much lower than the mortality rate (11.84‰, 2012), resulting in a shrinking (−0.26% per year, 2012) and aging population (median age: 39.1, 2012), with approximately 14.9% of total population aged 65 years and over. The life expectancy in 2013 was estimated at 74.45 years (70.99 years male, 78.13 years female).
The number of Romanians and individuals with ancestors born in Romania living abroad is estimated at around 12 million. After the Romanian Revolution of 1989, a significant number of Romanians emigrated to other European countries, North America or Australia.
Languages.
The official language is Romanian, an Eastern Romance language similar to Aromanian, Megleno-Romanian, and Istro-Romanian, but sharing many features with other Romance languages such as Italian, French, Spanish, Catalan and Portuguese. Romanian is spoken as a first language by 85% of the population, while Hungarian and Vlax Romani are spoken by 6.2% and 1.2% of the population, respectively. There are 25,000 native German speakers, and 32,000 Turkish speakers in Romania, as well as almost 50,000 speakers of Ukrainian, concentrated in some compact regions, near the border, where they form a majority. According to the Constitution, local councils ensure linguistic rights to all minorities, with localities with ethnic minorities of over 20%, that minority's language can be used in the public administration, justice system, and education. Foreign citizens and stateless persons that live in Romania have access to justice and education in their own language. English and French are the main foreign languages taught in schools. In 2010, the Organisation internationale de la Francophonie identifies French speakers in the country. According to the 2012 Eurobarometer, English is spoken by 31% of Romanians, French is spoken by 17%, and Italian by 7%.
Religion.
Romania is a secular state and has no state religion. An overwhelming majority of the population identify themselves as Christians. At the country's 2011 census, 86.5% of respondents for whom data is available identified as Orthodox Christians belonging to the Romanian Orthodox Church. Other denominations include Protestantism (6.9%), Roman Catholicism (4.6%), and Greek Catholicism (0.8%). The remaining 2.2% of the population include 64,337 Muslims (mostly of Turkish and Tatar ethnicity), 3,519 Jews, and 39,660 people who are of no religion or atheist.
The Romanian Orthodox Church is an autocephalous Eastern Orthodox Church in full communion with other Orthodox churches, with a Patriarch as its leader. It is the only Orthodox church using a Romance language and the second-largest in size after the Russian Orthodox Church. Its jurisdiction covers the territory of Romania, with dioceses for Romanians living in nearby Moldova, Serbia and Hungary, as well as diaspora communities in Central and Western Europe, North America and Oceania.
Urbanization.
Although 54.0% of the population lived in 2011 in urban areas, this percentage has been on the decline since 1996. Counties with over 2/3 urban population are Hunedoara, Brașov and Constanța, while with less than a third are Dâmbovița (30.06%) and Giurgiu and Teleorman. Bucharest is the capital and the largest city in Romania, with a population of over 1.8 million in 2011. Its larger urban zone has a population of almost 2.2 million, which are planned to be included into a metropolitan area up to 20 times the area of the city proper. Another 19 cities have a population of over 100,000, with Cluj-Napoca and Timișoara of slightly more than 300,000 inhabitants, and Iași, Constanța, Craiova, Brașov, Galați and Ploiești with over 200,000 inhabitants. Metropolitan areas have been constituted for most of these cities.
Education.
Since the Romanian Revolution of 1989, the Romanian educational system has been in a continuous process of reform that has received mixed criticism. In 2004, some 4.4 million of the population were enrolled in school. Out of these, 650,000 in kindergarten (3–6 years), 3.11 million in primary and secondary level, and 650,000 in tertiary level (universities). In the same year, the adult literacy rate was 97.3% (45th worldwide), while the combined gross enrollment ratio for primary, secondary and tertiary schools was 75% (52nd worldwide). Schooling is compulsory until the first ten years of the primary and secondary schools. There also exists a semi-legal, informal private tutoring system used mostly during secondary school, which has prospered during the Communist regime.
Higher education is aligned with the European higher education area. The results of the PISA assessment study in schools for the year 2012 placed Romania on the 45th rank out of 65 participant countries, though Romania often wins medals in the mathematical olympiads and not only. Alexandru Ioan Cuza University of Iași, Babeș-Bolyai University of Cluj-Napoca, University of Bucharest, and West University of Timișoara have been included in the QS World University Rankings' top 800.
Healthcare.
Romania has a universal health care system, and total health expenditures by the government are roughly 5% of the GDP. It covers medical examinations, any surgical interventions, and any post-operator medical care, and provides free or subsidized medicine for a range of diseases. The state is obliged to fund public hospitals and clinics. The most common causes of death are cardiovascular diseases and cancer. Transmissible diseases, such as tuberculosis, syphilis or viral hepatitis, are more common than in Western Europe. In 2010, Romania had 428 state and 25 private hospitals, with 6.2 hospital beds per 1,000 people, and over 200,000 medical staff, including over 52,000 doctors. s of 2013[ [update]], the emigration rate of doctors was 9%, higher than the European average of 2.5%.
Culture.
Arts and monuments.
The topic of the origin of the Romanians began to be discussed and by the end of the 18th century among the Transylvanian School scholars.
Several writers rose to prominence in the 19th century, including George Coșbuc, Ioan Slavici Mihail Kogălniceanu, Vasile Alecsandri, Nicolae Bălcescu, Ion Luca Caragiale, Ion Creangă, and Mihai Eminescu, the later being considered the greatest and most influential Romanian poet, particularly for the poem "Luceafărul". In the 20th century, Romanian artists reached international acclaim, including Tristan Tzara, Marcel Janco, Mircea Eliade, Nicolae Grigorescu, Marin Preda, Liviu Rebreanu, Eugène Ionesco, Emil Cioran, and Constantin Brâncuși. The latter has a sculptural ensemble in Târgu Jiu, while his sculpture "Bird in Space", was auctioned in 2005 for $27.5 million. Romanian-born Holocaust survivor Elie Wiesel received the Nobel Peace Prize in 1986, while writer Herta Müller received the Nobel Prize in Literature in 2009.
In cinema, several movies of the Romanian New Wave have achieved international acclaim. At the Cannes Film Festival, "4 Months, 3 Weeks and 2 Days" by Cristian Mungiu won "Palme d'Or" in 2007. At the Berlin International Film Festival, "Child's Pose" by Călin Peter Netzer won the Golden Bear in 2013.
The annual George Enescu Festival is held in Bucharest in honor of the 20th century emponymous composer. Musicians like Angela Gheorghiu, Gheorghe Zamfir, Inna, Alexandra Stan and many others have achieved various levels of international acclaim. At the Eurovision Song Contest Romanian singers have achieved third place in 2005 and 2010.
The list of World Heritage Sites includes six cultural sites located within Romania, including eight Painted churches of northern Moldavia, eight Wooden Churches of Maramureș, seven Villages with fortified churches in Transylvania, the Horezu Monastery, and the Historic Centre of Sighișoara.
 The city of Sibiu, with its Brukenthal National Museum, was selected as the 2007 European Capital of Culture. Multiple castles exist in Romania, including popular tourist attractions of Peleș Castle, Corvin Castle, and "Dracula's Castle".
Holidays, traditions and cuisine.
There are 12 non-working public holidays, including the Great Union Day, celebrated on 1 December in commemoration of the 1918 union of Transylvania with Romania. Winter holidays include the Christmas festivities and the New Year during which, various unique folklore dances and games are common: "pluguşorul", "sorcova", "ursul", and "capra". The traditional Romanian dress that otherwise has largely fell out of use during the 20th century, is a popular ceremonial vestment worn on these festivities, especially in the rural areas. Sacrifices of live pigs during Christmas and lambs during Easter has required a special derogation from EU law after 2007. During Easter, painted eggs are very common, while on 1 March features "mărțișor" gifting, a tradition likely of Thracian origin.
Romanian cuisine shares some similarities with other Balkan cuisines such as Greek, Bulgarian and Turkish cuisine. "Ciorbă" includes a wide range of sour soups, while "mititei", "mămăligă" (similar to polenta), and "sarmale" are featured commonly in main courses. Pork, chicken and beef are the preferred meats, but lamb and fish are also popular.
 Certain traditional recipes are made in direct connection with the holidays: "chiftele", "tobă" and "tochitura" at Christmas; "drob", "pască" and "cozonac" at Easter and other Romanian holidays. "Țuică" is a strong plum brandy reaching a 70% alcohol content which is the country's traditional alcoholic beverage, taking as much as 75% of the national production (Romania is one of the largest plum producers in the world). Traditional alcoholic beverages also include wine, "rachiu", "palincă" and "vișinată", but beer consumption has increased dramatically over the recent years.
Sports.
Association football is the most popular sport in Romania with over 234,000 are registered players as of 2010. The governing body is the Romanian Football Federation, which belongs to UEFA. The Romania national football team has taken part seven times in the FIFA World Cup games and had its most successful period during the 1990s, when they reached the quarterfinals of the 1994 FIFA World Cup and was ranked third by FIFA in 1997. The core player of this "Golden Generation" was Gheorghe Hagi, who was nicknamed "the Maradona of the Carpathians." Other successful players include Nicolae Dobrin, Dudu Georgescu, Florea Dumitrache, Liță Dumitru, Ilie Balaci, Loți Bölöni, Costică Ștefănescu, Cornel Dinu or Gheorghe Popescu, and most recently Adrian Mutu, Cristian Chivu, Dan Petrescu or Cosmin Contra.
The most famous successful club is Steaua București and was the first Eastern European team to win the European Champions Cup in 1986, and bing runners-up and in 1989. Dinamo București reached the European Champions' Cup semifinal in 1984 and the Cup Winners' Cup semifinal in 1990. Other important Romanian football clubs are Rapid București, UTA Arad, Universitatea Craiova, CFR Cluj and Petrolul Ploiești.
Tennis is the second most popular sport, with over 15,000 registered players. Romania reached the Davis Cup finals three times (1969, 1971, 1972). The tennis player Ilie Năstase won several Grand Slam titles, and was the first player to be ranked as number 1 by ATP between 1973 and 1974. Virginia Ruzici won the French Open in 1978, and was runner-up in 1980, Simona Halep played the final in 2014 and is currently ranked 2nd by the WTA. Other popular team sport clubs are rugby union and handball. The rugby national team has competed in every Rugby World Cup, while both the men's and women's handball national teams are multiples world champions. On 13 January 2010, Cristina Neagu became the first Romanian in handball to win the IHF World Player of the Year award. Popular individual sports include athletics, chess, judo, dancesport, table tennis and combat sports (Lucian Bute, Leonard Dorin Doroftei, Mihai Leu aka Michael Loewe, Daniel Ghiță, Benjamin Adegbuyi, Andrei Stoica, etc.). While it has a limited popularity nowadays, oină is a traditional Romanian sporting game similar to baseball that has been continuously practiced since at least the 14th century.
Romania participated in the Olympic Games for the first time in 1900 and has taken part in 18 of the 24 summer games. It has been one of the more successful countries at the Summer Olympic Games, with a total of 301 medals won throughout the years, of which 88 gold ones, ranking 15th overall, and second (behind neighbour Hungary) of the nations that have never hosted the game. It participated at the 1984 Summer Olympics in Los Angeles in defiance of a Warsaw Pact boycott and finished second in gold medals (20) and third in total medal count (53). Almost a quarter of all the medals and 25 of the gold ones were won in gymnastics, with Nadia Comăneci becoming the first gymnast ever to score a perfect ten in an Olympic event at the 1976 Summer Olympics. Romanian competitors have won gold medals in other Olympic sports: rowing, athletics, canoeing, wrestling, shooting, fencing, swimming, weightlifting, boxing, and judo. At the Winter Olympic Games, Romania has won only a bronze medal in bobsleigh at the 1968 Winter Olympics.
Sources.
Primary sources.
</dl>
Secondary sources.
</dl>

</doc>
<doc id="25446" url="http://en.wikipedia.org/wiki?curid=25446" title="Robert Chambers (publisher born 1802)">
Robert Chambers (publisher born 1802)

Robert Chambers FRSE FGS (10 July 1802 – 17 March 1871) was a Scottish publisher, geologist, evolutionary thinker, author and journal editor who, like his elder brother and business partner William Chambers, was highly influential in mid-19th century scientific and political circles.
Chambers was an early phrenologist and was the anonymous author of "Vestiges of the Natural History of Creation", which was so controversial that his authorship was not acknowledged until after his death.
Early life.
Robert and his elder brother William were both born in the rural country town of Peebles in the Borders at the turn of the 19th century. The town had changed little in centuries. The town had old and new parts, each consisting of little more than a single street. Peebles was mainly inhabited by weavers and labourers living in thatched cottages. His father, James Chambers, made his living as a cotton manufacturer. Their slate-roofed house was built by James Chambers' father as a wedding gift for his son, and the ground floor served as the family workshop.
A small circulating library in the town, run by Alexander Elder, introduced Robert to books and developed his literary interests when he was young. Occasionally his father would buy books for the family library, and one day Robert found a complete set of the fourth edition of the "Encyclopædia Britannica" hidden away in a chest in the attic. He eagerly read this for many years. Near the end of his life, Chambers remembered feeling "a profound thankfulness that such a convenient collection of human knowledge existed, and that here it was spread out like a well-plenished table before me." William later recalled that for Robert, "the acquisition of knowledge was with him the highest of earthly enjoyments."
Robert was sent to local schools and showed unusual literary taste and ability, though he found his schooling to be uninspiring. His education was typical for the day. The country school, directed by James Gray, taught the boys reading, writing, and, for an additional charge, arithmetic. In grammar school it was the classics – Latin and Ancient Greek, with some English composition. Boys bullied one another and the teacher administered corporal punishment in the classroom for unruly behaviour. Although uninspired by the school, Robert made up for this at the bookseller.
Both Robert and William were born with six fingers on each hand and six toes on each foot. Their parents attempted to correct this abnormality through operations, and while William's was successful Robert was left partially lame. So while other boys roughed it outside, Robert was content to stay indoors and study his books.
Robert surpassed his elder brother in his education, which he continued for several years beyond William's. Robert had been destined for the ministry, but at the age of fifteen he dropped this intended career. The arrival of the power loom suddenly threatened James Chambers' cotton business, forcing him to close it down and become a draper. During this time, James began to socialise with a number of French prisoners-of-war on parole who were stationed in Peebles. Unfortunately, James Chambers lent these exiles a large amount of credit, and when they were abruptly transferred away he was forced to declare bankruptcy. The family moved to Edinburgh in 1813. Robert continued his education at the High School, and William became a bookseller's apprentice. In 1818 Robert, at 16 years old, began his own business as a bookstall-keeper on Leith Walk. At first, his entire stock consisted of some old books belonging to his father, amounting to thirteen feet of shelf space and worth no more than a few pounds. By the end of the first year the value of his stock went up to twelve pounds, and modest success came gradually.
Early works.
While Robert built up a business, his brother William expanded his own by purchasing a home-made printing press and publishing pamphlets as well as creating his own type. Soon afterwards, Robert and William decided to join forces – with Robert writing and William printing. Their first joint venture was a magazine series called "The Kaleidoscope, or Edinburgh Literary Amusement", sold for threepence. This was issued every two weeks between 6 October 1821 and 12 January 1822. It was followed by "Illustrations of the Author of Waverley" (1822), which offered sketches of individuals believed to have been the inspirations for some of the characters in Walter Scott's works of fiction. The last book to be printed on William's old press was "Traditions of Edinburgh" (1824), derived from Robert's enthusiastic interest in the history and antiquities of Edinburgh. He followed this with "Walks in Edinburgh" (1825), and these books gained him the approval and personal friendship of Walter Scott. After Scott's death, Robert paid tribute to him by writing a "Life of Sir Walter Scott" (1832). Robert also wrote a "History of the Rebellions in Scotland from 1638 to 1745" (5 vols, 1828) and numerous other works on Scotland and Scottish traditions.
Marriage.
On 7 December 1829 Robert married Anne Kirkwood, the only child of John Kirkwood. Together they had 14 children, three of whom died in infancy. Excluding these three, their children were Robert, Nina (Mrs. Frederick Lehmann, and mother of Rudolf Chambers Lehmann), Mary (Mrs. Alexander Mackenzie Edwards, mother of Bob Edwards), Anne (Mrs. Dowie, mother of Ménie Muriel Dowie), Janet, Eliza (Mrs. William Overend Priestley), Amelia (Mrs. Rudolf Lehmann), James, William, Phoebe (Mrs. Zeigler), and Alice.
W. & R. Chambers.
At the beginning of 1832 Robert's brother William Chambers started a weekly publication entitled "Chambers's Edinburgh Journal", which speedily gained a large circulation. Robert was at first only a contributor, but after 14 volumes had appeared, he became joint editor with his brother, and his collaboration contributed more perhaps than anything else to the success of the "Journal". The two brothers eventually united as partners in the firm of W. & R. Chambers Publishers.
At the same time Robert ran a bookshop and circulating library from 48 Hanover Street with his younger brother, James Chambers. Meanwhile William ran his shop from 47 Broughton Street. Robert at this time was living close to the shop, at 27 Elder Street (demolished in the 1960s to improve access to Edinburgh Bus Station).
Among the other numerous works of which Robert was in whole or in part the author, the "Biographical Dictionary of Eminent Scotsmen" (4 vols., Glasgow, 1832–1835), the "Cyclopædia of English Literature" (1844), the "Life and Works of Robert Burns" (4 vols., 1851), "Ancient Sea Margins" (1848), the "Domestic Annals of Scotland" (1859–1861) and the "Book of Days" (2 vols., 1862–1864) were the most important.
"Chambers's Encyclopaedia" (1859–1868), with Dr Andrew Findlater as editor, was carried out under the superintendence of the brothers. The "Cyclopædia of English Literature" contains a series of admirably selected extracts from the best authors of every period, "set in a biographical and critical history of the literature itself." For the "Life of Burns" he made diligent and laborious original investigations, gathering many hitherto unrecorded facts from the poet's sister, Mrs. Begg, to whose benefit the whole profits of the work were generously devoted.
"Vestiges".
During the 1830s, Robert Chambers took a particularly keen interest in the then rapidly expanding field of geology, and he was elected a fellow of the Geological Society of London in 1844. Prior to this, he was elected a member of the Royal Society of Edinburgh in 1840, which connected him through correspondence to numerous scientific men. William later recalls that "His mind had become occupied with speculative theories which brought him into communication with Sir Charles Bell, George Combe, his brother Dr. Andrew Combe, Dr. Neil Arnott, Professor Edward Forbes, Dr. Samuel Brown, and other thinkers on physiology and mental philosophy." In 1848 Chambers published his first geological book on "Ancient Sea Margins." Later, he toured Scandinavia and Canada for the purpose of geological exploration. The results of his travels were published in "Tracings of the North of Europe" (1851) and "Tracings in Iceland and the Faroe Islands" (1856). 
However, his most popular book, influenced by his geological studies and interest in speculative theories, was a work to which he never officially attached his name. In 1844, Chambers completed the dictation of his "Vestiges of the Natural History of Creation" to his wife, Anne Kirkwood, as he recuperated from depression at his holiday home in St Andrews. The composition of "Vestiges" may have served a therapeutic purpose. Chambers had been an enthusiastic phrenologist in Edinburgh in the 1830s, and the anonymously authored "Vestiges" became an international bestseller and a powerful public influence, subsequent to Combe's "Constitution of Man" (1828), and anticipating the publication of Charles Darwin's "Origin of Species" in 1859. In a strange parallel to Robert and Anne Chambers, Prince Albert read the "Vestiges" aloud to Queen Victoria over several days in 1845. The first edition of "Vestiges of the Natural History of Creation" was released in 1844 and published anonymously. Literary anonymity was not uncommon at the time, especially in periodical journalism. However, in the science genre, anonymity was especially rare, due to the fact that science writers typically wanted to take credit for their work to claim priority for their findings.
The reason for Chambers' anonymity was clear enough as soon as one began reading the text. The book was arguing for a developmental view of the cosmos combining stellar evolution with progressive transmutation of species in the same spirit as the late Frenchman Jean-Baptiste Lamarck. Lamarck had been discredited among intellectuals by this time, and evolutionary (or development) theories were exceedingly unpopular, except among political radicals, materialists, and atheists. Chambers, however, tried to explicitly distance his own theory from that of Lamarck's by denying Lamarck's evolutionary mechanism any plausibility. "Now it is possible that wants and the exercise of faculties have entered in some manner into the production of the phenomena which we have been considering; but certainly not in the way suggested by Lamarck, whose whole notion is obviously so inadequate to account for the rise of the organic kingdoms, that we only can place it with pity among the follies of the wise." Additionally, his work was far more sweeping in scope than any of his predecessors. "The book, as far as I am aware," he writes in his concluding chapter, "is the first attempt to connect the natural sciences in a history of creation."
Robert Chambers was certainly aware of the storm that would probably be raised at the time by his treatment of the subject, and most importantly he did not wish to get his and his brother's publishing firm involved in any kind of scandal that could potentially ruin or severely impact their business venture. The arrangements for publication, therefore, were made through a friend named Alexander Ireland, of Manchester. To further prevent the possibility of any unwanted revelations, Chambers only disclosed the secret to four people: his wife, his brother William, Ireland, and George Combe's nephew, Robert Cox. All correspondence to and from Chambers passed through Ireland's hands first, and all letters and manuscripts were dutifully transcribed in Mrs. Chamber's hand to prevent the possibility of anyone recognising Robert's handwriting.
By implying that God might not actively sustain the natural and social hierarchies, the book threatened the social order and could provide ammunition to Chartists and revolutionaries. Anglican clergymen and naturalists attacked the book, with the geologist Adam Sedgwick predicting "ruin and confusion in such a creed" which, if taken up by the working classes, "will undermine the whole moral and social fabric" bringing "discord and deadly mischief in its train". The book was liked by many Quakers and Unitarians. The Unitarian physiologist William Carpenter called it "a very beautiful and a very interesting book", and helped Chambers with correcting later editions. Critics thanked God that the author began "in ignorance and presumption", for the revised versions "would have been much more dangerous". Nevertheless, the book caused a sensation and quickly went through a number of new editions. "Vestiges" brought widespread discussion of evolution out of the streets and gutter presses and into the drawing rooms of respectable men and women.
Other activities.
Chambers gave a talk on ancient beaches at the British Association for the Advancement of Science meeting at Oxford in May 1847. An observer named Andrew Crombie Ramsay at the meeting reported that Chambers "pushed his conclusions to a most unwarrantable length and got roughly handled on account of it by Buckland, De la Beche, Sedgwick, Murchison, and Lyell. The last told me afterwards that he did so purposely that [Chambers] might see that reasonings in the style of the author of the Vestiges would not be tolerated among scientific men." On the Sunday Samuel Wilberforce, Bishop of Oxford, used his sermon at St. Mary's Church on "the wrong way of doing science" to deliver a stinging attack obviously aimed at Chambers. The church, "crowded to suffocation" with geologists, astronomers and zoologists, heard jibes about the "half-learned" seduced by the "foul temptation" of speculation looking for a self-sustaining universe in a "mocking spirit of unbelief", showing a failure to understand the "modes of the Creator's acting" or to meet the responsibilities of a gentleman. Chambers denounced this as an attempt to stifle progressive opinion, but others thought he must have gone home "with the feeling of a martyr".
Near the close of autumn 1848, Chambers allowed himself to be brought forward as a candidate for the administrative position of Lord Provost of Edinburgh. The timing was especially poor, with others seeking any means possible to try and discredit his character. His adversaries found the perfect opportunity to do so in the swirling allegations that he was the author of the much reviled "Vestiges". William Chambers, in his "Memoir of Robert Chambers", still sworn to secrecy despite his brother's recent passing, makes his only mention of "Vestiges" in connection with this affair: "(Robert) might have been well assured that a rumor to the effect that he was the author of 'Vestiges of the Natural History of Creation' would be used to his disadvantage, and that anything he might say on the subject would be unavailing.". Robert withdrew his candidacy in disgust.
In 1851 Chambers was one of a group of writers who joined the publisher John Chapman in reinvigorating the "Westminster Review" as a flagship of free thought and reform, spreading the ideas of evolutionism.
"Book of Days".
The "Book of Days" was Chambers's last major publication, and perhaps his most elaborate. It was a miscellany of popular antiquities in connection with the calendar, and it is supposed that his excessive labour in connexion with this book hastened his death. Two years before, the University of St Andrews had conferred upon him the degree of Doctor of Laws, and he was elected a member of the Athenaeum Club in London.
Death.
Robert Chambers died on 17 March 1871 in St Andrews. He was buried in the Cathedral burial ground in the interior of the old Church of St. Regulus, according to his wishes. A memorial window was erected to Robert by William in St Giles Cathedral next to a larger window to William himself, placed at the time of his restoration of the cathedral.
A year after Robert's death, his brother William published a biography under the title "Memoir of Robert Chambers; With Autobiographical Reminisces of William Chambers". However, the book did not reveal Robert's authorship of the "Vestiges". Milton Millhauser, in his 1959 book "Just Before Darwin", wrote the following about William's memoir: "The fraternal "Memoir of Robert Chambers" might have been an excellent biography had not the author been concerned to keep the "Vestiges" secret and one or two others. Despite the author's intelligence and sympathy, such omissions inevitably produced a distorted picture" (p. 191, note 7). The book contains some reminisces by Robert of his early life, with the rest of the narration filled in by William.
Alexander Ireland, in 1884, issued a 12th edition of "Vestiges" with Robert Chambers finally listed as the author and a preface giving an account of its authorship. Ireland felt that there was no longer any reason for concealing the author's name.
Works.
Edited and Contributed to.
</dl>
Sources.
</dl>

</doc>
<doc id="25447" url="http://en.wikipedia.org/wiki?curid=25447" title="Rhetoric">
Rhetoric

Rhetoric (pronounced ) is the art of discourse, an art that aims to improve the capability of writers or speakers to inform, persuade, or motivate particular audiences in specific situations. As a subject of formal study and a productive civic practice, rhetoric has played a central role in the European tradition. Its best known definition comes from Aristotle, who considers it a counterpart of both logic and politics, and calls it "the faculty of observing in any given case the available means of persuasion." Rhetorics typically provide heuristics for understanding, discovering, and developing arguments for particular situations, such as Aristotle's three persuasive audience appeals, logos, pathos, and ethos. The five canons of rhetoric, which trace the traditional tasks in designing a persuasive speech, were first codified in classical Rome: invention, arrangement, style, memory, and delivery. Along with grammar and logic (or dialectic—see Martianus Capella), rhetoric is one of the three ancient arts of discourse.
From Ancient Greece to the late 19th century, it was a central part of Western education, filling the need to train public speakers and writers to move audiences to action with arguments. The word is derived from the Greek ῥητορικός "rhētorikós", "oratorical", from ῥήτωρ "rhḗtōr", "public speaker", related to ῥῆμα "rhêma", "that which is said or spoken, word, saying", and ultimately derived from the verb ἐρῶ "erō", "I say, I speak".
Uses of rhetoric.
Scope of rhetoric.
Scholars have debated the scope of rhetoric since ancient times. Although some have limited rhetoric to the specific realm of political discourse, many modern scholars liberate it to encompass every aspect of culture. Contemporary studies of rhetoric address a more diverse range of domains than was the case in ancient times. While classical rhetoric trained speakers to be effective persuaders in public forums and institutions such as courtrooms and assemblies, contemporary rhetoric investigates human discourse writ large. Rhetoricians have studied the discourses of a wide variety of domains, including the natural and social sciences, fine art, religion, journalism, digital media, fiction, history, cartography, and architecture, along with the more traditional domains of politics and the law. Many contemporary approaches treat rhetoric as human communication that includes purposeful and strategic manipulation of symbols. Public relations, lobbying, law, marketing, professional and technical writing, and advertising are modern professions that employ rhetorical practitioners.
Because the ancient Greeks highly valued public political participation, rhetoric emerged as a crucial tool to influence politics. Consequently, rhetoric remains associated with its political origins. However, even the original instructors of Western speech—the Sophists—disputed this limited view of rhetoric. According to the Sophists, such as Gorgias, a successful rhetorician could speak convincingly on any topic, regardless of his experience in that field. This method suggested rhetoric could be a means of communicating any expertise, not just politics. In his "Encomium to Helen", Gorgias even applied rhetoric to fiction by seeking for his own pleasure to prove the blamelessness of the mythical Helen of Troy in starting the Trojan War.
Looking to another key rhetorical theorist, Plato defined the scope of rhetoric according to his negative opinions of the art. He criticized the Sophists for using rhetoric as a means of deceit instead of discovering truth. In "Gorgias," one of his Socratic Dialogues, Plato defines rhetoric as the persuasion of ignorant masses within the courts and assemblies. Rhetoric, in Plato's opinion, is merely a form of flattery and functions similarly to cookery, which masks the undesirability of unhealthy food by making it taste good. Thus, Plato considered any speech of lengthy prose aimed at flattery as within the scope of rhetoric.
Aristotle both redeemed rhetoric from his teacher and narrowed its focus by defining three genres of rhetoric—deliberative, forensic or judicial, and epideictic. Yet, even as he provided order to existing rhetorical theories, Aristotle extended the definition of rhetoric, calling it the ability to identify the appropriate means of persuasion in a given situation, thereby making rhetoric applicable to all fields, not just politics. When one considers that rhetoric included torture (in the sense that the practice of torture is a form of persuasion or coercion), it is clear that rhetoric cannot be viewed only in academic terms. However, the enthymeme based upon logic (especially, based upon the syllogism) was viewed as the basis of rhetoric.
However, since the time of Aristotle, logic has changed. For example, Modal logic has undergone a major development that also modifies rhetoric. Yet, Aristotle also outlined generic constraints that focused the rhetorical art squarely within the domain of public political practice. He restricted rhetoric to the domain of the contingent or probable: those matters that admit multiple legitimate opinions or arguments.
The contemporary neo-Aristotelian and neo-Sophistic positions on rhetoric mirror the division between the Sophists and Aristotle. Neo-Aristotelians generally study rhetoric as political discourse, while the neo-Sophistic view contends that rhetoric cannot be so limited. Rhetorical scholar Michael Leff characterizes the conflict between these positions as viewing rhetoric as a "thing contained" versus a "container." The neo-Aristotelian view threatens the study of rhetoric by restraining it to such a limited field, ignoring many critical applications of rhetorical theory, criticism, and practice. Simultaneously, the neo-Sophists threaten to expand rhetoric beyond a point of coherent theoretical value.
Over the past century, people studying rhetoric have tended to enlarge its object domain beyond speech texts. Kenneth Burke asserted humans use rhetoric to resolve conflicts by identifying shared characteristics and interests in symbols. By nature, humans engage in identification, either to identify themselves or another individual with a group. This definition of rhetoric as identification broadened the scope from strategic and overt political persuasion to the more implicit tactics of identification found in an immense range of sources.
Among the many scholars who have since pursued Burke's line of thought, James Boyd White sees rhetoric as a broader domain of social experience in his notion of constitutive rhetoric. Influenced by theories of social construction, White argues that culture is "reconstituted" through language. Just as language influences people, people influence language. Language is socially constructed, and depends on the meanings people attach to it. Because language is not rigid and changes depending on the situation, the very usage of language is rhetorical. An author, White would say, is always trying to construct a new world and persuading his or her readers to share that world within the text.
Individuals engage in the rhetorical process anytime they speak or produce meaning. Even in the field of science, the practices of which were once viewed as being merely the objective testing and reporting of knowledge, scientists must persuade their audience to accept their findings by sufficiently demonstrating that their study or experiment was conducted reliably and resulted in sufficient evidence to support their conclusions.
The vast scope of rhetoric is difficult to define; however, political discourse remains, in many ways, the paradigmatic example for studying and theorizing specific techniques and conceptions of persuasion, considered by many a synonym for "rhetoric."
Rhetoric as a civic art.
Throughout European History, rhetoric has concerned itself with persuasion in public and political settings such as assemblies and courts. Because of its associations with democratic institutions, rhetoric is commonly said to flourish in open and democratic societies with rights of free speech, free assembly, and political enfranchisement for some portion of the population. Those who classify rhetoric as a civic art believe that rhetoric has the power to shape communities, form the character of citizens and greatly impact civic life.
Rhetoric was viewed as a civic art by several of the ancient philosophers. Aristotle and Isocrates were two of the first to see rhetoric in this light. In his work, Antidosis, Isocrates states, "We have come together and founded cities and made laws and invented arts; and, generally speaking, there is no institution devised by man which the power of speech has not helped us to establish." With this statement he argues that rhetoric is a fundamental part of civic life in every society and that it has been necessary in the foundation of all aspects of society. He further argues in his piece Against the Sophists that rhetoric, although it cannot be taught to just anyone, is capable of shaping the character of man. He writes, "I do think that the study of political discourse can help more than any other thing to stimulate and form such qualities of character." Aristotle, writing several years after Isocrates, supported many of his arguments and continued to make arguments for rhetoric as a civic art.
In the words of Aristotle, in his essay "Rhetoric", rhetoric is "... the faculty of observing in any given case the available means of persuasion." According to Aristotle, this art of persuasion could be used in public settings in three different ways. He writes in Book I, Chapter III, "A member of the assembly decides about future events, a juryman about past events: while those who merely decide on the orator's skill are observers. From this it follows that there are three divisions of oratory- (1) political, (2) forensic, and (3) the ceremonial oratory of display". Eugene Garver, in his critique of "Aristotle's Rhetoric", confirms that Aristotle viewed rhetoric as a civic art. Garver writes, "Rhetoric articulates a civic art of rhetoric, combining the almost incompatible properties of techne and appropriateness to citizens." Each of Aristotle's divisions plays a role in civic life and can be used in a different way to impact cities.
Because rhetoric is a public art capable of shaping opinion, some of the ancients including Plato found fault in it. They claimed that while it could be used to improve civic life, it could be used equally easily to deceive or manipulate with negative effects on the city. The masses were incapable of analyzing or deciding anything on their own and would therefore be swayed by the most persuasive speeches. Thus, civic life could be controlled by the one who could deliver the best speech. Plato's explores the problematic moral status of rhetoric twice: in "Gorgias", a dialogue named for the famed Sophist, and in "The Phaedrus", a dialogue best known for its commentary on love.
More trusting in the power of rhetoric to support a republic, the Roman orator Cicero argued that art required something more than eloquence. A good orator needed also to be a good man, a person enlightened on a variety of civic topics. He describes the proper training of the orator in his major text on rhetoric, "De Oratore", modeled on Plato's dialogues.
Modern day works continue to support the claims of the ancients that rhetoric is an art capable of influencing civic life. In his work "Political Style", Robert Hariman claims, "Furthermore, questions of freedom, equality, and justice often are raised and addressed through performances ranging from debates to demonstrations without loss of moral content". James Boyd White argues further that rhetoric is capable not only of addressing issues of political interest but that it can influence culture as a whole. In his book, "When Words Lose Their Meaning", he argues that words of persuasion and identification define community and civic life. He states that words produce "... the methods by which culture is maintained, criticized, and transformed." Both White and Hariman agree that words and rhetoric have the power to shape culture and civic life.
In modern times, rhetoric has consistently remained relevant as a civic art. In speeches, as well as in non-verbal forms, rhetoric continues to be used as a tool to influence communities from local to national levels.
Rhetoric as a course of study.
Rhetoric as a course of study has evolved significantly since its ancient beginnings. Through the ages, the study and teaching of rhetoric has adapted to the particular exigencies of the time and venue. The study of rhetoric has conformed to a multitude of different applications, ranging from architecture to literature. Although the curriculum has transformed in a number of ways, it has generally emphasized the study of principles and rules of composition as a means for moving audiences. Generally speaking, the study of rhetoric trains students to speak and/or write effectively, as well as critically understand and analyze discourse.
Rhetoric began as a civic art in Ancient Greece where students were trained to develop tactics of oratorical persuasion, especially in legal disputes. Rhetoric originated in a school of pre-Socratic philosophers known as the Sophists circa 600 BC. Demosthenes and Lysias emerged as major orators during this period, and Isocrates and Gorgias as prominent teachers. Rhetorical education focused on five particular canons: "inventio" (invention), "dispositio" (arrangement), "elocutio" (style), "memoria" (memory), and "actio" (delivery). Modern teachings continue to reference these rhetorical leaders and their work in discussions of classical rhetoric and persuasion.
Rhetoric was later taught in universities during the Middle Ages as one of the three original liberal arts or trivium (along with logic and grammar). During the medieval period, political rhetoric declined as republican oratory died out and the emperors of Rome garnered increasing authority. With the rise of European monarchs in following centuries, rhetoric shifted into the courtly and religious applications. Augustine exerted strong influence on Christian rhetoric in the Middle Ages, advocating the use of rhetoric to lead audiences to truth and understanding, especially in the church. The study of liberal arts, he believed, contributed to rhetorical study: "In the case of a keen and ardent nature, fine words will come more readily through reading and hearing the eloquent than by pursuing the rules of rhetoric." Poetry and letter writing, for instance, became a central component of rhetorical study during the Middle Ages. After the fall of the Republic in Rome, poetry became a tool for rhetorical training since there were fewer opportunities for political speech. Letter writing was the primary form through which business was conducted both in state and church, so it became an important aspect of rhetorical education.
Rhetorical education became more restrained as style and substance separated in 16th-century France with Peter Ramus, and attention turned to the scientific method. That is, influential scholars like Ramus argued that the processes of invention and arrangement should be elevated to the domain of philosophy, while rhetorical instruction should be chiefly concerned with the use of figures and other forms of the ornamentation of language. Scholars such as Francis Bacon developed the study of "scientific rhetoric." This concentration rejected the elaborate style characteristic of the classical oration. This plain language carried over to John Locke's teaching, which emphasized concrete knowledge and steered away from ornamentation in speech, further alienating rhetorical instruction, which was identified wholly with this ornamentation, from the pursuit of knowledge.
In the 18th century, rhetoric assumed a more social role, initiating the creation of new education systems. "Elocution schools" arose (predominantly in England) in which females analyzed classic literature, most notably the works of William Shakespeare, and discussed pronunciation tactics.
The study of rhetoric underwent a revival with the rise of democratic institutions during the late 18th and early 19th centuries. Scotland's author and theorist Hugh Blair served as a key leader of this movement during the late 18th century. In his most famous work "Lectures on Rhetoric and Belles Lettres", he advocates rhetorical study for common citizens as a resource for social success. Many American colleges and secondary schools used Blair's text throughout the 19th century to train students of rhetoric.
Political rhetoric also underwent renewal in the wake of the US and French revolutions. The rhetorical studies of ancient Greece and Rome were resurrected in the studies of the era as speakers and teachers looked to Cicero and others to inspire defense of the new republic. Leading rhetorical theorists included John Quincy Adams of Harvard who advocated the democratic advancement of rhetorical art. Harvard's founding of the Boylston Professorship of Rhetoric and Oratory sparked the growth of rhetorical study in colleges across the United States. Harvard's rhetoric program drew inspiration from literary sources to guide organization and style.
Debate clubs and lyceums also developed as forums in which common citizens could hear speakers and sharpen debate skills. The American lyceum in particular was seen as both an educational and social institution, featuring group discussions and guest lecturers. These programs cultivated democratic values and promoted active participation in political analysis.
Throughout the 20th century, rhetoric developed as a concentrated field of study with the establishment of rhetorical courses in high schools and universities. Courses such as public speaking and speech analysis apply fundamental Greek theories (such as the modes of persuasion: ethos, pathos, and logos) as well as trace rhetorical development throughout the course of history. Rhetoric has earned a more esteemed reputation as a field of study with the emergence of Communication Studies departments in university programs and in conjunction with the linguistic turn. Rhetorical study has broadened in scope, and is especially utilized by the fields of marketing, politics, and literature.
Rhetoric, as an area of study, is concerned with how humans use symbols, especially language, to reach agreement that permits coordinated effort of some sort. Harvard University, the first university in the United States, based on the European model, taught a basic curriculum, including rhetoric. Rhetoric, in this sense, how to properly give speeches, played an important role in their training. Rhetoric was soon taught in departments of English as well.
Rhetoric and knowledge.
The relationship between rhetoric and knowledge is an old and interesting philosophical problem, partly because of our different assumptions on the nature of knowledge. But it is fairly clear that while knowledge is primarily concerned with truth (i.e. assuming that there is such a thing as truth), rhetoric is primarily concerned with statements and their effects on the audience. The word "rhetoric" may also refer to "empty speak", which reflects an indifference to truth, and in this sense rhetoric is adversarial to knowledge. Plato famously criticized the Sophists for their rhetoric which had persuaded people to sentence his friend Socrates to death regardless of what was true. However, rhetoric is also used in the construction of true arguments, or in identifying what is relevant, the crux of the matter, in a selection of true but otherwise trivial statements. Hence, rhetoric is also closely related to knowledge.
History.
Rhetoric has its origins in Mesopotamia. Some of the earliest examples of rhetoric can be found in the Akkadian writings of the princess and priestess Enheduanna (ca. 2285–2250 BC), while later examples can be found in the Neo-Assyrian Empire during the time of Sennacherib (704–681 BC). In ancient Egypt, rhetoric had existed since at least the Middle Kingdom period (ca. 2080–1640 BC). The Egyptians held eloquent speaking in high esteem, and it was a skill that had a very high value in their society. The "Egyptian rules of rhetoric" also clearly specified that "knowing when not to speak is essential, and very respected, rhetorical knowledge." Their "approach to rhetoric" was thus a "balance between eloquence and wise silence." Their rules of speech also strongly emphasized "adherence to social behaviors that support a conservative status quo" and they held that "skilled speech should support, not question, society." In ancient China, rhetoric dates back to the Chinese philosopher, Confucius (551-479 BC), and continued with later followers. The tradition of Confucianism emphasized the use of eloquence in speaking. The use of rhetoric can also be found in the ancient Biblical tradition.
In ancient Greece, the earliest mention of oratorical skill occurs in Homer's "Iliad", where heroes like Achilles, Hector, and Odysseus were honored for their ability to advise and exhort their peers and followers (the "Laos" or army) in wise and appropriate action. With the rise of the democratic "polis", speaking skill was adapted to the needs of the public and political life of cities in ancient Greece, much of which revolved around the use of oratory as the medium through which political and judicial decisions were made, and through which philosophical ideas were developed and disseminated. For modern students today, it can be difficult to remember that the wide use and availability of written texts is a phenomenon that was just coming into vogue in Classical Greece. In Classical times, many of the great thinkers and political leaders performed their works before an audience, usually in the context of a competition or contest for fame, political influence, and cultural capital; in fact, many of them are known only through the texts that their students, followers, or detractors wrote down. As has already been noted, "rhetor" was the Greek term for "orator:" A "rhetor" was a citizen who regularly addressed juries and political assemblies and who was thus understood to have gained some knowledge about public speaking in the process, though in general facility with language was often referred to as "logôn techne", "skill with arguments" or "verbal artistry."
Rhetoric thus evolved as an important art, one that provided the orator with the forms, means, and strategies for persuading an audience of the correctness of the orator's arguments. Today the term "rhetoric" can be used at times to refer only to the form of argumentation, often with the pejorative connotation that rhetoric is a means of obscuring the truth. Classical philosophers believed quite the contrary: the skilled use of rhetoric was essential to the discovery of truths, because it provided the means of ordering and clarifying arguments.
Sophists.
In Europe, organized thought about public speaking began in ancient Greece. Possibly, the first study about the power of language may be attributed to the philosopher Empedocles (d. ca. 444 BC), whose theories on human knowledge would provide a basis for many future rhetoricians. The first written manual is attributed to Corax and his pupil Tisias. Their work, as well as that of many of the early rhetoricians, grew out of the courts of law; Tisias, for example, is believed to have written judicial speeches that others delivered in the courts. Teaching in oratory was popularized in the 5th century BC by itinerant teachers known as sophists, the best known of whom were Protagoras (c.481-420 BC), Gorgias (c.483-376 BC), and Isocrates (436-338 BC). The Sophists were a disparate group who travelled from city to city, teaching in public places to attract students and offer them an education. Their central focus was on logos or what we might broadly refer to as discourse, its functions and powers. They defined parts of speech, analyzed poetry, parsed close synonyms, invented argumentation strategies, and debated the nature of reality. They claimed to make their students "better," or, in other words, to teach virtue. They thus claimed that human "excellence" was not an accident of fate or a prerogative of noble birth, but an art or "techne" that could be taught and learned. They were thus among the first humanists. Several sophists also questioned received wisdom about the gods and the Greek culture, which they believed was taken for granted by Greeks of their time, making them among the first agnostics. For example, they argued that cultural practices were a function of convention or "nomos" rather than blood or birth or "phusis". They argued even further that morality or immorality of any action could not be judged outside of the cultural context within which it occurred. The well-known phrase, "Man is the measure of all things" arises from this belief. One of their most famous, and infamous, doctrines has to do with probability and counter arguments. They taught that every argument could be countered with an opposing argument, that an argument's effectiveness derived from how "likely" it appeared to the audience (its probability of seeming true), and that any probability argument could be countered with an inverted probability argument. Thus, if it seemed likely that a strong, poor man were guilty of robbing a rich, weak man, the strong poor man could argue, on the contrary, that this very likelihood (that he would be a suspect) makes it unlikely that he committed the crime, since he would most likely be apprehended for the crime. They also taught and were known for their ability to make the weaker (or worse) argument the stronger (or better). Aristophanes famously parodies the clever inversions that sophists were known for in his play "The Clouds".
The word "sophistry" developed strong negative connotations in ancient Greece that continue today, but in ancient Greece sophists were nevertheless popular and well-paid professionals, widely respected for their abilities but also widely criticized for their excesses.
Isocrates.
Isocrates (436-338 BC), like the sophists, taught public speaking as a means of human improvement, but he worked to distinguish himself from the Sophists, whom he saw as claiming far more than they could deliver. He suggested that while an art of virtue or excellence did exist, it was only one piece, and the least, in a process of self-improvement that relied much more heavily on native talent and desire, constant practice, and the imitation of good models. Isocrates believed that practice in speaking publicly about noble themes and important questions would function to improve the character of both speaker and audience while also offering the best service to a city. In fact, Isocrates was an outspoken champion of rhetoric as a mode of civic engagement. He thus wrote his speeches as "models" for his students to imitate in the same way that poets might imitate Homer or Hesiod, seeking to inspire in them a desire to attain fame through civic leadership. His was the first permanent school in Athens and it is likely that Plato's Academy and Aristotle's Lyceum were founded in part as a response to Isocrates. Though he left no handbooks, his speeches ("Antidosis" and "Against the Sophists" are most relevant to students of rhetoric) became models of oratory (he was one of the canonical "Ten Attic Orators") and keys to his entire educational program. He had a marked influence on Cicero and Quintilian, and through them, on the entire educational system of the west.
Plato.
Plato (427-347 BC) famously outlined the differences between true and false rhetoric in a number of dialogues; particularly the "Gorgias" and "Phaedrus" dialogues wherein Plato disputes the sophistic notion that the art of persuasion (the sophists' art, which he calls "rhetoric"), can exist independent of the art of dialectic. Plato claims that since sophists appeal only to what seems probable, they are not advancing their students and audiences, but simply flattering them with what they want to hear. While Plato's condemnation of rhetoric is clear in the "Gorgias", in the "Phaedrus" he suggests the possibility of a true art wherein rhetoric is based upon the knowledge produced by dialectic, and relies on a dialectically informed rhetoric to appeal to the main character, Phaedrus, to take up philosophy. Thus Plato's rhetoric is actually dialectic (or philosophy) "turned" toward those who are not yet philosophers and are thus unready to pursue dialectic directly. Plato's animosity against rhetoric, and against the sophists, derives not only from their inflated claims to teach virtue and their reliance on appearances, but from the fact that his teacher, Socrates, was sentenced to death after sophists' efforts.
Aristotle.
Aristotle (384-322 BC) was a student of Plato who famously set forth an extended treatise on rhetoric that still repays careful study today. In the first sentence of The Art of Rhetoric, Aristotle says that "rhetoric is the counterpart [literally, the antistrophe] of dialectic." As the "antistrophe" of a Greek ode responds to and is patterned after the structure of the "strophe" (they form two sections of the whole and are sung by two parts of the chorus), so the art of rhetoric follows and is structurally patterned after the art of dialectic because both are arts of discourse production. Thus, while dialectical methods are necessary to find truth in theoretical matters, rhetorical methods are required in practical matters such as adjudicating somebody's guilt or innocence when charged in a court of law, or adjudicating a prudent course of action to be taken in a deliberative assembly. The core features dialectic include the absence of determined subject matter, its elaboration on earlier empirical practice, the explication of its aims, the type of utility and the definition of the proper function.
For Plato and Aristotle, dialectic involves persuasion, so when Aristotle says that rhetoric is the antistrophe of dialectic, he means that rhetoric as he uses the term has a domain or scope of application that is parallel to, but different from, the domain or scope of application of dialectic. In "Nietzsche Humanist" (1998: 129), Claude Pavur explains that "[t]he Greek prefix 'anti' does not merely designate opposition, but it can also mean 'in place of.'" When Aristotle characterizes rhetoric as the antistrophe of dialectic, he no doubt means that rhetoric is used in place of dialectic when we are discussing civic issues in a court of law or in a legislative assembly. The domain of rhetoric is civic affairs and practical decision making in civic affairs, not theoretical considerations of operational definitions of terms and clarification of thought. These, for him, are in the domain of dialectic.
Aristotle's treatise on rhetoric is an attempt to systematically describe civic rhetoric as a human art or skill (techne). It is more of an objective theory than it is an interpretive theory with a rhetorical tradition. Aristotle's "art" of rhetoric emphasizes on persuasion to be the purpose of rhetoric. His definition of rhetoric as "the faculty of observing in any given case the available means of persuasion," essentially a mode of discovery, seems to limit the art to the inventional process, and Aristotle heavily emphasizes the logical aspect of this process. In his world, rhetoric is the art of discovering all available means of persuasion. A speaker supports the probability of a message by logical, ethical, and emotional proofs. Some form of logos, ethos, and pathos is present in every possible public presentation that exists. But the treatise in fact also discusses not only elements of style and (briefly) delivery, but also emotional appeals (pathos) and characterological appeals (ethos).
Aristotle identifies three steps or "offices" of rhetoric—invention, arrangement, and style—and three different types of rhetorical proof: ethos (Aristotle's theory of character and how the character and credibility of a speaker can influence an audience to consider him/her to be believable—there being three qualities that contribute to a credible ethos: perceived intelligence, virtuous character, and goodwill); pathos (the use of emotional appeals to alter the audience's judgment through metaphor, amplification, storytelling, or presenting the topic in a way that evokes strong emotions in the audience.); and, logos (the use of reasoning, either inductive or deductive, to construct an argument).
Aristotle emphasized "enthymematic reasoning" as central to the process of rhetorical invention, though later rhetorical theorists placed much less emphasis on it. An "enthymeme" would follow today's form of a syllogism; however it would exclude either the major or minor premise. An enthymeme is persuasive because the audience is providing the missing premise. Because the audience is able to provide the missing premise, they are more likely to be persuaded by the message.
Aristotle identified three different types or genres of civic rhetoric. "Forensic" (also known as judicial, was concerned with determining the truth or falseness of events that took place in the past and issues of guilt. An example of forensic rhetoric would be in a courtroom. "Deliberative" (also known as political), was concerned with determining whether or not particular actions should or should not be taken in the future. Making laws would be an example of deliberative rhetoric. "Epideictic" (also known as ceremonial), was concerned with praise and blame, values, right and wrong, demonstrating beauty and skill in the present. Examples of epideictic rhetoric would include a eulogy or a wedding toast.
Canons.
The Five Canons of Rhetoric serve as a guide to creating persuasive messages and arguments. These are invention (the process of developing arguments); style (determining how to present the arguments); arrangement (organizing the arguments for extreme effect); delivery (the gestures, pronunciation, tone and pace used when presenting the persuasive arguments); and memory (the process of learning and memorizing the speech and persuasive messages.)
In the rhetoric field, there is an intellectual debate about Aristotle's definition of rhetoric. Some believe that Aristotle defines rhetoric in "On Rhetoric" as the art of persuasion, while others think he defines it as the art of judgment. Rhetoric as the art of judgment would mean the rhetor discerns the available means of persuasion with a choice. Aristotle also says rhetoric is concerned with judgment because the audience judges the rhetor's ethos.
One of the most famous of Aristotelian doctrines was the idea of topics (also referred to as common topics or commonplaces). Though the term had a wide range of application (as a memory technique or compositional exercise, for example) it most often referred to the "seats of argument"—the list of categories of thought or modes of reasoning—that a speaker could use to generate arguments or proofs. The topics were thus a heuristic or inventional tool designed to help speakers categorize and thus better retain and apply frequently used types of argument. For example, since we often see effects as "like" their causes, one way to invent an argument (about a future effect) is by discussing the cause (which it will be "like"). This and other rhetorical topics derive from Aristotle's belief that there are certain predictable ways in which humans (particularly non-specialists) draw conclusions from premises. Based upon and adapted from his dialectical Topics, the rhetorical topics became a central feature of later rhetorical theorizing, most famously in Cicero's work of that name.
Cicero.
For the Romans, oration became an important part of public life. Cicero (106-43 BC) was chief among Roman rhetoricians and remains the best known ancient orator and the only orator who both spoke in public and produced treatises on the subject. "Rhetorica ad Herennium", formerly attributed to Cicero but now considered to be of unknown authorship, is one of the most significant works on rhetoric and is still widely used as a reference today. It is an extensive reference on the use of rhetoric, and in the Middle Ages and Renaissance, it achieved wide publication as an advanced school text on rhetoric.
Cicero is considered one of the most significant rhetoricians of all time, charting a middle path between the competing Attic and Asiatic styles to become considered second only to Demosthenes among history's orators. His works include the early and very influential De Inventione (On Invention, often read alongside the "Ad Herennium" as the two basic texts of rhetorical theory throughout the Middle Ages and into the Renaissance), De Oratore (a fuller statement of rhetorical principles in dialogue form), Topics (a rhetorical treatment of common topics, highly influential through the Renaissance), Brutus (Cicero) (a discussion of famous orators) and Orator (a defense of Cicero's style). Cicero also left a large body of speeches and letters which would establish the outlines of Latin eloquence and style for generations to come. It was the rediscovery of Cicero's speeches (such as the defense of Archias) and letters (to Atticus) by Italians like Petrarch that, in part, ignited the cultural innovations that we know as the Renaissance. He championed the learning of Greek (and Greek rhetoric), contributed to Roman ethics, linguistics, philosophy, and politics, and emphasized the importance of all forms of appeal (emotion, humor, stylistic range, irony and digression in addition to pure reasoning) in oratory. But perhaps his most significant contribution to subsequent rhetoric, and education in general, was his argument that orators learn not only about the specifics of their case (the "hypothesis") but also about the general questions from which they derived (the "theses"). Thus, in giving a speech in defense of a poet whose Roman citizenship had been questioned, the orator should examine not only the specifics of that poet's civic status, he should also examine the role and value of poetry and of literature more generally in Roman culture and political life. The orator, said Cicero, needed to be knowledgeable about all areas of human life and culture, including law, politics, history, literature, ethics, warfare, medicine, even arithmetic and geometry. Cicero gave rise to the idea that the "ideal orator" be well-versed in all branches of learning: an idea that was rendered as "liberal humanism," and that lives on today in liberal arts or general education requirements in colleges and universities around the world.
Quintilian.
Quintilian (35-100 AD) began his career as a pleader in the courts of law; his reputation grew so great that Vespasian created a chair of rhetoric for him in Rome. The culmination of his life's work was the "Institutio Oratoria" ("Institutes of Oratory," or alternatively, "The Orator's Education"), a lengthy treatise on the training of the orator, in which he discusses the training of the "perfect" orator from birth to old age and, in the process, reviews the doctrines and opinions of many influential rhetoricians who preceded him.
In the Institutes, Quintilian organizes rhetorical study through the stages of education that an aspiring orator would undergo, beginning with the selection of a nurse. Aspects of elementary education (training in reading and writing, grammar, and literary criticism) are followed by preliminary rhetorical exercises in composition (the progymnasmata) that include maxims and fables, narratives and comparisons, and finally full legal or political speeches. The delivery of speeches within the context of education or for entertainment purposes became widespread and popular under the term "declamation." Rhetorical training proper was categorized under five canons that would persist for centuries in academic circles:
This work was available only in fragments in medieval times, but the discovery of a complete copy at the Abbey of St. Gall in 1416 led to its emergence as one of the most influential works on rhetoric during the Renaissance.
Quintilian's work describes not just the art of rhetoric, but the formation of the perfect orator as a politically active, virtuous, publicly minded citizen. His emphasis was on the ethical application of rhetorical training, in part a reaction against the growing tendency in Roman schools toward standardization of themes and techniques. At the same time that rhetoric was becoming divorced from political decision making, rhetoric rose as a culturally vibrant and important mode of entertainment and cultural criticism in a movement known as the "second sophistic," a development that gave rise to the charge (made by Quintilian and others) that teachers were emphasizing style over substance in rhetoric.
Medieval to Enlightenment.
After the breakup of the western Roman Empire, the study of rhetoric continued to be central to the study of the verbal arts; but the study of the verbal arts went into decline for several centuries, followed eventually by a gradual rise in formal education, culminating in the rise of medieval universities. But rhetoric transmuted during this period into the arts of letter writing ("ars dictaminis") and sermon writing ("ars praedicandi"). As part of the "trivium", rhetoric was secondary to the study of logic, and its study was highly scholastic: students were given repetitive exercises in the creation of discourses on historical subjects ("suasoriae") or on classic legal questions ("controversiae").
Although he is not commonly regarded as a rhetorician, St. Augustine (354-430) was trained in rhetoric and was at one time a professor of Latin rhetoric in Milan. After his conversion to Christianity, he became interested in using these "pagan" arts for spreading his religion. This new use of rhetoric is explored in the Fourth Book of his "De Doctrina Christiana", which laid the foundation of what would become homiletics, the rhetoric of the sermon. Augustine begins the book by asking why "the power of eloquence, which is so efficacious in pleading either for the erroneous cause or the right", should not be used for righteous purposes (IV.3).
One early concern of the medieval Christian church was its attitude to classical rhetoric itself. Jerome (d. 420) complained, "What has Horace to do with the Psalms, Virgil with the Gospels, Cicero with the Apostles?" Augustine is also remembered for arguing for the preservation of pagan works and fostering a church tradition that led to conservation of numerous pre-Christian rhetorical writings.
Rhetoric would not regain its classical heights until the renaissance, but new writings did advance rhetorical thought. Boethius (480?-524), in his brief "Overview of the Structure of Rhetoric", continues Aristotle's taxonomy by placing rhetoric in subordination to philosophical argument or dialectic. The introduction of Arab scholarship from European relations with the Muslim empire (in particular Al-Andalus) renewed interest in Aristotle and Classical thought in general, leading to what some historians call the 12th century renaissance. A number of medieval grammars and studies of poetry and rhetoric appeared.
Late medieval rhetorical writings include those of St. Thomas Aquinas (1225?-1274), Matthew of Vendome ("Ars Versificatoria", 1175?), and Geoffrey of Vinsauf ("Poetria Nova", 1200–1216). Pre-modern female rhetoricians, outside of Socrates' friend Aspasia, are rare; but medieval rhetoric produced by women either in religious orders, such as Julian of Norwich (d. 1415), or the very well-connected Christine de Pizan (1364?-1430?), did occur if not always recorded in writing.
In his 1943 Cambridge University doctoral dissertation in English, Canadian Marshall McLuhan (1911–1980) surveys the verbal arts from approximately the time of Cicero down to the time of Thomas Nashe (1567–1600?). His dissertation is still noteworthy for undertaking to study the history of the verbal arts together as the trivium, even though the developments that he surveys have been studied in greater detail since he undertook his study. As noted below, McLuhan became one of the most widely publicized thinkers in the 20th century, so it is important to note his scholarly roots in the study of the history of rhetoric and dialectic.
Another interesting record of medieval rhetorical thought can be seen in the many animal debate poems popular in England and the continent during the Middle Ages, such as The Owl and the Nightingale (13th century) and Geoffrey Chaucer's Parliament of Fowls (1382?).
Sixteenth century.
Walter J. Ong's article "Humanism" in the 1967 "New Catholic Encyclopedia" surveys Renaissance humanism, which defined itself broadly as disfavoring medieval scholastic logic and dialectic and as favoring instead the study of classical Latin style and grammar and philology and rhetoric. (Reprinted in Ong's "Faith and Contexts" (Scholars Press, 1999; 4: 69-91.))
One influential figure in the rebirth of interest in classical rhetoric was Erasmus (c.1466-1536). His 1512 work, "De Duplici Copia Verborum et Rerum" (also known as ""), was widely published (it went through more than 150 editions throughout Europe) and became one of the basic school texts on the subject. Its treatment of rhetoric is less comprehensive than the classic works of antiquity, but provides a traditional treatment of "res-verba" (matter and form): its first book treats the subject of elocutio, showing the student how to use schemes and tropes; the second book covers inventio. Much of the emphasis is on abundance of variation ("copia" means "plenty" or "abundance", as in copious or cornucopia), so both books focus on ways to introduce the maximum amount of variety into discourse. For instance, in one section of the "De Copia", Erasmus presents two hundred variations of the sentence "Semper, dum vivam, tui meminero." Another of his works, the extremely popular The Praise of Folly, also had considerable influence on the teaching of rhetoric in the later 16th century. Its orations in favour of qualities such as madness spawned a type of exercise popular in Elizabethan grammar schools, later called adoxography, which required pupils to compose passages in praise of useless things.
Juan Luis Vives (1492–1540) also helped shape the study of rhetoric in England. A Spaniard, he was appointed in 1523 to the Lectureship of Rhetoric at Oxford by Cardinal Wolsey, and was entrusted by Henry VIII to be one of the tutors of Mary. Vives fell into disfavor when Henry VIII divorced Catherine of Aragon and left England in 1528. His best-known work was a book on education, "De Disciplinis", published in 1531, and his writings on rhetoric included "Rhetoricae, sive De Ratione Dicendi, Libri Tres" (1533), "De Consultatione" (1533), and a rhetoric on letter writing, "De Conscribendis Epistolas" (1536).
It is likely that many well-known English writers would have been exposed to the works of Erasmus and Vives (as well as those of the Classical rhetoricians) in their schooling, which was conducted in Latin (not English) and often included some study of Greek and placed considerable emphasis on rhetoric. See, for example, T.W. Baldwin's "William Shakspere's Small Latine and Lesse Greeke", 2 vols. (University of Illinois Press, 1944).
The mid-16th century saw the rise of vernacular rhetorics—those written in English rather than in the Classical languages; adoption of works in English was slow, however, due to the strong orientation toward Latin and Greek. Leonard Cox's "The Art or Crafte of Rhetoryke" (c. 1524–1530; second edition published in 1532) is considered to be the earliest text on rhetorics in English; it was, for the most part, a translation of the work of Philipp Melanchthon. A successful early text was Thomas Wilson's "The Arte of Rhetorique" (1553), which presents a traditional treatment of rhetoric. For instance, Wilson presents the five canons of rhetoric (Invention, Disposition, Elocutio, Memoria, and Utterance or Actio). Other notable works included Angel Day's "The English Secretorie" (1586, 1592), George Puttenham's "The Arte of English Poesie" (1589), and Richard Rainholde's "Foundacion of Rhetorike" (1563).
During this same period, a movement began that would change the organization of the school curriculum in Protestant and especially Puritan circles and led to rhetoric losing its central place. A French scholar, Pierre de la Ramée, in Latin Petrus Ramus (1515–1572), dissatisfied with what he saw as the overly broad and redundant organization of the trivium, proposed a new curriculum. In his scheme of things, the five components of rhetoric no longer lived under the common heading of rhetoric. Instead, invention and disposition were determined to fall exclusively under the heading of dialectic, while style, delivery, and memory were all that remained for rhetoric. See Walter J. Ong, "Ramus, Method, and the Decay of Dialogue: From the Art of Discourse to the Art of Reason" (Harvard University Press, 1958; reissued by the University of Chicago Press, 2004, with a new foreword by Adrian Johns). Ramus, rightly accused of sodomy and erroneously of atheism, was martyred during the French Wars of Religion. His teachings, seen as inimical to Catholicism, were short-lived in France but found a fertile ground in the Netherlands, Germany and England.
One of Ramus' French followers, Audomarus Talaeus (Omer Talon) published his rhetoric, "Institutiones Oratoriae", in 1544. This work provided a simple presentation of rhetoric that emphasized the treatment of style, and became so popular that it was mentioned in John Brinsley's (1612) "Ludus literarius; or The Grammar Schoole" as being the "most used in the best schooles." Many other Ramist rhetorics followed in the next half-century, and by the 17th century, their approach became the primary method of teaching rhetoric in Protestant and especially Puritan circles. See Walter J. Ong, "Ramus and Talon Inventory" (Harvard University Press, 1958); Joseph S. Freedman, "Philosophy and the Art Europe, 1500–1700: Teaching and Texts at Schools and Universities" (Ashgate, 1999). John Milton (1608–1674) wrote a textbook in logic or dialectic in Latin based on Ramus' work, which has now been translated into English by Walter J. Ong and Charles J. Ermatinger in "The Complete Prose Works of John Milton" (Yale University Press, 1982; 8: 206-407), with a lengthy introduction by Ong (144-205). The introduction is reprinted in Ong's "Faith and Contexts" (Scholars Press, 1999; 4: 111-41).
Ramism could not exert any influence on the established Catholic schools and universities, which remained loyal to Scholasticism, or on the new Catholic schools and universities founded by members of the religious orders known as the Society of Jesus or the Oratorians, as can be seen in the Jesuit curriculum (in use right up to the 19th century, across the Christian world) known as the Ratio Studiorum (that Claude Pavur, S.J., has recently translated into English, with the Latin text in the parallel column on each page (St. Louis: Institute of Jesuit Sources, 2005)). If the influence of Cicero and Quintilian permeates the Ratio Studiorum, it is through the lenses of devotion and the militancy of the Counter-Reformation. The "Ratio" was indeed imbued with a sense of the divine, of the incarnate logos, that is of rhetoric as an eloquent and humane means to reach further devotion and further action in the Christian city, which was absent from Ramist formalism. The Ratio is, in rhetoric, the answer to St Ignatius Loyola's practice, in devotion, of "spiritual exercises." This complex oratorical-prayer system is absent from Ramism.
Seventeenth century.
In New England and at Harvard College (founded 1636), Ramus and his followers dominated, as Perry Miller shows in "The New England Mind: The Seventeenth Century" (Harvard University Press, 1939). However, in England, several writers influenced the course of rhetoric during the 17th century, many of them carrying forward the dichotomy that had been set forth by Ramus and his followers during the preceding decades. Of greater importance is that this century saw the development of a modern, vernacular style that looked to English, rather than to Greek, Latin, or French models.
Francis Bacon (1561–1626), although not a rhetorician, contributed to the field in his writings. One of the concerns of the age was to find a suitable style for the discussion of scientific topics, which needed above all a clear exposition of facts and arguments, rather than the ornate style favored at the time. Bacon in his "The Advancement of Learning" criticized those who are preoccupied with style rather than "the weight of matter, worth of subject, soundness of argument, life of invention, or depth of judgment." On matters of style, he proposed that the style conform to the subject matter and to the audience, that simple words be employed whenever possible, and that the style should be agreeable.
Thomas Hobbes (1588–1679) also wrote on rhetoric. Along with a shortened translation of Aristotle's "Rhetoric", Hobbes also produced a number of other works on the subject. Sharply contrarian on many subjects, Hobbes, like Bacon, also promoted a simpler and more natural style that used figures of speech sparingly.
Perhaps the most influential development in English style came out of the work of the Royal Society (founded in 1660), which in 1664 set up a committee to improve the English language. Among the committee's members were John Evelyn (1620–1706), Thomas Sprat (1635–1713), and John Dryden (1631–1700). Sprat regarded "fine speaking" as a disease, and thought that a proper style should "reject all amplifications, digressions, and swellings of style" and instead "return back to a primitive purity and shortness" ("History of the Royal Society", 1667).
While the work of this committee never went beyond planning, John Dryden is often credited with creating and exemplifying a new and modern English style. His central tenet was that the style should be proper "to the occasion, the subject, and the persons." As such, he advocated the use of English words whenever possible instead of foreign ones, as well as vernacular, rather than Latinate, syntax. His own prose (and his poetry) became exemplars of this new style.
Eighteenth century.
Arguably one of the most influential schools of rhetoric during this time was Scottish Belletristic rhetoric, exemplified by such professors of rhetoric as Hugh Blair whose Lectures on Rhetoric and Belles Lettres saw international success in various editions and translations.
Modern rhetoric.
At the turn of the 20th century, there was a revival of rhetorical study manifested in the establishment of departments of rhetoric and speech at academic institutions, as well as the formation of national and international professional organizations. Jim A. Kuypers and Andrew King suggest that the early interest in rhetorical studies was a movement away from elocution as taught in departments of English in the United States, and was an attempt to refocus rhetorical studies away from delivery only to civic engagement. Collectively, they write, twentieth century rhetorical studies offered an understanding of rhetoric that demonstrated a "rich complexity" of how rhetorical scholars understood the nature of rhetoric. Theorists generally agree that by the 1930s a significant reason for the revival of the study of rhetoric was the renewed importance of language and persuasion in the increasingly mediated environment of the 20th century (see Linguistic turn) and through the 21st century, with the media focus on the wide variations and analyses of political rhetoric and its consequences. The rise of advertising and of mass media such as photography, telegraphy, radio, and film brought rhetoric more prominently into people's lives. More recently the term rhetoric has been applied to media forms other than verbal language, e.g. Visual rhetoric.
Methods of analysis.
Criticism seen as a method.
Rhetoric can be analyzed by a variety of methods and theories. One such method is criticism. When those using criticism analyze instances of rhetoric what they do is called rhetorical criticism (see section below). According to rhetorical critic Jim A. Kuypers, “The use of rhetoric is an art; as such, it does not lend itself well to scientific methods of analysis. Criticism is an art as well; as such, it is particularly well suited for examining rhetorical creations.” He asserts that criticism is a method of generating knowledge just as the scientific method is a method for generating knowledge: “The way the Sciences and the Humanities study the phenomena that surround us differ greatly in the amount of researcher personality allowed to influence the results of the study. For example, in the Sciences researchers purposefully adhere to a strict method (the scientific method). All scientific researchers are to use this same basic method, and successful experiments must be 100 percent replicable by others. The application of the scientific method may take numerous forms, but the overall method remains the same--and the personality of the researcher is excised from the actual study. In sharp contrast, criticism (one of many Humanistic methods of generating knowledge) actively involves the personality of the researcher. The very choices of what to study, and how and why to study a rhetorical artifact are heavily influenced by the personal qualities of the researcher. In criticism this is especially important since the personality of the critic considered an integral component of the study. Further personalizing criticism, we find that rhetorical critics use a variety of means when examining a particular rhetorical artifact, with some critics even developing their own unique perspective to better examine a rhetorical artifact.”
Edwin Black (rhetorician) wrote on this point that, “Methods, then, admit of varying degrees of personality. And criticism, on the whole, is near the indeterminate, contingent, personal end of the methodological scale. In consequence of this placement, it is neither possible nor desirable for criticism to be fixed into a system, for critical techniques to be objectified, for critics to be interchangeable for purposes of [scientific] replication, or for rhetorical criticism to serve as the handmaiden of quasi-scientific theory. [The] idea is that critical method is too personally expressive to be systematized.
Jim A. Kuypers sums this idea of criticism as art in the following manner: “In short, criticism is an art, not a science. It is not a scientific method; it uses subjective methods of argument; it exists on its own, not in conjunction with other methods of generating knowledge (i.e., social scientific or scientific). [I]nsight and imagination top statistical applications when studying rhetorical action.” 
Observation on analytic rhetorical method.
There does not exist an analytic method that is widely recognized as "the" rhetorical method, partly because many in rhetorical study see rhetoric as merely produced by reality (see dissent from that view below). It is important to note that the object of rhetorical analysis is typically discourse, and therefore the principles of "rhetorical analysis" would be difficult to distinguish from those of "discourse analysis." However, rhetorical analytic methods can also be applied to almost anything, including objects—a car, a castle, a computer, a comportment.
Generally speaking, rhetorical analysis makes use of rhetorical concepts (ethos, logos, kairos, mediation, etc.) to describe the social or epistemological functions of the object of study. When the object of study happens to be some type of discourse (a speech, a poem, a joke, a newspaper article), the aim of rhetorical analysis is not simply to describe the claims and arguments advanced within the discourse, but (more important) to identify the specific semiotic strategies employed by the speaker to accomplish specific persuasive goals. Therefore, after a rhetorical analyst discovers a use of language that is particularly important in achieving persuasion, she typically moves onto the question of "How does it work?" That is, what effects does this particular use of rhetoric have on an audience, and how does that effect provide more clues as to the speaker's (or writer's) objectives?
There are some scholars who do partial rhetorical analysis and defer judgments about rhetorical success. In other words, some analysts attempt to avoid the question of "Was this use of rhetoric successful [in accomplishing the aims of the speaker]?" To others, however, that is the preeminent point: is the rhetoric strategically effective and what did the rhetoric accomplish? This question allows a shift in focus from the speaker's objectives to the effects and functions of the rhetoric itself.
Rhetorical strategies.
"Rhetorical strategies" are the efforts made by authors to persuade or inform their readers. Rhetorical strategies are employed by writers and refer to the different ways they can persuade the reader. According to Gray, there are various argument strategies used in writing. He describes four of these as argument from analogy, argument from absurdity, thought experiments, and inference to the best explanation.
Rhetorical criticism.
Modern rhetorical criticism explores the relationship between text and context; that is, how an instance of rhetoric relates to circumstances. In his Rhetorical Criticism: A Study in Method, scholar Edwin Black states, "It is the task of criticism not to measure ... discourses dogmatically against some parochial standard of rationality but, allowing for the immeasurable wide range of human experience, to see them as they really are." While the language "as they really are" is debatable, rhetorical critics explain texts and speeches by investigating their rhetorical situation, typically placing them in a framework of speaker/audience exchange. The antithetical view places the rhetor at the center of creating that which is considered the extant situation; i.e., the agenda and spin.
Additional theoretical approaches.
Following the neo-Aristotelian approaches to criticism, scholars began to derive methods from other disciplines, such as history, philosophy, and the social sciences. The importance of critics' personal judgment decreased in explicit coverage while the analytical dimension of criticism began to gain momentum. Throughout the 1960s and 1970s, methodological pluralism replaced the singular neo-Aristotelian method. Methodological rhetorical criticism is typically done by deduction, where a broad method is used to examine a specific case of rhetoric. These types include:
By the mid-1980s, however, the study of rhetorical criticism began to move away from precise methodology towards conceptual issues. Conceptually driven criticism operates more through abduction, according to scholar James Jasinski, who argues that this emerging type of criticism can be thought of as a back-and-forth between the text and the concepts, which are being explored at the same time. The concepts remain "works in progress," and understanding those terms develops through the analysis of a text.
Criticism is considered rhetorical when it focuses on the way some types of discourse react to situational exigencies—problems or demands—and constraints. This means that modern rhetorical criticism is based in how the rhetorical case or object persuades, defines, or constructs the audience. In modern terms, what can be considered rhetoric includes, but it is not limited to, speeches, scientific discourse, pamphlets, literary work, works of art, and pictures. Contemporary rhetorical criticism has maintained aspects of early neo-Aristotelian thinking through close reading, which attempts to explore the organization and stylistic structure of a rhetorical object. Using close textual analysis means rhetorical critics use the tools of classical rhetoric and literary analysis to evaluate the style and strategy used to communicate the argument.
Purpose of rhetorical criticism.
Rhetorical criticism serves several purposes or functions. First, rhetorical criticism hopes to help form or improve public taste. It helps educate audiences and develops them into better judges of rhetorical situations by reinforcing ideas of value, morality, and suitability. Rhetorical criticism can thus contribute to the audience's understanding of themselves and society.
According to Jim A. Kuypers, a dual purpose for performing criticism should be primarily to enhance our appreciation and understanding. ‘[W]e wish to enhance both our own and others’ understanding of the rhetorical act; we wish to share our insights with others, and to enhance their appreciation of the rhetorical act. These are not hollow goals, but quality of life issues. By improving understanding and appreciation, the critic can offer new and potentially exciting ways for others to see the world. Through understanding we also produce knowledge about human communication; in theory this should help us to better govern our interactions with others.’ Criticism is a humanizing activity in that it explores and highlights qualities that make us human.” 
French rhetoric.
Rhetoric was part of the curriculum in Jesuit and, to a lesser extent, Oratorian colleges until the French Revolution. For Jesuits, right from the foundation of the Society in France, rhetoric was an integral part of the training of young men toward taking up leadership positions in the Church and in State institutions, as Marc Fumaroli has shown it in his foundational "Âge de l'éloquence" (1980). The Oratorians, by contrast, reserved it a lesser place, in part due to the stress they placed on modern language acquisition and a more sensualist philosophy (like Bernard Lamy's "La Rhétorique ou l'Art de parler" (1675), which is an excellent example of their approach). Nonetheless, in the 18th Century, rhetoric was the structure and crown of secondary education, with works such as Rollin's "Treatise of Studies" achieving a wide and enduring fame across the Continent. Later, with Nicolas Boileau and François de Malherbe, rhetoric is the instrument of the clarity of the comment and speech ; the literature that ensues from it is named "Sublime". The main representative remains Rivarol.
The French Revolution, however, turned this around. Philosophers such as Condorcet, who drafted the French revolutionary chart for a people's education under the rule of reason, dismissed rhetoric as an instrument of oppression in the hands of clerics in particular. The Revolution went as far as to suppress the Bar, arguing that forensic rhetoric did disservice to a rational system of justice, by allowing fallacies and emotions to come into play. Nonetheless, as later historians of the 19th century were keen to explain, the Revolution was a high moment of eloquence and rhetorical prowess, although set against a background of rejecting rhetoric.
Under the First Empire and its wide-ranging educational reforms, imposed on or imitated across the Continent, rhetoric regained little ground. In fact, instructions to the newly founded Polytechnic School, tasked with training the scientific and technical elites, made it clear that written reporting was to supersede oral reporting. Rhetoric reentered secondary curriculum in fits and starts, but never regained the prominence it had enjoyed under the "ancien régime", although the penultimate year of secondary education was known as the Class of Rhetoric. When manuals were redrafted in the mid-century, in particular after the 1848 Revolution to formulate a national curriculum, care was taken to distance their approach to rhetoric from that of the Church, which was seen as an agent of conservatism and reactionary politics.
By the end of the 1870s, a major change had taken place: philosophy of the rationalist or eclectic kind, generally Kantian, had taken over rhetoric as the true end stage of secondary education (the so-called Class of Philosophy bridged secondary and university education). Rhetoric was then relegated to the study of literary figures of speech, a discipline later on taught as Stylistics within the French literature curriculum. More decisively, in 1890, a new standard written exercise superseded the rhetorical exercises of speech writing, letter writing and narration. The new genre, called dissertation, had been invented in 1866, for the purpose of rational argument in the philosophy class. Typically, in a dissertation, a question is asked, such as: "Is history a sign of humanity's freedom?" The structure of a dissertation consists in an introduction that elucidates the basic definitions involved in the question as set, followed by an argument or thesis, a counter-argument or antithesis, and a resolving argument or synthesis that is not a compromise between the former but the production of a new argument, ending with a conclusion that does not sum up the points but opens onto a new problem. Hegelianism influenced the dissertation design. It remains today the standard of writing in French humanities.
By the beginning of the 20th century, rhetoric was fast losing the remains of its former importance, and eventually was taken out of the school curriculum altogether at the time of the Separation of State and Churches (1905). Part of the argument was that rhetoric remained the last element of irrationality, driven by religious arguments, in what was perceived as inimical to Republican education. The move, initiated in 1789, found its resolution in 1902 when rhetoric was expunged from all curricula. At the same time, Aristotelian rhetoric, owing to a revival of Thomistic philosophy initiated by Rome, regained ground in what was left of Catholic education in France, in particular at the prestigious Faculty of Theology of Paris, now a private entity. Yet, rhetoric vanished substantially from the French scene, educational or intellectual, for some 60 years..
In the early 1960s a change began to take place, as the word rhetoric and the body of knowledge it covers began to be used again, in a modest and almost secret manner. The new linguistic turn, through the rise of semiotics as well as of structural linguistics, brought to the fore a new interest in figures of speech as signs, the metaphor in particular (in the works of Roman Jakobson, Groupe µ, Michel Charles, Gérard Genette) while famed Structuralist Roland Barthes, a classicist by training, perceived how some basic elements of rhetoric could be of use in the study of narratives, fashion and ideology. Knowledge of rhetoric was so dim in the early 1970s that his short memoir on rhetoric was seen as highly innovative. Basic as it was, it did help rhetoric regain some currency in avant-garde circles. Psychoanalyst Jacques Lacan, his contemporary, makes references to rhetoric, in particular to the Pre-Socratics. Philosopher Jacques Derrida wrote on Voice.
At the same time, more profound work was taking place that eventually gave rise to the French school of rhetoric as it exists today.
This rhetorical revival took place on two fronts. First, in 17th-century French studies, the mainstay of French literary education, awareness grew that rhetoric was necessary to push the limits of knowledge further, and also to provide an antidote to Structuralism and its denial of historicism in culture. This was the pioneering work of Marc Fumaroli who, building on the work of classicist and Neo-Latinist Alain Michel and French scholars such as Roger Zuber, published his famed "Age de l'Eloquence" (1980), was one of the founders of the International Society for the History of Rhetoric and was eventually elevated to a chair in rhetoric at the prestigious College de France. He is the editor in chief of a monumental "History of Rhetoric in Modern Europe". His disciples form the second generation, with rhetoricians such as Françoise Waquet and Delphine Denis, both of the Sorbonne, or Philippe-Joseph Salazar ( on the French Wikipedia), until recently at Derrida's College international de philosophie, laureate of the Harry Oppenheimer prize and whose recent book on "Hyperpolitique" has attracted the French media's attention on a "re-appropriation of the means of production of persuasion".
Second, in the area of Classical studies, in the wake of Alain Michel, Latin scholars fostered a renewal in Cicero studies. They broke away from a pure literary reading of his orations, in an attempt to embed Cicero in European ethics. Meanwhile, among Greek scholars, the literary historian and philologist Jacques Bompaire, the philologist and philosopher E. Dupréel, and later the literature historian Jacqueline de Romilly pioneered new studies in the Sophists and the Second Sophistic. The second generation of Classicists, often trained in philosophy as well (following Heidegger and Derrida, mainly), built on their work, with authors such as Marcel Detienne (now at Johns Hopkins), Nicole Loraux, Medievalist and logician (Geneva), Ciceronian scholar Carlos Lévy (Sorbonne, Paris) and Barbara Cassin (Collége international de philosophie, Paris). Sociologist of science Bruno Latour and economist Romain Laufer may also be considered part of, or close to this group. Also French philosophers specialized in Arabic commentaries on Aristotle's "Rhetoric".
Links between the two strands—literary and philosophical—of the French school of rhetoric are strong and collaborative, and bear witness to the revival of rhetoric in France. A recent issue of "Philosophy & Rhetoric" presents current writing in the field.
References.
The "locus classicus" for Greek and Latin primary texts on rhetoric is the Loeb Classical Library of the Harvard University Press, published with an English translation on the facing page.

</doc>
<doc id="25448" url="http://en.wikipedia.org/wiki?curid=25448" title="Red tide">
Red tide

Red tide is a common name for a phenomenon known as an algal bloom (large concentrations of aquatic microorganisms) when it is caused by a few species of dinoflagellates and the bloom takes on a red or brown color. Red tides are events in which estuarine, marine, or fresh water algae accumulate rapidly in the water column, resulting in coloration of the surface water. It is usually found in coastal areas. It kills many manatees every year.
These algae, known as phytoplankton, are single-celled protists, plant-like organisms that can form dense, visible patches near the water's surface. Certain species of phytoplankton, dinoflagellates, contain photosynthetic pigments that vary in color from green to brown to red.
When the algae are present in high concentrations, the water appears to be discolored or murky, varying in color from purple to almost pink, normally being red or green. Not all algal blooms are dense enough to cause water discoloration, and not all discolored waters associated with algal blooms are red. Additionally, red tides are not typically associated with tidal movement of water, hence the preference among scientists to use the term algal bloom.
Some red tides are associated with the production of natural toxins, depletion of dissolved oxygen or other harmful effects, and are generally described as harmful algal blooms. The most conspicuous effects of these kinds of red tides are the associated wildlife mortalities of marine and coastal species of fish, birds, marine mammals, and other organisms.
Overview.
Red tides in the Gulf of Mexico are a result of high concentrations of "Karenia brevis", a microscopic marine algae that occurs naturally but normally in lower concentrations. In high concentrations, its toxin paralyzes the central nervous system of fish so they cannot breathe. Dead fish wash up on Mexican gulf beaches. Dense concentrations appear as discolored water, often reddish in color. It is a natural phenomenon, but the exact cause or combination of factors that result in a red tide outbreak are unknown. Red tide causes economic harm and for this reason red tide outbreaks are carefully monitored. For example, the Florida Fish and Wildlife Conservation Commission provides an up-to-date status report on the red tide in Florida. Texas also provides a current status report.
Red tide is also potentially harmful to human health. Humans can become seriously ill from eating oysters and other shellfish contaminated with red tide toxin. "Karenia brevis" blooms can potentially cause eye and respiratory irritation (coughing, sneezing, tear production, and itching) to beachgoers, boaters and coastal residents. People with severe or persistent respiratory conditions (such as chronic lung disease or asthma) may experience stronger adverse reactions. The National Oceanic and Atmospheric Administration's National Ocean Service provides a public conditions report identifying possible respiratory irritation impacts in areas affected by red tides.
The debate over the cause of red tides is controversial. Red tides occur naturally off coasts all over the world. Not all red tides have toxins or are harmful.
Definition.
"Red tide" is a colloquial term used to refer to one of a variety of natural phenomena known as "harmful algal blooms" or "HABs". The term "red tide" specifically refers to blooms of a species of dinoflagellate known as "Karenia brevis". It is sometimes used to refer more broadly to other types of algal blooms as well.
The term "red tide" is being phased out among researchers for the following reasons:
As a technical term it is being replaced in favour of more precise terminology including the generic term "harmful algal bloom" for harmful species, and "algal bloom" for non-harmful species.
The term "red tide" is most often used in the United States of America to refer to "Karenia brevis" blooms in the eastern Gulf of Mexico, also called the "Florida red tide". These blooms occur almost annually along Florida waters. The density of these organisms during a bloom can exceed tens of millions of cells per litre of seawater, and often discolor the water a deep reddish-brown hue.
The term "red tide" is also sometimes used to describe harmful algal blooms on the northern east coast of the United States, particularly in the Gulf of Maine. This type of bloom is caused by another species of dinoflagellate known as "Alexandrium fundyense". These blooms of organisms cause severe disruptions in fisheries of these waters as the toxins in these organism cause filter-feeding shellfish in affected waters to become poisonous for human consumption due to saxitoxin. The related "Alexandrium monilatum" is found in subtropical or tropical shallow seas and estuaries in the western Atlantic Ocean, the Caribbean Sea, the Gulf of Mexico and the eastern Pacific Ocean.
Causes.
The occurrence of red tides in some locations appears to be entirely natural (algal blooms are a seasonal occurrence resulting from coastal upwelling, a natural result of the movement of certain ocean currents) while in others they appear to be a result of increased nutrient loading from human activities. The growth of marine phytoplankton is generally limited by the availability of nitrates and phosphates, which can be abundant in agricultural run-off as well as coastal upwelling zones. Coastal water pollution produced by humans and systematic increase in sea water temperature have also been implicated as contributing factors in red tides. Other factors such as iron-rich dust influx from large desert areas such as the Saharan desert are thought to play a major role in causing red tides. Some algal blooms on the Pacific coast have also been linked to occurrences of large-scale climatic oscillations such as El Niño events. While red tides in the Gulf of Mexico have been occurring since the time of early explorers such as Cabeza de Vaca, it is unclear what initiates these blooms and how large a role anthropogenic and natural factors play in their development. It is also debated whether the apparent increase in frequency and severity of algal blooms in various parts of the world is in fact a real increase or is due to increased observation effort and advances in species identification methods.

</doc>
<doc id="25452" url="http://en.wikipedia.org/wiki?curid=25452" title="Richard Wagner">
Richard Wagner

Wilhelm Richard Wagner (; ]; 22 May 1813 – 13 February 1883) was a German composer, theatre director, polemicist, and conductor who is primarily known for his operas (or, as some of his later works were later known, "music dramas"). Unlike most opera composers, Wagner wrote both the libretto and the music for each of his stage works. Initially establishing his reputation as a composer of works in the romantic vein of Weber and Meyerbeer, Wagner revolutionised opera through his concept of the "Gesamtkunstwerk" ("total work of art"), by which he sought to synthesise the poetic, visual, musical and dramatic arts, with music subsidiary to drama, and which was announced in a series of essays between 1849 and 1852. Wagner realised these ideas most fully in the first half of the four-opera cycle "Der Ring des Nibelungen" ("The Ring of the Nibelung").
His compositions, particularly those of his later period, are notable for their complex textures, rich harmonies and orchestration, and the elaborate use of leitmotifs—musical phrases associated with individual characters, places, ideas or plot elements. His advances in musical language, such as extreme chromaticism and quickly shifting tonal centres, greatly influenced the development of classical music. His "Tristan und Isolde" is sometimes described as marking the start of modern music.
Wagner had his own opera house built, the Bayreuth Festspielhaus, which embodied many novel design features. It was here that the "Ring" and "Parsifal" received their premieres and where his most important stage works continue to be performed in an annual festival run by his descendants. His thoughts on the relative contributions of music and drama in opera were to change again, and he reintroduced some traditional forms into his last few stage works, including "Die Meistersinger von Nürnberg" ("The Mastersingers of Nuremberg").
Until his final years, Wagner's life was characterised by political exile, turbulent love affairs, poverty and repeated flight from his creditors. His controversial writings on music, drama and politics have attracted extensive comment in recent decades, especially where they express antisemitic sentiments. The effect of his ideas can be traced in many of the arts throughout the 20th century; their influence spread beyond composition into conducting, philosophy, literature, the visual arts and theatre.
Biography.
Early years.
Richard Wagner was born in Leipzig, at No. 3, the Brühl ("The House of the Red and White Lions"), in the Jewish quarter. He was however an ethnic German, the ninth child of Carl Friedrich Wagner, who was a clerk in the Leipzig police service, and his wife, Johanna Rosine (née Paetz), the daughter of a baker. Wagner's father Carl died of typhus six months after Richard's birth, after which Johanna began living with Carl's friend, the actor and playwright Ludwig Geyer. In August 1814 Johanna and Geyer probably married—although no documentation of this has been found in the Leipzig church registers. She and her family moved to Geyer's residence in Dresden. Until he was fourteen, Wagner was known as Wilhelm Richard Geyer. He almost certainly thought that Geyer was his biological father.
Geyer's love of the theatre came to be shared by his stepson, and Wagner took part in his performances. In his autobiography "Mein Leben", Wagner recalled once playing the part of an angel. In late 1820, Wagner was enrolled at Pastor Wetzel's school at Possendorf, near Dresden, where he received a little piano instruction from his Latin teacher. He struggled to play a proper scale at the keyboard and preferred playing theatre overtures by ear. Following Geyer's death in 1821, Richard was sent to the Kreuzschule, the boarding school of the Dresdner Kreuzchor, at the expense of Geyer's brother. At the age of nine he was hugely impressed by the Gothic elements of Carl Maria von Weber's opera "Der Freischütz", which he saw Weber conduct. At this period Wagner entertained ambitions as a playwright. His first creative effort, listed in the "Wagner-Werk-Verzeichnis" (the standard listing of Wagner's works) as WWV 1, was a tragedy called "Leubald". Begun at school in 1826, it was strongly influenced by Shakespeare and Goethe. Wagner was determined to set it to music, and persuaded his family to allow him music lessons.
By 1827, the family had returned to Leipzig. Wagner's first lessons in harmony were taken during 1828–31 with Christian Gottlieb Müller. In January 1828 he first heard Beethoven's 7th Symphony and then, in March, the same composer's 9th Symphony (both at the Gewandhaus). Beethoven became a major inspiration, and Wagner wrote a piano transcription of the 9th Symphony. He was also greatly impressed by a performance of Mozart's Requiem. Wagner's early piano sonatas and his first attempts at orchestral overtures date from this period.
In 1829 he saw a performance by dramatic soprano Wilhelmine Schröder-Devrient, and she became his ideal of the fusion of drama and music in opera. In "Mein Leben", Wagner wrote "When I look back across my entire life I find no event to place beside this in the impression it produced on me", and claimed that the "profoundly human and ecstatic performance of this incomparable artist" kindled in him an "almost demonic fire."
In 1831, Wagner enrolled at the Leipzig University, where he became a member of the Saxon student fraternity. He also took composition lessons with the Thomaskantor Theodor Weinlig. Weinlig was so impressed with Wagner's musical ability that he refused any payment for his lessons. He arranged for his pupil's Piano Sonata in B-flat major (which was consequently dedicated to him) to be published as Wagner's Op. 1. A year later, Wagner composed his Symphony in C major, a Beethovenesque work performed in Prague in 1832 and at the Leipzig Gewandhaus in 1833. He then began to work on an opera, "Die Hochzeit" ("The Wedding"), which he never completed.
Early career (1833–42).
In 1833, Wagner's brother Albert managed to obtain for him a position as choir master at the theatre in Würzburg. In the same year, at the age of 20, Wagner composed his first complete opera, "Die Feen" ("The Fairies"). This work, which imitated the style of Weber, went unproduced until half a century later, when it was premiered in Munich shortly after the composer's death in 1883.
Having returned to Leipzig in 1834, Wagner held a brief appointment as musical director at the opera house in Magdeburg during which he wrote "Das Liebesverbot" ("The Ban on Love"), based on Shakespeare's "Measure for Measure". This was staged at Magdeburg in 1836 but closed before the second performance; this, together with the financial collapse of the theatre company employing him, left the composer with serious money problems. Wagner had fallen for one of the leading ladies at Magdeburg, the actress Christine Wilhelmine "Minna" Planer. After the disaster of "Das Liebesverbot", he followed her to Königsberg where she helped him to get an engagement at the theatre. The two married in Tragheim Church on 24 November 1836. In May 1837, Minna left Wagner for another man; this was but the first débâcle of a troubled marriage. In June 1837, Wagner moved to Riga (then in the Russian Empire), where he became music director of the local opera; having in this capacity engaged Minna's sister Amalie (also a singer) for the theatre, he presently resumed relations with Minna during 1838.
By 1839, the couple had amassed such large debts that they fled Riga to avoid their creditors; debt would plague Wagner for most of his life. Initially they took a stormy sea passage to London, from which Wagner drew the inspiration for "Der fliegende Holländer" ("The Flying Dutchman"), with a plot based on a sketch by Heinrich Heine. The Wagners arrived in Paris in September 1839 and stayed there until 1842. Richard made a scant living writing articles and arranging operas by other composers, largely on behalf of the Schlesinger publishing house. He also completed during this stay his third and fourth operas "Rienzi" and "Der fliegende Holländer".
Dresden (1842–49).
Wagner had completed "Rienzi" in 1840. With the strong support of Giacomo Meyerbeer, it was accepted for performance by the Dresden Court Theatre ("Hofoper") in the Kingdom of Saxony and in 1842, Wagner moved to Dresden. His relief at returning to Germany was recorded in his "Autobiographic Sketch" of 1842, where he wrote that, en route from Paris, "For the first time I saw the Rhine—with hot tears in my eyes, I, poor artist, swore eternal fidelity to my German fatherland." "Rienzi" was staged to considerable acclaim on 20 October.
Wagner lived in Dresden for the next six years, eventually being appointed the Royal Saxon Court Conductor. During this period, he staged there "Der fliegende Holländer" (2 January 1843) and "Tannhäuser" (19 October 1845), the first two of his three middle-period operas. Wagner also mixed with artistic circles in Dresden, including the composer Ferdinand Hiller and the architect Gottfried Semper.
Wagner's involvement in left-wing politics abruptly ended his welcome in Dresden. Wagner was active among socialist German nationalists there, regularly receiving such guests as the conductor and radical editor August Röckel and the Russian anarchist Mikhail Bakunin. He was also influenced by the ideas of Pierre-Joseph Proudhon and Ludwig Feuerbach. Widespread discontent came to a head in 1849, when the unsuccessful May Uprising in Dresden broke out, in which Wagner played a minor supporting role. Warrants were issued for the revolutionaries' arrest. Wagner had to flee, first visiting Paris and then settling in Zürich.
In exile: Switzerland (1849–58).
Wagner was to spend the next twelve years in exile from Germany. He had completed "Lohengrin", the last of his middle-period operas, before the Dresden uprising, and now wrote desperately to his friend Franz Liszt to have it staged in his absence. Liszt conducted the premiere in Weimar in August 1850.
Nevertheless, Wagner was in grim personal straits, isolated from the German musical world and without any regular income. In 1850, Julie, the wife of his friend Karl Ritter, began to pay him a small pension which she maintained until 1859. With help from her friend Jessie Laussot this was to have been augmented to an annual sum of 3000 Thalers per year; but this plan was abandoned when Wagner began an affair with Mme. Laussot. Wagner even planned an elopement with her in 1852, which her husband prevented. Meanwhile, Wagner's wife Minna, who had disliked the operas he had written after "Rienzi", was falling into a deepening depression. Wagner fell victim to ill-health, according to Ernest Newman "largely a matter of overwrought nerves", which made it difficult for him to continue writing.
Wagner's primary published output during his first years in Zürich was a set of essays. In "The Artwork of the Future" (1849), he described a vision of opera as "Gesamtkunstwerk" ("total work of art"), in which the various arts such as music, song, dance, poetry, visual arts and stagecraft were unified. "Judaism in Music" (1850) was the first of Wagner's writings to feature antisemitic views. In this polemic Wagner argued, frequently using traditional antisemitic abuse, that Jews had no connection to the German spirit, and were thus only capable of producing shallow and artificial music. According to him, they composed music to achieve popularity and, thereby, financial success, as opposed to creating genuine works of art.
In "Opera and Drama" (1851), Wagner described the aesthetics of drama that he was using to create the "Ring" operas. Before leaving Dresden, Wagner had drafted a scenario that eventually became the four-opera cycle "Der Ring des Nibelungen". He initially for a single opera, "Siegfrieds Tod" ("Siegfried's Death"), in 1848. After arriving in Zürich he expanded the story with the opera "Der junge Siegfried" ("Young Siegfried"), which explored the hero's background. He completed the text of the cycle by writing the libretti for "Die Walküre" ("The Valkyrie") and "Das Rheingold" ("The Rhine Gold") and revising the other libretti to agree with his new concept, completing them in 1852. The concept of opera expressed in "Opera and Drama" and in other essays effectively renounced the operas he had previously written, up to and including "Lohengrin". Partly in an attempt to explain his change of views, Wagner published in 1851 the autobiographical "A Communication to My Friends". This contained his first public announcement of what was to become the "Ring" cycle:
I shall never write an "Opera" more. As I have no wish to invent an arbitrary title for my works, I will call them Dramas ...
I propose to produce my myth in three complete dramas, preceded by a lengthy Prelude (Vorspiel). ...
At a specially-appointed Festival, I propose, some future time, to produce those three Dramas with their Prelude, "in the course of three days and a fore-evening" [emphasis in original].
Wagner began composing the music for "Das Rheingold" between November 1853 and September 1854, following it immediately with "Die Walküre" (written between June 1854 and March 1856). He began work on the third "Ring" opera, which he now called simply "Siegfried", probably in September 1856, but by June 1857 he had completed only the first two acts before deciding to put the work aside to concentrate on a new idea: "Tristan und Isolde", based on the Arthurian love story "Tristan and Iseult".
One source of inspiration for "Tristan und Isolde" was the philosophy of Arthur Schopenhauer, notably his "The World as Will and Representation", to which Wagner had been introduced in 1854 by his poet friend Georg Herwegh. Wagner later called this the most important event of his life. His personal circumstances certainly made him an easy convert to what he understood to be Schopenhauer's philosophy, a deeply pessimistic view of the human condition. He remained an adherent of Schopenhauer for the rest of his life.
One of Schopenhauer's doctrines was that music held a supreme role in the arts as a direct expression of the world's essence, namely, blind, impulsive will. This doctrine contradicted Wagner's view, expressed in "Opera and Drama", that the music in opera had to be subservient to the drama. Wagner scholars have argued that Schopenhauer's influence caused Wagner to assign a more commanding role to music in his later operas, including the latter half of the "Ring" cycle, which he had yet to compose. Aspects of Schopenhauerian doctrine found their way into Wagner's subsequent libretti.
A second source of inspiration was Wagner's infatuation with the poet-writer Mathilde Wesendonck, the wife of the silk merchant Otto Wesendonck. Wagner met the Wesendoncks, who were both great admirers of his music, in Zürich in 1852. From May 1853 onwards Wesendonck made several loans to Wagner to finance his household expenses in Zürich, and in 1857 placed a cottage on his estate at Wagner's disposal, which became known as the "Asyl" ("asylum" or "place of rest"). During this period, Wagner's growing passion for his patron's wife inspired him to put aside work on the "Ring" cycle (which was not resumed for the next twelve years) and begin work on "Tristan". While planning the opera, Wagner composed the "Wesendonck Lieder", five songs for voice and piano, setting poems by Mathilde. Two of these settings are explicitly subtitled by Wagner as "studies for "Tristan und Isolde"".
Amongst the conducting engagements that Wagner undertook for revenue during this period, he gave several concerts in 1855 with the London Philharmonic Society, including one before Queen Victoria. The Queen enjoyed his "Tannhäuser" overture and spoke with Wagner after the concert, writing of him in her diary that he was "short, very quiet, wears spectacles & has a very finely-developed forehead, a hooked nose & projecting chin."
In exile: Venice and Paris (1858–62).
Wagner's uneasy affair with Mathilde collapsed in 1858, when Minna intercepted a letter to Mathilde from him. After the resulting confrontation with Minna, Wagner left Zürich alone, bound for Venice, where he rented an apartment in the Palazzo Giustinian, while Minna returned to Germany. Wagner's attitude to Minna had changed; the editor of his correspondence with her, John Burk, has said that she was to him "an invalid, to be treated with kindness and consideration, but, except at a distance, [was] a menace to his peace of mind." Wagner continued his correspondence with Mathilde and his friendship with her husband Otto, who maintained his financial support of the composer. In an 1859 letter to Mathilde, Wagner wrote, half-satirically, of "Tristan": "Child! This Tristan is turning into something "terrible". This final act!!!—I fear the opera will be banned ... only mediocre performances can save me! Perfectly good ones will be bound to drive people mad."
In November 1859, Wagner once again moved to Paris to oversee production of a new revision of "Tannhäuser", staged thanks to the efforts of Princess Pauline von Metternich, whose husband was the Austrian ambassador in Paris. The performances of the Paris "Tannhäuser" in 1861 were a notable fiasco. This was partly a consequence of the conservative tastes of the Jockey Club, which organised demonstrations in the theatre to protest at the presentation of the ballet feature in act 1 (instead of its traditional location in the second act); but the opportunity was also exploited by those who wanted to use the occasion as a veiled political protest against the pro-Austrian policies of Napoleon III. The work was withdrawn after the third performance and Wagner left Paris soon after. He had sought a reconciliation with Minna during this Paris visit, and although she joined him there, the reunion was not successful and they again parted from each other when Wagner left.
Return and resurgence (1862–71).
The political ban that had been placed on Wagner in Germany after he had fled Dresden was fully lifted in 1862. The composer settled in Biebrich in Prussia. Here Minna visited him for the last time: they parted irrevocably, though Wagner continued to give financial support to her while she lived in Dresden until her death in 1866.
In Biebrich, Wagner at last began work on "Die Meistersinger von Nürnberg", his only mature comedy. Wagner wrote a first draft of the libretto in 1845, and he had resolved to develop it during a visit he had made to Venice with the Wesendoncks in 1860, where he was inspired by Titian's painting "The Assumption of the Virgin". Throughout this period (1861–64) Wagner sought to have "Tristan und Isolde" produced in Vienna. Despite numerous rehearsals, the opera remained unperformed, and gained a reputation as being "impossible" to sing, which added to Wagner's financial problems.
Wagner's fortunes took a dramatic upturn in 1864, when King Ludwig II succeeded to the throne of Bavaria at the age of 18. The young king, an ardent admirer of Wagner's operas, had the composer brought to Munich. The King, who was homosexual, expressed in his correspondence a passionate personal adoration for the composer, and Wagner in his responses had no scruples about counterfeiting a similar atmosphere. Ludwig settled Wagner's considerable debts, and proposed to stage "Tristan", "Die Meistersinger", the "Ring", and the other operas Wagner planned. Wagner also began to dictate his autobiography, "Mein Leben", at the King's request. Wagner noted that his rescue by Ludwig coincided with news of the death of his earlier mentor (but later supposed enemy) Giacomo Meyerbeer, and regretted that "this operatic master, who had done me so much harm, should not have lived to see this day."
After grave difficulties in rehearsal, "Tristan und Isolde" premiered at the National Theatre Munich on 10 June 1865, the first Wagner opera premiere in almost 15 years. (The premiere had been scheduled for 15 May, but was delayed by bailiffs acting for Wagner's creditors, and also because the Isolde, Malvina Schnorr von Carolsfeld, was hoarse and needed time to recover.) The conductor of this premiere was Hans von Bülow, whose wife, Cosima, had given birth in April that year to a daughter, named Isolde, a child not of Bülow but of Wagner.
Cosima was 24 years younger than Wagner and was herself illegitimate, the daughter of the Countess Marie d'Agoult, who had left her husband for Franz Liszt. Liszt initially disapproved of his daughter's involvement with Wagner, though nevertheless the two men were friends. The indiscreet affair scandalised Munich, and Wagner also fell into disfavour with many leading members of the court, who were suspicious of his influence on the King. In December 1865, Ludwig was finally forced to ask the composer to leave Munich. He apparently also toyed with the idea of abdicating to follow his hero into exile, but Wagner quickly dissuaded him.
Ludwig installed Wagner at the Villa Tribschen, beside Switzerland's Lake Lucerne. "Die Meistersinger" was completed at Tribschen in 1867, and premiered in Munich on 21 June the following year. At Ludwig's insistence, "special previews" of the first two works of the "Ring", "Das Rheingold" and "Die Walküre", were performed at Munich in 1869 and 1870, but Wagner retained his dream, first expressed in "A Communication to My Friends", to present the first complete cycle at a special festival with a new, dedicated, opera house.
Minna had died of a heart attack on 25 January 1866 in Dresden. Wagner did not attend the funeral. Following Minna's death Cosima wrote to Hans von Bülow on a number of occasions asking him to grant her a divorce, but Bülow refused to concede this. He only consented after she had two more children with Wagner; another daughter, named Eva, after the heroine of "Meistersinger", and a son Siegfried, named for the hero of the "Ring". The divorce was finally sanctioned, after delays in the legal process, by a Berlin court on 18 July 1870. Richard and Cosima's wedding took place on 25 August 1870. On Christmas Day of that year, Wagner arranged a surprise performance (its premiere) of the "Siegfried Idyll" for Cosima's birthday. The marriage to Cosima lasted to the end of Wagner's life.
Wagner, settled into his new-found domesticity, turned his energies towards completing the "Ring" cycle. He had not abandoned polemics: he republished his 1850 pamphlet "Judaism in Music", originally issued under a pseudonym, under his own name in 1869. He extended the introduction, and wrote a lengthy additional final section. The publication led to several public protests at early performances of "Die Meistersinger" in Vienna and Mannheim.
Bayreuth (1871–76).
In 1871, Wagner decided to move to Bayreuth, which was to be the location of his new opera house. The town council donated a large plot of land—the "Green Hill"—as a site for the theatre. The Wagners moved to the town the following year, and the foundation stone for the Bayreuth Festspielhaus ("Festival Theatre") was laid. Wagner initially announced the first Bayreuth Festival, at which for the first time the "Ring" cycle would be presented complete, for 1873, but since Ludwig had declined to finance the project, the start of building was delayed and the proposed date for the festival was deferred. To raise funds for the construction, "Wagner societies" were formed in several cities, and Wagner began touring Germany conducting concerts. By the spring of 1873, only a third of the required funds had been raised; further pleas to Ludwig were initially ignored, but early in 1874, with the project on the verge of collapse, the King relented and provided a loan. The full building programme included the family home, "Wahnfried", into which Wagner, with Cosima and the children, moved from their temporary accommodation on 18 April 1874. The theatre was completed in 1875, and the festival scheduled for the following year. Commenting on the struggle to finish the building, Wagner remarked to Cosima: "Each stone is red with my blood and yours".
For the design of the Festspielhaus, Wagner appropriated some of the ideas of his former colleague, Gottfried Semper, which he had previously solicited for a proposed new opera house at Munich. Wagner was responsible for several theatrical innovations at Bayreuth; these include darkening the auditorium during performances, and placing the orchestra in a pit out of view of the audience.
The Festspielhaus finally opened on 13 August 1876 with "Das Rheingold", at last taking its place as the first evening of the complete "Ring" cycle; the 1876 Bayreuth Festival therefore saw the premiere of the complete cycle, performed as a sequence as the composer had intended. The 1876 Festival consisted of three full "Ring" cycles (under the baton of Hans Richter). At the end, critical reactions ranged between that of the Norwegian composer Edvard Grieg, who thought the work "divinely composed", and that of the French newspaper "Le Figaro", which called the music "the dream of a lunatic". Amongst the disillusioned were Wagner's friend and disciple Friedrich Nietzsche, who, having published his eulogistic essay "Richard Wagner in Bayreuth" before the festival as part of his "Untimely Meditations", was bitterly disappointed by what he saw as Wagner's pandering to increasingly exclusivist German nationalism; his breach with Wagner began at this time. The festival firmly established Wagner as an artist of European, and indeed world, importance: attendees included Kaiser Wilhelm I, the Emperor Pedro II of Brazil, Anton Bruckner, Camille Saint-Saëns and Pyotr Ilyich Tchaikovsky.
Wagner was far from satisfied with the Festival; Cosima recorded that months later, his attitude towards the productions was "Never again, never again!" Moreover, the festival finished with a deficit of about 150,000 marks. The expenses of Bayreuth and of Wahnfried meant that Wagner still sought additional sources of income by conducting or taking on commissions such as the "Centennial March" for America, for which he received $5000.
Last years (1876–83).
Following the first Bayreuth Festival, Wagner began work on "Parsifal", his final opera. The composition took four years, much of which Wagner spent in Italy for health reasons. From 1876 to 1878 Wagner also embarked on the last of his documented emotional liaisons, this time with Judith Gautier, whom he had met at the 1876 Festival. Wagner was also much troubled by problems of financing "Parsifal", and by the prospect of the work being performed by other theatres than Bayreuth. He was once again assisted by the liberality of King Ludwig, but was still forced by his personal financial situation in 1877 to sell the rights of several of his unpublished works (including the "Siegfried Idyll") to the publisher Schott.
Wagner wrote a number of articles in his later years, often on political topics, and often reactionary in tone, repudiating some of his earlier, more liberal, views. These include "Religion and Art" (1880) and "Heroism and Christianity" (1881), which were printed in the journal "Bayreuther Blätter", published by his supporter Hans von Wolzogen. Wagner's sudden interest in Christianity at this period, which infuses "Parsifal", was contemporary with his increasing alignment with German nationalism, and required on his part, and the part of his associates, "the rewriting of some recent Wagnerian history", so as to represent, for example, the "Ring" as a work reflecting Christian ideals. Many of these later articles, including "What is German?" (1878, but based on a draft written in the 1860s), repeated Wagner's antisemitic preoccupations.
Wagner completed "Parsifal" in January 1882, and a second Bayreuth Festival was held for the new opera, which premiered on 26 May. Wagner was by this time extremely ill, having suffered a series of increasingly severe angina attacks. During the sixteenth and final performance of "Parsifal" on 29 August, he entered the pit unseen during act 3, took the baton from conductor Hermann Levi, and led the performance to its conclusion.
After the festival, the Wagner family journeyed to Venice for the winter. Wagner died of a heart attack at the age of 69 on 13 February 1883 at Ca' Vendramin Calergi, a 16th-century palazzo on the Grand Canal. The legend that the attack was prompted by argument with Cosima over Wagner's supposedly amorous interest in the singer Carrie Pringle, who had been a Flower-maiden in "Parsifal" at Bayreuth, is without credible evidence. After a funerary gondola bore Wagner's remains over the Grand Canal, his body was taken to Germany where it was buried in the garden of the Villa Wahnfried in Bayreuth.
Works.
Wagner's musical output is listed by the "Wagner-Werk-Verzeichnis" (WWV) as comprising 113 works, including fragments and projects.
Operas.
Wagner's operatic works are his primary artistic legacy. Unlike most opera composers, who generally left the task of writing the libretto (the text and lyrics) to others, Wagner wrote his own libretti, which he referred to as "poems".
From 1849 onwards, he urged a new concept of opera often referred to as "music drama" (although he later rejected this term), in which all musical, poetic and dramatic elements were to be fused together—the "Gesamtkunstwerk". Wagner developed a compositional style in which the importance of the orchestra is equal to that of the singers. The orchestra's dramatic role in the later operas includes the use of leitmotifs, musical phrases that can be interpreted as announcing specific characters, locales, and plot elements; their complex interweaving and evolution illuminates the progression of the drama. These operas are still, despite Wagner's reservations, referred to by many writers as "music dramas".
Early works (to 1842).
Wagner's earliest attempts at opera were often uncompleted. Abandoned works include a pastoral opera based on Goethe's "Die Laune des Verliebten" ("The Infatuated Lover's Caprice"), written at the age of 17, "Die Hochzeit" ("The Wedding"), on which Wagner worked in 1832, and the singspiel "Männerlist größer als Frauenlist" ("Men are More Cunning than Women", 1837–38). "Die Feen" ("The Fairies", 1833) was unperformed in the composer's lifetime and "Das Liebesverbot" ("The Ban on Love", 1836) was withdrawn after its first performance. "Rienzi" (1842) was Wagner's first opera to be successfully staged. The compositional style of these early works was conventional—the relatively more sophisticated "Rienzi" showing the clear influence of Grand Opera "à la" Spontini and Meyerbeer—and did not exhibit the innovations that would mark Wagner's place in musical history. Later in life, Wagner said that he did not consider these works to be part of his "oeuvre"; none of them has ever been performed at the Bayreuth Festival, and they have been performed only rarely in the last hundred years (although the overture to "Rienzi" is an occasional concert piece). "Die Feen", "Das Liebesverbot" and "Rienzi" were performed at both Leipzig and Bayreuth in 2013 to mark the composer's bicentenary.
"Romantic operas" (1843–51).
Wagner's middle stage output began with "Der fliegende Holländer" ("The Flying Dutchman", 1843), followed by "Tannhäuser" (1845) and "Lohengrin" (1850). These three operas are sometimes referred to as Wagner's "romantic operas". They reinforced the reputation, among the public in Germany and beyond, that Wagner had begun to establish with "Rienzi". Although distancing himself from the style of these operas from 1849 onwards, he nevertheless reworked both "Der fliegende Holländer" and "Tannhäuser" on several occasions. These three operas are considered to represent a significant developmental stage in Wagner's musical and operatic maturity as regards thematic handling, portrayal of emotions and orchestration. They are the earliest works included in the Bayreuth canon, the mature operas that Cosima staged at the Bayreuth Festival after Wagner's death in accordance with his wishes. All three (including the differing versions of "Der fliegende Holländer" and "Tannhäuser") continue to be regularly performed throughout the world, and have been frequently recorded. They were also the operas by which his fame spread during his lifetime.
"Music dramas" (1851–82).
Starting the "Ring".
Wagner's late dramas are considered his masterpieces. "Der Ring des Nibelungen", commonly referred to as the "Ring" or ""Ring" cycle", is a set of four operas based loosely on figures and elements of Germanic mythology—particularly from the later Norse mythology—notably the Old Norse "Poetic Edda" and "Volsunga Saga", and the Middle High German "Nibelungenlied". Wagner specifically developed the libretti for these operas according to his interpretation of "Stabreim", highly alliterative rhyming verse-pairs used in old Germanic poetry. They were also influenced by Wagner's concepts of ancient Greek drama, in which tetralogies were a component of Athenian festivals, and which he had amply discussed in his essay "Oper und Drama".
The first two components of the "Ring" cycle were "Das Rheingold" ("The Rhinegold"), which was completed in 1854, and "Die Walküre" ("The Valkyrie"), which was finished in 1856. In "Das Rheingold", with its "relentlessly talky 'realism' [and] the absence of lyrical 'numbers'", Wagner came very close to the musical ideals of his 1849–51 essays. "Die Walküre", which contains what is virtually a traditional aria (Siegmund's "Winterstürme" in the first act), and the quasi-choral appearance of the Valkyries themselves, shows more "operatic" traits, but has been assessed by Barry Millington as "the music drama that most satisfactorily embodies the theoretical principles of 'Oper und Drama'... A thoroughgoing synthesis of poetry and music is achieved without any notable sacrifice in musical expression."
"Tristan und Isolde" and "Die Meistersinger".
While composing the opera "Siegfried", the third part of the "Ring" cycle, Wagner interrupted work on it and between 1857 and 1864 wrote the tragic love story "Tristan und Isolde" and his only mature comedy "Die Meistersinger von Nürnberg" ("The Mastersingers of Nuremberg"), two works that are also part of the regular operatic canon.
"Tristan" is often granted a special place in musical history; many see it as the beginning of the move away from conventional harmony and tonality and consider that it lays the groundwork for the direction of classical music in the 20th century. Wagner felt that his musico-dramatical theories were most perfectly realised in this work with its use of "the art of transition" between dramatic elements and the balance achieved between vocal and orchestral lines. Completed in 1859, the work was given its first performance in Munich, conducted by Bülow, in June 1865.
"Die Meistersinger" was originally conceived by Wagner in 1845 as a sort of comic pendant to "Tannhäuser". Like "Tristan", it was premiered in Munich under the baton of Bülow, on 21 June 1868, and became an immediate success. Barry Millington describes "Meistersinger" as "a rich, perceptive music drama widely admired for its warm humanity"; but because of its strong German nationalist overtones, it is also cited by some as an example of Wagner's reactionary politics and antisemitism.
Completing the "Ring".
When Wagner returned to writing the music for the last act of "Siegfried" and for "Götterdämmerung" ("Twilight of the Gods"), as the final part of the "Ring", his style had changed once more to something more recognisable as "operatic" than the aural world of "Rheingold" and "Walküre", though it was still thoroughly stamped with his own originality as a composer and suffused with leitmotifs. This was in part because the libretti of the four "Ring" operas had been written in reverse order, so that the book for "Götterdämmerung" was conceived more "traditionally" than that of "Rheingold"; still, the self-imposed strictures of the "Gesamtkunstwerk" had become relaxed. The differences also result from Wagner's development as a composer during the period in which he wrote "Tristan", "Meistersinger" and the Paris version of "Tannhäuser". From act 3 of "Siegfried" onwards, the "Ring" becomes more chromatic melodically, more complex harmonically and more developmental in its treatment of leitmotifs.
Wagner took 26 years from writing the first draft of a libretto in 1848 until he completed "Götterdämmerung" in 1874. The "Ring" takes about 15 hours to perform and is the only undertaking of such size to be regularly presented on the world's stages.
"Parsifal".
Wagner's final opera, "Parsifal" (1882), which was his only work written especially for his Bayreuth Festspielhaus and which is described in the score as a "Bühnenweihfestspiel" ("festival play for the consecration of the stage"), has a storyline suggested by elements of the legend of the Holy Grail. It also carries elements of Buddhist renunciation suggested by Wagner's readings of Schopenhauer. Wagner described it to Cosima as his "last card". It remains controversial because of its treatment of Christianity, its eroticism, and its expression, as perceived by some commentators, of German nationalism and antisemitism. Despite the composer's own description of the opera to King Ludwig as "this most Christian of works", Ulrike Kienzle has commented that "Wagner's turn to Christian mythology, upon which the imagery and spiritual contents of "Parsifal" rest, is idiosyncratic and contradicts Christian dogma in many ways." Musically the opera has been held to represent a continuing development of the composer's style, and Barry Millington describes it as "a diaphanous score of unearthly beauty and refinement".
Non-operatic music.
Apart from his operas, Wagner composed relatively few pieces of music. These include a symphony in C major (written at the age of 19), the "Faust Overture" (the only completed part of an intended symphony on the subject), some overtures, and choral and piano pieces. His most commonly performed work that is not an extract from an opera is the "Siegfried Idyll" for chamber orchestra, which has several motifs in common with the "Ring" cycle. The "Wesendonck Lieder" are also often performed, either in the original piano version, or with orchestral accompaniment. More rarely performed are the "American Centennial March" (1876), and "Das Liebesmahl der Apostel" ("The Love Feast of the Apostles"), a piece for male choruses and orchestra composed in 1843 for the city of Dresden.
After completing "Parsifal", Wagner expressed his intention to turn to the writing of symphonies, and several sketches dating from the late 1870s and early 1880s have been identified as work towards this end. The overtures and certain orchestral passages from Wagner's middle and late-stage operas are commonly played as concert pieces. For most of these, Wagner wrote or rewrote short passages to ensure musical coherence. The "Bridal Chorus" from "Lohengrin" is frequently played as the bride's processional wedding march in English-speaking countries.
Prose writings.
Wagner was an extremely prolific writer, authoring numerous books, poems, and articles, as well as voluminous correspondence. His writings covered a wide range of topics, including autobiography, politics, philosophy, and detailed analyses of his own operas.
Wagner planned for a collected edition of his publications as early as 1865; he believed that such an edition would help the world understand his intellectual development and artistic aims. The first such edition was published between 1871 and 1883, but was doctored to suppress or alter articles that were an embarrassment to him (e.g. those praising Meyerbeer), or by altering dates on some articles to reinforce Wagner's own account of his progress. Wagner's autobiography "Mein Leben" was originally published for close friends only in a very small edition (15–18 copies per volume) in four volumes between 1870 and 1880. The first public edition (with many passages suppressed by Cosima) appeared in 1911; the first attempt at a full edition (in German) appeared in 1963.
There have been modern complete or partial editions of Wagner's writings, including a centennial edition in German edited by Dieter Borchmeyer (which, however, omitted the essay "Das Judenthum in der Musik" and "Mein Leben"). The English translations of Wagner's prose in eight volumes by W. Ashton Ellis (1892–99) are still in print and commonly used, despite their deficiencies. A complete edition of Wagner's correspondence, estimated to amount to between 10,000 and 12,000 items, is still under way under the supervision of the Institute for Music Research at the University of Würzburg. As of November 2014, 21 volumes have appeared, covering the period to 1870.
Influence and legacy.
Influence on music.
Wagner's later musical style introduced new ideas in harmony, melodic process (leitmotif) and operatic structure. Notably from "Tristan und Isolde" onwards, he explored the limits of the traditional tonal system, which gave keys and chords their identity, pointing the way to atonality in the 20th century. Some music historians date the beginning of modern classical music to the first notes of "Tristan", which include the so-called Tristan chord.
Wagner inspired great devotion. For a long period, many composers were inclined to align themselves with or against Wagner's music. Anton Bruckner and Hugo Wolf were greatly indebted to him, as were César Franck, Henri Duparc, Ernest Chausson, Jules Massenet, Richard Strauss, Alexander von Zemlinsky, Hans Pfitzner and numerous others. Gustav Mahler was devoted to Wagner and his music; aged 15, he sought him out on his 1875 visit to Vienna, became a renowned Wagner conductor, and his compositions are seen by Richard Taruskin as extending Wagner's "maximalization" of "the temporal and the sonorous" in music to the world of the symphony. The harmonic revolutions of Claude Debussy and Arnold Schoenberg (both of whose "oeuvres" contain examples of tonal and atonal modernism) have often been traced back to "Tristan" and "Parsifal". The Italian form of operatic realism known as verismo owed much to the Wagnerian concept of musical form.
Wagner made a major contribution to the principles and practice of conducting. His essay "About Conducting" (1869) advanced Hector Berlioz's technique of conducting and claimed that conducting was a means by which a musical work could be re-interpreted, rather than simply a mechanism for achieving orchestral unison. He exemplified this approach in his own conducting, which was significantly more flexible than the disciplined approach of Mendelssohn; in his view this also justified practices that would today be frowned upon, such as the rewriting of scores. Wilhelm Furtwängler felt that Wagner and Bülow, through their interpretative approach, inspired a whole new generation of conductors (including Furtwängler himself).
Amongst those claiming inspiration from Wagner's music are the German band Rammstein, and the electronic composer Klaus Schulze, whose 1975 album "Timewind" consists of two 30-minute tracks, "Bayreuth Return" and "Wahnfried 1883". Joey DeMaio of the band Manowar has described Wagner as "The father of heavy metal". The Slovenian group Laibach created the 2009 suite "VolksWagner", using material from Wagner's operas. Phil Spector's Wall of Sound recording technique was, it has been claimed, heavily influenced by Wagner.
Influence on literature, philosophy and the visual arts.
Wagner's influence on literature and philosophy is significant. Millington has commented:[Wagner's] protean abundance meant that he could inspire the use of literary motif in many a novel employing interior monologue; ... the Symbolists saw him as a mystic hierophant; the Decadents found many a frisson in his work.Friedrich Nietzsche was a member of Wagner's inner circle during the early 1870s, and his first published work, "The Birth of Tragedy", proposed Wagner's music as the Dionysian "rebirth" of European culture in opposition to Apollonian rationalist "decadence". Nietzsche broke with Wagner following the first Bayreuth Festival, believing that Wagner's final phase represented a pandering to Christian pieties and a surrender to the new German Reich. Nietzsche expressed his displeasure with the later Wagner in "The Case of Wagner" and "Nietzsche contra Wagner".
Charles Baudelaire, Stéphane Mallarmé and Paul Verlaine worshipped Wagner. Édouard Dujardin, whose influential novel "Les Lauriers sont coupés" is in the form of an interior monologue inspired by Wagnerian music, founded a journal dedicated to Wagner, "La Revue Wagnérienne", to which J. K. Huysmans and Téodor de Wyzewa contributed. In a list of major cultural figures influenced by Wagner, Bryan Magee includes D. H. Lawrence, Aubrey Beardsley, Romain Rolland, Gérard de Nerval, Pierre-Auguste Renoir, Rainer Maria Rilke and numerous others.
In the 20th century, W. H. Auden once called Wagner "perhaps the greatest genius that ever lived", while Thomas Mann and Marcel Proust were heavily influenced by him and discussed Wagner in their novels. He is also discussed in some of the works of James Joyce. Wagnerian themes inhabit T. S. Eliot's "The Waste Land", which contains lines from "Tristan und Isolde" and "Götterdämmerung" and Verlaine's poem on "Parsifal".
Many of Wagner's concepts, including his speculation about dreams, predated their investigation by Sigmund Freud. Wagner had publicly analysed the Oedipus myth before Freud was born in terms of its psychological significance, insisting that incestuous desires are natural and normal, and perceptively exhibiting the relationship between sexuality and anxiety. Georg Groddeck considered the "Ring" as the first manual of psychoanalysis.
Influence on cinema.
Wagner's concept of the use of leitmotifs and the integrated musical expression which they can enable has influenced many 20th and 21st century film scores. The critic Theodor Adorno has noted that the Wagnerian leitmotif "leads directly to cinema music where the sole function of the leitmotif is to announce heroes or situations so as to allow the audience to orient itself more easily". Amongst film scores citing Wagnerian themes are Francis Ford Coppola's "Apocalypse Now", which features a version of the "Ride of the Valkyries", Trevor Jones's soundtrack to John Boorman's film "Excalibur", and the 2011 films "A Dangerous Method" (dir. David Cronenberg) and "Melancholia" (dir. Lars von Trier). Hans-Jürgen Syberberg's 1977 film ""'s visual style and set design are strongly inspired by "Der Ring des Nibelungen", musical excerpts from which are frequently used in the film's soundtrack.
Opponents and supporters.
Not all reaction to Wagner was positive. For a time, German musical life divided into two factions, supporters of Wagner and supporters of Johannes Brahms; the latter, with the support of the powerful critic Eduard Hanslick (of whom Beckmesser in "Meistersinger" is in part a caricature) championed traditional forms and led the conservative front against Wagnerian innovations. They were supported by the conservative leanings of some German music schools, including the conservatories at Leipzig under Ignaz Moscheles and at Cologne under the direction of Ferdinand Hiller. Another Wagner detractor was the French composer Charles-Valentin Alkan, who wrote to Hiller after attending Wagner's Paris concert on 25 January 1860 at which Wagner conducted the overtures to "Der fliegende Holländer" and "Tannhäuser", the preludes to "Lohengrin" and "Tristan und Isolde", and six other extracts from "Tannhäuser" and "Lohengrin": "I had imagined that I was going to meet music of an innovative kind but was astonished to find a pale imitation of Berlioz... I do not like all the music of Berlioz while appreciating his marvellous understanding of certain instrumental effects... but here he was imitated and caricatured... Wagner is not a musician, he is a disease."
Even those who, like Debussy, opposed Wagner ("this old poisoner") could not deny his influence. Indeed, Debussy was one of many composers, including Tchaikovsky, who felt the need to break with Wagner precisely because his influence was so unmistakable and overwhelming. "Golliwogg's Cakewalk" from Debussy's "Children's Corner" piano suite contains a deliberately tongue-in-cheek quotation from the opening bars of "Tristan". Others who proved resistant to Wagner's operas included Gioachino Rossini, who said "Wagner has wonderful moments, and dreadful quarters of an hour." In the 20th century Wagner's music was parodied by Paul Hindemith and Hanns Eisler, among others.
Wagner's followers (known as Wagnerians or Wagnerites) have formed many societies dedicated to Wagner's life and work.
Film and stage portrayals.
Wagner has been the subject of many biographical films. The earliest was a silent film made by Carl Froelich in 1913 and featured in the title role the composer Giuseppe Becce, who also wrote the score for the film (as Wagner's music, still in copyright, was not available). Amongst other film portrayals of Wagner are: Alan Badel in "Magic Fire" (1955); Lyndon Brook in "Song Without End" (1960); Trevor Howard in "Ludwig" (1972); Paul Nicholas in "Lisztomania" (1975); and Richard Burton in "Wagner" (1983).
Jonathan Harvey's opera "Wagner Dream" (2007) intertwines the events surrounding Wagner's death with the story of Wagner's uncompleted opera outline "Die Sieger (The Victors)".
Bayreuth Festival.
Since Wagner's death, the Bayreuth Festival, which has become an annual event, has been successively directed by his widow, his son Siegfried, the latter's widow Winifred Wagner, their two sons Wieland and Wolfgang Wagner, and, presently, two of the composer's great-granddaughters, Eva Wagner-Pasquier and Katharina Wagner. Since 1973, the festival has been overseen by the Richard-Wagner-Stiftung (Richard Wagner Foundation), the members of which include a number of Wagner's descendants.
Controversies.
Wagner's operas, writings, politics, beliefs and unorthodox lifestyle made him a controversial figure during his lifetime. Following his death, debate about his ideas and their interpretation, particularly in Germany during the 20th century, has continued.
Racism and antisemitism.
Wagner's writings on the Jews corresponded to some existing trends of thought in Germany during the 19th century; however, despite his very public views on these themes, throughout his life Wagner had Jewish friends, colleagues and supporters. There have been frequent suggestions that antisemitic stereotypes are represented in Wagner's operas. The characters of Mime in the "Ring", Sixtus Beckmesser in "Die Meistersinger," and Klingsor in "Parsifal" are sometimes claimed as Jewish representations, though they are not identified as such in the librettos of these operas. The topic of Wagner and the Jews is further complicated by allegations, which may have been credited by Wagner, that he himself was of Jewish ancestry, via his supposed father Geyer.
Some biographers have asserted that Wagner in his final years came to believe in the racialist philosophy of Arthur de Gobineau, notably Gobineau's belief that Western society was doomed because of miscegenation between "superior" and "inferior" races. According to Robert Gutman, this theme is reflected in the opera "Parsifal". Other biographers (such as Lucy Beckett) believe that this is not true, as the original drafts of the story date back to 1857 and Wagner had completed the libretto for "Parsifal" by 1877; but he displayed no significant interest in Gobineau until 1880.
Other interpretations.
Wagner's ideas are amenable to socialist interpretations; many of his ideas on art were being formulated at the time of his revolutionary inclinations in the 1840s. Thus, for example, George Bernard Shaw wrote in "The Perfect Wagnerite" (1883):
[Wagner's] picture of Niblunghome under the reign of Alberic is a poetic vision of unregulated industrial capitalism as it was made known in Germany in the middle of the 19th century by Engels's book "The Condition of the Working Class in England".
Left-wing interpretations of Wagner also inform the writings of Theodor Adorno among other Wagner critics. Walter Benjamin gave Wagner as an example of "bourgeois false consciousness", alienating art from its social context.
The writer Robert Donington has produced a detailed, if controversial, Jungian interpretation of the "Ring" cycle, described as "an approach to Wagner by way of his symbols", which, for example, sees the character of the goddess Fricka as part of her husband Wotan's "inner femininity". Millington notes that Jean-Jacques Nattiez has also applied psychoanalytical techniques in an evaluation of Wagner's life and works.
Nazi appropriation.
Adolf Hitler was an admirer of Wagner's music and saw in his operas an embodiment of his own vision of the German nation; in a 1922 speech he claimed that Wagner's works glorified "the heroic Teutonic nature ... Greatness lies in the heroic." Hitler visited Bayreuth frequently from 1923 onwards and attended the productions at the theatre. There continues to be debate about the extent to which Wagner's views might have influenced Nazi thinking. Houston Stewart Chamberlain (1855–1927), who married Wagner's daughter Eva in 1908 but never met Wagner, was the author of the racist book "The Foundations of the Nineteenth Century", approved by the Nazi movement. Chamberlain met Hitler on a number of occasions between 1923 and 1927 in Bayreuth, but cannot credibly be regarded as a conduit of Wagner's own views. The Nazis used those parts of Wagner's thought that were useful for propaganda and ignored or suppressed the rest.
While Bayreuth presented a useful front for Nazi culture, and Wagner's music was used at many Nazi events, the Nazi hierarchy as a whole did not share Hitler's enthusiasm for Wagner's operas and resented attending these lengthy epics at Hitler's insistence.
Guido Fackler has researched evidence that indicates that it is possible that Wagner's music was used at the Dachau concentration camp in 1933–34 to "reeducate" political prisoners by exposure to "national music". There seems to be no evidence to support claims, sometimes made, that his music was played at Nazi death camps during the Second World War.
Because of the associations of Wagner with antisemitism and Nazism, the performance of his music in the State of Israel has been a source of controversy.
References.
Sources.
Prose works by Wagner
Other sources
</dl>

</doc>
<doc id="25453" url="http://en.wikipedia.org/wiki?curid=25453" title="Rheology">
Rheology

Rheology (; from Greek ῥέω "rhéō", "flow" and -λoγία, -"logia", "study of") is the study of the flow of matter, primarily in a liquid state, but also as 'soft solids' or solids under conditions in which they respond with plastic flow rather than deforming elastically in response to an applied force. 
It applies to substances which have a complex microstructure, such as muds, sludges, suspensions, polymers and other glass formers ("e.g.," silicates), as well as many foods and additives, bodily fluids ("e.g.," blood) and other biological materials or other materials which belong to the class of soft matter.
Newtonian fluids can be characterized by a single coefficient of viscosity for a specific temperature. Although this viscosity will change with temperature, it does not change with the strain rate. Only a small group of fluids exhibit such constant viscosity. The large class of fluids whose viscosity changes with the strain rate (the relative flow velocity) are called non-Newtonian fluids.
Rheology generally accounts for the behavior of non-Newtonian fluids, by characterizing the minimum number of functions that are needed to relate stresses with rate of change of strain or strain rates. For example, ketchup can have its viscosity reduced by shaking (or other forms of mechanical agitation, where the relative movement of different layers in the material actually causes the reduction in viscosity) but water cannot. Ketchup is a shear thinning material, like yoghurt and emulsion paint (US terminology latex paint or acrylic paint), exhibiting thixotropy, where an increase in relative flow velocity will cause a reduction in viscosity, for example, by stirring. Some other non-Newtonian materials show the opposite behavior: viscosity going up with relative deformation, which are called shear thickening or dilatant materials. Since Sir Isaac Newton originated the concept of viscosity, the study of liquids with strain rate dependent viscosity is also often called "Non-Newtonian fluid mechanics".
The term "rheology" was coined by Eugene C. Bingham, a professor at Lafayette College, in 1920, from a suggestion by a colleague, Markus Reiner. The term was inspired by the aphorism of Simplicius (often attributed to Heraclitus), "panta rhei," "everything flows"
The experimental characterization of a material's rheological behaviour is known as "rheometry", although the term "rheology" is frequently used synonymously with rheometry, particularly by experimentalists. Theoretical aspects of rheology are the relation of the flow/deformation behaviour of material and its internal structure (e.g., the orientation and elongation of polymer molecules), and the flow/deformation behaviour of materials that cannot be described by classical fluid mechanics or elasticity.
Scope.
In practice, rheology is principally concerned with extending continuum mechanics to characterize flow of materials, that exhibits a combination of elastic, viscous and plastic behavior by properly combining elasticity and (Newtonian) fluid mechanics. It is also concerned with establishing predictions for mechanical behavior (on the continuum mechanical scale) based on the micro- or nanostructure of the material, e.g. the molecular size and architecture of polymers in solution or the particle size distribution in a solid suspension.
Materials with the characteristics of a fluid will flow when subjected to a stress which is defined as the force per area. There are different sorts of stress (e.g. shear, torsional, etc.) and materials can respond differently for different stresses. Much of theoretical rheology is concerned with associating external forces and torques with internal stresses and internal strain gradients and flow velocities.
Rheology unites the seemingly unrelated fields of plasticity and non-Newtonian fluid dynamics by recognizing that materials undergoing these types of deformation are unable to support a stress (particularly a shear stress, since it is easier to analyze shear deformation) in static equilibrium. In this sense, a solid undergoing plastic deformation is a fluid, although no viscosity coefficient is associated with this flow. Granular rheology refers to the continuum mechanical description of granular materials.
One of the major tasks of rheology is to empirically establish the relationships between deformations (or rates of deformation) and stresses, by adequate measurements, although a number of theoretical developments (such as assuring frame invariants) are also required before using the empirical data. These experimental techniques are known as rheometry and are concerned with the determination with well-defined "rheological material functions". Such relationships are then amenable to mathematical treatment by the established methods of continuum mechanics.
The characterization of flow or deformation originating from a simple shear stress field is called shear rheometry (or shear rheology). The study of extensional flows is called extensional rheology. Shear flows are much easier to study and thus much more experimental data are available for shear flows than for extensional flows.
Rheologist.
A rheologist is an interdisciplinary scientist or engineer who studies the flow of complex liquids or the deformation of soft solids. It is not a primary degree subject; there is no qualification of rheologist as such. Most rheologists have a qualification in mathematics, the physical sciences (e.g. chemistry, physics, biology), engineering (e.g. mechanical, chemical, materials science and engineering or civil engineering), medicine, or certain technologies, notably materials or food. Typically, a small amount of rheology may be studied when obtaining a degree, but a person working in rheology will extend this knowledge during postgraduate research or by attending short courses and by joining a professional association (see below).
Applications.
Rheology has applications in materials science engineering, geophysics, physiology, human biology and pharmaceutics. Materials science is utilized in the production of many industrially important substances, such as cement, paint, and chocolate, which have complex flow characteristics. In addition, plasticity theory has been similarly important for the design of metal forming processes. The science of rheology and the characterization of viscoelastic properties in the production and use of polymeric materials has been critical for the production of many products for use in both the industrial and military sectors.
Study of flow properties of liquids is important for pharmacists working in the manufacture of several dosage forms, such as simple liquids, ointments, creams, pastes etc. The flow behavior of liquids under applied stress is of great relevance in the field of pharmacy. Flow properties are used as important quality control tools to maintain the superiority of the product and reduce batch to batch variations.
Materials science.
Polymers.
Examples may be given to illustrate the potential applications of these principles to practical problems in the processing and use of rubbers, plastics, and fibers. Polymers constitute the basic materials of the rubber and plastic industries and are of vital importance to the textile, petroleum, automobile, paper, and pharmaceutical industries. Their viscoelastic properties determine the mechanical performance of the final products of these industries, and also the success of processing methods at intermediate stages of production.
In viscoelastic materials, such as most polymers and plastics, the presence of liquid-like behaviour depends on the properties of and so varies with rate of applied load, i.e., how quickly a force is applied. The silicone toy 'Silly Putty' behaves quite differently depending on the time rate of applying a force. Pull on it slowly and it exhibits continuous flow, similar to that evidenced in a highly viscous liquid. Alternatively, when hit hard and directly, it shatters like a silicate glass.
In addition, conventional rubber undergoes a glass transition, (often called a "rubber-glass transition"). E.G. The Space Shuttle Challenger disaster was caused by rubber O-rings that were being used well below their glass transition temperature on an unusually cold Florida morning, and thus could not flex adequately to form proper seals between sections of the two solid-fuel rocket boosters.
Sol-gel.
With the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. The mechanisms of hydrolysis and condensation, and the rheological factors that bias the structure toward linear or branched structures are the most critical issues of sol-gel science and technology.
Geophysics.
Geophysics includes the flow of molten lava and debris flows (fluid mudslides). Also included in this disciplinary branch are solid Earth materials which only exhibit flow over extended time scales. Those that display viscous behaviour are known as rheids. E.G. granite can flow plastically with a negligible yield stress at room temperatures, (i.e. a viscous flow). Long term creep experiments (~ 10 years) indicate that the viscosity of granite and glass under ambient conditions are on the order of 1020 poises.
Physiology.
Physiology includes the study of many bodily fluids that have complex structure and composition, and thus exhibit a wide range of viscoelastic flow characteristics. In particular there is a specialist study of blood flow called hemorheology. This is the study of flow properties of blood and its elements (plasma and formed elements, including red blood cells, white blood cells and platelets). Blood viscosity is determined by plasma viscosity, hematocrit (volume fraction of red blood cell, which constitute 99.9% of the cellular elements) and mechanical behaviour of red blood cells. Therefore, red blood cell mechanics is the major determinant of flow properties of blood.
Food rheology.
Food rheology is important in the manufacture and processing of food products, e.g. cheese.
Thickening agents, or thickeners, are substances which, when added to an aqueous mixture, increase its viscosity without substantially modifying its other properties, such as taste. They provide body, increase stability, and improve suspension of added ingredients. Thickening agents are often used as food additives and in cosmetics and personal hygiene products. Some thickening agents are gelling agents, forming a gel. The agents are materials used to thicken and stabilize liquid solutions, emulsions, and suspensions. They dissolve in the liquid phase as a colloid mixture that forms a weakly cohesive internal structure. Food thickeners frequently are based on either polysaccharides (starches, vegetable gums, and pectin), or proteins.
Concrete rheology.
Concrete's and mortar's workability is related to the rheological properties of the fresh cement paste. The mechanical properties of hardened concrete increase if less water is used in the concrete mix design, however reducing the water-to-cement ratio may decrease the ease of mixing and application. To avoid these undesired effects, superplasticizers are typically added to decrease the apparent yield stress and the viscosity of the fresh paste. Their addition highly improves concrete and mortar properties.
Filled polymer rheology.
The use of various types of fillers incorporated into the polymer has become quite common as a means of reducing cost and to impart certain desirable mechanical, thermal, electrical and magnetic properties to the polymers. The advantages that filled polymer systems have to offer are normally offset to some extent by the increased complexity in the rheological behavior that is introduced by the inclusion of the fillers. Usually when the use of fillers is considered, a compromise has to be made between the improved mechanical properties in the solid state, the increased difficulty in melt processing, the problem of achieving uniform dispersion of the filler in the polymer matrix and the economics of the process due to the added step of compounding. The rheological properties of filled polymers are determined, not only by the type of the filler, but also by its size, shape, size distribution and amount. A key factor in the use of fillers without affecting the material properties is the stress transfer at the filler-polymer interface. The interfacial adhesion can be substantially enhanced via a coupling agent that adheres well to both the polymer and the filler particles. The type and amount of surface treatment on the filler are thus additional parameters affecting the rheological properties of filled polymeric systems.
Measurement.
Rheometers are instruments used to characterize the rheological properties of materials, typically fluids that are melts or solution. These instruments impose a specific stress field or deformation to the fluid, and monitor the resultant deformation or stress. Instruments can be run in steady flow or oscillatory flow, in both shear and extension.
In 1932 Fritz Höppler got a patent for the Falling Ball viscometer, named after him - the worldwide first viscometer to determine the dynamic viscosity.
Dimensionless numbers.
Deborah number.
On one end of the spectrum we have an inviscid or a simple Newtonian fluid and on the other end, a rigid solid; thus the behaviour of all materials fall somewhere in between these two ends. The difference in material behaviour is characterized by the level and nature of elasticity present in the material when it deforms, which takes the material behaviour to the non-Newtonian regime. The non-dimensional Deborah number is designed to account for the degree of non-Newtonian behaviour in a flow. The Deborah number is defined as the ratio of the characteristic time of relaxation (which purely depends on the material and other conditions like the temperature) to the characteristic time of experiment or observation. Small Deborah numbers represent Newtonian flow, while non-Newtonian (with both viscous and elastic effects present) behaviour occurs for intermediate range Deborah numbers, and high Deborah numbers indicate an elastic/rigid solid. Since Deborah number is a relative quantity, the numerator or the denominator can alter the number. A very small Deborah number can be obtained for a fluid with extremely small relaxation time or a very large experimental time, for example.
Reynolds number.
In fluid mechanics, the Reynolds number is a measure of the ratio of inertial forces ("vsρ") to viscous forces ("μ/L") and consequently it quantifies the relative importance of these two types of effect for given flow conditions. Under low Reynolds numbers viscous effects dominate and the flow is laminar, whereas at high Reynolds numbers inertia predominates and the flow may be turbulent. However, since rheology is concerned with fluids which do not have a fixed viscosity, but one which can vary with flow and time, calculation of the Reynolds number can be complicated.
It is one of the most important dimensionless numbers in fluid dynamics and is used, usually along with other dimensionless numbers, to provide a criterion for determining dynamic similitude. When two geometrically similar flow patterns, in perhaps different fluids with possibly different flow rates, have the same values for the relevant dimensionless numbers, they are said to be dynamically similar.
Typically it is given as follows:
where:

</doc>
<doc id="25456" url="http://en.wikipedia.org/wiki?curid=25456" title="Rifle">
Rifle

Top: Baker rifle, a 19th-century flintlock rifleSecond: M1903 Springfield, an early 20th-century bolt-action rifleThird: АК-47, a mid-20th-century gas-operated, magazine-fed automatic rifleFourth: AR-15, a mid 20th-century magazine-fed, semi-automatic rifle
A rifle is a firearm designed to be fired from the shoulder, with a barrel that has a helical groove or pattern of grooves ("rifling") cut into the barrel walls. The raised areas of the rifling are called "lands," which make contact with the projectile (for small arms usage, called a bullet), imparting spin around an axis corresponding to the orientation of the weapon. When the projectile leaves the barrel, this spin lends gyroscopic stability to the projectile and prevents tumbling, in the same way that a properly thrown American football or rugby ball behaves. This allows the use of aerodynamically-efficient pointed bullets (as opposed to the spherical balls used in muskets) and thus improves range and accuracy. The word "rifle" originally referred to the grooving, and a rifle was called a "rifled gun." Rifles are used in warfare, hunting and shooting sports.
Typically, a bullet is propelled by the contained deflagration of an explosive compound (originally black powder, later cordite, and now nitrocellulose), although other means such as compressed air are used in air rifles, which are popular for vermin control, hunting small game, formal target shooting and casual shooting ("plinking").
In most armed forces the term "gun" is incorrect when referring to small arms; in military parlance, the word "gun" refers to an artillery piece or crew-served machine gun. Furthermore, in many works of fiction a rifle refers to any weapon that has a stock and is shouldered before firing, even if the weapon is not rifled or does not fire solid projectiles (e.g. a "laser rifle").
Formerly, rifles only fired a single projectile with each squeeze of the trigger. Modern rifles are capable of firing more than one round per trigger squeeze; some fire in a fully automatic mode and others are limited to fixed bursts of two, three, or more rounds per squeeze. Thus, modern automatic rifles overlap to some extent in design and function with machine guns. In fact, many light machine guns (such as the Russian RPK) are adaptations of existing automatic rifle designs. A military's light machine guns are typically chambered for the same caliber ammunition as its service rifles. Generally, the difference between an automatic rifle and a machine gun comes down to weight, cooling system, and ammunition feed system. Rifles, with their relatively lighter components (which overheat quickly) and smaller capacity magazines, are incapable of sustained automatic fire in the way that machine guns are; they trade this capability in favor of increased mobility. Modern military rifles are fed by box magazines, while machine guns are generally belt-fed. Many machine guns allow the operator to quickly exchange barrels in order to prevent overheating, whereas rifles generally do not. Most machine guns fire from an open bolt in order to reduce the danger of "cook-off", while almost all rifles fire from a closed bolt for superior accuracy. Machine guns are often crewed by more than one soldier; the rifle is an individual weapon.
The term "rifle" is sometimes used to describe larger crew-served rifled weapons firing explosive shells, for example, recoilless rifles.
Historical overview.
The origins of rifling are difficult to trace, but some of the earliest practical experiments seem to have occurred in Europe during the 15th century. Archers had long realized that a twist added to the tail feathers of their arrows gave them greater accuracy. Early muskets produced large quantities of smoke and soot, which had to be cleaned from the action and bore of the musket frequently, either through the action of repeated bore scrubbing, or a deliberate attempt to create "soot grooves" that would allow for more shots to be fired from the firearm. This might also have led to a perceived increase in accuracy, although no one knows for sure. True rifling dates from the mid-15th century, although military commanders preferred smooth bore weapons for infantry use because rifles were much more prone to problems due to powder fouling the barrel.
Rifles were created as an improvement in the accuracy of smooth bore muskets. In the early 18th century, Benjamin Robins, an English mathematician, realized that an elongated bullet would retain the momentum and kinetic energy of a musket ball, but would slice through the air with greater ease. The black powder used in early muzzle-loading rifles quickly fouled the barrel, making loading slower and more difficult. Their greater range was also considered to be of little practical use, since the smoke from black powder quickly obscured the battlefield and made it almost impossible to target the enemy from a distance. Since musketeers could not afford to take the time to stop and clean their barrels in the middle of a battle, rifles were limited to use by sharpshooters and non-military uses like hunting.
Muskets were smoothbore, large caliber weapons using ball-shaped ammunition fired at relatively low velocity. Due to the high cost and great difficulty of precision manufacturing, and the need to load readily from the muzzle, the musket ball was a loose fit in the barrel. Consequently on firing the ball bounced off the sides of the barrel when fired and the final direction on leaving the muzzle was unpredictable. Muskets had to be long so the muzzles of the rear rank’s muskets projected well forward of the faces of the front rank.
The performance of early muskets was effective for the styles of warfare at the time, whereby soldiers tended to stand in long, stationary lines and fire at the opposing forces. Precise aiming and accuracy were not necessary to hit an opponent. Muskets were used for comparatively rapid, imprecisely aimed volley fire, and the average soldier could be easily trained to use them. The (muzzle-loaded) rifle was originally a sharpshooter's weapon used for targets of opportunity and deliberately aimed fire, first gaining notoriety in warfare during the Seven Years' War and American War for Independence through their use by American frontiersmen. Later during the Napoleonic Wars, the British 95th Regiment (Green Jackets) and 60th Regiment, (Royal American), as well as American sharpshooters and riflemen during the War of 1812, used the rifle to great effect during skirmishing. Because of a slower loading time than a musket, they were not adopted by the whole army. Since rifles were used by sharpshooters who didn't routinely fire over other men’s shoulders, long length was not required to avoid the forward line. A shorter length made a handier weapon in which tight-fitting balls did not have to be rammed so far down the barrel.
The invention of the minie balls in the 1840s solved the slow loading problem, and in the 1850s and 1860s rifles quickly replaced muskets on the battlefield. Many rifles, often referred to as rifled muskets, were very similar to the muskets they replaced, but the military also experimented with other designs. Breech-loading weapons proved to have a much faster rate of fire than muzzleloaders, causing military forces to abandon muzzle loaders in favor of breech-loading designs in the late 1860s. In the later part of the 19th century, rifles were generally single-shot, breech-loading — designed for aimed, discretionary fire by individual soldiers. Then, as now, rifles had a stock, either fixed or folding, to be braced against the shoulder when firing.
The adoption of cartridges and breech-loading in the 19th century was concurrent with the general adoption of rifles. In the early part of the 20th century, soldiers were trained to shoot accurately over long ranges with high-powered cartridges. World War I Lee-Enfield rifles (among others) were equipped with long-range 'volley sights' for massed firing at ranges of up to 1.6 km. Individual shots were unlikely to hit, but a platoon firing repeatedly could produce a 'beaten ground' effect similar to light artillery or machine guns; but experience in World War I showed that long-range fire was best left to the machine gun.
Currently, rifles are the most common firearm in general use for hunting purposes (with the exception of bird hunting where shotguns are favored). Rifles derived from military designs have long been popular with civilian shooters.
19th century.
During the Napoleonic Wars the British army created several experimental units known as "Rifles", armed with the Baker rifle. These Rifle Regiments were deployed as skirmishers during the Peninsular war in Spain and Portugal, and were more effective than skirmishers armed with muskets due to their accuracy and long range.
Muzzle-loading.
Gradually, rifles appeared with cylindrical barrels cut with helical grooves, the surfaces between the grooves being "lands". The innovation shortly preceded the mass adoption of breech-loading weapons, as it was not practical to push an overbore bullet down through a rifled barrel, only to then (try to) fire it back out. The dirt and grime from prior shots was pushed down ahead of a tight bullet or ball (which may have been a loose fit in the clean barrel before the first shot), and, of course, loading was far more difficult, as the lead had to be deformed to go down in the first place, reducing the accuracy due to deformation. Several systems were tried to deal with the problem, usually by resorting to an under-bore bullet that expanded upon firing.
The original muzzle-loading rifle, with a closely fitting ball to take the rifling grooves, was loaded with difficulty, particularly when foul, and for this reason was not generally used for military purposes. Even with the advent of rifling the bullet itself didn't change, but was wrapped in a greased, cloth patch to grip the rifling grooves.
The first half of the 19th century saw a distinct change in the shape and function of the bullet. In 1826 Delvigne, a French infantry officer, invented a breech with abrupt shoulders on which a spherical bullet was rammed down until it caught the rifling grooves. Delvigne's method, however, deformed the bullet and was inaccurate.
Soon after, the Carabine à tige was invented by Louis-Etienne de Thouvenin, which provided for a stem at the bottom at the barrel that would deform and expand the base of the bullet when rammed, therefore enabling accurate contact with the rifling. However, the area around the stem would clog and get dirty easily.
Minié system - The "Rifled Musket".
One of the most famous was the Minié system, invented by French Army Captain Claude-Étienne Minié, which relied on a conical bullet (known as a Minié ball) with a hollow skirt at the base of the bullet. When fired, the skirt would expand from the pressure of the exploding charge and grip the rifling as the round was fired. The better seal gave more power, as less gas escaped past the bullet, which combined with the fact that for the same bore (caliber) diameter a long bullet was heavier than a round ball. The extra grip also spun the bullet more consistently, which increased the range from about 50 yards for a smooth bore musket to about 300 yards for a rifle using the Minié system. The expanding skirt of the Minié ball also solved the problem that earlier tight fitting bullets were difficult to load as black powder residue fouled the inside of the barrel. The Minié system allowed conical bullets to be loaded into rifles just as quickly as round balls in smooth bores, which allowed rifle muskets to replace muskets on the battlefield. Minié system rifles, notably the U.S. Springfield and the British Enfield of the early 1860s, featured prominently in the U.S. Civil War, due to their enhanced power and accuracy.
Over the 19th century, bullet design also evolved, the bullets becoming gradually smaller and lighter. By 1910 the standard blunt-nosed bullet had been replaced with the pointed, 'spitzer' bullet, an innovation that increased range and penetration. Cartridge design evolved from simple paper tubes containing black powder and shot, to sealed brass cases with integral primers for ignition, while black powder itself was replaced with cordite, and then other nitro-cellulose-based smokeless powder mixtures, propelling bullets to higher velocities than before.
The increased velocity meant that new problems arrived, and so bullets went from being soft lead to harder lead, then to copper-jacketed, in order to better engage the spiraled grooves without "stripping" them in the same way that a screw or bolt thread would be stripped if subjected to extreme forces.
Breech loading.
From 1836, breech-loading rifles were introduced with the German Dreyse Needle gun, and followed by the French Tabatière in 1857 the British Calisher and Terry carbine made in Birmingham and later in 1864 and the more well known British Snider-Enfield. Primitive chamber-locking mechanisms were soon replaced by bolt-action mechanisms, exemplified by the Chassepot in 1866. Breech loading was to have a major impact on warfare, as breech-loading rifles can be fired at a rate many times higher than muzzle loaded rifles and significantly can be loaded from a prone rather than standing position. Firing prone (i.e., lying down) is more accurate than firing from a standing position, while a prone rifleman presents a much smaller target than a standing soldier. The higher accuracy and range, combined with reduced vulnerability generally benefited the defense while making the traditional battle between lines of standing and volleying infantry men obsolete.
Revolving Rifle.
Revolving rifles were an attempt to increase the rate of fire of rifles by combining them with the revolving firing mechanism that had been developed earlier for revolving pistols. Colt began experimenting with revolving rifles in the early 19th century, and other manufacturers like Remington later experimented with them as well. The Colt Revolving Rifle Model 1855 was an early repeating rifle and the first one to be used by the U.S. Government, and saw some limited action during the American Civil War. Revolvers, both rifles and pistols, tend to spray fragments of metal from the front of the cylinder. This is not a problem for pistols, since both of the shooter's hands are behind the cylinder. A rifleman needs to have one hand in front of the cylinder to balance the weapon, and as a result, would end up with shards of metal sprayed at high velocity into his forearm. Cap and ball type revolvers were also prone to chain fire, which again was more of a problem for rifles since the rifleman's arm was in front of the cylinder. These undesirable characteristics severely limited the revolving rifle's popularity.
Cartridge storage.
An important area of development was the way that cartridges were stored and used in the weapon. The Spencer repeating rifle was a breech-loading manually operated lever action rifle that was adopted by the United States. Over 20,000 were used during the American Civil War. It marked the first adoption of a removable magazine-fed infantry rifle by any country. The design was completed by Christopher Spencer in 1860. It used copper rimfire cartridges stored in a removable seven round tube magazine, enabling the rounds to be fired one after another. When the magazine was empty, it could be exchanged for another.
20th Century.
The Russo-Japanese War of 1904-1905 was the first modern war of the 20th century. Military observers from Great Britain, Germany, France, and the United States witnessed first hand the first major conflict fought with high velocity bolt action rifles firing smokeless powder on a massive scale.:179,229,230:104,105 The Battle of Mukden fought in 1905 consisted of nearly 343,000 Russian troops against over 281,000 Japanese troops. The Russian Mosin-Nagant Model 1891 in 7.62mm was pitted against the Japanese Arisaka Type 30 bolt action rifle in 6.5mm,:104,105,155 each had velocities well over the 19th century black powder velocities of under 2,000 feet per second (610 m/s).:187:28,29
Until the early 20th century rifles tended to be very long; an 1890 Martini-Henry was almost 2 m (6 ft) in length with a fixed bayonet. The demand for more compact weapons for cavalrymen led to the carbine, or shortened rifle.
The advent of massed, rapid firepower of the machine gun, submachine gun and the rifled artillery piece was so quick as to outstrip the development of any way to attack a trench defended by riflemen and machine gunners. The carnage of World War I was perhaps the greatest vindication and vilification of the rifle as a military weapon.
During and after World War II it became accepted that most infantry engagements occur at ranges of less than 300 m; the range and power of the large battle rifles was "overkill"; and the weapons were heavier than the ideal. This led to Germany's development of the 7.92 x 33 mm "Kurz" (short) round, the MKb-42, and ultimately, the assault rifle. Today, an infantryman's rifle is optimized for ranges of 300 m or less, and soldiers are trained to deliver individual rounds or bursts of fire within these distances. Typically, the application of accurate, long-range fire is the domain of the marksman and the sniper in warfare, and of enthusiastic target shooters in peacetime. The modern marksman rifle and sniper rifle are usually capable of accuracy better than 0.3 mrad at 100 yards (1 arcminute).
By contrast, civilian rifle design has not significantly advanced since the early part of the 20th century. Modern hunting rifles have fiberglass and carbon fiber stocks and more advanced recoil pads, but are fundamentally the same as infantry rifles from 1910. Many modern sniper rifles can trace their ancestry back for well over a century, and the Russian 7.62x54mm rimmed cartridge, as used in the front-line Dragunov Sniper Rifle (SVD), dates from 1891.
3D printed rifle.
The Grizzly is a 3D printed .22-caliber rifle created around August 2013. It was created using a Stratasys Dimension 1200es printer. It was created by a Canadian only known by the pseudo name "Matthew" who told the The Verge that he is in his late 20s, and his main job is making tools for the construction industry.
The original Grizzly fired 1 shot then broke. Grizzly 2.0 fired 14 bullets before getting damaged due to the strain. According to the Daily Mail, the Grizzly 2.0 performed so well that the inventor "Matthew" was able to put it to his shoulder and shot off three rounds with the rifle pressed against his cheek without hurting him.
Youth rifle.
A youth rifle is a rifle designed or modified for fitting children, or small-framed shooters. A youth rifle is often a single shot .22 caliber rifle, or a bolt action rifle, although some youth rifles are semi-automatic. They are usually very light, with a greatly shortened length of pull, which is necessary to accommodate children. Youth stocks are available for many popular rifles, such as the Ruger 10/22, a semi-automatic .22 LR rifle, allowing a youth rifle to be made from a standard rifle by simply changing the stock. The typical ages of shooters for such rifles vary from about age 5 to 11 years old.
Technical aspects.
Rifling.
The usual form of rifling was helical grooves in a round bore.
Some early rifled guns had barrels with a twisted polygonal bore. The Whitworth rifle was the first such type designed to spin the round for accuracy. Bullets for these guns were made to match the shape of the bore so the bullet would grip the rifle bore and take a spin that way. These were generally large caliber weapons and the ammunition still did not fit tightly in the barrel. Many different shapes and degrees of spiraling were used in experimental designs. One widely produced example was the Metford rifling in the Pattern 1888 Lee-Metford service rifle. Although uncommon, polygonal rifling is still used in some weapons today, with one example being the Glock line of pistols (which fire standard bullets). Unfortunately, many of these early designs were prone to dangerous backfiring, which could lead to destruction of the weapon and serious injury to the person firing it.
Barrel wear.
As the bullet enters the barrel, it inserts itself into the rifling, a process that gradually wears down the barrel, and also causes the barrel to heat up more rapidly. Therefore, some machine-guns are equipped with quick-change barrels that can be swapped every few thousand rounds, or in earlier designs, were water-cooled. Unlike older carbon steel barrels, which were limited to around 1,000 shots before the extreme heat caused accuracy to fade, modern stainless steel barrels for target rifles are much more resistant to wear, allowing many thousands of rounds to be fired before accuracy drops. (Many shotguns and small arms have chrome-lined barrels to reduce wear and enhance corrosion resistance. This is rare on rifles designed for extreme accuracy, as the plating process is difficult and liable to reduce the effect of the rifling.) Modern ammunition has a hardened lead core with a softer outer cladding or jacket, typically of an alloy of copper and nickel - cupro-nickel. Some ammunition is even coated with molybdenum disulfide to further reduce internal friction - the so-called 'moly-coated' bullet.
Rate of fire.
Rifles were initially single-shot, muzzle-loading weapons. During the 18th century, breech-loading weapons were designed, which allowed the rifleman to reload while under cover, but defects in manufacturing and the difficulty in forming a reliable gas-tight seal prevented widespread adoption. During the 19th century, multi-shot repeating rifles using lever, pump or linear bolt actions became standard, further increasing the rate of fire and minimizing the fuss involved in loading a firearm. The problem of proper seal creation had been solved with the use of brass cartridge cases, which expanded in an elastic fashion at the point of firing and effectively sealed the breech while the pressure remained high, then relaxed back enough to allow for easy removal. By the end of the 19th century, the leading bolt-action design was that of Paul Mauser, whose action—wedded to a reliable design possessing a five-shot magazine—became a world standard through two world wars and beyond. The Mauser rifle was paralleled by Britain's ten-shot Lee-Enfield and America's 1903 Springfield Rifle models (the latter pictured above). The American M1903 closely copied Mauser's original design.
Range.
Barrel rifling dramatically increased the range and accuracy of the musket. Indeed, throughout its development, the rifle's history has been marked by increases in range and accuracy. From the Minié rifle and beyond, the rifle has become ever more potent at long range strikes.
In recent decades, large-caliber anti-materiel rifles, typically firing between 12.7 mm and 20 mm caliber cartridges, have been developed. The US Barrett M82A1 is probably the best-known such rifle. These weapons are typically used to strike critical, vulnerable targets such as computerized command and control vehicles, radio trucks, radar antennae, vehicle engine blocks and the jet engines of enemy aircraft. Anti-materiel rifles can be used against human targets, but the much higher weight of rifle and ammunition, and the massive recoil and muzzle blast, usually make them less than practical for such use. The Barrett M82 is credited with a maximum effective range of 1800 m; and it was with a .50BMG caliber McMillan TAC-50 rifle that Canadian Master Corporal Rob Furlong made the longest recorded (until 2010) confirmed sniper kill in history, when he shot a Taliban fighter at a range of 2430 m in Afghanistan during Operation Anaconda in 2002.
Since then a British Army sniper beat the Canadian's record by 150 ft, by killing two Taliban insurgents at a range of 8120 ft. He was using a standard issue British Army sniper rifle, the L115A3.
Bullet Rotational Speed (RPM).
Bullets leaving a rifled barrel can spin at a rotational speed of over 100,000 revolutions per minute (rpm, or 1.67 kilohertz) depending on the muzzle velocity of the bullet and the pitch of the rifling. The rotational speed of the bullet can be calculated by using the formula below. The formula divides the number of inches in a foot (12) by the rate of twist that the barrel has. This number is multiplied by the muzzle velocity (MV) and the number of seconds in a minute (60). For example, a bullet with a muzzle velocity of 3000 ft/s leaving a barrel that twists once per foot (1/12") would rotate at 180,000rpm.
Example using a barrel that has a twist rate of 1 turn in 8" with a muzzle velocity of 3000 ft/s:
Excessive rotational speed can exceed the bullet's designed limits and the inadequate centripetal force will fail to keep the bullet from disintegrating in a radial fashion.
Caliber.
Rifles may be chambered in a variety of calibers, from as low as .17 (4.4mm) varmint calibers to as high as .80 caliber in the case of the largest anti tank rifles. The term caliber essentially refers to the width of the bullet fired through a rifle's barrel. Armies have consistently attempted to find and procure the most lethal and accurate caliber for their firearms.
The standard calibers used by the world's militaries tend to follow worldwide trends. These trends have significantly changed during the centuries of firearm design and re-design. Muskets were normally chambered for large calibers, such as .50 or .59, with the theory that these large bullets caused the most damage.
During World War I and II, most rifles were chambered in .30 caliber, a combination of power and speed. Examples would be the .303 British Lee-Enfield, the American M1903 .30-06, and the German 8mm Mauser K98.
An exception was the Italian Modello 91 rifle, that used the 6.5×52mm Mannlicher-Carcano cartridge.
Detailed study of infantry combat during and after WWII revealed that most small-arms engagements occurred within 100 meters, meaning that the power and range of the traditional .30-caliber weapons (designed for engagements at 500 meters and beyond) was essentially wasted. The single greatest predictor of an individual soldier's combat effectiveness was the number of rounds he fired. Weapons designers and strategists realized that service rifles firing smaller-caliber projectiles would allow troops to carry far more ammunition for the same weight. The lower recoil and more generous magazine capacities of small-caliber weapons also allows troops a much greater volume of fire, compared to historical battle rifles. Smaller, faster traveling, less stable projectiles have also demonstrated greater terminal ballistics and therein, a greater lethality than traditional .30-caliber rounds. Most modern service rifles fire a projectile of approximately 5.56mm. Examples of firearms in this range are the American 5.56 mm M16 and the Russian 5.45×39mm AK-74.

</doc>
<doc id="25457" url="http://en.wikipedia.org/wiki?curid=25457" title="Republic of Texas">
Republic of Texas

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1836
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1840 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1840 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |  United States
The Republic of Texas (Spanish: "República de Texas") was an independent sovereign country in North America that existed from March 2, 1836, to February 19, 1846. It was bordered by the nation of Mexico to the southwest, the Gulf of Mexico to the southeast, the two U.S. states of Louisiana and Arkansas to the east and northeast, and the United States territories encompassing the current U.S. states of Oklahoma, Kansas, Colorado, Wyoming, and New Mexico to the north and west. The citizens of the republic were known as Texians.
Formed as a separate nation after gaining independence from Mexico in 1836, the republic claimed borders that included all of the present U.S. state of Texas as well as parts of present-day Oklahoma, Kansas, Colorado, Wyoming, and New Mexico based upon the Treaties of Velasco between the newly created Texas Republic and Mexico. The eastern boundary with the United States was defined by the Adams-Onís Treaty between the United States and Spain in 1819. Its southern and western-most boundary with Mexico was under dispute throughout the entire existence of the republic with Texas claiming the boundary as the Rio Grande (known as the Río Bravo del Norte or Río Bravo in Mexico), and Mexico claiming the boundary as the Nueces River. This dispute would later become a trigger for the Mexican–American War from 1846 to 1848 between Mexico and the United States after the annexation of Texas by the United States on December 29, 1845.
History.
Texas prior to independence.
Texas had been one of the "Provincias Internas" of New Spain, a region known historiographically as Spanish Texas. Though claimed by Spain, it was not formally colonized by them until competing French interests at Fort St. Louis encouraged Spain to establish permanent settlements in the area. Sporadic missionary incursions occurred into the area during the period from 1690s–1710s, before the establishment of San Antonio as a permanent civilian settlement. Owing to high Native American populations in the area and remoteness from the population centers of New Spain, Texas remained largely unsettled by Europeans, though Spain maintained a military presence, both to protect Christian missionaries working among Native American tribes, and to act as a buffer against the French in Louisiana and British North America. In 1762, France ceded to Spain most of its claims to the interior of North America, including its largely defunct claim to Texas, as well as the vast interior which became Spanish Louisiana. During the period of 1799–1803, at the height of the Napoleonic Empire, Spain receded Louisiana back to France, which soon after sold the territory to the United States. The status of Texas during these transfers was uncertain, and not resolved until 1819 when the Adams–Onís Treaty ceded Spanish Florida to the United States, and established a clear boundary between Texas and Louisiana.
Starting in 1810, the Mexican War of Independence sought independence for the territories of New Spain north of the Isthmus of Panama, including Texas. In the Texas area, many Americans fought on the side of the Mexicans against Spain during filibustering expeditions. One of these, the Gutiérrez–Magee Expedition (also known as the Republican Army of the North) consisted of a group of about 130 Americans under the joint leadership of Bernardo Gutiérrez de Lara and Augustus Magee. Bolstered by new recruits, and under the leadership of Samuel Kemper (who succeeded Magee after his death in battle in 1813) the expedition experienced a series of victories against forces led by the Spanish governor Manuel María de Salcedo, the most significant of which was the Battle of Rosillo Creek, which convinced Salcedo to surrender on April 1, 1813. Salcedo was executed two days later. On April 6, 1813, the victorious Republican Army of the North drafted a constitution and declared an independent Republic of Texas with Gutiérrez as president. Disillusioned with the Mexican leadership, the Americans under Kemper withdrew from the expedition and returned to the United States. The ephemeral Republic of Texas would come to an end following the August 18, 1813 Battle of Medina, where the Spanish Army would crush the Republican Army of the North, and reprisals against rebels in the area would engender a deep distrust of the Royal Spanish authorities; veterans of the Battle of Medina for the Rebel side would be later leaders of the Texas Revolution and signatories of the Texas Declaration of Independence from Mexico some 20 years later.
Along with the rest of Mexico, Texas became independent from Spain following the Treaty of Córdoba and the new Mexican state was organized under the Plan of Iguala which created Mexico as a constitutional monarchy under its first Emperor Agustín de Iturbide. During the transition from a Spanish territory to part of the independent country of Mexico, Stephen F. Austin led a group of American settlers known as the Old Three Hundred, who negotiated the right to settle in Texas with the Spanish Royal governor of the territory. Since Mexican independence had been ratified by Spain shortly thereafter, Austin would later travel to Mexico City to secure the support of the new country in his right to settle. The establishment of Mexican Texas coincided with the Austin-led settlement, leading to animosity between Mexican authorities and ongoing American settlement of Texas. The First Mexican Empire was short lived, being replaced by a republican form of government in 1823. Following Austin's lead, additional groups of settlers, known as Empresarios, continued to colonize Mexican Texas from the United States. In 1830, Mexican President Anastasio Bustamante outlawed American immigration to Texas, following several conflicts with the Empresarios over the status of slavery in the region. Angered at the interference of the Mexican government, the Empresarios held the Convention of 1832, which is considered the first formal step in what would later become the Texas Revolution.
On the eve of war, the American settlers in the area outnumbered Mexicans by a considerable margin. Following a series of minor skirmishes between Mexican authorities and the settlers, the Mexican government, fearing open rebellion of their Anglo subjects, began to step up military presence in Texas throughout 1834 and early 1835. Mexican President Antonio López de Santa Anna revoked the 1824 Constitution of Mexico and began to consolidate power in the central government under his own leadership. The Texian leadership under Austin began to organize its own military, and hostilities broke out on October 2, 1835 at the Battle of Gonzales, the first engagement of the Texas Revolution. In November, 1835 a provisional government known as the Consultation was established to oppose the Santa Anna regime (but stopped short of declaring independence from Mexico). On March 1, 1836 the Convention of 1836 came to order, and the next day declared independence from Mexico, establishing the Republic of Texas.
Independent republic.
Politics.
The second Congress of the Republic of Texas convened in October 1836 at Columbia (now West Columbia). Stephen F. Austin, known as the "Father of Texas", died December 27, 1836, after serving two months as Secretary of State for the new Republic.
In 1836, five sites served as temporary capitals of Texas (Washington-on-the-Brazos, Harrisburg, Galveston, Velasco and Columbia), before President Sam Houston moved the capital to Houston in 1837. The next president, Mirabeau B. Lamar, moved the capital to the new town of Austin in 1839.
The first flag of the republic was the "Burnet Flag" (a gold star on an azure field), followed in 1839 by official adoption of the Lone Star Flag.
Internal politics of the Republic centred on two factions. The nationalist faction, led by Lamar, advocated the continued independence of Texas, the expulsion of the Native Americans (Indians), and the expansion of Texas to the Pacific Ocean. Their opponents, led by Houston, advocated the annexation of Texas to the United States and peaceful coexistence with the Indians, when possible. The Texas Congress even passed a resolution over Houston's veto claiming the Californias for Texas. The 1844 presidential election split the electorate dramatically, with the newer western regions of the Republic preferring the nationalist candidate Edward Burleson, while the cotton country, particularly east of the Trinity River, went for Anson Jones.
Armed conflict.
The warlike Comanche Indians furnished the main Indian opposition to the Texas Republic, manifested in multiple raids on settlements, capture and rape of women pioneers, torture killings, and trafficking in captive slaves. In the late 1830s Sam Houston negotiated a peace between Texas and the Comanches. Lamar replaced Houston as president in 1838 and reversed the Indian policies. He returned to war with the Comanches and invaded Comancheria itself. In retaliation, the Comanches attacked Texas in a series of raids. After peace talks in 1840 ended with the massacre of 34 Comanche leaders in San Antonio, the Comanches launched a major attack deep into Texas, known as the Great Raid of 1840. Under command of Potsanaquahip (Buffalo Hump), 500 to 700 Comanche cavalry warriors swept down the Guadalupe River valley, killing and plundering all the way to the shore of the Gulf of Mexico, where they sacked the towns of Victoria and Linnville. Houston became president again in 1841 and, with both Texians and Comanches exhausted by war, a new peace was established.
Although Texas achieved self-government, Mexico refused to recognize its independence. On March 5, 1842, a Mexican force of over 500 men, led by Ráfael Vásquez, invaded Texas for the first time since the revolution. They soon headed back to the Rio Grande after briefly occupying San Antonio. About 1,400 Mexican troops, led by the French mercenary general Adrián Woll, launched a second attack and captured San Antonio on September 11, 1842. A Texas militia retaliated at the Battle of Salado Creek while simultaneously, a mile and a half away, Mexican soldiers and Texas Cherokee Indians massacred a militia of fifty-three Texas volunteers who had surrendered after a skirmish. That night, the Mexican Army retreated from the city of San Antonio back to Mexico.
Mexico's attacks on Texas intensified conflicts between political factions, including an incident known as the Texas Archive War. To "protect" the Texas national archives, President Sam Houston ordered them removed from Austin. The archives were eventually returned to Austin, albeit at gunpoint. The Texas Congress admonished Houston for the incident, and this episode in Texas history would solidify Austin as Texas's seat of government for the Republic and the future state.
There were also domestic disturbances. The Regulator–Moderator War involved a land feud in Harrison and Shelby Counties in East Texas from 1839 to 1844. The feud eventually involved Nacogdoches, San Augustine, and other East Texas counties. Harrison County Sheriff John J. Kennedy and county judge Joseph U. Fields helped end the conflict, siding with the law-and-order party. Sam Houston ordered 500 militia to help end the feud.
Government.
After gaining their independence, the Texas voters had elected a Congress of 14 senators and 29 representatives in September 1836. The Constitution allowed the first president to serve for two years and subsequent presidents for 3 years.
The first Congress of the Republic of Texas convened in October 1836 at Columbia (now West Columbia). Stephen F. Austin, often referred to as the "Father of Texas," died on December 27, 1836, after serving just two months as the republic's secretary of state. Due mainly to the ongoing war for independence, five sites served as temporary capitals of Texas in 1836: (Washington-on-the-Brazos, Harrisburg, Galveston, Velasco and Columbia). The capital was moved to the new city of Houston in 1837.
In 1839, a small pioneer settlement situated on the Colorado River in central Texas was chosen as the republic's seventh and final capital. Incorporated under the name Waterloo, the town was renamed Austin shortly thereafter in honor of Stephen F. Austin.
The court system inaugurated by Congress included a Supreme Court consisting of a chief justice appointed by the president and four associate justices, elected by a joint ballot of both houses of Congress for four-year terms and eligible for re-election. The associates also presided over four judicial districts. Houston nominated James Collinsworth to be the first chief justice. The county-court system consisted of a chief justice and two associates, chosen by a majority of the justices of the peace in the county. Each county was also to have a sheriff, a coroner, justices of the peace, and constables to serve two-year terms. Congress formed 23 counties, whose boundaries generally coincided with the existing municipalities.
In 1839, Texas became the first nation in the world to enact a homestead exemption under which a person's primary residence could not be seized by creditors.
Boundaries.
The Texan leaders at first intended to extend their national boundaries to the Pacific Ocean, but ultimately decided to claim the Rio Grande as boundary, including much of New Mexico, which the Republic never controlled. They also hoped, after peace was made with Mexico, to run a railroad to the Gulf of California to give "access to the East Indian, Peruvian and Chilean trade."
When negotiating for the possibility of annexation to the US in late 1836, the Texan government instructed its minister Wharton in Washington that if the boundary were an issue, Texas was willing to settle for a boundary at the watershed between the Nueces River and Rio Grande, and leave out New Mexico.
In 1840 the first and only census of the Republic of Texas was taken, recording a population of about 70,000 people. San Antonio and Houston were recorded as the largest and second largest cities respectively.
Diplomatic relations.
On March 3, 1837, US President Andrew Jackson appointed Alcée La Branche American "chargé d'affaires" to the Republic of Texas, thus officially recognizing Texas as an independent republic. France granted official recognition of Texas on September 25, 1839, appointing Alponse Dubois de Saligny to serve as "chargé d'affaires". The French Legation was built in 1841, and still stands in Austin as the oldest frame structure in the city. Conversely, the Republic of Texas embassy in Paris was located in what is now the "Hôtel de Vendôme", adjacent to the Place Vendôme in Paris' 2e arrondissement.
The Republic also received diplomatic recognition from Belgium, the Netherlands, and the Republic of Yucatán. The United Kingdom never granted official recognition of Texas due to its own friendly relations with Mexico, but admitted Texan goods into British ports on their own terms. In London, the original Embassy of the Republic of Texas still stands, and there is a restaurant with such a name west of Trafalgar Square. Immediately opposite the gates to St. James's Palace, Sam Houston's original Embassy of the Republic of Texas to the Court of St. James's is now a hat shop, but is clearly marked with a large plaque and a nearby restaurant is called Texas Embassy. A plaque on the exterior of 3 St. James's Street in London notes the upper floors of the building (which have housed the noted wine merchant Berry Brothers and Rudd since 1698) housed the Texas Legation.
Statehood.
On February 28, 1845, the US Congress passed a bill that would authorize the United States to annex the Republic of Texas. On March 1, US President John Tyler signed the bill. The legislation set the date for annexation for December 29 of the same year. Faced with imminent American annexation of Texas, Charles Elliot and Alphonse de Saligny, the British and French ministers to Texas, were dispatched to Mexico City by their governments. Meeting with Mexico's foreign secretary, they signed a "Diplomatic Act" in which Mexico offered to recognize an independent Texas with boundaries that would be determined with French and British mediation. Texas President Anson Jones forwarded both offers to a specially elected convention meeting at Austin, and the American proposal was accepted with only one dissenting vote. The Mexican proposal was never put to a vote. Following the previous decree of President Jones, the proposal was then put to a vote throughout the republic.
On October 13, 1845, a large majority of voters in the republic approved both the American offer and the proposed constitution that specifically endorsed slavery and emigrants bringing slaves to Texas. This constitution was later accepted by the US Congress, making Texas a US state on the same day annexation took effect, December 29, 1845 (therefore bypassing a territorial phase). One of the motivations for annexation was the huge debts which the Republic of Texas government had incurred. As part of the Compromise of 1850, in return for $10,000,000 in Federal bonds, Texas dropped claims to territory which included parts of present-day Colorado, Kansas, Oklahoma, New Mexico, and Wyoming.
The resolution did include two unique provisions: First, it said up to four additional states could be created from Texas' territory with the consent of the State of Texas (and that new states north of the Missouri Compromise Line would be free states). Though the resolution did not make exceptions to the constitution, the U.S. Constitution neither requires Congressional consent to the creation of new states to be "ex post" to applications nor to expire. To show the strength of the latter caveat, the 27th Amendment was submitted in the 18th century, yet was not ratified until the 1990s; thus, congressional consent via the resolution to new states would not expire, or require renewal. Second, Texas did not have to surrender its public lands to the federal government. While Texas did cede all territory outside of its current area to the federal government in 1850, it did not cede any public lands within its current boundaries. Consequently, the lands in Texas owned by the federal government are those which were subsequently purchased by it. This also means the state government has control over oil reserves which were later used to fund the state's public university system through the Permanent University Fund. In addition, the state's control over offshore oil reserves in Texas runs out to 3 nautical leagues (9 nautical miles, 10.357 statute miles, 16.668 km) rather than three nautical miles (3.45 statute miles, 5.56 km) as with other states.

</doc>
<doc id="25458" url="http://en.wikipedia.org/wiki?curid=25458" title="Rome">
Rome

Rome (, Italian: "Roma" ], Latin: "Rōma") is a city and special "comune" (named "Roma Capitale") in Italy. Rome is the capital of Italy and region of Lazio. With 2.9 million residents in 1285 km2, it is also the country's largest and most populated "comune" and fourth-most populous city in the European Union by population within city limits. The Metropolitan City of Rome has a population of 4.3 million residents. The city is located in the central-western portion of the Italian Peninsula, within Lazio (Latium), along the shores of Tiber river. Vatican City is an independent country within the city boundaries of Rome, the only existing example of a country within a city: for this reason Rome has been often defined as capital of two states.
Rome's history spans more than two and a half thousand years. While Roman mythology dates the founding of Rome at only around 753 BC, the site has been inhabited for much longer, making it one of the oldest continuously occupied cities in Europe. The city's early population originated from a mix of Latins, Etruscans and Sabines. Eventually, the city successively became the capital of the Roman Kingdom, the Roman Republic and the Roman Empire, and is regarded as one of the birthplaces of Western civilization. It is referred to as "Roma Aeterna" (The Eternal City) and "Caput Mundi" (Capital of the World), two central notions in ancient Roman culture.
After the fall of the Empire, which marked the beginning of the Middle Ages, Rome slowly fell under the political control of the Papacy, which had settled in the city since the 1st century AD, until in the 8th century it became the capital of the Papal States, which lasted until 1870.
Beginning with the Renaissance, almost all the popes since Nicholas V (1422–55) pursued coherently along four hundred years an architectonic and urbanistic program aimed to make of the city the world's artistic and cultural center. Due to that, Rome became first one of the major centers of the Italian Renaissance, and then the birthplace of the Baroque style. Famous artists and architects of the Renaissance and Baroque period made Rome the center of their activity, creating masterpieces throughout the city. In 1871 Rome became the capital of the Kingdom of Italy, and in 1946 that of the Italian Republic.
Rome has the status of a global city. In 2011, Rome was the 18th-most-visited city in the world, 3rd most visited in the European Union, and the most popular tourist attraction in Italy. Its historic centre is listed by UNESCO as a World Heritage Site. Monuments and museums such as the Vatican Museums and the Colosseum are among the world's most visited tourist destinations with both locations receiving millions of tourists a year. Rome hosted the 1960 Summer Olympics and is the seat of United Nations' Food and Agriculture Organization (FAO).
Etymology.
According to the founding myth of the city by the Ancient Romans themselves, the long-held tradition of the origin of the name "Roma" is believed to have come from the city's founder and first king Romulus.
However, it is likely that the name Romulus was actually derived from Rome itself. As early as the 4th century, there has been alternate theories proposed on the origin of the name Roma. Several hypotheses have been advanced focusing on its uncertain linguistic roots.:
History.
Earliest history.
There is archaeological evidence of human occupation of the Rome area from approximately 14,000 years ago, but the dense layer of much younger debris obscures Palaeolithic and Neolithic sites. Evidence of stone tools, pottery and stone weapons attest to about 10,000 years of human presence. Several excavations support the view that Rome grew from pastoral settlements on the Palatine Hill built above the area of the future Roman Forum. While some archaeologists argue that Rome was indeed founded in the middle of the 8th century BC (the date of the tradition), the date is subject to controversy. However, the power of the well known tale of Rome's legendary foundation tends to deflect attention from its actual, more ancient, origins. 
Legend of the founding of Rome.
Traditional stories handed down by the ancient Romans themselves explain the earliest history of their city in terms of legend and myth. The most familiar of these myths, and perhaps the most famous of all Roman myths, is the story of Romulus and Remus, the twins who were suckled by a she-wolf. They decided to build a city, but after an argument, Romulus killed his brother. According to the Roman annalists, this happened on 21 April 753 BC. This legend had to be reconciled with a dual tradition, set earlier in time, that had the Trojan refugee Aeneas escape to Italy and found the line of Romans through his son Iulus, the namesake of the Julio-Claudian dynasty.
This was accomplished by the Roman poet Virgil in the first century BC.
Monarchy, republic, empire.
After the legendary foundation by Romulus, Rome was ruled for a period of 244 years by a monarchical system, initially with sovereigns of Latin and Sabine origin, later by Etruscan kings. The tradition handed down seven kings: Romulus, Numa Pompilius, Tullus Hostilius, Ancus Marcius, Tarquinius Priscus, Servius Tullius and Tarquin the Proud.
In 509 BC the Romans expelled from the city the last king and established an oligarchic republic: since then, for Rome began a period characterized by internal struggles between patricians (aristocrats) and plebeians (small landowners), and by constant warfare against the populations of central Italy: Etruscans, Latins, Volsci, Aequi. After becoming master of Latium, Rome led several wars (against the Gauls, Osci-Samnites and the Greek colony of Taranto, allied with Pyrrhus, king of Epirus) whose result was the conquest of the Italian peninsula, from the central area up to Magna Graecia.
The third and second century BC saw the establishment of the Roman hegemony over the Mediterranean and the East, through the three Punic Wars (264-146 BC) fought against the city of Carthage and the three Macedonian Wars (212-168 BC) against Macedonia. Then were established the first Roman provinces: Sicily, Sardinia and Corsica, Spain, Macedonia, Greece (Achaia), Africa.
From the beginning of the 2nd century BC, the power was contended between two groups of aristocrats: the optimates, representing the conservative part of the Senate, and the populares, which relied on the help of the urban populace to gain the power. In the same period, the bankruptcy of the small farmers and the establishment of large slave estates provoked the migration to the city of a large number of people. The continuous warfare made necessary a professional army, which was more loyal to its chiefs than to the republic. Due to that, in the second half of the second century and during the first century BC saw fights abroad and at home: after the failed attempt of social reform of Tiberius and Gaius Gracchus, and the war against Jugurtha, there was a first civil war between Gaius Marius and Sulla. To this followed the slave revolt under Spartacus, and the establishment of a first Triumvirate with Caesar, Pompey and Crassus.
The conquest of Gaul let rise the star of Caesar, who fought a second civil war against the Senate and Pompey and, after his victory, established a lifelong dictatorship. His assassination led to a second Triumvirate among Octavian (Caesar's nephew and heir), Mark Antony and Lepidus, and to another civil war between the Octavian and Antony. The former in 27 BC became "princeps civitatis" and got the title of Augustus, founding the principate, a diarchy between the "princeps" and the senate. Established de facto the empire, which reached its greatest expansion in the second century under the Emperor Trajan, Rome was confirmed as caput Mundi, i.e. the capital of the world, an expression which had already been given in the Republican period. During its first two centuries the empire saw as rulers, after Octavian Augustus, the emperors of the Julio-Claudian, Flavian (who also built eponymous amphitheater, known as the Colosseum) and Antonine dynasties. This time was also characterized by the spread of the Christian religion, preached by Jesus Christ in Judea in the first half of the century (under Tiberius) and popularized by his apostles through the empire. The Antonine age is considered the apogee of the Empire, whose territory ranged from the Atlantic Ocean to the Euphrates, from the central-northern part of Britain to Egypt.
In the third century, at the end of the Antonine dynasty, with the Severan dynasty the "principatus" was substituted by a military government, which was soon followed by a destabilising period of military anarchy, while the economy deteriorated, the inflation rose and the historical enemies of Rome, the Germanic tribes in the West and the Persian Empire in the East, bore a continue pressure on the frontiers.
Emperor Diocletian (284) fought the economic and military problems introducing the dominate, an absolute monarchy where the emperor was deified, imposing the price control and decentralising the administration: the emperor divided the empire into twelve dioceses, ruling under the title of "Augustus" the eastern half (with residence in Nicomedia) and naming Maximian "Augustus" of the western half, whose capital was moved to Mediolanum. The succession was regulated with the creation of the Tetrarchy: each "Augustus", in fact, had to appoint a junior emperor, named "Caesar", which would rule part of the roman territory on behalf of his "Augustus" and which would become, at the end, the new emperor.
After the abdication of Diocletian and Maximian in 305 and many dynastic fights, this system collapsed, and the new ruler, Constantine, centralized power again and, with the Edict of Milan in 313, gave freedom of worship for Christians, pledging himself to give stability to the new religion. He built several churches, gave the civil power of Rome to Pope Sylvester I and founded in the eastern part the new capital, Constantinople.
Christianity became the official religion of the empire, thanks to an edict issued in 380 by Theodosius, who was the last emperor of a unified empire: after his death, in fact, his sons, Arcadius and Honorius, divided the empire in a western and an eastern part. The capital of the western Roman Empire became Ravenna.
Rome, which had lost its central role in the administration of the empire, was sacked in 410 by the Visigoths led by Alaric I, but also embellished by the construction of sacred buildings by the popes (with the collaboration of the emperors). The city, impoverished and depopulated, suffered a new looting in 455, by Genseric, king of the Vandals. The weak emperors of the fifth century could not stop the decay, until the deposition of Romulus Augustus on 22 August 476 marked the end of the Western Roman Empire and, for many historians, the beginning of the Middle Ages.
Middle Ages.
The Bishop of Rome, called the Pope, was important since the early days of Christianity because of the martyrdom of both the apostles Peter and Paul there. The Bishops of Rome were also seen (and still are seen by the Catholics) as the successors of Peter, he being the first Bishop of Rome. The city thus became of increasing importance in the Catholic Church. After the fall of the Western Roman Empire in 476 AD, Rome was first under the control of Odoacer and then became part of the Ostrogothic Kingdom before returning to East Roman control after the Gothic War, which devastated the city. Its population declined from more than a million in 210 AD to 500,000 in 273 to 35,000 after the Gothic War, reducing the sprawling city to groups of inhabited buildings interspersed among large areas of ruins, vegetation, vineyards and market gardens.
After the Lombard invasion of Italy, the city remained nominally Byzantine, but in reality the popes pursued a policy of equilibrium between the Byzantines, the Franks and the Lombards. In 729, the Lombard king Liutprand donated to the church the north Latium town of Sutri, starting the temporal power of the church. In 756, Pepin the Short, after having defeated the Lombards, gave to the Pope temporal jurisdiction over the Roman Duchy and the Exarchate of Ravenna, thus creating the Papal States. Since this period three powers tried to rule the city: the pope, the nobility, together with the chefs of militias, the judges, the Senate and the populace; and the Frankish king, as king of the Lombards, patricius and Emperor. These three parties (theocratic, republican and imperial) were a characteristic of Roman life during the whole Middle Ages. On the Christmas night of 800, Charlemagne was crowned in Rome first emperor of the Holy Roman Empire by Pope Leo III: in that occasion the city hosted for the first time the two powers whose struggle for the universal power had to be a constant of the Middle Ages.
In 846, Muslim Arabs unsuccessfully stormed the city's walls, but managed to loot St. Peter's and St. Paul's basilica, both outside the city wall. After the decay of Carolingian power, Rome fell prey to feudal anarchy: several noble families kept fighting against the pope, the emperor and each other. These were the fishy times of Theodora and her daughter Marozia, concubines and mothers of several popes, and of Crescentius, a powerful feudal lord, who fought against the emperors Otto II and III. The scandals of this period pushed the papacy to reform itself: the election of the pope was reserved to the cardinals, and a reform of the clergy was attempted: but the driving force behind this renewal, monk Ildebrando da Soana, once elected pope under the name of Gregory VII, got involved into the Investiture Controversy against Emperor Henry IV. In that occasion Rome was sacked and burned by the Normans of Robert Guiscard, rushed to the Pope`s aid, who was besieged in Castel S. Angelo.
During this period, the city was autonomously ruled by a "senatore" or "patrizio": in the 12th century. This administration, as often in the Italian cities, evolved into the commune, a new form of social organisation, expression of the new wealthy classes. Pope Lucius II had already to fight against the Roman commune, and the struggle was continued by his successor pope Eugenius III: then the commune, allied with the nobility, was supported by Arnaldo da Brescia, a monk who was a religious and social reformer. After the pope's death, Arnaldo was taken prisoner by Adrianus IV, which marked the end of the comune's autonomy. Under Pope Innocent III, whose reign marked the apogee of the papacy, the commune liquidated the senate, and replaced it with a "Senatore", who was subject to the pope.
In this period the papacy played a role of secular importance in Western Europe, often acting as arbitrators between Christian monarchs and exercising additional political powers.
In 1266 Charles of Anjou, who was heading south to fight the Hohenstaufen on behalf of the pope, was appointed Senator. Charles founded the Sapienza, the university of Rome. In that period the pope died, and the cardinals, summoned in Viterbo, could not agree about his successor: the people of the city, got angry about that, unroofed the building where they had met, imprisoning them until they had nominated the new pope: this happening marked the birth of the conclave. In this period the city was also shattered by continuous fights among the noble families: Annibaldi, Caetani, Colonna, Orsini, Conti, nested in their fortresses built above ancient Roman edifices, fought each other to control the papacy.
Pope Boniface VIII, born Caetani, was the last pope to fight for the church's universal domain: he proclaimed a crusade against the Colonna, and in 1300 he called for the first Jubilee of Christianity, which brought to Rome millions of pilgrims. However his hopes were crushed against the French king Philip the Fair, who let him taken prisoner and slashed in Anagni, causing his death. Afterwards, a new pope faithful to the French was elected, and the papacy was briefly relocated to Avignon (1309–1377). During this period the city was neglected, until the power fell in the hand of a plebeian man, Cola di Rienzo. An idealist and a lover of ancient Rome, Cola dreamed about a ribirth of the Roman Empire: after assuming the power with the title of "Tribuno", his reforms were rejected by the populace. Forced to flee, Cola could come back among the suite of cardinal Albornoz, in charge of restoring the church power in Italy. Back in power for a short time, he was lynched by the populace, and Albornoz could take possession of the city, that in 1377 under Gregory XI became again the seat of the papacy. The return of the pope to Rome in that year unleashed the western Schism (1377–1418), and during the next forty years, the city was prey of the fights which shattered the church.
Early modern.
In 1418, the Council of Constance settled the western schism, and a Roman pope, Martin V, was elected.
This brought to Rome a century of internal peace, which marked the beginning of the Renaissance. The ruling popes until the first half of the 16th century, from Nicholas V, founder of the Vatican Library, to Pius II, humanist and literate, from Sixtus IV, a warrior pope, to Alexander VI, immoral and nepotist, from Julius II, soldier and patron, to Leo X, who gave his name to this period ("the century of Leo X"), all devoted their energy to the greatness and the beauty of the Eternal City, to the power of their stock, and to the patronage of the arts. During those years the center of the Italian Renaissance moved to Rome from Florence. Majestic works, as the new Saint Peter's Basilica, the Sistine Chapel and "Ponte Sisto" (the first bridge to be built across the Tiber since antiquity, although on Roman foundation) were created. To accomplish that the Popes engaged the best artists of the time, including Michelangelo, Perugino, Raphael, Ghirlandaio, Luca Signorelli, Botticelli, and Cosimo Rosselli.
The period was also infamous for papal corruption, with many Popes fathering children, and engaging in nepotism and simony. The corruption of the Popes and the huge expenses for their building projects led, in part, to the Reformation and, in turn, the Counter-Reformation. Popes, such as Alexander VI, were well known for their decadence, wild parties, extravagance and immoral lives. However, under these extravagant and rich popes, Rome was transformed into a centre of art, poetry, music, literature, education and culture. Rome became able to compete with other major European cities of the time in terms of wealth, grandeur, the arts, learning and architecture.
The Renaissance period changed Rome's face dramatically, with works like the Pietà by Michelangelo and the frescoes of the Borgia Apartment, all made during Innocent's reign. Rome reached the highest point of splendour under Pope Julius II (1503–1513) and his successors Leo X and Clement VII, both members of the Medici family.
In this twenty-year period, Rome became one of the greatest centres of art in the world. The old St. Peter's Basilica built by Emperor Constantine the Great (which by then was in a dilapidated state) was demolished and a new one begun. The city hosted artists like Ghirlandaio, Perugino, Botticelli and Bramante, who built the temple of San Pietro in Montorio and planned a great project to renovate the Vatican. Raphael, who in Rome became one of the most famous painters of Italy, created frescoes in the Villa Farnesina, the Raphael's Rooms, plus many other famous paintings. Michelangelo started the decoration of the ceiling of the Sistine Chapel and executed the famous statue of the Moses for the tomb of Julius II. Rome lost in part its religious character, becoming increasingly a true Renaissance city, with a great number of popular feasts, horse races, parties, intrigues and licentious episodes. 
Its economy was rich, with the presence of several Tuscan bankers, including Agostino Chigi, who was a friend of Raphael and a patron of arts. Before his early death, Raphael also promoted for the first time the preservation of the ancient ruins. The fight between France and Spain in Europe caused the first plunder of the city in more than one thousand years. In 1527, the Landsknechts of Emperor Charles V sacked the city, putting to an abrupt end the golden age of the Renaissance in Rome.
Beginning with the Council of Trent in 1545, the Church began the Counter-Reformation as an answer to the Reformation, a large-scale questioning of the Church's authority on spiritual matters and governmental affairs. (This loss of confidence then led to major shifts of power away from the Church.) Under the popes from Pius IV to Sixtus V, Rome became the centre of the reformed Catholicism and saw the installment of new monuments which celebrated the papacy's restored greatness. The popes and cardinals of the 17th and early 18th centuries continued the movement by having city's landscape enriched with baroque buildings.
 This was another nepotistic age: the new noble families (Barberini, Pamphili, Chigi, Rospigliosi, Altieri, Odescalchi) were protected by their respective popes, who built for their relatives huge baroque buildings. During the Age of Enlightenment, new ideas reached also the Eternal City, where the papacy supported archaeological studies and improved the people's welfare. But not everything went well for the Church during the Counter-Reformation. There were setbacks in the attempts to restrain the anti-Church policies of European powers of the time, the most notable setback perhaps being in 1773 when Pope Clement XIV was forced by secular powers to have the Jesuit order suppressed.
Late modern and contemporary.
The rule of the Popes was interrupted by the short-lived Roman Republic (1798–1800), which was built under the influence of the French Revolution. The Papal States were restored in June 1800, but during Napoleon's reign Rome was annexed as a "Département" of the French Empire: first as "Département du Tibre" (1808–10) and then as "Département Rome" (1810–14). After the fall of Napoleon, the Church State under the pope was reinstated through the Congress of Vienna of 1814.
In 1849 another Roman Republic arose within the framework of the revolutions of 1848. Two of the most influential figures of the Italian unification, Giuseppe Mazzini and Giuseppe Garibaldi, fought for the short-lived republic.
Rome then became the focus of hopes of Italian reunification, as the rest of Italy was reunited as the Kingdom of Italy, with a temporary capital at Florence. In 1861 Rome was declared capital of Italy even though it was still under the Pope's control. During the 1860s, the last vestiges of the Papal States were under French protection, thanks to the foreign policy of Napoleon III. It was only when this was lifted in 1870, owing to the outbreak of the Franco-Prussian War, that Italian troops were able to capture Rome entering the city through a breach near Porta Pia. Afterwards, Pope Pius IX declared himself as prisoner in the Vatican, and in 1871 the capital of Italy was finally moved from Florence to Rome.
Soon after World War I, Rome witnessed the rise of Italian Fascism, led by Benito Mussolini, who marched on the city in 1922, eventually declaring a new Italian Empire and allying Italy with Nazi Germany. Mussolini pulled down large parts of the city center in order to build wide avenues and squares which were supposed to celebrate the fascist regime and the resurgence of classical Rome. The interwar period saw a rapid growth in the city's population, which surpassed one million inhabitants. In World War II, due to its art treasuries and the presence of Vatican, Rome largely escaped the tragic destiny of other European cities. However, on 19 July 1943 the San Lorenzo district was bombed by Anglo-American forces, resulting in about 3,000 deaths and 11,000 wounded. After the fall of Mussolini and the Italian Armistice on 8 September 1943, the city was occupied by the Germans and declared an open city until its liberation on 4 June 1944.
Rome developed momentously after the war, as one of the driving forces behind the "Italian economic miracle" of post-war reconstruction and modernisation in the 1950s and early 1960s. During this period, the years of "la dolce vita" ("the sweet life"), Rome became a fashionable city, with popular classic films such as "Ben Hur", "Quo Vadis", "Roman Holiday" and "La Dolce Vita" filmed in the city's iconic Cinecittà film studios. The rising trend in population growth continued until the mid-1980s, when the "comune" had more than 2.8 million residents. After that, population started to decline slowly as inhabitants began to move to nearby suburbs of Rome.
Government.
Local government.
Rome constitutes a "comune speciale", named "Roma Capitale", and is the largest both in terms of land area and population among the 8,101 "comuni" of Italy. It is governed by a mayor, currently Ignazio Marino, and a city council. The seat of the "comune" is the "Palazzo Senatorio" on the Capitoline Hill, the historic seat of the city government. The local administration in Rome is commonly referred to as "Campidoglio", the Italian name of the hill.
Administrative and historical subdivisions.
Since 1972 the city has been divided into administrative areas, called "municipi" (sing. "municipio") (until 2001 named "circoscrizioni"). They were created for administrative reasons to increase decentralisation in the city. Each "municipio" is governed by a president and a council of four members who are elected by its residents every five years. The "municipi" frequently cross the boundaries of the traditional, non-administrative divisions of the city.<br> The municipi where originally 20, then 19. In 2013 their number has been reduced to 15.
Rome is also divided into differing types of non-administrative units. The historic centre is divided into 22 "rioni", all of which are located within the Aurelian Walls except Prati and Borgo.
These originate from the Regiones of ancient Rome, which evolved in the Middle Ages into the medieval rioni. In the Renaissance, under Pope Sixtus V, they reached again the number of fourteen, and their boundaries were finally defined under Pope Benedict XIV in 1743.
A new subdivision of the city under Napoleon was ephemeral, and there were no sensible changes in the organisation of the city until 1870, when Rome became the third capital of Italy. The needs of the new capital led to an explosion both in the urbanisation and in the population within and outside the Aurelian walls. In 1874 a fifteenth rione, Esquilino, was created on the newly urbanised zone of Monti. At the beginning of the 20th century other rioni where created (the last one was Prati – the only one outside the Walls of Pope Urban VIII – in 1921). Afterward, for the new administrative subdivisions of the city the name "quartiere" was used. Today all the rioni are part of the first Municipio, which therefore coincides completely with the "historical city" ("Centro Storico").
Metropolitan and regional government.
Rome is the principal town of the Metropolitan City of Rome, operative since 1 January 2015. The Metropolitan City replaced the old province, which included the city's metropolitan area and extends further north until Civitavecchia. The Metropolitan City of Rome is the largest by area in Italy. At 5352 km2, its dimensions are comparable to the region of Liguria. Moreover, the city is also the capital of the Lazio region.
National government.
Rome is the national capital of Italy and is the seat of the Italian Government. The official residences of the President of the Italian Republic and the Italian Prime Minister, the seats of both houses of the Italian Parliament and that of the Italian Constitutional Court are located in the historic centre. The state ministries are spread out around the city; these include the Ministry of Foreign Affairs, which is located in Palazzo della Farnesina near the Olympic stadium.
Geography.
Location.
Rome is in the Lazio region of central Italy on the Tiber river (Italian: "Tevere"). The original settlement developed on hills that faced onto a ford beside the Tiber Island, the only natural ford of the river in this area. The Rome of the Kings was built on seven hills: the Aventine Hill, the Caelian Hill, the Capitoline Hill, the Esquiline Hill, the Palatine Hill, the Quirinal Hill, and the Viminal Hill. Modern Rome is also crossed by another river, the Aniene, which flows into the Tiber north of the historic centre.
Although the city centre is about 24 km inland from the Tyrrhenian Sea, the city territory extends to the shore, where the south-western district of Ostia is located. The altitude of the central part of Rome ranges from 13 m above sea level (at the base of the Pantheon) to 139 m above sea level (the peak of Monte Mario). The "Comune" of Rome covers an overall area of about 1285 km2, including many green areas.
Topography.
Throughout the history of Rome, the urban limits of the city were considered to be the area within the city walls. Originally, these consisted of the Servian Wall, which was built twelve years after the Gaulish sack of the city in 390 BC. This contained most of the Esquiline and Caelian hills, as well as the whole of the other five. Rome outgrew the Servian Wall, but no more walls were constructed until almost 700 years later, when, in 270 AD, Emperor Aurelian began building the Aurelian Walls. These were almost 19 km long, and were still the walls the troops of the Kingdom of Italy had to breach to enter the city in 1870. The city's urban area is cut in two by its ring-road, the "Grande Raccordo Anulare" ("GRA"), finished in 1962, which circles the city centre at a distance of about 10 km. Although when the ring was completed most part of the inhabited area lay inside it (one of the few exceptions was the former village of Ostia, which lies along the tyrrhenian coast), in the meantime quarters have been built which extend up to 20 km beyond it.
The "comune" covers an area roughly three times the total area within the "Raccordo" and is comparable in area to the entire metropolitan cities of Milan and Naples, and to an area six times the size of the territory of these cities. It also includes considerable areas of abandoned marsh land which is suitable neither for agriculture nor for urban development.
As a consequence, the density of the "comune" is not that high, its territory being divided between highly urbanised areas and areas designated as parks, nature reserves, and for agricultural use.
Climate.
Rome enjoys a Mediterranean climate (Köppen climate classification: "Csa"), with cool, humid winters and hot, dry summers.
Its average annual temperature is above 20 °C during the day and 10 °C at night. In the coldest month – January, the average temperature is 12 °C during the day and 3 °C at night. In the warmest months – July and August, the average temperature is 30 °C during the day and 18 °C at night.
December, January and February are the coldest months, with average temperatures around 12.5 °C during the day and 3.6 °C at night. Temperatures generally vary between 10 and during the day and between 3 and at night, with colder or warmer spells occurring frequently. Snowfall is rare but not unheard of, with light snow or flurries occurring almost every winter, generally without accumulation, and major snowfalls once every 20 or 25 years (the last one in 2012).
The average relative humidity is 75%, varying from 72% in July to 77% in November. Sea temperatures vary from a low of 13 °C in February and March to a high of 24 °C in August.
Demographics.
In 550 BC Rome was the second largest city in Italy, with Tarentum being the largest. It had an area of about 285 hectares (1.1 sq mile) and an estimated population of 35,000. Other sources suggest the population was just under 100,000 from 600–500 BC. When the Republic was founded in 509 BC the census recorded a population of 130,000. The republic included the city itself and the immediate surroundings. Other sources suggest a population of 150,000 in 500 BC. It surpassed 300,000 in 150 BC.
At the time of the Emperor Augustus, Rome was the largest city in the world: with a population of about one million people (about the size of London in the early 19th century, when London was the largest city in the world).
After the fall of the Western Roman Empire, the city's population declined to less than 50,000 people. It continued to stagnate or shrink until the Renaissance. When the Kingdom of Italy annexed Rome in 1870, the city had a population of about 200,000. This increased to 600,000 by the eve of World War I. The Fascist regime of Mussolini tried to block an excessive demographic rise of the city, but failed to prevent it from reaching one million people by the early 1930s. Population growth continued after the Second World War, helped by a post-war economic boom. A construction boom also created a large number of suburbs during the 1950s and 1960s.
In mid-2010, there were 2,754,440 residents in the city proper, while some 4.2 million people lived in the greater Rome area (which can be approximately identified with its administrative metropolitan city, with a population density of about 800inhab./km2 stretching over more than 5,000 km²). Minors (children ages 18 and younger) totalled 17.00 percent of the population compared to pensioners who number 20.76 percent. This compares with the Italian average of 18.06 percent (minors) and 19.94 percent (pensioners). The average age of a Roman resident is 43 compared to the Italian average of 42. In the five years between 2002 and 2007, the population of Rome grew by 6.54 percent, while Italy as a whole grew by 3.56 percent. The current birth rate of Rome is 9.10 births per 1,000 inhabitants compared to the Italian average of 9.45 births.
The urban area of Rome extends beyond the administrative city limits with a population of around 3.9 million. Between 3.2 and 4.2 million people live in the Rome metropolitan area.
Ethnic groups.
According to the latest statistics conducted by ISTAT, approximately 9.5% of the population consists of non-Italians. About half of the immigrant population consists of those of various other European origins (chiefly Romanian, Polish, Ukrainian, and Albanian) numbering a combined total of 131,118 or 4.7 percent of the population. The remaining 4.8 percent are those with non-European origins, chiefly Filipinos (26,933), Bangladeshis (12,154), and Chinese (10,283).
The Esquilino "rione", off Termini Railway Station, has evolved into a largely immigrant neighbourhood. It is perceived as Rome's Chinatown. Immigrants from more than a hundred different countries reside there. A commercial district, Esquilino contains restaurants featuring many kinds of international cuisine. There are wholesale clothes shops. Of the 1,300 or so commercial premises operating in the district 800 are Chinese-owned; around 300 are run by immigrants from other countries around the world; 200 are owned by Italians.
Religion.
Much like the rest of Italy, Rome is predominantly Roman Catholic, and the city has been an important centre of religion and pilgrimage for centuries, the base of the ancient Roman Religion with the pontifex maximus and later the seat of the Vatican and the pope. Before the arrival of the Christians in Rome, the Religio Romana (literally, the "Roman Religion") was the major religion of the city in classical antiquity. The first gods held sacred by the Romans were Jupiter, the most high, and Mars, god of war, and father of Rome's twin founders, Romulus and Remus, according to tradition. Other gods and goddesses such as Vesta and Minerva were honoured. Rome was also the base of several mystery cults, such as Mithraism. Later, after St Peter and St Paul were martyred in the city, and the first Christians began to arrive, Rome became Christian, and the Old St. Peter's Basilica was constructed in 313 AD. Despite some interruptions (such as the Avignon papacy), Rome has for centuries been the home of the Roman Catholic Church and the Bishop of Rome, otherwise known as the Pope.
Despite the fact that Rome is home to the Vatican City and St. Peter's Basilica, Rome's cathedral is the Basilica of St. John Lateran, located to the south-east of the city-centre. There are around 900 churches in Rome in total, aside from the cathedral itself, some others of note include: the Basilica di Santa Maria Maggiore, the Basilica of Saint Paul Outside the Walls, the Basilica di San Clemente, San Carlo alle Quattro Fontane and the Church of the Gesu. There are also the ancient Catacombs of Rome underneath the city. Numerous highly important religious educational institutions are also in Rome, such as the Pontifical Lateran University, Pontifical Biblical Institute, Pontifical Gregorian University, and Pontifical Oriental Institute.
In recent years, there has been a significant growth in Rome's Muslim community, mainly due to immigration from North African and Middle Eastern countries into the city. As a result of this increase of the local practitioners of the Islamic faith, the "comune" promoted the building of the Mosque of Rome, which is the largest mosque in Europe, that was designed by architect Paolo Portoghesi and inaugurated on 21 June 1995. Since the end of the Roman Republic, Rome is also the center of an important Jewish community, which was once based in Trastevere, and later in the Roman Ghetto. There lies also the major synagogue in Rome, the "Tempio Maggiore".
Vatican City.
St. Peter's Square in Vatican City.
The territory of Vatican City is part of the "Mons Vaticanus" (Vatican Hill), and of the adjacent former Vatican Fields, where St. Peter's Basilica, the Apostolic Palace, the Sistine Chapel, and museums were built, along with various other buildings. The area was part of the Roman rione of Borgo until 1929. Being separated from the city on the west bank of the Tiber river, the area was an suburb that was protected by being included within the walls of Leo IV, later expanded by the current fortification walls of Paul III/Pius IV/Urban VIII.
When the Lateran Treaty of 1929 that created the Vatican state was being prepared, the boundaries of the proposed territory were influenced by the fact that much of it was all but enclosed by this loop. For some tracts of the frontier, there was no wall, but the line of certain buildings supplied part of the boundary, and for a small part of the frontier a modern wall was constructed.
The territory includes Saint Peter's Square, separated from the territory of Italy only by a white line along the limit of the square, where it borders Piazza Pio XII. St. Peter's Square is reached through the Via della Conciliazione, which runs from the Tiber River to St. Peter's. This grand approach was designed by architects Piacentini and Spaccarelli, for want of Benito Mussolini and in accordance with the church, after the conclusion of the Lateran Treaty. According to the Lateran Treaty, certain properties of the Holy See located in Italian territory, most notably the Papal Palace of Castel Gandolfo and the major basilicas, enjoy extraterritorial status similar to that of foreign embassies.
Pilgrimage.
Rome has been a major Christian pilgrimage site since the Middle Ages. People from all over the Christian world visit Vatican City, within the city of Rome, the seat of the papacy. The Pope was the most influential figure during the Middle Ages. The city became a major pilgrimage site during the Middle Ages and the focus of struggles between the Papacy and the Holy Roman Empire starting with Charlemagne, who was crowned its first emperor in Rome in 800 by Pope Leo III. Apart from brief periods as an independent city during the Middle Ages, Rome kept its status as Papal capital and "holy city" for centuries, even when the Papacy briefly relocated to Avignon (1309–1377). Catholics believe that the Vatican is the last resting place of St. Peter.
Pilgrimages to Rome can involve visits to a large number of sites, both within the Vatican City and in Italian territory. A popular stopping point is the Pilate's stairs: these are, according to the Christian tradition, the steps that led up to the praetorium of Pontius Pilate in Jerusalem, which Jesus Christ stood on during his Passion on his way to trial. The stairs were, reputedly, brought to Rome by St. Helena in the 4th Century. For centuries, the "Scala Santa" has attracted Christian pilgrims who wished to honor the Passion of Jesus. Object of pilgrimage are also several catacombs built in the Roman age, in which Christians prayed, buried their dead and performed worship during periods of persecution, and various national churches (among them San Luigi dei francesi and Santa Maria dell'Anima), or churches associated with individual religious orders, such as the Jesuit Churches of Jesus and Sant`Ignazio.
Traditionally, pilgrims in Rome and Roman citizens thanking God for a grace should visit by foot the seven pilgrim churches (Italian: "Le sette chiese") in 24 hours. This custom, mandatory for each pilgrim in the Middle Ages, was codified in the 16th century by Saint Philip Neri. The seven churches are the four major Basilicas (St Peter in Vatican, St Paul outside the Walls, St John in Lateran and Santa Maria Maggiore), while the other three are San Lorenzo fuori le mura (a paleochristian Basilica), Santa Croce in Gerusalemme (a church founded by Helena, the mother of Constantine, which hosts fragments of wood attributed to the holy cross) and San Sebastiano fuori le mura (which lies on the Appian Way and is built above Roman catacombs).
Cityscape.
The Colosseum and the Arch of Constantine.
Architecture.
Rome's architecture over the centuries has greatly developed, especially from the Classical and Imperial Roman styles to modern Fascist architecture. Rome was for a period one of the world's main epicentres of classical architecture, developing new forms such as the arch, the dome and the vault. The Romanesque style in the 11th, 12th and 13th centuries was also widely used in Roman architecture, and later the city became one of the main centres of Renaissance and Baroque architecture.
Ancient Rome.
One of the symbols of Rome is the Colosseum (70–80 AD), the largest amphitheatre ever built in the Roman Empire. Originally capable of seating 60,000 spectators, it was used for gladiatorial combat. A list of important monuments and sites of ancient Rome includes the Roman Forum, the Domus Aurea, the Pantheon, Trajan's Column, Trajan's Market, the Catacombs, the Circus Maximus, the Baths of Caracalla, Castel Sant'Angelo, the Mausoleum of Augustus, the Ara Pacis, the Arch of Constantine, the Pyramid of Cestius, and the Bocca della Verità.
Medieval.
Often overlooked, Rome's medieval heritage is one of the largest in Italian cities. Basilicas dating from the Paleochristian age include Santa Maria Maggiore and San Paolo Fuori le Mura (the latter largely rebuilt in the 19th century), both housing precious 4th century AD mosaics. Later notable medieval mosaic and fresco art can be also found in the churches of Santa Maria in Trastevere, Santi Quattro Coronati, and Santa Prassede. Lay buildings include a number of towers, the largest being the Torre delle Milizie and the Torre dei Conti, both next the Roman Forum, and the huge staircase leading to the basilica of Santa Maria in Ara Coeli.
Renaissance and Baroque.
Rome was a major world centre of the Renaissance, second only to Florence, and was profoundly affected by the movement. Among others, a masterpiece of Renaissance architecture in Rome is the Piazza del Campidoglio by Michelangelo. During this period, the great aristocratic families of Rome used to build opulent dwellings as the Palazzo del Quirinale (now seat of the President of the Italian Republic), the Palazzo Venezia, the Palazzo Farnese, the Palazzo Barberini, the Palazzo Chigi (now seat of the Italian Prime Minister), the Palazzo Spada, the Palazzo della Cancelleria, and the Villa Farnesina.
Panoramic view of Piazza del Campidoglio, with the copy of the Equestrian Statue of Marcus Aurelius.
Many of the famous city's squares – some huge, majestic and often adorned with obelisks, some small and picturesque – got their present shape during the Renaissance and Baroque. The principal ones are Piazza Navona, Piazza di Spagna, Campo de' Fiori, Piazza Venezia, Piazza Farnese, Piazza della Rotonda and Piazza della Minerva. One of the most emblematic examples of Baroque art is the Fontana di Trevi by Nicola Salvi. Other notable 17th-century baroque palaces are the Palazzo Madama, now the seat of the Italian Senate and the Palazzo Montecitorio, now the seat of the Chamber of Deputies of Italy.
Neoclassicism.
In 1870, Rome became the capital city of the new Kingdom of Italy. During this time, neoclassicism, a building style influenced by the architecture of antiquity, became a predominant influence in Roman architecture. During this period, many great palaces in neoclassical styles were built to host ministries, embassies, and other governing agencies. One of the best-known symbols of Roman neoclassicism is the Monument of Vittorio Emanuele II or "Altar of the Fatherland", where the Grave of the Unknown Soldier, that represents the 650,000 Italians that fell in World War I, is located.
Fascist architecture.
The Fascist regime that ruled in Italy between 1922 and 1943 developed an architectural style that was characterised by its links with ancient Roman architecture. The most important Fascist site in Rome is the E.U.R district, designed in 1938 by Marcello Piacentini. It was originally conceived for the 1942 world exhibition, and was called "E.42" ("Esposizione 42"). The world exhibition, however, never took place because Italy entered the Second World War in 1940. The most representative building of the Fascist style at E.U.R. is the Palazzo della Civiltà Italiana (1938–1943), the iconic design of which has been labelled the cubic of Square Colosseum. After World War II, the Roman authorities found that they already had the seed of an off-centre business district of the type that other capitals were still planning (London Docklands and La Défense in Paris). Also the Palazzo della Farnesina, the current seat of the Italian Ministry of Foreign Affairs, was designed in 1935 in pure Fascist style.
Parks and gardens.
Public parks and nature reserves cover a large area in Rome, and the city has one of the largest areas of green space among European capitals. The most notable part of this green space is represented by the large number of villas and landscaped gardens created by the Italian aristocracy. While most of the parks surrounding the villas were destroyed during the building boom of the late 19th century, some of them remain. The most notable of these are Villa Borghese, Villa Ada, and Villa Doria Pamphili. Villa Doria Pamphili is west of the Gianicolo hill comprising some 1.8 km2. Also on the Gianicolo hill there is Villa Sciarra, with playgrounds for children and shaded walking areas. In the nearby area of Trastevere the Orto Botanico (Botanical Garden) is a cool and shady green space. The old Roman hippodrome (Circus Maximus) is another large green space: it has few trees, but is overlooked by the Palatine and the Rose Garden ('roseto comunale'). Nearby is the lush Villa Celimontana, close to the gardens surrounding the Baths of Caracalla. The Villa Borghese garden is the best known large green space in Rome, with famous art galleries among its shaded walks. Overlooking Piazza del Popolo and the Spanish Steps are the gardens of Pincio and Villa Medici. Noteworthy is also the Pine wood of Castelfusano, near Ostia. Rome also has a number of regional parks of much more recent origin including the Pineto Regional Park and the Appian Way Regional Park. There are also nature reserves at Marcigliana and at Tenuta di Castelporziano.
Fountains and aqueducts.
The Trevi Fountain, one of the most famous in the world.
Rome is a city famous for its numerous fountains, built in all different styles, from Classical and Medieval, to Baroque and Neoclassical. The city has had fountains for more than two thousand years, and they have provided drinking water and decorated the piazzas of Rome. During the Roman Empire, in 98 AD, according to Sextus Julius Frontinus, the Roman consul who was named "curator aquarum" or guardian of the water of the city, Rome had nine aqueducts which fed 39 monumental fountains and 591 public basins, not counting the water supplied to the Imperial household, baths and owners of private villas. Each of the major fountains was connected to two different aqueducts, in case one was shut down for service.
During the 17th and 18th century the Roman popes reconstructed other ruined Roman acqueducts and built new display fountains to mark their termini, launching the golden age of the Roman fountain. The fountains of Rome, like the paintings of Rubens, were expressions of the new style of Baroque art. They were crowded with allegorical figures, and filled with emotion and movement. In these fountains, sculpture became the principal element, and the water was used simply to animate and decorate the sculptures. They, like baroque gardens, were "a visual representation of confidence and power".
Statues.
Rome is well known for its statues but, in particular, the talking statues of Rome. These are usually ancient statues which have become popular soapboxes for political and social discussion, and places for people to (often satirically) voice their opinions. There are two main talking statues: the Pasquino and the Marforio, yet there are four other noted ones: il Babuino, Madama Lucrezia, il Facchino and Abbot Luigi. Most of these statues are ancient Roman or classical, and most of them also depict mythical gods, ancient people or legendary figures; il Pasquino represents Menelaus, Abbot Luigi is an unknown Roman magistrate, il Babuino is supposed to be Silenus, Marforio represents Oceanus, Madama Lucrezia is a bust of Isis, and il Facchino is the only non-Roman statue, created in 1580, and not representing anyone in particular. They are often, due to their status, covered with placards or graffiti expressing political ideas and points of view. Other statues in the city, which are not related to the talking statues, include those of the Ponte Sant'Angelo, or several monuments scattered across the city, such as that to Giordano Bruno in the Campo de'Fiori.
Obelisks and columns.
The city hosts eight ancient Egyptian and five ancient Roman obelisks, together with a number of more modern obelisks; there was also formerly (until 2005) an ancient Ethiopian obelisk in Rome. The city contains some of obelisks in piazzas, such as in Piazza Navona, St Peter's Square, Piazza Montecitorio, and Piazza del Popolo, and others in villas, thermae parks and gardens, such as in Villa Celimontana, the Baths of Diocletian, and the Pincian Hill. Moreover, the centre of Rome hosts also Trajan's and Antonine Column, two ancient Roman columns with spiral relief.
Bridges.
The city of Rome contains numerous famous bridges which cross the Tiber. The only bridge to remain unaltered until today from the classical age is Ponte dei Quattro Capi, which connects the Isola Tiberina with the left bank. The other surviving - albeit modified - ancient Roman bridges crossing the Tiber are Ponte Cestio, Ponte Sant'Angelo and Ponte Milvio. Considering Ponte Nomentano, also built during ancient Rome, which crosses the Aniene, currently there are five ancient Roman bridges still remaining in the city. Other noteworthy bridges are Ponte Sisto, the first bridge built in the Renaissance above Roman foundations; Ponte Rotto, actually the only remaining arch of the ancient "Pons Aemilius", collapsed during the flood of 1598 and demolished at the end of the 19th century; and Ponte Vittorio Emanuele II, a modern bridge connecting Corso Vittorio Emanuele and Borgo. Most of the city's public bridges were built in Classical or Renaissance style, but also in Baroque, Neoclassical and Modern styles. According to the Encyclopædia Britannica, the finest ancient bridge remaining in Rome is the Ponte Sant'Angelo, which was completed in 135 AD, and was decorated with ten statues of the angels, designed by Bernini in 1688.
Catacombs.
Rome has extensive amount of ancient catacombs, or underground burial places under or near the city, of which there are at least forty, some discovered only in recent decades. Though most famous for Christian burials, they include pagan and Jewish burials, either in separate catacombs or mixed together. The first large-scale catacombs were excavated from the 2nd century onwards. Originally they were carved through tuff, a soft volcanic rock, outside the boundaries of the city, because Roman law forbade burial places within city limits. Currently maintenance of the catacombs is in the hands of the Papacy which has invested in the Salesians of Don Bosco the supervision of the Catacombs of St. Callixtus on the outskirts of Rome.
Economy.
Panoramic view of EUR business district.
Being the capital city of Italy, Rome hosts all the principal institutions of the nation, like the Presidency of the Republic, the government (and its single Ministeri), the Parliament, the main judicial Courts, and the diplomatic representatives of all the countries for the states of Italy and the Vatican City (curiously, Rome also hosts, in the Italian part of its territory, the Embassy of Italy for the Vatican City, a unique case of an Embassy within the boundaries of its own country). Many international institutions are located in Rome, notably cultural and scientific ones – such as the American Institute, the British School, the French Academy, the Scandinavian Institutes, the German Archaeological Institute – for the honour of scholarship in the Eternal City, and Specialized Agencies of the United Nations, such as the FAO. Rome, also hosts major international and worldwide political and cultural organisations, such as the International Fund for Agricultural Development (IFAD), World Food Programme (WFP), the NATO Defence College and ICCROM, the International Center for the Study of the Preservation and Restoration of Cultural Property. Rome is currently an beta+ world city, falling down from its alpha- status in 2008, along with Berlin, Bucharest, Athens, Lisbon, Montreal and Budapest.
Rome was also ranked in 2014 as 32nd in the Global Cities Index, being the highest-ranking city in Italy. With a 2005 GDP of €94.376 billion (US$121.5 billion), the city produces 6.7% of the national GDP (more than any other single city in Italy), and its unemployment rate, lowered from 11.1% to 6.5% between 2001 and 2005, is now one of the lowest rates of all the European Union capital cities. Rome grows +4.4% annually and continues to grow at a higher rate in comparison to any other city in the rest of the country. This means that were Rome a country, it would be the world's 52nd richest country by GDP, near to the size to that of Egypt. Rome also had a 2003 GDP per capita of €29,153 (US$37,412), which was second in Italy, (after Milan), and is more than 134.1% of the EU average GDP per capita. Rome, on the whole, has the highest total earnings in Italy, reaching €47,076,890,463 in 2008, yet, in terms of average workers' incomes, the city places itself 9th in Italy, with €24,509. On a global level, Rome's workers receive the 30th highest wages in 2009, coming three places higher than in 2008, in which the city ranked 33rd. The Rome area had a GDP amounting to $167.8 billion, and $38,765 per capita.
Although the economy of Rome is characterised by the absence of heavy industry and it is largely dominated by services, high-technology companies (IT, aerospace, defence, telecommunications), research, construction and commercial activities (especially banking), and the huge development of tourism are very dynamic and extremely important to its economy. Rome's international airport, Fiumicino, is the largest in Italy, and the city hosts the head offices of the vast majority of the major Italian companies, as well as the headquarters of three of the world's 100 largest companies: Enel, Eni, and Telecom Italia.
Universities, national radio and television and the movie industry in Rome are also important parts of the economy: Rome is also the hub of the Italian film industry, thanks to the Cinecittà studios, working since the 1930s. The city is also a centre for banking and insurance as well as electronics, energy, transport, and aerospace industries. Numerous international companies and agencies headquarters, government ministries, conference centres, sports venues, and museums are located in Rome's principal business districts: the Esposizione Universale Roma (EUR); the "Torrino" (further south from the EUR); the "Magliana"; the "Parco de' Medici-Laurentina" and the so-called "Tiburtina-valley" along the ancient Via Tiburtina.
Education.
Rome is a nationwide and major international centre for higher education, containing numerous academies, colleges and universities. According to the City Brands Index, Rome is considered the world's second most historically, educationally and culturally interesting and beautiful city. It boasts a large variety of academies and colleges, and has always been a major worldwide intellectual and educational centre, especially during Ancient Rome and the Renaissance, along with Florence.
Rome has a large number of universities and colleges. Its first university, La Sapienza (founded in 1303), is the largest in Europe and the second-largest in the world, with more than 140,000 students attending; in 2005 it ranked as Europe's 33rd best university and currently ranks among Europe's 50 and the world's 150 best colleges. In order to decrease the overcrowding of La Sapienza, two new public universities were founded during the last decades: Tor Vergata in 1982, and Roma Tre in 1992. Rome hosts also the , Italy's most important graduate university in the areas of international affairs and European studies. Rome ISIA was founded in 1973 by Giulio Carlo Argan and is Italy's oldest institution in the field of industrial design.
Rome contains also a large number of pontifical universities and other institutes, including the British School at Rome, the French School in Rome, the Pontifical Gregorian University (The oldest Jesuit university in the world, founded in 1551), Istituto Europeo di Design, the, the Scuola Lorenzo de' Medici, the Link Campus of Malta, and the Università Campus Bio-Medico. Rome is also the location of two American Universities; The American University of Rome and John Cabot University as well as St. John's University branch campus, John Felice Rome Center, a campus of Loyola University Chicago and Temple University Rome, a campus of Temple University. The Roman Colleges are several seminaries for students from foreign countries studying for the priesthood at the Pontifical Universities.
Examples include the Venerable English College, the Pontifical North American College, the Scots College, and the Pontifical Croatian College of St. Jerome.
Rome's major libraries include: the Biblioteca Angelica, opened in 1604, making it Italy's first public library; the Biblioteca Casanatense, opened in 1701; the Biblioteca Vallicelliana; Bibliotheca Hertziana – Max Planck Institute of Art History, a German library located in Rome, often noted for excellence in the arts and sciences; the National Central Library, one of the two national libraries in Italy, which contains 4,126,002 volumes; The Biblioteca del Ministero degli Affari Esteri, specialised in diplomacy, foreign affairs and modern history; the Biblioteca dell'Istituto dell'Enciclopedia Italiana; the Biblioteca Don Bosco, one of the largest and most modern of all Salesian libraries; the Biblioteca e Museo teatrale del Burcardo, a museum-library specialised in history of drama and theatre; the Biblioteca della Società Geografica Italiana, which is based in the Villa Celimontana and is the most important geographical library in Italy, and one of Europe's most important; and the Vatican Library, one of the oldest and most important libraries in the world, which was formally established in 1475, though in fact much older and has 75,000 codices from throughout history.
Culture.
Entertainment and performing arts.
Rome is an important centre for music, and it has an intense musical scene, including several prestigious music conservatories and theatres. It hosts the Accademia Nazionale di Santa Cecilia (founded in 1585), for which new concert halls have been built in the new Parco della Musica, one of the largest musical venues in the world. Rome also has an opera house, the Teatro dell'Opera di Roma, as well as several minor musical institutions. The city also played host to the Eurovision Song Contest in 1991 and the MTV Europe Music Awards in 2004.
Rome has also had a major impact in music history. The Roman School was a group of composers of predominantly church music, which were active in the city during the 16th and 17th centuries, therefore spanning the late Renaissance and early Baroque eras. The term also refers to the music they produced. Many of the composers had a direct connection to the Vatican and the papal chapel, though they worked at several churches; stylistically they are often contrasted with the Venetian School of composers, a concurrent movement which was much more progressive. By far the most famous composer of the Roman School is Giovanni Pierluigi da Palestrina, whose name has been associated for four hundred years with smooth, clear, polyphonic perfection. However, there were other composers working in Rome, and in a variety of styles and forms.
Tourism.
Rome today is one of the most important tourist destinations of the world, due to the incalculable immensity of its archaeological and artistic treasures, as well as for the charm of its unique traditions, the beauty of its panoramic views, and the majesty of its magnificent "villas" (parks). Among the most significant resources are the many museums – Musei Capitolini, the Vatican Museums and the Galleria Borghese and others dedicated to modern and contemporary art – aqueducts, fountains, churches, palaces, historical buildings, the monuments and ruins of the Roman Forum, and the Catacombs. Rome is the third most visited city in the EU, after London and Paris, and receives an average of 7–10 million tourists a year, which sometimes doubles on holy years. The Colosseum (4 million tourists) and the Vatican Museums (4.2 million tourists) are the 39th and 37th (respectively) most visited places in the world, according to a recent study.
Rome is a major archaeological hub, and one of the world's main centres of archaeological research. There are numerous cultural and research institutes located in the city, such as the American Academy in Rome, and The Swedish Institute at Rome. Rome contains numerous ancient sites, including the Forum Romanum, Trajan's Market, Trajan's Forum, the Colosseum, and the Pantheon, to name but a few. The Colosseum, arguably one of Rome's most iconic archaeological sites, is regarded as a wonder of the world.
Rome contains a vast and impressive collection of art, sculpture, fountains, mosaics, frescos, and paintings, from all different periods. Rome first became a major artistic centre during ancient Rome, with forms of important Roman art such as architecture, painting, sculpture and mosaic work. Metal-work, coin die and gem engraving, ivory carvings, figurine glass, pottery, and book illustrations are considered to be 'minor' forms of Roman artwork. Rome later became a major centre of Renaissance art, since the popes spent vast sums of money for the constructions of grandiose basilicas, palaces, piazzas and public buildings in general. Rome became one of Europe's major centres of Renaissance artwork, second only to Florence, and able to compare to other major cities and cultural centres, such as Paris and Venice. The city was affected greatly by the baroque, and Rome became the home of numerous artists and architects, such as Bernini, Caravaggio, Carracci, Borromini and Cortona. In the late 18th century and early 19th century, the city was one of the centres of the Grand Tour, when wealthy, young English and other European aristocrats visited the city to learn about ancient Roman culture, art, philosophy and architecture. Rome hosted a great number of neoclassical and rococo artists, such as Pannini and Bernardo Bellotto. Today, the city is a major artistic centre, with numerous art institutes and museums.
Internal view of the Colosseum
Rome has a growing stock of contemporary and modern art and architecture. The National Gallery of Modern Art has works by Balla, Morandi, Pirandello, Carrà, De Chirico, De Pisis, Guttuso, Fontana, Burri, Mastroianni, Turcato, Kandisky and Cézanne on permanent exhibition. 2010 saw the opening of Rome's newest arts foundation, a contemporary art and architecture gallery designed by acclaimed Iraqi architect Zaha Hadid. Known as MAXXI – National Museum of the 21st Century Arts it restores a dilapidated area with striking modern architecture. Maxxi features a campus dedicated to culture, experimental research laboratories, international exchange and study and research. It is one of Rome's most ambitious modern architecture projects alongside Renzo Piano's Auditorium Parco della Musica and Massimiliano Fuksas' Rome Convention Center, Centro Congressi Italia EUR, in the EUR district, due to open in 2011. The Convention Center features a huge translucent container inside which is suspended a steel and teflon structure resembling a cloud and which contains meeting rooms and an auditorium with two piazzas open to the neighbourhood on either side.
Rome is also widely recognised as a world fashion capital. Although not as important as Milan, Rome is the world's fourth most important center for fashion in the world, according to the 2009 Global Language Monitor after Milan, New York and Paris, and beating London. Major luxury fashion houses and jewellery chains, such as Bulgari, Fendi, Laura Biagiotti and Brioni (fashion), are headquartered or were founded in the city. Also, other major labels, such as Chanel, Prada, Dolce & Gabbana, Armani and Versace have luxury boutiques in Rome, primarily along its prestigious and upscale Via dei Condotti.
Cuisine.
Rome's cuisine has evolved through centuries and periods of social, cultural, and political changes. Rome became a major gastronomical centre during the ancient Age. Ancient Roman cuisine was highly influenced by Ancient Greek culture, and after, the empire's enormous expansion exposed Romans to many new, provincial culinary habits and cooking techniques. Later, during the Renaissance, Rome became well known as a centre of high-cuisine, since some of the best chefs of the time, worked for the popes. An example of this could be Bartolomeo Scappi, who was a chef, working for Pius IV in the Vatican kitchen, and he acquired fame in 1570 when his cookbook "Opera dell'arte del cucinare" was published. In the book he lists approximately 1000 recipes of the Renaissance cuisine and describes cooking techniques and tools, giving the first known picture of a fork. <br> In the modern age, the city developed its own peculiar cuisine, based on products of the nearby Campagna, as lamb and vegetables (globe artichokes are common). In parallel, Roman Jews -present in the city since the 1st century BC- developed their own cuisine, the "cucina giudaico-romanesca". Examples of Roman dishes include ""Saltimbocca alla Romana" – a veal cutlet, Roman-style; topped with raw ham and sage and simmered with white wine and butter; "Carciofi alla romana" – artichokes Roman-style; outer leaves removed, stuffed with mint, garlic, breadcrumbs and braised; "Carciofi alla giudia" – artichokes fried in olive oil, typical of Roman Jewish cooking; outer leaves removed, stuffed with mint, garlic, breadcrumbs and braised; "Spaghetti alla carbonara" – spaghetti with bacon, eggs and pecorino, and "Gnocchi di semolino alla romana"" – semolina dumpling, Roman-style, to name but a few.
Cinema.
Rome hosts the Cinecittà Studios, the largest film and television production facility in continental Europe and the centre of the Italian cinema, where a large number of today's biggest box office hits are filmed. The 99 acre studio complex is 5.6 mi from the centre of Rome and is part of one of the biggest production communities in the world, second only to Hollywood, with well over 5,000 professionals – from period costume makers to visual effects specialists. More than 3,000 productions have been made on its lot, from recent features like The Passion of the Christ, Gangs of New York, HBO's Rome, The Life Aquatic and Dino De Laurentiis' Decameron, to such cinema classics as Ben-Hur, Cleopatra, and the films of Federico Fellini.
Founded in 1937 by Benito Mussolini, the studios were bombed by the Western Allies during the Second World War. In the 1950s, Cinecittà was the filming location for several large American film productions, and subsequently became the studio most closely associated with Federico Fellini. Today Cinecittà is the only studio in the world with pre-production, production, and full post-production facilities on one lot, allowing directors and producers to walk in with their script and "walk out" with a completed film.
Language.
Although associated today only with Latin, ancient Rome was in fact multilingual. In highest antiquity Sabine tribes shared the area of what is today Rome with Latin tribes. The Sabine language was one of the Italic group of ancient Italian languages, along with Etruscan, which would have been the main language of the last three kings who ruled the city till the founding of the Republic in 509 BC. Urganilla, or Plautia Urgulanilla, wife of Emperor Claudius, is thought to have been a speaker of Etruscan many centuries after this date, according to Suetonius' entry on Claudius. However Latin, in various evolving forms, was the main language of classical Rome, but as the city had immigrants, slaves, residents, ambassadors from many parts of the world it was also multilingual. Many educated Romans also spoke Greek, and there was a large Greek, Syriac and Jewish population in parts of Rome from well before the Empire.
Latin evolved during the Middle Ages into a new language, the "volgare". The latter emerged as the confluence of various regional dialects, among which the Tuscan dialect predominated, but the population of Rome also developed its own dialect, the Romanesco. The "Romanesco" spoken during the Middle Ages was a southern Italian dialect, very close to the Neapolitan. The influence of the Florentine culture during the renaissance, and, above all, the immigration to Rome of many Florentines following the two Medici Popes (Leo X and Clement VII), caused a major shift in the dialect, which began to resemble more the Tuscan varieties. This remained largely confined to Rome until the 19th century, but then expanded to other zones of Lazio (Civitavecchia, Latina), from the beginning of the 20th century, thanks to the rising population of Rome and to better transportation systems. As a consequence of education and media like radio and television, Romanesco became more and more similar to standard Italian. Dialectal literature in the traditional form Romanesco includes the works of such authors as Giuseppe Gioachino Belli (one of the most important Italian poets altogether), Trilussa, and Cesare Pascarella. Contemporary Romanesco is mainly represented by popular actors such as Aldo Fabrizi, Alberto Sordi, Nino Manfredi, Anna Magnani, Gigi Proietti, Enrico Montesano, and Carlo Verdone.
Rome's historic contribution to language in a worldwide sense is much more extensive however. Through the process of Romanisation, the peoples of Gallia, the Iberian Peninsula, Italy and Dacia developed languages which derive directly from Latin and were adopted in large areas of the world both through colonization and cultural influence. Moreover, also modern English, because of the Norman Conquest, borrowed a large percentage of its vocabulary from the Latin language. The Roman or Latin alphabet is the most widely used writing system in the world used by the greatest number of languages.
Rome has long hosted artistic communities, foreign resident communities and a large number of foreign religious students or pilgrims and so has always been a multilingual city. Today because of mass tourism many languages are used in servicing tourism, especially English which is widely known in tourist areas, and the city hosts large numbers of immigrants and so has many multilingual immigrant areas.
Sports.
 The Olympic Stadium is mostly used as a shared home stadium for Serie A football clubs S.S. Lazio and A.S. Roma, who contest the Derby della Capitale.
Association football is the most popular sport in Rome, as in the rest of the country.
The city hosted the final games of the 1934 and 1990 FIFA World Cup.
The latter took place in the Olympic Stadium, which is also the home stadium for local Serie A clubs S.S. Lazio, founded in 1900, and A.S. Roma was founded in 1927, whose rivalry has become a staple of Roman sports culture.
Footballers who play for these teams and are also born in the city tend to become especially popular, as has been the case with players such as Francesco Totti and Daniele De Rossi (both for A.S. Roma).
Atletico Roma is a minor team that played in First Division until 2012; its home stadium was Stadio Flaminio.
Rome hosted the 1960 Summer Olympics, with great success, using many ancient sites such as the Villa Borghese and the Thermae of Caracalla as venues. For the Olympic Games many new structures were created, notably the new large Olympic Stadium (which was also enlarged and renewed to host qualification and the final match of the 1990 FIFA World Cup), the Villaggio Olimpico (Olympic Village, created to host the athletes and redeveloped after the games as a residential district), ecc. Rome made a bid to host the 2020 Summer Olympics but it was withdrawn before the deadline for applicant files.
Rugby union is gaining wider acceptance.
Until 2011 the Stadio Flaminio was the home stadium for the Italy national rugby union team, which has been playing in the Six Nations Championship since 2000. The team now plays home games at the Stadio Olimpico because the Stadio Flaminio needs works of renovation in order to improve both its capacity and safety. 
Rome is home to local rugby union teams such as Rugby Roma (founded in 1930 and winner of five Italian championships, the latter in 1999–2000), Unione Rugby Capitolina and S.S. Lazio 1927 (rugby union branch of the multisport club S.S. Lazio).
Every May, Rome hosts the ATP Masters Series tennis tournament on the clay courts of the Foro Italico. Cycling was popular in the post-World War II period, although its popularity has faded. Rome has hosted the final portion of the Giro d'Italia twice, in 1989 and 2000. Rome is also home to other sports teams, including basketball (Virtus Roma), volleyball (M. Roma Volley), handball or waterpolo.
Transport.
Rome is at the centre of the radial network of roads that roughly follow the lines of the ancient Roman roads which began at the Capitoline Hill and connected Rome with its empire. Today Rome is circled, at a distance of about 10 km from the Capitol, by the ring-road (the "Grande Raccordo Anulare" or "GRA").
Due to its location in the centre of the Italian peninsula, Rome is the principal railway node for central Italy. Rome's main railway station, Termini, is one of the largest railway stations in Europe and the most heavily used in Italy, with around 400 thousand travellers passing through every day. The second-largest station in the city, Roma Tiburtina, has been redeveloped as a high-speed rail terminus.
Rome is served by three airports. The intercontinental Leonardo da Vinci International Airport is Italy's chief airport, is located within the nearby Fiumicino, south-west of Rome. The older Rome Ciampino Airport is a joint civilian and military airport. It is commonly referred to as "Ciampino Airport", as it is located beside Ciampino, south-east of Rome. A third airport, the Roma-Urbe Airport, is a small, low-traffic airport located about 6 km north of the city centre, which handles most helicopter and private flights.
Although the city has its own quarter on the Mediterranean Sea (Lido di Ostia), this has only a marina and a small channel-harbour for fisher boats. The main harbour which serves Rome is Port of Civitavecchia, located about 62 km northwest of the city.
The city suffers from traffic problems largely due to this radial street pattern, making it difficult for Romans to move easily from the vicinity of one of the radial roads to another without going into the historic centre or using the ring-road. These problems are not helped by the limited size of Rome's metro system when compared to other cities of similar size. In addition, Rome has only 21 taxis for every 10,000 inhabitants, far below other major European cities. Chronic congestion caused by cars during the 1970s and 1980s led to restrictions being placed on vehicle access to the inner city-centre during the hours of daylight. Areas where these restriction apply are known as Limited Traffic Zones ("Zona a Traffico Limitato" (ZTL) in Italian). More recently, heavy night-time traffic in Trastevere, Testaccio and San Lorenzo has led to the creation of night-time ZTLs in those districts.
A 3-line metro system called the "Metropolitana" operates in Rome. Construction on the first branch started in the 1930s. The line had been planned to quickly connect the main railway station with the newly planned E42 area in the southern suburbs, where the 1942 World Fair was supposed to be held. The event never took place because of war, but the area was later partly redesigned and renamed EUR (Esposizione Universale di Roma: Rome Universal Exhibition) in the 1950s to serve as a modern business district. The line was finally opened in 1955, and it is now the south part of the B Line.
The A line opened in 1980 from Ottaviano to Anagnina stations, later extended in stages (1999–2000) to Battistini. In the 1990s, an extension of the B line was opened from Termini to Rebibbia. This underground network is generally reliable (although it may become very congested at peak times and during events, especially the A line) as it is relatively short.
The A and B lines intersect at Roma Termini station. A new branch of the B line (B1) opened on 13 June 2012 after an estimated building cost of €500 million. B1 connects to line B at Piazza Bologna and has four stations over a distance of 3.9 km.
A third line, the C line, is under construction with an estimated cost of €3 billion and will have 30 stations over a distance of 25.5 km. It will partly replace the existing Termini-Pantano rail line. It will feature full automated, driverless trains. The first section with 15 stations connecting Pantano with the quarter of Centocelle in the eastern part of the city, opened on 9 November 2014. The end of the work was scheduled in 2015, but archaeological findings often delay underground construction work.
A fourth line, D line, is also planned. It will have 22 stations over a distance of 20 km. The first section was projected to open in 2015 and the final sections before 2035, but due to the city's financial crisis the project has been put on hold.
Above-ground public transport in Rome is made up of a bus, tram and urban train network (FR lines). The bus, tram, metro and urban railways network is run by "Atac S.p.A." (which originally stood for the Municipal Bus and Tramways Company, "Azienda Tramvie e Autobus del Comune" in Italian). The bus network has in excess of 350 bus lines and over eight thousand bus stops, whereas the more-limited tram system has 39 km of track and 192 stops. There is also one trolleybus line, opened in 2005, and additional trolleybus lines are planned.
International entities, organisations and involvement.
Among the global cities, Rome is unique in having a sovereign state located entirely within its city limits, the Vatican City. The Vatican is an enclave of the Italian capital city and a sovereign possession of the Holy See which is the Diocese of Rome and the supreme government of the Roman Catholic Church. Rome hosts foreign embassies to both Italy and the Holy See, although frequently the same ambassador is accredited to both. Several international Roman Colleges and Pontifical Universities are located in Rome.
The Pope is the Bishop of Rome and its official seat is the Archbasilica of St. John Lateran (of which the President of the French Republic is "ex officio" the "first and only honorary canon", a title held by the heads of the French state since King Henry IV of France). Another body, the Sovereign Military Order of Malta (SMOM), took refuge in Rome in 1834, due to the conquest of Malta by Napoleon in 1798. It is sometimes classified as having sovereignty but does not claim any territory in Rome or anywhere else, hence leading to dispute over its actual sovereign status.
Rome is the seat of the so-called Polo Romano made up by three main international agencies of the United Nations : the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD).
Rome has traditionally been involved in the process of European political integration. The Treaties of the EU are located in Palazzo della Farnesina, seat of the Ministry of Foreign Affairs, due the fact that the Italian government is the depositary of the treaties. In 1957 the city hosted the signing of the Treaty of Rome, which established the European Economic Community (predecessor to the European Union), and also played host to the official signing of the proposed European Constitution in July 2004.
Rome is the seat of the European Olympic Committee and of the NATO Defense College. The city is the place where the Statute of the International Criminal Court and the European Convention on Human Rights were formulated.
The city hosts also other important international entities such as the IDLO (International Development Law Organisation), the ICCROM (International Centre for the Study of the Preservation and Restoration of Cultural Property) and the UNIDROIT (International Institute for the Unification of Private Law).
Twin towns, sister cities and partner cities.
Rome is since April 9, 1956 exclusively and reciprocally twinned only with:
Rome's sister and partner cities are:
Bibliography.
</dl>

</doc>
<doc id="25459" url="http://en.wikipedia.org/wiki?curid=25459" title="Rocky Mountains">
Rocky Mountains

The Rocky Mountains, commonly known as the Rockies, are a major mountain range in western North America. The Rocky Mountains stretch more than 3000 mi from the northernmost part of British Columbia, in western Canada, to New Mexico, in the southwestern United States. Within the North American Cordillera, the Rockies are somewhat distinct from the Pacific Coast Ranges and the Cascade Range and Sierra Nevada which all lie further to the west.
The Rocky Mountains were initially formed from 80 million to 55 million years ago during the Laramide orogeny, in which a number of plates began to slide underneath the North American plate. The angle of subduction was shallow, resulting in a broad belt of mountains running down western North America. Since then, further tectonic activity and erosion by glaciers have sculpted the Rockies into dramatic peaks and valleys. At the end of the last ice age, humans started to inhabit the mountain range. After Europeans, such as Sir Alexander Mackenzie and Americans, such as the Lewis and Clark expedition, started to explore the range, minerals and furs drove the initial economic exploitation of the mountains, although the range itself never became densely populated.
Currently, much of the mountain range is protected by public parks and forest lands, and is a popular tourist destination, especially for hiking, camping, mountaineering, fishing, hunting, mountain biking, skiing, and snowboarding.
Geography.
The Rocky Mountains are commonly defined as stretching from the Liard River in British Columbia south to the Rio Grande in New Mexico. Other mountain ranges continue beyond those two rivers, including the Selwyn Mountains in Yukon, the Brooks Range in Alaska, and the Sierra Madre in Mexico, but those are not part of the Rockies, though they are part of the American Cordillera. The United States definition of the Rockies includes the Cabinet and Salish Mountains of Idaho and Montana. Their counterparts north of the Kootenai River, the Columbia Mountains, are considered a separate system in Canada, lying to the west of the huge Rocky Mountain Trench. This runs the length of British Columbia from its beginnings in the middle Flathead River valley in western Montana to the south bank of the Liard River. The Rockies vary in width from 70 to 300 miles (110 to 480 kilometers). Also west of the Rocky Mountain Trench, farther north and facing the Muskwa Range across the trench, are the Stikine Ranges and Omineca Mountains of the Interior Mountains system of British Columbia. A small area east of Prince George, British Columbia on the eastern side of the Trench, the McGregor Plateau, resembles the Rockies but is considered part of the Interior Plateau.
The Front Range of the Rocky Mountains near Denver, Colorado
The eastern edge of the Rockies rises dramatically above the Interior Plains of central North America, including the Front Range of Colorado, the Wind River Range and Big Horn Mountains of Wyoming, the Absaroka-Beartooth ranges and Rocky Mountain Front of Montana and the Clark Range of Alberta. In Canada geographers define three main groups of ranges: the Continental Ranges, Hart Ranges and Muskwa Ranges (the latter two flank the Peace River, the only river to pierce the Rockies, and are collectively referred to as the Northern Rockies). The Muskwa and Hart Ranges together comprise what is known as the Northern Rockies (the Mackenzie Mountains north of the Liard River are sometimes referred to as being part of the Rocky Mountains but this is an unofficial designation).
The western edge of the Rockies includes ranges such as the Wasatch near Salt Lake City and the Bitterroots along the Idaho-Montana border. The Great Basin and Columbia River Plateau separate these sub-ranges from distinct ranges further to the west, most prominent among which are the Sierra Nevada, Cascade Range and Coast Mountains. The Rockies do not extend into the Yukon or Alaska, or into central British Columbia, where the Rocky Mountain System (but not the Rocky Mountains) includes the Columbia Mountains, the southward extension of which is considered part of the Rockies in the United States. The Rocky Mountain System within the United States is a United States physiographic region; the Rocky Mountain System is known in Canada as the Eastern System.
The Rocky Mountains are notable for containing the highest peaks in central North America. The range's highest peak is Mount Elbert located in Colorado at 14440 ft above sea level. Mount Robson in British Columbia, at 12972 ft, is the highest peak in the Canadian Rockies. 
The Continental Divide of the Americas is located in the Rocky Mountains and designates the line at which waters flow either to the Atlantic or Pacific Oceans. Triple Divide Peak (8020 ft) in Glacier National Park is so named because water that falls on the mountain reaches not only the Atlantic and Pacific, but Hudson Bay as well. Farther north in Alberta, the Athabasca and other rivers feed the basin of the Mackenzie River, which has its outlet on the Beaufort Sea of the Arctic Ocean. See Rivers of the Rocky Mountains for a list of rivers.
Human population is not very dense in the Rocky Mountains, with an average of four people per square kilometer and few cities with over 50,000 people. However, the human population grew rapidly in the Rocky Mountain states between 1950 and 1990. The 40-year statewide increases in population range from 35% in Montana to about 150% in Utah and Colorado. The populations of several mountain towns and communities have doubled in the last 40 years. Jackson Hole, Wyoming, increased 260%, from 1,244 to 4,472 residents, in 40 years.
Geology.
The rocks in the Rocky Mountains were formed before the mountains were raised by tectonic forces. The oldest rock is Precambrian metamorphic rock that forms the core of the North American continent. There is also Precambrian sedimentary argillite, dating back to 1.7 billion years ago. During the Paleozoic, western North America lay underneath a shallow sea, which deposited many kilometers of limestone and dolomite.
In the southern Rocky Mountains, near present-day Colorado, these ancestral rocks were disturbed by mountain building approximately 300 Ma, during the Pennsylvanian. This mountain building produced the Ancestral Rocky Mountains. They consisted largely of Precambrian metamorphic rock forced upward through layers of the limestone laid down in the shallow sea. The mountains eroded throughout the late Paleozoic and early Mesozoic, leaving extensive deposits of sedimentary rock.
Terranes started to collide with the western edge of North America in the Mississippian (approximately 350 million years ago), causing the Antler orogeny. For 270 million years, the effects of plate collisions were focused very near the edge of the North American plate boundary, far to the west of the Rocky Mountain region. It was not until 80 Ma that these effects began to reach the Rockies.
The current Rocky Mountains were raised in the Laramide orogeny from between 80 to 55 Ma. For the Canadian Rockies, the mountain building is analogous to a rug being pushed on a hardwood floor: the rug bunches up and forms wrinkles (mountains). In Canada, the terranes and subduction are the foot pushing the rug, the ancestral rocks are the rug, and the Canadian Shield in the middle of the continent is the hardwood floor.
Further south, the growth of the Rocky Mountains in the United States was probably caused by an unusual subduction, where the Farallon plate dove at a shallow angle below the North American plate. This low angle moved the focus of melting and mountain building much farther inland than the normal 200 to. It is postulated that the shallow angle of the subducting plate greatly increased the friction and other interactions with the thick continental mass above it. Tremendous thrusts piled sheets of crust on top of each other, building the extraordinarily broad, high Rocky Mountain range.
The current southern Rockies were forced upwards through the layers of Pennsylvanian and Permian sedimentary remnants of the Ancestral Rocky Mountains. Such sedimentary remnants were often tilted at steep angles along the flanks of the modern range; they are now visible in many places throughout the Rockies, and are prominently shown along the Dakota Hogback, an early Cretaceous sandstone formation that runs along the eastern flank of the modern Rockies.
Immediately after the Laramide orogeny, the Rockies were like Tibet: a high plateau, probably 6000 m above sea level. In the last 60 million years, erosion stripped away the high rocks, revealing the ancestral rocks beneath, and forming the current landscape of the Rockies. 
Periods of glaciation occurred from the Pleistocene Epoch (1.8 million - 70,000 years ago) to the Holocene Epoch (fewer than 11,000 years ago). These ice ages left their mark on the Rockies, forming extensive glacial landforms, such as U-shaped valleys and cirques. Recent glacial episodes included the Bull Lake Glaciation that began about 150,000 years ago and the Pinedale Glaciation that probably remained at full glaciation until 15,000-20,000 years ago.
All of the geological processes, above, have left a complex set of rocks exposed at the surface. For example, volcanic rock from the Paleogene and Neogene periods (66 million - 2.6 million years ago) occurs in the San Juan Mountains and in other areas. Millennia of severe erosion in the Wyoming Basin transformed intermountain basins into a relatively flat terrain. The Tetons and other north-central ranges contain folded and faulted rocks of Paleozoic and Mesozoic age draped above cores of Proterozoic and Archean igneous and metamorphic rocks ranging in age from 1.2 billion (e.g., Tetons) to more than 3.3 billion years (Beartooth Mountains).
Ecology and climate.
There are a wide range of environmental factors in the Rocky Mountains. The Rockies range in latitude between the Liard River in British Columbia (at 59° N) and the Rio Grande in New Mexico (at 35° N). Prairie occurs at or below 1800 ft, while the highest peak in the range is Mount Elbert at 14440 ft. Precipitation ranges from 10 in per year in the southern valleys to 60 in per year locally in the northern peaks. Average January temperatures can range from 20 F in Prince George, British Columbia, to 43 F in Trinidad, Colorado. Therefore, there is not a single monolithic ecosystem for the entire Rocky Mountain Range.
Instead, ecologists divide the Rocky Mountain into a number of biotic zones. Each zone is defined by whether it can support trees, and the presence of one or more indicator species. Two zones that do not support trees are the Plains and the Alpine tundra. The Great Plains lie to the east of the Rockies, and is characterized by prairie grasses (below roughly 1800 ft). Alpine tundra occurs in regions above the treeline for the Rocky Mountains, which varies from 12000 ft in New Mexico to 2500 ft at the northern end of the Rocky Mountains (near the Yukon).
The USGS defines ten forested zones in the Rocky Mountains. Zones in more southern, warmer, or drier areas are defined by the presence of pinyon pines/junipers, ponderosa pines, or oaks mixed with pines. In more northern, colder, or wetter areas, zones are defined by Douglas-firs, Cascadian species (such as western hemlock), lodgepole pines/quaking aspens, or firs mixed with spruce. Near treeline, zones can consist of white pines (such as whitebark pine or bristlecone pine); or a mixture of white pine, fir, and spruce that appear as shrub-like krummholz. Finally, rivers and canyons can create a unique forest zone in more arid parts of the mountain range.
The Rocky Mountains are an important habitat for a great deal of well-known wildlife, such as elk, moose, mule and white-tailed deer, pronghorns, mountain goats, bighorn sheep, badgers, black bears, grizzly bears, coyotes, lynxes, and wolverines. For example, North America's largest herds of moose is in the Alberta-British Columbia foothills forests.
The status of most species in the Rocky Mountains is unknown, due to incomplete information. European-American settlement of the mountains has adversely impacted native species. Examples of some species that have declined include western toads, greenback cutthroat trouts, white sturgeons, white-tailed ptarmigans, trumpeter swans, and bighorn sheep. In the United States portion of the mountain range, apex predators such as grizzly bears and gray wolves had been extirpated from their original ranges, but have partially recovered due to conservation measures and reintroduction. Other recovering species include the bald eagle and the peregrine falcon.
History.
Indigenous people.
Since the last great ice age, the Rocky Mountains were home first to indigenous peoples including the Apache, Arapaho, Bannock, Blackfoot, Cheyenne, Crow Nation, Flathead, Shoshone, Sioux, Ute, Kutenai (Ktunaxa in Canada), Sekani, Dunne-za, and others. Paleo-Indians hunted the now-extinct mammoth and ancient bison (an animal 20% larger than modern bison) in the foothills and valleys of the mountains. Like the modern tribes that followed them, Paleo-Indians probably migrated to the plains in fall and winter for bison and to the mountains in spring and summer for fish, deer, elk, roots, and berries. In Colorado, along the crest of the Continental Divide, rock walls that Native Americans built for driving game date back 5,400–5,800 years. A growing body of scientific evidence indicates that indigenous people had significant effects on mammal populations by hunting and on vegetation patterns through deliberate burning.
European exploration.
Recent human history of the Rocky Mountains is one of more rapid change. The Spanish explorer Francisco Vázquez de Coronado—with a group of soldiers, missionaries, and African slaves—marched into the Rocky Mountain region from the south in 1540. The introduction of the horse, metal tools, rifles, new diseases, and different cultures profoundly changed the Native American cultures. Native American populations were extirpated from most of their historical ranges by disease, warfare, habitat loss (eradication of the bison), and continued assaults on their culture.
In 1739, French fur traders Pierre and Paul Mallet, while journeying through the Great Plains, discovered a range of mountains at the headwaters of the Platte River, which local American Indian tribes called the "Rockies", becoming the first Europeans to report on this uncharted mountain range.
Sir Alexander MacKenzie (1764–March 11, 1820) became the first European to cross the Rocky Mountains in 1793. He found the upper reaches of the Fraser River and reached the Pacific coast of what is now Canada on July 20 of that year, completing the first recorded transcontinental crossing of North America north of Mexico. He arrived at Bella Coola, British Columbia, where he first reached saltwater at South Bentinck Arm, an inlet of the Pacific Ocean.
The Lewis and Clark Expedition (1804–1806) was the first scientific reconnaissance of the Rocky Mountains. Specimens were collected for contemporary botanists, zoologists, and geologists. The expedition was said to have paved the way to (and through) the Rocky Mountains for European-Americans from the East, although Lewis and Clark met at least 11 European-American mountain men during their travels.
Mountain men, primarily French, Spanish, and British, roamed the Rocky Mountains from 1720 to 1800 seeking mineral deposits and furs. The fur-trading North West Company established Rocky Mountain House as a trading post in what is now the Rocky Mountain Foothills of present-day Alberta in 1799, and their business rivals the Hudson's Bay Company established Acton House nearby. These posts served as bases for most European activity in the Canadian Rockies in the early 19th century. Among the most notable are the expeditions of David Thompson (explorer), who followed the Columbia River to the Pacific Ocean. On his 1811 expedition, he camped at the junction of the Columbia River and the Snake River and erected a pole and notice claiming the area for the United Kingdom and stating the intention of the North West Company to build a fort at the site.
By the Anglo-American Convention of 1818, which established the 49th parallel north as the international boundary west from Lake of the Woods to the "Stony Mountains"; the UK and the USA agreed to what has since been described as "joint occupancy" of lands further west to the Pacific Ocean. Resolution of the territorial and treaty issues, the Oregon dispute, was deferred until a later time.
In 1819, Spain ceded their rights north of the 42nd Parallel to the United States, though these rights did not include possession and also included obligations to Britain and Russia concerning their claims in the same region.
Settlement.
After 1802, American fur traders and explorers ushered in the first widespread Caucasian presence in the Rockies south of the 49th parallel. The more famous of these include Americans William Henry Ashley, Jim Bridger, Kit Carson, John Colter, Thomas Fitzpatrick, Andrew Henry, and Jedediah Smith. On July 24, 1832, Benjamin Bonneville led the first wagon train across the Rocky Mountains by using South Pass in the present State of Wyoming. Similarly, in the wake of Mackenzie's 1793 expedition, fur trading posts were established west of the Northern Rockies in a region of the northern Interior Plateau of British Columbia which came to be known as New Caledonia, beginning with Fort McLeod (today's community of McLeod Lake) and Fort Fraser, but ultimately focused on Stuart Lake Post (today's Fort St. James).
Negotiations between the United Kingdom and the United States over the next few decades failed to settle upon a compromise boundary and the Oregon Dispute became important in geopolitical diplomacy between the British Empire and the new American Republic. In 1841 James Sinclair, Chief Factor of the Hudson's Bay Company, guided some 200 settlers from the Red River Colony west to bolster settlement around Fort Vancouver in an attempt to retain the Columbia District for Britain. The party crossed the Rockies into the Columbia Valley, a region of the Rocky Mountain Trench near present-day Radium Hot Springs, British Columbia, then traveled south. Despite such efforts, in 1846, Britain ceded all claim to Columbia District lands south of the 49th parallel to the United States; as resolution to the Oregon boundary dispute by the Oregon Treaty.
Thousands passed through the Rocky Mountains on the Oregon Trail beginning in the 1840s. The Mormons began to settle near the Great Salt Lake in 1847. From 1859 to 1864, gold was discovered in Colorado, Idaho, Montana, and British Columbia, sparking several gold rushes bringing thousands of prospectors and miners to explore every mountain and canyon and to create the Rocky Mountains' first major industry. The Idaho gold rush alone produced more gold than the California and Alaska gold rushes combined and was important in the financing of the Union Army during the American Civil War. The transcontinental railroad was completed in 1869, and Yellowstone National Park was established as the world's first national park in 1872. Meanwhile, a transcontinental railroad in Canada was originally promised in 1871. Though political complications pushed its completion to 1885, the Canadian Pacific Railway eventually followed the Kicking Horse and Rogers Passes to the Pacific Ocean. Canadian railway officials also convinced Parliament to set aside vast areas of the Canadian Rockies as Jasper, Banff, Yoho, and Waterton Lakes National Parks, laying the foundation for a tourism industry which thrives to this day. Glacier National Park (MT) was established with a similar relationship to tourism promotions by the Great Northern Railway. While settlers filled the valleys and mining towns, conservation and preservation ethics began to take hold. U.S. President Harrison established several forest reserves in the Rocky Mountains in 1891–92. In 1905, U.S. President Theodore Roosevelt extended the Medicine Bow Forest Reserve to include the area now managed as Rocky Mountain National Park. Economic development began to center on mining, forestry, agriculture, and recreation, as well as on the service industries that support them. Tents and camps became ranches and farms, forts and train stations became towns, and some towns became cities.
Economy.
Industry and development.
Economic resources of the Rocky Mountains are varied and abundant. Minerals found in the Rocky Mountains include significant deposits of copper, gold, lead, molybdenum, silver, tungsten, and zinc. The Wyoming Basin and several smaller areas contain significant reserves of coal, natural gas, oil shale, and petroleum. For example, the Climax mine, located near Leadville, Colorado, was the largest producer of molybdenum in the world. Molybdenum is used in heat-resistant steel in such things as cars and planes. The Climax mine employed over 3,000 workers. The Coeur d'Alene mine of northern Idaho produces silver, lead, and zinc. Canada's largest coal mines are near Fernie, British Columbia and Sparwood, British Columbia; additional coal mines exist near Hinton, Alberta, and in the Northern Rockies surrounding Tumbler Ridge, British Columbia.
Abandoned mines with their wakes of mine tailings and toxic wastes dot the Rocky Mountain landscape. In one major example, eighty years of zinc mining profoundly polluted the river and bank near Eagle River in north-central Colorado. High concentrations of the metal carried by spring runoff harmed algae, moss, and trout populations. An economic analysis of mining effects at this site revealed declining property values, degraded water quality, and the loss of recreational opportunities. The analysis also revealed that cleanup of the river could yield $2.3 million in additional revenue from recreation. In 1983, the former owner of the zinc mine was sued by the Colorado Attorney General for the $4.8 million cleanup costs; five years later, ecological recovery was considerable.
The Rocky Mountains contain several sedimentary basins that are rich in coalbed methane. Coalbed methane is natural gas that arises from coal, either through bacterial action, or through exposure to high temperature. Coalbed methane supplies 7 percent of the natural gas used in the United States. The largest coalbed methane sources in the Rocky Mountains are in the San Juan Basin in New Mexico and Colorado and the Powder River Basin in Wyoming. These two basins are estimated to contain 38 trillion cubic feet of gas. Coalbed methane can be recovered by dewatering the coal bed, and separating the gas from the water; or injecting water to fracture the coal to release the gas (so-called hydraulic fracturing).
Agriculture and forestry are major industries. Agriculture includes dryland and irrigated farming and livestock grazing. Livestock are frequently moved between high-elevation summer pastures and low-elevation winter pastures, a practice known as transhumance.
Tourism.
"See also:" List of U.S. Rocky Mountain ski resorts, List of Alberta ski resorts, List of B.C. ski resorts
Every year the scenic areas and recreational opportunities of the Rocky Mountains draw millions of tourists. The main language of the Rocky Mountains is English. But there are also linguistic pockets of Spanish and indigenous languages. French is another official language in Canada's national parks.
People from all over the world visit the sites to hike, camp, or engage in mountain sports. In the summer season, examples of tourist attractions are:
In the United States:
In Canada, the mountain range contains these national parks:
Glacier National Park in Montana and Waterton Lakes National Park in Alberta border each other and collectively are known as Waterton-Glacier International Peace Park. (See also International Peace Park.)
In the winter, skiing is the main attraction. A list of the major ski resorts can be found at List of U.S. Rocky Mountain ski resorts.
The adjacent Columbia Mountains in British Columbia contain major resorts such as Panorama and Kicking Horse, as well as Mount Revelstoke National Park and Glacier National Park.
There are numerous provincial parks in the British Columbia Rockies, the largest and most notable being Mount Assiniboine Provincial Park, Mount Robson Provincial Park, Northern Rocky Mountains Provincial Park, Kwadacha Wilderness Provincial Park, Stone Mountain Provincial Park and Muncho Lake Provincial Park.

</doc>
<doc id="25470" url="http://en.wikipedia.org/wiki?curid=25470" title="Feminazi">
Feminazi

Feminazi is a term popularized by radio talk-show host Rush Limbaugh and in use since the early 1990s. It is a portmanteau of the nouns "feminist" and "Nazi". The online version of the Merriam-Webster dictionary defines the term as used in a "usually disparaging" manner, to describe "an extreme or militant feminist".
The term is used pejoratively in popular culture to describe either feminists who are perceived as extreme or radical, women who are perceived to seek gender superiority over men, or, in some cases, to describe all feminists.
Etymology and usage.
In his 1992 book "The Way Things Ought to Be", Limbaugh credited his friend Tom Hazlett, professor of economics at the University of California at Davis, with coining the term. In the book, Limbaugh also stated that the word refers to unspecified women whose goal is to allow as many abortions as possible, saying at one point that there were fewer than 25 "true feminazis" in the U.S. Limbaugh has used the term to refer to members of the National Center for Women and Policing, the Feminist Majority Foundation, the National Organization for Women, and other organizations at the March for Women's Lives, a large pro-choice demonstration.
In 2004, Limbaugh named feminist activists Gloria Steinem, Susan Sarandon, Christine Lahti, and Camryn Manheim as "famous feminazis." In 2005, Limbaugh said "I haven't used that term on this program in years. But it still gets to 'em, doesn't it? And you know why? Because it's right. Because it's accurate." As of October 2012 Limbaugh was still using the word regularly on his show.
Other political commentators have also made comparisons between militant feminism and totalitarian ideologies. In 1994, Camille Paglia described some feminist groups as "Stalinist" for engaging in what she describes as censorship and quashing of dissent. In 1983, a year before Limbaugh debuted as a political talk-show host, anarchist Bob Black wrote an essay called "Feminism as Fascism".
In 2012, Limbaugh sarcastically blamed "feminazis" as the cause of a decades-long trend of penis shrinking.
Criticism.
In a 1996 interview, Gloria Steinem criticized Limbaugh's use of the term "feminazi". According to Steinem, "Hitler came to power against the strong feminist movement in Germany, padlocked the family planning clinics, and declared abortion a crime against the state—all views that more closely resemble Rush Limbaugh's." In her book "Outrageous Acts and Everyday Rebellions", Steinem characterised the term as "cruel and ahistorical", and elaborated on the repression of feminism under Hitler, noting that many prominent German feminists like Helene Stöcker, Trude Weiss-Rosmarin and Clara Zetkin were forced to flee Nazi Germany while others were killed in concentration camps.
John K. Wilson, in his book "The Most Dangerous Man in America: Rush Limbaugh's Assault on Reason", cites Limbaugh's definition of the term as meaning "radical feminists whose objective is to see that there are as many abortions as possible" and says "by this definition, there are literally no feminazis."

</doc>
<doc id="25473" url="http://en.wikipedia.org/wiki?curid=25473" title="Richard Nixon">
Richard Nixon

Richard Milhous Nixon (January 9, 1913 – April 22, 1994) was the 37th President of the United States, serving from 1969 to 1974 when he became the only U.S. president to resign the office. Nixon had previously served as a U.S. representative and senator from California and as the 36th Vice President of the United States from 1953 to 1961.
Nixon was born in Yorba Linda, California. After completing his undergraduate work at Whittier College, he graduated from Duke University School of Law in 1937 and returned to California to practice law. He and his wife, Pat Nixon, moved to Washington to work for the federal government in 1942. He subsequently served in the United States Navy during World War II. Nixon was elected to the House of Representatives in 1946 and to the Senate in 1950. His pursuit of the Hiss Case established his reputation as a leading anti-communist, and elevated him to national prominence. He was the running mate of Dwight D. Eisenhower, the Republican Party presidential nominee in the 1952 election. Nixon served for eight years as vice president. He waged an unsuccessful presidential campaign in 1960, narrowly losing to John F. Kennedy, and lost a race for Governor of California in 1962. In 1968 he ran again for the presidency and was elected.
Although Nixon initially escalated the war in Vietnam, he subsequently ended the U.S. involvement in 1973, along with the military draft. Nixon's visit to the People's Republic of China in 1972 opened diplomatic relations between the two nations, and he initiated détente and the Anti-Ballistic Missile Treaty with the Soviet Union the same year. His administration generally transferred power from Washington to the states. He imposed wage and price controls, enforced desegregation of Southern schools and established the Environmental Protection Agency. Nixon also presided over the Apollo 11 moon landing which signaled the end of the moon race. He was reelected by one of the largest landslides in U.S. history in 1972.
The year 1973 saw an Arab oil embargo and a continuing series of revelations about the Watergate scandal. The scandal escalated, costing Nixon much of his political support, and on August 9, 1974, he resigned in the face of almost certain impeachment and removal from office. After his resignation, he was issued a pardon by his successor, Gerald Ford. In retirement, Nixon's work authoring several books and undertaking of many foreign trips helped to rehabilitate his image. He suffered a debilitating stroke on April 18, 1994, and died four days later at the age of 81. Nixon remains a source of considerable interest among historians.
Early life.
Nixon was born on January 9, 1913 in Yorba Linda, California, in a house his father built. He was the son of Hannah (Milhous) Nixon and Francis A. Nixon. His mother was a Quaker and his father converted from Methodism to the Quaker faith; Nixon's upbringing was marked by conservative Quaker observances of the time, such as refraining from alcohol, dancing, and swearing. Nixon had four brothers: Harold (1909–33), Donald (1914–87), Arthur (1918–25), and Edward (born 1930). Four of the five Nixon boys were named after kings who had ruled in historical or legendary England; Richard, for example, was named after Richard the Lionheart.
Nixon's early life was marked by hardship, and he later quoted a saying of Eisenhower to describe his boyhood: "We were poor, but the glory of it was we didn't know it". The Nixon family ranch failed in 1922, and the family moved to Whittier, California. In an area with many Quakers, Frank Nixon opened a grocery store and gas station. Richard's younger brother Arthur died in 1925 after a short illness. At the age of twelve, Richard was found to have a spot on his lung and, with a family history of tuberculosis, he was forbidden to play sports. Eventually, the spot was found to be scar tissue from an early bout of pneumonia.
Primary and secondary education.
Young Richard attended East Whittier Elementary School, where he was president of his eighth-grade class. His parents believed that attendance at Whittier High School had caused Richard's older brother Harold to live a dissolute lifestyle before the older boy fell ill of tuberculosis (he died of the disease in 1933). Instead, they sent Richard to the larger Fullerton Union High School. He received excellent grades, even though he had to ride a school bus for an hour each way during his freshman year—later, he lived with an aunt in Fullerton during the week. He played junior varsity football, and seldom missed a practice, even though he was rarely used in games. He had greater success as a debater, winning a number of championships and taking his only formal tutelage in public speaking from Fullerton's Head of English, H. Lynn Sheller. Nixon later remembered Sheller's words, "Remember, speaking is conversation ... don't shout at people. Talk to them. Converse with them." Nixon stated that he tried to use the conversational tone as much as possible.
His parents permitted Richard to transfer to Whittier High School for his junior year, beginning in September 1928. At Whittier High, Nixon suffered his first electoral defeat, for student body president. He generally rose at 4 a.m., to drive the family truck into Los Angeles and purchase vegetables at the market. He then drove to the store to wash and display them, before going to school. Harold had been diagnosed with tuberculosis the previous year; when their mother took him to Arizona in the hopes of improving his health, the demands on Richard increased, causing him to give up football. Nevertheless, Richard graduated from Whittier High third in his class of 207 students.
Collegiate and law school education.
Nixon was offered a tuition grant to attend Harvard University, but Harold's continued illness and the need for their mother to care for him meant Richard was needed at the store. He remained in his hometown and attended Whittier College, his expenses there covered by a bequest from his maternal grandfather. Nixon played for the basketball team; he also tried out for football, but lacked the size to play. He remained on the team as a substitute, and was noted for his enthusiasm. Instead of fraternities and sororities, Whittier had literary societies. Nixon was snubbed by the only one for men, the Franklins; many members of the Franklins were from prominent families but Nixon was not. He responded by helping to found a new society, the Orthogonian Society. In addition to the society, schoolwork, and work at the store, Nixon found time for a large number of extracurricular activities, becoming a champion debater and gaining a reputation as a hard worker. In 1933, he became engaged to Ola Florence Welch, daughter of the Whittier police chief. The two broke up in 1935.
After his graduation from Whittier in 1934, Nixon received a full scholarship to attend Duke University School of Law. The school was new and sought to attract top students by offering scholarships. It paid high salaries to its professors, many of whom had national or international reputations. The number of scholarships was greatly reduced for second- and third-year students, forcing recipients into intense competition. Nixon not only kept his scholarship but was elected president of the Duke Bar Association, inducted into the Order of the Coif, and graduated third in his class in June 1937.
Early career, marriage, and war service.
After graduating from Duke, Nixon initially hoped to join the Federal Bureau of Investigation. He received no response to his letter of application and learned years later that he had been hired, but his appointment had been canceled at the last minute due to budget cuts. Instead, he returned to California and was admitted to the bar in 1937. He began practicing with the law firm Wingert and Bewley in Whittier, working on commercial litigation for local petroleum companies and other corporate matters, as well as on wills. In later years, Nixon proudly stated that he was the only modern president to have previously worked as a practicing attorney. Nixon was reluctant to work on divorce cases, disliking frank sexual talk from women. In 1938, he opened up his own branch of Wingert and Bewley in La Habra, California, and became a full partner in the firm the following year.
In January 1938, Nixon was cast in the Whittier Community Players production of "The Dark Tower". There he played opposite a high school teacher named Thelma "Pat" Ryan. Nixon described it in his memoirs as "a case of love at first sight"—for Nixon only, as Pat Ryan turned down the young lawyer several times before agreeing to date him. Once they began their courtship, Ryan was reluctant to marry Nixon; they dated for two years before she assented to his proposal. They wed at a small ceremony on June 21, 1940. After a honeymoon in Mexico, the Nixons began their married life in Whittier. They had two daughters, Tricia (born 1946) and Julie (born 1948).
In January 1942, the couple moved to Washington, D.C., where Nixon took a job at the Office of Price Administration. In his political campaigns, Nixon would suggest that this was his response to Pearl Harbor, but he had sought the position throughout the latter part of 1941. Both Nixon and his wife believed he was limiting his prospects by remaining in Whittier. He was assigned to the tire rationing division, where he was tasked with replying to correspondence. He did not enjoy the role, and four months later, applied to join the United States Navy. As a birthright Quaker, he could have claimed exemption from the draft, and deferments were routinely granted for those in government service. His application was successful, and he was inducted into the Navy in August 1942.
Nixon completed Officers Candidate School and was commissioned as an ensign in October 1942. His first post was as aide to the commander of the Naval Air Station Ottumwa in Iowa. Seeking more excitement, he requested sea duty and was reassigned as the naval passenger control officer for the South Pacific Combat Air Transport Command, supporting the logistics of operations in the South West Pacific theater. He was Officer in Charge of the Combat Air Transport Command at Guadalcanal in the Solomons and later at Green Island (Nissan island) just north of Bougainville. His unit prepared manifests and flight plans for C-47 operations and supervised the loading and unloading of the cargo aircraft. For this service he received a Letter of Commendation for "meritorious and efficient performance of duty as Officer in Charge of the South Pacific Combat Air Transport Command" On October 1, 1943, Nixon was promoted to lieutenant. Nixon earned two service stars and that citation of commendation, although he saw no actual combat. Upon his return to the U.S., Nixon was appointed the administrative officer of the Alameda Naval Air Station in California. In January 1945, he was transferred to the Bureau of Aeronautics office in Philadelphia to help negotiate the termination of war contracts, and he received another letter of commendation for his work there. Later, Nixon was transferred to other offices to work on contracts and finally to Baltimore. In October 1945, he was promoted to lieutenant commander. He resigned his commission on New Year's Day 1946.
Rising politician.
Congressional career.
In 1945, Republicans in California's 12th congressional district, frustrated by their inability to defeat Democratic Congressman Jerry Voorhis, sought a consensus candidate who would run a strong campaign against him. They formed a "Committee of 100" to decide on a candidate, hoping to avoid internal dissensions which had led to Voorhis victories. After the committee failed to attract higher-profile candidates, Herman Perry, Whittier's Bank of America branch manager, suggested Nixon, a family friend with whom he had served on the Whittier College Board of Trustees before the war. Perry wrote to Nixon in Baltimore. After a night of excited talk between the Nixons, the naval officer responded to Perry with enthusiasm. Nixon flew to California and was selected by the committee. When he left the Navy at the start of 1946, Nixon and his wife returned to Whittier, where Nixon began a year of intensive campaigning. He contended that Voorhis had been ineffective as a congressman and suggested that Voorhis's endorsement by a group linked to communists meant that Voorhis must have radical views. Nixon won the election, receiving 65,586 votes to Voorhis' 49,994.
In Congress, Nixon supported the Taft–Hartley Act of 1947, a federal law that monitors the activities and power of labor unions, and served on the Education and Labor Committee. He was part of the Herter Committee, which went to Europe to report on the need for U.S. foreign aid. Nixon was the youngest member of the committee, and the only Westerner. Advocacy by Herter Committee members, including Nixon, led to congressional passage of the Marshall Plan.
Nixon first gained national attention in 1948 when his investigation, as a member of the House Un-American Activities Committee (HUAC), broke the Alger Hiss spy case. While many doubted Whittaker Chambers' allegations that Hiss, a former State Department official, had been a Soviet spy, Nixon believed them to be true and pressed for the committee to continue its investigation. Under suit for defamation filed by Hiss, Chambers produced documents corroborating his allegations. These included paper and microfilm copies that Chambers turned over to House investigators after having hidden them overnight in a field; they became known as the "Pumpkin Papers". Hiss was convicted of perjury in 1950 for denying under oath he had passed documents to Chambers. In 1948, Nixon successfully cross-filed as a candidate in his district, winning both major party primaries, and was comfortably reelected.
In 1949, Nixon began to consider running for the United States Senate against the Democratic incumbent, Sheridan Downey, and entered the race in November of that year. Downey, faced with a bitter primary battle with Representative Helen Gahagan Douglas, announced his retirement in March 1950. Nixon and Douglas won the primary elections and engaged in a contentious campaign in which the ongoing Korean War was a major issue. Nixon tried to focus attention on Douglas' liberal voting record. As part of that effort, a "Pink Sheet" was distributed by the Nixon campaign suggesting that, as Douglas' voting record was similar to that of New York Congressman Vito Marcantonio (believed by some to be a communist), their political views must be nearly identical. Nixon won the election by almost twenty percentage points. During this campaign, Nixon was first called "Tricky Dick" by his opponents for his campaign tactics.
In the Senate, Nixon took a prominent position in opposing global communism, traveling frequently and speaking out against the threat. He maintained friendly relations with his fellow anti-communist, the controversial Wisconsin senator, Joseph McCarthy, but was careful to keep some distance between himself and McCarthy's allegations. Nixon also criticized President Harry S. Truman's handling of the Korean War. He supported statehood for Alaska and Hawaii, voted in favor of civil rights for minorities, and supported federal disaster relief for India and Yugoslavia. He voted against price controls and other monetary restrictions, benefits for illegal immigrants, and public power.
Vice Presidency.
General Dwight D. Eisenhower was nominated for president by the Republicans in 1952. He had no strong preference for a vice presidential candidate, and Republican officeholders and party officials met in a "smoke-filled room" and recommended Nixon to the general, who agreed to the senator's selection. Nixon's youth (he was then 39), stance against communism, and political base in California—one of the largest states—were all seen as vote-winners by the leaders. Among the candidates considered along with Nixon were Ohio Senator Robert A. Taft, New Jersey Governor Alfred Driscoll and Illinois Senator Everett Dirksen. On the campaign trail, Eisenhower spoke to his plans for the country, leaving the negative campaigning to his running mate.
In mid-September, the Republican ticket faced a major crisis. The media reported that Nixon had a political fund, maintained by his backers, which reimbursed him for political expenses. Such a fund was not illegal, but it exposed Nixon to allegations of possible conflict of interest. With pressure building for Eisenhower to demand Nixon's resignation from the ticket, the senator went on television to deliver an address to the nation on September 23, 1952. The address, later termed the Checkers speech, was heard by about 60 million Americans—including the largest television audience up to that point. Nixon emotionally defended himself, stating that the fund was not secret, nor had donors received special favors. He painted himself as a man of modest means (his wife had no mink coat; instead she wore a "respectable Republican cloth coat") and a patriot. The speech would be remembered for the gift which Nixon had received, but which he would not give back: "a little cocker spaniel dog ... sent all the way from Texas. And our little girl—Tricia, the 6-year-old—named it Checkers." The speech was a masterpiece and prompted a huge public outpouring of support for Nixon. Eisenhower decided to retain him on the ticket, which proved victorious in the November election.
Eisenhower gave Nixon responsibilities during his term as vice president—More than any previous vice president. Nixon attended Cabinet and National Security Council meetings and chaired them when Eisenhower was absent. A 1953 tour of the Far East succeeded in increasing local goodwill toward the United States and prompted Nixon to appreciate the potential of the region as an industrial center. He visited Saigon and Hanoi in French Indochina. On his return to the United States at the end of 1953, Nixon increased the amount of time he devoted to foreign relations.
Biographer Irwin Gellman, who chronicled Nixon's congressional years, said of his vice presidency:
Eisenhower radically altered the role of his running mate by presenting him with critical assignments in both foreign and domestic affairs once he assumed his office. The vice president welcomed the president's initiatives and worked energetically to accomplish White House objectives. Because of the collaboration between these two leaders, Nixon deserves the title, "the first modern vice president".
Despite intense campaigning by Nixon, who reprised his strong attacks on the Democrats, the Republicans lost control of both houses of Congress in the 1954 elections. These losses caused Nixon to contemplate leaving politics once he had served out his term. On September 24, 1955, President Eisenhower suffered a heart attack; his condition was initially believed to be life-threatening. Eisenhower was unable to perform his duties for six weeks. The 25th Amendment to the United States Constitution had not yet been proposed, and the Vice President had no formal power to act. Nonetheless, Nixon acted in Eisenhower's stead during this period, presiding over Cabinet meetings and ensuring that aides and Cabinet officers did not seek power. According to Nixon biographer Stephen Ambrose, Nixon had "earned the high praise he received for his conduct during the crisis ... he made no attempt to seize power".
His spirits buoyed, Nixon sought a second term, but some of Eisenhower's aides aimed to displace him. In a December 1955 meeting, Eisenhower proposed that Nixon not run for reelection in order to give him administrative experience before a 1960 presidential run and instead become a Cabinet officer in a second Eisenhower administration. Nixon, however, believed such an action would destroy his political career. When Eisenhower announced his reelection bid in February 1956, he hedged on the choice of his running mate, stating that it was improper to address that question until he had been renominated. Although no Republican was opposing Eisenhower, Nixon received a substantial number of write-in votes against the President in the 1956 New Hampshire primary election. In late April, the President announced that Nixon would again be his running mate. Eisenhower and Nixon were reelected by a comfortable margin in the November 1956 election.
In the spring of 1957, Nixon undertook another major foreign trip, this time to Africa. On his return, he helped shepherd the Civil Rights Act of 1957 through Congress. The bill was weakened in the Senate, and civil rights leaders were divided over whether Eisenhower should sign it. Nixon advised the President to sign the bill, which he did. Eisenhower suffered a mild stroke in November 1957, and Nixon gave a press conference, assuring the nation that the Cabinet was functioning well as a team during Eisenhower's brief illness.
On April 27, 1958, Richard and Pat Nixon embarked on a goodwill tour of South America. In Montevideo, Uruguay, Nixon made an impromptu visit to a college campus, where he fielded questions from students on U.S. foreign policy. The trip was uneventful until the Nixon party reached Lima, Peru, where he was met with student demonstrations. Nixon went to the campus, got out of his car to confront the students, and stayed until forced back into the car by a volley of thrown objects. At his hotel, Nixon faced another mob, and one demonstrator spat on him. In Caracas, Venezuela, Nixon and his wife were spat on by anti-American demonstrators and their limousine was attacked by a pipe-wielding mob. According to Ambrose, Nixon's courageous conduct "caused even some of his bitterest enemies to give him some grudging respect".
In July 1959, President Eisenhower sent Nixon to the Soviet Union for the opening of the American National Exhibition in Moscow. On July 24, while touring the exhibits with Soviet Premier Nikita Khrushchev, the two stopped at a model of an American kitchen and engaged in an impromptu exchange about the merits of capitalism versus communism that became known as the "Kitchen Debate".
1960 and 1962 elections; wilderness years.
In 1960, Nixon launched his first campaign for President of the United States. He faced little opposition in the Republican primaries and chose former Massachusetts Senator Henry Cabot Lodge Jr. as his running mate. His Democratic opponent was John F. Kennedy, and the race remained close for the duration. Nixon campaigned on his experience, but Kennedy called for new blood and claimed the Eisenhower–Nixon administration had allowed the Soviet Union to overtake the U.S. in ballistic missiles (the "missile gap"). A new political medium was introduced in the campaign: televised presidential debates. In the first of four such debates, Nixon appeared pale, with a five o'clock shadow, in contrast to the photogenic Kennedy. Nixon's performance in the debate was perceived to be mediocre in the visual medium of television, though many people listening on the radio thought that Nixon had won. Nixon lost the election narrowly, with Kennedy ahead by only 120,000 votes (0.2 percent) in the popular vote.
There were charges of vote fraud in Texas and Illinois, both states won by Kennedy; Nixon refused to consider contesting the election, feeling a lengthy controversy would diminish the United States in the eyes of the world, and the uncertainty would hurt U.S. interests. At the end of his term of office as vice president in January 1961, Nixon and his family returned to California, where he practiced law and wrote a bestselling book, "Six Crises", which included coverage of the Hiss case, Eisenhower's heart attack, and the Fund Crisis, which had been resolved by the Checkers speech.
Local and national Republican leaders encouraged Nixon to challenge incumbent Pat Brown for Governor of California in the 1962 election. Despite initial reluctance, Nixon entered the race. The campaign was clouded by public suspicion that Nixon viewed the office as a stepping-stone for another presidential run, some opposition from the far-right of the party, and his own lack of interest in being California's governor. Nixon hoped that a successful run would confirm him in his status as the nation's leading active Republican politician, and ensure he remained a major player in national politics. Instead, he lost to Brown by more than five percentage points, and the defeat was widely believed to be the end of his political career. In an impromptu concession speech the morning after the election, Nixon blamed the media for favoring his opponent, saying, "You won't have Nixon to kick around anymore because, gentlemen, this is my last press conference". The California defeat was highlighted in the November 11, 1962, episode of ABC's "" entitled "The Political Obituary of Richard M. Nixon". Alger Hiss appeared on the program, and many members of the public complained that it was unseemly to allow a convicted felon air time to attack a former vice president. The furor drove Smith and his program from the air, and public sympathy for Nixon grew.
The Nixon family traveled to Europe in 1963, where Nixon gave press conferences and met with leaders of the countries he visited. The family moved to New York City, where Nixon became a senior partner in the leading law firm Nixon, Mudge, Rose, Guthrie & Alexander. Nixon had pledged, when announcing his California campaign, not to run for president in 1964; even if he had not, he believed it would be difficult to defeat Kennedy, or after his assassination, Kennedy's successor, Lyndon Johnson. In 1964, he supported Arizona Senator Barry Goldwater for the Republican nomination for president; when Goldwater was successful in gaining the nomination, Nixon was selected to introduce the candidate to the convention. Although he thought Goldwater unlikely to win, Nixon campaigned for him loyally. The election was a disaster for the Republicans; Goldwater's landslide loss to Johnson was matched by heavy losses for the party in Congress and among state governors.
Nixon was one of the few leading Republicans not blamed for the disastrous results, and he sought to build on that in the 1966 congressional elections. He campaigned for many Republicans seeking to regain seats lost in the Johnson landslide and received credit for helping the Republicans make major gains in the midterm election.
1968 presidential election.
At the end of 1967, Nixon told his family he planned to run for president a second time. Although Pat Nixon did not always enjoy public life (for example, she had been embarrassed by the need to reveal how little the family owned in the Checkers speech), she was supportive of her husband's ambitions. Nixon believed that with the Democrats torn over the issue of the Vietnam War, a Republican had a good chance of winning, although he expected the election to be as close as in 1960.
One of the most tumultuous primary election seasons ever began as the Tet Offensive was launched, followed by the withdrawal of President Johnson as a candidate after doing unexpectedly poorly in the New Hampshire primary; it concluded with the assassination of one of the Democratic candidates, Senator Robert F. Kennedy, just moments after his victory in the California primary. On the Republican side, Nixon's main opposition was Michigan Governor George Romney, though New York Governor Nelson Rockefeller and California Governor Ronald Reagan each hoped to be nominated in a brokered convention. Nixon secured the nomination on the first ballot. He selected Maryland Governor Spiro Agnew as his running mate, a choice which Nixon believed would unite the party, appealing to both Northern moderates and Southerners disaffected with the Democrats.
Nixon's Democratic opponent in the general election was Vice President Hubert Humphrey, who was nominated at a convention marked by violent protests. Throughout the campaign, Nixon portrayed himself as a figure of stability during a period of national unrest and upheaval. He appealed to what he later called the "silent majority" of socially conservative Americans who disliked the hippie counterculture and the anti-war demonstrators. Agnew became an increasingly vocal critic of these groups, solidifying Nixon's position with the right.
Nixon waged a prominent television advertising campaign, meeting with supporters in front of cameras. He stressed that the crime rate was too high, and attacked what he perceived as a surrender by the Democrats of the United States' nuclear superiority. Nixon promised "peace with honor" in the Vietnam War and proclaimed that "new leadership will end the war and win the peace in the Pacific". He did not release specifics of how he hoped to end the war, resulting in media intimations that he must have a "". His slogan of "Nixon's the One" proved to be effective.
Johnson's negotiators hoped to reach a truce in Vietnam prior to the election. Nixon received astute analysis on the talks from Henry Kissinger, then a consultant to U.S. negotiator Averell Harriman, and his campaign was in regular contact with Anna Chennault in Saigon. She advised South Vietnamese president Thieu not to go to Paris to join the talks, hinting that Nixon would give him a better deal if elected. Johnson was aware of what was going on, as he had both Chennault and the South Vietnamese ambassador to Washington bugged, and was enraged by what he considered an attempt by Nixon to undermine U.S. foreign policy. On October 31, with no agreement, Johnson announced a unilateral halt to the bombing, and that peace negotiations would start in Paris on November 6, the day after Election Day. On November 2, after speaking with Chennault again, Thieu stated he would not go to Paris. Johnson telephoned Nixon, who denied any involvement; the President did not believe him. Johnson felt he could not publicly mention Chennault's involvement, which had been obtained by wiretapping, but told Humphrey, who chose not to use the information.
In a three-way race between Nixon, Humphrey, and independent candidate former Alabama Governor George Wallace, Nixon defeated Humphrey by nearly 500,000 votes (seven-tenths of a percentage point), with 301 electoral votes to 191 for Humphrey and 46 for Wallace. In his victory speech, Nixon pledged that his administration would try to bring the divided nation together. Nixon said: "I have received a very gracious message from the Vice President, congratulating me for winning the election. I congratulated him for his gallant and courageous fight against great odds. I also told him that I know exactly how he felt. I know how it feels to lose a close one."
Presidency (1969–74).
Nixon was inaugurated as president on January 20, 1969, sworn in by his onetime political rival, Chief Justice Earl Warren. Pat Nixon held the family Bibles open at Isaiah 2:4, which reads, "They shall beat their swords into plowshares, and their spears into pruning hooks." In his inaugural address, which received almost uniformly positive reviews, Nixon remarked that "the greatest honor history can bestow is the title of peacemaker"—a phrase that would later be placed on his gravestone. He spoke about turning partisan politics into a new age of unity:
In these difficult years, America has suffered from a fever of words; from inflated rhetoric that promises more than it can deliver; from angry rhetoric that fans discontents into hatreds; from bombastic rhetoric that postures instead of persuading. We cannot learn from one another until we stop shouting at one another, until we speak quietly enough so that our words can be heard as well as our voices.
Foreign policy.
China.
Nixon laid the groundwork for his overture to China even before he became president, writing in "Foreign Affairs" a year before his election: "There is no place on this small planet for a billion of its potentially most able people to live in angry isolation." Assisting him in this venture was his National Security Advisor and future Secretary of State, Henry Kissinger, with whom the President worked closely, bypassing Cabinet officials. With relations between the Soviet Union and China at a nadir—border clashes between the two took place during Nixon's first year in office—Nixon sent private word to the Chinese that he desired closer relations. A breakthrough came in early 1971, when Chairman Mao invited a team of American table tennis players to visit China and play against top Chinese players. Nixon followed up by sending Kissinger to China for clandestine meetings with Chinese officials. On July 15, 1971, it was simultaneously announced by Beijing and by Nixon (on television and radio) that the President would visit China the following February. The announcements astounded the world. The secrecy allowed both sets of leaders time to prepare the political climate in their countries for the contact.
In February 1972, Nixon and his wife traveled to China. Kissinger briefed Nixon for over 40 hours in preparation. Upon touching down, the President and First Lady emerged from Air Force One and greeted Chinese Premier Zhou Enlai. Nixon made a point of shaking Zhou's hand, something which then-Secretary of State John Foster Dulles had refused to do in 1954 when the two met in Geneva. Over 100 television journalists accompanied the president. On Nixon's orders, television was strongly favored over printed publications, as Nixon felt that the medium would capture the visit much better than print. It also gave him the opportunity to snub the print journalists he despised.
Nixon and Kissinger met for an hour with Mao and Zhou at Mao's official private residence, where they discussed a range of issues. Mao later told his doctor that he had been impressed by Nixon, whom he considered forthright, unlike the leftists and the Soviets. He said he was suspicious of Kissinger, though the National Security Advisor referred to their meeting as his "encounter with history". A formal banquet welcoming the presidential party was given that evening in the Great Hall of the People. The following day, Nixon met with Zhou; the joint communique following this meeting recognized Taiwan as a part of China, and looked forward to a peaceful solution to the problem of reunification. When not in meetings, Nixon toured architectural wonders including the Forbidden City, Ming Tombs, and the Great Wall. Americans received their first glimpse into Chinese life through the cameras which accompanied Pat Nixon, who toured the city of Beijing and visited communes, schools, factories, and hospitals.
The visit ushered in a new era of Sino-American relations. Fearing the possibility of a Sino-American alliance, the Soviet Union yielded to pressure for détente with the United States.
Vietnam War.
When Nixon took office, about 300 American soldiers were dying each week in Vietnam, and the war was broadly unpopular in the United States, with violent protests against the war ongoing. The Johnson administration had agreed to suspend bombing in exchange for negotiations without preconditions, but this agreement never fully took force. According to Walter Isaacson, soon after taking office, Nixon had concluded that the Vietnam War could not be won and he was determined to end the war quickly. Conversely, Black argues that Nixon sincerely believed he could intimidate North Vietnam through the "Madman theory". Nixon sought some arrangement which would permit American forces to withdraw, while leaving South Vietnam secure against attack.
Nixon approved a secret bombing campaign of North Vietnamese and allied Khmer Rouge positions in Cambodia in March 1969 (code-named "Operation Menu"), a policy begun under Johnson. These operations resulted in heavy bombing of Cambodia; by one measurement more bombs were dropped over Cambodia under Johnson and Nixon than the Allies dropped during World War II. In mid-1969, Nixon began efforts to negotiate peace with the North Vietnamese, sending a personal letter to North Vietnamese leaders, and peace talks began in Paris. Initial talks, however, did not result in an agreement. In May 1969 he publicly proposed to withdraw all American troops from South Vietnam provided North Vietnam also did so and for South Vietnam to hold internationally supervised elections with Viet Cong participation.
In July 1969, Nixon visited South Vietnam, where he met with his U.S. military commanders and President Nguyen Van Thieu. Amid protests at home demanding an immediate pullout, he implemented a strategy of replacing American troops with Vietnamese troops, known as "Vietnamization". He soon instituted phased U.S. troop withdrawals but authorized incursions into Laos, in part to interrupt the Ho Chi Minh trail, used to supply North Vietnamese forces, that passed through Laos and Cambodia. Nixon announced the ground invasion of Cambodia to the American public on April 30, 1970. His responses to protesters included an impromptu, early morning meeting with them at the Lincoln Memorial on May 9, 1970. Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese attempt to overrun Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea. Nixon's campaign promise to curb the war, contrasted with the escalated bombing, led to claims that Nixon had a "credibility gap" on the issue.
In 1971, excerpts from the "Pentagon Papers", which had been leaked by Daniel Ellsberg, were published by "The New York Times" and "The Washington Post". When news of the leak first appeared, Nixon was inclined to do nothing; the Papers, a history of United States' involvement in Vietnam, mostly concerned the lies of prior administrations and contained few real revelations. He was persuaded by Kissinger that the papers were more harmful than they appeared, and the President tried to prevent publication. The Supreme Court eventually ruled for the newspapers.
As U.S. troop withdrawals continued, conscription was reduced and in 1973 ended; the armed forces became all-volunteer. After years of fighting, the Paris Peace Accords were signed at the beginning of 1973. The agreement implemented a cease fire and allowed for the withdrawal of remaining American troops; however, it did not require the 160,000 North Vietnam Army regulars located in the South to withdraw. Once American combat support ended, there was a brief truce, before fighting broke out again, this time without American combat involvement. North Vietnam conquered South Vietnam in 1975.
Latin American policy.
Nixon had been a firm supporter of Kennedy in the 1961 Bay of Pigs Invasion and 1962 Cuban Missile Crisis; on taking office he stepped up covert operations against Cuba and its president, Fidel Castro. He maintained close relations with the Cuban-American exile community through his friend, Bebe Rebozo, who often suggested ways of irritating Castro. These activities concerned the Soviets and Cubans, who feared Nixon might attack Cuba and break the understanding between Kennedy and Khrushchev which had ended the missile crisis. In August 1970, the Soviets asked Nixon to reaffirm the understanding; despite his hard line against Castro, Nixon agreed. The process had not yet been completed when the Soviets began expanding their base at the Cuban port of Cienfuegos in October 1970. A minor confrontation ensued, which was concluded with an understanding that the Soviets would not use Cienfuegos for submarines bearing ballistic missiles. The final round of diplomatic notes, reaffirming the 1962 accord, were exchanged in November.
The election of Marxist candidate Salvador Allende as President of Chile in September 1970 spurred Nixon and Kissinger to pursue a vigorous campaign of covert resistance to Allende,:25 first designed to convince the Chilean congress to confirm Jorge Alessandri as the winner of the election and then messages to military officers in support of a coup. Other support included strikes organized against Allende and funding for Allende opponents. It was even alleged that "Nixon personally authorized" $700,000 in covert funds to print anti-Allende messages in a prominent Chilean newspaper.:93 Following an extended period of social, political, and economic unrest, General Augusto Pinochet assumed power in a violent coup d'état on September 11, 1973; among the dead was Allende.
Soviet Union.
Nixon used the improving international environment to address the topic of nuclear peace. Following the announcement of his visit to China, the Nixon administration concluded negotiations for him to visit the Soviet Union. The President and First Lady arrived in Moscow on May 22, 1972 and met with Leonid Brezhnev, the General Secretary of the Communist Party; Alexei Kosygin, the Chairman of the Council of Ministers; and Nikolai Podgorny, the head of state, among other leading Soviet officials.
Nixon engaged in intense negotiations with Brezhnev. Out of the summit came agreements for increased trade and two landmark arms control treaties: SALT I, the first comprehensive limitation pact signed by the two superpowers, and the Anti-Ballistic Missile Treaty, which banned the development of systems designed to intercept incoming missiles. Nixon and Brezhnev proclaimed a new era of "peaceful coexistence". A banquet was held that evening at the Kremlin.
Seeking to foster better relations with the United States, both China and the Soviet Union cut back on their diplomatic support for North Vietnam and advised Hanoi to come to terms militarily. Nixon later described his strategy:
I had long believed that an indispensable element of any successful peace initiative in Vietnam was to enlist, if possible, the help of the Soviets and the Chinese. Though rapprochement with China and détente with the Soviet Union were ends in themselves, I also considered them possible means to hasten the end of the war. At worst, Hanoi was bound to feel less confident if Washington was dealing with Moscow and Beijing. At best, if the two major Communist powers decided that they had bigger fish to fry, Hanoi would be pressured into negotiating a settlement we could accept.
Having made considerable progress over the previous two years in U.S.-Soviet relations, Nixon embarked on a second trip to the Soviet Union in 1974. He arrived in Moscow on June 27 to a welcome ceremony, cheering crowds, and a state dinner at the Grand Kremlin Palace that evening. Nixon and Brezhnev met in Yalta, where they discussed a proposed mutual defense pact, détente, and MIRVs. While he considered proposing a comprehensive test-ban treaty, Nixon felt he would not have time as president to complete it. There were no significant breakthroughs in these negotiations.
Middle Eastern policy.
As part of the Nixon Doctrine that the U.S. would avoid direct combat assistance to allies where possible, instead giving them assistance to defend themselves, the U.S. greatly increased arms sales to the Middle East—particularly Israel, Iran and Saudi Arabia—during the Nixon administration. The Nixon administration strongly supported Israel, an American ally in the Middle East, but the support was not unconditional. Nixon believed that Israel should make peace with its Arab neighbors and that the United States should encourage it. The president believed that—except during the Suez Crisis—the U.S. had failed to intervene with Israel, and should use the leverage of the large U.S. military aid to Israel to urge the parties to the negotiating table. However, the Arab-Israeli conflict was not a major focus of Nixon's attention during his first term—for one thing, he felt that no matter what he did, American Jews would oppose his reelection.
On October 6, 1973, an Arab coalition led by Egypt and Syria, supported with tons of arms and materiel by the Soviet Union, attacked Israel in what was known as the Yom Kippur War. Israel suffered heavy losses and Nixon ordered an airlift to resupply Israeli losses, cutting through inter-departmental squabbles and bureaucracy and taking personal responsibility for any response by Arab nations. More than a week later, by the time the U.S. and Soviet Union began negotiating a truce, Israel had penetrated deep into enemy territory. The truce negotiations rapidly escalated into a superpower crisis; when Israel gained the upper-hand, Egyptian President Sadat requested a joint U.S.-USSR peacekeeping mission, which the U.S. refused. When Soviet Premier Brezhnev threatened to unilaterally enforce any peacekeeping mission militarily, Nixon ordered the U.S. military to DEFCON3, placing all U.S. military personnel and bases on alert for nuclear war. This was the closest that the world had come to nuclear war since the Cuban Missile Crisis. Brezhnev backed down as a result of Nixon's actions.
Because Israel's victory was largely due to U.S. support, the Arab OPEC nations retaliated by refusing to sell crude oil to the U.S., resulting in the 1973 oil crisis. The embargo caused gasoline shortages and rationing in the United States in late 1973, and was eventually ended by the oil-producing nations as peace in the Middle East took hold.
After the war, and under Nixon's presidency, the U.S. reestablished relations with Egypt for the first time since 1967. Nixon used the Middle East crisis to restart the stalled Middle East Peace Negotiations; he wrote in a confidential memo to Kissinger on October 20:
I believe that, beyond a doubt, we are now facing the best opportunity we have had in 15 years to build a lasting peace in the Middle East. I am convinced history will hold us responsible if we let this opportunity slip by... I now consider a permanent Middle East settlement to be the most important final goal to which we must devote ourselves.
Nixon made one of his final international visits as president to the Middle East in June 1974, and became the first President to visit Israel.
Domestic policy.
Economy.
At the time Nixon took office in 1969, inflation was at 4.7 percent—its highest rate since the Korean War. The Great Society had been enacted under Johnson, which, together with the Vietnam War costs, was causing large budget deficits. Unemployment was low, but interest rates were at their highest in a century. Nixon's major economic goal was to reduce inflation; the most obvious means of doing so was to end the war. This could not be accomplished overnight, and the U.S. economy continued to struggle through 1970, contributing to a lackluster Republican performance in the midterm congressional elections (Democrats controlled both Houses of Congress throughout Nixon's presidency). According to political economist Nigel Bowles in his 2011 study of Nixon's economic record, the new president did little to alter Johnson's policies through the first year of his presidency.
Nixon was far more interested in foreign affairs than domestic policies, but believed that voters tend to focus on their own financial condition, and that economic conditions were a threat to his reelection. As part of his "New Federalism" views, he proposed grants to the states, but these proposals were for the most part lost in the congressional budget process. However, Nixon gained political credit for advocating them. In 1970, Congress had granted the President the power to impose wage and price freezes, though the Democratic majorities, knowing Nixon had opposed such controls through his career, did not expect Nixon to actually use the authority. With inflation unresolved by August 1971, and an election year looming, Nixon convened a summit of his economic advisers at Camp David. He then announced temporary wage and price controls, allowed the dollar to float against other currencies, and ended the convertibility of the dollar into gold. Bowles points out, "by identifying himself with a policy whose purpose was inflation's defeat, Nixon made it difficult for Democratic opponents ... to criticize him. His opponents could offer no alternative policy that was either plausible or believable since the one they favored was one they had designed but which the president had appropriated for himself." Nixon's policies dampened inflation through 1972, although their aftereffects contributed to inflation during his second term and into the Ford administration.
After he won reelection, Nixon found inflation returning. He reimposed price controls in June 1973. The price controls became unpopular with the public and businesspeople, who saw powerful labor unions as preferable to the price board bureaucracy. The controls produced food shortages, as meat disappeared from grocery stores and farmers drowned chickens rather than sell them at a loss. Despite the failure to control inflation, controls were slowly ended, and on April 30, 1974, their statutory authorization lapsed.
Governmental initiatives and organization.
Nixon advocated a "New Federalism", which would devolve power to state and local elected officials, though Congress was hostile to these ideas and enacted few of them. He eliminated the Cabinet-level United States Post Office Department, which in 1971 became the government-run United States Postal Service.
Nixon was a late convert to the conservation movement. Environmental policy had not been a significant issue in the 1968 election; the candidates were rarely asked for their views on the subject. He saw that the first Earth Day in April 1970 presaged a wave of voter interest on the subject, and sought to use that to his benefit; in June he announced the formation of the Environmental Protection Agency (EPA). Nixon broke new ground by discussing environment policy in his State of the Union speech; other initiatives supported by Nixon included the Clean Air Act of 1970 and Occupational Safety and Health Administration (OSHA); the National Environmental Policy Act required environmental impact statements for many Federal projects. Nixon vetoed the Clean Water Act of 1972—objecting not to the policy goals of the legislation but to the amount of money to be spent on them, which he deemed excessive. After Congress overrode his veto, Nixon impounded the funds he deemed unjustifiable.
In 1971, Nixon proposed health insurance reform—a private health insurance employer mandate, federalization of Medicaid for poor families with dependent minor children, and support for health maintenance organizations (HMOs). A limited HMO bill was enacted in 1973. In 1974, Nixon proposed more comprehensive health insurance reform—a private health insurance employer mandate and replacement of Medicaid by state-run health insurance plans available to all, with income-based premiums and cost sharing.
Concerned about the prevalence of drug use both domestically and among American soldiers in Vietnam, Nixon called for a War on Drugs, pledging to cut off sources of supply abroad, and to increase funds for education and for rehabilitation facilities.
As one policy initiative, Nixon called for more money for sickle cell research, treatment, and education in February 1971 and signed the National Sickle Cell Anemia Control Act on May 16, 1972. While Nixon called for increased spending on such high-profile items as cancer and sickle cell, at the same time he sought to reduce overall spending at the National Institutes of Health.
Civil rights.
The Nixon years witnessed the first large-scale integration of public schools in the South. Nixon sought a middle way between the segregationist Wallace and liberal Democrats, whose support of integration was alienating some Southern whites. Hopeful of doing well in the South in 1972, he sought to dispose of desegregation as a political issue before then. Soon after his inauguration, he appointed Vice President Agnew to lead a task force, which worked with local leaders—both white and black—to determine how to integrate local schools. Agnew had little interest in the work, and most of it was done by Labor Secretary George Shultz. Federal aid was available, and a meeting with President Nixon was a possible reward for compliant committees. By September 1970, less than ten percent of black children were attending segregated schools. By 1971, however, tensions over desegregation surfaced in Northern cities, with angry protests over the busing of children to schools outside their neighborhood to achieve racial balance. Nixon opposed busing personally but enforced court orders requiring its use.
In addition to desegregating public schools, Nixon implemented the Philadelphia Plan in 1970—the first significant federal affirmative action program. He also endorsed the Equal Rights Amendment after it passed both houses of Congress in 1972 and went to the states for ratification. Nixon had campaigned as an ERA supporter in 1968, though feminists criticized him for doing little to help the ERA or their cause after his election. Nevertheless, he appointed more women to administration positions than Lyndon Johnson had.
Space policy.
After a nearly decade-long national effort, the United States won the race to land astronauts on the Moon on July 20, 1969, with the flight of Apollo 11. Nixon spoke with Neil Armstrong and Buzz Aldrin during their moonwalk. He called the conversation "the most historic phone call ever made from the White House".
Nixon, however, was unwilling to keep funding for the National Aeronautics and Space Administration (NASA) at the high level seen through the 1960s as NASA prepared to send men to the Moon. NASA Administrator Thomas O. Paine drew up ambitious plans for the establishment of a permanent base on the Moon by the end of the 1970s and the launch of a manned expedition to Mars as early as 1981. Nixon, however, rejected both proposals due to the expense. Nixon also canceled the Air Force Manned Orbital Laboratory program in 1969, because unmanned spy satellites were shown to be a more cost-effective way to achieve the same reconnaissance objective.
On March 7, 1970, Nixon announced the end of the Kennedy-Johnson era's massive efforts in the space race, stating "We must think of [space activities] as part of a continuing process... and not as a series of separate leaps, each requiring a massive concentration of energy. Space expenditures must take their proper place within a rigorous system of national priorities... What we do in space from here on in must become a normal and regular part of our national life and must therefore be planned in conjunction with all of the other undertakings which are important to us." He then cancelled the last three planned Apollo lunar missions to place Skylab in orbit more efficiently and free money up for the design and construction of the Space Shuttle.
On May 24, 1972, Nixon approved a five-year cooperative program between NASA and the Soviet space program, culminating in the 1975 joint mission of an American Apollo and Soviet Soyuz spacecraft linking in space.
Reelection, Watergate scandal, and resignation.
1972 presidential campaign.
Nixon believed his rise to power had peaked at a moment of political realignment. The Democratic "Solid South" had long been a source of frustration to Republican ambitions. Goldwater had won several Southern states by opposing the Civil Rights Act of 1964 but had alienated more moderate Southerners. Nixon's efforts to gain Southern support in 1968 were diluted by Wallace's candidacy. Through his first term, he pursued a Southern Strategy with policies, such as his desegregation plans, that would be broadly acceptable among Southern whites, encouraging them to realign with the Republicans in the aftermath of the Civil Rights era. He nominated two Southern conservatives, Clement Haynsworth and G. Harrold Carswell to the Supreme Court, but neither was confirmed by the Senate.
Nixon entered his name on the New Hampshire primary ballot on January 5, 1972, effectively announcing his candidacy for reelection. Virtually assured the Republican nomination, the President had initially expected his Democratic opponent to be Massachusetts Senator Ted Kennedy (brother of the late president), but he was largely removed from contention after the 1969 Chappaquiddick incident. Instead, Maine Senator Edmund Muskie became the front runner, with South Dakota Senator George McGovern in a close second place.
On June 10, McGovern won the California primary and secured the Democratic nomination. The following month, Nixon was renominated at the 1972 Republican National Convention. He dismissed the Democratic platform as cowardly and divisive. McGovern intended to sharply reduce defense spending and supported amnesty for draft evaders as well as abortion rights. With some of his supporters believed to be in favor of drug legalization, McGovern was perceived as standing for "amnesty, abortion and acid". McGovern was also damaged by his vacillating support for his original running mate, Missouri Senator Thomas Eagleton, dumped from the ticket following revelations that he had received treatment for depression. Nixon was ahead in most polls for the entire election cycle, and was reelected on November 7, 1972 in one of the largest landslide election victories in American history. He defeated McGovern with over 60 percent of the popular vote, losing only in Massachusetts and the District of Columbia.
Watergate.
The term "Watergate" has come to encompass an array of clandestine and often illegal activities undertaken by members of the Nixon administration. Those activities included "dirty tricks," or bugging the offices of political opponents and the harassment of activist groups and political figures. The activities were brought to light after five men were caught breaking into Democratic party headquarters at the Watergate complex in Washington, D.C. on June 17, 1972. "The Washington Post" picked up on the story; reporters Carl Bernstein and Bob Woodward relied on an informant known as "Deep Throat"—later revealed to be Mark Felt, associate director at the FBI—to link the men to the Nixon administration. Nixon downplayed the scandal as mere politics, calling news articles biased and misleading. A series of revelations made it clear that the Committee to Re-elect President Nixon, and later the White House, was involved in attempts to sabotage the Democrats. Senior aides such as White House Counsel John Dean faced prosecution; in total 48 officials were convicted of wrongdoing.
In July 1973, White House aide Alexander Butterfield testified under oath to Congress that Nixon had a secret taping system that recorded his conversations and phone calls in the Oval Office. These tapes were subpoenaed by Watergate Special Counsel Archibald Cox; Nixon provided transcripts of the conversations but not the actual tapes, citing executive privilege. With the White House and Cox at loggerheads, Nixon had Cox fired in October in the "Saturday Night Massacre"; he was replaced by Leon Jaworski. In November, Nixon's lawyers revealed that an audio tape of conversations, held in the White House on June 20, 1972, featured an 18½ minute gap. Rose Mary Woods, the President's personal secretary, claimed responsibility for the gap, alleging that she had accidentally wiped the section while transcribing the tape, though her tale was widely mocked. The gap, while not conclusive proof of wrongdoing by the President, cast doubt on Nixon's statement that he had been unaware of the cover-up.
Though Nixon lost much popular support, even from his own party, he rejected accusations of wrongdoing and vowed to stay in office. He insisted that he had made mistakes, but had no prior knowledge of the burglary, did not break any laws, and did not learn of the cover-up until early 1973. On October 10, 1973, Vice President Agnew resigned —unrelated to Watergate— and was convicted on charges of bribery, tax evasion and money laundering during his tenure as Governor of Maryland. Nixon chose Gerald Ford, Minority Leader of the House of Representatives, to replace Agnew.
On November 17, 1973, during a televised question and answer session with the press, Nixon said,
"People have got to know whether or not their President is a crook. Well, I'm not a crook. I've earned everything I've got."
The legal battle over the tapes continued through early 1974, and in April 1974 Nixon announced the release of 1,200 pages of transcripts of White House conversations between him and his aides. The House Judiciary Committee opened impeachment hearings against the President on May 9, 1974, which were televised on the major TV networks. These hearings culminated in votes for impeachment. On July 24, the Supreme Court ruled unanimously that the full tapes, not just selected transcripts, must be released.
The scandal grew to involve a slew of additional allegations against the President, ranging from the improper use of government agencies to accepting gifts in office and his personal finances and taxes; Nixon repeatedly stated his willingness to pay any outstanding taxes due, and paid $465,000 in back taxes in 1974.
Even with support diminished by the continuing series of revelations, Nixon hoped to fight the charges. However, one of the new tapes, recorded soon after the break-in, demonstrated that Nixon had been told of the White House connection to the Watergate burglaries soon after they took place, and had approved plans to thwart the investigation. In a statement accompanying the release of what became known as the "Smoking Gun Tape" on August 5, 1974, Nixon accepted blame for misleading the country about when he had been told of White House involvement, stating that he had a lapse of memory. He met with Republican congressional leaders soon after, and was told he faced certain impeachment in the House and had, at most, only 15 votes in his favor in the Senate— far fewer than the 34 he needed to avoid removal from office.
Resignation.
In light of his loss of political support and the near-certainty of impeachment, Nixon resigned the office of the presidency on August 9, 1974, after . The resignation speech was delivered from the Oval Office and was carried live on radio and television. Nixon stated that he was resigning for the good of the country and asked the nation to support the new president, Gerald Ford. Nixon went on to review the accomplishments of his presidency, especially in foreign policy. He defended his record as president, quoting from Theodore Roosevelt's 1910 speech "Citizenship in a Republic":
Sometimes I have succeeded and sometimes I have failed, but always I have taken heart from what Theodore Roosevelt once said about the man in the arena, "whose face is marred by dust and sweat and blood, who strives valiantly, who errs and comes up short again and again because there is not effort without error and shortcoming, but who does actually strive to do the deed, who knows the great enthusiasms, the great devotions, who spends himself in a worthy cause, who at the best knows in the end the triumphs of high achievements and who at the worst, if he fails, at least fails while daring greatly".
Nixon's speech received generally favorable initial responses from network commentators, with only Roger Mudd of CBS stating that Nixon had not admitted wrongdoing. It was termed "a masterpiece" by Conrad Black, one of his biographers. Black opined that "What was intended to be an unprecedented humiliation for any American president, Nixon converted into a virtual parliamentary acknowledgement of almost blameless insufficiency of legislative support to continue. He left while devoting half his address to a recitation of his accomplishments in office." 
Later years and death.
Pardon and illness.
Following his resignation, the Nixons flew to their home La Casa Pacifica in San Clemente, California. According to his biographer, Aitken, after his resignation, "Nixon was a soul in torment". Congress had funded Nixon's transition costs, including some salary expenses, though reducing the appropriation from $850,000 to $200,000. With some of his staff still with him, Nixon was at his desk by 7 a.m.—with little to do. His former press secretary, Ron Ziegler, sat with him alone for hours each day.
Nixon's resignation had not put an end to the desire among many to see him punished. The Ford White House considered a pardon of Nixon, though it would be unpopular in the country. Nixon, contacted by Ford emissaries, was initially reluctant to accept the pardon, but then agreed to do so. Ford, however, insisted on a statement of contrition; Nixon felt he had not committed any crimes and should not have to issue such a document. Ford eventually agreed, and on September 8, 1974, he granted Nixon a "full, free, and absolute pardon", which ended any possibility of an indictment. Nixon then released a statement:
I was wrong in not acting more decisively and more forthrightly in dealing with Watergate, particularly when it reached the stage of judicial proceedings and grew from a political scandal into a national tragedy. No words can describe the depth of my regret and pain at the anguish my mistakes over Watergate have caused the nation and the presidency, a nation I so deeply love, and an institution I so greatly respect.
In October 1974, Nixon fell ill with phlebitis. Told by his doctors that he could either be operated on or die, a reluctant Nixon chose surgery, and President Ford visited him in the hospital. Nixon was under subpoena for the trial of three of his former aides—Dean, Haldeman, and John Ehrlichman—and "The Washington Post", disbelieving his illness, printed a cartoon showing Nixon with a cast on the "wrong foot". Judge John Sirica excused Nixon's presence despite the defendants' objections. Congress instructed Ford to retain Nixon's presidential papers—beginning a three-decade legal battle over the documents that was eventually won by the former president and his estate. Nixon was in the hospital when the 1974 midterm elections were held, and Watergate and the pardon were contributing factors to the Republican loss of 43 seats in the House and three in the Senate.
Return to public life.
In December 1974, Nixon began planning his comeback despite the considerable ill-will against him in the country. He wrote in his diary, referring to himself and Pat,
So be it. We will see it through. We've had tough times before and we can take the tougher ones that we will have to go through now. That is perhaps what we were made for—to be able to take punishment beyond what anyone in this office has had before particularly after leaving office. This is a test of character and we must not fail the test.
By early 1975, Nixon's health was improving. He maintained an office in a Coast Guard station 300 yards from his home, at first taking a golf cart and later walking the route each day; he mainly worked on his memoirs. He had hoped to wait before writing his memoirs; the fact that his assets were being eaten away by expenses and lawyer fees compelled him to begin work quickly. He was handicapped in this work by the end of his transition allowance in February, which compelled him to part with much of his staff, including Ziegler. In August of that year, he met with British talk-show host and producer David Frost, who paid him $600,000 for a series of sit-down interviews, filmed and aired in 1977. They began on the topic of foreign policy, recounting the leaders he had known, but the most remembered section of the interviews was that on Watergate. Nixon admitted that he had "let down the country" and that "I brought myself down. I gave them a sword and they stuck it in. And they twisted it with relish. And, I guess, if I'd been in their position, I'd have done the same thing." The interviews garnered 45–50 million viewers—becoming the most-watched program of their kind in television history.
The interviews helped improve Nixon's financial position—at one point in early 1975 he had only $500 in the bank—as did the sale of his Key Biscayne property to a trust set up by wealthy Nixon friends such as Bebe Rebozo. In February 1976, Nixon visited China at the personal invitation of Mao. Nixon had wanted to return to China, but chose to wait until after Ford's own visit in 1975. Nixon remained neutral in the close 1976 primary battle between Ford and Reagan. Ford won, but was defeated by Georgia Governor Jimmy Carter in the general election. The Carter administration had little use for Nixon and blocked his planned trip to Australia, causing the government of Prime Minister Malcolm Fraser to withhold its official invitation.
In 1976, Nixon was disbarred in the state of New York for obstruction of justice in the Watergate affair. Nixon chose not to present any defense.
In early 1978, Nixon went to the United Kingdom. He was shunned by American diplomats and by most ministers of the James Callaghan government. He was welcomed, however, by the Leader of the Opposition, Margaret Thatcher, as well as by former prime ministers Lord Home and Sir Harold Wilson. Two other former prime ministers, Harold Macmillan and Edward Heath declined to meet him. Nixon addressed the Oxford Union regarding Watergate:
Some people say I didn't handle it properly and they're right. I screwed it up. "Mea culpa". But let's get on to my achievements. You'll be here in the year 2000 and we'll see how I'm regarded then.
Author and elder statesman.
In 1978, Nixon published his memoirs, "RN: The Memoirs of Richard Nixon", the first of ten books he was to author in his retirement. The book was a bestseller and attracted a generally positive critical response. Nixon journeyed to the White House in 1979, invited by Carter for the state dinner for Chinese Vice Premier Deng Xiaoping. Carter had not wanted to invite Nixon, but Deng had stated he would visit Nixon in California if the former president was not invited. Nixon had a private meeting with Deng and visited Beijing again in mid-1979.
On August 10, 1979, the Nixons purchased a New York City townhouse at 817 Fifth Avenue after being rejected by two Manhattan co-ops. When the former Shah of Iran died in Egypt in July 1980, Nixon defied the State Department, which intended to send no U.S. representative, by attending the funeral. Though Nixon had no official credentials, as a former president he was seen as the American presence at its former ally's funeral. Nixon supported Ronald Reagan for president in 1980, making television appearances portraying himself as, in biographer Stephen Ambrose's words, "the senior statesman above the fray". He wrote guest articles for many publications both during the campaign and after Reagan's victory. After eighteen months in the New York City townhouse, Nixon and his wife moved in 1981 to Saddle River, New Jersey.
Throughout the 1980s, Nixon maintained an ambitious schedule of speaking engagements and writing, traveled, and met with many foreign leaders, especially those of Third World countries. He joined former Presidents Ford and Carter as representatives of the United States at the funeral of Egyptian President Anwar Sadat. On a trip to the Middle East, Nixon made his views known regarding Saudi Arabia and Libya, which attracted significant U.S. media attention; "The Washington Post" ran stories on Nixon's "rehabilitation". Nixon journeyed to the Soviet Union in 1986 and on his return sent President Reagan a lengthy memorandum containing foreign policy suggestions and his personal impressions of Mikhail Gorbachev. Following this trip, Nixon was ranked in a Gallup poll as one of the ten most admired men in the world.
In 1986, Nixon addressed a convention of newspaper publishers, impressing his audience with his "tour d'horizon" of the world. At the time, political pundit Elizabeth Drew wrote, "Even when he was wrong, Nixon still showed that he knew a great deal and had a capacious memory, as well as the capacity to speak with apparent authority, enough to impress people who had little regard for him in earlier times." "Newsweek" ran a story on "Nixon's comeback" with the headline "He's back".
On July 19, 1990, the Richard Nixon Library and Birthplace in Yorba Linda, California opened as a private institution with the Nixons in attendance. They were joined by a large crowd of people, including Presidents Ford, Reagan, and George H. W. Bush, as well as their wives, Betty, Nancy, and Barbara. In January 1991, the former president founded the Nixon Center (today the Center for the National Interest), a Washington policy think tank and conference center.
Pat Nixon died on June 22, 1993, of emphysema and lung cancer. Her funeral services were held on the grounds of the Richard Nixon Library and Birthplace. Former President Nixon was distraught throughout the interment and delivered a moving tribute to her inside the library building.
Death and funeral.
Nixon suffered a severe stroke on April 18, 1994, while preparing to eat dinner in his Park Ridge home. A blood clot resulting from his heart condition had formed in his upper heart, broken off, and traveled to his brain. He was taken to New York Hospital–Cornell Medical Center in Manhattan, initially alert but unable to speak or to move his right arm or leg. Damage to the brain caused swelling (cerebral edema), and Nixon slipped into a deep coma. He died at 9:08 p.m. on April 22, 1994, with his daughters at his bedside. He was 81 years old.
Nixon's funeral took place on April 27, 1994. Eulogists at the Nixon Library ceremony included President Bill Clinton, former Secretary of State Henry Kissinger, Senate Minority Leader Bob Dole, California Governor Pete Wilson, and the Reverend Billy Graham. Also in attendance were former Presidents Ford, Carter, Reagan, George H. W. Bush, and their wives.
Richard Nixon is buried beside his wife Pat on the grounds of the Nixon Library. He was survived by his two daughters, Tricia and Julie, and four grandchildren. In keeping with his wishes, his funeral was not a full state funeral, though his body did lie in repose in the Nixon Library lobby from April 26 to the morning of the funeral service. Mourners waited in line for up to eight hours in chilly, wet weather to pay their respects. At its peak, the line to pass by Nixon's casket was three miles long with an estimated 42,000 people waiting to pay their respects.
John F. Stacks of "Time" magazine said of Nixon shortly after his death, "An outsize energy and determination drove him on to recover and rebuild after every self-created disaster that he faced. To reclaim a respected place in American public life after his resignation, he kept traveling and thinking and talking to the world's leaders ... and by the time Bill Clinton came to the White House [in 1993], Nixon had virtually cemented his role as an elder statesman. Clinton, whose wife served on the staff of the committee that voted to impeach Nixon, met openly with him and regularly sought his advice." Tom Wicker of "The New York Times" noted that Nixon had been equalled only by Franklin Roosevelt in being five times nominated on a major party ticket and, quoting Nixon's 1962 farewell speech, wrote, "Richard Nixon's jowly, beard-shadowed face, the ski-jump nose and the widow's peak, the arms upstretched in the V-sign, had been so often pictured and caricatured, his presence had become such a familiar one in the land, he had been so often in the heat of controversy, that it was hard to realize the nation really would not 'have Nixon to kick around anymore'." Ambrose said of the reaction to Nixon's death, "To everyone's amazement, except his, he's our beloved elder statesman."
Upon Nixon's death, almost all of the news coverage mentioned Watergate, but for the most part, the coverage was favorable to the former president. "The Dallas Morning News" stated, "History ultimately should show that despite his flaws, he was one of our most farsighted chief executives." This offended some; columnist Russell Baker complained of "a group conspiracy to grant him absolution". Cartoonist Jeff Koterba of the "Omaha World-Herald" depicted History before a blank canvas, his subject Nixon, as America looks on eagerly. The artist urges his audience to sit down; the work will take some time to complete, as "this portrait is a little more complicated than most".
Legacy.
Historian and political scientist James MacGregor Burns observed of Nixon, "How can one evaluate such an idiosyncratic president, so brilliant and so morally lacking?" Nixon's biographers disagree on how he will be perceived by history. According to Ambrose, "Nixon wanted to be judged by what he accomplished. What he will be remembered for is the nightmare he put the country through in his second term and for his resignation." Irwin Gellman, who chronicled Nixon's congressional career, suggests that "he was remarkable among his congressional peers, a success story in a troubled era, one who steered a sensible anti-Communist course against the excess of McCarthy". Aitken feels that "Nixon, both as a man and as a statesman, has been excessively maligned for his faults and inadequately recognised for his virtues. Yet even in a spirit of historical revisionism, no simple verdict is possible."
Nixon's Southern Strategy is credited by some historians as causing the South to become a Republican stronghold, though others deem economic factors more important to the change. Throughout his career, he was instrumental in moving his party away from the control of isolationists, and as a congressman was a persuasive advocate of containing Soviet communism. According to his biographer, Herbert Parmet, "Nixon's role was to steer the Republican party along a middle course, somewhere between the competitive impulses of the Rockefellers, the Goldwaters, and the Reagans."
Nixon is given credit for his stance on domestic affairs, which resulted in the passage and enforcement of environmental and regulatory legislation. Historian Paul Charles Milazzo in his 2011 paper on Nixon and the environment, points to Nixon's creation of the EPA and his enforcement of legislation such as the 1973 Endangered Species Act, stating that "though unsought and unacknowledged, Richard Nixon's environmental legacy is secure."
Nixon saw his policies regarding Vietnam, China, and the Soviets as key to his place in history. George McGovern, Nixon's onetime opponent, commented in 1983, "President Nixon probably had a more practical approach to the two superpowers, China and the Soviet Union, than any other president since World War II ... With the exception of his inexcusable continuation of the war in Vietnam, Nixon really will get high marks in history." Political scientist Jussi M. Hanhimäki disagrees, saying Nixon's diplomacy was merely a continuation of the Cold War policy of containment, using diplomatic rather than military means.
Historian Keith W. Olson has written that Nixon left a negative legacy: fundamental mistrust of government with its roots in Vietnam and Watergate. During the impeachment of Bill Clinton in 1998, both sides tried to use Nixon and Watergate to their advantage: Republicans suggested that Clinton's misconduct had been comparable to Nixon's, while Democrats contended that Nixon's actions had been far more serious than those of the incumbent. Another legacy, for a time, was a decrease in the power of the presidency as Congress passed restrictive legislation in the wake of Watergate. Olson suggests that grants of power to George W. Bush in the aftermath of the 9/11 attacks restored the president's power.
Personality and public image.
Nixon's career was frequently dogged by his persona and the public's perception of it. Editorial cartoonists and comedians often exaggerated his appearance and mannerisms, to the point where the line between the human and the caricature became increasingly blurred. He was often portrayed with unshaven jowls, slumped shoulders, and a furrowed, sweaty brow.
Nixon had a complex personality, both very secretive and awkward, yet strikingly reflective about himself. He was inclined to distance himself from people and was formal in all aspects, wearing a coat and tie even when home alone. Nixon biographer Conrad Black described him as being "driven" though also "uneasy with himself in some ways". According to Black, Nixon "thought that he was doomed to be traduced, double-crossed, unjustly harassed, misunderstood, underappreciated, and subjected to the trials of Job, but that by the application of his mighty will, tenacity, and diligence, he would ultimately prevail". Biographer Elizabeth Drew summarized Nixon as a "smart, talented man, but most peculiar and haunted of presidents". In his account of the Nixon presidency, author Richard Reeves described Nixon as "a strange man of uncomfortable shyness, who functioned best alone with his thoughts". Nixon's presidency was doomed by his personality, Reeves argues: "He assumed the worst in people and he brought out the worst in them ... He clung to the idea of being 'tough'. He thought that was what had brought him to the edge of greatness. But that was what betrayed him. He could not open himself to other men and he could not open himself to greatness."
Nixon believed that putting distance between himself and other people was necessary for him as he advanced in his political career and became president. Even Bebe Rebozo, by some accounts his closest friend, did not call him by his first name. Nixon stated of this, "Even with close friends, I don't believe in letting your hair down, confiding this and that and the other thing—saying, 'Gee, I couldn't sleep' ... I believe you should keep your troubles to yourself. That's just the way I am. Some people are different. Some people think it's good therapy to sit with a close friend and, you know, just spill your guts ... [and] reveal their inner psyche—whether they were breast-fed or bottle-fed. Not me. No way." When told that most Americans, even at the end of his career, did not feel they knew him, Nixon replied, "Yeah, it's true. And it's not necessary for them to know."
References.
Other sources.
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="25475" url="http://en.wikipedia.org/wiki?curid=25475" title="Role-playing game">
Role-playing game

A role-playing game (RPG and sometimes roleplaying game) is a game in which players assume the roles of characters in a fictional setting. Players take responsibility for acting out these roles within a narrative, either through literal acting or through a process of structured decision-making or character development. Actions taken within many games succeed or fail according to a formal system of rules and guidelines.
There are several forms of RPG. The original form, sometimes called the "tabletop RPG", is conducted through discussion, whereas in live action role-playing games (LARP) players physically perform their characters' actions. In both of these forms, an arranger called a game master (GM) usually decides on the rules and setting to be used and acts as referee, while each of the other players plays the role of a single character.
Several varieties of RPG also exist in electronic media, such as multi-player text-based MUDs and their graphics-based successors, massively multiplayer online role-playing games (MMORPGs). Role-playing games also include single-player offline role-playing video games in which players control a character or team who undertake quests, and may include capabilities that advance using statistical mechanics. These games often share settings and rules with tabletop RPGs, but emphasize character advancement more than collaborative storytelling.
Despite this variety of forms, some game forms such as trading card games and wargames that are related to role-playing games may not be included. Role-playing activity may sometimes be present in such games, but it is not the primary focus. The term is also sometimes used to describe roleplay simulation games and exercises used in teaching, training, and academic research.
Purpose.
Both authors and major publishers of tabletop role-playing games consider them to be a form of interactive and collaborative storytelling. Events, characters, and narrative structure give a sense of a narrative experience, and the game need not have a strongly-defined storyline. Interactivity is the crucial difference between role-playing games and traditional fiction. Whereas a viewer of a television show is a passive observer, a player in a role-playing game makes choices that affect the story. Such role-playing games extend an older tradition of storytelling games where a small party of friends collaborate to create a story.
While simple forms of role-playing exist in traditional children's games of make believe, role-playing games add a level of sophistication and persistence to this basic idea with additions such as game facilitators and rules of interaction. Participants in a role-playing game will generate specific characters and an ongoing plot. A consistent system of rules and a more or less realistic campaign setting in games aids suspension of disbelief. The level of realism in games ranges from just enough internal consistency to set up a believable story or credible challenge up to full-blown simulations of real-world processes.
Varieties.
Role-playing games are played in a wide variety of formats ranging from discussing character interaction in tabletop form to physically acting out characters in LARP to playing characters virtually in digital media. There is also a great variety of systems of rules and game settings. Games that emphasize plot and character interaction over game mechanics and combat sometimes prefer the name storytelling game. These types of games tend to minimize or altogether eliminate the use of dice or other randomizing elements. Some games are played with characters created before the game by the GM, rather than those created by the players. This type of game is typically played at gaming conventions, or in standalone games that do not form part of a campaign.
Tabletop.
Tabletop and "pen-and-paper" (PnP) RPGs are conducted through discussion in a small social gathering. The GM describes the game world and its inhabitants. The other players describe the intended actions of their characters, and the GM describes the outcomes. Some outcomes are determined by the game system, and some are chosen by the GM.
This is the format in which role-playing games were first popularized. The first commercially available RPG, "Dungeons & Dragons" (D&D), was inspired by fantasy literature and the wargaming hobby and was published in 1974. The popularity of D&D led to the birth of the tabletop role-playing game industry, which publishes games with many different themes, rules, and styles of play.
This format is often referred to simply as a "role-playing game". To distinguish this form of RPG from other formats, the retronyms "tabletop role-playing game" or "pen and paper role-playing game" are sometimes used, though neither a table nor pen and paper are strictly necessary.
Live action.
A LARP is played more like improvisational theatre. Participants act out their characters' actions instead of describing them, and the real environment is used to represent the imaginary setting of the game world. Players are often costumed as their characters and use appropriate props, and the venue may be decorated to resemble the fictional setting. Some live action role-playing games use rock-paper-scissors or comparison of attributes to resolve conflicts symbolically, while other LARPs use physical combat with simulated arms such as airsoft guns or foam weapons.
LARPs vary in size from a handful of players to several thousand, and in duration from a couple of hours to several days. Because the number of players in a LARP is usually larger than in a tabletop role-playing game, and the players may be interacting in separate physical spaces, there is typically less of an emphasis on tightly maintaining a narrative or directly entertaining the players, and game sessions are often managed in a more distributed manner.
Electronic media.
Tabletop role-playing games have been translated into a variety of electronic formats. As early as 1974, the same year as the release of Dungeons & Dragons, unlicensed versions of it were developed on mainframe university systems under titles such as "dnd" and "Dungeon". These early computer RPGs influenced all of electronic gaming, as well as spawning the role-playing video game genre. Some authors divide digital role-playing games into two intertwined groups: single player games using RPG-style mechanics, and multiplayer games incorporating social interaction.
Single-player.
Single player role-playing video games form a loosely defined genre of computer and console games with origins in role-playing games such as "Dungeons & Dragons", on which they base much of their terminology, settings, and game mechanics. This translation changes the experience of the game, providing a visual representation of the world but emphasizing statistical character development over collaborative, interactive storytelling.
Multi-player.
Online text-based role-playing games involve many players using some type of text-based interface and an Internet connection to play an RPG. Games played in a real-time way include MUDs, MUSHes, and other varieties of MU*. Games played in a turn-based fashion include play-by-mail games and play-by-post games.
Massively multi-player online role-playing games (MMORPGs) combine the large-scale social interaction and persistent world of MUDs with graphic interfaces. Most MMORPGs do not actively promote in-character role-playing, however players can use the games' communication functions to role-play so long as other players cooperate. The majority of players in MMORPGs do not engage in role-play in this sense.
Computer-assisted gaming can be used to add elements of computer gaming to in-person tabletop role-playing, where computers are used for record-keeping and sometimes to resolve combat, while the participants generally make decisions concerning character interaction.
Gamemaster.
One common feature of many RPGs is the role of gamemaster, a participant who has special duties to present the fictional setting, arbitrate the results of character actions, and maintain the narrative flow. In tabletop and live action RPGs the GM performs these duties in person. In video RPGs many of the functions of a GM are fulfilled by the game engine, however some multi-player video RPGs also allow for a participant to take on a GM role through a visual interface called a "GM toolkit", albeit with abilities limited by the available technology.
Player character.
Another standard concept in RPGs is the player character, a character in the fictional world of the game whose actions the player controls. Typically each player controls a separate player character, each of whom acts as a protagonist in the story.
Non-player character.
In contrast to player characters, non-player characters (NPCs) are controlled by the gamemaster or game engine, or by people assisting the gamemaster. Non-player characters fill out the population of the fictional setting and can act as antagonists, bystanders or allies of the player characters.
References.
</dl>

</doc>
<doc id="25476" url="http://en.wikipedia.org/wiki?curid=25476" title="Raised-bed gardening">
Raised-bed gardening

Raised-bed gardening is a form of gardening in which the soil is formed in three-to-four-foot-wide (1.0–1.2 m) beds, which can be of any length or shape. The soil is raised above the surrounding soil (approximately six inches to waist-high), is sometimes enclosed by a frame generally made of wood, rock, or concrete blocks, and may be enriched with compost. The vegetable plants are spaced in geometric patterns, much closer together than in conventional row gardening. The spacing is such that when the vegetables are fully grown, their leaves just barely touch each other, creating a microclimate in which weed growth is suppressed and moisture is conserved. Raised beds produce a variety of benefits: they extend the planting season, they can reduce weeds if designed and planted properly, and they reduce the need to use poor native soil. Since the gardener does not walk on the raised beds, the soil is not compacted and the roots have an easier time growing. The close plant spacing and the use of compost generally result in higher yields with raised beds in comparison to conventional row gardening. Waist-high raised beds enable the elderly and physically disabled to grow vegetables without having to bend over to tend them.
Overview.
Raised beds lend themselves to the development of complex agriculture systems that utilize many of the principles and methods of permaculture. They can be used effectively to control erosion and recycle and conserve water and nutrients by building them along contour lines on slopes. This also makes more space available for intensive crop production. They can be created over large areas with the use of several commonly available tractor-drawn implements and efficiently maintained, planted and harvested using hand tools.
This form of gardening is compatible with square foot gardening and companion planting.
Circular raised beds with a path to the center (a slice of the circle cut out) are called keyhole gardens. Often the center has a chimney of sorts built with sticks and then lined with feedbags or grasses that allows water placed at the center to flow out into the soil and reach the plants' roots.
Materials and construction.
Vegetable garden bed construction materials should be chosen carefully. Some concerns exist regarding the use of pressure-treated timber. Pine that was treated using chromated copper arsenate or CCA, a toxic chemical mix for preserving timber that may leach chemicals into the soil which in turn can be drawn up into the plants, is a concern for vegetable growers, where part or all of the plant is eaten. If using timber to raise the garden bed, ensure that it is an untreated hardwood to prevent the risk of chemicals leaching into the soil. A common approach is to use timber sleepers joined with steel rods to hold them together. Another approach is to use concrete blocks, although less aesthetically pleasing, they are inexpensive to source and easy to use. On the market are also prefab raised garden bed solutions which are made from long lasting polyethylene that is UV stabilized and food grade so it will not leach undesirable chemicals into the soil or deteriorate in the elements. A double skinned wall provides an air pocket of insulation that minimizes the temperature fluctuations and drying out of the soil in the garden bed. Sometimes raised bed gardens are covered with clear plastic to protect the crops from wind and strong rains. Pre-manufactured raised bed gardening boxes also exist.

</doc>
<doc id="25477" url="http://en.wikipedia.org/wiki?curid=25477" title="Ring">
Ring

Ring may refer to:

</doc>
<doc id="25479" url="http://en.wikipedia.org/wiki?curid=25479" title="Miniature">
Miniature

A miniature is a small-scale reproduction, or a small version. It may refer to:

</doc>
<doc id="25480" url="http://en.wikipedia.org/wiki?curid=25480" title="Richard Petty">
Richard Petty

Richard Lee Petty (born July 2, 1937), nicknamed The King, is a former NASCAR driver who raced in the Strictly Stock/Grand National Era and the NASCAR Winston Cup Series. He is most well known for winning the NASCAR Championship seven times (Dale Earnhardt is the only other driver to accomplish this feat), winning a record 200 races during his career, winning the Daytona 500 a record 7 times, and winning a record 27 races (10 of them consecutively) in the 1967 season alone (a 1972 rule change eliminated races under 250 mi in length, reducing the schedule to 30 [now 36] races.) Statistically, he is the most accomplished driver in the history of the sport and is one of the most respected figures in motorsports as a whole. He also collected a record number of poles (127) and over 700 Top 10 finishes in his 1,184 starts, including 513 consecutive starts from 1971–1989. Petty was the only driver to ever win in his 500th race start, until Matt Kenseth joined him in 2013. Petty was inducted into the inaugural class of the NASCAR Hall of Fame in 2010.
Petty is a second generation driver. His father, Lee Petty, won the first Daytona 500 in 1959 and was also a 3-time NASCAR champion. His son Kyle was also a well-known NASCAR driver. His grandson, Adam, was killed in a practice crash at New Hampshire International Speedway on May 12, 2000, five weeks after the death of Lee Petty. Adam's brother Austin works on day-to-day operations of the Victory Junction Gang Camp, a Hole in the Wall Gang Camp established by the Pettys after Adam's death. Petty married Lynda Owens (who died on March 25, 2014 at her home in Level Cross, North Carolina at age 72, after a long battle with cancer) in 1958. They had four children—Kyle Petty, Sharon Petty-Farlow, Lisa Petty-Luck, and Rebecca Petty-Moffit. The family resides in Petty's home town of Level Cross, North Carolina and operates Richard Petty Motorsports. The Richard Petty Museum was formerly in nearby Randleman, North Carolina but moved back to its original location in March 2014.
Racing career.
Petty was born in Level Cross, North Carolina to Elizabeth (née Toomes) and Lee Arnold Petty, also a NASCAR driver, and the older brother of Maurice Petty. He began his NASCAR career on July 18, 1958, 16 days after his 21st birthday. His first race was held at CNE Stadium in Toronto, Canada. In 1959, he was named NASCAR Rookie of the Year, after he produced 9 top 10 finishes, including six Top 5 finishes.(21) In Lakewood Georgia in 1959 Petty won his first race but his father Lee protested and due to a scoring error was awarded the win.(20)
The 1960s.
In 1960, he finished 2nd in the NASCAR Grand National Points Race. 1963 was his breakout year, winning at tracks like Martinsville and Bridgehampton. In 1964, driving a potent Plymouth with a new Hemi engine, Petty led 184 of the 200 laps to capture his first Daytona 500, en route to 9 victories, earning over $114,000 and his first Grand National championship.
Joining in the Chrysler boycott of NASCAR due to the organizing body's ban of the Hemi engine, Petty spent much of 1965 competing as a drag racer. He crashed his car at the Southeastern Dragway, in Dallas, Georgia, on February 28, 1965, killing a six-year-old boy, Wayne Dye, and injuring seven others. Petty, his father Lee, and Chrysler Corporation faced lawsuits totaling more than $1 million, though Petty and his team came to settlements with the lawsuits within 1 month of the suits being filed.
On February 27, 1966, Richard Petty overcame a 2-lap deficit to win his second Daytona 500 when the race was stopped on lap 198 of 200 because of a thunderstorm. This made him the first driver to win the event twice. 1967 was a milestone year. In that year, Petty won 27 of the 48 races he entered, including a record 10 wins in a row (between August 12 and October 1, 1967). He won his second Grand National Championship. One of the 27 victories was the Southern 500 at Darlington, which would be his only Southern 500 victory. His dominance in this season earned him the nickname "King Richard". He had previously been known as "the Randleman Rocket". 
In 1969 Petty switched brands to Ford, due to his belief the Plymouth was not competitive on super-speedways; he wanted a slippery Dodge Daytona but Chrysler executives insisted he stay with Plymouth. He would win 10 races and finish second in points. Won back in 1970 by the sleek new Plymouth Superbird with shark nose and goalpost wing, Petty returned to Plymouth for the 1970 season. This is the car in which Petty is cast in the 2006 Pixar film "Cars", in which Richard and Lynda Petty had voice roles.
The 1970s.
On February 14, 1971, Petty won his third Daytona 500, driving a brand-new (for 1971) Plymouth Road Runner and beating Buddy Baker, by little more than a car length en route to another historic year, making him the first driver to win the event three times. He won 20 more races, became the first driver to earn more than $1 million in career earnings, and claimed his third Grand National Championship. At the end of the 1971 season Chrysler told the Pettys they would no longer receive direct factory funding support, causing the Petty team great concern. In 1972, STP (motor oil company) began what would turn into a successful 28-year sponsorship arrangement with Petty, however it marked the end of Petty's famous all "Petty Blue" paint job. STP at first insisted on an all STP orangish-red color for the cars, but Petty balked, and after an all-night negotiation session the familiar STP orange/Petty blue paint scheme was agreed to as a compromise that would find its way to all STP racing operations, most notably Gordon Johncock's 1982 Indianapolis 500 winner. Petty won his 4th Winston Cup Championship, thanks to his 28 top-10 finishes, including 25 top-5 finishes and 8 victories. 1972 was a year of change in other ways, as it was the last year Petty would campaign a Plymouth based race car; as in the middle of the year Petty debuted and drove a newly built 1972 Dodge Charger in a few races (winning one of them), as Petty believed the car to have a slight aero advantage over the Plymouth body style. On February 18, 1973, in a driver’s duel, Petty, in a newly built 1973 Dodge Charger (a body style he would use exclusively until the end of 1977) outlasted Baker (now with the K&K Insurance Dodge race team) to win his fourth Daytona 500 after Baker's engine gave out with six laps left. One year later, Petty won the Daytona "450" (shortened 20 laps {50 mi/80 km} due to the energy crisis) for the fifth time en route to his fifth Winston Cup Championship.
The year 1975 was another historic year for Petty, as he won the World 600 for the first time in his career, one of 13 victories en route to his sixth Winston Cup. The 13 victories is a modern (1972 to present) NASCAR record for victories in a season, and was tied in 1998 by Jeff Gordon, although Gordon won 13 out of 33 races, compared to Petty's 13 out of 30 races. In 1976, Petty was involved in one of the most famous finishes in NASCAR history. Petty and David Pearson were racing on the last lap out of turn 4 in the Daytona 500. As Petty tried to pass Pearson, at the exit of turn 4, Petty's right rear bumper hit Pearson's left front bumper. Pearson and Petty both spun and hit the front stretch wall. Petty's car came to rest just yards from the finish line, but his engine stalled. Pearson's car had hit the front stretch wall and clipped another car, but his engine was running. Pearson was able to drive his car toward the finish line, while Petty's car would not restart. Pearson passed Petty on the infield grass and won the Daytona 500. Petty was given credit for second place. Oddly 1978 will stand out as the one year during his prime that Petty did not visit the winners circle. The Petty Enterprises Team could not get the new 1978 Dodge Magnum to handle properly, even though much time, effort, and faith were spent massaging the cars. Unhappy with the seven top-5 finishes (including two second places) Petty climbed out of the Dodge and into a four-year-old used Chevy Monte Carlo after 17 races. The switch to Chevy did not produce any wins however, in the remaining 1978 races. Petty would go on to rebound in 1979, winning the NASCAR championship for the seventh, and last time by 11 points which was the closest points margin in NASCAR history until 1992 .
The Twilight Years.
Petty won two more Daytona 500s in 1979 and 1981. In 1979, he snapped a 45-race drought, winning his sixth Daytona 500, the first to be televised live flag-to-flag; it would become notorious for a fistfight between competitors following the controversial finish. Petty won the race as the first and second place cars of Donnie Allison and Cale Yarborough crashed on the last lap. Petty held off Darrell Waltrip and A. J. Foyt. The race is also regarded as being the genesis of the current surge in NASCAR's popularity. The East Coast was snowed in by a blizzard, giving CBS a captive audience. The win was part of Petty's seventh and last NASCAR Winston Cup Championship. He was able to hold off Waltrip to win the title in 1979.
For 1981, NASCAR dictated that all teams had to show up with the new downsized cars of 110" wheel-base, that Detroit had been building since 1979. Though Petty had been successful with the Chevrolet and Oldsmobile cars he had been running, he wanted to get back to his Mopar roots. After taking a phone call from Lee Iacocca (who personally asked Petty to campaign a Dodge for 1981), the Petty team built a stunning 1981 Dodge Mirada and took it to Daytona in January 1981 for high speed tests. Petty's fans were also in a large part fans of his Dodges, so when word got out about the Mirada testing, 15,000 or so showed up on January 17, 1981 at Daytona Speedway to watch Petty put the Dodge through its paces. Sadly for the fans, the car could do no better than 186 miles per hour, about eight miles per hour slower than the GM and Ford cars. Petty gave up on returning to Dodge knowing that for the superspeedways the Mirada would not be competitive, and bought a Buick Regal for the Daytona race. In the 1981 Daytona 500, Petty used a "fuel only" for his last pit stop, with 25 laps to go, to outfox Bobby Allison and grab his seventh and final Daytona 500 win. This win marked a large change in Petty's racing team. Dale Inman, Petty's longtime crew chief, left the team after the Daytona victory (Inman would win an eighth championship as crew chief in 1984 with Terry Labonte).
While the 1981 season gave Petty 3 wins, he felt the season was a failure, and the Regals being ill-handling and poor in reliability. For 1982, he made the move to the Pontiac Grand Prix, with the promise of substantial factory support from Pontiac. 1982 was a repeat of 1978, and no victories were to be had. At first, the Grand Prix behaved much like the Dodge Magnum of 1978, with handling and speed problems. Toward the end of 1982 things improved with several top-10 finishes, which opened the door to a successful 1983 season with 3 victories, and several top-5 and top-10 finishes. In 1983, he broke his 43-race winless streak from 1982 with a win in the 1983 Carolina 500, barely edging out a young Bill Elliott. After a controversial win at Charlotte in October 1983 (win No. 198), Petty left the race team his father founded for the 1984 season. He spent '84 and '85 driving for Mike Curb before returning to Petty Enterprises in 1986. 
On July 4, 1984, Petty won his 200th (and what would turn to be his final victory) race at the Firecracker 400 at Daytona International Speedway. The race was memorable: On lap 158, Doug Heveron crashed, bringing out the yellow caution flag, essentially turning lap 158 into the last lap as the two drivers battled back to the start-finish line. Petty and Cale Yarborough diced it out on that lap, with Yarborough drafting and taking an early lead before Petty managed to cross the start/finish line only a fender-length ahead. (This is no longer possible because of the 2003 rule change freezing the field immediately upon caution. Furthermore, the green-white-checkered rule was created for if the yellow flag waves with two laps remaining, but not with one lap remaining.) President Ronald Reagan was in attendance, the first sitting president to attend a NASCAR race. Reagan celebrated the milestone with Petty and his family in victory lane.
In early 1988, Petty travelled to Australia to help promote a NASCAR exhibition race at the then new Calder Park Thunderdome, the first NASCAR race outside of North America. While he did not compete in the tracks inaugural race, the Goodyear NASCAR 500 (though his son Kyle did), in testing at the 1.119 mi (1.801 km) track which owner Bob Jane had modelled on the Charlotte Motor Speedway, Richard Petty set an unofficial lap record of 28.2 seconds (142.85 mp/h). This would have in fact landed him on pole position for the race as the fastest time in official qualifying was by Alabama Gang member Neil Bonnett who recorded a 28.829 second lap (139.734 mp/h) in his Pontiac Grand Prix.
Petty's Last Ride.
On October 1, 1991, Petty announced he would retire after the 1992 season. Petty's final top ten finish came at the 1991 Budweiser at the Glen which was the same race J. D. McDuffie was killed in a fifth lap accident. Petty chose to run the entire 1992 season, not just selected events as other drivers have done before retirement. His year-long Fan Appreciation Tour took him around the country, participating in special events, awards ceremonies, and fan-related meetings. Racing Champions ran a promotional line of diecast cars for every race in Petty's Farewell Tour. In his final year behind the wheel, he had two notable races.
At the 1992 Pepsi 400 on July 4, Petty qualified second. Before the start of the race, he was honored with a gift ceremony which included a visit from President George H. W. Bush. At the start, Petty led the first five laps, but dropped out on lap 84 due to fatigue.
Despite the tremendously busy appearance schedule, and mediocre race results, Petty managed to qualify for all 29 races in 1992. On his final visit to each track, Petty would lead the field on the pace lap to salute the fans. Petty's final race was the season-ending Hooter's 500 at Atlanta Motor Speedway. The race was notable in that it was the first career start for Jeff Gordon, and it was the 2nd closest points championship in NASCAR history, with six drivers mathematically eligible to win the championship. A record 160,000 spectators attended the race, which went down to the final lap with Bill Elliott winning the race, and Alan Kulwicki winning the championship by 10 points over Elliott after Davey Allison dropped out early after a crash.
Facing the intense pressure, Petty barely managed to qualify at Atlanta, posting the 39th fastest speed out of 41 cars. He would not have been eligible for the provisional starting position, and had to qualify on speed. On the 94th lap, Petty became tangled up in an accident, and his car caught fire. Petty pulled the car off the track, and climbed out of the burning machine uninjured. His pit crew worked diligently with less than 20 laps to go to get the car running again, and with two laps to go, Petty pulled out of the pits and was credited as running at the finish in his final race. He took his final checkered flag finishing in 35th position. After the race, Petty circled the track to salute the fans one final time in his trademark STP Pontiac.
The following year, he was back into a race car one more time. On August 18, 1993, NASCAR participated in a tire test at the Indianapolis Motor Speedway, in preparations for the 1994 Brickyard 400. Petty drove several laps around the track, and then donated his car to the Speedway's museum.
Petty would again step into a race car in 2003 on the week of the final race under the Winston banner at Homestead-Miami Speedway and took a solo lap honoring his seven Winston Cup Championships for Winston's salute to the champions.
In 2007 at the Pepsi 400 in Daytona, Petty was behind the wheel of a Daytona car during the pace laps, leading the field for the first lap. The field split him and he followed behind the field for one more pace lap before he pulled it in. This was in tribute to Bill France, Jr.
Petty as an owner.
In later years of his career, Petty developed the career of crew leader Robbie Loomis, who was at the helm of Petty Enterprises as crew chief in the 1990s, and won three races—the 1996 Checker Auto Parts 500 at Phoenix, the 1997 ACDelco 400 at North Carolina Speedway, both with Bobby Hamilton driving, and the 1999 Goody's Body Pain 500 at Martinsville Speedway, with John Andretti driving.
He remained as operating owner until his son Kyle Petty took over day-to-day operations a decade later.
However, in 2008, Kyle Petty was released by Petty Enterprises, and due to lack of sponsorship, Petty Enterprises was bought out by Gillett-Evernham Motorsports. The name was originally going to stay the same, but due to Evernham leaving the team, It was renamed Richard Petty Motorsports, despite George Gillett continuing to own the majority of the team.
In November 2010, an investment group including Medallion Financial Corp., Douglas G. Bergeron and Petty, signed and closed sale on racing assets of Richard Petty Motorsports. Andrew M. Murstein, president of Medallion, had been seeking a sports investment since 2008 when he formed a special purpose acquisition company together with Hank Aaron, a Medallion board member, and others.
Petty as a broadcaster.
In 1995 Petty moved to the television broadcast booth joining CBS as a colour commentator immediately following his retirement, but his career in television did not last long.
Sponsorship.
Petty promised his mother not to accept alcohol sponsorship. Therefore, he never collected purses for the Bud Pole Award, and competed at the Busch Clash only once, in 1980. His team Petty Enterprises never competed at the Busch Clash. 
Close calls.
Of all the races he won, Petty is also remembered for three of the many incredible crashes that he survived:
Life after racing.
Petty is currently a spokesman for Liberty Medical, Cheerios and GlaxoSmithKline products Nicorette and Goody's Headache Powder. Petty and his son Kyle have lent their talent to host "Lifting It Right" a lift safety training DVD produced and distributed by the (ALI); it is used in high school vocational programs and community colleges. He has recorded public service announcements for Civitan International, a nonprofit organization of which he is a former member. He is usually seen wearing his trademark sunglasses and a Charlie One Horse cowboy hat, with a large snakeskin hat band and a plume of rooster feathers at the front. In 1996, he was the Republican nominee for North Carolina Secretary of State, but was defeated by State Senator Elaine Marshall in the general election. Petty was mistakenly seen as a shoo-in and his campaigning was sporadic. Following his loss, Petty stated "If I had known I wasn't going to win, I wouldn't have run." He was cast as Strip "The King" Weathers in Pixar's 2006 animated film "Cars" as his 1970 Plymouth Superbird with the number "43", with his wife Lynda Petty appearing as Lynda Weathers ("Mrs. The King") as a 1976 Chrysler Town and Country. A cereal "43's" was created with Petty information on the boxes.
References.
21. 
22. 

</doc>
<doc id="25481" url="http://en.wikipedia.org/wiki?curid=25481" title="Reduction">
Reduction

Reduction, reduced, or reduce may refer to:

</doc>
<doc id="25484" url="http://en.wikipedia.org/wiki?curid=25484" title="Rosemary">
Rosemary

Rosmarinus officinalis, commonly known as rosemary, is a woody, perennial herb with fragrant, evergreen, needle-like leaves and white, pink, purple, or blue flowers, native to the Mediterranean region. It is a member of the mint family Lamiaceae, which includes many other herbs. The name "rosemary" derives from the Latin for "dew" ("ros") and "sea" ("marinus"), or "dew of the sea". The plant is also sometimes called anthos, from the ancient Greek word ἄνθος, meaning "flower". Rosemary has a fibrous root system.
Taxonomy.
"Rosmarinus officinalis" is one of 2–4 species in the genus "Rosmarinus". The other species most often recognized is the closely related, "Rosmarinus eriocalyx", of the Maghreb of Africa and Iberia. The genus was named by the 18th-century naturalist and founding taxonomist Carolus Linnaeus.
Description.
Rosemary is an aromatic evergreen shrub that has leaves similar to hemlock needles. The leaves are used as a flavoring in foods such as stuffings and roast lamb, pork, chicken and turkey. It is native to the Mediterranean and Asia, but is reasonably hardy in cool climates. It can withstand droughts, surviving a severe lack of water for lengthy periods. Forms range from upright to trailing; the upright forms can reach 1.5 m tall, rarely 2 m. The leaves are evergreen, 2 - long and 2–5 mm broad, green above, and white below, with dense, short, woolly hair. The plant flowers in spring and summer in temperate climates, but the plants can be in constant bloom in warm climates; flowers are white, pink, purple or deep blue. Rosemary also has a tendency to flower outside its normal flowering season; it has been known to flower as late as early December, and as early as mid-February.
Mythology.
According to legend, it was draped around the Greek goddess Aphrodite when she rose from the sea, born of Uranus's semen. The Virgin Mary is said to have spread her blue cloak over a white-blossomed rosemary bush when she was resting, and the flowers turned blue. The shrub then became known as the "Rose of Mary".
Usage.
Rosemary is used as a decorative plant in gardens where it may have pest control effects. The leaves are used to flavor various foods, such as stuffings and roast meats.
Cultivation.
Since it is attractive and drought-tolerant, rosemary is used as an ornamental plant in gardens and for xeriscape landscaping, especially in regions of Mediterranean climate. It is considered easy to grow and pest-resistant. Rosemary can grow quite large and retain attractiveness for many years, can be pruned into formal shapes and low hedges, and has been used for topiary. It is easily grown in pots. The groundcover cultivars spread widely, with a dense and durable texture.
Rosemary grows on friable loam soil with good drainage in an open, sunny position. It will not withstand waterlogging and some varieties are susceptible to frost. It grows best in neutral to alkaline conditions (pH 7–7.8) with average fertility. It can be propagated from an existing plant by clipping a shoot (from a soft new growth) 10 - long, stripping a few leaves from the bottom, and planting it directly into soil.
Cultivars.
Numerous cultivars have been selected for garden use. The following are frequently sold:
The following cultivars have gained the Royal Horticultural Society's Award of Garden Merit:-
Culinary use.
Fresh or dried leaves are used in traditional Italian cuisine. They have a bitter, astringent taste and a characteristic aroma which complements many cooked foods. Herbal tea can be made from the leaves. When roasted with meats or vegetables, the leaves impart a mustard-like aroma with an additional fragrance of charred wood compatible with barbecued foods.
In amounts typically used to flavor foods, such as one teaspoon (1 gram), rosemary provides no nutritional value. Rosemary extract has been shown to improve the shelf life and heat stability of omega 3-rich oils which are prone to rancidity.
Fragrance.
Rosemary oil is used for purposes of fragrant bodily perfumes or to emit an aroma into a room. It is also burnt as incense, and used in shampoos and cleaning products.
Phytochemicals and traditional medicine.
Rosemary contains a number of phytochemicals, including rosmarinic acid, camphor, caffeic acid, ursolic acid, betulinic acid, and the antioxidants carnosic acid and carnosol.
In traditional medicine of India, extracts and essential oil from flowers and leaves are used to treat a variety of disorders.
Folklore and customs.
In the Middle Ages, rosemary was associated with wedding ceremonies. The bride would wear a rosemary headpiece and the groom and wedding guests would all wear a sprig of rosemary. From this association with weddings, rosemary was thought to be a love charm.
In myths, rosemary has a reputation for improving memory and has been used as a symbol for remembrance during war commemorations and funerals in Europe and Australia. Mourners would throw it into graves as a symbol of remembrance for the dead. In Shakespeare's "Hamlet", Ophelia says, "There's rosemary, that's for remembrance." (Hamlet, iv. 5.) In Australia, sprigs of rosemary are worn on ANZAC Day and sometimes Remembrance Day to signify remembrance; the herb grows wild on the Gallipoli Peninsula.
Hungary water was first prepared for the Queen of Hungary Elisabeth of Poland to " ... renovate vitality of paralyzed limbs ... " and to treat gout. It was used externally and prepared by mixing fresh rosemary tops into spirits of wine. Don Quixote (Part One, Chapter XVII) mixes it in his recipe of the miraculous balm of Fierabras.

</doc>
<doc id="25486" url="http://en.wikipedia.org/wiki?curid=25486" title="Rosales">
Rosales

Rosales is an order of flowering plants. It is sister to a clade consisting of Fagales and Cucurbitales. It contains about 7700 species, distributed into about 260 genera. Rosales comprise nine families, the type family being the rose family, Rosaceae. The largest of these families are Rosaceae (90/2500) and Urticaceae (54/2600). The order Rosales is divided into three clades that have never been assigned a taxonomic rank. The basal clade consists of the family Rosaceae; another clade consists of four families, including Rhamnaceae; and the third clade consists of the four urticalean families.
The order Rosales is strongly supported as monophyletic in phylogenetic analyses of DNA sequences, such as those carried out by members of the Angiosperm Phylogeny Group. In their APG III system of plant classification, they defined Rosales as consisting of the nine families listed in the box on the right. The relationships of these families were uncertain until 2011, when they were resolved in a molecular phylogenetic study based on two nuclear genes and ten chloroplast genes.
Well-known members of Rosales include: roses, strawberries, blackberries and raspberries, apples and pears, plums, peaches and apricots, almonds, rowan and hawthorn, jujube, elms, banyans, figs, mulberries, breadfruit, nettles, hops, and cannabis.
Cronquist system.
In the obsolete Cronquist system, the order Rosales was many times polyphyletic. It consisted of the family Rosaceae and 23 other families that are now placed in various other orders. These families and their placement in the APG III system are:
Phylogeny.
The following phylogenetic tree is from a cladistic analysis of DNA that was published in 2011.

</doc>
<doc id="25488" url="http://en.wikipedia.org/wiki?curid=25488" title="Rebuttal">
Rebuttal

In law, the rebuttal is a form of evidence that is presented to contradict or nullify other evidence that has been presented by an adverse party. By analogy the same term is used in politics and public affairs to refer to the informal process by which statements, designed to refute or negate specific arguments put forward by opponents, are deployed in the media.
In law, special rules apply to rebuttal. Rebuttal evidence or rebuttal witnesses must be confined solely to the subject matter of the evidence rebutted. New evidence on other subjects may not be brought in rebuttal. However, rebuttal is one of the few vehicles whereby a party may introduce surprise evidence or witnesses. The basic process is as follows: Both sides of a controversy are obliged to declare in advance of trial what witnesses they plan to call, and what each witness is expected to testify to. When either a plaintiff (or prosecutor) or defendant brings direct evidence or testimony which was not anticipated, the other side may be granted a specific opportunity to rebut it. In rebuttal, the rebutting party may generally bring witnesses and evidence which were never declared before, so long as they serve to rebut the prior evidence.

</doc>
<doc id="25489" url="http://en.wikipedia.org/wiki?curid=25489" title="Res ipsa loquitur">
Res ipsa loquitur

In common law, res ipsa loquitur (Latin for "the thing speaks for itself") is a doctrine in tort law that infers negligence from the very nature of an accident or injury, in the absence of direct evidence on how any defendant behaved. Although modern formulations differ by jurisdiction, common law originally stated that the accident must satisfy the necessary elements of negligence, which are duty, breach of duty, causation, and injury. In res ipsa loquitur, the elements of duty of care, breach, and causation are inferred from an injury that does not ordinarily occur without negligence.
History.
Latin phrase.
The term comes from Latin and is literally translated "the thing itself speaks", but the sense is well conveyed in the more common translation, "the thing speaks for itself." The earliest known use of the phrase was by Cicero in his defence speech "Pro Milone". The circumstances of the genesis of the phrase and application by Cicero in Roman legal trials has led to questions whether it reflects on the quality of "res ipsa loquitur" as a legal doctrine subsequent 52 BC, some 1,915 years before "Byrne v Boadle", as well as the question whether Chief Baron Pollock might have taken direct inspiration from Cicero's application of the maxim in writing his judgment in that case.
Leading case.
The legal doctrine was first formulated by Baron Pollock in the 1863 English case "Byrne v Boadle".
Elements of res ipsa loquitur.
The first element may be satisfied in one of three ways:
(a) The injury itself is sufficient to prove blatant or "palpable" negligence as a matter of law. e.g. amputation of the wrong limb, leaving instruments inside body after surgery.
(b) The general experience and observation of mankind is sufficient to support the conclusion that the injury would not have resulted without negligence. e.g. A hysterectomy (removal of the uterus) was performed when the patient consented only to a tubal ligation (clipping of the fallopian tubes for purposes of sterilization).
(c) Expert testimony creates an inference that negligence caused the injury. e.g. An expert general surgeon testifies that he has performed over one thousand appendectomies (removal of the appendix) and has never caused injury to a patient's liver. He also does not know of any of his surgeon colleagues having inflicted injury to a patient's liver during an appendectomy. This testimony would create an inference that injuring the liver in the course of an appendectomy is negligence.
The second element is discussed further in the section below. The third element requires the absence of contributory negligence from the plaintiff. The fourth element emphasizes that defendant may defeat a res ipsa loquitur claim by producing evidence of a non-negligent scenario that would completely explain plaintiff's injury and negate all possible inferences that negligence could have occurred.
The exclusive control requirement.
The common law traditionally required that "the instrumentality or agent which caused the accident was under the exclusive control of the defendant." See e.g., Eaton v. Eaton, 575 A2d 858 (NJ 1990). However, the Second and Third versions of the Restatement of Torts eliminated this strict requirement, because it can be difficult to prove "exclusive control." Accordingly, this element has largely given way in modern cases to a less rigid formulation: that the evidence eliminates, to a sufficient degree, other responsible causes (including the conduct of the plaintiff and third parties). For example, in New York State, the defendant's exclusivity of control must be such that the likelihood of injury was, more likely than not, the result of the defendant's negligence. The likelihood of other possibilities do not need to be eliminated altogether but they must be so reduced that the greater probability lies with the defendant. 
For a fictitious example of the exclusive control rule:
In some cases a closed group of people may be held in breach of a duty of care under the rule of "res ipsa loquitur". In Ybarra v. Spangard, a patient undergoing surgery experienced back complications as a result of the surgery, but it could not be determined exactly which member of the surgical team had breached his or her duty, and so it was held that they had all breached, because it was certain that at least one of them was the only person who was in exclusive control of the instrumentality of harm. 
In jurisdictions that employ this less rigid formulation of exclusive control, this element subsumes the element that the plaintiff did not contribute to his injury. In modern case law, contributory negligence is compared to the injury caused by the other. For example, if the negligence of the other is 95% of the cause of the plaintiff's injury, and the plaintiff is 5% responsible, then the plaintiff's slight fault cannot negate the negligence of the other. This new type of split liability is commonly called "comparative negligence".
Typical in medical malpractice.
"Res ipsa loquitur" often arises in the "scalpel left behind" variety of case. For example, a person goes to a doctor with abdominal pains after having his appendix removed. X-rays show the patient has a metal object the size and shape of a scalpel in his abdomen. It requires no further explanation to show the surgeon who removed the appendix was negligent, as there is no legitimate reason for a doctor to leave a scalpel in a body at the end of an appendectomy.
Contrast to "prima facie".
"Res ipsa loquitur" is often confused with "prima facie" ("at first sight"), the common law doctrine that a party must show some minimum amount of evidence before a trial is worthwhile.
The difference between the two is that "prima facie" is a term meaning there is enough evidence for there to be a case to answer. "Res ipsa loquitur" means that because the facts are so obvious, a party need not explain any more. For example: "There is a "prima facie" case that the defendant is liable. They controlled the pump. The pump was left on and flooded the plaintiff's house. The plaintiff was away and had left the house in the control of the defendant. "Res ipsa loquitur"."
Examples by jurisdictions.
Canada.
In Canada the doctrine of "res ipsa loquitur" has been largely overturned by the Supreme Court. In case of "Fontaine v. British Columbia (Official Administrator)" the Court rejected the use of "res ipsa loquitur" and instead proposed the rule that once the plaintiff has proven that the harm was under exclusive control of the defendant and that they were not contributorily negligent a tactical burden is placed on the defendant in which the judge has the discretion to infer negligence unless the defendant can produce evidence to the contrary.
Hong Kong.
Hong Kong is one of the common law jurisdictions that use the doctrine of "res ipsa loquitur".
Some lawyers prefer to avoid the expression "res ipsa loquitur" (for example, Hobhouse LJ in "Radcliff v. Plymouth"). But other lawyers (and judges too) still find the expression a convenient one (for example, see the judgement of Mr Justice Bokhary, a Permanent Judge of the Court of Final Appeal of Hong Kong, in "Sanfield Building Contractors Ltd v. Li Kai Cheong").
The expression "res ipsa loquitur" is not a doctrine but a “mode of inferential reasoning” and applies only to accidents of "unknown cause". "Res ipsa loquitur" comes into play where an accident of unknown cause is one that would not normally happen without negligence on the part of the defendant in control of the object or activity which injured the plaintiff or damaged his property. In such a situation the court is able to infer negligence on the defendant's part unless he offers an acceptable explanation consistent with his having taken reasonable care.
South Africa.
In South African law (which is modelled on Roman Dutch Law), there is no doctrine of "res ipsa loquitur", although the phrase is used regularly to mean the "facts speak for themselves." "Res ipsa loquitur" does not shift any burden of proof or onus from one party to the other. The phrase is merely a handy phrase used by lawyers.
United Kingdom.
The doctrine exists in both English law and Scots law.
England and Wales.
In English tort law, the effect of "res ipsa loquitur" is a strong inference in favour of the claimant that negligence has taken place. It does not however fully reverse the burden of proof ("Ng Chun Pui v. Li Chuen Tat", 1988).
The requirement of control is important in English law. This requirement was not satisfied in "Easson v. LNE Ry" [1944] 2 KB 421, where a small child fell off a train several miles after it had left the station. It was considered that the door of the train was not sufficiently under control of the railway company after the train started moving and could have been opened by somebody for whom the company was not responsible. This case was distinguished from the earlier "Gee v. Metropolitan Ry" where the plaintiff fell from the train immediately after it left the station, when the door through which he fell could still be considered to be fully controlled by the railway company.
The requirement that the exact cause of the accident must be unknown is illustrated by the case of "Barkway v. South Wales Transport". In this case a bus veered across the road and it was known that the accident was caused by a flat tire. In this case, the plaintiff could not be assisted by "res ipsa loquitur" and had to go on to prove that the flat tire was caused by the transport company's negligence.
Scotland.
The doctrine exists in the Scots law of "delict". The leading case is that of "Scott v London & Catherine Dock Co". This case laid down 3 requirements for the doctrine to apply:
In "Scott", the court held that sacks of sugar do not fall out of warehouses and crush passers-by without somebody having been negligent along the way.
Recent examples in Scotland are "McDyer v Celtic Football Club" and "McQueen v The Glasgow Garden Festival 1988 Ltd".
United States.
Under United States common law, "res ipsa loquitur" has the following requirements:
Most American courts recognize "res ipsa loquitur". The Restatement (Second) of Torts, § 328D describes a two step process for establishing "res ipsa loquitur". The first step is whether the accident is the kind usually be caused by negligence, and the second is whether or not the defendant had exclusive control over the instrumentality that caused the accident. If found, "res ipsa loquitur" creates an inference of negligence, although in most cases it does not necessarily result in a directed verdict. The Restatement (Third) of Torts, § 17, adopts a similar test, although it eschews the 'exclusive control' element.
The doctrine was not initially welcome in medical malpractice cases. In "Gray v. Wright", a seven-inch hemostat was left in Mrs. Gray during gall bladder surgery in June, 1947, and despite her chronic complaints about stomach pain over the years, the device was not found until an X-ray in March, 1953, when it was removed. Her $12,000 award was reversed by the Supreme Court of West Virginia because she was outside the statutes of limitation when she filed and could not prove that the doctor concealed knowledge of his error. This "guilty knowledge" requirement disappeared over the years, and the "discovery rule" by which statutes of limitation run from the date of discovery of the wrongdoing rather than the date of the occurrence has become the rule in most states, allowing "res ipsa loquitur" to take its rightful place.
Forty years later, leaving a medical device in a patient was medical malpractice, provable without expert testimony, in almost every jurisdiction. Virginia has limited the rule. "In Virginia the doctrine, if not entirely abolished, has been limited and restricted to a very material extent." It may be utilized only when the circumstances of the incident, without further proof, are such that, in the ordinary course of events, the incident could not have happened except on the theory of negligence..."
A contention of "res ipsa loquitur" commonly is made in cases of commercial airplane accidents. It was part of the commentary in a train collision in California in 2008: "If two trains are in the same place at the same time, someone was negligent."
In some states, the doctrine of "res ipsa loquitur" is also used as a method of proving the intent or "mens rea" element of the inchoate crime of attempt. Under the Model Penal Code, "the behavior in question is thought to corroborate the defendant's criminal purpose," for example:
Possession of materials to be employed in the commission of the crime, which are specifically designed for such unlawful use or which serve no lawful purpose of the actor under the circumstances—Model Penal Code

</doc>
