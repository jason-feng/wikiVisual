<doc id="25715" url="http://en.wikipedia.org/wiki?curid=25715" title="Rail transport">
Rail transport

Rail transport is a means of conveyance of passengers and goods, by way of wheeled vehicles running on rails. It is also commonly referred to as train transport. In contrast to road transport, where vehicles merely run on a prepared surface, rail vehicles are also directionally guided by the tracks on which they run. Track usually consists of steel rails installed on ties (sleepers) and ballast, on which the rolling stock, usually fitted with metal wheels, moves. However, other variations are also possible, such as slab track where the rails are fastened to a concrete foundation resting on a prepared subsurface.
Rolling stock in railway transport systems generally has lower frictional resistance when compared with highway vehicles and the passenger and freight cars (carriages and wagons) can be coupled into longer trains. The operation is carried out by a railway company, providing transport between train stations or freight customer facilities. Power is provided by locomotives which either draw electrical power from a railway electrification system or produce their own power, usually by diesel engines. Most tracks are accompanied by a signalling system. Railways are a safe land transport system when compared to other forms of transport. Railway transport is capable of high levels of passenger and cargo utilization and energy efficiency, but is often less flexible and more capital-intensive than highway transport is, when lower traffic levels are considered.
The oldest, man-hauled railways date back to the 6th century B.C, with Periander, one of the Seven Sages of Greece, credited with its invention. Rail transport blossomed after the British development of the steam locomotive as a viable source of the power in the 18th and 19th centuries. With steam engines, it was possible to construct mainline railways, which were a key component of the industrial revolution. Also, railways reduced the costs of shipping, and allowed for fewer lost goods, compared with shipping, which faced occasional sinking of ships. The change from canals to railways allowed for "national markets" in which prices varied very little from city to city. Studies have shown that the invention and development of the railway in Europe was one of the most important technological inventions of the late 19th century for the United States, without which, GDP would have been lower by 7.0% in 1890.
In the 1880s, electrified trains were introduced, and also the first tramways and rapid transit systems came into being. Starting during the 1940s, the non-electrified railways in most countries had their steam locomotives replaced by diesel-electric locomotives, with the process being almost complete by 2000. During the 1960s, electrified high-speed railway systems were introduced in Japan and later in some other countries. Other forms of guided ground transport outside the traditional railway definitions, such as monorail or maglev, have been tried but have seen limited use.
History.
The history of the growth, decline and restoration to use of rail transport can be divided up into several discrete periods defined by the principal means of motive power used.
Pre-steam.
The earliest evidence of a railway was a 6 km Diolkos wagonway, which transported boats across the Corinth isthmus in Greece during the 6th century BC. Trucks pushed by slaves ran in grooves in limestone, which provided the track element. The Diolkos operated for over 600 years.
Railways began reappearing in Europe after the Dark Ages. The earliest known record of a railway in Europe from this period is a stained-glass window in the Minster of Freiburg im Breisgau in Germany, dating from around 1350. In 1515, Cardinal Matthäus Lang wrote a description of the Reisszug, a funicular railway at the Hohensalzburg Castle in Austria. The line originally used wooden rails and a hemp haulage rope, and was operated by human or animal power. The line still exists, albeit in updated form, and is one of the oldest railways still to operate.
By 1550, narrow gauge railways with wooden rails were common in mines in Europe. By the 17th century, wooden wagonways were common in England and Wales for transporting coal from mines to canal wharfs for transshipment to boats. The world's oldest working railway, built in 1758, is the Middleton Railway in Leeds. In 1764, the first gravity railroad in the United States was built in Lewiston, New York. The first permanent tramway was the Leiper Railroad in 1810.
The first iron plate railway, made with wrought iron plates on top of wooden rails, came into use in 1768. This allowed a variation of gauge to be used. At first only balloon loops could be used for turning, but later, movable points were taken into use that allowed for switching. From the 1790s, iron edge rails began to appear in Great Britain. In 1803, William Jessop opened the Surrey Iron Railway in south London, arguably the world's first horse-drawn public railway. The invention of the wrought iron rail by John Birkinshaw in 1820 allowed the short, brittle, and often uneven, cast iron rails to be extended to 15 ft lengths. These were succeeded by steel in 1857.
Age of steam.
The development of the steam engine during the Industrial Revolution in Great Britain, initially for pumping water, spurred ideas for mobile steam locomotives that could haul heavy weights on tracks. James Watt's patented steam engines of 1769 (patent revised in 1782) were heavy low-pressure engines which were not suitable for use in locomotives. However, in 1804, using high-pressure steam, Richard Trevithick demonstrated the first locomotive-hauled train at Merthyr Tydfil, in South Wales. Accompanied with Andrew Vivian, it ran with mixed success, breaking some of the brittle cast-iron plates. Two years later, the first passenger horse-drawn railway was opened nearby between Swansea and Mumbles.
Earliest British steam railways.
In 1811, John Blenkinsop designed the first successful and practical railway locomotive—a rack railway worked by a steam locomotive between Middleton Colliery and Leeds on the Middleton Railway. His first locomotive, called "Salamanca", was built the following year.:20 In 1825, George Stephenson built the "Locomotion" for the Stockton and Darlington Railway, north east England, which was the first public steam railway in the world. In 1829, he built the "Rocket", which was entered in and won the Rainhill Trials. This success led to Stephenson establishing his company as the pre-eminent builder of steam locomotives for Railways in Great Britain and Ireland, the United States, and much of Europe.:24–30
In 1830, the first intercity route, the Liverpool and Manchester Railway, was opened. The gauge was that used for the early wagon-ways, which had been adopted for the Stockton and Darlington Railway, with a width which became known as the international standard gauge, still used by about 60% of the world's railways. This spurred the spread of rail transport outside the British Isles.
By the early 1850s, Great Britain had over 7000 mi of railway, a stunning achievement given that only twenty years had elapsed since the opening of the Liverpool and Manchester Railway.
Early railroads in the US.
Railroads (as they are known in the US) were built on a far larger scale than those in Continental Europe, both in terms of the distances covered and also in the loading gauge adopted, which allowed for heavier locomotives and double-deck trains. The railroad era in the United States began in 1830 when Peter Cooper's locomotive, "Tom Thumb", first steamed along 13 mi of Baltimore and Ohio railroad track. In 1833, the nation's second railroad ran 136 mi from Charleston to Hamburg in South Carolina. Not until the 1850s, though, did railroads offer long distance service at reasonable rates. A journey from Philadelphia to Charleston involved eight different gauges, which meant that passengers and freight had to change trains seven times. Only at places like Bowling Green, Kentucky, were the railroads connected to one another.
The Baltimore and Ohio Railroad that opened in 1830 was the first to evolve from a single line to a network in the United States. By 1831, a steam railway connected Albany and Schenectady, New York, a distance of 16 mi, which was covered in 40 minutes.
The years between 1850 and 1890 saw phenomenal growth in the US railroad system, which at its peak constituted one third of the world's total mileage. Although the American Civil War placed a temporary halt to major new developments, the conflict did demonstrate the enormous strategic importance of railways at times of war. After the war, major developments include the first elevated railway built in New York in 1867 as well as the symbolically important first transcontinental railroad completed in 1869.
Electrification and dieselisation.
Experiments with electrical railways were started by Robert Davidson in 1838. He completed a battery-powered carriage capable of 6.4 km/h. The Gross-Lichterfelde Tramway was the first to use electricity fed to the trains en route, when it opened in 1881. Overhead wires were taken into use in the Mödling and Hinterbrühl Tram in Austria in October 1883. At first, this was taken into use on tramways that, until then, had been horse-drawn tramcars. The first conventional completely electrified railway mainline was the 106 km Valtellina line in Italy that was opened on 4 September 1902.
During the 1890s, many large cities, such as London, Paris and New York City used the new technology to build rapid transit for urban commuting. In smaller cities, tramways became common and were often the only mode of public transport until the introduction of buses in the 1920s. In North America, interurbans became a common mode to reach suburban areas. At first, all electric railways used direct current but, in 1904, the Stubaital Line in Austria opened with alternating current.
Steam locomotives require large pools of labour to clean, load, maintain and run. After World War II, dramatically increased labour costs in developed countries made steam an increasingly costly form of motive power. At the same time, the war had forced improvements in internal combustion engine technology that made diesel locomotives cheaper and more powerful. This caused many railway companies to initiate programmes to convert all unelectrified sections from steam to diesel locomotion.
Following the large-scale construction of motorways after the war, rail transport became less popular for commuting and air transport started taking large market shares from long-haul passenger trains. Most tramways were either replaced by rapid transit or buses, while high transshipment costs caused short-haul freight trains to become uncompetitive. The 1973 oil crisis led to a change of mind set and most tram systems that had survived into the 1970s remain today. At the same time, containerization allowed freight trains to become more competitive and participate in intermodal freight transport. With the 1964 introduction of the Shinkansen high-speed rail in Japan, trains could again have a dominant position on intercity travel. During the 1970s, the introduction of automated rapid transit systems allowed cheaper operation. The 1990s saw an increased focus on accessibility and low-floor trains. Many tramways have been upgraded to light rail and many cities that closed their old tramways have reopened new light railway systems.
Innovations.
Many benchmarks in equipment and infrastructure led to the growing use of railways. Some innovative features taking place in the 19th and 20th centuries included wood cars replaced with all-steel cars, which provided better safety and maintenance; iron rails replaced with steel rails, which provided higher speed and capacity with lower weight and cost; stove-heated cars to steam-heating cars, piped from locomotive; gas lighting to electric lighting, with use of battery/alternator unit beneath the car; development of air-conditioning with additional underbody equipment and ice compartment. Some innovative rolling stock included the lightweight, diesel-powered streamliner, which was a modernistic, aerodynamically styled train with flowing contours; then came the ultra-lightweight car with internal combustion engine in each train's power car; others included the dome car, turbined-powered trains, bi-level rolling stock, and the high-tech/high-speed electric trains.
Even more, in the first half of the 20th century, infrastructure elements adopted technological changes including the continuously welded rail that was 1/4 mi long; concrete tie usage; double tracking major lines; intermodal terminal and handling technology; advances in diesel-electric propulsion to include AC traction systems and propulsion braking systems; and just-in-time inventory control. Beyond technology, even management of systems saw improvements with the adoption of environmental impact concerns; heightened concern of employee and public safety; introduction of urban area rail networks and public agencies to manage them; and downsizing of industry employment with greater use of contractors and consultants.
Trains.
A train is a connected series of rail vehicles that move along the track. Propulsion for the train is provided by a separate locomotive or from individual motors in self-propelled multiple units. Most trains carry a revenue load, although non-revenue cars exist for the railway's own use, such as for maintenance-of-way purposes. The engine driver (engineer in North America) controls the locomotive or other power cars, although people movers and some rapid transits are under automatic control.
Haulage.
Traditionally, trains are pulled using a locomotive. This involves one or more powered vehicles being located at the front of the train, providing sufficient tractive force to haul the weight of the full train. This arrangement remains dominant for freight trains and is often used for passenger trains. A push-pull train has the end passenger car equipped with a driver's cab so that the engine driver can remotely control the locomotive. This allows one of the locomotive-hauled train's drawbacks to be removed, since the locomotive need not be moved to the front of the train each time the train changes direction. A railroad car is a vehicle used for the haulage of either passengers or freight.
A multiple unit has powered wheels throughout the whole train. These are used for rapid transit and tram systems, as well as many both short- and long-haul passenger trains. A railcar is a single, self-powered car, and may be electrically-propelled or powered by a diesel engine. Multiple units have a driver's cab at each end of the unit, and were developed following the ability to build electric motors and engines small enough to fit under the coach. There are only a few freight multiple units, most of which are high-speed post trains.
Motive power.
Steam locomotives are locomotives with a steam engine that provides adhesion. Coal, petroleum, or wood is burned in a firebox, boiling water in the boiler to create pressurized steam. The steam travels through the smokebox before leaving via the chimney or smoke stack. In the process, it powers a piston that transmits power directly through a connecting rod (US: main rod) and a crankpin (US: wristpin) on the driving wheel (US main driver) or to a crank on a driving axle. Steam locomotives have been phased out in most parts of the world for economical and safety reasons, although many are preserved in working order by heritage railways.
Electric locomotives draw power from a stationary source via an overhead wire or third rail. Some also or instead use a battery. In locomotives that are powered by high voltage alternating current, a transformer in the locomotive converts the high voltage, low current power to low voltage, high current used in the traction motors that power the wheels. Modern locomotives may use three-phase AC induction motors or direct current motors. Under certain conditions, electric locomotives are the most powerful traction. They are also the cheapest to run and provide less noise and no local air pollution. However, they require high capital investments both for the overhead lines and the supporting infrastructure, as well as the generating station that is needed to produce electricity. Accordingly, electric traction is used on urban systems, lines with high traffic and for high-speed rail.
Diesel locomotives use a diesel engine as the prime mover. The energy transmission may be either diesel-electric, diesel-mechanical or diesel-hydraulic but diesel-electric is dominant. Electro-diesel locomotives are built to run as diesel-electric on unelectrified sections and as electric locomotives on electrified sections.
Alternative methods of motive power include magnetic levitation, horse-drawn, cable, gravity, pneumatics and gas turbine.
Passenger trains.
A passenger train travels between stations where passengers may embark and disembark. The oversight of the train is the duty of a guard/train manager/conductor. Passenger trains are part of public transport and often make up the stem of the service, with buses feeding to stations. Passenger trains provide long-distance intercity travel, daily commuter trips, or local urban transit services. They even include a diversity of vehicles, operating speeds, right-of-way requirements, and service frequency. Passenger trains usually can be divided into two operations: intercity railway and intracity transit. Whereas as intercity railway involve higher speeds, longer routes, and lower frequency (usually scheduled), intracity transit involves lower speeds, shorter routes, and higher frequency (especially during peak hours).
Intercity trains are long-haul trains that operate with few stops between cities. Trains typically have amenities such as a dining car. Some lines also provide over-night services with sleeping cars. Some long-haul trains have been given a specific name. Regional trains are medium distance trains that connect cities with outlying, surrounding areas, or provide a regional service, making more stops and having lower speeds. Commuter trains serve suburbs of urban areas, providing a daily commuting service. Airport rail links provide quick access from city centres to airports.
High-speed rail are special inter-city trains that operate at much higher speeds than conventional railways, the limit being regarded at 200 to. High-speed trains are used mostly for long-haul service and most systems are in Western Europe and East Asia. The speed record is 574.8 km/h, set by a modified French TGV. Magnetic levitation trains such as the Shanghai airport train use under-riding magnets which attract themselves upward towards the underside of a guideway and this line has achieved somewhat higher peak speeds in day-to-day operation than conventional high-speed railways, although only over short distances. Due to their heightened speeds, route alignments for high-speed rail tend to have shallower grades and broader curves than conventional railways.
Their high kinetic energy translates to higher horsepower-to-ton ratios (e.g. 20 hp/ST); this allows trains to accelerate and maintain higher speeds and negotiate steep grades as momentum builds up and recovered in downgrades (reducing cut, fill, and tunneling requirements). Since lateral forces act on curves, curvatures are designed with the highest possible radius. All these features are dramatically different from freight operations, thus justifying exclusive high-speed rail lines if it is economically feasible. 
Higher-speed rail services are intercity rail services that have top speeds higher than conventional intercity trains but the speeds are not as high as those in the high-speed rail services. These services are provided after improvements to the conventional rail infrastructure in order to support trains that can operate safely at higher speeds.
High speed railway (High Speed Railroad/Railway & Railroad/Railway high speed) commonly referred to as the High-speed Rail (abbre:HR[hairei]), is a kind of operating speed of at least eighty percent more than in the whole operation process of distance over 200 km/h (>200 km/h&130MPH) railway. As of 2014, the operating speed of High-speed Rail systems in the world are running about all set at 300 kilometers per hour, a few systems have relatively high speed. For High-speed Rail speed can be considered: 1 is High-speed Rail lines after transformation from common railroad, such High-speed Rail maximum speed of approximately 250 km/h (250 km/h&150MPH). 2 is the construction standards for the ballasted bed (stone ballast) high-speed railway was built, such High-speed Rail maximum speed of approximately 320 km / h (320 km/h&200MPH). 3 is the construction of ballastless track technology standards (that is popular opinion of to lay the rail on the reinforced concrete pavement directly ) was built, to China High-speed Rail represented such High-speed Rail mostly built in high-speed bridge (overhead bridge), so called air railway (air railroad), such High-speed Rail maximum speed of approximately 400 km/h (400 km/h&250MPH). (the above rate refers to the commercial operation speed)
High speed railway is a high-tech integrated system, including 6 aspects: infrastructure, 1 parts, also including the station construction. The 2 part: high speed train. The 3 part: electricity and telecommunications, including electrical and contact network, safe and reliable communication, signal, dispatching center console. 4: safety control part, including meteorological conditions monitoring, line monitoring, automatic train control and retrieval, road, vehicle automatic monitoring and response system. 5: maintenance and maintenance section, including the line maintenance, high speed train (highspeed divided dynamic train abbre:HDDT) maintenance and repair, involving a variety of instrumentation maintenance and repair, and maintenance of equipment and monitoring instruments. 6: the other parts, including passenger safety assessment sent, High-speed Rail prospective study, the new High-speed Rail line and the new high speed train or high speed train test, High-speed Rail financial, High-speed Rail operational data storage and analysis, etc.
Rapid transit is an intracity system built in large cities and has the highest capacity of any passenger transport system. It is usually grade-separated and commonly built underground or elevated. At street level, smaller trams can be used. Light rails are upgraded trams that have step-free access, their own right-of-way and sometimes sections underground. Monorail systems are elevated, medium-capacity systems. A people mover is a driverless, grade-separated train that serves only a few stations, as a shuttle. Due to the lack of uniformity of rapid transit systems, route alignment varies, with diverse rights-of-way (private land, side of road, street median) and geometric characteristics (sharp or broad curves, steep or gentle grades). For instance, the Chicago 'L' trains are designed with extremely short cars to negotiate the sharp curves in the Loop. New Jersey's PATH has similar-sized cars to accommodate curves in the trans-Hudson tunnels. San Francisco's BART operates large cars on its well-engineered routes.
Freight train.
A freight train hauls cargo using freight cars specialized for the type of goods. Freight trains are very efficient, with economy of scale and high energy efficiency. However, their use can be reduced by lack of flexibility, if there is need of transshipment at both ends of the trip due to lack of tracks to the points of pick-up and delivery. Authorities often encourage the use of cargo rail transport due to its environmental profile.
Container trains have become the dominant type in the US for non-bulk haulage. Containers can easily be transshipped to other modes, such as ships and trucks, using cranes. This has succeeded the boxcar (wagon-load), where the cargo had to be loaded and unloaded into the train manually. The intermodal containerization of cargo has revolutionized the supply chain logistics industry, reducing ship costs significantly. In Europe, the sliding wall wagon has largely superseded the ordinary covered wagons. Other types of cars include refrigerator cars, stock cars for livestock and autoracks for road vehicles. When rail is combined with road transport, a roadrailer will allow trailers to be driven onto the train, allowing for easy transition between road and rail.
Bulk handling represents a key advantage for rail transport. Low or even zero transshipment costs combined with energy efficiency and low inventory costs allow trains to handle bulk much cheaper than by road. Typical bulk cargo includes coal, ore, grains and liquids. Bulk is transported in open-topped cars, hopper cars and tank cars.
Infrastructure.
Right of way.
Railway tracks are laid upon land owned or leased by the railway company. Owing to the desirability of maintaining modest grades, rails will often be laid in circuitous routes in hilly or mountainous terrain. Route length and grade requirements can be reduced by the use of alternating cuttings, bridges and tunnels—all of which can greatly increase the capital expenditures required to develop a right of way, while significantly reducing operating costs and allowing higher speeds on longer radius curves. In densely urbanized areas, railways are sometimes laid in tunnels to minimize the effects on existing properties..
Trackage.
Track consists of two parallel steel rails, anchored perpendicular to members called ties (sleepers) of timber, concrete, steel, or plastic to maintain a consistent distance apart, or rail gauge. Rail gauges are usually categorised as Standard gauge () used on approximately 60% of the world's existing railway lines, Broad gauge and Narrow gauge. In addition to the rail gauge, the tracks will be laid to conform with a Loading gauge which defines the maximum height and width for railway vehicles and their loads to ensure safe passage through bridges, tunnels and other structures.
The track guides the conical, flanged wheels, keeping the cars on the track without active steering and therefore allowing trains to be much longer than road vehicles. The rails and ties are usually placed on a foundation made of compressed earth on top of which is placed a bed of ballast to distribute the load from the ties and to prevent the track from buckling as the ground settles over time under the weight of the vehicles passing above.
The ballast also serves as a means of drainage. Some more modern track in special areas is attached by direct fixation without ballast. Track may be prefabricated or assembled in place. By welding rails together to form lengths of continuous welded rail, additional wear and tear on rolling stock caused by the small surface gap at the joints between rails can be counteracted; this also makes for a quieter ride (passenger trains).
On curves the outer rail may be at a higher level than the inner rail. This is called superelevation or cant. This reduces the forces tending to displace the track and makes for a more comfortable ride for standing livestock and standing or seated passengers. A given amount of superelevation will be the most effective over a limited range of speeds.
Turnouts, also known as points and switches, are the means of directing a train onto a diverging section of track. Laid similar to normal track, a point typically consists of a frog (common crossing), check rails and two switch rails. The switch rails may be moved left or right, under the control of the signalling system, to determine which path the train will follow.
Spikes in wooden ties can loosen over time, but split and rotten ties may be individually replaced with new wooden ties or concrete substitutes. Concrete ties can also develop cracks or splits, and can also be replaced individually. Should the rails settle due to soil subsidence, they can be lifted by specialized machinery and additional ballast tamped under the ties to level the rails.
Periodically, ballast must be removed and replaced with clean ballast to ensure adequate drainage. Culverts and other passages for water must be kept clear lest water is impounded by the trackbed, causing landslips. Where trackbeds are placed along rivers, additional protection is usually placed to prevent streambank erosion during times of high water. Bridges require inspection and maintenance, since they are subject to large surges of stress in a short period of time when a heavy train crosses.
Train inspection systems.
The inspection of railway equipment is essential for the safe movement of trains. Many types of defect detectors are in use on the world's railroads. These devices utilize technologies that vary from a simplistic paddle and switch to infrared and laser scanning, and even ultrasonic audio analysis. Their use has avoided many rail accidents over the 70 years they have been used.
Signalling.
Railway signalling is a system used to control railway traffic safely to prevent trains from colliding. Being guided by fixed rails with low friction, trains are uniquely susceptible to collision since they frequently operate at speeds that do not enable them to stop quickly or within the driver's sighting distance. Most forms of train control involve movement authority being passed from those responsible for each section of a rail network to the train crew. Not all methods require the use of signals, and some systems are specific to single track railways.
The signalling process is traditionally carried out in a signal box, a small building that houses the lever frame required for the signalman to operate switches and signal equipment. These are placed at various intervals along the route of a railway, controlling specified sections of track. More recent technological developments have made such operational doctrine superfluous, with the centralization of signalling operations to regional control rooms. This has been facilitated by the increased use of computers, allowing vast sections of track to be monitored from a single location. The common method of block signalling divides the track into zones guarded by combinations of block signals, operating rules, and automatic-control devices so that only one train may be in a block at any time.
Electrification.
The electrification system provides electrical energy to the trains, so they can operate without a prime mover on board. This allows lower operating costs, but requires large capital investments along the lines. Mainline and tram systems normally have overhead wires, which hang from poles along the line. Grade-separated rapid transit sometimes use a ground third rail.
Power may be fed as direct or alternating current. The most common DC voltages are 600 and 750 V for tram and rapid transit systems, and 1,500  and 3,000 V for mainlines. The two dominant AC systems are 15 kV AC and 25 kV AC.
Stations.
A railway station serves as an area where passengers can board and alight from trains. A goods station is a yard which is exclusively used for loading and unloading cargo. Large passenger stations have at least one building providing conveniences for passengers, such as purchasing tickets and food. Smaller stations typically only consist of a platform. Early stations were sometimes built with both passenger and goods facilities.
Platforms are used to allow easy access to the trains, and are connected to each other via underpasses, footbridges and level crossings. Some large stations are built as culs-de-sac, with trains only operating out from one direction. Smaller stations normally serve local residential areas, and may have connection to feeder bus services. Large stations, in particular central stations, serve as the main public transport hub for the city, and have transfer available between rail services, and to rapid transit, tram or bus services.
Operations.
Ownership.
Since the 1980s, there has been an increasing trend to split up railway companies, with companies owning the rolling stock separated from those owning the infrastructure. This is particularly true in Europe, where this arrangement is required by the European Union. This has allowed open access by any train operator to any portion of the European railway network. This is different in the UK however, where the railway track is state owned, with a public controlled body (Network Rail) running, maintaining and developing the track.
In the U.S., virtually all rail networks and infrastructure outside the Northeast Corridor are privately owned by freight lines. Passenger lines, primarily Amtrak, operate as tenants on the freight lines. Consequently, operations must be closely synchronized and coordinated between freight and passenger railroads, with passenger trains often being dispatched by the host freight railroad. Due to this shared system, both are regulated by the Federal Railroad Administration (FRA) and may follow the AREMA recommended practices for track work and AAR standards for vehicles.
Financing.
The main source of income for railway companies is from ticket revenue (for passenger transport) and shipment fees for cargo. Discounts and monthly passes are sometimes available for frequent travellers (e.g. season ticket and rail pass). Freight revenue may be sold per container slot or for a whole train. Sometimes, the shipper owns the cars and only rents the haulage. For passenger transport, advertisement income can be significant.
Governments may choose to give subsidies to rail operation, since rail transport has fewer externalities than other dominant modes of transport. If the railway company is state-owned, the state may simply provide direct subsidies in exchange for increased production. If operations have been privatized, several options are available. Some countries have a system where the infrastructure is owned by a government agency or company—with open access to the tracks for any company that meets safety requirements. In such cases, the state may choose to provide the tracks free of charge, or for a fee that does not cover all costs. This is seen as analogous to the government providing free access to roads. For passenger operations, a direct subsidy may be paid to a public-owned operator, or public service obligation tender may be helt, and a time-limited contract awarded to the lowest bidder.
Amtrak, the US passenger rail service, and Canada's Via Rail are private railroad companies chartered by their respective national governments. As private passenger services declined because of competition from automobiles and airlines, they became shareholders of Amtrak either with a cash entrance fee or relinquishing their locomotives and rolling stock. The government subsidizes Amtrak by supplying start-up capital and making up for losses at the end of the fiscal year.
Safety.
Trains can travel at very high speed, but they are heavy, are unable to deviate from the track and require a great distance to stop. Possible accidents include derailment (jumping the track), a collision with another train or collision with automobiles, other vehicles or pedestrians at level crossings. The last accounts for the majority of rail accidents and casualties. The most important safety measures to prevent accidents are strict operating rules, e.g. railway signalling and gates or grade separation at crossings. Train whistles, bells or horns warn of the presence of a train, while trackside signals maintain the distances between trains.
An important element in the safety of many high-speed inter-city networks such as Japan's Shinkansen is the fact that trains only run on dedicated railway lines, without level crossings. This effectively eliminates the potential for collision with automobiles, other vehicles or pedestrians, vastly reduces the likelihood of collision with other trains and helps ensure services remain timely.
Maintenance.
As in any infrastructure asset, railways must keep up with periodic inspection and maintenance in order to minimize effect of infrastructure failures that can disrupt freight revenue operations and passenger services. Because passengers are considered the most "crucial cargo" and usually operate at higher speeds, steeper grades, and higher capacity/frequency, their lines are especially important. Inspection practices include track geometry cars or walking inspection. Curve maintenance especially for transit services includes gauging, fastener tightening, and rail replacement.
Rail corrugation is a common issue with transit systems due to the high number of light-axle, wheel passages which result in grinding of the wheel/rail interface. Since maintenance may overlap with operations, maintenance windows (nighttime hours, off-peak hours, altering train schedules or routes) must be closely followed. In addition, passenger safety during maintenance work (inter-track fencing, proper storage of materials, track work notices, hazards of equipment near states) must be regarded at all times. At times, maintenance access problems can emerge due to tunnels, elevated structures, and congested cityscapes. Here, specialized equipment or smaller versions of conventional maintenance gear are used.
Unlike highways or road networks where capacity is disaggregated into unlinked trips over individual route segments, railway capacity is fundamentally considered a network system. As a result, many components are causes and effects of system disruptions. Maintenance must acknowledge the vast array of a route's performance (type of train service, origination/destination, seasonal impacts), line's capacity (length, terrain, number of tracks, types of train control), trains throughput (max speeds, acceleration/deceleration rates), and service features with shared passenger-freight tracks (sidings, terminal capacities, switching routes, and design type).
Social, economical, and energetic aspects.
Energy.
Rail transport is an energy-efficient but capital-intensive, means of mechanized land transport. The tracks provide smooth and hard surfaces on which the wheels of the train can roll with a minimum of friction. Moving a vehicle on and/or through a medium (land, sea, or air) requires overcoming resistance to motion. A land vehicle's total resistance (in pounds or Newtons) is a quadratic function of the vehicle's speed:
where:
Essentially, resistance differs between vehicle's contact point and surface of roadway. Metal wheels on metal rails have a significant advantage of overcoming resistance compared to rubber-tired wheels on any road surface (railway – 0.001g at 10 mph and 0.024g at 60 mph; truck – 0.009g at 10 mph and 0.090 at 60 mph). In terms of cargo capacity combining speed and size being moved in a day:
In terms of the horsepower to weight ratio, a slow-moving barge requires 0.2 hp/ST, a railway and pipeline requires 2.5 hp/ST, and truck requires 10 hp/ST. However, at higher speeds, a railway overcomes the barge and proves most economical.
As an example, a typical modern wagon can hold up to 113 t of freight on two four-wheel bogies. The track distributes the weight of the train evenly, allowing significantly greater loads per axle and wheel than in road transport, leading to less wear and tear on the permanent way. This can save energy compared with other forms of transport, such as road transport, which depends on the friction between rubber tires and the road. Trains have a small frontal area in relation to the load they are carrying, which reduces air resistance and thus energy usage.
In addition, the presence of track guiding the wheels allows for very long trains to be pulled by one or a few engines and driven by a single operator, even around curves, which allows for economies of scale in both manpower and energy use; by contrast, in road transport, more than two articulations causes fishtailing and makes the vehicle unsafe.
Energy efficiency.
Considering only the energy spent to move the means of transport, and using the example of the urban area of Lisbon, trains seem to be on average 20 times more efficient than automobiles for transportation of passengers, if we consider energy spent per passenger-distance with similar occupation ratios. Considering an automobile with a consumption of around 6 l/100 km of gasoline, the average car in Europe has an occupancy of around 1.2 passengers per automobile (occupation ratio around 24%) and that one litre of gasoline amounts to about 8.8 kWh, equating to an average of 441 Wh per passenger-km. This compares to a modern train with an average occupancy of 20% and a consumption of about 8.5 kWh/km, equating to 21.5 Wh per passenger-km, 20 times less than the automobile.
Usage.
Due to these benefits, rail transport is a major form of passenger and freight transport in many countries. It is ubiquitous in Europe, with an integrated network covering virtually the whole continent. In India, China, South Korea and Japan, many millions use trains as regular transport. In North America, freight rail transport is widespread and heavily used, but intercity passenger rail transport is relatively scarce outside the Northeast Corridor, due to increased preference of other modes, particularly automobiles and airplanes.
South Africa, northern Africa and Argentina have extensive rail networks, but some railways elsewhere in Africa and South America are isolated lines. Australia has a generally sparse network befitting its population density but has some areas with significant networks, especially in the southeast. In addition to the previously existing east-west transcontinental line in Australia, a line from north to south has been constructed. The highest railway in the world is the line to Lhasa, in Tibet, partly running over permafrost territory. Western Europe has the highest railway density in the world and many individual trains there operate through several countries despite technical and organizational differences in each national network.
Social and economic benefits.
Railways are central to the formation of modernity and ideas of progress. Railways contribute to social vibrancy and economic competitiveness by transporting multitudes of customers and workers to city centres and inner suburbs. Hong Kong has recognized rail as "the backbone of the public transit system" and as such developed their franchised bus system and road infrastructure in comprehensive alignment with their rail services. China's large cities such as Beijing, Shanghai, and Guangzhou recognize rail transit lines as the framework and bus lines as the main body to their metropolitan transportation systems. The Japanese Shinkansen was built to meet the growing traffic demand in the "heart of Japan's industry and economy" situated on the Tokyo-Kobe line.
 During much of the 20th Century, rail was an invaluable element of military mobilization, allowing for the quick and efficient transport of large numbers of reservists to their mustering-points, and infantry soldiers to the front lines. However, by the 21st Century, rail transport - limited to locations on the same continent, and vulnerable to air attack - had largely been displaced by the adoption of aerial transport.
Railways channel growth toward dense city agglomerations and along their arteries, as opposed to highway expansion, indicative of the U.S. transportation policy, which incents development of suburbs at the periphery, contributing to increased vehicle miles traveled, carbon emissions, development of greenfield spaces, and depletion of natural reserves. These arrangements revalue city spaces, local taxes, housing values, and promotion of mixed use development.
Modern rail as economic development indicator.
European development economists have argued that the existence of modern rail infrastructure is a significant indicator of a country's economic advancement: this perspective is illustrated notably through the Basic Rail Transportation Infrastructure Index (known as BRTI Index)

</doc>
<doc id="25716" url="http://en.wikipedia.org/wiki?curid=25716" title="Refreshable braille display">
Refreshable braille display

A refreshable braille display or braille terminal is an electro-mechanical device for displaying braille characters, usually by means of round-tipped pins raised through holes in a flat surface. Blind computer users who cannot use a computer monitor can use it to read text output. Speech synthesizers are also commonly used for the same task, and a blind user may switch between the two systems or use both at the same time depending on circumstances. Deafblind computer users may also use refreshable braille displays.
Mechanical details.
The base of a refreshable braille display is a pure braille terminal. There, the input is performed by two sets of three keys plus a space bar (as in the Perkins Brailler), while output is via a refreshable braille display consisting of a row of electromechanical character cells, each of which can raise or lower a combination of six (or in some cases, eight) round-tipped pins. Other variants exist that use a conventional QWERTY keyboard for input and braille pins for output, as well as input-only and output-only devices.
On some models the position of the cursor is represented by vibrating the dots, and some models have a switch associated with each cell to move the cursor to that cell directly.
The mechanism which raises the dots uses the piezo effect of some crystals, whereby they expand when a voltage is applied to them. Such a crystal is connected to a lever, which in turn raises the dot. There has to be a crystal for each dot of the display, i.e. eight per character.
Because of the complexity of producing a reliable display that will cope with daily wear and tear, these displays are expensive. Usually, only 40 or 80 braille cells are displayed. Models with between 18 and 40 cells exist in some notetaker devices.
Software.
The software that controls the display is called a screen reader. It gathers the content of the screen from the operating system, converts it into braille characters and sends it to the display. Screen readers for graphical operating systems are especially complex, because graphical elements like windows or slidebars have to be interpreted and described in text form. Modern operating systems usually have an Application Programming Interface to help screen readers obtain this information, such as UI Automation (UIA) for Microsoft Windows, VoiceOver for OS X and iOS, and AT-SPI for GNOME.

</doc>
<doc id="25717" url="http://en.wikipedia.org/wiki?curid=25717" title="Regular expression">
Regular expression

In theoretical computer science and formal language theory, a regular expression (abbreviated regex or regexp and sometimes called a rational expression) is a sequence of characters that forms a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. "find and replace"-like operations. The concept arose in the 1950s, when the American mathematician Stephen Kleene formalized the description of a "regular language", and came into common use with the Unix text processing utilities ed, an editor, and grep (global regular expression print), a filter.
Regular expressions are so useful in computing that the various systems to specify regular expressions have evolved to provide both a "basic" and "extended" standard for the grammar and syntax; "modern" regular expressions heavily augment the standard. Regular expression processors are found in several search engines, search and replace dialogs of several word processors and text editors, and in the command lines of , such as sed and AWK.
Many programming languages provide regular expression capabilities, some built-in, for example Perl, JavaScript, Ruby, AWK, and Tcl, and others via a standard library, for example .NET languages, Java, Python and C++ (since C++11).
Most other languages offer regular expressions via a library.
Patterns.
Each character in a regular expression is either understood to be a metacharacter with its special meaning, or a regular character with its literal meaning. Together, they can be used to identify textual material of a given pattern, or process a number of instances of it that can vary from a precise equality to a very general similarity of the pattern. The pattern sequence itself is an expression that is a statement in a language designed specifically to represent prescribed targets in the most concise and flexible way to direct the automation of text processing of general text files, specific textual forms, or of random input strings.
Expressive power and compactness.
The formal definition of regular expressions is purposely parsimonious and avoids defining the redundant quantifiers codice_1 and codice_2, which can be expressed as follows: codice_3 = codice_4, and codice_5 = codice_6. Sometimes the complement operator is added, to give a "generalized regular expression"; here "Rc" matches all strings over Σ* that do not match "R". In principle, the complement operator is redundant, as it can always be circumscribed by using the other operators. However, the process for computing such a representation is complex, and the result may require expressions of a size that is double exponentially larger.
Regular expressions in this sense can express the regular languages, exactly the class of languages accepted by deterministic finite automata. There is, however, a significant difference in compactness. Some classes of regular languages can only be described by deterministic finite automata whose size grows exponentially in the size of the shortest equivalent regular expressions. The standard example here is the languages
"Lk" consisting of all strings over the alphabet {"a","b"} whose "kth"-from-last letter equals "a". On one hand, a regular expression describing "L"4 is given by formula_1. Generalizing this pattern to "Lk" gives the expression
On the other hand, it is known that every deterministic finite automaton accepting the language "Lk" must have at least 2"k" states. Luckily, there is a simple mapping from regular expressions to the more general nondeterministic finite automata (NFAs) that does not lead to such a blowup in size; for this reason NFAs are often used as alternative representations of regular languages. NFAs are a simple variation of the type-3 grammars of the Chomsky hierarchy.
Finally, it is worth noting that many real-world "regular expression" engines implement features that cannot be described by the regular expressions in the sense of formal language theory; see below for more on this.
Deciding equivalence of regular expressions.
As seen in many of the examples above, there is more than one way to construct a regular expression to achieve the same results.
It is possible to write an algorithm that, for two given regular expressions, decides whether the described languages are equal; the algorithm reduces each expression to a minimal deterministic finite state machine, and determines whether they are isomorphic (equivalent).
The redundancy can be eliminated by using Kleene star and set union to find an interesting subset of regular expressions that is still fully expressive, but perhaps their use can be restricted. This is a surprisingly difficult problem. As simple as the regular expressions are, there is no method to systematically rewrite them to some normal form. The lack of axiom in the past led to the star height problem. In 1991, Dexter Kozen axiomatized regular expressions with Kleene algebra; see Kleene algebra#History for details.
Syntax.
A regexp "pattern" matches a target "string". The pattern is composed of a sequence of "atoms". An atom is a single point within the regexp pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using codice_7 as metacharacters. Metacharacters help form: "atoms"; "quantifiers" telling how many atoms (and whether it is a "greedy" quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and back references to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regular expression have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.
Depending on the regexp processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are "escaped", i.e. preceded by an escape sequence, in this case, the backslash codice_8. Modern and POSIX extended regular expressions use metacharacters more often than their literal meaning, so to avoid "backslash-osis" it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters codice_7 and codice_10 be primarily literal, and "escape" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are codice_11 and codice_8. The usual characters that become metacharacters when escaped are codice_13 and codice_14.
Delimiters.
When entering a regular expression in a programming language, they may be represented as a usual string literal, hence usually quoted; this is common in C, Java, and Python for instance, where the regular expression codice_15 is entered as codice_16. However, they are often written with slashes as delimiters, as in codice_17 for the regular expression codice_15. This originates in ed, where codice_19 is the editor command for searching, and an expression codice_17 can be used to specify a range of lines (matching the pattern), which can be combined with other commands on either side, most famously codice_21 as in grep ("global regex print"), which is included in most Unix-based operating systems, such as Linux distributions. A similar convention is used in sed, where search and replace is given by codice_22 and patterns can be joined with a comma to specify a range of lines as in codice_23. This notation is particularly well-known due to its use in Perl, where it forms part of the syntax distinct from normal string literals. In some cases, such as sed and Perl, alternative delimiters can be used to avoid collision with contents, and to avoid having to escape occurrences of the delimiter character in the contents. For example, in sed the command codice_24 will replace a codice_19 with an codice_26, using commas as delimiters.
Standards.
The IEEE POSIX standard has three sets of compliance: BRE, ERE, and SRE for Basic, Extended, and Simple Regular Expressions. SRE is deprecated, in favor of BRE, as both provide backward compatibility. The subsection below covering the "character classes" applies to both BRE and ERE.
BRE and ERE work together. ERE adds codice_1, codice_2, and codice_29, and it removes the need to escape the metacharacters codice_7 and codice_10, which are "required" in BRE. Furthermore, as long as the POSIX standard syntax for regular expressions is adhered to, there can be, and often is, additional syntax to serve specific (yet POSIX compliant) applications. Although POSIX.2 leaves some implementation specifics undefined, BRE and ERE provide a "standard" which has since been adopted as the default syntax of many tools, where the choice of BRE or ERE modes is usually a supported option. For example, GNU grep has the following options: "grep -E" for ERE, and "grep -G" for BRE (the default), and "grep -P" for Perl regular expressions.
Perl regular expressions have become a de facto standard, having a rich and powerful set of atomic expressions. Perl has no "basic" or "extended" levels, where the codice_7 and codice_10 may or may not have literal meanings. They are always metacharacters, as they are in "extended" mode for POSIX. To get their "literal" meaning, you escape them. Other metacharacters are known to be literal or symbolic based on context alone. Perl offers much more functionality: "lazy" regular expressions, backtracking, named capture groups, and recursive patterns, all of which are powerful additions to POSIX BRE/ERE. (See lazy quantification below.)
POSIX basic and extended.
In the POSIX standard, Basic Regular Syntax, BRE, requires that the metacharacters codice_7 and codice_10 be designated codice_36 and codice_37, whereas Extended Regular Syntax, ERE, does not.
Examples:
POSIX extended.
The meaning of metacharacters escaped with a backslash is reversed for some characters in the POSIX Extended Regular Expression (ERE) syntax. With this syntax, a backslash causes the metacharacter to be treated as a literal character. So, for example, codice_48 is now codice_7 and codice_50 is now codice_10. Additionally, support is removed for codice_52 backreferences and the following metacharacters are added:
Examples:
POSIX Extended Regular Expressions can often be used with modern Unix utilities by including the command line flag -E.
Character classes.
The character class is the most basic regular expression concept after a literal match. It makes one small sequence of characters match a larger set of characters. For example, codice_57 could stand for the upper case alphabet, and codice_58 could mean any digit. Character classes apply to both POSIX levels.
When specifying a range of characters, such as codice_59 (i.e. lowercase "codice_60" to upper-case "codice_61"), the computer's locale settings determine the contents by the numeric ordering of the character encoding. They could store digits in that sequence, or the ordering could be "abc...zABC...Z", or "aAbBcC...zZ". So the POSIX standard defines a character class, which will be known by the regular expression processor installed. Those definitions are in the following table:
POSIX character classes can only be used within bracket expressions. For example, codice_62 matches the uppercase letters and lowercase "a" and "b".
An additional non-POSIX class understood by some tools is codice_63, which is usually defined as codice_64 plus underscore. This reflects the fact that in many programming languages these are the characters that may be used in identifiers. The editor [[Vim (text editor)|Vim]] further distinguishes "word" and "word-head" classes (using the notation codice_65 and codice_66) since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions.
Note that what the POSIX regular expression standards call "character classes" are commonly referred to as "POSIX character classes" in other regular expression flavors which support them. With most other regular expression flavors, the term "character class" is used to describe what POSIX calls "bracket expressions".
Perl.
Because of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's — for example, [[Java (programming language)|Java]], [[JavaScript]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], [[Microsoft]]'s [[.NET Framework]], and [[XML Schema (W3C)|XML Schema]]. Some languages and tools such as [[Boost C++ Libraries|Boost]] and [[PHP]] support multiple regular expression flavors. Perl-derivative regular expression implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages, for example, Perl 5.10 implements syntactic extensions originally developed in [[PCRE]] and Python.
Lazy quantification.
Quantifiers match as many times as possible unless followed by codice_1, when they match as few times as possible. If a quantifier is not followed by codice_1, we say the quantifier is "[[Greedy algorithm|greedy]]". For example, consider the string
 Another whale sighting occurred on <January 26>, <2004>.
To match (then display) only "<January 26>" and not ", <2004>" it is tempting to write codice_69. But there is more than one codice_70, and the expression can take the second one, and having both, still match, displaying "<January 26>, <2004>". Because the codice_71 quantifier is greedy, it will consume as many characters as possible from the string, and "<January 26>, <2004>" has more characters than "<January 26>".
This problem can be avoided by specifying the text that is "not" to be matched: codice_72), but modern regular expressions allow a quantifier to be specified as "lazy". They put a question mark after the quantifier to make it lazy codice_73). By using a lazy quantifier, the expression tries the minimal match first. Lazy matching may also be used to improve performance by avoiding [[backtracking]] in cases when the longest match is likely to be incorrect.
Patterns for non-regular languages.
Many features found in modern regular expression libraries provide an expressive power that far exceeds the [[regular language]]s. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences). This means that, among other things, a pattern can match strings of repeated words like "papa" or "WikiWiki", called "[[square (formal language theory)|square]]s" in formal language theory. The pattern for these strings is codice_74.
The language of squares is not regular, nor is it [[context-free language|context-free]], due to the [[Pumping lemma for context-free languages|pumping lemma]]. However, [[pattern matching]] with an unbounded number of back references, as supported by numerous modern tools, is still [[context-sensitive language|context sensitive]].
However, many tools, libraries, and engines that provide such constructions still use the term "regular expression" for their patterns. This has led to a nomenclature where the term regular expression has different meanings in [[formal language|formal language theory]] and pattern matching. For this reason, some people have taken to using the term "regex" or simply "pattern" to describe the latter. [[Larry Wall]], author of the Perl programming language, writes in an essay about the design of Perl 6:
'Regular expressions' [...] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here. I will, however, generally call them "regexes" (or "regexen", when I'm in an Anglo-Saxon mood).
Fuzzy regular expressions.
Variants of regular expressions can be used for working with text in [[natural language]], when it is necessary to take into account possible typos and spelling variants. For example, the text "Julius Caesar" might be a fuzzy match for:
In such cases the mechanism implements some [[Approximate string matching|fuzzy string matching]] algorithm and possibly some algorithm for finding the [[Edit distance|similarity]] between text fragment and pattern.
This task is closely related to both [[full text search]] and [[named entity recognition]].
Some [[Library (computing)|software libraries]] work with fuzzy regular expressions:
Implementations and running times.
There are at least three different [[algorithm]]s that decide whether and how a given regular expression matches a string.
The oldest and fastest relies on a result in formal language theory that allows every [[nondeterministic finite automaton]] (NFA) to be transformed into a [[deterministic finite automaton]] (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size "m" has the time and memory cost of [[Big O notation|"O"]]("2m"), but it can be run on a string of size "n" in time "O"("n"). An alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to "O"("m n"). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the "lazy DFA" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky.
The third algorithm is to match the pattern against the input string by [[backtracking]]. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like codice_75 that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called [[ReDoS|Regular expression Denial of Service]].
Although backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match.
Unicode.
In theoretical terms, any token set can be matched by regular expressions as long as it is pre-defined. In terms of historical implementations, regular expressions were originally written to use [[American Standard Code for Information Interchange|ASCII]] characters as their token set though regular expression libraries have supported numerous other [[character set]]s. Many modern regular expression engines offer at least some support for [[Unicode]]. In most respects it makes no difference what the character set is, but some issues do arise when extending regular expressions to support Unicode.
Uses.
Regular expressions are useful in the production of [[syntax highlighting]] systems, [[data validation]], and many other tasks.
While regular expressions would be useful on Internet [[Search engine (computing)|search engine]]s, processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex. Although in many cases system administrators can run regex-based queries internally, most search engines do not offer regex support to the public. Notable exceptions: [[Google Code Search]], [[Exalead]].
Examples.
A regular expression is a string that is used to describe or match a set of strings according to certain [[syntax]] rules. The specific syntax rules vary depending on the specific implementation, [[programming language]], or [[Library (computing)|library]] in use. Additionally, the functionality of regex implementations can vary between [[Software versioning|version]]s.
Despite this variability, and because regular expressions can be difficult to both explain and understand without examples.
[[#OnlineRegexTesters|Interactive web sites]] for testing regular expressions are a useful resource for learning regular expressions by experimentation.
This section provides a basic description of some of the properties of regular expressions by way of illustration.
The following conventions are used in the examples.
 metacharacter(s) ;; the metacharacters column specifies the regex syntax being demonstrated
 =~ m// ;; indicates a regex match operation in Perl
 =~ s/// ;; indicates a regex substitution operation in Perl
Also worth noting is that these regular expressions are all Perl-like syntax. Standard [[#POSIX Basic Regular Expressions|POSIX]] regular expressions are different.
Unless otherwise indicated, the following examples conform to the [[Perl]] programming language, release 5.8.8, January 31, 2006. This means that other implementations may lack support for some parts of the syntax shown here (e.g. basic vs. extended regex, codice_95 vs. codice_96, or lack of codice_58 instead of [[POSIX]] codice_98).
The syntax and conventions used in these examples coincide with that of other programming environments as well.
References.
</dl>
External links.
Other.
[[Category:Automata theory]]
[[Category:Formal languages]]
[[Category:Pattern matching]]
[[Category:Programming constructs]]
[[Category:Regular expressions]]

</doc>
<doc id="25721" url="http://en.wikipedia.org/wiki?curid=25721" title="Red Dwarf">
Red Dwarf

Red Dwarf is a British comedy franchise which primarily comprises ten series (including a ninth mini-series titled "Back To Earth") of a television science fiction sitcom that aired on BBC Two between 1988 and 1993 and from 1997 to 1999, and on Dave in 2009 and 2012, gaining a cult following. In Spring 2015, it was announced that Series XI and XII will film back-to-back in Autumn 2015 and will air exclusively on the Dave channel in 2016 and 2017.
The series was created by Rob Grant and Doug Naylor. In addition to the television episodes, there are four novels, two pilot episodes for an American version of the show, a radio version produced for BBC Radio 7, tie-in books, magazines and other merchandise.
Despite the pastiche of science fiction used as a backdrop, "Red Dwarf" is primarily a character-driven comedy, with off-the-wall, often scatological science fiction elements used as complementary plot devices. In the early episodes, a recurring source of comedy was the "Odd Couple"-style relationship between the two central characters of the show, who have an intense dislike for each other and are trapped together deep in space. The main characters are Dave Lister, the last known human alive, and Arnold Rimmer, a hologram of Lister's dead bunkmate. The other regular characters are Cat, a lifeform which evolved from the descendants of Lister's pregnant pet cat Frankenstein; Holly, "Red Dwarf's" computer; Kryten, a service mechanoid; and, as of Series VII to Back to Earth, Kristine Kochanski, an alternative-reality version of Lister's long-lost love.
One of the series' highest accolades came in 1994, when an episode from the sixth series, "Gunmen of the Apocalypse", won an International Emmy Award in the Popular Arts category, and in the same year the series was also awarded "Best BBC Comedy Series" at the British Comedy Awards. The series attracted its highest ratings, of more than eight million viewers, during the eighth series in 1999. The series was revived after a ten-year break, when digital channel Dave screened a three-episode production, titled "", in April 2009 during the Easter weekend. This was followed by Series X, consisting of six episodes, which was first broadcast on Dave in October/November 2012. The show has been critically acclaimed, and has a Metacritic score of 84/100.
Radio origins.
The show was based on "Dave Hollins: Space Cadet", a series of five sketches that aired in the BBC Radio 4 series "Son of Cliché", produced by Rob Grant and Doug Naylor in 1984.
The sketches recounted the adventures of Dave Hollins (voiced by Nick Wilton), a hapless space traveler that is marooned in space far from earth. His only steady companion is the computer Hab (voiced by Chris Barrie).
Grant and Naylor chose to use the "Dave Hollins: Space Cadet" sketches as a base for a television show after watching the 1974 film "Dark Star". They changed some elements from the sketches:
The 7 trillion year figure was first changed to 7 billion years and then to 3 million and the characters of Arnold Rimmer and the Cat were created. The name Dave Hollins was changed to Dave Lister when a football player called Dave Hollins became well-known, and Hab was replaced by Holly. One of the voice actors from "Son of Cliché", Chris Barrie went on to portray Arnold Rimmer in the "Red Dwarf" TV series.
Episodes of Dave Hollins can be found on the 2 disc Red Dwarf DVD sets starting with series 5 and ending with series 8.
Setting and plot.
The main setting of the series is the eponymous mining spaceship "Red Dwarf", which is 6 mi long, 4 mi tall, and 3 mi wide and is operated by the Jupiter Mining Corporation. In the first episode set sometime in the late 22nd century, an on-board radiation leak of cadmium II kills everyone except for lowest-ranking technician Dave Lister, who is in suspended animation at the time, and his pregnant cat, Frankenstein, who is safely sealed in the cargo hold. Following the accident, the ship's computer Holly keeps Lister in stasis until the background radiation dies down – a process that takes three million years. Lister therefore emerges as the last human being in the universe – but not alone on-board the ship. His former bunkmate and immediate superior Arnold Judas Rimmer is resurrected by Holly as a hologram to keep Lister sane. At the same time, a creature known only as Cat is the last member on board of "Felis sapiens", a race of humanoid felines that evolved in the ship's hold from Lister's cat, Frankenstein, and her kittens during the 3 million years that Lister was in stasis.
The main dramatic thrust of the early series is Lister's desire to return home to Earth, although the crew's ownership of an unlimited time-space travel drive in series seven was to later negate this intention. As their journey begins, the not-so-intrepid crew encounters such phenomena as time distortions, faster-than-light travel, mutant diseases and strange lifeforms that had developed in the intervening millions of years. During the second series, the group encounter the service mechanoid Kryten, rescuing him from a long-since crashed vessel. Initially, Kryten only appeared in one episode of series two, but by the beginning of series three he had become a regular character. At the end of series five, "Red Dwarf" itself is stolen by persons unknown, forcing the crew to travel in the smaller "Starbug" craft for two series, with the side-effect that they lose contact with Holly. In series seven, Rimmer departs the crew to take up the role of his alter ego from a parallel universe, Ace Rimmer, whose name has become a long-standing legend and a legacy passed down from dimension to dimension. Shortly afterwards, the crew encounters a parallel version of themselves from a universe in which Kristine Kochanski, Lister's long-term love interest, had been put into stasis at the time of the leak and so became the last remaining human. A complicated series of events leaves Kochanski stranded in "our" universe, where she is forced to join the crew. At the end of series seven, we learn that Kryten's service nanobots, which had abandoned him years earlier, were behind the theft of the "Red Dwarf" at the end of series five.
At the beginning of the eighth series, Kryten's nanobots reconstruct the "Red Dwarf", which they had broken down into its constituent atoms. In the process, the entire crew of the ship – including a pre-accident Rimmer – are resurrected, but the "Starbug" crew find themselves sentenced to two years in the ship's brig (at first, for crashing a "Starbug" and bringing onboard Kryten and Cat as stowaways, but later for using information from the confidential files). The series ends with a metal-eating virus loose on "Red Dwarf". The entire resurrected crew evacuates save the original dwarfers. In the cliffhanger ending, Rimmer is left stranded alone to face Death (and promptly knees him in the groin and flees).
Nine years later, the four are once more the only beings on the ship. Rimmer is again a hologram, Holly is offline, and Lister is mourning Kochanski, lost to him out of an airlock some time previously. A chance to get back to Earth through a dimension warp presents itself; although it is not quite what it appears to be, it gives Lister new hope when he learns that Kochanski is still alive after all.
The tenth series sees Lister still traveling with Rimmer, Kryten and Cat in "Red Dwarf", in hopes of eventually locating Kochanski or returning to Earth, whichever comes first.
Production.
The first series aired on BBC2 in 1988. Ten further series have so far been produced, and a film has been in development almost continually since before series VIII in 1999.
Concept and commission.
The concept for the show was originally developed from the sketch-series "" on the BBC Radio 4 show "Son of Cliché" in the mid-1980s, written by Rob Grant and Doug Naylor. Their influences came from films and television programmes such as "Silent Running" (1972), "Alien" (1979), "Dark Star" (1974) and "The Hitchhiker's Guide to the Galaxy" (1981), but also had a large element of British-style comedy and satire thrown into the mix, ultimately moulded into the form of a sitcom. Many visual and character elements bear similarities to the Trident nuclear submarine BBC documentary "Defence of the Realm". Having first written the pilot script in 1983, the former "Spitting Image" writers had hawked their unusual and original script around but it was rejected by everyone at the BBC, as it was believed a science fiction sitcom would not be popular.
It was finally accepted by BBC North in 1986, a result of a spare budget being assigned for a second series of "Happy Families" that would never arise, and producer Paul Jackson's insistence that "Red Dwarf" should be filmed instead. The show was lucky to be remounted after an electricians' strike partway through rehearsals in early 1987 shut the entire production down (the title sequence was filmed in January 1987). The filming was rescheduled for September, and the pilot episode finally made it onto television screens on 15 February 1988.
Casting.
Alan Rickman and Alfred Molina auditioned for roles in the series, with Molina being cast as Rimmer. However, after Molina had difficulties with the concept of the series, and of his role in particular, the role was recast and filled by Chris Barrie, a professional voice-actor and impressionist who had previously worked with both the writers on "Spitting Image", and with the producers on "Happy Families" and Jasper Carrott productions. Craig Charles, a Liverpudlian "punk poet", was given the role of Dave Lister. He was approached by the production team for his opinion about the "Cat" character, as they were concerned it may be considered by people as racist. Charles described "Cat" as 'pretty cool' and after reading the script he decided he wanted to audition for the part of Dave Lister. Laconic stand up comedian Norman Lovett, who had originally tried out for the role of Rimmer, was kept in the show as Holly, the senile computer of the titular ship. A professional dancer and singer, Danny John-Jules, arriving half an hour late for his appointment, stood out as the Cat immediately. This was partly due to his "cool" exterior, dedicated research (reading Desmond Morris' book "Catwatching"), and his showing up in character, wearing his father's 1950s-style zoot suit.
Writing, producing, and directing.
Grant and Naylor wrote the first six series together (using the pseudonym Grant Naylor on the first two novels and later as the name of their production company, although never on the episodes themselves). Grant left in 1995, to pursue other projects, leaving Naylor to write series VII and VIII with a group of new writers, including Paul Alexander and actor Robert Llewellyn who portrayed the character Kryten.
For the most part, Ed Bye produced and directed the series. He left before series V due to a scheduling clash (he ended up directing a show starring his wife, Ruby Wax) so Juliet May took over as director. May parted ways with the show halfway through the series for personal and professional reasons and Grant and Naylor took over direction of the series, in addition to writing and producing. Series VI was directed by Andy de Emmony, and Ed Bye returned to direct series VII and VIII. Series I, II and III were made by Paul Jackson Productions, with subsequent series produced by the writers' own company Grant Naylor Productions for BBC North. All eight series were broadcast on BBC2. At the beginning of series IV, production moved from the BBC North's New Broadcasting House in Manchester to Shepperton.
Theme song and music.
The theme tune and incidental music were written and performed by Howard Goodall, with the distinctive vocals on the closing theme tune by Jenna Russell. The first two series used a relatively sombre instrumental version of the closing theme for the opening titles; from series III onwards this switched to a more upbeat version. Goodall also wrote music for the show's various songs, including "Tongue Tied", with lyrics written by Grant and Naylor. Danny John-Jules (credited as 'The Cat') re-orchestrated and released "Tongue Tied" in October 1993; it reached number 17 on the UK charts. Goodall himself sang "The Rimmer Song" heard during the series VII episode "Blue", to which Chris Barrie mimed.
Remastered.
In 1998, on the tenth anniversary of the show's first airing (and between the broadcast of series VII and VIII), the first three series of "Red Dwarf" were remastered and released on VHS. The remastering included replacing model shots with computer graphics, cutting certain dialogue and scenes, re-filming Norman Lovett's Holly footage, creating a consistent set of opening titles, replacing music and creating ambient sound effects with a digital master. The remastered series were released in a 4-disc DVD boxset "The Bodysnatcher Collection" in 2007.
Hiatus.
Three years elapsed between series VI and VII, partly due to the dissolving of the Grant and Naylor partnership, but also due to cast and crew working on other projects. When the series eventually returned, it was filmised and no longer shot in front of a live audience, allowing for greater use of four-walled sets, location shooting and single-camera techniques. When the show returned for its eighth series two years later, it had dropped use of the filmising process and returned to using a live audience.
The show received a setback when the BBC rejected proposals for a series IX. Doug Naylor confirmed that the BBC decided not to renew the series as they preferred to work on other projects. A short animated Christmas special was, however, made available to mobile phone subscribers. Ultimately, however, fans had to wait a decade before the series returned to television.
Revival.
"Red Dwarf: Back to Earth".
In 2008, a three-episode production was commissioned by the digital channel Dave. "Red Dwarf: Back to Earth" was broadcast over the Easter weekend of 2009, along with a "making of" documentary. The episode was set nine years after the events of "Only the Good..." (with the cliffhanger ending of that episode left unresolved, a situation that would continue with Series X). The storyline involves the characters arriving back on Earth, circa 2009, only to find that they are characters in a TV show called "Red Dwarf". Kochanski is supposedly dead and Holly is offline due to water damage caused by Lister leaving a tap running. Actress Sophie Winkleman played a character called Katerina, a resurrected hologram of a Red Dwarf science officer intent on replacing Rimmer.
To achieve a more cinematic atmosphere, "Back to Earth" was not filmed in front of a studio audience. Some previous "Red Dwarf" episodes had been shot in that way ("Bodyswap" and all of the seventh series), but "Back to Earth" represented the first time that a laughter track was not added before broadcast. It was also the first episode of "Red Dwarf" to be filmed in high definition.
The specials were televised over three nights starting on Friday, 10 April 2009. The broadcasts received record ratings for Freeview channel Dave; the first of the three episodes represented the UK's highest ever viewing figures for a commissioned programme on a digital network. "Back to Earth" was released on DVD on 15 June 2009, and on Blu-ray on 31 August 2009. "Back to Earth" was subsequently described on the series' official website as "for all intents and purposes, the 'ninth series' of "Red Dwarf"". Its placement as Series IX was confirmed when Series X was commissioned and branded as the 10th season.
"Red Dwarf X".
On 10 April 2011 Dave announced it had commissioned a six-episode "Red Dwarf" "Series X" to be broadcast on Dave in autumn 2012. Filming dates for the new series Red Dwarf X were announced on 11 November 2011, along with confirmation that the series would be shot at Shepperton Studios in front of an audience. Principal filming began on 16 December 2011 and ended on 27 January 2012, and the cast and crew subsequently returned for six days filming pick ups. Discounting guest stars, only the core cast of Charles, Barrie, Llewellyn and John-Jules returned for Series X, with Annett and Lovett absent, though the scripts include references to Kochanski and Holly.
On 20 July 2012, a 55-second trailer for series X was released on Facebook, followed by a new teaser every Friday. The new series debuted on Thursday 4 October 2012.
Since series X aired, which produced high ratings, Dave, Doug Naylor and the cast have shown a strong interest in doing another series with Naylor already starting on scripts, but this is dependent on UKTV commissioning the series. During the Dimension Jump fan convention in May 2013, Doug Naylor stated that discussions were happening with all involved parties but arrangements had not been finalized, but he hoped shooting could begin in February 2014. In October 2013, Doug Naylor played down reports of a new series being commissioned; but in January 2014 Danny John-Jules stated that the eleventh series of Red Dwarf was currently being written.
"Red Dwarf XI and XII".
In October 2013, Robert Llewellyn posted on his blog, stating that "an eleventh series would happen" and that it would be "sometime in 2014". Llewellyn removed the post from his blog and Doug Naylor issued a statement on Twitter, saying: "Getting tweets claiming Red Dwarf XI is commissioned. Not true. Not yet."
At the April 2014 Sci-Fi Scarborough Festival, during the "Red Dwarf" cast panel, Danny John-Jules stated that filming of the eleventh season would commence in October 2014, with an expected release of Autumn 2015 on Dave.
On 2 May 2015, at the Dimension Jump XVIII convention, Naylor announced that an eleventh and a twelfth series had been commissioned. The two series will be shot back-to-back towards the end of 2015 for broadcast on Dave in 2016 and 2017 respectively. The new series will be co-produced by Baby Cow Productions, with company CEO, Henry Normal, executive producing the new episodes.
Themes.
"Red Dwarf" was founded on the standard sitcom focus of a disparate and frequently dysfunctional group of individuals living together in a restricted setting. With the main characters routinely displaying their cowardice, incompetence and laziness, while exchanging insulting and sarcastic dialogue, the series provided a humorous antidote to the fearless and morally upright space explorers typically found in science-fiction series, with its main characters acting bravely only when there was no other possible alternative. The increasing science-fiction elements of the series were treated seriously by creators Rob Grant and Doug Naylor. Satire, parody and drama were alternately woven into the episodes, referencing other television series, films and books. These have included references to the likes of "" (1968), "Top Gun" (1986), "RoboCop" (1987), "Star Wars" (1977), "Citizen Kane" (1942), "The Wild One" (1953), "High Noon" (1952), "Rebel Without a Cause" (1955), "Easy Rider" (1969), "The Terminator" (1984) and "Pride and Prejudice" (1813).
The writers based the whole theme of some episodes on the plots of feature films. The series III episode "Polymorph" references and parodies key moments from "Alien" (1979); from series IV, "Camille" echoes key scenes from "Casablanca" (1942), while "Meltdown" borrows the main plot from "Westworld" (1973). For series IX, "" was partially inspired by "Blade Runner" (1982). The series' themes are not limited to films or television, having also incorporated historical events and figures. Religion also plays a part in the series, as a significant factor in the ultimate fate of the Cat race, and the perception of Lister as their 'God', both within the episode "Waiting for God" (whose title makes a literary reference to the Samuel Beckett play "Waiting for Godot"), as well as the crew meeting a man they believe to be Jesus Christ in series X episode "Lemons". The series VII episode titled "Ouroboros" derives its name and theme from the ancient mythological snake by the same name.
The series explores many science-fiction staples such as time-travel paradoxes (including the grandfather paradox), the question of determinism and free will (on several episodes), the pursuit of happiness in virtual reality and, crucially to the show's premise of Lister being the last human, the near-certainty of the human species' extinction some time in the far future.
Aliens do not feature in the series, as Grant and Naylor decided very early in the process that they did not want aliens involved. However, there are non-human life forms such as evolutions of Earth species (e.g. the Cat race), robotic or holo-life forms created by humans, and a kind of 'Genetically Engineered Life Form' (GELF), an artificially-created creature. Simulants and GELFs frequently serve as antagonists among the later series of the show.
Hallmarks.
The series developed its own distinct vocabulary. Words and phrases such as hologramatic ["sic"], Dollarpound, "Felis sapiens", Simulants, GELF, space weevil and Zero Gee Football appear throughout the series, highlighting a development in language, political climate, technology, evolution and culture in the future. The creators also employed a vocabulary of fictional expletives in order to avoid using potentially offensive words in the show, and to give nuance to futuristic colloquial language; in particular "smeg" (and variants such as "smegging", "smegger", and "smeg-head") features prominently, alongside the terms "gimboid" and "goit".
Reception and achievements.
Mixed reactions.
The changes that were made to the series' cast, setting, creative teams and even production values from series to series have meant that opinions differ greatly between fans and critics alike as to the quality of certain series. In the "Great Red Dwarf Debate", published in volume 2 issue 3 of the "Red Dwarf Smegazine", science-fiction writers Steve Lyons and Joe Nazzaro both argued on the pros and cons of the early series against the later series. Lyons stated that what the show "once had was a unique balance of sci-fi comedy, which worked magnificently." Nazarro agreed that "the first two series are very original and very funny", but went on to say that "it wasn't until series III that the show hit its stride." Series VI is regarded as a continuation of the "Monster of the week" philosophy of series V, which was nevertheless considered to be visually impressive. Discussions revolve around the quality of series VI, seen by viewers as just as good as the earlier series', but has been criticised as a descent into formulaic comedy with an unwelcome change of setting.
The changes seen in series VII were seen by some as a disappointment; while much slicker and higher-budget in appearance, the shift away from outright sitcom and into something approaching comedy drama was seen as a move in the wrong direction. Furthermore, the attempt to shift back into traditional sitcom format for series VIII was greeted with a response that was similarly lukewarm. There was criticism aimed at the decision to resurrect the entire crew of "Red Dwarf", as it was felt this detracted from the series' central premise of Lister being the last human being alive. There are other critics who feel that series VII and VIII are no weaker than the earlier series, however, and the topic is the subject of constant fervent debate among the show's fanbase.
Achievements.
Although the pilot episode of the show gathered over four million viewers, viewing figures dipped in successive episodes and the first series had generally poor ratings. Through to series VI the ratings had steadily increased and peaked at over six million viewers, achieved with the episode "Gunmen of the Apocalypse". When the series returned in 1999 it gained the highest audience figures yet – over eight million viewers tuned in for series VIII's opening episode "". In its eight-series history, the series has won numerous awards including the Royal Television Society Award for special effects, the British Science Fiction award for Best Dramatic Presentation, as well as an International Emmy Award for series VI episode "Gunmen of the Apocalypse", which tied with an "Absolutely Fabulous" episode, "Hospital", in the Popular Arts category. The show had also been nominated for the International Emmy Award in 1987, 1989, and 1992. Series VI won a British Comedy Award for 'Best BBC Comedy Series'. The video sales have won eight Gold Awards from the British Video Association, and the series still holds the record for being BBC2's longest-running, highest-rated sitcom. In 2007 the series was voted 'Best Sci-Fi Show Of All Time' by the readers of "Radio Times" magazine. Editor Gill Hudson stated that this result had surprised them as 'the series had not given any new episodes this century'.
Spin-offs and merchandise.
The show's logo and characters have appeared on a wide range of merchandise. "Red Dwarf" has also been spun off in a variety of different media formats. For instance, the song "Tongue Tied", featured in the "Parallel Universe" episode of the show, was released in 1993 as a single and became a top 20 UK hit for Danny John Jules (under the name 'The Cat'). Stage plays of the show have been produced through Blak Yak, a theatre group in Perth, Western Australia, who were given permission by Grant Naylor Productions to mount stage versions of certain episodes in 2002, 2004 and 2006. In October 2006 an Interactive Quiz DVD entitled "Red Dwarf: Beat The Geek" was released, hosted by Norman Lovett and Hattie Hayridge, both reprising their roles as Holly.
Novels.
Working together under the name "Grant Naylor", the creators of the series collaboratively wrote two novels. The first, "Infinity Welcomes Careful Drivers", was published in November 1989, and incorporates plot lines from several episodes of the show's first two series. The second novel, "Better Than Life", followed in October 1990, and is largely based on the second-series episode of the same name. Together, the two novels provide expanded backstory and development of the series' principal characters and themes.
The authors began work on a sequel to "Better than Life", called "The Last Human", but Rob Grant was drawn away from "Red Dwarf" by an interest in other projects. Still owing Penguin Publishing two more "Red Dwarf" novels, Grant and Naylor decided to each write an alternative sequel to "Better than Life". Two completely different sequels were made as a result, each presenting a possible version of the story's continuation. "Last Human", by Doug Naylor, adds Kochanski to the crew and places more emphasis on the science-fiction and plot elements, while Rob Grant's novel "Backwards", is more in keeping with the previous two novels, and borrows more extensively from established television stories.
An omnibus edition of the first two novels was released in 1992, including edits to the original text and extra material such as the original pilot script of the TV series. All four novels have been released in audiobook format, the first two read by Chris Barrie, "Last Human" read by Craig Charles, and "Backwards" read by author Rob Grant.
In December 2009, "Infinity Welcomes Careful Drivers" was released in Germany with the title "Roter Zwerg" (Red Dwarf in German).
Home video releases.
For the initial release of the VHS editions, episodes of "Red Dwarf" were separated and two tapes were released for each series (except series VII, which was released on three separate tapes), labelled 'byte one' and 'byte two'. These videos were named after the first episode of the three presented on the tape, as was typical with other BBC video releases at the time. However, on occasions the BBC decided to ignore the original running order and use the most popular episodes from the series to maximise sales of the videos. For series V, "Back to Reality" and "Quarantine" were given top billing on their respective video release. For the second VHS volume of series I, "Confidence and Paranoia" was given top billing, even though the original broadcast order was retained; this was due to the leading episode being "Waiting for God" which shared its name with the title of another comedy series (set in a retirement home). Future releases would increasingly observe authenticity with the 'original broadcast' context. All eight series were made available on VHS, and three episodes of series VII were also released as special "Xtended" ["sic"] versions with extra scenes (including an original, unbroadcast ending for the episode "Tikka To Ride") and no laugh track; the remastered versions of series I–III were also released individually and in a complete box-set. Finally, two outtake videos were released, "Smeg Ups" in 1994, and its sequel "Smeg Outs" in 1995.
The first eight series have since been released on DVD in Region 1, 2 and 4, each with a bonus disc of extra material and each release from series III onwards being accompanied by an original documentary about the making of each respective series. Regions 2 and 4 have also seen the release of two "Just The Shows", digipack boxsets containing the episodes from series I–IV (Volume 1) and V-VIII (Volume 2) with static menus and no extras. "Red Dwarf: The Bodysnatcher Collection", containing the 1997 remastered episodes, as well as new documentaries for series I and II, was released in 2007. This release showcased a storyboard construction of "Bodysnatcher", an unfinished script from 1987, which was finally completed in 2007 by Rob Grant and Doug Naylor who were working together for the first time since 1993. In December 2008 an anniversary DVD set entitled "Red Dwarf: All The Shows" was released, reworking the vanilla disc content of the two Just The Shows sets within A4 packaging resembling a 'photo album', which carefully omitted information that no extras were included. This box-set was re-released in a smaller slip-case sized box, reverting to the "Just the Shows" title, in November 2009. The series is also available for download on iTunes.
Magazine.
The "Red Dwarf Magazine" – the magazine part of the title changed to "Smegazine" from issue 3 – was launched in 1992 by Fleetway Editions. It comprised a mix of news, reviews, interviews, comic strips and competitions. The comic strips featured episode adaptations and original material, including further stories of popular characters like Mr. Flibble, the Polymorph and Ace Rimmer.
Notably, the comic strip stories' holographic characters, predominately Rimmer, were drawn in greyscale. This was at the request of Grant and Naylor, who had wanted to use the technique for the television series, but the process was deemed too expensive to produce. Despite achieving circulation figures of over 40,000 per month, the magazine's publisher decided to close the title down to concentrate on their other publications. A farewell issue was published, cover dated January 1994, and featured the remaining interviews, features and comic strips that were to feature in the following issues.
Another Red Dwarf magazine was started called "Red Dwarf: Better Than Life" which is only available through the "Red Dwarf Official Fan Club". It features cast interviews and the latest news. Each person gets four issues each year.
U.S. version.
A pilot episode for an American version (known as Red Dwarf USA) was produced through Universal Studios with the intention of broadcasting on NBC in 1992. The show essentially followed the same story as the first episode of the original series, using American actors for most of the main roles: Craig Bierko as Lister, Chris Eigeman as Rimmer, and Hinton Battle as Cat. Exceptions to this were Llewellyn, who reprised his role as Kryten, and the British actress Jane Leeves, who played Holly. It was written by Linwood Boomer and directed by Jeffrey Melman, with Grant and Naylor onboard as creators and executive producers. Llewellyn, Grant and Naylor travelled to America for the filming of the American pilot after production of the fifth series of the UK series. According to Llewellyn and Naylor, the cast were not satisfied with Linwood Boomer's script. Grant and Naylor rewrote the script, but although the cast preferred the re-write, the script as filmed was closer to Boomer's version. The pilot episode includes footage from the UK series in its title sequence, although it did not retain the logo or the theme music of the UK series. During filming of the pilot, the audience reaction was good and it was felt that the story had been well received.
The studio executives were not entirely happy with the pilot, especially the casting, but decided to give the project another chance with Grant and Naylor in charge. The intention was to shoot a "promo video" for the show in a small studio described by the writers as "a garage". New cast members were hired for the roles of Cat and Rimmer, Terry Farrell and Anthony Fuscle respectively. This meant that, unlike the original British series, the cast was all Caucasian. Chris Barrie was asked to play Rimmer in the second pilot, but he declined. With a small budget and deadline, new scenes were quickly shot and mixed in with existing footage of the pilot and UK series V episodes, to give an idea of the basic plot and character dynamics, alongside proposed future episodes, remakes of episodes from the original show. Llewellyn did not participate in the re-shoot, though clips from the British version were used to show the character. Despite the re-shoots and re-casting, the option on the pilot was not picked up. Farrell was cast almost immediately afterwards for "", in which she was cast as Jadzia Dax and went on to star as 6 in the 2nd version of the pilot of the cartoon sci-fi spoof series Tripping the Rift.
The cast of both the British and American versions criticised the casting of "Red Dwarf USA", particularly the part of Lister who is portrayed in the British version as a likable slob but in the US version as somewhat clean cut. In the 2004 documentary "Dwarfing USA", Danny John-Jules said the only actor who could have successfully portrayed an American Lister was John Belushi. In a 2009 interview on "Kevin Pollak's Chat Show", Bierko said that casting him as Lister was a "huge mistake" and also said a "John Belushi type" would have been better suited to the role.
The American pilot has been heavily bootlegged, but it has never been broadcast on TV in any country. Excerpts from the first pilot are included in "Dwarfing USA", a featurette on the making of the pilots included on the DVD release of "Red Dwarf's" fifth series. Because of rights clearance issues, no footage from the second pilot is included in the featurette.
Film.
Since the end of the eighth series in 1999, Doug Naylor has been attempting to make a feature length version of the show. A final draft of the script was written, by Naylor, and flyers began circulating around certain websites. The flyer was genuine and had been distributed by Winchester Films to market the film overseas. Plot details were included as part of the teaser. It was set in the distant future where "Homo sapienoids" - a race of cyborgs — had taken over the solar system and were wiping out the human race. Spaceships that tried to escape Earth were hunted down until only one remained... "Red Dwarf".
Naylor had scouted Australia to get an idea of locations and finance costs, with pre-production beginning in 2004 and filming planned for 2005. However, finding sufficient funding has been difficult. Naylor explained at a "Red Dwarf" Dimension Jump convention that the film had been rejected by the BBC and the British Film Council. Reasons given for the rejections were that while the script was considered to be funny, it was not ready.
Contents from early drafts of the film were eventually used in the Series X finale "The Beginning".
Roleplaying game.
Deep7 LLC released "Red Dwarf - The Roleplaying Game" in February 2003 (although the printed copyright is 2002). Based on the series, the game allows its players to portray original characters within the Red Dwarf universe. Player characters can be human survivors, holograms, evolved house pets (cats, dogs, iguanas, rabbits, rats and mice), various types of mechanoid (Series 4000, Hudzen 10 and Waxdroids in the corebook, Series 3000 in the Extra Bits Book) or GELFs (Kinatawowi and Pleasure GELF in the corebook, "Vindaloovians" in the Extra Bits Book).
A total of three products were released for the game: the core 176-page rulebook, the "AI Screen" (analogous to the "Game Master's Screen" used in other roleplaying games, also featuring the "Extra Bits Book" booklet), and the "Series Sourcebook". The "Series Sourcebook" contains plot summaries of each episode of every series as well as game rules for all major and minor characters from each series.
The game has been praised for staying true to the comedic nature of the series, for its entertaining writing, and for the detail to which the background material is explained. However, some reviewers found the game mechanics to be simplistic and uninspiring compared to other science fiction roleplaying games on the market.
Red Dwarf Night.
On 14 February 1998, the night before the tenth anniversary of the show's pilot episode broadcast, BBC Two devoted an evening of programmes to the series, under the banner of "Red Dwarf Night". The evening consisted of a mixture of new and existing material, and was introduced and linked by actor and fan Patrick Stewart. In addition, a series of special take-offs on BBC2's idents, featuring the "2" logo falling in love with a skutter, were used. The night began with "Can't Smeg, Won't Smeg", a spoof of the cookery programme "Can't Cook, Won't Cook", presented by that show's host Ainsley Harriott who had himself appeared as a GELF in the series VI episode "". Taking place outside the continuity of the series, two teams (Kryten and Lister versus Rimmer and Cat, although Cat quickly departs to be replaced by alter ego Duane Dibbley) were challenged to make the best chicken vindaloo.
After a compilation bloopers show, featuring out-takes, the next programme was "Universe Challenge", a spoof of University Challenge. Hosted by original "University Challenge" presenter Bamber Gascoigne, the show had a team of knowledgeable "Dwarf" fans compete against a team consisting of Chris Barrie, Craig Charles, Robert Llewellyn, Chloë Annett and Danny John Jules. This was followed by "The Red Dwarf A–Z", a half-hour documentary that chose a different aspect of the show to focus on for each letter of the alphabet. Talking heads on the episode included Stephen Hawking, Terry Pratchett, original producer Paul Jackson, and Patrick Stewart. Finally, the night ended with a showing of the episode "Gunmen of the Apocalypse".

</doc>
<doc id="25723" url="http://en.wikipedia.org/wiki?curid=25723" title="Regular language">
Regular language

In theoretical computer science and formal language theory, a regular language (also called a rational language) is a formal language that can be expressed using a regular expression, in the strict sense of the latter notion used in theoretical computer science. (Many regular expressions engines provided by modern programming languages are augmented with features that allow recognition of languages that cannot be expressed by a classic regular expression.)
Alternatively, a regular language can be defined as a language recognized by a finite automaton. The equivalence of regular expressions and finite automata is known as Kleene's theorem. In the Chomsky hierarchy, regular languages are defined to be the languages that are generated by Type-3 grammars (regular grammars).
Regular languages are very useful in input parsing and programming language design.
Formal definition.
The collection of regular languages over an alphabet Σ is defined recursively as follows:
See regular expression for its syntax and semantics. Note that the above cases are in effect the defining rules of regular expression.
Examples.
All finite languages are regular; in particular the empty string language {ε} = Ø* is regular. Other typical examples include the language consisting of all strings over the alphabet {"a", "b"} which contain an even number of "a"s, or the language consisting of all strings of the form: several "a"s followed by several "b"s.
A simple example of a language that is not regular is the set of strings formula_1. Intuitively, it cannot be recognized with a finite automaton, since a finite automaton has finite memory and it cannot remember the exact number of a's. Techniques to prove this fact rigorously are given below.
Equivalent formalisms.
A regular language satisfies the following equivalent properties:
Some authors use one of the above properties different from "1." as alternative definition of regular languages.
Some of the equivalences above, particularly those among the first four formalisms, are called "Kleene's theorem" in textbooks. Precisely which one (or which subset) is called such varies between authors. One textbook calls the equivalence of regular expressions and NFAs ("1." and "2." above) "Kleene's theorem". Another textbook calls the equivalence of regular expressions and DFAs ("1." and "3." above) "Kleene's theorem". Two other textbooks first prove the expressive equivalence of NFAs and DFAs ("2." and "3.") and then state "Kleene's theorem" as the equivalence between regular expressions and finite automata (the latter said to describe "recognizable languages"). A linguistically oriented text first equates regular grammars ("4." above) with DFAs and NFAs, calls the languages generated by (any of) these "regular", after which it introduces regular expressions which it terms to describe "rational languages", and finally states "Kleene's theorem" as the coincidence of regular and rational languages. Other authors simply "define" "rational expression" and "regular expressions" as synonymous and do the same with "rational languages" and "regular languages".
Closure properties.
The regular languages are closed under the various operations, that is, if the languages "K" and "L" are regular, so is the result of the following operations:
Deciding whether a language is regular.
To locate the regular languages in the Chomsky hierarchy, one notices that every regular language is context-free. The converse is not true: for example the language consisting of all strings having the same number of "a"'s as "b"'s is context-free but not regular. To prove that a language such as this is not regular, one often uses the Myhill–Nerode theorem or the pumping lemma among other methods.
There are two purely algebraic approaches to define regular languages. If:
then the set formula_11 is regular. Every regular language arises in this fashion.
If "L" is any subset of Σ*, one defines an equivalence relation ~ (called the syntactic relation) on Σ* as follows: "u" ~ "v" is defined to mean
The language "L" is regular if and only if the number of equivalence classes of ~ is finite (A proof of this is provided in the article on the syntactic monoid). When a language is regular, then the number of equivalence classes is equal to the number of states of the minimal deterministic finite automaton accepting "L".
A similar set of statements can be formulated for a monoid formula_12. In this case, equivalence over "M" leads to the concept of a recognizable language.
Complexity results.
In computational complexity theory, the complexity class of all regular languages is sometimes referred to as REGULAR or REG and equals DSPACE(O(1)), the decision problems that can be solved in constant space (the space used is independent of the input size). REGULAR ≠ AC0, since it (trivially) contains the parity problem of determining whether the number of 1 bits in the input is even or odd and this problem is not in AC0. On the other hand, REGULAR does not contain AC0, because the nonregular language of palindromes, or the nonregular language formula_13 can both be recognized in AC0.
If a language is "not" regular, it requires a machine with at least Ω(log log "n") space to recognize (where "n" is the input size). In other words, DSPACE(o(log log "n")) equals the class of regular languages. In practice, most nonregular problems are solved by machines taking at least logarithmic space.
Subclasses.
Important subclasses of regular languages include
The number of words in a regular language.
Let formula_16 denote the number of words of length formula_17 in formula_18. The ordinary generating function for "L" is the formal power series
The generating function of a language "L" is a rational function if "L" is regular. Hence for any regular language formula_18 there exist an integer constant formula_21, complex constants formula_22 and complex polynomials formula_23
such that for every formula_24 the number formula_16 of words of length formula_17 in formula_18 is
formula_28.
Thus, non-regularity of certain languages formula_29 can be proved by counting the words of a given length in
formula_29. Consider, for example, the Dyck language of strings of balanced parentheses. The number of words of length formula_31
in the Dyck language is equal to the Catalan number formula_32, which is not of the form formula_33,
witnessing the non-regularity of the Dyck language. Care must be taken since some of the eigenvalues formula_34 could have the same magnitude. For example, the number of words of length formula_17 in the language of all even binary words is not of the form formula_33, but the number of words of even or odd length are of this form; the corresponding eigenvalues are formula_37. In general, for every regular language there exists a constant formula_38 such that for all formula_39, the number of words of length formula_40 is asymptotically formula_41.
The "zeta function" of a language "L" is
The zeta function of a regular language is not in general rational, but that of a cyclic language is.
Generalizations.
The notion of a regular language has been generalized to infinite words (see ω-automata) and to trees (see tree automaton).
Rational set generalizes the notion (of regular/rational language) to monoids that are not necessarily free. Likewise, the notion of a recognizable language (by a finite automaton) has namesake as recognizable set over a monoid that is not necessarily free. Howard Straubing notes in relation to these facts that “The term "regular language" is a bit unfortunate. Papers influenced by Eilenberg's monograph often use either the term "recognizable language", which refers to the behavior of automata, or "rational language", which refers to important analogies between regular expressions and rational power series. (In fact, Eilenberg defines rational and recognizable subsets of arbitrary monoids; the two notions do not, in general, coincide.) This terminology, while better motivated, never really caught on, and "regular language" is used almost universally.”
Rational series is another generalization, this time in the context of a formal power series over a semiring. This approach gives rise to weighted rational expressions and weighted automata. In this algebraic context, the regular languages (corresponding to Boolean-weighted rational expressions) are usually called "rational languages". Also in this context, Kleene's theorem finds a generalization called the Kleene-Schützenberger theorem.
References.
</dl>

</doc>
<doc id="25727" url="http://en.wikipedia.org/wiki?curid=25727" title="Reference work">
Reference work

A reference work is a book or periodical (or its electronic equivalent) to which one can refer to for confirmed facts. The information is intended to be found quickly when needed. Reference works are usually "referred" to for particular pieces of information, rather than read beginning to end. The writing style used in these works is informative; the authors avoid use of the first person, and emphasize facts. Many reference works are compiled by a team of contributors whose work is coordinated by one or more editors rather than by an individual author. Indices are commonly provided in many types of reference work. Updated editions are usually published as needed, in some cases annually (e.g. "Whitaker's Almanack", "Who's Who"). Reference works include dictionaries, thesauruses, encyclopedias, almanacs, bibliographies, and catalogs (e.g. catalogs of libraries, museums or the works of individual artists). Many reference works are available in electronic form and can be obtained as application software, CD-ROMs, DVDs, or online through the Internet.
Reference book.
In comparison, a reference book or reference-only book in a library is one that may only be used in the library and may not be borrowed from the library. Many such books are reference works (in the first sense), which are, usually, used briefly or photocopied from, and, therefore, do not need to be borrowed. Keeping them in the library assures that they will always be available for use on demand. Some reference-only books are too valuable to permit borrowers to take them out. Reference-only items may be shelved in a reference collection located separately from circulating items. Some libraries consist entirely, or to a large extent, of books which may not be borrowed
Electronic resources.
An electronic resource is a piece of information that is stored electronically, which is usually found on a computer, including information available on the internet . Libraries offer many types of electronic resources: subject research guides, indices, electronic books and texts, electronic journals, library catalogs, reference sources, statistical sources, sound recordings, and image databases .
Further reading.
Sheehy's Guide is less international in its scope than Walford: "It seems that Walford is a somewhat better balanced work than Winchell, and is certainly much more comprehensive"--"American Reference Books Annual", quoted in Walford, A. J. (1981) "Walford's Concise Guide to Reference Material". London: Library Association ISBN 0-85365-882-X; p. 19.

</doc>
<doc id="25730" url="http://en.wikipedia.org/wiki?curid=25730" title="Roger Casement">
Roger Casement

Roger David Casement (Irish: "Ruairí Dáithí Mac Easmainn"; 1 September 1864 – 3 August 1916) – known as Sir Roger Casement CMG between 1911 and shortly before his execution for treason, when he was stripped of his knighthood – was an Irish diplomat, human rights activist, Irish nationalist and a poet. Described as the "father of twentieth-century human rights investigations," he was knighted for his important investigations of human rights abuses in Peru and awarded honours for his report on the Congo. These achievements became overshadowed by his efforts during the Great War to gain German collaboration for an armed uprising in Ireland to gain independence.
In Africa as a young man, Casement first worked for commercial interests before joining the British Colonial Service. In 1891 he was appointed as a British consul, a profession he followed for more than 20 years. Influenced by the Boer War and his investigation into colonial atrocities against indigenous peoples, Casement developed anti-imperialist opinions. After retiring from the consular service in 1913, he became more involved with the Irish Republican and separatist movement. He sought to obtain German support for an armed rebellion in Ireland against British rule during the Great War. He was arrested, convicted and executed for treason.
During this period, the government circulated excerpts from his private journals, known as the "Black Diaries," which detailed homosexual activities. Given prevailing views and existing laws on homosexuality, this material undermined support for clemency for Casement. Debates have continued about these diaries: a forensic study concluded in 2002 that Casement had written them, but interpretations differ as to their meaning in his life.
Early life and education.
Roger Casement was born near Dublin, living in very early childhood at Doyle's Cottage, Lawson Terrace, Sandycove.
His Protestant father, Captain Roger Casement of (The King’s Own) Regiment of Dragoons, was the son of a bankrupt Belfast shipping merchant (Hugh Casement), who later moved to Australia. Captain Casement had served in the 1842 Afghan campaign. He traveled to Europe to fight as a volunteer in the Hungarian Revolution of 1848 but arrived after the Surrender at Világos.
Roger Casement's mother, Anne Jephson (or Jepson) of a Dublin Anglican family had him rebaptised secretly in Rhyl, Wales as a Roman Catholic when he reached the age of three, after they had moved to England. According to an 1892 letter, Casement believed that she was descended from the Jephson family of Mallow, County Cork. However, the Jephson family's historian provides no evidence of this. The family lived in England in Worthing in a kind of genteel poverty; their mother Anne died when Roger was nine. They returned to Ireland, to County Antrim in Ulster to live near paternal relatives. By the time Casement was 13 years old, his father was also dead, having ended his days in Ballymena dependent on the charity of relatives.
After his father's death, Roger and his brother Tom were looked after by paternal relatives, the Youngs of Galgorm Castle in Ballymena and the Casements of Magherintemple. He was educated at the Diocesan School, Ballymena, later the Ballymena Academy. He left school at the age of 16 and went to England for work. There he took a clerical job with Elder Dempster, a Liverpool shipping company headed by Alfred Lewis Jones.
The Congo and the Casement Report.
Casement went to the Congo, where he was working for Henry Morton Stanley and the African International Association from 1884; it was a front for King Leopold II in his takeover of the Congo Free State. He worked on a survey to improve communication and began to recruit and supervise labor for construction of a railroad to bypass the lower 220 miles of the Congo River, which is filled with cataracts. During his commercial work, Casement learned more than one African language.
In 1890 he met Joseph Conrad, who had come to the Congo to use a merchant ship, "Le Roi des Belges," to recover a European from a trading post on the upper reaches of the Congo River. Each man had come inspired by the idea that "European colonization would bring moral and social progress to the continent and free its inhabitants “from slavery, paganism and other barbarities.” Each man would soon learn the gravity of his error." Conrad published his short novel, "Heart of Darkness," in 1899. Casement would take on a different kind of writing to expose what he found during an official investigation for the British government. In these formative years, he also met Herbert Ward, and they became nearly lifelong friends. Ward left Africa in 1889, and devoted his time to becoming an artist, but his experience there strongly influenced his work.
Casement joined the Foreign Office (British Colonial Service), first serving as a clerk in British West Africa (which later became the independent countries of The Gambia, Sierra Leone, Ghana, and Nigeria), before being appointed in August 1901 as British consul in the eastern part of French Congo. In 1903 the British government commissioned Casement, then the British consul at Boma in the Congo Free State, to investigate the human-rights situation in that Belgian colony of Leopold II. Setting up a private army known as the Force Publique, Léopold II had squeezed revenue out of the people through a reign of terrorism in the harvesting and export of rubber and other resources. In trade, Belgium shipped guns, whips ("cocote") and other materials to the Congo, used chiefly to suppress the native peoples. Casement traveled for weeks in the upper Congo Basin to interview people throughout the region, including workers, overseers and mercenaries. He delivered a long, detailed eyewitness report to the Crown that exposed abuses: "the enslavement, mutilation, and torture of natives on the rubber plantations," the "Casement Report" of 1904. The Congo Free State had been in the possession of King Leopold II of Belgium since 1885, when the Berlin Conference of European powers and the United States effectively gave him free rein in the area.
Leopold had exploited the territory's natural resources (mostly rubber) as a private entrepreneur, not as king of the Belgians. Using violence and murder against men and their families, Leopold's private "Force Publique" had decimated many native villages in the course of forcing the men to gather rubber and abusing them to increase productivity. Casement's report provoked controversy, and some companies with a business interest in the Congo rejected its findings, as did Casement's former boss, Alfred Lewis Jones. In the longer term, Casement's report would prove instrumental in gaining international pressure that forced Leopold in 1908 to relinquish his personal holdings in Africa.
When the report was made public, opponents of Leopold formed interest groups, such as the Congo Reform Association, founded by E. D. Morel with Casement's support, and demanded action to relieve the situation of the natives. Other European nations followed suit, as did the United States; and the British Parliament demanded a meeting of the 14 signatory powers to review the 1885 Berlin Agreement defining interests in Africa. The Belgian Parliament, pushed by Socialist leader Emile Vandervelde and other critics of the king's Congolese policy, forced Léopold to set up an independent commission of inquiry. In 1905, despite Léopold's efforts, it confirmed the essentials of Casement's report. On 15 November 1908, the parliament of Belgium took over the Congo Free State from Léopold and organised its administration as the Belgian Congo.
Peru: Abuses against the Putumayo Indians.
In 1906 the Foreign Office sent Casement to Brazil: first as consul in Pará, then transferred to Santos, and lastly promoted to consul-general in Rio de Janeiro. He was attached as a consular representative to a commission investigating murderous rubber slavery by the Peruvian Amazon Company (PAC), which had been registered in Britain in 1908 and had a British board of directors and numerous stockholders. In September 1909 journalist Sidney Paternoster wrote in "Truth", a British muckraking magazine, of abuses against PAC workers and competing Colombians in the disputed region of the Peruvian Amazon. In addition, the British consul at Iquitos had said that Barbadians, considered British subjects as part of the empire, had been ill-treated while working for PAC, which gave the government a reason to intervene. Ordinarily it could not investigate the internal affairs of another country. American civil engineer Walter Hardenburg had told Paternoster of witnessing a joint PAC and Peruvian military action against a Colombian rubber station, which they destroyed, stealing the rubber. He also saw Peruvian Indians whose backs were marked by severe whipping, in a pattern called the Mark of Arana, and reported other abuses.
PAC, with its operational headquarters in Iquitos, dominated the city and the region. The area was separated from the main population of Peru by the Andes, and it was 1900 miles from the Amazon's mouth at Pará. The British-registered company was effectively controlled by the archetypal rubber baron Julio Cesar Arana and his brother. Born in Lima, Arana had wrested his way up from poverty to own and operate a company harvesting great quantities of rubber in the Peruvian Amazon, which was much in demand on the world market. The rubber boom had led to expansion in Iquitos as a trading center, as all the company rubber was shipped down the Amazon River from there to the Atlantic port. Numerous foreigners had flocked to the area seeking their fortunes in the rubber boom, or at least some piece of the business. The rough frontier city, both respectable businesses and the vice district, was highly dependent on the PAC.
Casement traveled to the Putumayo District, where the rubber was harvested deep in the Amazon Basin, and explored the treatment of the local Indians of Peru. The isolated area was outside the reach of the national government and near the border with Colombia, which periodically made incursions in competition for the rubber. For years, the Indians had been forced into unpaid labor by field staff of the PAC, who exerted absolute power over them and subjected them to near starvation, severe physical abuse, rape of women and girls by the managers and overseers, branding and casual murder. Casement found conditions similarly inhumane as those in the Congo. He interviewed both the Putumayo and men who had abused them, including three Barbadians who had also suffered from conditions of the company. When the report was publicized, there was public outrage in Britain over the abuses. Casement made two lengthy visits to the region, first in 1910 with a commission of investigators.
Casement's report has been described as a "brilliant piece of journalism", as he wove together first-person accounts by both "victims and perpetrators of atrocities." "Never before had distant colonial subjects been given such personal voices in an official document." After his report was made to the British government, the wealthy board members of the PAC were horrified by what they learned. Arana and the Peruvian government promised to make changes.
In 1911, the British government asked Casement to return to Iquitos and Putumayo to see if promised changes in treatment had occurred. In a report to the British foreign secretary, dated 17 March 1911, Casement detailed the rubber company's continued use of pillories to punish the Indians:
Men, women, and children were confined in them for days, weeks, and often months. ... Whole families ... were imprisoned—fathers, mothers, and children, and many cases were reported of parents dying thus, either from starvation or from wounds caused by flogging, while their offspring were attached alongside of them to watch in misery themselves the dying agonies of their parents.
After his return to Britain, Casement repeated his extra-consular campaigning work by organising Anti-Slavery Society and Catholic mission interventions in the region. Some of the company men exposed as killers in his 1910 report were charged by Peru, while most fled the region and were never captured. Some entrepreneurs had smuggled out cuttings from rubber plants and began cultivation in southeast Asia in colonies of the British Empire. The scandal of the PAC caused major losses in business to the company, and rubber demand began to be met by farmed rubber in other parts of the world. With the collapse of business for PAC, most foreigners left Iquitos and it quickly returned to its former status as an isolated backwater. For a period, the Putumayo Indians were largely left alone.
Arana was never prosecuted as head of the company. After having lived in London for years, he returned to Peru. Despite the scandal associated with Casement's report and international pressure on the Peruvian government to change conditions, Arana later had a successful political career. He was elected as a senator and died in Lima, Peru in 1952 at age eighty-eight.
Casement wrote extensively for his private record (as always) in those two years. During this period he continued to write in his diaries, and the one for 1911 was described as being unusually discursive. He kept them in London along with the 1903 diary and other papers of the period, presumably so they could be consulted in his continuing work as "Congo Casement" and as the saviour of the Putumayo Indians. In 1911 Casement received a knighthood for his efforts on behalf of the Amazonian Indians, having been reluctantly appointed Companion of the Order of St Michael and St George (CMG) in 1905 for his Congo work.
Irish revolutionary.
In Ireland on leave from Africa in 1904-05, in 1904 Casement joined the Gaelic League. It was established in 1893 to preserve the Irish language. He later tried to learn Gaelic but had difficulty, despite his command of several languages and gift for them. He also met with the leaders of the Home Rule Irish Parliamentary Party (IPP) to lobby for his work in the Congo. He did not support those proposing Home Rule, as he felt that the House of Lords would always veto their efforts. He was more impressed by Arthur Griffith's new Sinn Féin party, which called for Irish independence by using a non-violent series of strikes and boycotts, modelled on the policy of Ferenc Deák in Hungary, and he joined it in 1905.
Increasingly committed to the cause of Irish independence, Casement retired from the British consular service in the summer of 1913. In November that year, he helped form the Irish Volunteers with Eoin MacNeill, later the organisation's chief of staff. They co-wrote the Volunteers' manifesto. In July 1914, Casement journeyed to the United States to promote and raise money for the Volunteers among the large and numerous ethnic Irish communities. Through his friendship with men such as Bulmer Hobson, who was a member of the Volunteers and the Irish Republican Brotherhood (IRB), Casement established connections with exiled Irish nationalists, particularly in "Clan na Gael."
Elements of the Clan did not trust him completely, as he was not a member of the IRB and held views considered by many to be too moderate, although others such as John Quinn regarded him as extreme. John Devoy, who was initially hostile to Casement for his part in conceding control of the Irish Volunteers to Redmond, in June was won over, and the more extreme Clan leader Joseph McGarrity became and remained devoted to Casement. The Howth gun-running in late July 1914, which Casement had helped to organise and finance, further enhanced his reputation.
In August 1914, at the outbreak of World War I, Casement and John Devoy arranged a meeting in New York with the Western Hemisphere’s top-ranking German diplomat, Count Bernstorff, to propose a mutually beneficial plan: if Germany would sell guns to the Irish rebels and provide military leaders, the rebels would stage a revolt against England, diverting troops and attention from the war on Germany. Bernstorff appeared sympathetic, but Casement and Devoy decided to send an envoy, Clan na Gael president John Kenny, to present their plan personally. Kenny, unable to meet the German Emperor, was given a warm reception by Flotow, the German ambassador to Italy, and by Prince von Bülow.
In October, Casement secretly sailed for Germany, via Norway. He was traveling in disguise and viewed himself as an ambassador of the Irish nation. While the journey was his idea, "Clan na Gael" financed the expedition. During their stop in Christiania, his companion Adler Christensen was taken to the British legation. According to him, a reward was offered if Casement was "knocked on the head".
The British minister, in contrast, advised London that Christensen had approached them and "implied that their relations were of an unnatural nature and that consequently he had great power over this man." It was this episode that first provided London with the intimation that Casement was homosexual.
In November 1914, Casement negotiated a declaration by Germany which stated, 
"The Imperial Government formally declares that under no circumstances would Germany invade Ireland with a view to its conquest or the overthrow of any native institutions in that country. Should the fortune of this Great War, that was not of Germany’s seeking, ever bring in its course German troops to the shores of Ireland, they would land there not as an army of invaders to pillage and destroy but as the forces of a Government that is inspired by goodwill towards a country and people for whom Germany desires only national prosperity and national freedom".In Berlin Casement negotiated with Arthur Zimmermann, then Under Secretary of State in the Foreign Office, and with the Imperial Chancellor, Theobald von Bethmann Hollweg.
Casement spent most of his time in Germany seeking to recruit an "Irish Brigade" from among more than 2,000 Irish prisoners-of-war taken in the early months of the war and held in the prison camp of Limburg an der Lahn. His plan was that they would be trained to fight against Britain in the cause of Irish independence.
During the Great War, Casement is also known to have been involved in the Hindu–German Conspiracy, recommending Joseph McGarrity to Franz von Papen as an intermediary for the plot. The Indian nationalists may also have followed Casement's strategy in trying to recruit from among Indian prisoners of war to fight for Indian independence.
Both efforts proved unsuccessful. The Irish plan failed. All Irishmen fighting in the British army did so voluntarily. In addition to finding it difficult to ally with the Germans while held as prisoners, potential recruits to Casement's brigade knew they would be liable to the death penalty as traitors if Britain won the war. He abandoned this effort after much time and money were wasted. The Germans were sceptical of Casement but aware of the military advantage they could gain from an uprising in Ireland. In April 1916 they offered the Irish 20,000 Mosin–Nagant 1891 rifles, ten machine guns and accompanying ammunition, and no German officers; it was a fraction of the quantity of the arms Casement had hoped for, with no military support. Michael McKeogh, recruiting officer and Sergeant Major in the Irish Brigade in Germany and Casement’s adjutant, left papers and a manuscript about this unit, which was the basis of a non-fiction book published in 2009.
Casement did not learn about the Easter Rising until after the plan was fully developed. The IRB purposely kept him in the dark, and tried to replace him. Casement may never have learned that it was not the Volunteers who were planning the rising, but IRB members such as Patrick Pearse and Tom Clarke who were pulling the strings behind the scenes.
The German weapons were never landed in Ireland. The ship transporting them, a German cargo vessel called "Libau", was intercepted, although it had been thoroughly disguised as a Norwegian vessel, "Aud-Norge". All the crew were German sailors, but their clothes and effects, even the charts and books on the bridge, were Norwegian. The British had intercepted German communications coming from Washington and knew there was going to be an attempt to land arms at Ireland, even if the Royal Navy was not precisely aware of the location. The arms ship, under Captain Karl Spindler, was apprehended by HMS "Bluebell" on the late afternoon of Good Friday. About to be escorted into Queenstown (now Cobh, County Cork) on the morning of Saturday, 22 April, after surrendering, the "Aud Norge" was scuttled by pre-set explosive charges. She lies at 40 metres depth. Her crew became prisoners of war.
Capture, trial and execution.
Casement confided his personal papers to Dr. Charles Curry, with whom he had stayed at Riederau on the Ammersee, before he left Germany. He departed with Robert Monteith and Sergeant Daniel Beverley (Bailey) of the Irish Brigade in a submarine, initially the SM "U-20", which developed engine trouble, and then the SM "U-19", shortly after the "Aud" sailed.
According to Monteith, Casement believed that the Germans were toying with him from the start and providing inadequate aid that would doom a rising to failure. He wanted to reach Ireland before the shipment of arms and convince Eoin MacNeill (whom he believed to be still in control) to cancel the rising. Casement sent John McGoey, a recently arrived Irish American, through Denmark to Dublin, ostensibly to advise of what military aid was coming from Germany and when, but with Casement's orders "to get the Heads in Ireland to call off the rising and merely try to land the arms and distribute them". McGoey did not reach Dublin, nor did his message. His fate was unknown until recently. He joined the Royal Navy in 1916, survived the war, and later returned to the United States. There he died in a 1925 building accident. Despite Monteith's view, Casement expected to be involved in the rising if it went ahead.
In the early hours of 21 April 1916, three days before the rising began, Casement was taken by a German submarine and was put ashore at Banna Strand in Tralee Bay, County Kerry. Too weak to travel, he was discovered at McKenna's Fort (an ancient ring fort now called Casement's Fort) in Rathoneen, Ardfert, and subsequently arrested on charges of treason, sabotage and espionage against the Crown. He was taken straight to the Tower of London where he was imprisoned. He sent word to Dublin about the inadequate German assistance. The Kerry Brigade of the Irish Volunteers might have tried to rescue him over the next three days, but was ordered by its leadership in Dublin to "do nothing".
At Casement's highly publicised trial for treason, the prosecution had trouble arguing its case. Casement's crimes had been carried out in Germany and the Treason Act 1351 seemed to apply only to activities carried out on English (or, arguably, British) soil. A close reading of the Act allowed for a broader interpretation: the court decided that a comma should be read in the unpunctuated original Anglo-French text, crucially widening the sense so that "in the realm or elsewhere" referred to where acts were done and not just to where the "King's enemies" may be. This led to the claim that Casement was "hanged on a comma".
During the trial and appeal, the government secretly circulated excerpts of Casement's journals, revealed his homosexuality and numerous explicit accounts of sexual activity, which became known as the "Black Diaries," to influence those notables of the day who might have intervened. Given societal views and the illegality of homosexuality at the time, support for Casement declined among some readers.
Casement unsuccessfully appealed against the conviction and death sentence. Among the many people who pleaded for clemency were Sir Arthur Conan Doyle, who was acquainted with Casement through the work of the Congo Reform Association, the Anglo-Irish poet W. B. Yeats, and the playwright George Bernard Shaw. Edmund Dene Morel could not visit Casement in prison, being under attack for his own pacifist position. On the other hand, the author Joseph Conrad could not forgive Casement for his treachery towards Britain, nor could Casement's longtime friend, the sculptor Herbert Ward, whose son Charles was killed in the war in 1916. Members of the Casement family in Antrim contributed discreetly to the defence fund, although they had sons in the British Army and Navy.
On the day of his execution, Casement was received into the Catholic Church and was attended by two Irish Catholic priests Dean Timothy Ring and Father James Carey, from the parish of St Mary and St Michael's, East London.https://en.wikipedia.org/w/index.php?title=Roger_Casement&action=edit&section=5#
The latter, also known as James McCarroll, said of Casement that he was "a saint ... we should be praying to him [Casement] instead of for him". Casement was hanged by John Ellis and his assistants at Pentonville Prison in London on 3 August 1916, at the age of 51.
The Black Diaries and Casement's sexuality.
The Black Diaries are a set of diaries, claimed to have been written by Casement and covering the years 1903, 1910 and 1911 (twice). "His homosexual life was almost entirely out of
sight and disconnected from his career and political work" which has caused controversy since his death. If genuine, the diaries portrayed Casement as a promiscuous homosexual who had a fondness for young men and mostly paid for sex. In 1916 after Casement's conviction for treason, the British government circulated photographs of pages of the diary to individuals who were urging commutation of Casement's death sentence. At a time of strong social conservatism, not least among Irish Catholics, publicizing of the "Black Diaries" and his homosexuality undermined support for Casement.
The question of whether the diaries are genuine or forgeries has been much debated. The diaries were declassified for public inspection in August 1959. The original diaries may be seen at the British National Archives in Kew. Historians and biographers of Casement's life have taken opposing views, with Roger McHugh in 1976 and Angus Mitchell in 2000 and later arguing the diaries were forged; Mitchell wrote several articles in 2012 in "Field Day Review" of Notre Dame University.
A detailed forensic investigation in 2002 had concluded that the diaries had been written by Casement, but this has not settled questions of his intent, truthfulness and other elements of his writing.
Peruvian novelist Mario Vargas Llosa is noted as presenting a mixed account of Casement's sexuality in his 2010 novel, "The Dream of the Celt," suggesting that the nationalist wrote partially fictional diaries of what he wished had taken place in homosexual encounters. Jeffrey Dudgeon in a 2013 article suggests that some Irish, including biographers, needed Casement to be "sexless" to fit with his Catholic martyr role in the nationalist movement. He writes, "The evidence that Casement was a busy homosexual is in his own words and handwriting in the diaries, and is colossally convincing because of its detail and extent." Dudgeon relates the cult of a "sexless" Casement to an intense Irish Catholicism that was strong in the nation for 50 years. In addition, "he was not just a party to the founding of the state, he was himself a saintly martyred figure, a humanitarian who sacrificed himself for others, both in Ireland and beyond."
State funeral.
As was the custom at the time, Casement's body was buried in quicklime in the prison cemetery at the rear of Pentonville Prison, where he was hanged.
In 1965, his remains were repatriated to the Republic of Ireland, which had gained effective independence in 1922. Despite the withdrawal of his knighthood in 1916, the 1965 British Cabinet record of the repatriation decision refers to him as "Sir" Roger Casement. Casement's last wish, to be buried at Murlough Bay on the North Antrim coast, may never be satisfied. Prime Minister Harold Wilson's government released the remains only on condition that they not be brought into Northern Ireland, as "the government feared that a reburial there could provoke Catholic celebrations and Protestant reactions."
Casement's remains lay in state at Arbour Hill for five days, during which time an estimated half a million people filed past his coffin. After a state funeral, the remains were buried with full military honours in the Republican section with other national heroes in Glasnevin Cemetery in Dublin. The President of Ireland, Éamon de Valera, who in his mid-eighties was the last surviving leader of the Easter Rising, defied the advice of his doctors and attended the ceremony, along with an estimated 30,000 Irish citizens.
Legacy.
Quotations.
Self-government is our right, a thing born in us at birth; a thing no more to be doled out to us or withheld from us by another people than the right to life itself.
Landmarks, buildings and organisations.
Many landmarks, buildings and organisations in Ireland are named after Casement including:
Representation in culture.
Casement has been the subject of ballads, poetry, novels, and TV series since his death, including:
Bibliography.
By Roger Casement:
"Secondary Literature, and other materials cited in this entry":

</doc>
<doc id="25731" url="http://en.wikipedia.org/wiki?curid=25731" title="Real Irish Republican Army">
Real Irish Republican Army

The Real Irish Republican Army or Real IRA, also referred to as the New IRA, is an Irish republican paramilitary organisation which aims to bring about a united Ireland. It formed in 1997 following a split in the Provisional IRA, which had declared a ceasefire that year. Like the Provisional IRA before it, the RIRA sees itself as the only rightful successor to the original Irish Republican Army and styles itself as simply "the Irish Republican Army" in English or "Óglaigh na hÉireann" in Irish. It is an illegal organisation in the Republic of Ireland and designated as a terrorist organisation in the United Kingdom and the United States.
Since its formation, the RIRA has waged a campaign in Northern Ireland against the British Army and the Police Service of Northern Ireland (PSNI), formerly the Royal Ulster Constabulary (RUC). The RIRA is the biggest and most active of the "dissident republican" paramilitaries operating against the British security forces. It has targeted the security forces in gun attacks and bombings, as well as with grenades, mortars and rockets. The organisation has also been responsible for a number of bombings in Northern Ireland and England with the goal of causing economic harm and/or disruption. The most notable of these was the 15 August 1998 Omagh bombing, which killed 29 people. After the bombing, the RIRA went on ceasefire, but began operations again in 2000. In March 2009, it claimed responsibility for an attack on Massereene Barracks that killed two British soldiers, the first to be killed in Northern Ireland since 1997.
The Real IRA has also been involved in vigilantism, mainly against alleged drug dealers and organized crime gangs. In Dublin particularly, it has been accused of extorting and engaging in feuds with these gangs.
In July 2012, it was reported that Republican Action Against Drugs (RAAD) and other small republican militant groups were merging with the Real IRA. As before, the group continues to refer to itself as "the Irish Republican Army".
Origins.
In July 1997, the Provisional IRA called a ceasefire. On 10 October 1997, a Provisional IRA General Army Convention was held in Falcarragh, County Donegal. At the convention, Provisional IRA Quartermaster General Michael McKevitt, also a member of the 12-person Provisional IRA Executive, denounced the leadership and called for an end to the group's ceasefire and to the participation in the Northern Ireland peace process. He was backed by his partner and fellow Executive member Bernadette Sands-McKevitt. The pair were outmanoeuvred by the leadership, and a key ally, Kevin McKenna, was voted off the IRA Army Council leaving the two dissidents isolated. The convention backed the pro-ceasefire line, and on 26 October McKevitt and Sands-McKevitt resigned from the Executive along with several other members.
In November 1997, McKevitt and other dissidents held a meeting in a farmhouse in Oldcastle, County Meath, and a new organisation, styling itself "Óglaigh na hÉireann", was formed. The organisation attracted disaffected Provisional IRA members from Derry and the republican stronghold of South Armagh, as well as other areas including Dublin and Belfast cities and Counties Limerick, Tipperary, Louth, Tyrone, Monaghan, and Kerry.
The name "Real IRA" entered common usage when members had a roadblock in Jonesborough, County Armagh and told motorists "We're from the IRA. The "real" IRA".
Objectives.
The RIRA's ultimate objective is a united Ireland by forcing the end of British sovereignty over Northern Ireland through the use of physical force. The organisation rejects the Mitchell Principles and the Good Friday Agreement, comparing the latter to the 1921 Anglo-Irish Treaty which resulted in the partition of Ireland. The organisation aims to uphold an uncompromising form of Irish republicanism and opposes any political settlement that falls short of Irish unity and independence.<ref name="e316/7">English, pp. 316–317.</ref>
Sands-McKevitt, sister of hunger striker Bobby Sands and a founder of the RIRA's political wing, the 32 County Sovereignty Movement, said in an interview that "Bobby did not die for cross-border bodies with executive powers. He did not die for nationalists to be equal British citizens within the Northern Ireland state". The RIRA adopts similar tactics to those used by the Provisional IRA in the 1990s, primarily using bombs in town centres to damage the economic infrastructure of Northern Ireland. The organisation also attempts to kill members of the security forces using land mines, home-made mortars and car bombs, and targets England using incendiary and car bombs to "spread terror and disruption".
Campaign.
Early campaign.
The organisation's first action was an attempted bombing in Banbridge, County Down on 7 January 1998. The plot involved a 300 lb car bomb, but it was thwarted after being defused by security forces. The organisation continued its campaign in late February, with bombings in Moira, County Down and Portadown, County Armagh. On 9 May the organisation formally announced its existence in a coded telephone call to Belfast media claiming responsibility for a mortar attack on a police station in Belleek, County Fermanagh.
The organisation also carried out attacks in Newtownhamilton and Newry, and a second attack in Banbridge on 1 August injured 35 people and caused £3.5 million of damage when a 500 lb car bomb exploded. Despite these attacks the RIRA lacked a significant base and was heavily infiltrated by informers. This led to a series of high-profile arrests and seizures by the Garda Síochána in the first half of 1998, including the death of member Rónán Mac Lochlainn who was shot dead trying to escape from police following an attempted robbery of a security van in County Wicklow.
Omagh bombing.
On 15 August 1998 the RIRA left a car containing 500 lb of home-made explosives in the centre of Omagh, County Tyrone. The bombers could not find a parking space near the intended target of the courthouse, and the car was left 400 metres away. As a result three inaccurate telephone warnings were issued, and the Royal Ulster Constabulary (RUC) believed the bomb was actually located outside the courthouse. They attempted to establish a security cordon to keep civilians clear of the area, which inadvertently pushed people closer to the actual location of the bomb. Shortly after, the bomb exploded killing 29 people and injuring 220 others, in what became the single deadliest strike of the Troubles.
The bombing caused a major outcry throughout the world, and the Irish and British governments introduced new legislation in an attempt to destroy the organisation. The RIRA also came under pressure from the Provisional IRA, when Provisional IRA members visited the homes of 60 people connected with the RIRA and ordered them to disband and stop interfering with Provisional IRA arms dumps. With the organisation under intense pressure, which included McKevitt and Sands-McKevitt being forced from their home after the media named McKevitt in connection with the bombing, the RIRA called a ceasefire on 8 September.
Ceasefire.
Following the declaration of the ceasefire the RIRA began to regroup, and by the end of October had elected a new leadership and were planning their future direction. In late December Irish government representative Martin Mansergh held a meeting with McKevitt in Dundalk, in an attempt to convince McKevitt to disband the RIRA. McKevitt refused, stating that members would be left defenceless to attacks by the Provisional IRA. In 1999 the RIRA began preparations for a renewed campaign, and in May three members travelled across Europe to Split in Croatia to purchase arms which were subsequently smuggled back to Ireland. On 20 October ten people were arrested when Gardaí raided a RIRA training camp near Stamullen, County Meath.
Officers found a firing range inside a disused wine cellar being used as an underground bunker, and seized weapons including an assault rifle, a submachine gun, a semi-automatic pistol and an RPG-18 rocket launcher. An earlier version of the rocket launcher, the RPG-7, had been in the possession of the Provisional IRA from as early as 1972, but this was the first time the RPG-18 had been found in the possession of a paramilitary organisation in Ireland. Among those convicted were Alan Ryan, who was on bail for possession of a loaded revolver at his home in Dublin.
Return to activity.
On 20 January 2000 the RIRA issued a call-to-arms in a statement to the "Irish News". The statement condemned the Northern Ireland Executive, and stated: "Once again, Óglaigh na hÉireann declares the right of the Irish people to the ownership of Ireland. We call on all volunteers loyal to the Irish Republic to unite to uphold the Republic and establish a permanent national parliament representative of all the people." The RIRA launched its new campaign on 25 February with an attempted bombing of Shackleton Army Barracks in Ballykelly. The bombers were disturbed as they were assembling the device, which would have caused mass murder if detonated, according to soldiers.
On 29 February a rocket launcher similar to one seized in the 1999 raid was found near an army base in Dungannon, County Tyrone, and on 15 March three men were arrested following the discovery of 500 lb of home-made explosives when the RUC searched two cars in Hillsborough, County Down. On 6 April a bomb attack took place at Ebrington Barracks in Derry. RIRA members lowered a device consisting of 5 lb of homemade explosives over the perimeter fence using ropes, and the bomb subsequently exploded damaging the fence and an unmanned guardhouse.
Bombings in England.
After the Omagh bombing, the RIRA leadership were unwilling to launch a full-scale campaign in Northern Ireland due to the possibility of civilians being killed. Instead they decided to launch a series of attacks in England, in particular London, which they hoped would attract disenchanted Provisional IRA members to join the RIRA. On 1 June 2000 a bomb damaged Hammersmith Bridge; a symbolic target for Irish republican paramilitary groups. The bridge had previously been targeted by the Irish Republican Army on 29 March 1939 as part of its Sabotage Campaign, and by the Provisional IRA on 24 April 1996.
One month later on 19 July, security forces carried out a controlled explosion on a bomb left at Ealing Broadway station and public transport was disrupted when the Metropolitan Police closed Victoria and Paddington train stations and halted services on the London Underground. On 21 September a rocket-propelled grenade was fired at the MI6 headquarters using an RPG-22 rocket launcher, which generated headlines around the world. On 21 February 2001 a bomb disguised as a torch left outside a Territorial Army base in Shepherd's Bush seriously injured a 14-year-old cadet, who was blinded and had his hand blown off. A second attack in Shepherd's Bush, the 4 March BBC bombing, injured a civilian outside the BBC Television Centre. The explosion was captured by a BBC cameraman, and the footage was broadcast on TV stations worldwide, and gained mass publicity for the group. On 14 April a bomb exploded at a postal sorting office in Hendon, causing minor damage but no injuries. Three weeks later on 6 May a second bomb exploded at the same building, causing slight injuries to a passer-by. The 3 August Ealing bombing injured seven people, and on 3 November a car bomb containing 60 lb of home-made explosives was planted in the centre of Birmingham. The bomb did not fully detonate and no one was injured.
Renewed campaign in Northern Ireland.
The successful attack on Hammersmith Bridge encouraged the RIRA leadership to launch further attacks in Northern Ireland. On 19 June 2000 a bomb was found in the grounds of Hillsborough Castle, home of Secretary of State for Northern Ireland Peter Mandelson. On 30 June a bomb exploded on the Dublin-to-Belfast railway line near the village of Meigh in County Armagh. The explosion damaged the tracks, and caused disruption to train services. On 9 July a car bomb damaged buildings in Stewartstown, County Tyrone including an RUC station, and on 10 August an attack in Derry was thwarted by the RUC after a van containing a 500 lb bomb failed to stop at a police checkpoint. Following a car chase the bombers escaped across the Irish border, and the Irish Army carried out a controlled explosion on the bomb after the van was found abandoned in County Donegal. On 13 September two 80 lb bombs were planted at the Magilligan army camp in County Londonderry, one of which was planted in a wooden hut and partially exploded when a soldier opened the door to the hut. The second bomb was found during a follow-up search and made safe by bomb disposal experts. On 11 November the RUC and British Army prevented a mortar attack after stopping a van near Derrylin, County Fermanagh, and the RUC prevented a further attack on 13 January 2001 when an 1100 lb bomb was found in Armagh — the largest bomb found in several years according to the RUC.
On 23 January the RIRA attacked Ebrington Army Barracks in Derry for a second time, firing a mortar over a perimeter fence. A mortar similar to the one used in the attack was found by Gardaí near Newtowncunningham on 13 February, and British army bomb disposal experts made safe another mortar found between Dungannon and Carrickmore on 12 April. On 1 August a 40 lb bomb was discovered in a car at the long stay car park of Belfast International Airport following a telephone warning, and was made safe with two controlled explosions by bomb disposal experts. In December a six-day security operation ended when a 70 lb bomb found under railway tracks at Killeen Bridge near Newry was successfully defused. The operation began following a number of telephone warnings, and both the road and railway line connecting Newry to Dundalk were closed due to security alerts. A pipe bomb was discovered at a police officer's home in Annalong, County Down on 3 January 2002, and two teenage boys were injured in County Armagh on 2 March when a bomb hidden in a traffic cone exploded. On 29 March 2002 the RIRA targeted a former member of the Royal Irish Regiment from Sion Mills, County Tyrone, with a bomb attached to his car that failed to explode. On 1 August 2002 a civilian worker was killed by an explosion at a Territorial Army base in Derry. The man, a 51-year-old former member of the Ulster Defence Regiment, was the thirtieth person killed by the RIRA.
Arrests.
Despite the RIRA's renewed activity, the organisation became increasingly weaker due to the arrest of key members and continued infiltration by informers. McKevitt was arrested on 29 March 2001 and charged with membership of an illegal organisation and directing terrorism, and remanded into custody. In July 2001, following the arrests of McKevitt and other RIRA members, British and Irish government sources hinted that the organisation was now in disarray. Other key figures were jailed, including the RIRA's Director of Operations, Liam Campbell, who was convicted of membership of an illegal organisation, and Colm Murphy who was convicted of conspiring to cause the Omagh bombing, although this conviction was later overturned on appeal.
On 10 April 2002, Ruairi Convey, from Donaghmede, Dublin was jailed for three years for membership of the RIRA. During a search of his home a list of names and home addresses of members of the Gardaí's Emergency Response Unit was found. Five RIRA members were also convicted in connection with the 2001 bombing campaign in England, and received sentences varying from 16 years to 22 years imprisonment. In October 2002, McKevitt and other RIRA members imprisoned in Portlaoise Prison issued a statement calling for the organisation to stand down. After a two-month trial, McKevitt was sentenced to twenty years imprisonment in August 2003 after being convicted of directing terrorism.
2002-2007.
After McKevitt's imprisonment, the RIRA regrouped. The RIRA claimed responsibility for a series of firebomb attacks against premises in Belfast in November 2004, and an attack on a Police Service of Northern Ireland (PSNI) patrol in Ballymena during March 2006 was attributed to the RIRA by the Independent Monitoring Commission (IMC). On 9 August 2006 a number of fire bomb attacks by the RIRA hit businesses in Newry, County Down. Buildings belonging to JJB Sports and Carpetright were destroyed, and ones belonging to MFI and TK Maxx were badly damaged. On 27 October 2006, a large amount of explosives was found in Kilbranish, Mount Leinster, County Carlow by police, who believe the RIRA were trying to derail the peace process with a bomb attack. The IMC believe the RIRA were also responsible for a failed mortar attack on Craigavon PSNI Station on 4 December 2006. The IMC's October 2006 report stated that the RIRA remains "active and dangerous" and that it seeks to "sustain its position as a terrorist organisation". The RIRA has previously stated it has no intention of calling a ceasefire unless a declaration of intent to withdraw from Northern Ireland is made by the British Government.
In a lengthy interview with An Phoblacht newspaper in 2003, the leadership of the Provisional IRA said that the RIRA had "no coherent strategy".
The Real IRA were suspected of complicity in the murder in December 2006 of drug-dealer Martin 'Marlo' Hyland. Hyland was shot dead at his Dublin home, along with a plumber, Anthony Campbell, who was carrying out work at the house. The organisation was embroiled in a feud with Hyland's gang at the time.
2007-2013.
On 8 November 2007 two RIRA members shot an off-duty PSNI officer as he sat in his car on Bishop Street in Derry, causing injuries to his face and arm. On 12 November another PSNI member was shot by RIRA members in Dungannon, County Tyrone. On 7 February 2008, the RIRA stated that, after experiencing a three-year period of reorganisation, it intends to "go back to war" by launching a new offensive against "legitimate targets". It also, despite having initially apologised for the Omagh bombing, denied any large scale involvement with the attack and said that their part had only gone as far as their codeword being used. On 12 May 2008 the RIRA seriously injured a member of the PSNI when a booby trap bomb exploded underneath his car near Spamount, County Tyrone. On 25 September 2008 the RIRA shot a man in the neck in St Johnston, near the Derry border. The same man was targeted in a pipe bomb attack on his home on 25 October, the RIRA did not claim responsibility for the attack, but security forces believe they were responsible for it.
On 7 March 2009, the RIRA claimed responsibility for the 2009 Massereene Barracks shooting. This shooting occurred outside the Massereene Barracks as four soldiers were receiving a pizza delivery. Two soldiers were killed, and the other two soldiers and two deliverymen were injured. On 3 April 2009 the RIRA in Derry claimed responsibility for carrying out a punishment shooting against a convicted rapist who was awaiting sentencing for raping a 15-year-old girl. The RIRA were also blamed for orchestrating rioting in the Ardoyne area of Belfast on 13 July 2009 as an Apprentice Boys parade was passing. A number of PSNI officers were injured in the rioting and at least one shot was fired at police. In early November, the Independent Monitoring Commission released a report stating that the threat from the RIRA and other dissident republicans was at its most serious level since the 1998 Good Friday Agreement.
When drug dealer Sean Winters was shot dead in Portmarnock, north Dublin, in September 2010, the Real IRA "emerged as the chief suspects". They were also suspected of shooting dead drugs gang leader Michael Kelly in Coolock in September 2011.
On 5 October 2010, a car bomb exploded outside a branch of the Ulster Bank on Culmore Road in Derry. Two police officers were slightly injured in the blast, which also damaged a hotel and other businesses. Several telephone warnings were received an hour prior to the blast allowing police to cordon off the area. The RIRA later claimed responsibility in a telephone call to the "Derry Journal".
A large Real IRA explosives dump and arms cache were discovered in Dunleer, County Louth, by Gardaí in October 2010, following a weekend of searches and arrests in the east of the country. In addition, two Real IRA men were charged in Dublin's non-jury Special Criminal Court of membership of an illegal organisation. At this time the Real IRA claimed responsibility for kidnapping and shooting dead of one of their members, Kieran Doherty, for alleged drug dealing. Further significant seizures of Real IRA arms and explosives were made by the Gardai during 2012 and 2013, leading to the arrest of over a dozen persons. In 2011 Michael Campbell, brother of Liam, was found guilty in Vilnius, Lithuania, of trying to purchase arms and explosives and was sentenced to twelve years in jail. In October 2013, Campbell was freed on appeal only to the have the Lithuanian Supreme Court order a retrial in June 2014. Campbell has maintained his innocence - accusing British intelligence of attempting to frame him. In June 2013 Gardai arrested eight people after a Real IRA meeting and uncovered a massive haul of the plastic explosive Semtex in two raids in Dublin. In October 2013 the Real IRA claimed responsibility for "executing" an alleged leading cocaine dealer in north Belfast. 
Since the merger ("New IRA").
On 26 July 2012, it was reported that Republican Action Against Drugs (RAAD) and other small republican militant groups were merging with the Real IRA. As before, the group would continue to refer to itself as "the Irish Republican Army". It issued the following statement:
The leadership of the Irish Republican Army remains committed to the full realisation of the ideals and principles enshrined in the Proclamation of 1916.
In recent years the establishment of a free and independent Ireland has suffered setbacks due to the failure among the leadership of Irish nationalism and fractures within republicanism. The root cause of conflict in our country is the subversion of the nation's inalienable right to self-determination and this has yet to be addressed. Instead the Irish people have been sold a phoney peace, rubber-stamped by a token legislature in Stormont.
Non-conformist republicans are being subjected to harassment, arrest and violence by the forces of the British crown; others have been interned on the direction of an English overlord. It is Britain, not the IRA, which has chosen provocation and conflict.
The IRA's mandate for armed struggle derives from Britain's denial of the fundamental right of the Irish people to national self-determination and sovereignty -- so long as Britain persists in its denial of national and democratic rights in Ireland the IRA will have to continue to assert those rights.
The necessity of armed struggle in pursuit of Irish freedom can be avoided through the removal of the British military presence in our country, the dismantling of their armed militias and the declaration of an internationally observed timescale that details the dismantling of British political interference in our country.
After the merger, the media began to refer to the group as the "New IRA". As well as RAAD, the alliance includes an east Tyrone group thought to be responsible for killing PSNI officer Ronan Kerr in 2011, and a Belfast group who badly wounded PSNI officer Peadar Heffron in 2010. The Continuity IRA, and the group often referred to as ONH, remain independent. The PSNI reckoned that the new group has a membership of "between 250 and 300 military activists, backed up by associates". In November 2012 it claimed responsibility for shooting dead a Prison Officer near Craigavon, the first prison officer to be killed since 1993.
On 3 September 2012 prominent Real IRA member Alan Ryan was shot dead in Dublin. Gardaí believed that he had been involved in a feud with major crime gangs from whom he was trying to extort money. In the aftermath of Ryan's death an internal feud developed in the Real IRA. Ryan's replacement as leader and another associate were shot, but not fatally, in November 2012, allegedly on the orders of the Northern leadership. In February 2013 several associates of Ryan were arrested for extortion in Sligo. In March 2013, another prominent ex-member of the Real IRA, Peter Butterly from Dunleer, was shot dead; three Dublin men, allegedly from the Alan Ryan faction, who were also charged with membership of an illegal organisation, were charged with his murder.
In February 2014 the group sent seven letter bombs to British Army recruitment offices in south-east England; the first time republicans had struck in Britain since 2001. The following month, a PSNI landrover was hit by an explosively formed projectile in Belfast. A civilian car was also hit by debris, but there were no injuries. The Real IRA claimed responsibility.
Structure and status.
The RIRA has a command structure similar to the Provisional IRA, with a seven-member Army Council consisting of a chief of staff, quartermaster general, director of training, director of operations, director of finance, director of publicity, and adjutant general. The rank-and-file members operate in active service units of covert cells to prevent the organisation from being compromised by informers. As of June 2005, the organisation is believed to have a maximum of about 150 members, according to a statement by the Irish Minister for Justice, Equality and Law Reform, Michael McDowell.
The RIRA also has a political wing, the 32 County Sovereignty Movement (formerly the 32 County Sovereignty Committee), led by Francis Mackey. The RIRA is distinct from the Continuity IRA, another Provisional IRA splinter group founded in 1986, although the two groups have been known to co-operate at a local level. The Provisional IRA has been hostile to the RIRA and issued threats to RIRA members, and in October 2000 was alleged to be responsible for the fatal shooting of Belfast RIRA member Joe O'Connor according to O'Connor's family and 32 County Sovereignty Movement member Marian Price.
The RIRA is an illegal organisation under Irish and UK law (section 11(1) of the Terrorism Act 2000) because of the use of 'IRA' in the group's name. Membership of the organisation is punishable by a sentence of up to ten years imprisonment under UK law. In 2001 the United States government designated the RIRA as a "Foreign Terrorist Organization" (FTO). This makes it illegal for Americans to provide material support to the RIRA, requires American financial institutions to freeze the group's assets, and denies suspected RIRA members visas into the United States.
Funding.
In 2014, "Forbes" magazine estimated the group's annual turnover at $50 million. According to the police in Northern Ireland, the main sources of the Real IRA's funding are illegal fuel operations and various smuggling activities.
Weaponry.
The RIRA initially took small amounts of materiel from Provisional IRA arms dumps under the control of McKevitt and other former Provisional IRA members, including the plastic explosive Semtex, Uzi submachine guns, AK-47 and AK-74 assault rifles, handguns, shotguns, detonators, and timing devices. The defection of senior Provisional IRA members also gave the RIRA the ability to manufacture home-made explosives and improvised mortars, including the Mark 15 mortar capable of firing a 200 lb shell.
In 1999 the organisation supplemented its equipment by importing arms from Croatia, including military explosive TM500, CZ Model 25 submachine guns, modified AK-47 assault rifles with a folding stock, and RPG-18 and RPG-22 rocket launchers. But a July 2000 attempt to smuggle a second consignment of arms was foiled by Croatian police, who seized seven RPG-18s, AK-47 assault rifles, detonators, ammunition, and twenty packs of TM500.
In 2001 RIRA members travelled to Slovakia to procure arms, and were caught in a sting operation by the British security agency MI5. The men attempted to purchase five tonnes of plastic explosives, 2,000 detonators, 500 handguns, 200 rocket-propelled-grenades, and also wire-guided missiles and sniper rifles. Three men from County Louth were arrested and extradited to the UK and subsequently imprisoned for 30 years each after pleading guilty to conspiring to cause explosions and other charges.
In June 2006, the PSNI made a number of arrests following an MI5 sting operation targeting a dissident republican gun smuggling plot. The RIRA had attempted to procure arms from France including Semtex and C-4 plastic explosives, SA-7 surface-to-air missiles, AK-47s, rocket launchers, heavy machine guns, sniper rifles, pistols with silencers, anti-tank weapons and detonators. On 30 June 2010, two of those arrested were found guilty following a trial by judge in Belfast. On 1 October 2010 one man was sentenced to 20 years imprisonment for attempting to import weapons and explosives, while the other was sentenced to 4 years imprisonment for making a Portuguese property available for the purpose of terrorism.

</doc>
<doc id="25732" url="http://en.wikipedia.org/wiki?curid=25732" title="Roy Chapman Andrews">
Roy Chapman Andrews

Roy Chapman Andrews (January 26, 1884 – March 11, 1960) was an American explorer, adventurer and naturalist who became the director of the American Museum of Natural History. He is primarily known for leading a series of expeditions through the fragmented China of the early 20th century into the Gobi Desert and Mongolia. The expeditions made important discoveries and brought the first-known fossil dinosaur eggs to the museum. His popular writings about his adventures made him famous.
Biography.
Early life and education.
Andrews was born on January 26, 1884, in Beloit, Wisconsin. As a child, he explored forests, fields, and waters nearby, developing marksmanship skills. He taught himself taxidermy and used funds from this hobby to pay tuition to Beloit College. After graduating, Andrews applied for work at the American Museum of Natural History in New York City. He so much wanted to work there that after being told that there were no openings at his level, Andrews took a job as a janitor in the taxidermy department and began collecting specimens for the museum. During the next few years, he worked and studied simultaneously, earning a Master of Arts degree in mammalogy from Columbia University.
Career.
From 1909 to 1910, Andrews sailed on the USS "Albatross" to the East Indies, collecting snakes and lizards and observing marine mammals. 
In 1913, he sailed aboard the schooner "Adventuress" with owner John Borden to the Arctic. They were hoping to obtain a bowhead whale specimen for the American Museum of Natural History. On this expedition, he filmed some of the best footage of seals ever seen, though did not succeed in acquiring a whale specimen.
He married Yvette Borup in 1914. From 1916 to 1917, Andrews and his wife led the Asiatic Zoological Expedition of the museum through much of western and southern Yunnan, as well as other provinces of China. The book "Camps and Trails in China" records their experiences.
In 1920, Andrews began planning for expeditions to Mongolia and drove a fleet of Dodge cars westward from Peking. In 1922, the party discovered a fossil of "Indricotherium" (then named "Baluchitherium"), a gigantic hornless rhinoceros, which was sent back to the museum, arriving on December 19. In the 1920s, he went to Mongolia, hoping to find out something about the origins of the human race. He didn't find out anything about early humans, but he discovered a treasure trove of dinosaur bones. During four expeditions in the Gobi Desert between 1922 and 1925, he discovered "Protoceratops" (the species "P. andrewsi" was named after him), a nest of "Protoceratops" eggs (later studies revealed them to be "Oviraptor" eggs), "Pinacosaurus", "Saurornithoides", "Oviraptor" and "Velociraptor", none of which were known before. "Andrewsarchus" was named after him.
Andrews, along with Henry Fairfield Osborn, was a proponent of the Out of Asia theory of humanity's origins and led several expeditions to Asia from 1922 to 1928 known as the "Central Asiatic Expeditions" to look for the earliest human remains in Asia. The expeditions did not find human remains. However, Andrews and his team made many other finds, including dinosaurs bones and fossil mammals and most notably the first nests full of dinosaur eggs ever discovered (see below). Andrews's main account of these expeditions can be found in his book "The New Conquest of Central Asia".
In his preface to Andrews's 1926 book, "On the Trail of the Ancient Man", Henry Fairfield Osborn predicted that the birthplace of modern humans would be found in Asia and stated that he had predicted this decades earlier, even before the Asiatic expeditions were carried out.
On July 13, 1923, the party was the first in the world to discover dinosaur eggs. Initially thought to be eggs of a ceratopsian, "Protoceratops", they were determined in 1995 actually to belong to the theropod "Oviraptor." . During that same expedition, Walter W. Granger discovered a skull from the Cretaceous period. In 1925, the museum sent a letter back informing the party that the skull was that of a mammal, and therefore even more rare and valuable; more were uncovered. Expeditions in the area stopped during 1926 and 1927. In 1928, the expedition's finds were seized by Chinese authorities but were eventually returned. The 1929 expedition was cancelled. In 1930, Andrews made one final trip and discovered some mastodon fossils. A cinematographer, James B. Shackelford, made filmed records of many of Andrews' expeditions. (Sixty years after Andrews' initial expedition, the American Museum of Natural History returned to Mongolia on the invitation of its government to continue exploration.) Later that year, Andrews returned to the United States and divorced his wife, with whom he had two sons. He married his second wife, Wilhelmina Christmas, in 1935.
In 1927, the Boy Scouts of America made Andrews an "Honorary Scout", a new category of Scout created that same year. This distinction was given to "American citizens whose achievements in outdoor activity, exploration and worthwhile adventure are of such an exceptional character as to capture the imagination of boys...".
Andrews joined the Explorers Club in New York in 1908, four years after its founding. He later served as its President from 1931 to 1934. In 1934, Andrews became the director of the Natural History museum. In his 1935 book "The Business of Exploring", he wrote "I was born to be an explorer...There was never any decision to make. I couldn't do anything else and be happy." In 1942, Andrews retired to Carmel-by-the-Sea, California, where he wrote about his life.
He died on March 11, 1960 of a heart attack at Peninsula Community Hospital in Carmel, California. He is buried in Oakwood Cemetery in his hometown of Beloit.
"Indiana Jones" connection.
Douglas Preston of the American Museum of Natural History wrote:
Although some sources speculate that Andrews was the inspiration for Indiana Jones, neither George Lucas nor the other creators of the films have ever confirmed this. Other candidates have been suggested, including Colonel Percy Fawcett. The 120-page transcript of the story conferences for the movie does not mention Andrews. An analysis by the Smithsonian Channel concludes that the linkage was indirect, with Andrews (and other explorers) serving as the model for heroes in adventure films of the 1940s and 1950s, who in turn inspired Lucas and his fellow writers.
Bibliography.
Books listed on Worldcat 

</doc>
<doc id="25734" url="http://en.wikipedia.org/wiki?curid=25734" title="Taiwan">
Taiwan

Taiwan ( ; see below), officially the Republic of China (ROC; ), is a sovereign state in East Asia. The Republic of China, originally based in mainland China, now governs the island of Taiwan, which makes up over 99% of its territory, as well as Penghu, Kinmen, Matsu, and other minor islands. Neighboring states include the People's Republic of China to the west, Japan to the east and northeast, and the Philippines to the south. Taiwan is one of the most densely populated countries in the world with a population density of 648 people per km² in March 2015. Taipei is the seat of the central government, and which together with the surrounding cities of New Taipei and Keelung, forms the largest metropolitan area on the island.
The island of Taiwan (formerly known as "Formosa") was mainly inhabited by Taiwanese aborigines until the Dutch and Spanish settlement during the Age of Discovery in the 17th century, when Han Chinese began immigrating to the island. In 1662, the pro-Ming loyalist Koxinga expelled the Dutch and established the first Han Chinese polity on the island, the Kingdom of Tungning. The Qing dynasty of China later defeated the kingdom and annexed Taiwan. By the time Taiwan was ceded to Japan in 1895, the majority of Taiwan's inhabitants were Han Chinese either by ancestry or by assimilation. The Republic of China (ROC) was established in mainland China in 1912. After Japan's surrender in 1945, the ROC assumed its control of Taiwan. Following the Chinese civil war, the Communist Party of China took full control of mainland China and founded the People's Republic of China (PRC) in 1949. The ROC relocated its government to Taiwan, and its jurisdiction became limited to Taiwan and its surrounding islands. Despite this, the ROC continued to represent China at the United Nations until 1971, when the PRC assumed China's seat via Resolution 2758 and the ROC lost its UN membership. International recognition of the ROC has gradually eroded as most countries switched recognition to the PRC. 21 UN member states and the Holy See currently maintain official diplomatic relations with the ROC. It has unofficial ties with most other states via its representative offices.
Ongoing issues of Cross-Strait relations as well as political status of Taiwan are major factors of contention in Taiwanese politics and a cause of social and political division among political parties and their respective supporters within the country. Constitutionally, there is dispute over whether the ROC still lays claim to the sovereignty over all of "China," in a definition that includes mainland China and Outer Mongolia basing on its pre-1949 territories, but the ROC has not made retaking mainland China a political goal since 1992. However, the government's stance on defining its political position of relation with China largely depends on which political coalition is in charge. Meanwhile, the PRC also asserts itself to be the sole legal representation of China and claims Taiwan as its 23rd province to be under its sovereignty, denying the status and existence of ROC as a sovereign state. The PRC has threatened the use of military force as a response to any formal declaration of Taiwanese independence, or if it deems peaceful reunification no longer possible.
During the latter half of the 20th century, Taiwan experienced rapid economic growth and industrialization and is now an advanced industrial economy. In the 1980s and early 1990s, Taiwan evolved into a multi-party democracy with universal suffrage. Taiwan is one of the Four Asian Tigers and a member of the WTO and APEC. The 21st-largest economy in the world, its high-tech industry plays a key role in the global economy. Taiwan is ranked highly in terms of freedom of the press, health care, public education, economic freedom, and human development.
Names.
There are various names for the island of Taiwan in use today, derived from explorers or rulers by each particular period. The former name Formosa (福爾摩沙) dates from 1542, when Portuguese sailors sighted the main island of Taiwan and named it "Ilha Formosa", which means "beautiful island". The name "Formosa" eventually "replaced all others in European literature" and was in common use in English in the early 20th century.
In the early 17th century, the Dutch East India Company established a commercial post at Fort Zeelandia (modern-day Anping) on a coastal sandbar they called "Tayouan", meaning "foreigners" in the indigenous Siraya language. The Sirayan name was also adopted into the Chinese vernacular (in particular, Hokkien, as ) as the name of the sandbar and nearby area (modern-day Tainan). The modern word "Taiwan" is derived from this usage, which has also been written as 大員, 大圓, 大灣, 臺員, 臺圓 and 臺窩灣 in various Chinese historical records. The area of modern-day Tainan was the first permanent settlement by Western colonists and Chinese immigrants, grew to be the most important trading center, and served as the capital of the island until 1887. Use of the current Chinese name 臺灣 was formalized as early as 1684 with the establishment of Taiwan Prefecture. Through its rapid development, the entire Formosan mainland eventually became known as "Taiwan".
The official name of the state is the "Republic of China"; it has also been known under various names throughout its existence. Shortly after the ROC's establishment in 1912, while it was still located on the Asian mainland, the government used the abbreviation "China" ("Zhōngguó") to refer to itself. During the 1950s and 1960s, it was common to refer to it as "Nationalist China" (or "Free China") to differentiate it from "Communist China" (or "Red China"). It was a member of the UN representing "China" until 1971, when it lost its seat to the People's Republic of China. Over subsequent decades, the Republic of China has become commonly known as "Taiwan", after the island that composes most of its controlled territory. The Republic of China participates in most international forums and organizations under the name "Chinese Taipei" due to diplomatic pressure from the PRC. For instance, it is the name under which it has competed at the Olympic Games since 1984, and its name as an observer at the World Health Organization.
History.
Prehistoric Taiwan.
Taiwan was joined to the Asian mainland in the Late Pleistocene, until sea levels rose about 10,000 years ago. Fragmentary human remains have been found on the island, dated 20,000 to 30,000 years ago, as well as later artifacts of a Paleolithic culture.
More than 8,000 years ago, Austronesians first settled on Taiwan. The languages of their descendants, who are known as the Taiwanese aborigines nowadays, belong to the Austronesian language family, which also includes the Malayo-Polynesian languages spanning a huge area, including the entire Maritime Southeast Asia (i.e., Tagalog of the Philippines, Malay and Indonesian of Malaysia and Indonesia, or the Javanese of Java), the Pacific and Indian Ocean: westernmost to the Malagasies of Madagascar and easternmost to the Rapa Nui people of Easter Island. The aboriginal languages on Taiwan show much greater diversity than the rest of Austronesian put together, leading linguists to propose Taiwan as the Urheimat of the family, from which seafaring peoples dispersed across Southeast Asia and the Pacific and Indian Oceans.
Han Chinese began settling in the Penghu islands in the 13th century, but Taiwan's hostile tribes and its lack of trade resources valued in that era rendered it unattractive to all but "occasional adventurers or fishermen engaging in barter" until the 16th century.
Opening in the 17th century.
The Dutch East India Company attempted to establish a trading outpost on the Penghu Islands (Pescadores) in 1622, but were militarily defeated and driven off by the Ming authorities.
In 1624, the company established a stronghold called Fort Zeelandia on the coastal islet of Tayouan, which is now part of the main island at Anping, Tainan.
David Wright, a Scottish agent of the company who lived on the island in the 1650s, described the lowland areas of the island as being divided among 11 chiefdoms ranging in size from two settlements to 72. Some of these fell under Dutch control, while others remained independent. The Company began to import laborers from Fujian and Penghu (Pescadores), many of whom settled.
In 1626, the Spanish landed on and occupied northern Taiwan, at the ports of Keelung and Tamsui, as a base to extend their trading. This colonial period lasted 16 years until 1642, when the last Spanish fortress fell to Dutch forces.
Following the fall of the Ming dynasty, Koxinga (Zheng Chenggong), a self-styled Ming loyalist, arrived on the island and captured Fort Zeelandia in 1662, expelling the Dutch government and military from the island. Koxinga established the Kingdom of Tungning (1662–1683), with his capital at Tainan. He and his heirs, Zheng Jing, who ruled from 1662 to 1682, and Zheng Keshuang, who ruled less than a year, continued to launch raids on the southeast coast of mainland China well into the Qing dynasty.
Qing rule.
In 1683, following the defeat of Koxinga's grandson by an armada led by Admiral Shi Lang of southern Fujian, the Qing dynasty formally annexed Taiwan, placing it under the jurisdiction of Fujian province. The Qing imperial government tried to reduce piracy and vagrancy in the area, issuing a series of edicts to manage immigration and respect aboriginal land rights. Immigrants mostly from southern Fujian continued to enter Taiwan. The border between taxpaying lands and "savage" lands shifted eastward, with some aborigines becoming sinicized while others retreated into the mountains. During this time, there were a number of conflicts between groups of Chinese from different regions of southern Fujian, and between southern Fujian Chinese and aborigines.
Northern Taiwan and the Penghu Islands were the scene of subsidiary campaigns in the Sino-French War (August 1884 to April 1885). The French occupied Keelung on 1 October 1884, but were repulsed from Tamsui a few days later. The French won some tactical victories but were unable to exploit them, and the Keelung Campaign ended in stalemate. The Pescadores Campaign, beginning on 31 March 1885, was a French victory, but had no long-term consequences. The French evacuated both Keelung and the Penghu archipelago after the end of the war.
In 1887, the Qing upgraded the island's administration from Taiwan Prefecture of Fujian to Fujian-Taiwan-Province (), the twentieth in the empire, with its capital at Taipei. This was accompanied by a modernization drive that included building China's first railroad.
Japanese rule.
The Qing dynasty was defeated in the First Sino-Japanese War (1894–1895) and Taiwan and Penghu were ceded in full sovereignty to the Empire of Japan. Inhabitants wishing to remain Qing subjects were given a two-year grace period to sell their property and move to mainland China. Very few Taiwanese saw this as feasible. On 25 May 1895, a group of pro-Qing high officials proclaimed the Republic of Formosa to resist impending Japanese rule. Japanese forces entered the capital at Tainan and quelled this resistance on 21 October 1895. Guerrilla fighting continued periodically until about 1902 and ultimately took the lives of 14,000 Taiwanese, or 0.5% of the population. Several subsequent rebellions against the Japanese (the Beipu Uprising of 1907, the Tapani Incident of 1915, and the Wushe Incident of 1930) were all unsuccessful but demonstrated opposition to Japanese colonial rule.
Japanese colonial rule was instrumental in the industrialization of the island, extending the railroads and other transportation networks, building an extensive sanitation system, and establishing a formal education system. Japanese rule ended the practice of headhunting. During this period the human and natural resources of Taiwan were used to aid the development of Japan and the production of cash crops such as rice and sugar greatly increased. By 1939, Taiwan was the seventh greatest sugar producer in the world. Still, the Taiwanese and aborigines were classified as second- and third-class citizens. After suppressing Chinese guerrillas in the first decade of their rule, Japanese authorities engaged in a series of bloody campaigns against the mountain aboriginals, culminating in the Wushe Incident of 1930.
Around 1935, the Japanese began an island-wide assimilation project to bind the island more firmly to the Japanese Empire and people were taught to see themselves as Japanese under the Kominka Movement, during which time Taiwanese culture and religion were outlawed and the citizens were encouraged to adopt Japanese surnames. During World War II, tens of thousands of Taiwanese served in the Japanese military. For example, former ROC President Lee Teng-hui's elder brother served in the Japanese navy and died while on duty in the Philippines in February 1945.
The Imperial Japanese Navy operated heavily out of Taiwanese ports. The "South Strike Group" was based at the Taihoku Imperial University in Taipei. Many of the Japanese forces participating in the Aerial Battle of Taiwan-Okinawa were based in Taiwan. Important Japanese military bases and industrial centers throughout Taiwan, like Kaohsiung, were targets of heavy American bombing. Also during this time, over 2,000 women were forced into sexual slavery for Imperial Japanese troops, now euphemistically called "comfort women."
In 1938, there were 309,000 Japanese settlers in Taiwan. After World War II, most of the Japanese were repatriated to Japan.
After World War II.
On 25 October 1945, the U.S. Navy ferried ROC troops to Taiwan in order to accept the formal surrender of Japanese military forces in Taipei (then part of Taihoku Prefecture), as part of General Order No. 1 for temporary military occupation. General Rikichi Andō, governor-general of Taiwan and commander-in-chief of all Japanese forces on the island, signed the instrument of surrender and handed it over to General Chen Yi of the ROC military to complete the official turnover. Chen Yi proclaimed that day to be "Taiwan Retrocession Day", but the Allies considered Taiwan and the Penghu Islands to be under military occupation and still under Japanese sovereignty until 1952, when the Treaty of San Francisco took effect.
The ROC administration of Taiwan under Chen Yi was strained by increasing tensions between Taiwan-born people and newly arrived mainlanders, which were compounded by economic woes, such as hyperinflation. Furthermore, cultural and linguistic conflicts between the two groups quickly led to the loss of popular support for the new government. The shooting of a civilian on 28 February 1947 triggered island-wide unrest, which was suppressed with military force in what is now called the 228 Incident. Mainstream estimates of the number killed range from 18,000 to 30,000, mainly Taiwanese elites.
Chinese Nationalist one-party rule.
After the end of World War II, the Chinese Civil War resumed between the Chinese Nationalists (Kuomintang), led by Chiang Kai-shek, and the Chinese Communist Party, led by Mao Zedong. By 1949, a series of Chinese Communist offensives led to the defeat of the Nationalist army, and the Communists founded the People's Republic of China on 1 October.
In December 1949, Chiang evacuated his government to Taiwan and made Taipei the temporary capital of the ROC (also called the "wartime capital" by Chiang Kai-shek). Some 2 million people, consisting mainly of soldiers, members of the ruling Kuomintang and intellectual and business elites, were evacuated from mainland China to Taiwan at that time, adding to the earlier population of approximately six million. In addition, the ROC government took to Taipei many national treasures and much of China's gold reserves and foreign currency reserves.
From this point onwards, the Kuomintang was reduced to control of Taiwan, Kinmen, Matsu Islands, and two major islands of Dongsha Islands and Nansha Islands. The Kuomintang continued to claim sovereignty over all "China", which it defined to include mainland China, Taiwan, Outer Mongolia and other areas. On mainland China, the victorious Communists claimed they ruled the sole and only China (which they claimed included Taiwan) and that the Republic of China no longer existed.
Martial law, declared on Taiwan in May 1949, continued to be in effect after the central government relocated to Taiwan. It was not repealed until 1987, and was used as a way to suppress the political opposition in the intervening years. During the White Terror, as the period is known, 140,000 people were imprisoned or executed for being perceived as anti-KMT or pro-Communist. Many citizens were arrested, tortured, imprisoned and executed for their real or perceived link to the Communists. Since these people were mainly from the intellectual and social elite, an entire generation of political and social leaders was decimated. In 1998 law was passed to create the "Compensation Foundation for Improper Verdicts" which oversaw compensation to White Terror victims and families. President Ma Ying-jeou made an official apology in 2008, expressing hope that there will never be a tragedy similar to White Terror.
Initially, the United States abandoned the KMT and expected that Taiwan would fall to the Communists. However, in 1950 the conflict between North Korea and South Korea, which had been ongoing since the Japanese withdrawal in 1945, escalated into full-blown war, and in the context of the Cold War, US President Harry S. Truman intervened again and dispatched the U.S. Navy's 7th Fleet into the Taiwan Strait to prevent hostilities between Taiwan and mainland China. In the Treaty of San Francisco and the Treaty of Taipei, which came into force respectively on 28 April 1952 and 5 August 1952, Japan formally renounced all right, claim and title to Taiwan and Penghu, and renounced all treaties signed with China before 1942. Neither treaty specified to whom sovereignty over the islands should be transferred, because the United States and the United Kingdom disagreed on whether the ROC or the PRC was the legitimate government of China. Continuing conflict of the Chinese Civil War through the 1950s, and intervention by the United States notably resulted in legislation such as the Sino-American Mutual Defense Treaty and the Formosa Resolution of 1955.
As the Chinese Civil War continued without truce, the government built up military fortifications throughout Taiwan. Within this effort, KMT veterans built the now famous Central Cross-Island Highway through the Taroko Gorge in the 1950s. The two sides would continue to engage in sporadic military clashes with seldom publicized details well into the 1960s on the China coastal islands with an unknown number of night raids. During the Second Taiwan Strait Crisis in September 1958, Taiwan's landscape saw Nike-Hercules missile batteries added, with the formation of the 1st Missile Battalion Chinese Army that would not be deactivated until 1997. Newer generations of missile batteries have since replaced the Nike Hercules systems throughout the island.
During the 1960s and 1970s, the ROC maintained an authoritarian, single-party government while its economy became industrialized and technology oriented. This rapid economic growth, known as the Taiwan Miracle, was the result of a fiscal regime independent from mainland China and backed up, among others, by the support of US funds and demand for Taiwanese products. In the 1970s, Taiwan was economically the second fastest growing state in Asia after Japan. Taiwan, along with Hong Kong, South Korea and Singapore, became known as one of the Four Asian Tigers. Because of the Cold War, most Western nations and the United Nations regarded the ROC as the sole legitimate government of China until the 1970s. Later, especially after the termination of the Sino-American Mutual Defense Treaty, most nations switched diplomatic recognition to the PRC (see United Nations General Assembly Resolution 2758).
Up until the 1970s, the government was regarded by Western critics as undemocratic for upholding martial law, for severely repressing any political opposition and for controlling media. The KMT did not allow the creation of new parties and those that existed did not seriously compete with the KMT. Thus, competitive democratic elections did not exist. From the late 1970s to the 1990s, however, Taiwan went through reforms and social changes that transformed it from an authoritarian state to a democracy. In 1979, a pro-democracy protest known as the Kaohsiung Incident took place in Kaohsiung to celebrate Human Rights Day. Although the protest was rapidly crushed by the authorities, it is today considered as the main event that united Taiwan's opposition.
Democratization.
Chiang Ching-kuo, Chiang Kai-shek's son and successor as the president, began to liberalize the political system in the mid-1980s. In 1984, the younger Chiang selected Lee Teng-hui, a Taiwanese-born, U.S.-educated technocrat, to be his vice president. In 1986, the Democratic Progressive Party (DPP) was formed and inaugurated as the first opposition party in the ROC to counter the KMT. A year later, Chiang Ching-kuo lifted martial law on the main island of Taiwan (martial law was lifted on Penghu in 1979, Matsu island in 1992 and Kinmen island in 1993). With the advent of democratization, the issue of the political status of Taiwan gradually resurfaced as a controversial issue where, previously, the discussion of anything other than unification under the ROC was taboo.
After the death of Chiang Ching-kuo in January 1988, Lee Teng-hui succeeded him as president. Lee continued to democratize the government and decrease the concentration of government authority in the hands of mainland Chinese. Under Lee, Taiwan underwent a process of localization in which Taiwanese culture and history were promoted over a pan-China viewpoint in contrast to earlier KMT policies which had promoted a Chinese identity. Lee's reforms included printing banknotes from the Central Bank rather than the Provincial Bank of Taiwan, and streamlining the Taiwan Provincial Government with most of its functions transferred to the Executive Yuan. Under Lee, the original members of the Legislative Yuan and National Assembly, elected in 1947 to represent mainland Chinese constituencies and having held the seats without re-election for more than four decades, were forced to resign in 1991. The previously nominal representation in the Legislative Yuan was brought to an end, reflecting the reality that the ROC had no jurisdiction over mainland China, and vice versa. Restrictions on the use of Taiwanese Hokkien in the broadcast media and in schools were also lifted.
Democratic reforms continued in the 1990s, with Lee Teng-hui re-elected in 1996, in the first direct presidential election in the history of the ROC. During the later years of Lee's administration, he was involved in corruption controversies relating to government release of land and weapons purchase, although no legal proceedings commenced. In 2000, Chen Shui-bian of the Democratic Progressive Party was elected as the first non-Kuomintang (KMT) President and was re-elected to serve his second and last term since 2004. Polarized politics has emerged in Taiwan with the formation of the Pan-Blue Coalition of parties led by the KMT, favoring eventual Chinese reunification, and the Pan-Green Coalition of parties led by the DPP, favoring an eventual and official declaration of Taiwanese independence.
On 30 September 2007, the ruling DPP approved a resolution asserting a separate identity from China and called for the enactment of a new constitution for a "normal country". It also called for general use of "Taiwan" as the country's name, without abolishing its formal name, the Republic of China. The Chen administration also pushed for referendums on national defense and UN entry in the 2004 and 2008 elections, which failed due to voter turnout below the required legal threshold of 50% of all registered voters. The Chen administration was dogged by public concerns over reduced economic growth, legislative gridlock due to a pan-blue, opposition-controlled Legislative Yuan and corruption involving the First Family as well as government officials.
The KMT increased its majority in the Legislative Yuan in the January 2008 legislative elections, while its nominee Ma Ying-jeou went on to win the presidency in March of the same year, campaigning on a platform of increased economic growth and better ties with the PRC under a policy of "mutual nondenial". Ma took office on 20 May 2008, the same day that President Chen Shui-bian stepped down and was notified by prosecutors of possible corruption charges. Part of the rationale for campaigning for closer economic ties with the PRC stems from the strong economic growth China attained since joining the World Trade Organization. However, some analysts say that despite the election of Ma Ying-jeou, the diplomatic and military tensions with the PRC have not been reduced.
Geography.
The total area of the current jurisdiction of the Republic of China is 36193 km2, making it the world's 137th-largest country/dependency, smaller than Switzerland and larger than Belgium.
The island of Taiwan lies some 180 km off the southeastern coast of mainland China, which lies across the Taiwan Strait, and has an area of 35883 km2. The East China Sea lies to the north, the Philippine Sea to the east, the Bashi Channel of the Luzon Strait directly to the south, and the South China Sea to the southwest. All are arms of the Pacific Ocean.
The shape of the main island of Taiwan is similar to a sweet potato seen in a south-to-north direction, and therefore, Taiwanese (especially Min Nan speakers) often call themselves "children of the Sweet Potato."
The island is characterized by the contrast between the eastern two-thirds, consisting mostly of rugged mountains running in five ranges from the northern to the southern tip of the island, and the flat to gently rolling Chianan Plains in the west that are also home to most of Taiwan's population. Taiwan's highest point is Yu Shan (Jade Mountain) at 3952 m; Taiwan is the world's fourth-highest island.
The Penghu Islands, 50 km west of the main island, have an area of 126.9 km2. More distant islands controlled by the Republic of China are the Kinmen, Wuchiu and Matsu Islands off the coast of Fujian, with a total area of 180.5 km2, and the Pratas Islands and Taiping Island in the South China Sea, with a total area of 2.9 km2 and no permanent inhabitants.
Climate.
Taiwan lies on the Tropic of Cancer, and its general climate is marine tropical. The northern and central regions are subtropical, whereas the south is tropical and the mountainous regions are temperate. The average rainfall is 2,600 mm per year for the island proper; the rainy season is concurrent with the onset of the summer East Asian Monsoon in May and June. The entire island experiences hot, humid weather from June through September. Typhoons are most common in July, August and September. During the winter (November to March), the northeast experiences steady rain, while the central and southern parts of the island are mostly sunny.
Geology.
The island of Taiwan lies in a complex tectonic area between the Yangtze Plate to the west and north, the Okinawa Plate on the north-east, and the Philippine Mobile Belt on the east and south. The upper part of the crust on the island is primarily made up of a series of terranes, mostly old island arcs which have been forced together by the collision of the forerunners of the Eurasian Plate and the Philippine Sea Plate. These have been further uplifted as a result of the detachment of a portion of the Eurasian Plate as it was subducted beneath remnants of the Philippine Sea Plate, a process which left the crust under Taiwan more buoyant.
The east and south of Taiwan are a complex system of belts formed by, and part of the zone of, active collision between the North Luzon Trough portion of the Luzon Volcanic Arc and South China, where accreted portions of the Luzon Arc and Luzon forearc form the eastern Coastal Range and parallel inland Longitudinal Valley of Taiwan respectively.
The major seismic faults in Taiwan correspond to the various suture zones between the various terranes. These have produced major quakes throughout the history of the island. On 21 September 1999, a 7.3 quake known as the "921 earthquake" killed more than 2,400 people. The seismic hazard map for Taiwan by the USGS shows 9/10 of the island as the highest rating (most hazardous).
Political and legal status.
The political and legal statuses of Taiwan are contentious issues. The People's Republic of China (PRC) claims that the Republic of China government is illegitimate, referring to it as the "Taiwan Authority". The ROC, however, with its own constitution, independently elected president and armed forces, continues to view itself as a sovereign state. The present territory of the state has never been controlled by the PRC. Internationally, there is controversy on whether the ROC still exists as a state or a defunct state per international law due to the loss of membership/recognition in the United Nations and lack of wide diplomatic recognition. In a poll of Taiwanese aged 20 and older taken by the TVBS in March 2009, a majority of 64% opted for the status quo, while 19% favored independence and 5% unification.
Relations with the PRC.
The political environment is complicated by the potential for military conflict should Taiwan make overt actions toward de jure independence; it is the official PRC policy to use force to ensure reunification if peaceful reunification is no longer possible, as stated in its anti-secession law, and for this reason there are substantial military installations on the Fujian coast. However, in recent years, the PRC has moved towards promoting peaceful relations, including stronger economic ties, with the current ROC government aimed at unification through the one country, two systems formula.
The PRC supports a version of the One-China policy, which states that Taiwan and mainland China are both part of China, and that the PRC is the only legitimate government of China. It uses this policy to prevent the international recognition of the ROC as an independent sovereign state. For its part, the People's Republic of China appears to find the retention of the name "Republic of China" more acceptable than an official declaration of an independent Taiwan. With the rise of the Taiwanese independence movement, the name "Taiwan" has been employed increasingly often on the island.
Foreign relations.
Before 1928, the foreign policy of Republican China was complicated by a lack of internal unity—competing centers of power all claimed legitimacy. This situation changed after the defeat of the Peiyang Government by the Kuomintang, which led to widespread diplomatic recognition of the Republic of China.
After the KMT's retreat to Taiwan, most countries, notably the countries in the Western Bloc, continued to maintain relations with the ROC. Due to diplomatic pressure, recognition gradually eroded and many countries switched recognition to the PRC in the 1970s. UN Resolution 2758 (25 October 1971) recognized the People's Republic of China as China's sole representative in the United Nations.
The PRC refuses to have diplomatic relations with any nation that recognizes the ROC, and requires all nations with which it has diplomatic relations to make a statement recognizing its claims to Taiwan. As a result, only 21 UN member states and the Holy See maintain official diplomatic relations with the Republic of China. The ROC maintains unofficial relations with most countries via "de facto" embassies and consulates called Taipei Economic and Cultural Representative Offices (TECRO), with branch offices called "Taipei Economic and Cultural Offices" (TECO). Both TECRO and TECO are "unofficial commercial entities" of the ROC in charge of maintaining diplomatic relations, providing consular services (i.e. visa applications), and serving the national interests of the ROC in other countries.
The United States remains one of the main allies of the ROC and, through the Taiwan Relations Act passed in 1979, has continued selling arms and provide military training to the Republic of China Armed Forces. This situation continues to be an issue for the People's Republic of China which considers US involvement disruptive to the stability of the region. In January 2010, the Obama administration announced its intention to sell $6.4 billion worth of military hardware to Taiwan. As a consequence, the PRC threatened the US with economic sanctions and warned that their cooperation on international and regional issues could suffer.
The official position of the United States is that the PRC is expected to "use no force or threat[en] to use force against Taiwan" and the ROC is to "exercise prudence in managing all aspects of Cross-Strait relations." Both are to refrain from performing actions or espousing statements "that would unilaterally alter Taiwan's status."
Participation in international events and organizations.
The ROC was a founding member of the United Nations, and held the seat of China on the Security Council and other UN bodies until 1971, when it was expelled by Resolution 2758 and replaced in all UN organs with the PRC. Each year since 1992, the ROC has petitioned the UN for entry, but its applications have not made it past committee.
Due to its limited international recognition, the Republic of China is a member of the Unrepresented Nations and Peoples Organization, represented by a government-funded organization, the Taiwan Foundation for Democracy (TFD) under the name "Taiwan".
Also due to its One China policy, the PRC only participates in international organizations where the ROC is not recognized as a sovereign country. Most member states, including the United States, do not wish to discuss the issue of the ROC's political status for fear of souring diplomatic ties with the PRC. However, both the U.S. and Japan publicly support the ROC's bid for membership in the World Health Organization as an observer. However, though the ROC has sought to participate in the WHO since 1997, their efforts have consistently been blocked by the PRC, until 2010, when they were invited as observers to attend the World Health Assembly, under the name "Chinese Taipei".
Due to PRC pressure, the ROC is forced to use the name "Chinese Taipei" in international events, such as the Olympic Games, where the PRC is also a party. The ROC is typically barred from using its national anthem and national flag in international events due to PRC pressure; ROC spectators attending events such as the Olympics are often barred from bringing ROC flags into venues. The ROC is able to participate as "China" in organizations that the PRC does not participate in, such as the World Organization of the Scout Movement.
Opinions within Taiwan.
Within Taiwan, opinions are polarized between those supporting unification, represented by the Pan-Blue Coalition of parties, and those supporting independence, represented by the Pan-Green Coalition.
The KMT, the largest Pan-Blue party, supports the status quo for the indefinite future with a stated ultimate goal of unification. However, it does not support unification in the short term with the PRC as such a prospect would be unacceptable to most of its members and the public. Ma Ying-jeou, chairman of the KMT and the incumbent president of the ROC, has set out democracy, economic development to a level near that of Taiwan, and equitable wealth distribution as the conditions that the PRC must fulfill for reunification to occur.
The Democratic Progressive Party, the largest Pan-Green party, officially seeks independence, but in practice also supports the status quo because its members and the public would not accept the risk of provoking the PRC.
On 2 September 2008, Mexican newspaper "El Sol de México" asked President Ma about his views on the subject of "two Chinas" and if there was a solution for the sovereignty issues between the two. The president replied that the relations are neither between two Chinas nor two states. It is a special relationship. Further, he stated that the sovereignty issues between the two cannot be resolved at present, but he quoted the "1992 Consensus", currently accepted by both the Kuomintang and the Communist Party of China, as a temporary measure until a solution becomes available.
The relationship with the PRC and the related issues of Taiwanese independence and Chinese reunification continue to dominate politics.
Government.
The government of the Republic of China was founded on the Constitution of the ROC and its Three Principles of the People, which states that the ROC "shall be a democratic republic of the people, to be governed by the people and for the people." The government is divided into five administrative branches ("Yuan"): the Executive Yuan (cabinet), the Legislative Yuan, the Judicial Yuan, the Control Yuan (audit agency), and the Examination Yuan (civil service examination agency). The Pan-Blue and Pan-Green coalitions are presently the dominant political blocs in the Republic of China.
The head of state and commander-in-chief of the armed forces is the president, who is elected by popular vote for a maximum of 2 four-year terms on the same ticket as the vice-president. The president has authority over the Yuan. The president appoints the members of the Executive Yuan as his cabinet, including a premier, who is officially the President of the Executive Yuan; members are responsible for policy and administration.
The main legislative body is the unicameral Legislative Yuan with 113 seats. Seventy-three are elected by popular vote from single-member constituencies; thirty-four are elected based on the proportion of nationwide votes received by participating political parties in a separate party list ballot; and six are elected from two three-member aboriginal constituencies. Members serve four-year terms. Originally the unicameral National Assembly, as a standing constitutional convention and electoral college, held some parliamentary functions, but the National Assembly was abolished in 2005 with the power of constitutional amendments handed over to the Legislative Yuan and all eligible voters of the Republic via referendums.
The premier is selected by the president without the need for approval from the legislature, but the legislature can pass laws without regard for the president, as neither he nor the Premier wields veto power. Thus, there is little incentive for the president and the legislature to negotiate on legislation if they are of opposing parties. After the election of the pan-Green's Chen Shui-bian as President in 2000, legislation repeatedly stalled because of deadlock with the Legislative Yuan, which was controlled by a pan-Blue majority. Historically, the ROC has been dominated by strongman single party politics. This legacy has resulted in executive powers currently being concentrated in the office of the president rather than the premier, even though the constitution does not explicitly state the extent of the president's executive power.
The Judicial Yuan is the highest judicial organ. It interprets the constitution and other laws and decrees, judges administrative suits, and disciplines public functionaries. The president and vice-president of the Judicial Yuan and additional thirteen justices form the Council of Grand Justices. They are nominated and appointed by the president, with the consent of the Legislative Yuan. The highest court, the Supreme Court, consists of a number of civil and criminal divisions, each of which is formed by a presiding judge and four associate judges, all appointed for life. In 1993, a separate constitutional court was established to resolve constitutional disputes, regulate the activities of political parties and accelerate the democratization process. There is no trial by jury but the right to a fair public trial is protected by law and respected in practice; many cases are presided over by multiple judges.
Capital punishment is still used in Taiwan, although efforts have been made by the government to reduce the number of executions. Nevertheless, according to a survey in 2006, about 80% of Taiwanese still wanted to keep the death penalty.
The Control Yuan is a watchdog agency that monitors (controls) the actions of the executive. It can be considered a standing commission for administrative inquiry and can be compared to the Court of Auditors of the European Union or the Government Accountability Office of the United States.
The Examination Yuan is in charge of validating the qualification of civil servants. It is based on the old imperial examination system used in dynastic China. It can be compared to the European Personnel Selection Office of the European Union or the Office of Personnel Management of the United States.
Politics.
The constitution of the Republic of China was drafted before the fall of mainland China to the Communist Party of China. It was created by the KMT for the purpose of all of its claimed territory, including Taiwan, even though the Communist Party boycotted the drafting of the constitution. The constitution went into effect on 25 December 1947.
The ROC remained under martial law from 1948 until 1987 and much of the constitution was not in effect. Political reforms beginning in the late 1970s and continuing through the early 1990s liberalized the country and transformed into a multiparty democracy. Since the lifting of martial law, the Republic of China has democratized and reformed, suspending constitutional components that were originally meant for the whole of China. This process of amendment continues. In 2000, the Democratic Progressive Party (DPP) won the presidency, ending KMT's continuous control of the government. In May 2005, a new National Assembly was elected to reduce the number of parliamentary seats and implement several constitutional reforms. These reforms have been passed; the National Assembly has essentially voted to abolish itself and transfer the power of constitutional reform to the popular ballot.
Major camps.
The tension between the PRC and Taiwan colors most of the political life, and any government move towards "Taiwan independence" is met by threat of military attack from the PRC. The PRC's official policy is to reunify Taiwan and mainland China under the formula of "one country, two systems" and refuses to renounce the use of military force, especially should Taiwan seek a declaration of independence.
The political scene is generally divided into two major camps in terms of views on how Taiwan should relate to China or the PRC, referred to as cross-Strait relations. It is the main political difference between two camps: the Pan-Blue Coalition, composed of the pro-unification Kuomintang, People First Party (PFP), and New Party, who believe that the ROC is the sole legitimate government of "China" (including Taiwan) and supports eventual Chinese reunification. The opposition Pan-Green Coalition is composed of the pro-independence DPP and Taiwan Solidarity Union (TSU). It regards Taiwan as an independent, sovereign state synonymous with the ROC, opposes the definition that Taiwan is part of "China", and seeks wide diplomatic recognition and an eventual declaration of formal Taiwan independence. The Pan-Green camp tends to favor emphasizing the Republic of China as being a distinct country from the People's Republic of China. Thus, in September 2007, the then ruling Democratic Progressive Party approved a resolution asserting separate identity from China and called for the enactment of a new constitution for a ""normal country". It called also for general use of "Taiwan"" as the country's name, without abolishing its formal name, the "Republic of China". Some members of the coalition, such as former President Chen Shui-bian, argue that it is unnecessary to proclaim independence because "Taiwan is already an independent, sovereign country" and the Republic of China is the same as Taiwan. Despite being a member of KMT prior to and during his presidency, Lee Teng-hui also held a similar view and was a supporter of the Taiwanization movement.
Pan-Blue members generally support the concept of the One-China policy, which states that there is only one China and that its only government is the ROC. They favor eventual re-unification of China. The more mainstream Pan-Blue position is to lift investment restrictions and pursue negotiations with the PRC to immediately open direct transportation links. Regarding independence, the mainstream Pan-Blue position is to maintain the status quo, while refusing immediate reunification. President Ma Ying-jeou stated that there will be no unification nor declaration of independence during his presidency. As of 2009, Pan-Blue members usually seek to improve relationships with mainland China, with a current focus on improving economic ties.
Current political issues.
The dominant political issue in Taiwan is its relationship with the PRC. For almost 60 years, there were no direct transportation links, including direct flights, between Taiwan and mainland China. This was a problem for many Taiwanese businesses that had opened factories or branches in mainland China. The former DPP administration feared that such links would lead to tighter economic and political integration with mainland China, and in the 2006 Lunar New Year Speech, President Chen Shui-bian called for managed opening of links. Direct weekend charter flights between Taiwan and mainland China began in July 2008 under the current KMT government, and the first direct daily charter flights took off in December 2008.
Other major political issues include the passage of an arms procurement bill that the United States authorized in 2001. In 2008, however, the United States was reluctant to send over more arms to Taiwan out of fear that it would hinder the recent improvement of ties between the PRC and the ROC. Another major political issue is the establishment of a National Communications Commission to take over from the Government Information Office, whose advertising budget exercised great control over the media.
The politicians and their parties have themselves become major political issues. Corruption among some DPP administration officials has been exposed. In early 2006, President Chen Shui-bian was linked to possible corruption. The political effect on President Chen Shui-bian was great, causing a divide in the DPP leadership and supporters alike. It eventually led to the creation of a political camp led by ex-DPP leader Shih Ming-teh which believes the president should resign. The KMT assets continue to be another major issue, as it was once the richest political party in the world. Nearing the end of 2006, KMT's chairman Ma Ying-jeou was also hit by corruption controversies, although he has since then been cleared of any wrongdoings by the courts. After completing his second term as President, Chen Shui-bian was charged with corruption and money laundering. Following his conviction, he is serving a 17-year sentence in Taipei Prison.
National identity.
Roughly 84% of Taiwan's population descends from Han Chinese who migrated from mainland China between 1661 and 1895. Another significant fraction descends from Han Chinese who immigrated from mainland China in the 1940s and 1950s. The shared cultural origin combined with several hundred years of geographical separation, some hundred years of political separation and foreign influences, as well as hostility between the rival ROC and PRC have resulted in national identity being a contentious issue with political overtones. Since democratization and the lifting of martial law, a distinct Taiwanese identity (as opposed to Taiwanese identity as a subset of a Chinese identity) is often at the heart of political debates. Its acceptance makes the island distinct from mainland China, and therefore may be seen as a step towards forming a consensus for "de jure" Taiwan independence. The pan-green camp supports a distinct Taiwanese identity, while the pan-blue camp supports a Chinese identity only. The KMT has downplayed this stance in the recent years and now supports a Taiwanese identity as part of a Chinese identity.
According to a survey conducted in March 2009, 49% of the respondents consider themselves as Taiwanese only, and 44% of the respondents consider themselves as Taiwanese and Chinese. 3% consider themselves as only Chinese. Another survey, conducted in Taiwan in July 2009, showed that 82.8% of respondents consider the ROC and the PRC as two separate countries with each developing on its own. A survey conducted in December 2009 showed that 62% of the respondents consider themselves as Taiwanese only, and 22% of the respondents consider themselves as both Taiwanese and Chinese. 8% consider themselves as only Chinese. The survey also shows that among 18- to 29-year-old respondents, 75% consider themselves as Taiwanese only.
In the latest survey conducted by National Chengchi University in 2014 and published in early 2015, 60.6% of respondents identified themselves exclusively as Taiwanese, 32.5% identified themselves as both Taiwanese and Chinese and 3.5% identified themselves as Chinese.
Military.
The Republic of China Army takes its roots in the National Revolutionary Army, which was established by Sun Yat-sen in 1925 in Guangdong with a goal of reunifying China under the Kuomintang. When the People's Liberation Army won the Chinese Civil War, much of the National Revolutionary Army retreated to Taiwan along with the government. It was later reformed into the Republic of China Army. Units which surrendered and remained in mainland China were either disbanded or incorporated into the People's Liberation Army.
Today, the Republic of China maintains a large and technologically advanced military, mainly as defense against the constant threat of invasion by the PRC under the Anti-Secession Law of the People's Republic of China. From 1949 to the 1970s, the primary mission of the military was to "retake the mainland" through Project National Glory. As this mission has shifted to defense, the ROC military has begun to shift emphasis from the traditionally dominant Army to the air force and navy.
Control of the armed forces has also passed into the hands of the civilian government. As the ROC military shares historical roots with the KMT, the older generation of high-ranking officers tends to have Pan-Blue sympathies. However, many have retired and there are many more non-mainlanders enlisting in the armed forces in the younger generations, so the political leanings of the military have moved closer to the public norm in Taiwan.
The ROC began a force reduction program, Jingshi An (translated to streamlining program), to scale down its military from a level of 450,000 in 1997 to 380,000 in 2001. As of 2009, the armed forces of the ROC number approximately 300,000, with nominal reserves totaling 3.6 million as of 2005. Conscription remains universal for qualified males reaching age eighteen, but as a part of the reduction effort many are given the opportunity to fulfill their draft requirement through alternative service and are redirected to government agencies or defense related industries. Current plans call for a transition to a predominantly professional army over the next decade. Conscription periods are planned to decrease from 14 months to 12. In the last months of the Bush administration, Taipei took the decision to reverse the trend of declining defense spending, at a time when most Asian countries kept on reducing their military expenditures. It also decided to modernize both defensive and offensive capabilities. Taipei still keeps a large military apparatus relative to the island’s population: defense expenditures for 2008 were NTD 334 billion (approximately U.S. $10.5 billion), which accounted for 2.94% of GDP.
The armed forces' primary concern at this time, according to the "National Defense Report", is the possibility of an invasion by the PRC, consisting of a naval blockade, airborne assault, and/or missile bombardment. Four upgraded "Kidd"-class destroyers were purchased from the United States, and commissioned into the Republic of China Navy in 2005-2006, significantly upgrading Taiwan's air defense and submarine hunting abilities. The Ministry of National Defense planned to purchase diesel-powered submarines and Patriot anti-missile batteries from the United States, but its budget has been stalled repeatedly by the opposition-Pan-Blue Coalition controlled legislature. The defense package was stalled from 2001 to 2007 where it was finally passed through the legislature and the US responded on 3 October 2008, with a $6.5 billion arms package including PAC III Anti-Air defense systems, AH-64D Apache Attack helicopters and other arms and parts. A significant amount of military hardware has been bought from the United States, and, as of 2009, continues to be legally guaranteed by the Taiwan Relations Act. In the past, France and the Netherlands have also sold military weapons and hardware to the ROC, but they almost entirely stopped in the 1990s under pressure of the PRC.
The first line of defense against invasion by the PRC is the ROC's own armed forces. Current ROC military doctrine is to hold out against an invasion or blockade until the US military responds. There is, however, no guarantee in the Taiwan Relations Act or any other treaty that the United States will defend Taiwan, even in the event of invasion. The joint declaration on security between the US and Japan signed in 1996 may imply that Japan would be involved in any response. However, Japan has refused to stipulate whether the "area surrounding Japan" mentioned in the pact includes Taiwan, and the precise purpose of the pact is unclear. The Australia, New Zealand, United States Security Treaty (ANZUS Treaty) may mean that other US allies, such as Australia, could theoretically be involved. In practice, the risk of losing economic ties with China may prevent Australia from taking action. The United States, United Kingdom, Japan, South Korea, Australia, Canada, Chile, and Peru conduct maritime exercises in the Pacific Ocean every 2 years called RIMPAC. They are conducted to promote stability and to be able to respond in case of an armed conflict in the region – that includes an invasion of Taiwan by China.
Administrative divisions.
 
Taipei 
New Taipei 
Keelung 
Taoyuan 
Hsinchu County 
Hsinchu City 
Miaoli 
Taichung 
Changhua 
Penghu 
Nantou 
Yunlin 
Chiayi County 
Chiayi City 
Tainan 
Kaohsiung 
Pingtung 
Yilan 
Hualien 
Taitung 
"Taiwan Province" 
Kinmen 
Lienchiang (Matsu) 
"Fujian Province" 
According to the 1947 constitution, written and promulgated whilst the ROC government still controlled mainland China, the territory of the ROC consisted of provinces, special municipalities, special administrative regions and autonomous regions (Mongolia and Tibet), which were given extremely high levels of autonomy.
Accordingly, when the ROC retreated to Taiwan in 1949, its claimed territory consisted of 35 provinces, 12 special municipalities, 1 special administrative region and 2 autonomous regions. However, since its retreat, the ROC has controlled only Taiwan Province and some islands of Fujian Province. The ROC also controls the Pratas Islands and Taiping Island in the Spratly Islands, which are part of the disputed South China Sea Islands. They were placed under Kaohsiung administration after the retreat to Taiwan.
Since 1949, the government has made some changes in the area under its control. Taipei became a special municipality in 1967 and Kaohsiung in 1979. The two provincial governments were "streamlined", with their functions transferred to the central government (Fujian in 1956 and Taiwan in 1998). In 2010, New Taipei, Taichung and Tainan were upgraded to special municipalities. And in 2014, Taoyuan County was also upgraded to a special municipality. This brought the top-level divisions of the ROC to their current state:
According to Article 4 of the Local Government Act, laws pertaining to special municipalities also apply to counties with a population exceeding 2 million. This provision does not currently apply to any county, although it previously applied to Taipei County (now New Taipei City) and Taoyuan County (now Taoyuan City).
Economy and industry.
The quick industrialization and rapid growth of Taiwan during the latter half of the 20th century has been called the "Taiwan Miracle". Taiwan is one of the "Four Asian Tigers" alongside Hong Kong, South Korea and Singapore.
Japanese rule prior to and during World War II brought changes in the public and private sectors, most notably in the area of public works, which enabled rapid communications and facilitated transport throughout much of the island. The Japanese also improved public education and made it compulsory for all residents of Taiwan.
By 1945, hyperinflation was in progress in mainland China and Taiwan as a result of the war with Japan. To isolate Taiwan from it, the Nationalist government created a new currency area for the island, and began a price stabilization program. These efforts significantly slowed inflation.
When the KMT government fled to Taiwan it brought millions of taels (where 1 tael ~1.2 ozt) of gold and the foreign currency reserve of mainland China, which, according to the KMT, stabilized prices and reduced hyperinflation. Perhaps more importantly, as part of its retreat to Taiwan, the KMT brought the intellectual and business elites from Mainland China. The KMT government instituted many laws and land reforms that it had never effectively enacted on mainland China. The government also implemented a policy of import-substitution, attempting to produce imported goods domestically.
In 1950, with the outbreak of the Korean War, the United States began an aid program which resulted in fully stabilized prices by 1952. Economic development was encouraged by American economic aid and programs such as the Joint Commission on Rural Reconstruction, which turned the agricultural sector into the basis for later growth. Under the combined stimulus of the land reform and the agricultural development programs, agricultural production increased at an average annual rate of 4 per cent from 1952 to 1959, which was greater than the population growth, 3.6%.
In 1962, Taiwan had a (nominal) per-capita gross national product (GNP) of $170, placing its economy on a par with those of Zaire and Congo. On a purchasing power parity (PPP) basis, its GDP per capita in early 1960s was $1,353 (in 1990 prices). By 2011 per-capita GNP, adjusted for purchasing power parity (PPP), had risen to $37,000, contributing to a Human Development Index (HDI) equivalent to that of other developed countries. Taiwan's HDI in 2012 is 0.890, (23rd, very high), according to the UN's new "Inequality-adjusted HDI" calculation method.
In 1974, Chiang Ching-kuo implemented the Ten Major Construction Projects, the beginning foundations that helped Taiwan transform into its current export driven economy. Since the 1990s, a number of Taiwan-based technology firms have expanded their reach around the world. Well-known international technology companies headquartered in Taiwan include personal computer manufacturers Acer Inc. and Asus, mobile phone maker HTC, as well as electronics manufacturing giant Foxconn, which makes products for Apple, Amazon, and Microsoft. Computex Taipei is a major computer expo, held since 1981.
Today Taiwan has a dynamic, capitalist, export-driven economy with gradually decreasing state involvement in investment and foreign trade. In keeping with this trend, some large government-owned banks and industrial firms are being privatized. Real growth in GDP has averaged about 8% during the past three decades. Exports have provided the primary impetus for industrialization. The trade surplus is substantial, and foreign reserves are the world's fifth largest. The Republic of China has its own currency, the New Taiwan dollar.
Since the beginning of the 1990s, the economic ties between Taiwan and Mainland China have been very prolific. As of 2008, more than US$150 billion have been invested in the PRC by Taiwanese companies, and about 10% of the Taiwanese labour force works in the PRC, often to run their own businesses. Although the economy of Taiwan benefits from this situation, some have expressed the view that the island has become increasingly dependent on the Mainland Chinese economy. A 2008 white paper by the Department of Industrial Technology states that "Taiwan should seek to maintain stable relation with China while continuing to protect national security, and avoiding excessive 'Sinicization' of Taiwanese economy." Others argue that close economic ties between Taiwan and Mainland China would make any military intervention by the PLA against Taiwan very costly, and therefore less probable.
Taiwan's total trade in 2010 reached an all-time high of US$526.04 billion, according to Taiwan's Ministry of Finance. Both exports and imports for the year reached record levels, totaling US$274.64 billion and US$251.4 billion, respectively.
In 2001, agriculture constituted only 2% of GDP, down from 35% in 1952. Traditional labor-intensive industries are steadily being moved offshore and with more capital and technology-intensive industries replacing them. High-technology industrial parks have sprung up in every region in Taiwan. The ROC has become a major foreign investor in the PRC, Thailand, Indonesia, the Philippines, Malaysia, and Vietnam. It is estimated that some 50,000 Taiwanese businesses and 1,000,000 businesspeople and their dependents are established in the PRC.
Because of its conservative financial approach and its entrepreneurial strengths, Taiwan suffered little compared with many of its neighbors from the 1997 Asian financial crisis. Unlike its neighbors, South Korea and Japan, the Taiwanese economy is dominated by small and medium-sized businesses, rather than the large business groups. The global economic downturn, however, combined with poor policy coordination by the new administration and increasing bad debts in the banking system, pushed Taiwan into recession in 2001, the first whole year of negative growth since 1947. Due to the relocation of many manufacturing and labor-intensive industries to the PRC, unemployment also reached a level not seen since the 1970s oil crisis. This became a major issue in the 2004 presidential election. Growth averaged more than 4% in the 2002–2006 period and the unemployment rate fell below 4%.
The ROC often joins international organizations under a politically neutral name. The ROC is a member of governmental trade organizations such as the World Trade Organization under the name Separate Customs Territory of Taiwan, Penghu, Kinmen and Matsu (Chinese Taipei) since 2002.
Transportation.
The Ministry of Transportation and Communications of the Republic of China is the cabinet-level governing body of the transportation network in Taiwan. Taiwan has an extensive highway network, classified into five levels: National highways, provincial highways, county routes, township routes, and special routes, with the first four being common. Taiwan also has an extensive bus network, which are mostly run by private bus companies. There are two rail systems in Taiwan: Taiwan Railway Administration and Taiwan High Speed Rail. The Taipei Metro and the Kaohsiung Mass Rapid Transit serve the Taipei metropolitan area and Kaohsiung, respectively. The Taoyuan Metro and Taichung Metro are currently under construction. Major airports include Taiwan Taoyuan International Airport, Taipei Songshan Airport, Kaohsiung International Airport, and Taichung Airport. The four international seaports are the Port of Keelung, the Port of Kaohsiung, the Port of Taichung, and the Port of Hualien.
Education, research, and academia.
The higher education system was established in Taiwan by Japan during the colonial period. However, after the Republic of China took over Taiwan from Japan in 1945, the system was promptly replaced by the same system as in mainland China which mixed with features of the Chinese and American educational systems.
The educational system includes six years of elementary school, three years of middle school, three years of high school, and four years of university. The system has been successful in that pupils in Taiwan boast some of the highest test scores in the world, especially in mathematics and science; However, it has also been criticized for placing excessive pressure on students and eschewing creativity in favor of rote memorization.
Many Taiwanese students attend cram schools, or bushiban, to improve skills and knowledge on problem solving against exams of subjects like mathematics, nature science, history and many others. Courses are available for most popular subjects. Lessons are organized in lectures, reviews, private tutorial sessions, and recitations.
As of 2013, the literacy rate in Taiwan is 97.15%.
Demographics.
Taiwan's population is about 23.4 million, most of whom are on the island proper. The remainder live on Penghu (100,400), Kinmen (120,713) and the Matsu Islands (12,165).
Ethnic groups.
The ROC government reports that over 95% of the population is Han Chinese, of which the majority includes descendants of early Han Chinese immigrants who arrived in Taiwan in large numbers starting in the 17th century. Alternatively, the ethnic groups of Taiwan may be roughly divided among the "Taiwanese" (84%, including Hakka), mainland Chinese (14%), and indigenous peoples (2%).
The Hoklo people are the largest Han subgroup (70% of the total population), whose ancestors migrated from the coastal southern Fujian region across the Taiwan Strait starting in the 17th century. The Hakka comprise about 15% of the total population, and descend from Han migrants to Guangdong, its surrounding areas and Taiwan. Additional people of Han origin include and descend from the 2 million Nationalists who fled to Taiwan following the communist victory on the mainland in 1949.
The indigenous Taiwanese aborigines number about 533,600 and are divided into 16 recognized groups. The Ami, Atayal, Bunun, Kanakanavu, Kavalan, Paiwan, Puyuma, Rukai, Saisiyat, Saaroa, Sakizaya, Sediq, Thao, Truku and Tsou live mostly in the eastern half of the island, while the Yami inhabit Orchid Island.
Languages.
Mandarin is the official national language and is spoken by the vast majority of the population of Taiwan. It has been the primary language of instruction in schools since the end of Japanese rule. As in Hong Kong and Macau, Traditional Chinese is used as the writing system in Taiwan.
The 70% of the population belonging to the Hoklo ethnic group speak Taiwanese Hokkien (a variant of the Min Nan speech of Fujian province) as their mother tongue, in addition to Mandarin, and many others have some degree of understanding. The Hakka ethnic group (15% of the population) use the Hakka language. Most "waishengren" speak primarily Mandarin. Although Mandarin is the language of instruction in schools and dominates television and radio, non-Mandarin languages or dialects have undergone a revival in public life in Taiwan, particularly since restrictions on their use were lifted in the 1990s.
Taiwan's indigenous languages, the Formosan languages, do not belong to the Chinese or Sino-Tibetan language family, but rather to the Austronesian language family. Their use among Taiwan's aboriginal minority groups has been in decline as usage of Mandarin has risen. Of the 14 extant languages, five are considered moribund.
Religion.
The Constitution of the Republic of China protects people's freedom of religion and the practices of belief. There are approximately 18,718,600 religious followers in Taiwan as of 2005 (81.3% of total population) and 14–18% are non-religious. According to the 2005 census, of the 26 religions recognized by the ROC government, the five largest are: Buddhism (8,086,000 or 35.1%), Taoism (7,600,000 or 33%), Yiguandao (810,000 or 3.5%), Protestantism (605,000 or 2.6%), and Roman Catholicism (298,000 or 1.3%).
The CIA World Factbook reports that over 93% of Taiwanese are adherents of a combination of the polytheistic ancient Chinese religion, Buddhism, Confucianism, and Taoism; 4.5% are adherents of Christianity, which includes Protestants, Catholics, and other, non-denominational, Christian groups; and less than 2.5% are adherents of other religions, such as Islam; it is not clear how recent its figures are. Taiwanese aborigines comprise a notable subgroup among professing Christians: "...over 64% identify as Christian... Church buildings are the most obvious markers of Aboriginal villages, distinguishing them from Taiwanese or Hakka villages."
Confucianism is a philosophy that deals with secular moral ethics, and serves as the foundation of both Chinese and Taiwanese culture. The majority of Taiwanese people usually combine the secular moral teachings of Confucianism with whatever religions they are affiliated with.
As of 2009, there were 14,993 temples in Taiwan, approximately one place of worship per 1,500 residents. 9,202 of those temples were dedicated to Taoism. In 2008, Taiwan had 3,262 Churches, an increase of 145.
Largest cities.
The figures below are the 2011 estimates for the twenty largest urban populations within administrative city limits; a different ranking exists when considering the total metropolitan area populations (in such rankings the Taipei-Keelung metro area is by far the largest agglomeration).
Public health.
Health care in Taiwan is managed by the Bureau of National Health Insurance (BNHI).
The current program was implemented in 1995, and is considered to be a form of social insurance. The government health insurance program maintains compulsory insurance for citizens who are employed, impoverished, unemployed, or victims of natural disasters with fees that correlate to the individual and/or family income; it also maintains protection for non-citizens working in Taiwan. A standardized method of calculation applies to all persons and can optionally be paid by an employer or by individual contributions.
BNHI insurance coverage requires co-payment at the time of service for most services unless it is a preventative health service, for low-income families, veterans, children under three years old, or in the case of catastrophic diseases. Low income households maintain 100% premium coverage by the BNHI and co-pays are reduced for disabled or certain elderly peoples.
According to a recently published survey, out of 3,360 patients surveyed at a randomly chosen hospital, 75.1% of the patients said they are "very satisfied" with the hospital service; 20.5% said they are "okay" with the service. Only 4.4% of the patients said they are either "not satisfied" or "very not satisfied" with the service or care provided.
Taiwan has its own Center for Disease Control, and during the SARS outbreak in March 2003 there were 347 confirmed cases. During the outbreak the Centers for Disease Control and local governments set up monitored stations throughout public transportation, recreational sites and other public areas. With full containment in July 2003, there has not been a case of SARS since.
As of 2006, the BNHI Facility Contract Distribution facilities total 17,259, including:
Basic coverage areas of the insurance include:
In 2004, the infant mortality rate was 5.3 with 15 physicians and 63 hospital beds per 10,000 people. The life expectancy for males was 73.5 years and 79.7 years for females according to the World Health Report.
In July 2013, the Department of Health was restructured as the Ministry of Health and Welfare.
Culture.
 The cultures of Taiwan are a hybrid blend of various sources, incorporating elements of traditional Chinese culture, attributable to the historical and ancestry origin of the majority of its current residents, Japanese culture, traditional Confucianist beliefs, and increasingly Western values.
After their move to Taiwan, the Kuomintang imposed an official interpretation of traditional Chinese culture over Taiwan. The government launched a program promoting Chinese calligraphy, traditional Chinese painting, folk art, and Chinese opera.
The status of Taiwanese culture is debated. It is disputed whether Taiwanese culture is a regional form of Chinese culture or a distinct culture. Reflecting the continuing controversy surrounding the political status of Taiwan, politics continues to play a role in the conception and development of a Taiwanese cultural identity, especially in the prior dominant frame of a Taiwanese and Chinese dualism. In recent years, the concept of Taiwanese multiculturalism has been proposed as a relatively apolitical alternative view, which has allowed for the inclusion of mainlanders and other minority groups into the continuing re-definition of Taiwanese culture as collectively held systems of meaning and customary patterns of thought and behavior shared by the people of Taiwan. Identity politics, along with the over one hundred years of political separation from mainland China, has led to distinct traditions in many areas, including cuisine and music.
One of Taiwan's greatest attractions is the National Palace Museum, which houses more than 650,000 pieces of Chinese bronze, jade, calligraphy, painting, and porcelain and is considered one of the greatest collections of Chinese art and objects in the world. The KMT moved this collection from the Forbidden City in Beijing in 1933 and part of the collection was eventually transported to Taiwan during the Chinese Civil War. The collection, estimated to be one-tenth of China's cultural treasures, is so extensive that only 1% is on display at any time. The PRC had said that the collection was stolen and has called for its return, but the ROC has long defended its control of the collection as a necessary act to protect the pieces from destruction, especially during the Cultural Revolution. Relations regarding this treasure have warmed recently; Beijing Palace Museum Curator Zheng Xinmiao said that artifacts in both Chinese and Taiwanese museums are "China's cultural heritage jointly owned by people across the Taiwan Strait."
The classical music culture in Taiwan is highly developed and features artists such as violinist Cho-Liang Lin, pianist Ching-Yun Hu, and the Lincoln Center Chamber Music Society Artist Director Wu Han. Karaoke, drawn from contemporary Japanese culture, is extremely popular in Taiwan, where it is known as KTV. KTV businesses operate in a hotel-like style, renting out small rooms and ballrooms varying on the number of guests in a group. Many KTV establishments partner with restaurants and buffets to form all-encompassing elaborate evening affairs for families, friends, or businessmen. Tour buses that travel around Taiwan have several TV's, equipped not for watching movies, but primarily for singing Karaoke. The entertainment counterpart of a KTV is an MTV, being found much less frequently out of the city. There, movies out on DVD can be selected and played in a private theater room. However, MTV, more so than KTV, has a growing reputation for being a place that young couples will go to be alone and intimate.
Taiwan has a high density of 24-hour convenience stores, which, in addition to the usual services, provide services on behalf of financial institutions or government agencies such as collection of parking fees, utility bills, traffic violation fines, and credit card payments. They also provide a service for mailing packages.
Taiwanese culture has also influenced other cultures. Bubble tea and milk tea are available in Singapore, Malaysia, Australia, Europe, and North America. Taiwan television shows are popular in Singapore, Malaysia, and other Asian countries. Taiwanese films have won various international awards at film festivals around the world. Ang Lee, a Taiwanese director, has directed critically acclaimed films such as: "Crouching Tiger, Hidden Dragon"; "Eat Drink Man Woman"; "Sense and Sensibility"; "Brokeback Mountain"; "Life of Pi"; and "Lust, Caution". Other famous Taiwanese directors include Tsai Ming-Liang, Edward Yang, and Hou Hsiao-hsien.
Sports.
Baseball is Taiwan's national sport and it is a popular spectator sport. Two of the most famous Taiwanese baseball pitchers are Chien-Ming Wang and Wei-Yin Chen, both are starting pitchers in Major League Baseball. Other notable players playing in the United States include Chin-hui Tsao who played for the Colorado Rockies (2003–2005) and the Los Angeles Dodgers (2007), Hong-Chih Kuo, Fu-Te Ni, and Chin-lung Hu. The Chinese Professional Baseball League in Taiwan was established in 1989, and eventually absorbed the competing Taiwan Major League in 2003. s of 2008[ [update]], the CPBL has four teams with average attendance of approximately 3,000 per game.
Besides baseball, basketball is Taiwan's major sport. Taekwondo has also become a mature and successful sport in recent years. In the 2004 Olympics, Chen Shih-hsin and Chu Mu-yen won the first two gold medals in women's flyweight event and men's flyweight event, respectively. Subsequent taekwondo competitors such as Yang Shu-chun have strengthened Taiwan's taekwondo culture.
Taiwan participates in international sporting organizations and events under the name of "Chinese Taipei" due to its political status. In 2009, Taiwan hosted two international sporting events on the island. The World Games 2009 were held in Kaohsiung between 16 and 26 July 2009. Taipei hosted the 21st Summer Deaflympics in September of the same year. Furthermore, Taipei will host the Summer Universiade in 2017.
Taiwan is also a major Asian country for Korfball. In 2008, Taiwan hosted the World Youth Korfball Championship and took the silver medal. In 2009, Taiwan's korfball team won a bronze medal at the World Game.
Yani Tseng is the most famous Taiwanese professional golfer currently playing on the U.S.-based LPGA Tour. She is the youngest player ever, male or female, to win five major championships and had been ranked number 1 in the Women's World Golf Rankings for 109 consecutive weeks from 2011 to 2013.
Calendar.
Taiwan uses two official calendars: the Gregorian calendar and the Minguo calendar. The latter numbers years starting from 1911, the year of the founding of the Republic of China. For example, 2007 is the "96th year of the Republic" (民國96年), while its months and days are numbered according to the Gregorian calendar.
Usually, year numbering may use the Gregorian system as well as the ROC era system. For example, 3 May 2004, may be written 2004-05-03 or 93–05–03. The use of two different calendar systems in Taiwan may be confusing, in particular for foreigners. For instance, products for export marked using the Minguo calendar can be misunderstood as having an expiration date 11 years earlier than intended.
Taiwan also uses the lunar calendar for traditional festivals such as the Chinese New Year, the Lantern Festival, and the Dragon Boat Festival.
References.
Works cited.
</dl>
Further reading.
</dl>

</doc>
<doc id="25735" url="http://en.wikipedia.org/wiki?curid=25735" title="Rugby league">
Rugby league

Rugby league football, usually called rugby league, or simply league, is a full contact sport played by two teams of thirteen players on a rectangular field. One of the two codes of rugby football, it originated in England in 1895 as a split from the Rugby Football Union over the issue of payments to players. Its rules gradually changed with the purpose of producing a faster, more entertaining game for spectators. It is frequently cited as the toughest, most physically demanding of team sports.
In rugby league points are scored by carrying or kicking the ball down the field, until it can be moved past the opponents' designated goal line and touched to the ground; this is called a "try", and is the primary method of scoring. The opposing team attempts to stop the attacking side gaining points by preventing their progress up the field by tackling the player carrying the ball. In addition to tries, points can be scored by kicking goals. After each try, the scoring team gains a free kick to "try at goal" with a conversion for further points. Kicks at goal may also be awarded for penalties, and field goals can be attempted at any time during general play.
Rugby league is among the most popular sports in Northern England, Australia, New Zealand, France, Tonga and Papua New Guinea, where it is the national sport. The European Super League and Australasian National Rugby League (NRL) are the premier club competitions. Rugby league is played internationally, predominantly by European, Australasian and Pacific countries, and is governed by the Rugby League International Federation. The first Rugby League World Cup was held in France in 1954; the current holders are Australia.
Etymology.
"Rugby league football" takes its name from the bodies that split to create a new form of rugby football, distinct from that run by the "Rugby Football Unions", in Britain, Australia and New Zealand between 1895 and 1908.
The first of these, the Northern Rugby Football Union, was established in 1895 as a breakaway faction of England's Rugby Football Union (RFU). Both organisations played the game under the same rules at first, although the Northern Union began to modify rules almost immediately, thus creating a new faster paced form of rugby football. Similar breakaway factions split from RFU-affiliated unions in Australia and New Zealand in 1907 and 1908, renaming themselves "rugby football "league"s" and introducing Northern Union rules. In 1922, the Northern Union also changed its name to the Rugby Football League and thus over time the sport itself became known as "rugby league" football.
History.
In 1895, a schism in Rugby football resulted in the formation of the Northern Rugby Football Union (NRFU). Although many factors played a part in the split, including the success of working class northern teams, the main division was caused by the RFU decision to enforce the amateur principle of the sport, preventing "broken time payments" to players who had taken time off work to play rugby. Northern teams typically had more working class players (coal miners, mill workers etc.) who could not afford to play without this compensation, in contrast to affluent southern teams who had other sources of income to sustain the amateur principle. In 1895, a decree by the RFU banning the playing of rugby at grounds where entrance fees were charged led to twenty-two clubs (including Stockport who negotiated by telephone) meeting at the George Hotel, Huddersfield on 29 August 1895 and forming the "Northern Rugby Football Union". Within fifteen years of that first meeting in Huddersfield, more than 200 RFU clubs had left to join the rugby revolution.
In 1897, the line-out was abolished and in 1898 professionalism introduced. In 1906, the Northern Union changed its rules, reducing teams from 15 to 13 a side and replacing the ruck formed after every tackle with the play the ball.
A similar schism to that which occurred in England took place in Sydney, Australia. There, on 8 August 1907 the New South Wales Rugby Football League was founded at Bateman's Hotel in George Street. Rugby league then went on to displace rugby union as the primary football code in New South Wales and Queensland.
On 5 May 1954 over 100,000 (official figure 102,569) spectators watched the 1953–54 Challenge Cup final at Odsal Stadium, Bradford, England, setting a new record for attendance at a rugby football match of either code. Also in 1954 the Rugby League World Cup, the first for either code of rugby, was formed at the instigation of the French. In 1966, the International Board introduced a rule that a team in possession was allowed three play-the-balls and on the fourth tackle a scrum was to be formed. This was increased to six tackles in 1972 and in 1983 the scrum was replaced by a handover. 1967 saw the first professional Sunday matches of rugby league played.
The first sponsors, Joshua Tetley and John Player, entered the game for the 1971–72 Northern Rugby Football League season. Television would have an enormous impact on the sport of rugby league in the 1990s when Rupert Murdoch's News Corporation sought worldwide broadcasting rights and refused to take no for an answer. The media giant's "Super League" movement saw big changes for the traditional administrators of the game. In Europe it resulted in a move from a winter sport to a summer one as the new Super League competition tried to expand its market. In Australasia, the Super League war resulted: long and costly legal battles and changing loyalties, causing significant damage to the code in an extremely competitive sporting market. In 1997 two competitions were run alongside each other in Australia, after which a peace deal in the form of the National Rugby League was formed. The NRL has since become recognised as the sport's flagship competition and since that time has set record TV ratings and crowd figures.
Rules.
The objective in rugby league is to score more points through tries, goals (also known as conversions) and field goals (also known as drop goals) than the opposition within the 80 minutes of play. If after two halves of play, each consisting of forty minutes, the two teams are drawing, a draw may be declared, or the game may enter extra time under the golden point rule, depending on the relevant competition's format.
The try is the most common form of scoring, and a team will usually attempt to score one by running and kicking the ball further upfield, or passing from player-to-player in order to manoeuvre around the opposition's defence. A try involves touching the ball to the ground on or beyond the defending team's goal-line and is worth four points. A goal is worth two points and may be gained from a conversion or a penalty. A field goal, or drop goal, is only worth one point and is gained by dropping and then kicking the ball on the half volley between the uprights in open play.
Field position is crucial in rugby league, achieved by running with or kicking the ball. Passing in rugby league may only be in a backward or sideways direction. Teammates therefore have to remain on-side by not moving ahead of the player with the ball. However the ball may be kicked ahead for teammates, but again, if they are in front of the kicker, when the ball is kicked, they are deemed off-side. Tackling is a key component of rugby league play. Only the player holding the football may be tackled. A tackle is completed when that player's progress is halted, or he is put to ground. An attacking team gets a maximum of six tackles to progress up the field before possession is changed over. Ball control is also important in rugby league, as a fumble of the ball on the ground forces a handover, unless the ball is fumbled backwards. The ball can also be turned over by going over the sideline.
Positions.
Players on the pitch are divided into forwards and backs, although the game's rules apply to all players the same way. Each position has a designated number to identify himself from other players. These numbers help to identify which position a person is playing. The system of numbering players is different depending on which country the match is played in. In Australia and New Zealand, each player is usually given a number corresponding to their playing position on the field. However, since 1996 European teams have been able to grant players specific squad numbers, which they keep in irrelevance to the position they play, similarly to association football.
Interchanges (generally referred to as "The Bench") are allowed in the sport, and are typically used when a player gets tired or injured, although they can also be used tactically. Each team is currently allowed four substitutes, and in Australia and New Zealand, these players occupy shirt numbers 14 to 22. There are no limitations on which players must occupy these interchangeable slots. Generally, twelve interchanges are allowed in any game from each team, although in the National Rugby League, this was reduced to ten prior to the 2008 season. If a team has to interchange a player due to the Blood Bin rule or due to injury, and this was the result of misconduct from the opposing team, the compromised team does not have to use one of its allocated interchanges to take the player in question off the field.
Backs.
The backs are generally smaller, faster and more agile than the forwards. They are often the most creative and evasive players on the field, relying on running, kicking and handling skills, as well as tactics and set plays, to break the defensive line, instead of brute force. Generally forwards do the majority of the work (hit-ups/tackling).
Usually, the Five-Eighth and Half-Back are a team's creative unit or 'playmakers'. During the interactions between a team's 'key' players (Five-Eighth, Half-Back, Fullback, Lock Forward, and Hooker), the Five-Eighth and Half-Back will usually be involved in most passing moves.
Forwards.
The forwards' two responsibilities can be broken into "normal play" and "scrum play". For information on a forward's role in the scrum see rugby league scrummage. Forward positions are traditionally named after the player's position in the scrum yet are equal with respect to "normal play" with the exception of the hooker. Forward positions are traditionally assigned as follows:
Rugby league worldwide.
Rugby league is played in over 40 nations throughout the world, 34 are ranked by the RLIF and a further 15 are officially recognized and unranked. The strongest rugby league nations are Australia, England and New Zealand. The Rugby League World Cup is the highest form of representative rugby league and currently features 14 teams (ordered by RLIF rank) (1) Australia, (2) New Zealand, (3) England, (4) France, (5) Fiji, (6) Wales,(7) Papua New Guinea,(8) Samoa, (9) Ireland, (10) USA, (11) Scotland, (12) Italy (13) Tonga, and (14) Cook Islands. (20) Lebanon, (15) Russia and (25) South Africa have previously contested in World Cups. The current World Champions are Australia, who won the 2013 Rugby League World Cup.
The game is particularly popular in the nations of the South Pacific. Rugby League is the 3rd most attended sport in Australia and neighbouring Papua New Guinea is the only country to have rugby league as its national sport. This has led to officials from that country lobbying to have a team admitted to the National Rugby League, Australia's elite club competition which also features a team from Auckland, New Zealand's biggest city. The popularity of the code in New Zealand has led to the possibility of another team from the South Island entering the competition as well. Rugby league is the dominant winter sport in the eastern Australian states of New South Wales, Queensland and Australian Capital Territory, The game is also among the predominant sports of Tonga and is played in other Pacific nations such as Samoa and the Cook Islands. In Australia, and indeed the rest of the region, the annual State of Origin series ranks among the most popular sporting events.
The Rugby League European Federation are responsible for developing rugby league in Europe and the Northern Hemisphere, while the Asia Pacific Rugby League Confederation are responsible for developing rugby league in the Asia-Pacific region. The Rugby League European Cup and Rugby League Pacific Cup are both run by the RLEF and APRLC respectively and are used as a stepping stone to the Rugby League Four Nations with Australia, New Zealand and England, the fourth team being decided by who wins the Pacific and European Cup, and rotates each year from Europe to the Pacific.
In England, rugby league has traditionally been associated with the northern counties of Yorkshire, Lancashire and Cumbria where the game originated, although its popularity has also increased elsewhere. Currently, two of the fourteen Super League teams are based outside of these traditional counties: London Broncos and Catalans Dragons. Figures published by the Rugby Football League showed an 81% increase in women playing the sport in the twelve months prior to October 2008, as well as an increase in juniors of both genders nationwide. Over 40,000 players were registered by the RFL as of October 2008 with an overall participation rate in the game doubling in the last four years to well over 285,000 by late 2009.
France first played rugby league as late as 1934, where in the five years prior to World War II, the sport's popularity increased as Frenchmen became disenchanted with the state of French rugby union in the 1930s. However, after the Allied Forces were defeated by Germany in June 1940, the Vichy regime in the south seized assets belonging to rugby league authorities and clubs and banned the sport for its association with the left-wing Popular Front government that had governed France before the War. The sport was unbanned after the Liberation of Paris in August 1944 and the collapse of the Vichy regime, although it was still actively marginalised by the French authorities until the 1990s. Despite this, the national side appeared in the finals of the 1954 and 1968 World Cups, and the country hosted the 1954 event. In 1996, a French team, Paris Saint-Germain was one of eleven teams which formed the new European Super League, although the club was dissolved in 1997 due to its failure to run at a profit and poor attendances. In 2006, the Super League admitted the Catalans Dragons, a team from Perpignan in the southern Languedoc-Roussillon region. They have subsequently reached the 2007 Challenge Cup Final and made the play-offs of the 2008 Super League XIII season. The success of the 'Dragons' in Super League has initiated a renaissance in French rugby league, with new-found enthusiasm for the sport in the south of the country where most of the Elite One Championship teams are based.
The early 21st century has seen take up the game and compete in international rugby league with efforts being made by the Rugby League European Federation to expand the game to new areas such as Germany, Sweden, Norway and Hungary.
Domestic competitions.
The two most prominent fully professional leagues are the Australasian National Rugby League and the European Super League and to a lesser extent the semi professional French Elite One Championship and Elite Two Championship. Domestic semi professional leagues exist below the NRL and Super League, especially on a state or county level, in Australia the Queensland Cup (which includes a team from Papua New Guinea) and NSW Cup, which provides players to various NRL teams. In the United Kingdom below Super League is the Championship and Championship 1. The Fiji National Rugby League Competition also runs semi professional teams. In other countries the game is played at an amateur level.
Attendances.
While rugby league does not enjoy the same international profile as rugby union or soccer, mostly due to the game only being played at the highest level in Australia/New Zealand (NRL) and in UK/France (Super League), the game continues to attract large numbers to both international and domestic competitions.
International.
The top five attendances for rugby league test matches (International) are:
Domestic.
The top five attendances for domestic based rugby league matches are:

</doc>
<doc id="25736" url="http://en.wikipedia.org/wiki?curid=25736" title="Rowing (sport)">
Rowing (sport)

Rowing, often referred to as crew in the United States, is a sport with origins back to Ancient Egyptian times. It is based on propelling a boat (racing shell) on water using oars. By pushing against the water with an oar, a force is generated to move the boat. The sport can be either recreational - focusing on learning the technique of rowing, or competitive - where athletes race against each other in boats. There are a number of different boat classes in which athletes compete, ranging from an individual shell (called a single scull) to an eight person shell with coxswain (called a coxed eight).
Modern rowing as a competitive sport can be traced to the early 18th century when races were held between professional watermen on the River Thames in London, United Kingdom. Often prizes were offered by the London Guilds and Livery Companies. Amateur competition began towards the end of the 18th century with the arrival of "boat clubs" at the British public schools of Eton College and Westminster School. Similarly, clubs were formed at the University of Oxford, with a race held between Brasenose College and Jesus College in 1815. At the University of Cambridge the first recorded races were in 1827. Public rowing clubs were beginning at the same time; in England Leander Club was founded in 1818, in Germany Der Hamburger und Germania Ruder Club was founded in 1836 and in the United States Narragansett Boat Club was founded in 1838 and Detroit Boat Club was founded in 1839. In 1843, the first American college rowing club was formed at Yale University.
The International Rowing Federation (French: "Fédération Internationale des Sociétés d’Aviron", abbreviated FISA) is responsible for international governance of rowing and was founded in 1892 to provide regulation at a time when the sport was gaining popularity. Across six continents there are now 118 countries with rowing federations that participate in the sport.
Rowing is one of the oldest Olympic sports and has been competed since 1900. Women's rowing was added to the Olympic programme in 1976. Today, only fourteen boat classes are raced at the Olympics, across men and women. 
Each year the World Rowing Championships is held by FISA with 22 boat classes raced. In Olympic years only the non-Olympic boat classes are raced at the World Championships. The European Rowing Championships are held annually, along with three World Rowing Cups in which each event earns a number of points for a country towards the World Cup title. Since 2008, rowing has also been competed at the Paralympic Games.
Major domestic competitions take place in dominant rowing nations and include The Boat Race and Henley Royal Regatta in the United Kingdom, the Australian Rowing Championships in Australia, the Harvard-Yale Regatta and Head of the Charles Regatta in the United States and Royal Canadian Henley Regatta in Canada. Many other competitions often exist for racing between clubs, schools and universities in each nation.
Basic information.
While rowing, the athlete sits in the boat facing toward the stern, and uses the oars which are held in place by the oarlocks to propel the boat forward (towards the bow). This may be done on a canal, river, lake, sea, or other large bodies of water. The sport requires strong core balance, physical strength, flexibility, and cardiovascular endurance.
Whilst the action of rowing and equipment used remains fairly consistent throughout the world, there are many different types of competition. These include endurance races, time trials, stake racing, bumps racing, and the side-by-side format used in the Olympic games. The many different formats are a result of the long history of the sport, its development in different regions of the world, and specific local requirements and restrictions.
There are two forms of rowing:
Anatomy of a stroke.
The rowing stroke may be characterized by two fundamental reference points. The "catch", which is placement of the oar blade in the water, and the "extraction", also known as the "finish" or "release", when the rower removes the oar blade from the water. The action between catch and release is the first phase of the stroke that propels the boat.
At the catch the rower places the blade in the water and applies pressure to the oar by pushing the seat toward the bow of the boat by extending the legs, thus pushing the boat through the water. The point of placement of the blade in the water is a relatively fixed point about which the oar serves as a lever to propel the boat. As the rower's legs approach full extension, the rower pivots the torso toward the bow of the boat and then finally pulls the arms towards his or her chest. The hands meet the chest right above the diaphragm.
At the end of the stroke, with the blade still in the water, the hands drop slightly to unload the oar so that spring energy stored in the bend of the oar gets transferred to the boat, which eases removing the oar from the water and minimizes energy wasted on lifting water above the surface (splashing).
The recovery phase follows the drive. The recovery starts with the extraction and involves coordinating the body movements with the goal to move the oar back to the catch position. In extraction, the rower pushes down on the oar handle to quickly lift the blade from the water and rapidly rotates the oar so that the blade is parallel to the water. This process is sometimes referred to as "feathering the blade". Simultaneously, the rower pushes the oar handle away from the chest. The blade emerges from the water square and feathers immediately once clear of the water. After feathering and extending the arms, the rower pivots the body forward. Once the hands are past the knees, the rower compresses the legs which moves the seat towards the stern of the boat. The leg compression occurs relatively slowly compared to the rest of the stroke, which affords the rower a moment to recover, and allows the boat to glide through the water. The gliding of the boat through the water during recovery is often called "run".
A controlled slide is necessary to maintain momentum and achieve optimal boat run. However, various teaching methods disagree about the optimal relation in timing between drive and recovery. Near the end of the recovery, the rower squares the blade into perpendicular orientation with respect to the water, and begins another stroke.
Breathing during a rowing stroke.
There are two schools of thought with respect to the appropriate breathing technique during the rowing motion: Full lungs at the catch and empty lungs at the catch.
With the full lung technique, rowers exhale during the stroke and inhale during the recovery. In laboured circumstances, rowers will take a quick pant at the end of the stroke before taking a deep breath on the recovery that fills the lungs by the time the catch is reached.
In the empty-lung technique, rowers inhale during the drive, and exhale during the recovery so that they have empty lungs at the catch. Because the knees come up to the chest when the lungs are empty, this technique allows the rower to reach a little bit further than if the lungs were full of air. Full lungs at the release also can help the rower to maintain a straighter back, a style encouraged by many coaches.
A scientific study of the benefits of entrained breathing technique in relatively fit, but untrained rowers did not show any physiological or psychological benefit to either technique.
Rowing propulsion.
Rowing is a cyclic (or intermittent) form of propulsion such that in the quasi-steady state the motion of the system (the system comprising the rower, the oars, and the boat), is repeated regularly. In order to maintain the steady-state propulsion of the system without either accelerating or decelerating the system, the sum of all the external forces on the system, averaged over the cycle, must be zero. Thus, the average drag (retarding) force on the system must equal the average propulsion force on the system. The drag forces consist of aerodynamic drag on the superstructure of the system (components of the boat situated above the waterline), as well as the hydrodynamic drag on the submerged portion of the system. The propulsion forces are the forward reaction of the water on the oars while in the water. Note also that the oar can be used to provide a drag force (a force acting against the forward motion) when the system is brought to rest.
Although the oar can be conveniently thought of as a lever with a "fixed" pivot point in the water, the blade moves sideways and sternwards through the water, so that the magnitude of the propulsion force developed is the result of a complex interaction between unsteady fluid mechanics (the water flow around the blade) and solid mechanics and dynamics (the handle force applied to the oar, the oar's inertia and bending characteristic, the acceleration of the boat and so on).
Distinction from other watercraft.
The distinction between rowing and other forms of water transport, such as canoeing or kayaking, is that in rowing the oars are held in place at a pivot point that is in a fixed position relative to the boat, this point is the load point for the oar to act as a second class lever (the blade fixed in the water is the fulcrum). In flatwater rowing, the boat (also called a "shell" or "fine boat") is narrow to avoid drag, and the oars are attached to oarlocks at the end of outriggers extending from the sides of the boat. Racing boats also have sliding seats to allow the use of the legs in addition to the body to apply power to the oar. Racing shells are inherently unstable, much like racing kayaks or canoes. The rowing boats require oars on either side to prevent them from rolling over.
Fitness and health.
Rowing is one of the few non-weight bearing sports that exercises all the major muscle groups, including quads, biceps, triceps, lats, glutes and abdominal muscles. In fact, racing a 2k is as physically demanding as playing 2 basketball games back-to-back. The sport also improves cardiovascular endurance and muscular strength. High-performance rowers tend to be tall and muscular: although extra weight does increase the drag on the boat, the larger athletes' increased power tends to be more significant. The increased power is achieved through increased length of leverage on the oar through longer limbs of the athlete. In multi-person boats (2,4,or 8), the lightest person typically rows in the bow seat at the front of the boat.
Rowing is a low impact activity with movement only in defined ranges, so twist and sprain injuries are rare. However, the repetitive rowing action can put strain on knee joints, the spine and the tendons of the forearm, and inflammation of these are the most common rowing injuries. If one rows with poor technique, especially rowing with a curved rather than straight back, other injuries may surface, including back pains. Blisters occur for almost all rowers, especially in the beginning of one's rowing career, as every stroke puts pressure on the hands, though rowing frequently tends to harden hands and generate protective calluses. Holding the oars too tightly or making adjustments to technique may cause recurring or new blisters, as it is common to feather the blade (previously described). Another common injury is getting "track bites", thin cuts on one's calf caused by contact with the track on the drive and/or the recovery.
History.
Even since the earliest recorded references to rowing, the sporting element has been present. An Egyptian funerary inscription of 1430 BC records that the warrior Amenhotep (Amenophis) II was also renowned for his feats of oarsmanship. In the Aeneid, Virgil mentions rowing forming part of the funeral games arranged by Aeneas in honour of his father. In the 13th century, Venetian festivals called "regata" included boat races among others.
The first known "modern" rowing races began from competition among the professional watermen in the United Kingdom that provided ferry and taxi service on the River Thames in London. Prizes for wager races were often offered by the London Guilds and Livery Companies or wealthy owners of riverside houses.
The oldest surviving such race, Doggett's Coat and Badge was first contested in 1715 and is still held annually from London Bridge to Chelsea. During the 19th century these races were to become numerous and popular, attracting large crowds. Prize matches amongst professionals similarly became popular on other rivers throughout Great Britain in the 19th century, notably on the Tyne. In America, the earliest known race dates back to 1756 in New York, when a pettiauger defeated a Cape Cod whaleboat in a race.
Amateur competition in England began towards the end of the 18th century. Documentary evidence from this period is sparse, but it is known that the Monarch Boat Club of Eton College and the Isis Club of Westminster School were both in existence in the 1790s. The Star Club and Arrow Club in London for gentlemen amateurs were also in existence before 1800. At the University of Oxford bumping races were first organised in 1815 when Brasenose College and Jesus College boat clubs had the first annual race while at Cambridge the first recorded races were in 1827. Brasenose beat Jesus to win Oxford University's first Head of the River; the two clubs claim to be the oldest established boat clubs in the world. The Boat Race between Oxford University and Cambridge University first took place in 1829, and was the second intercollegiate sporting event (following the first Varsity Cricket Match by 2 years). The interest in the first Boat Race and subsequent matches led the town of Henley-on-Thames to begin hosting an annual regatta in 1839.
Founded in 1818, Leander Club is the world's oldest public rowing club. The second oldest club which still exists is the Der Hamburger und Germania Ruder Club which was founded 1836 and marked the beginning of rowing as an organized sport in Germany. During the 19th century, as in England, wager matches in North America between professionals became very popular attracting vast crowds. was founded in 1838 exclusively for rowing. During an 1837 parade in Providence, R.I, a group of boatmen were pulling a longboat on wheels, which carried the oldest living survivor of the 1772 Gaspee Raid. They boasted to the crowd that they were the fastest rowing crew on the Bay. A group of Providence locals took issue with this and challenged them to race, which the Providence group summarily won. The six-man core of that group went on the following year to found NBC in 1838. Detroit Boat Club was founded in 1839 and is the oldest continuously-operated rowing club in the U.S. In 1843, the first American college rowing club was formed at Yale University. The Harvard-Yale Regatta is the oldest intercollegiate sporting event in the United States,http://rowinghistory.net/Time%20Line/TL%20-1849images.htm having been contested every year since 1852 (excepting interruptions for wars).
FISA.
FISA, the “Fédération Internationale des Sociétés d’Aviron” in French (or the English equivalent "International Federation of Rowing Associations") was founded by representatives from France, Switzerland, Belgium, Adriatica (now a part of Italy) and Italy in Turin on 25 June 1892. It is the oldest international sports federation in the Olympic movement.
FISA first organized a European Rowing Championships in 1893. An annual World Rowing Championships was introduced in 1962. Rowing has also been conducted at the Olympic Games since 1900 (cancelled at the first modern Games in 1896 due to bad weather).
Equipment.
Racing boats (often called "shells") are long, narrow, and broadly semi-circular in cross-section in order to reduce drag to a minimum. There is some trade off between boat speed and stability in choice of hull shape. They usually have a fin towards the rear, to help prevent roll and yaw and to increase the effectiveness of the rudder.
Originally made from wood, shells are now almost always made from a composite material (usually a double skin of carbon-fibre reinforced plastic with a sandwich of honeycomb material) for strength and weight advantages. FISA rules specify minimum weights for each class of boat so that no individual team will gain a great advantage from the use of expensive materials or technology.
There are several different types of boats. They are classified using:
Although sculling and sweep boats are generally identical to each other (except having different riggers), they are referred to using different names:
With the smaller boats, specialist versions of the shells for sculling can be made lighter. The riggers in sculling apply the forces symmetrically to each side of the boat, whereas in sweep oared racing these forces are staggered alternately along the boat. The sweep oared boat has to be stiffer to handle these unmatched forces, so consequently requires more bracing and is usually heavier – a pair (2-) is usually a more robust boat than a double scull (2x) for example, and being heavier is also slower when used as a double scull. In theory this could also apply to the 4x and 8x, but most rowing clubs cannot afford to have a dedicated large hull which might be rarely used and instead generally opt for versatility in their fleet by using stronger shells which can be rigged for either sweep rowing or sculling. The symmetrical forces also make sculling more efficient than rowing: the double scull is faster than the coxless pair, and the quadruple scull is faster than the coxless four.
One additional boat is the "queep", a coxed or non-coxed shell. The bow and stroke positions have a set of sculling riggers and two and three have a sweep set. These shells have been used in the UK and recently at a club in Victoria BC, Canada. In addition to the queep the "trop" and the "coxed trop" are become more mainstream. They are mainly rowed in central Canada. The trop shell consists of three people where the bow has a pair of sculling oars, and 2,3 each a sweeping oar. A coxed trop is the same configuration as the trop plus a coxed seated at the stern of the boat. 
Many adjustments can be made to the equipment to accommodate the physiques of the crew. Collectively these adjustments are known as the boat's rigging.
Steering.
Single, double, and quad sculls are usually steered by the scullers pulling harder on one side or the other. In other boats, there is a rudder, controlled by the coxswain, if present, or by one of the crew. In the latter case, the rudder cable is attached to the toe of one of his shoes which can pivot about the ball of the foot, moving the cable left or right. The bowman may steer since he has the best vision when looking over his shoulder. On straighter courses, the strokesman may steer, since he can point the stern of the boat at some landmark at the start of the course. On international courses, landmarks for the steersmen, consisting of two aligned poles, may be provided.
Oars.
Oars are used to propel the boat. They are long (sculling: 250–300 cm; rowing 340–360 cm) poles with one flat end about 50 cm long and 25 cm wide, called the blade. Classic oars were made out of wood, but modern oars are made from more expensive and durable synthetic material, the most common being carbon fiber.
An oar is often referred to as a "blade" in the case of sweep oar rowing and as a "scull" in the case of sculling. A sculling oar is shorter and has a smaller blade area than the equivalent sweep oar. The combined blade area of a pair of sculls is however greater than that of a single sweep oar, so the oarsman when sculling is working against more water than when rowing sweep-oared. He is able to do this because the body action in sculling is more anatomically efficient (due to the symmetry).
The "spoon" of oars is normally painted with the colours of the club to which they belong. This greatly simplifies identification of boats at a distance. As many sports teams have logos printed on their jerseys, rowing clubs have specifically painted blades that each team is associated with.
Indoor rowing.
Indoor rowing (on ergometer, or tank) is a way to train technique and strength by going through the same motions as rowing, with resistance. Indoor rowing is helpful when there are no rowable bodies of water near by, or weather conditions don't permit rowing.
Rowing tank.
A rowing tank is an indoor facility which attempts to mimic the conditions rowers face on open water. Rowing tanks are primarily used for off-season rowing, muscle specific conditioning and technique training, or simply when bad weather doesn't allow for open water training.
Ergometer.
Ergometer rowing machines (colloquially "ergs" or "ergo") simulate the rowing action and provide a means of training on land when waterborne training is restricted, and of measuring rowing fitness. Ergometers do not simulate the lateral balance challenges, the exact resistance of water, or the exact motions of true rowing including the sweep of the oar handles. For that reason ergometer scores are generally not used as the sole selection criterion for crews, and technique training is limited to the basic body position and movements. However, this action can still allow a comparable workout to those experienced on the water.
Sometimes, slides are placed underneath the erg to try to simulate the movement of being on the water. It allows the machine to move back and forth smoothly as if there is water beneath you. The slides can be connected in rows or columns so that rowers are forced to move together on the ergometer, similar to how they would match up their rhythm in a boat. 
Indoor rowing has become popular as a sport in its own right with numerous indoor competitions (and the annual World Championship CRASH-B Sprints in Boston) during the winter off-season.
Damage.
The most commonly damaged piece of rowing equipment is the skeg, which is a metal or plastic fin that comes out of the bottom of the boat to help maintain stability, and to assist in steering. Since the skeg sticks out below the hull of the boat it is the most vulnerable to damage, however it is relatively easy to replace skegs by gluing a new one on. Hull damage is also a significant concern both for maintaining equipment, and for rower safety. Hull damage can be caused by submerged logs, poor strapping to trailers, and collisions with other boats, docks, rocks, etc.
Transportation.
Boats are conveyed to competitions on special trailers accommodating up to 20 boats.
Boat storage, boat houses, and boat centers.
Racing boats are stored in boat houses. These are specially designed storage areas which usually consist of a long two-story building with a large door at one end which leads out to a pontoon or slipway on the river or lakeside. The boats are stored on racks (horizontal bars, usually metal) on the ground floor. Oars, riggers, and other equipment is stored around the boats. Boat houses are typically associated with rowing clubs and include some social facilities on the upper floor: a cafe, bar, or gym.
Boat centers are commonly built along river banks in major U.S. cities. The Thompson Boat Center (TBC), managed by the U.S. National Park Service, is used as a "home base" for high school and adult teams in the Washington, D.C. metropolitan area. Regattas are frequently held at TBC through the spring, summer, and fall.
Competition.
Rowers may take part in the sport for their leisure or they may row competitively. There are different types of competition in the sport of rowing. In the U.S. all types of races are referred to as "regattas" whereas this term is only used in the UK for head-to-head or multi-lane races (such as those that take place at Dorney Lake), which generally take place in the summer season. Time trials occur in the UK during the winter, and are referred to as Head races.
Rowing is unusual in the demands it places on competitors. The standard world championship race distance of 2,000 metres is long enough to have a large endurance element, but short enough (typically 5.5 to 7.5 minutes) to feel like a sprint. This means that rowers have some of the highest power outputs of athletes in any sport. At the same time the motion involved in the sport compresses the rowers' lungs, limiting the amount of oxygen available to them. This requires rowers to tailor their breathing to the stroke, typically inhaling and exhaling twice per stroke, unlike most other sports such as cycling where competitors can breathe freely.
Side by side.
Most races that are held in the spring and summer feature side by side racing, or sprint racing, sometimes called a regatta; all the boats start at the same time from a stationary position and the winner is the boat that crosses the finish line first. The number of boats in a race typically varies between two (which is sometimes referred to as a "dual race") to six, but any number of boats can start together if the course is wide enough.
The standard length races for the Olympics and the World Rowing Championships is 2 km) long, 1.5 km - 2 km for US high school races on the east coast and 1,000 m for "masters" rowers (rowers older than 27). However the race distance can and does vary from "dashes" or sprints, which may be 500 m long, to races of marathon or ultra-marathon length races such as the Tour du Léman in Geneva, Switzerland which is 160 km, and the 2 day, 185 km Corvallis to Portland Regatta held in Oregon, USA. In the UK, regattas are generally between 500 m and 2 km long.
A feature of the end of twentieth century rowing was the development of non-olympic multicrew racing boats, typically fixed seat-gigs, pilot boats and in Finland church- or longboats. The most usual craft in races held around the coasts of Britain during summer months is the Cornish pilot gig, most typically in the south-west, with crews of 6 from local towns and races of varying distances. The Cornish pilot gig was designed and built to ferry harbour and river pilots to and from ships in fierce coastal waters. The boat needed to be stable and fast with the large crew hence making it ideal for its modern racing usage. In Finland 14-oared churchboats race throughout the summer months, usually on lakes, and often with mixed crews. The largest gathering sees over 7000 rowers mainly rowing the 60 km course at Sulkava near the eastern border over a long weekend in mid July. The weekend features the World Masters churchboat event which also includes a 2 km dash.
Two traditional non-standard distance shell races are the annual Boat Race between Oxford and Cambridge and the Harvard-Yale Boat Race which cover courses of approximately 4 mi. The Henley Royal Regatta is also raced upon a non-standard distance at 2,112 meters (1 mile, 550 yards).
In general, multi-boat competitions are organized in a series of rounds, with the fastest boats in each heat qualifying for the next round. The losing boats from each heat may be given a second chance to qualify through a repechage. The World Rowing Championships offers multi-lane racing in heats, finals and repechages. At Henley Royal Regatta two crews compete side by side in each round, in a straightforward knock-out format, with no repechages.
Head races.
Head races are time trial / processional races that take place from autumn (fall) to early spring (depending on local conditions). Boats begin with a rolling start at intervals of 10 – 20 seconds, and are timed over a set distance. Head courses usually vary in length from 2000 m to 12000 m, though there are longer races such as the Boston Rowing Marathon and shorter such as Pairs Head.
The oldest, and arguably most famous, head race is the Head of the River Race, founded by Steve Fairbairn in 1926 which takes place each March on the river Thames in London, United Kingdom. Head racing was exported to the United States in the 1950s, and the Head of the Charles Regatta held each October on the Charles River in Boston, Massachusetts, USA is now the largest rowing event in the world.
These processional races are known as "Head Races", because, as with bumps racing, the fastest crew is awarded the title "Head of the River" (as in "head of the class"). It was not deemed feasible to run bumps racing on the Tideway, so a timed format was adopted and soon caught on.
Time trials are sometimes used to determine who competes in an event where there is a limited number of entries, for example the qualifying races for Henley Royal Regatta, and "rowing on" and "getting on" for the Oxford and Cambridge Bumps races respectively.
Bumps races.
A bumps race is a multi-day race beginning with crews lined up along the river at set intervals. They start simultaneously and all pursue the boat ahead while avoiding being bumped by a boat from behind. If a crew overtakes or makes physical contact with the crew ahead, a "bump" is awarded. As a result, damage to boats and equipment is common during bumps racing. To avoid damage the cox of the crew being bumped may concede the bump before contact is actually made. The next day, the bumping crew will start ahead of any crews that have been bumped. The positions at the end of the last race are used to set the positions on the first day of the races the next year. Oxford and Cambridge Universities hold bumps races for their respective colleges twice a year, and there are also "Town Bumps" races in both cities, open to non-university crews. Oxford's races are organised by City of Oxford Rowing Club and Cambridge's are organised by the Cambridgeshire Rowing Association.
Stake races.
The stake format was often used in early American races. Competitors line up at the start, race to a stake, moored boat, or buoy some distance away, and return. The 180° turn requires mastery of steering. These races are popular with spectators because one may watch both the start and finish. Usually only two boats would race at once to avoid collision. The Green Mountain Head Regatta continues to use the stake format but it is run as a head race with an interval start. A similar type of racing is found in UK and Irish coastal rowing, where a number of boats race out to a given point from the coast and then return fighting rough water all the way. In Irish coastal rowing the boats are in individual lanes with the races consisting of up to 3 turns to make the race distance 2.3 km.
World Championships and Olympics.
The Olympic Games are held every four years, where only select boat classes are raced (14 in total):
At the end of each year, the FISA holds the World Rowing Championships with events in 22 different boat classes. Athletes generally consider the Olympic classes to be premier events . During Olympic years only non-Olympic boats compete at the World Championships.
Rules of racing.
There are many differing sets of rules governing racing, and these are generally defined by the governing body of the sport in a particular country—e.g., British Rowing in England and Wales, Rowing Australia in Australia, and USRowing in the United States. In international competitions, the rules are set out by the world governing body, the Fédération Internationale des Sociétés d'Aviron (FISA). The rules are mostly similar but do vary; for example, British Rowing requires coxswains to wear buoyancy aids at all times, whereas FISA rules do not.
Rowing crew.
Boat positions.
In all boats, with the exception of single sculls, each rower is numbered in sequential order, low numbers at the bow, up to the highest at the stern. The person seated on the first seat is called the bowman, or just 'bow', whilst the rower closest to the stern is called the 'strokeman' or just 'stroke'. There are some exceptions to this – some UK coastal rowers, and in France, Spain, and Italy rowers number from stern to bow.
In addition to this, certain crew members have other titles and roles. In an 8+ the stern pair are responsible for setting the stroke rate and rhythm for the rest of the boat to follow. The middle four (sometimes called the "engine room" or "power house") are usually the less technical, but more powerful rowers in the crew, whilst the bow pair are the more technical and generally regarded as the pair to set up the balance of the boat. They also have most influence on the line the boat steers.
Coxswain.
The coxswain (or simply the cox) is the member who sits in the boat facing the bow, steers the boat, and coordinates the power and rhythm of the rowers - by communicating to the crew through a device called a cox box and speakers. They usually sit in the stern of the boat, except in bowloaders where the coxswain lies in the bow. Bowloader are usually seen as the coxed four and coxed pair type of boat.
It is an advantage for the coxswain to be light, as this requires less effort for the crew to propel the boat. In many competitive events there is a minimum weight set for the coxswain to prevent unfair advantage. 
If a coxswain is under the minimum weight allowance (underweight) they may have to carry weights in the boat such as sandbags.
Weight classes.
In most levels of rowing there are different weight classes – typically "open" (or referred to as "heavyweight") and lightweight. Competitive rowing favours tall, muscular athletes due to the additional leverage height provides in pulling the oar through the water as well as the explosive power needed to propel the boat at high speed.
Heavyweight.
Heavyweight rowers of both sexes tend to be very tall, broad-shouldered, have long arms and legs as well as tremendous cardiovascular capacity and very low body fat ratios. Olympic or International level heavyweight male oarsmen are typically anywhere between 190 cm and 206 cm (6'3" to 6'9") tall with most being around 198 cm (6'6") and weighing approximately 102 kg (225 lb) with about 6 to 7% body fat.
Heavyweight women are slightly shorter at around 186 cm (6'1") and lighter than their male counterparts.
Some rowing enthusiasts claim that the disproportionate number of tall rowers is simply due to the unfair advantage that tall rowers have on the ergometer. This is due to the ergometer's inability to properly simulate the larger rowers drag on a boat due to weight. Since the ergometer is used to assess potential rowers, results on the ergometer machine play a large role in a rower's career success. Thus, many erg scores are weight-adjusted, as heavyweights typically find it easier to get better erg scores. Also, since crew selection has favored tall rowers long before the advent of the ergometer, and bigger, taller crews are almost universally faster than smaller, shorter crews on the water, being tall is a definite advantage ultimately having little to do with the ergometer.
Lightweight.
Unlike most other non-combat sports, rowing has a special weight category called "lightweight" (Lwt for short). According to FISA, this weight category was introduced "to encourage more universality in the sport especially among nations with less statuesque people". The first lightweight events were held at the World Championships in 1974 for men and 1985 for women. Lightweight rowing was added to the Olympics in 1996.
At international level the limits are:
The Olympic lightweight boat classes are limited to; Men's double (LM2x), Men's four (LM4-), Women's double (LW2x).
At the junior level (in the United States), regattas require each rower to weigh in at least two hours before their race; they are sometimes given two chances to make weight at smaller regattas, with the exception of older more prestigious regattas, which allow only one opportunity to make weight. For juniors in the United States, the lightweight cutoff for men is 150.0 lb.; for women, it is 130.0 lb. In the fall the weight limits are increased for women, with the cutoff being 135 lb.
At the collegiate level (in the United States), the lightweight weight requirements can be different depending on competitive season. For fall regattas (typically head races), the lightweight cutoff for men is 165.0 lb. and 135.0 lb. for women. In the spring season (typically sprint races), the lightweight cutoff for men is 160.0 lb., with a boat average of 155.0 lb. for the crew; for women, the lightweight cutoff is 130.0 lb.
Women.
Women row in all boat classes, from single scull to coxed eights, across the same age ranges and standards as men, from junior amateur through university-level to elite athlete. Typically men and women compete in separate crews although mixed crews and mixed team events also take place. Coaching for women is similar to that for men. The introduction of women's rowing at the 1976 Summer Olympics in Montreal increased the growth of women's rowing because it created the incentive for national rowing federations to support women's events. Rowing at the 2012 Summer Olympics in London included six events for women compared with eight for men.
At the international level, women's rowing traditionally has been dominated by Eastern European countries, such as Romania, Russia, and Bulgaria, although other countries such as Germany, Canada, the Netherlands, Great Britain and New Zealand often field competitive teams. The United States also has had very competitive crews, and in recent years these crews have become even more competitive given the surge in women's collegiate rowing.
Adaptive athletes.
Adaptive rowing is a special category of races for those with physical disabilities. Under FISA rules there are 5 boat classes for adaptive rowers; mixed (2 men and 2 women plus cox) LTA (Legs, Trunk, Arms), mixed intellectual disability (2 men and 2 women plus cox) LTA (Legs, Trunk, Arms), mixed (1 man and 1 woman) TA (Trunk and Arms), and men's and women's AS (Arms and Shoulders). Events are held at the World Rowing Championships and were also held at the 2008 Summer Paralympics.
Terminology and event nomenclature.
Rowing events use a systematic nomenclature for the naming of events, so that age, gender, ability and size of boat can all be expressed in a few numbers and letters. The first letter to be used is 'L' or 'Lt' for lightweight. If absent then the crew is open weight. This can be followed by either a 'J' or 'B' to signify junior (under 19 years) or under 23 years respectively. If absent the crew is open age (the letter 'O' is sometimes used). Next is either an 'M' or 'W' to signify if the crew are men or women. Then there is a number to show how many athletes are in the boat (1,2,4 or 8). An 'x' following the number indicates a sculling boat. Finally either a + or – is added to indicate whether the boat is coxed or coxswainless.
Some events will use an experience rating to separate races. In the UK boats are classed as "Elite", "Senior", "Intermediate 1/2/3" or "Novice", depending on the number of wins the athletes have accumulated. Masters events use age ranges to separate crews of older rowers.
Examples:

</doc>
<doc id="25737" url="http://en.wikipedia.org/wiki?curid=25737" title="RuneQuest">
RuneQuest

RuneQuest is a fantasy role-playing game first published in 1978 by Chaosium, created by Steve Perrin and set in Greg Stafford's mythical world of Glorantha. RuneQuest was notable for its original gaming system (designed around a percentile die and with an early implementation of skill rules). There have been several incarnations of the game. The most recent version was released in July 2012 by The Design Mechanism under the title "RuneQuest 6".
In Britain in the 1980s, "RuneQuest" was recognised by the gaming world as one of the 'Big Three' games with the largest market share, the others being "Dungeons & Dragons" and "Traveller.
Setting.
With the exception of the Third and Sixth (current) Editions, the default setting for "RuneQuest" has been the world of Glorantha. However, supplements published by Mongoose showcase other settings. (Young Kingdoms, Sláine, a pirates setting, a clockpunk version of the English Civil War, etc.)
The well-developed background of the game offered a breadth of material for players and gamemasters to draw from. At a time when many RPG settings were cobbled together, "RuneQuest" offered players a vibrant living world, giving them a much more developed fictional world with established geography, history, and religion.
The Dragon Pass Area.
The original rules contained a map of an area called Dragon Pass, a region offered as the default setting for adventures. The original "RuneQuest" game was set during a period of invasion, offering plenty of opportunities for game scenarios. A supplement titled "Cults of Prax" added more detail to many of the setting's locations.
Cults and Religion.
A key element of "RuneQuest" flavor is a character's affiliation with a cult. Characters begin as lay members and progress through a series of membership levels, such as initiate or Rune Lord. This system offers narrative and mechanical benefits to players who chose to have their characters join a cult.
The basic rules described a handful of original and mythological gods. These were greatly expanded upon in the supplements "Cults of Prax" and "Cults of Terror".
Magic in "RuneQuest".
Characters in "RuneQuest" are not divided into magic using and non-magic using characters. At the time of the game's release, this was an unorthodox mechanic. Although all characters have access to magic, for practical gameplay purposes a character's magical strength is proportional to his or her connection to the divine or natural skill at sorcery.
The exact divisions of magic vary from edition to edition, but most contain divisions such as Battle Magic, Sorcery, Petty Magicks, Divine Magic, Spirit Magic, and Enchantments.
System.
RuneQuest's system has been praised as a realistic, robust simulation.
In many ways, the system was developed as a response to more scalar systems, such as Dungeons & Dragons' level-based system. Through the removal of levelling, and the adherence to skill improvement, "RuneQuest" avoided many of the perceived flaws of such systems.
The game is presided over by a moderator or gamemaster, whose job is to interpret player decisions and their result on the shared game world. The gamemaster is also responsible for describing the setting and non-player characters. The gamemaster's role is fundamentally different from that of the other participants.
Character Creation.
As with most RPGs, players begin by making a Player Character. This character serves as the player's avatar in the shared fictional game world, and is the agent through which the player makes gameplay and narrative decisions. Player characters are devised through a number of dice rolls to represent physical, mental and spiritual characteristics.
Characters in "RuneQuest" gain power as they are used in play, but not to the degree that characters do in other fantasy RPGs. It is still possible for a weak character to slay a strong one through luck, tactics, or careful planning.
Combat.
The game's combat system was designed in an attempt to recreate designer Steve Perrin's experience with live-action combat. Perrin experienced mock medieval combat through the Society for Creative Anachronism. In the RuneQuest system, an attack is rolled using percentile dice. If the number rolled is equal to or less than the character's skill level, they have hit their target. The defender has the chance to roll to avoid the blow or parry it. The game features mechanics for critical hits and fumbling.
A key component of the "RuneQuest" combat system is a subsystem for "hit location". Successful attacks are allocated randomly (or by decision) to a part of the target's body. In RuneQuest, a lucky hit against a character's leg, weapon arm, or head could have specific effects on the game's mechanics and narrative. This was a unique part of the game's combat system and helped to separate it from the more abstracted, hit-point-based combat of competitors such as Dungeons & Dragons.
Combat in "RuneQuest" is more detailed, slower and often riskier than in competing RPGs. When combat takes place it is tactical, and outcomes depend on strategic advantages from terrain, position, numerical superiority, or clever thinking.
Non-combat.
Non-combat activities are also determined via percentile roll. As an example, if a character has climbing at 35% and her player rolls 25 on a D100, the character has succeeded. However, a nuanced range of results existed in every die roll. If a die roll was 1/5 of the necessary percentile roll or less, it was a special success, and if it was 1/20 of the necessary roll or less it was a critical success. Very high rolls (in the range 96-00) on the other hand, could be "fumbles" or spectacular failures if they were in the top 1/20 of possible failed rolls.
Rules for skill advancement rely on percentile dice and were a key feature of the system: to improve a character's abilities, her player needs to roll higher than the character's skill rating. For the climber example used earlier, the player would need to roll greater than 35 on a D100 in order to advance the character's skill. Thus, the better the character is at a skill the more difficult it is to improve.
Other rules.
The "RuneQuest" rule book contained a large selection of fantasy monsters and their physical stats. As well as traditional fantasy staples (Dwarves, Trolls, Undead, Lycanthropes, etc.), the book featured original creatures such as the goat-headed creatures called Broo. Some of its traditional fantasy creatures differed notably from the versions from other games (or fantasy or traditional sources), for example, Elves are humanoid plant life. Unlike other fantasy RPGs of the time, "RuneQuest" encouraged the use of monsters as player characters.
History.
In 1975, games designer Greg Stafford released the fantasy board game White Bear and Red Moon (later "Dragon Pass"), produced and marketed by Chaosium, a game publishing company set up by Stafford solely for the release of the game. The board game introduced the region of Dragon Pass and many of the creatures and personalities that would appear in the world of the "RuneQuest" games. In 1978 Chaosium published the first edition of "RuneQuest", a role playing game set in the world of Glorantha (first explored in White Bear and Red Moon). "RuneQuest" quickly established itself as the second most popular fantasy role-playing game, after "Dungeons & Dragons". The first and second editions are set in the mythical world of Glorantha, while the third edition in the mid 1980s is more generic and was much less successful. "RuneQuest" is the original percentile die-based and skill-based rule set.
The game was sold to Avalon Hill under a complex agreement that required all Glorantha-related content first be approved by Chaosium. In an attempt to also have a setting they could release freely, Avalon Hill also supported a new "default" setting, Fantasy Earth, based on fantasy interpretations of several eras of earth's pre-modern history. Later Avalon Hill published "generic"/"Gateway" fantasy material ("Lost City of Eldarad, Daughters of Darkness"). Critics consider these later "generic"/"Gateway" publications inferior to the earlier "RuneQuest" publications.
Although both supplements for Fantasy Earth ("Vikings, Land of Ninja") were well regarded, the popularity of "RuneQuest" as a system seems to have come from the strength of its original setting, reflected in the remarkably high sales of materials that were new editions of out-of-print Glorantha content. A proposed fourth edition was originally meant to return the tight "RuneQuest"/Glorantha relationship, but it was shelved in 1994, mid-project.
Glorantha is the official setting of a new rules system called "HeroQuest", which is the successor to "Hero Wars". One reason for the new Glorantha-based game was that Avalon Hill retained rights to the ""RuneQuest" name but not to the game's rules. A new game called "" entered development in 1997, but it was shelved when Avalon Hill was bought by toymaker Hasbro. At some stage in 2003 the rights to the trademarked name "RuneQuest"" were acquired by Issaries, Inc.
Mongoose Publishing released a new version of "RuneQuest" in August 2006, under a license from Issaries, Inc., and "developed under the watchful eyes of Messrs Stafford and Perrin". However, Steve Perrin was no longer associated with the Mongoose "RuneQuest" project as of December 2005. The new rules were released under a variant of the Open Game License, and the official setting takes place during the Second Age (previous editions covered the Third Age). In 2010, Mongoose published a much-revised version called "RuneQuest II", this time with no OGL system reference document (SRD) for third-party publishers.
In May 2011, Mongoose Publishing announced that they had parted company with Issaries, Inc., and that the RuneQuest II rules system that they had devised would live on under a "Wayfarer" banner, but without the Gloranthan content. A month later Mongoose announced a further name change to "Legend", so as not to conflict with the already existing "Wayfarers RPG".
In July, 2011, The Design Mechanism announced that they had entered a partnership with Issaries, Inc. and would be producing a 6th edition of RuneQuest. RuneQuest 6 was released in July 2012. RuneQuest 6 rules are broadly similar to the Mongoose Publishing "RuneQuest II" rules. The RuneQuest 6 rules aim at providing rules that can be adapted to many fantasy or historical settings, and do not contain any specifically Gloranthan content (though they do use the Gloranthan runes). The Design Mechanism intend to support multiple settings, including original, licences, and historic settings, through separate supplements. Glorantha is one of those settings. The Design Mechanism also has the rights to several supplements originally produced for Mongoose Publishing editions of RuneQuest, including both Gloranthan and generic supplements, and some of them they have made commercially available as PDF documents (though they have withdrawn some of them from sale).
Reception.
Jennell Jaquays comments: "After "RuneQuest" and Glorantha, detailed fantasy worlds would become the norm, not the exception. Dragon Pass paved the way for TSR's Faerûn, better known as the Forgotten Realms, and Krynn, setting for the Dragonlance saga. But few would ever achieve the elegant but approachable rules complexity of the original "RuneQuest" or instill a fervent loyalty in fans that would span decades."
Legacy.
Chaosium reused the rules system developed in "RuneQuest" to form the basis of several other games: in 1980 the "RuneQuest" system of rules was simplified and published by Greg Stafford and Lynn Willis under the name of "Basic Role-Playing" (or "BRP", for short). "BRP" was a generic role-playing game system, derived from the two first "RuneQuest" editions (1978 and 1979). It was used for many Chaosium role-playing games that followed "RuneQuest", including:
The science-fiction roleplaying game "Other Suns" by Fantasy Games Unlimited, 1983, licensed the Basic Role Playing system as well.
Minor modifications of the "BRP" rules were introduced in every one of those games, to suit the flavor of each game's universe. "Pendragon" used a 1-20 scale and 1d20 roll instead of a percentile scale and 1d100. In combat, it used a single STR-based damage value where weapons only gave bonuses or penalties to the number of d6s. "" (1989), which used coin tosses instead of dice rolls, was the only Chaosium role-playing game that didn't use any variant of the "BRP" system.
In 2004, Chaosium released a print-on-demand version of the 3rd edition "RuneQuest" rules under the titles "Basic Roleplaying Players Book", "Basic Roleplaying Magic Book", and "Basic Roleplaying Creatures Book". The same year, Chaosium began preparing the most complete version yet of "Basic Role-Playing". This new "BRP" edition was provisionally named "Deluxe Basic Role-Playing" ("DBRP") but was finally released on June 24, 2008 as a single comprehensive book with the title "Basic Role-Playing". The book offers many optional rules, as well as genre-specific advice for fantasy, horror, and science-fiction. Currently Chaosium is selling both a printed and pdf version of the game. No current version of "BRP" includes any Gloranthan content.
Steve Perrin, one of two authors of the original "RuneQuest" game, later developed a similar system known as "Steve Perrin's Quest Rules" ("SPQR"), which some "RuneQuest" fans consider to be a successor to the original game.
Ray Turney, one of two authors of the original "RuneQuest" game, later developed a similar system known as "Fire and Sword", which some "RuneQuest" fans consider to be a successor to the original game.
Since losing the license to use the "RuneQuest" name and Glorantha setting, Mongoose Publishing have announced that they will release a new series of books under the title of "Legend", which are designed to be 100% compatible with the "RuneQuest II" ruleset. Legend will be released in late 2011 under an open license so that others will be able to release books based on those rules. As well as a new series of titles to be released for "Legend", current Mongoose titles for "RuneQuest II", such as the "Vikings" sourcebook, will be re-released as Legend-compatible books.

</doc>
<doc id="25739" url="http://en.wikipedia.org/wiki?curid=25739" title="Rich Text Format">
Rich Text Format

The Rich Text Format (often abbreviated RTF) is a proprietary document file format with published specification developed by Microsoft Corporation from 1987 until 2008 for cross-platform document interchange with Microsoft products.
Most word processors are able to read and write some versions of RTF. There are several different revisions of RTF specification and portability of files will depend on what version of RTF is being used. RTF specifications were changed and published with major Microsoft Word and Office versions.
It should not be confused with enriched text (mimetype "text/enriched" of RFC 1896) or its predecessor Rich Text (mimetype "text/richtext" of RFC 1341 and ); nor with IBM's RFT-DCA (Revisable Format Text-Document Content Architecture) which are completely different specifications.
History.
Richard Brodie, Charles Simonyi, and David Luebbert, members of the Microsoft Word development team, developed the original RTF in the middle to late 1980s. Its syntax was influenced by the TeX typesetting language. The first RTF reader and writer shipped in 1987 as part of Microsoft Word 3.0 for Macintosh, which implemented the RTF version 1.0 specification. All subsequent releases of Microsoft Word for the Macintosh and all versions for Windows can read and write files in RTF format.
Microsoft maintains the format. The final version was 1.9.1 in 2008, implementing features of Office 2007. Microsoft has discontinued enhancements to the RTF specification. New features in Word 2010 and later versions will not save properly to the RTF format. Microsoft anticipates no further updates to RTF, but has stated willingness to consider editorial and other non-substantive modifications of the RTF Specification during an associated ISO/IEC 29500 balloting period.
Code example.
As an example, the following RTF code:
 {\rtf1\ansi{\fonttbl\f0\fswiss Helvetica;}\f0\pard
 This is some {\b bold} text.\par
is a document which would be rendered like this when read by a program that supports RTF:
This is some bold text.
Character encoding.
A standard RTF file can consist of only 7-bit ASCII characters, but can encode characters beyond ASCII by escape sequences. The character escapes are of two types: code page escapes and, starting with RTF 1.5, Unicode escapes. In a code page escape, two hexadecimal digits following a backslash and typewriter apostrophe are used for denoting a character taken from a Windows code page. For example, if the code page is set to Windows-1256, the sequence codice_7 will encode the Arabic letter bāʼ (ب).
For a Unicode escape the control word codice_8 is used, followed by a 16-bit signed decimal integer giving the Unicode UTF-16 code unit number. For the benefit of programs without Unicode support, this must be followed by the nearest representation of this character in the specified code page. For example, codice_9 would give the Arabic letter "bāʼ" ب, specifying that older programs which do not have Unicode support should render it as a question mark instead.
The control word codice_10 can be used to indicate that subsequent Unicode escape sequences within the current group do not specify the substitution character.
Until RTF specification version 1.5 release in 1997, RTF has only handled 7-bit characters directly and 8-bit characters encoded as hexadecimal (using codice_11). RTF control words (since RTF 1.5) generally accept signed 16-bit numbers as arguments. Unicode values greater than 32767 must be expressed as negative numbers. If a Unicode character is outside BMP, it is encoded with a surrogate pair. Support for Unicode was made due to text handling changes in Microsoft Word – Microsoft Word 97 is a partially Unicode-enabled application and it handles text using the 16-bit Unicode character encoding scheme. Microsoft Word 2000 and later versions are Unicode-enabled applications that handle text using the 16-bit Unicode character encoding scheme.
RTF files are usually 7-bit ASCII plain text. RTF consists of control words, control symbols, and groups. RTF files can be easily transmitted between PC based operating systems because they are encoded as a text file with 7-bit graphic ASCII characters. Converters that communicate with Microsoft Word for MS Windows or Macintosh should expect data transfer as 8-bit characters and binary data can contain any 8-bit values.
Human readability.
Unlike many word processing formats, RTF code can be human-readable: when an RTF file is viewed as a plain text file, the contained ASCII text is legible. The formatting code is not too distracting nor counter-intuitive, provided that the document's creator kept formatting concise. At the time of RTF's initial release, this was rare among document formats. Today, XML-based formats, which are human-readable, are more common. In contrast, MS Word's codice_12 format is binary, with only a few scraps of legible text.
Despite the format's legibility, RTF files are often hard to read. The files produced by most programs, such as Microsoft Word, contain large amounts of formatting code. Such files are easily an order of magnitude larger than the corresponding plain text. They are not legible. Also, non-ASCII characters must be escaped in standard-compliant RTF. Thus, even with concise formatting, text that uses certain dashes and quotation marks is less legible. Latin languages that make heavy use of characters with diacritics, such as \'f1 for ñ and \'e9 for é are difficult to read in RTF. Non-Latin scripts, consisting of characters such as \u21563 for 吻, are
illegible in RTF. Finally, from its beginnings, RTF has supported Microsoft OLE embedded objects and Macintosh Edition Manager subscriber objects, which are not human-readable.
RTF is a data format for saving and sharing documents. It is not intended for intuitive and easy typing by hand. Thus, it is not really a markup language.
Common uses and interoperability.
Most word processing software supports RTF format importing and exporting (following some version of RTF specification), and/or direct editing, often making it a "common" format between otherwise incompatible word processing software and operating systems. These factors contribute to its interoperability, but it will depend on what version of RTF is being used. There are several consciously designed or accidentally born RTF dialects. Most applications which read RTF files silently ignore unknown RTF control words.
RTF is the internal markup language used by Microsoft Word. Overall, since 1987, RTF files may be transferred back and forth between many old and new computer systems (and now over the Internet) despite differences between operating systems and their versions. (But there are some compatibility problems, e.g. between RTF 1.0 1987 and later specifications, or between RTF 1.0-1.4 and RTF 1.5+ in use of Unicode characters.) This makes it a useful format for basic formatted text documents such as instruction manuals, résumés, letters, and modest information documents. These documents at minimum support bold, italic, and underline text formatting. Also typically supported are left-, center-, and right-aligned text. Also, font specification and document margins are supported in RTF documents.
Font and margin defaults, as well as style presets and other functions will vary according to program defaults. There may also be subtle differences perhaps between different versions of the RTF specification implemented in differing programs and program versions. Nevertheless, the RTF format is consistent enough from computer to computer to be considered highly portable and acceptable for cross-platform use. The format supports metadata such as title, author, etc. but not all implementations support this.
Objects.
Use of Microsoft Object Linking and Embedding (OLE) objects or Macintosh Edition Manager subscriber objects limits the interoperability, because these objects are not widely supported in programs for viewing or editing RTF files (e.g. embedding of other files inside the RTF, such as tables or charts from spreadsheet application). If a software that understands an OLE object is not available, the object is usually replaced by a picture (bitmap representation of the object) or not displayed at all.
Pictures.
RTF supports inclusion of JPEG, Portable Network Graphics (PNG), Enhanced Metafile (EMF), Windows Metafile (WMF), Apple PICT, Windows Device-dependent bitmap, Windows Device Independent bitmap and OS/2 Metafile picture types in hexadecimal (the default) or binary format in a RTF file. Not all of these picture types are supported in all RTF readers. When a RTF document is opened in software that does not support the picture type of an inserted picture, such picture is not displayed at all.
RTF writers usually convert inserted pictures from an unsupported picture types (e.g. BMP, TIFF, GIF, etc.) to one of supported picture types (PNG, WMF) or they do not include pictures at all.
For better compatibility with Microsoft products, some RTF writers include the same picture in two different picture types in one RTF file:
This method increases the RTF file size rapidly. The RTF specification does not require this method and there are various implementations that include pictures without the WMF copy (e.g. Abiword or Ted).
For Microsoft Word it is also possible to set a specific registry value ("ExportPictureWithMetafile=0") in order to prevent Word from saving the WMF copy (see link "Document file size increases with EMF, PNG, GIF, or JPEG graphics in Word" at the beginning).
Fonts.
RTF supports embedding of fonts used in the document, but this feature is not widely supported in software implementations.
RTF also supports generic font family names used for font substitution: "roman" (serif), "swiss" (sans-serif), "modern" (monospace), "script", "decorative", "technical". This feature is not widely supported for font substitution, e.g. in OpenOffice.org or Abiword.
Annotations.
RTF specification supports annotations (comments in documents) since version 1.0. RTF 1.7 specification defined some new features for annotations: date stamp (there was previously only "time stamp") and parents of annotations. When a RTF document with annotations is opened in an application that does not support RTF annotations, they are not displayed at all. Similarly, when a document with annotations is saved as RTF in an application that does not support RTF annotations, annotations are not preserved in the RTF file. Some implementations may hide annotations by default or require some user action to display them - e.g. in Abiword since version 2.8 or in IBM Lotus Symphony (up to version 1.3).
Microsoft products do not support comments within footers, footnotes or headers. Inserting a comment within headers, footers, or footnotes may result in a corrupted RTF document.
RTF specification also supports footnotes (not to be confused with annotations), which are widely supported in RTF implementations (e.g. in OpenOffice.org, Abiword, KWord, Ted, but not in Wordpad).
Drawing objects.
RTF 1.2 specification defined use of drawing objects such as rectangles, ellipses, lines, arrows, polygons and various other shapes. RTF 1.5 specification introduced many new control words for drawing objects. RTF drawing objects are also called "shapes" since RTF 1.5.
However, RTF drawing objects are not supported in many RTF implementations, such as Apache OpenOffice (though they are supported in LibreOffice 4.0 on) or Abiword. When an RTF document with drawing objects is opened in an application that does not support RTF drawing objects, they are not displayed at all. Some implementations will also not display any text inside drawing objects. Similarly, when a document with drawing objects is saved as RTF in an application that does not support RTF drawing objects, these are not preserved in the RTF file.
Security concerns.
Unlike Microsoft Word's DOC format, as well as the newer Office Open XML and OpenDocument formats, RTF does not support macros. For this reason, RTF is recommended over these formats when the spread of computer viruses is a concern. However, having the .RTF extension does not guarantee that a file is safe, since Microsoft Word will open standard DOC files renamed with an RTF extension and run any contained macros as usual. Manual examination of a file in a plain text editor such as Notepad, or use of the codice_13 command in UNIX-like systems, is required to determine whether or not a suspect file is really RTF.
Implementations.
Each RTF implementation usually implements only some versions or subsets of the RTF specification. Many of the available RTF converters cannot understand all new features in the latest RTF specifications.
The WordPad editor in Microsoft Windows creates RTF files by default. It once defaulted to the Microsoft Word 6.0 file format, but write support for Word documents (.doc) was dropped in a security update. Read support was also dropped in Windows 7. WordPad does not support some RTF features, such as headers and footers. However, WordPad can read and save many RTF features that it cannot create such as: tables, strikeout, superscript, subscript, "extra" colors, text background colors, numbered lists, right or left indent, quasi-hypertext and URL linking, and various line spacings. RTF is also the data format for "rich text controls" in MS Windows APIs.
The default text editor for Mac OS X, TextEdit, can also view, edit and save RTF files as well as RTFD files. TextEdit currently (as of July 2009) has limited ability to edit RTF document margins. Much older Mac word processing application programs such as MacWrite and WriteNow were able to view, edit, and save RTF files as well.
The free and open-source word processors AbiWord, Apache OpenOffice, Bean, Calligra, KWord, LibreOffice and NeoOffice can view, edit and save RTF files. RTF format is also used in the Ted word processor.
Scrivener uses individual RTF files for all the text files that make up a given "project".
SIL International’s freeware application for developing and publishing dictionaries uses RTF as its most common form of document output. RTF files produced by Toolbox are designed to be used in Microsoft Word, but can also be used by other RTF-aware word processors.
RTF can be used on some ebook readers because of its interoperability, simplicity, and low CPU processing requirements.
Libraries and converters.
The open-source script rtf2xml can partially convert RTF to XML.
GNU is an open-source program to convert RTF into HTML, LaTeX, troff macros and other formats. is a Python library to create and convert documents in RTF, XHTML and PDF format. is a project to create Rich Text content via Ruby. is a library of Tcl routines, free software, to generate RTF output, and a Cost script to convert SGML to RTF. is a Perl module for generating RTF documents. is an API enabling developers to create RTF documents with PHP. Pandoc is an open source document converter with multiple output formats, including RTF. is a project to create RTF documents via pure PHP.
The Mac OS X command line tool textutil enables you to convert files between rtf, rtfd, text, doc, docx, wordml, odt, and webarchive.
Criticism.
The Rich Text Format was the standard file format for text-based documents in applications developed for Microsoft Windows. Microsoft did not initially make the RTF specification publicly available, making it difficult for competitors to develop document conversion features in their applications. Because Microsoft's developers had access to the specification, Microsoft's applications had better compatibility with the format. Also, every time Microsoft changed the RTF specification, Microsoft's own applications had a lead in time-to-market, because competitors had to redevelop their applications after studying the newer version of the format.
Novell alleged that Microsoft's practices were anticompetitive in its antitrust complaint against Microsoft. The RTF specifications lack some of the semantic definitions necessary to read, write and modify documents.

</doc>
<doc id="25740" url="http://en.wikipedia.org/wiki?curid=25740" title="Robert E. Lee">
Robert E. Lee

Robert Edward Lee (January 19, 1807 – October 12, 1870) was an American soldier best known for commanding the Confederate Army of Northern Virginia in the American Civil War from 1862 until his surrender in 1865. The son of Revolutionary War officer Henry "Light Horse Harry" Lee III and a top graduate of the United States Military Academy, Robert E. Lee was an exceptional officer and combat engineer in the United States Army for 32 years. During this time, he served throughout the United States, distinguished himself during the Mexican–American War, served as Superintendent of the United States Military Academy, and married Mary Custis.
When Virginia declared its secession from the Union in April 1861, Lee chose to follow his home state, despite his personal desire for the country to remain intact and despite an offer of a senior Union command. During the first year of the Civil War, Lee served as a senior military adviser to President Jefferson Davis. Once he took command of the main field army in 1862 he soon emerged as a shrewd tactician and battlefield commander, winning most of his battles, all against far superior Union armies. Lee's strategic foresight was more questionable, and both of his major offensives into the North ended in defeat. Lee's aggressive tactics, which resulted in high casualties at a time when the Confederacy had a shortage of manpower, have come under criticism in recent years. Union General Ulysses S. Grant's campaigns bore down on the Confederacy in 1864 and 1865, and despite inflicting heavy casualties, Lee was unable to turn the war's tide. He surrendered to Grant at Appomattox Court House on April 9, 1865. By this time, Lee had assumed supreme command of the remaining Southern armies; other Confederate forces swiftly capitulated after his surrender. Lee rejected the proposal of a sustained insurgency against the North and called for reconciliation between the two sides.
After the war, as President of what is now Washington and Lee University, Lee supported President Andrew Johnson's program of Reconstruction and intersectional friendship, while opposing the Radical Republican proposals to give freed slaves the vote and take the vote away from ex-Confederates. He urged them to rethink their position between the North and the South, and the reintegration of former Confederates into the nation's political life. Lee became the great Southern hero of the War, a postwar icon of the "Lost Cause of the Confederacy" to some. But his popularity grew even in the North, especially after his death in 1870.
Early life and career.
Lee was born at Stratford Hall Plantation in Westmoreland County, Virginia, the son of Major General Henry Lee III (Light Horse Harry) (1756–1818), Governor of Virginia, and his second wife, Anne Hill Carter (1773–1829). His birth date has traditionally been recorded as January 19, 1807, but according to the historian Elizabeth Brown Pryor, "Lee's writings indicate he may have been born the previous year."
One of Lee's great grandparents, Henry Lee I, was a prominent Virginian colonist of English descent. Lee's family is one of Virginia's first families, originally arriving in Virginia from England in the early 1600s with the arrival of Richard Lee I, Esq., "the Immigrant" (1618–64). His mother grew up at Shirley Plantation, one of the most elegant homes in Virginia. Lee's father, a tobacco planter, suffered severe financial reverses from failed investments.
Little is known of Lee as a child; he rarely spoke of his boyhood as an adult. Nothing is known of his relationship with his father who, after leaving his family, mentioned Robert only once in a letter. When given the opportunity to visit his father's Georgia grave, he remained there only briefly yet, while as president of Washington College, he defended his father in a biographical sketch while editing Light Horse Harry's memoirs. In 1809, Harry Lee was put in debtors prison; soon after his release the following year, Harry and Anne Lee and their five children moved to a small house on Cameron Street in Alexandria, Virginia, both because there were then high quality local schools there, and because several members of her extended family lived nearby. In 1811, the family, including the newly born sixth child, Mildred, moved to a house on Oronoco Street, still close to the center of town and with the houses of a number of Lee relatives close by. In 1812, Harry Lee was badly injured in a political riot in Baltimore and traveled to the West Indies. He would never return, dying when his son Robert was eleven years old. Left to raise six children alone in straitened circumstances, Anne Lee and her family often paid extended visits to relatives and family friends. Robert Lee attended school at Eastern View, a school for young gentlemen, in Fauquier County, and then at the Alexandria Academy, free for local boys, where he showed an aptitude for mathematics. Although brought up to be a practicing Christian, he was not confirmed in the Episcopal Church until age 46.
Anne Lee's family was often supported by a relative, William Henry Fitzhugh, who owned the Oronoco Street house and allowed the Lees to stay at his home in Fairfax County, Ravensworth. When Robert was 17 in 1824, Fitzhugh wrote to the Secretary of War, John C. Calhoun, urging that Robert be given an appointment to the United States Military Academy at West Point. Fitzhugh wrote little of Robert's academic prowess, dwelling much on the prominence of his family, and erroneously stated the boy was 18. Instead of mailing the letter, Fitzhugh had young Robert deliver it. In March 1824, Robert Lee received his appointment to West Point, but due to the large number of cadets admitted, Lee would have to wait a year to begin his studies there.
Lee entered West Point in the summer of 1825. At the time, the focus of the curriculum was engineering; the head of the Army Corps of Engineers supervised the school and the superintendent was an engineering officer. Cadets were not permitted leave until they had finished two years of study, and were rarely allowed off the Academy grounds. Lee graduated second in his class behind Charles Mason, who resigned from the Army a year after graduation, and Lee did not incur any demerits during his four-year course of study, a distinction shared by 5 of his 45 classmates. In June 1829, Lee was commissioned a brevet second lieutenant in the Corps of Engineers. After graduation, while awaiting assignment, he returned to Virginia to find his mother on her deathbed; she died at Ravensworth on July 26, 1829.
Military engineer career.
On August 11, 1829, Brigadier General Charles Gratiot ordered Lee to Cockspur Island, Georgia. The plan was to build a fort on the marshy island which would command the outlet of the Savannah River. Lee was involved in the early stages of construction as the island was being drained and built up. In 1831, it became apparent that the existing plan to build what became known as Fort Pulaski would have to be revamped, and Lee was transferred to Fort Monroe at the tip of the Virginia Peninsula (today in Hampton, Virginia).
While home in the summer of 1829, Lee had apparently courted Mary Custis whom he had known as a child. Lee obtained permission to write to her before leaving for Georgia, though Mary Custis warned Lee to be "discreet" in his writing, as her mother read her letters, especially from men. Custis refused Lee the first time he asked to marry her; her father did not believe the son of the disgraced Light Horse Harry Lee was a suitable man for his daughter. She accepted him with her father's consent in September 1830, while he was on summer leave, and the two were wed on June 30, 1831.
Lee's duties at Fort Monroe were varied, typical for a junior officer, and ranged from budgeting to designing buildings. Although Mary Lee accompanied her husband to Hampton Roads, she spent about a third of her time at Arlington, though the couple's first son, Custis Lee was born at Fort Monroe. Although the two were by all accounts devoted to each other, they were different in character: Robert Lee was tidy and punctual, qualities his wife lacked. Mary Lee also had trouble transitioning from being a rich man's daughter to having to manage a household with only one or two slaves. Beginning in 1832, Robert Lee had a close but platonic relationship with Harriett Talcott, wife of his fellow officer Andrew Talcott.
Life at Fort Monroe was marked by conflicts between artillery and engineering officers. Eventually the War Department transferred all engineering officers away from Fort Monroe, except Lee, who was ordered to take up residence on the artificial island of Rip Raps across the river from Fort Monroe, where Fort Wool would eventually rise, and continue work to improve the island. Lee duly moved there, then discharged all workers and informed the War Department he could not maintain laborers without the facilities of the fort.
In 1834, Lee was transferred to Washington as General Gratiot's assistant. Lee had hoped to rent a house in Washington for his family, but was not able to find one; the family lived at Arlington, though Lieutenant Lee rented a room at a Washington boarding house for when the roads were impassable. In mid-1835, Lee was assigned to assist Andrew Talcott in surveying the southern border of Michigan. While on that expedition, he responded to a letter from an ill Mary Lee, which had requested he come to Arlington, "But why do you urge my "immediate" return, & tempt one in the "strongest" manner[?] ... I rather require to be strengthened & encouraged to the "full" performance of what I am called on to execute." Lee completed the assignment and returned to his post in Washington, finding his wife ill at Ravensworth. Mary Lee, who had recently given birth to their second child, remained bedridden for several months. In October 1836, Lee was promoted to first lieutenant.
Lee served as an assistant in the chief engineer's office in Washington, D.C. from 1834 to 1837, but spent the summer of 1835 helping to lay out the state line between Ohio and Michigan. As a first lieutenant of engineers in 1837, he supervised the engineering work for St. Louis harbor and for the upper Mississippi and Missouri rivers. Among his projects was the mapping of the Des Moines Rapids on the Mississippi above Keokuk, Iowa, where the Mississippi's mean depth of 2.4 ft was the upper limit of steamboat traffic on the river. His work there earned him a promotion to captain. Around 1842, Captain Robert E. Lee arrived as Fort Hamilton's post engineer.
Marriage and family.
While Lee was stationed at Fort Monroe, he married Mary Anna Randolph Custis (1808–73), great-granddaughter of Martha Washington by her first husband Daniel Parke Custis, and step-great-granddaughter of George Washington, the first president of the United States. Mary was the only surviving child of George Washington Parke Custis, George Washington's stepgrandson, and Mary Lee Fitzhugh Custis, daughter of William Fitzhugh and Ann Bolling Randolph. Robert and Mary married on June 30, 1831, at Arlington House, her parents' house just across from Washington, D.C. The 3rd U.S. Artillery served as honor guard at the marriage. They eventually had seven children, three boys and four girls:
All the children survived him except for Annie, who died in 1862. They are all buried with their parents in the crypt of the Lee Chapel at Washington and Lee University in Lexington, Virginia.
Lee was a great-great-great grandson of William Randolph and a great-great grandson of Richard Bland. He was also related to Helen Keller through Helen's mother, Kate, and was a distant relative of Admiral Willis Augustus Lee.
On May 1, 1864, General Lee was at the baptism of General A.P. Hill's daughter, Lucy Lee Hill, to serve as her godfather. This is referenced in the painting "Tender is the Heart" by Mort Künstler.
Mexican–American War.
Lee distinguished himself in the Mexican–American War (1846–48). He was one of Winfield Scott's chief aides in the march from Veracruz to Mexico City. He was instrumental in several American victories through his personal reconnaissance as a staff officer; he found routes of attack that the Mexicans had not defended because they thought the terrain was impassable.
He was promoted to brevet major after the Battle of Cerro Gordo on April 18, 1847. He also fought at Contreras, Churubusco, and Chapultepec and was wounded at the last. By the end of the war, he had received additional brevet promotions to Lieutenant Colonel and Colonel, but his permanent rank was still Captain of Engineers and he would remain a Captain until his transfer to the cavalry in 1855.
For the first time, Robert E. Lee and Ulysses S. Grant met and worked with each other during the Mexican–American War. Both Lee and Grant participated in Scott's march from the coastal town of Vera Cruz to Mexico City. Grant gained wartime experience as a quartermaster, Lee as an engineer who positioned troops and artillery. Both did their share of actual fighting. At Vera Cruz, Lee earned a commendation for "greatly distinguished" service. Grant was among the leaders at the bloody assault at Molino del Rey, and both soldiers were among the forces that entered Mexico City. Close observations of their commanders constituted a learning process for both Lee and Grant. The Mexican–American War concluded on February 2, 1848.
After the Mexican War, Lee spent three years at Fort Carroll in Baltimore harbor. During this time, his service was interrupted by other duties, among them surveying and updating maps in Florida. Cuban revolutionary Narciso López intended to forcibly liberate Cuba from Spanish rule. In 1849, searching for a leader for his filibuster expedition, he approached Jefferson Davis, then a United States senator. Davis declined and suggested Lee, who also declined. Both decided it was inconsistent with their duties.
Early 1850s: West Point and Texas.
The 1850s were a difficult time for Lee, with his long absences from home, the increasing disability of his wife, troubles in taking over the management of a large slave plantation, and his often morbid concern with his personal failures.
In 1852, Lee was appointed Superintendent of the Military Academy at West Point; he was reluctant to enter what he called a "snake pit", but the War Department insisted and he obeyed. His wife occasionally came to visit. During his three years at West Point, Brevet Colonel Robert E. Lee improved the buildings and courses and spent much time with the cadets. Lee's oldest son, George Washington Custis Lee, attended West Point during his tenure. Custis Lee graduated in 1854, first in his class.
Lee was enormously relieved to receive a long-awaited promotion as second-in-command of the Second Cavalry regiment in Texas in 1855. It meant leaving the Engineering Corps and its sequence of staff jobs for the combat command he truly wanted. He served under Colonel Albert Sidney Johnston at Camp Cooper, Texas; their mission was to protect settlers from attacks by the Apache and the Comanche.
Late 1850s: Arlington plantation and the Custis slaves.
In 1857, his father-in-law George Washington Parke Custis died, creating a serious crisis when Lee took on the burden of executing the will. Custis's will encompassed vast landholdings and hundreds of slaves balanced against massive debts, and required Custis's former slaves "to be emancipated by my executors in such manner as to my executors may seem most expedient and proper, the said emancipation to be accomplished in not exceeding five years from the time of my decease." The estate was in disarray, and the plantations had been poorly managed and were losing money.
Lee tried to hire an overseer to handle the plantation in his absence, writing to his cousin, "I wish to get an energetic honest farmer, who while he will be considerate & kind to the negroes, will be firm & make them do their duty." But Lee failed to find a man for the job, and had to take a two-year leave of absence from the army in order to run the plantation himself. He found the experience frustrating and difficult; some of the slaves were unhappy and demanded their freedom. Many of them had been given to understand that they were to be made free as soon as Custis died. In May 1858, Lee wrote to his son Rooney, "I have had some trouble with some of the people. Reuben, Parks & Edward, in the beginning of the previous week, rebelled against my authority—refused to obey my orders, & said they were as free as I was, etc., etc.—I succeeded in capturing them & lodging them in jail. They resisted till overpowered & called upon the other people to rescue them." Less than two months after they were sent to the Alexandria jail, Lee decided to remove these three men and three female house slaves from Arlington, and sent them under lock and key to the slave-trader William Overton Winston in Richmond, who was instructed to keep them in jail until he could find "good & responsible" slaveholders to work them until the end of the five-year period.
The Norris case.
In 1859, three of the Arlington slaves—Wesley Norris, his sister Mary, and a cousin of theirs—fled for the North, but were captured a few miles from the Pennsylvania border and forced to return to Arlington. On June 24, 1859, the anti-slavery newspaper "New York Daily Tribune" published two anonymous letters (dated June 19, 1859 and June 21, 1859), each claiming to have heard that Lee had the Norrises whipped, and each going so far as to claim that the overseer refused to whip the woman but that Lee took the whip and flogged her personally. Lee privately wrote to his son Custis that "The N. Y. Tribune has attacked me for my treatment of your grandfather's slaves, but I shall not reply. He has left me an unpleasant legacy."
Biographers of Lee have differed over the credibility of the letters in the "Tribune". They broadly agree that Lee had a group of escaped slaves recaptured, and that after recapturing them he hired them out off of the Arlington plantation as a punishment; but they disagree over the likelihood that Lee flogged them, and over the charge that he personally whipped Mary Norris. In 1934, Douglas S. Freeman described them as "Lee's first experience with the extravagance of irresponsible antislavery agitators" and asserted that "There is no evidence, direct or indirect, that Lee ever had them or any other Negroes flogged. The usage at Arlington and elsewhere in Virginia among people of Lee's station forbade such a thing." In 2000, Michael Fellman, in "The Making of Robert E. Lee", found the claims that Lee had "personally" whipped "Mary" Norris "extremely unlikely," but found it not at all unlikely that Lee had ordered the runaways whipped: "corporal punishment (for which Lee substituted the euphemism 'firmness') was (believed to be) an intrinsic and necessary part of slave discipline. Although it was supposed to be applied only in a calm and rational manner, overtly physical domination of slaves, unchecked by law, was always brutal and potentially savage." In 2003, Bernice-Marie Yates's "The Perfect Gentleman", cited Freeman's denial and followed his account in holding that, because of Lee's family connections to George Washington, he "was a prime target for abolitionists who lacked all the facts of the situation." In 2014, Michael Korda wrote that that "Although these letters are dismissed by most of Lee's biographers as exaggerated, or simply as unfounded abolitionist propaganda, it is hard to ignore them. [...] It seems incongruously out of character for Lee to have whipped a slave woman himself, particularly one stripped to the waist, and that charge may have been a flourish added by the two correspondents; interestingly enough, it was not repeated by Wesley Norris when his account of the incident was published in 1866. [...] [A]lthough it seems unlikely that he would have done any of the whipping himself, he may not have flinched from observing it to make sure his orders were carried out exactly."
Wesley Norris himself spoke out about the incident after the war, in an 1866 interview printed in an abolitionist newspaper, the "National Anti-Slavery Standard". Norris stated that after they had been captured, and forced to return to Arlington, Lee told them that "he would teach us a lesson we would not soon forget." According to Norris, Lee then had the three of them firmly tied to posts by the overseer, and ordered them whipped with fifty lashes for the men and twenty for Mary Norris. Norris claimed that Lee encouraged the whipping, and that when the overseer refused to do it, called in the county constable to do it instead, but that Lee himself did not personally whip any of the slaves. According to Norris, Lee then had the overseer rub their lacerated backs with brine.
The Norris men were then sent by Lee's agent to work on the railroads in Richmond and Alabama. Wesley Norris gained his freedom in January 1863 by slipping through the Confederate lines near Richmond to Union-controlled territory. Lee freed the other Custis slaves after the end of the five-year period in the winter of 1862, filing the deed of manumission on December 29, 1862.
Lee's views on slavery.
Since the end of the Civil War, it has often been suggested Lee was in some sense opposed to slavery. In the period following the war, Lee became a central figure in the Lost Cause interpretation of the war. The argument that Lee had always somehow opposed slavery helped maintain his stature as a symbol of Southern honor and national reconciliation. Freeman's analysis places Lee's attitude toward slavery and abolition in a historical context:
This [opinion] was the prevailing view among most religious people of Lee's class in the border states. They believed that slavery existed because God willed it and they thought it would end when God so ruled. The time and the means were not theirs to decide, conscious though they were of the ill-effects of Negro slavery on both races. Lee shared these convictions of his neighbors without having come in contact with the worst evils of African bondage. He spent no considerable time in any state south of Virginia from the day he left Fort Pulaski in 1831 until he went to Texas in 1856. All his reflective years had been passed in the North or in the border states. He had never been among the blacks on a cotton or rice plantation. At Arlington, the servants had been notoriously indolent, their master's master. Lee, in short, was only acquainted with slavery at its best, and he judged it accordingly. At the same time, he was under no illusion regarding the aims of the Abolitionists or the effect of their agitation.
A key source cited by defenders and critics is Lee's 1856 letter to his wife:
... In this enlightened age, there are few I believe, but what will acknowledge, that slavery as an institution, is a moral & political evil in any Country. It is useless to expatiate on its disadvantages. I think it however a greater evil to the white man than to the black race, & while my feelings are strongly enlisted in behalf of the latter, my sympathies are more strong for the former. The blacks are immeasurably better off here than in Africa, morally, socially & physically. The painful discipline they are undergoing, is necessary for their instruction as a race, & I hope will prepare & lead them to better things. How long their subjugation may be necessary is known & ordered by a wise Merciful Providence.—Robert E. Lee, to Mary Anna Lee, December 27, 1856
The evidence cited in favor of the claim that Lee opposed slavery included his direct statements and his actions before and during the war, including Lee's support of the work by his wife and her mother to liberate slaves and fund their move to Liberia, the success of his wife and daughter in setting up an illegal school for slaves on the Arlington plantation, the freeing of Custis' slaves in 1862, and, as the Confederacy's position in the war became desperate, his petitioning slaveholders in 1864–65 to allow slaves to volunteer for the Army with manumission offered as a reward for outstanding service.
In December 1864 Lee was shown a letter by Louisiana Senator Edward Sparrow, written by General St. John R. Liddell, which noted Lee would be hard-pressed in the interior of Virginia by spring, and the need to consider Patrick Cleburne's plan to emancipate the slaves and put all men in the army who were willing to join. Lee was said to have agreed on all points and desired to get black soldiers, saying "he could make soldiers out of any human being that had arms and legs."
Harpers Ferry and Texas, 1859–61.
Both Harpers Ferry and the secession of Texas were monumental events leading up to the Civil War. Robert E. Lee was at both events. Lee initially remained loyal to the Union after Texas seceded.
Harpers Ferry.
John Brown led a band of 21 abolitionists who seized the federal arsenal at Harpers Ferry, Virginia, in October 1859, hoping to incite a slave rebellion. President James Buchanan gave Lee command of detachments of militia, soldiers, and United States Marines, to suppress the uprising and arrest its leaders. By the time Lee arrived that night, the militia on the site had surrounded Brown and his hostages. At dawn, Brown refused the demand for surrender. Lee attacked, and Brown and his followers were captured after three minutes of fighting. Lee's summary report of the episode shows Lee believed it "was the attempt of a fanatic or madman". Lee said Brown achieved "temporary success" by creating panic and confusion and by "magnifying" the number of participants involved in the raid.
Texas.
In 1860, Lt. Col. Robert E. Lee relieved Major Heintzelman, Fort Brown, and the Mexican authorities offered to restrain "their citizens from making predatory descents upon the territory and people of Texas...this was the last active operation of the Cortina War". Rip Ford, a Texas Ranger at the time, described Lee as, "dignified without hauteur, grand without pride...he evinced an imperturbable self-possession, and a complete control of his passions...possessing the capacity to accomplish great ends and the gift of controlling and leading men."
When Texas seceded from the Union in February 1861, General David E. Twiggs surrendered all the American forces (about 4,000 men, including Lee, and commander of the Department of Texas) to the Texans. Twiggs immediately resigned from the U. S. Army and was made a Confederate general. Lee went back to Washington and was appointed Colonel of the First Regiment of Cavalry in March 1861. Lee's colonelcy was signed by the new President, Abraham Lincoln. Three weeks after his promotion, Colonel Lee was offered a senior command (with the rank of Major General) in the expanding Army to fight the Southern States that had left the Union. Fort Mason, Texas was Lee's last command with the United States Army.
Civil War.
Lee privately ridiculed the Confederacy in letters in early 1861, denouncing secession as "revolution" and a betrayal of the efforts of the founders. Writing to his son William Fitzhugh, Lee stated, "I can anticipate no greater calamity for the country than a dissolution of the Union." While he was not opposed in principle to secession, Lee wanted all peaceful ways of resolving the differences between North and South—such as the Crittenden Compromise—to be tried first, and was one of the few to foresee a long and difficult war.
The commanding general of the Union Army, Winfield Scott, told Lincoln he wanted Lee for a top command. Lee accepted a promotion to colonel on March 28. He had earlier been asked by one of his lieutenants if he intended to fight for the Confederacy or the Union, to which Lee replied, "I shall never bear arms against the Union, but it may be necessary for me to carry a musket in the defense of my native state, Virginia, in which case I shall not prove recreant to my duty." Meanwhile, Lee ignored an offer of command from the CSA. After Lincoln's call for troops to put down the rebellion, it was obvious that Virginia would quickly secede. Lee on April 18 was offered by presidential advisor Francis P. Blair a role as major general to command the defense of Washington. He replied:
Lee resigned from the U.S. Army on April 20 and took up command of the Virginia state forces on April 23. While historians have usually called his decision inevitable ("the answer he was born to make", wrote one; another called it a "no-brainer") given the ties to family and state, recent research shows that the choice was a difficult one that Lee made alone, without pressure from friends or family. His daughter Mary Custis was the only one among those close to Lee who favored secession, and wife Mary Anna especially favored the Union, so his decision astounded them. While Lee's immediate family followed him to the Confederacy, others, such as cousins and fellow officers Samuel Phillips and John Fitzgerald, remained loyal to the Union, as did 40% of all Virginian officers.
Early role.
At the outbreak of war, Lee was appointed to command all of Virginia's forces, but upon the formation of the Confederate States Army, he was named one of its first five full generals. Lee did not wear the insignia of a Confederate general, but only the three stars of a Confederate colonel, equivalent to his last U.S. Army rank. He did not intend to wear a general's insignia until the Civil War had been won and he could be promoted, in peacetime, to general in the Confederate Army.
Lee's first field assignment was commanding Confederate forces in western Virginia, where he was defeated at the Battle of Cheat Mountain and was widely blamed for Confederate setbacks. He was then sent to organize the coastal defenses along the Carolina and Georgia seaboard, appointed commander, "Department of South Carolina, Georgia and Florida" on November 5, 1861. Between then and the fall of Fort Pulaski, April 11, 1862, he put in place a defense of Savannah that proved successful in blocking Federal advance on Savannah. Confederate fort and naval gunnery dictated night time movement and construction by the besiegers. Federal preparations required four months. In those four months, Lee developed a defense in depth. Behind Fort Pulaski on the Savannah River, Fort Jackson was improved, and two additional batteries covered river approaches. In the face of the Union superiority in naval, artillery and infantry deployment, Lee was able to block any Federal advance on Savannah, and at the same time, well-trained Georgia troops were released in time to meet McClellan's Peninsula Campaign. The City of Savannah would not fall until Sherman's approach from the interior at the end of 1864.
At first, the press spoke to the disappointment of losing Fort Pulaski. Surprised by the effectiveness of large caliber Parrott Rifles in their first deployment, it was widely speculated that only betrayal could have brought overnight surrender to a Third System Fort. Lee was said to have failed to get effective support in the Savannah River from the three sidewheeler gunboats of the Georgia Navy. Although again blamed by the press for Confederate reverses, he was appointed military adviser to Confederate President Jefferson Davis, the former U.S. Secretary of War. While in Richmond, Lee was ridiculed as the 'King of Spades' for his excessive digging of trenches around the capitol. These trenches would later play a pivotal role in battles near the end of the war.
Commander, Army of Northern Virginia.
Following the wounding of Gen. Joseph E. Johnston at the Battle of Seven Pines, on June 1, 1862, Lee assumed command of the Army of Northern Virginia, his first opportunity to lead an army in the field. Early in the war, his men called him "Granny Lee" because of his allegedly timid style of command. Confederate newspaper editorials of the day objected to his appointment due to concerns that Lee would not be aggressive and would wait for the Union army to come to him. He oversaw substantial strengthening of Richmond's defenses during the first three weeks of June.
In the spring of 1862, as part of the Peninsula Campaign, the Union Army of the Potomac under General George B. McClellan advanced upon Richmond from Fort Monroe, eventually reaching the eastern edges of the Confederate capital along the Chickahominy River. Lee then launched a series of attacks, the Seven Days Battles, against McClellan's forces.
Lee's assaults resulted in heavy Confederate casualties. They were marred by clumsy tactical performances by his division commanders, but his aggressive actions unnerved McClellan, who retreated to a point on the James River and abandoned the Peninsula Campaign. These successes led to a rapid turnaround of Confederate public opinion, and the newspaper editorials quickly changed their tune on Lee's aggressiveness. After the Seven Days Battles until the end of the war his men called him simply "Marse Robert", a term of respect and affection.
This stunning Unionist setback—followed by an alarming drop in Northern morale—impelled Lincoln to adopt a new policy of relentless, committed warfare. Three weeks after the Seven Days Battles, Lincoln informed his cabinet that he intended to issue an executive order to free slaves as a military necessity.
After McClellan's retreat, Lee defeated another Union army at the Second Battle of Bull Run. Within 90 days of taking command, Lee had run McClellan off the Peninsula, defeated John Pope at Second Manassas, and the battle lines had moved from 6 mi outside Richmond, to 20 mi outside Washington. Instead of a quick end to the war that McClellen's Peninsula Campaign had promised, the war would go on for almost another 3 years and claim a half million more lives, and end with liberation of four million slaves and the devastation of the Southern slave-based society.
Lee then invaded Maryland, hoping to replenish his supplies and possibly influence the Northern elections to fall in favor of ending the war. McClellan's men recovered a lost order that revealed Lee's plans. McClellan always exaggerated Lee's numerical strength, but now he knew the Confederate army was divided and could be destroyed by an all-out attack at Antietam. McClellan, however, was too slow in moving, not realizing Lee had been informed by a spy that McClellan had the plans. Lee urgently recalled Stonewall Jackson, concentrating his forces west of Antietam Creek, near Sharpsburg, Maryland. In the bloodiest day of the war, with both sides suffering enormous losses, Lee withstood the Union assaults. He withdrew his battered army back to Virginia while President Abraham Lincoln used the Confederate reversal as an opportunity to announce the Emancipation Proclamation which put the Confederacy on the diplomatic and moral defensive, and would ultimately devastate the Confederacy's slave-based economy.
Disappointed by McClellan's failure to destroy Lee's army, Lincoln named Ambrose Burnside as commander of the Army of the Potomac. Burnside ordered an attack across the Rappahannock River at Fredericksburg. Delays in building bridges across the river allowed Lee's army ample time to organize strong defenses, and the frontal assault on December 13, 1862, was a disaster for the Union. There were 12,600 Union casualties to 5,000 Confederate; one of the most "one-sided battles" in the Civil War. Lee reportedly stated after the Confederate victory, "It is well that war is so terrible--we should grow too fond of it". At Fredericksburg, according to historian Michael Fellman, Lee had completely entered into the "spirit of war, where destructiveness took on its own beauty." After the bitter Union defeat at Fredericksburg, President Lincoln named Joseph Hooker commander of the Army of the Potomac. Hooker's advance to attack Lee in May 1863, near Chancellorsville, Virginia, was defeated by Lee and Stonewall Jackson's daring plan to divide the army and attack Hooker's flank. It was a victory over a larger force, but it also came with high casualties. It was particularly costly in one respect: Lee's finest corps commander, Stonewall Jackson, was accidentally fired upon by his own troops. Weakened by his wounds, he died some days later, most likely of pneumonia.
Battle of Gettysburg.
The critical decisions came in May–June 1863, after Lee's smashing victory at the Battle of Chancellorsville. The western front was crumbling, as multiple uncoordinated Confederate armies were unable to handle General Ulysses S. Grant's campaign against Vicksburg. The top military advisers wanted to save Vicksburg, but Lee persuaded Davis to overrule them and authorize yet another invasion of the North. The immediate goal was to acquire urgently needed supplies from the rich farming districts of Pennsylvania; a long-term goal was to stimulate peace activity in the North by demonstrating the power of the South to invade. Lee's decision proved a significant strategic blunder and cost the Confederacy control of its western regions, and nearly cost Lee his own army as Union forces cut him off from the South. Lee had to fight his way out at Gettysburg.
In the summer of 1863, Lee invaded the North again, marching through western Maryland and into south central Pennsylvania. He encountered Union forces under George G. Meade at the three-day Battle of Gettysburg in Pennsylvania in July; the battle would produce the largest number of casualties in the American Civil War. With some of his subordinates being new and inexperienced in their commands, J.E.B. Stuart's cavalry being out of the area, and Lee being slightly ill, he was less than comfortable with how events were unfolding. While the first day of battle was controlled by the Confederates, key terrain that should have been taken by General Ewell was not. The second day ended with the Confederates unable to break the Union position, and the Union being more solidified. Lee's decision on the third day, against the sound judgment of his best corps commander General Longstreet, to launch a massive frontal assault on the center of the Union line was disastrous. The assault known as Pickett's Charge was repulsed and resulted in heavy Confederate losses. The general rode out to meet his retreating army and proclaimed, "All this has been my fault." Lee was compelled to retreat. Despite flooded rivers that blocked his retreat, he escaped Meade's ineffective pursuit. Following his defeat at Gettysburg, Lee sent a letter of resignation to President Davis on August 8, 1863, but Davis refused Lee's request. That fall, Lee and Meade met again in two minor campaigns that did little to change the strategic standoff. The Confederate Army never fully recovered from the substantial losses incurred during the 3-day battle in southern Pennsylvania. The historian Shelby Foote stated, "Gettysburg was the price the South paid for having Robert E. Lee as commander."
Ulysses S. Grant and the Union offensive.
In 1864 the new Union general-in-chief, Lt. Gen. Ulysses S. Grant, sought to use his large advantages in manpower and material resources to destroy Lee's army by attrition, pinning Lee against his capital of Richmond. Lee successfully stopped each attack, but Grant with his superior numbers kept pushing each time a bit farther to the southeast. These battles in the Overland Campaign included the Wilderness, Spotsylvania Court House and Cold Harbor.
Grant eventually was able to stealthily move his army across the James River. After stopping a Union attempt to capture Petersburg, Virginia, a vital railroad link supplying Richmond, Lee's men built elaborate trenches and were besieged in Petersburg, a development which presaged the trench warfare of World War I. He attempted to break the stalemate by sending Jubal A. Early on a raid through the Shenandoah Valley to Washington, D.C., but was defeated early on by the superior forces of Philip Sheridan. The Siege of Petersburg lasted from June 1864 until March 1865, with Lee's outnumbered and poorly supplied army shrinking daily because of desertions by disheartened Confederates.
General-in-chief.
On January 31, 1865, Lee was promoted to general-in-chief of Confederate forces.
As the South ran out of manpower the issue of arming the slaves became paramount. By late 1864, the army so dominated the Confederacy that civilian leaders were unable to block the military's proposal, strongly endorsed by Lee, to arm and train slaves in Confederate uniform for combat. In return for this service, slave soldiers and their families would be emancipated. Lee explained, "We should employ them without delay ... [along with] gradual and general emancipation." The first units were in training as the war ended. As the Confederate army was devastated by casualties, disease and desertion, the Union attack on Petersburg succeeded on April 2, 1865. Lee abandoned Richmond and retreated west. Lee then made an attempt to escape to the southwest and join up with Joseph E. Johnston's Army of Tennessee in North Carolina. However, his forces were soon surrounded and he surrendered them to Grant on April 9, 1865, at the Battle of Appomattox Court House. Other Confederate armies followed suit and the war ended. The day after his surrender, Lee issued his Farewell Address to his army.
Lee resisted calls by some officers to reject surrender and allow small units to melt away into the mountains, setting up a lengthy guerrilla war. He insisted the war was over and energetically campaigned for inter-sectional reconciliation. "So far from engaging in a war to perpetuate slavery, I am rejoiced that slavery is abolished. I believe it will be greatly for the interests of the South."
Lee's Civil War battle summaries.
The following are summaries of Civil War campaigns and major battles where Robert E. Lee was the commanding officer:
After the war.
After the war, Lee was not arrested or punished, but he did lose the right to vote as well as some property. Lee supported President Johnson's plan of Reconstruction, but joined with Democrats in opposing the Radical Republicans who demanded punitive measures against the South, distrusted its commitment to the abolition of slavery and, indeed, distrusted the region's loyalty to the United States. Lee generally supported civil rights for all, as well as a system of free public schools for blacks, but forthrightly opposed allowing blacks to vote. "My own opinion is that, at this time, they [black Southerners] cannot vote intelligently, and that giving them the [vote] would lead to a great deal of demagogism, and lead to embarrassments in various ways," Lee stated. Emory Thomas says Lee had become a suffering Christ-like icon for ex-Confederates. President Grant invited him to the White House in 1869, and he went. Nationally he became an icon of reconciliation between the North and South, and the reintegration of former Confederates into the national fabric.
Lee's prewar family home, the Custis-Lee Mansion, was seized by Union forces during the war and turned into Arlington National Cemetery. The family was compensated in 1883.
Lee hoped to retire to a farm of his own, but he was too much a regional symbol to live in obscurity. From April to June 1865, he and his family resided in Richmond at the Stewart-Lee House. He accepted an offer to serve as the president of Washington College (now Washington and Lee University) in Lexington, Virginia, and served from October 1865 until his death. The Trustees used his famous name in large-scale fund-raising appeals and Lee transformed Washington College into a leading Southern college expanding its offerings significantly and added programs in commerce, journalism, and integrated the Lexington Law School. Lee was well liked by the students, which enabled him to announce an "honor system" like West Point's, explaining "We have but one rule here, and it is that every student be a gentleman." To speed up national reconciliation Lee recruited students from the North and made certain they were well treated on campus and in town.
Several glowing appraisals of Lee's tenure as college president have survived, depicting the dignity and respect he commanded among all. Previously, most students had been obliged to occupy the campus dormitories, while only the most mature were allowed to live off-campus. Lee quickly reversed this rule, requiring most students to board off-campus, and allowing only the most mature to live in the dorms as a mark of privilege; the results of this policy were considered a success. A typical account by a professor there states that "the students fairly worshipped him, and deeply dreaded his displeasure; yet so kind, affable, and gentle was he toward them that all loved to approach him. ... No student would have dared to violate General Lee's expressed wish or appeal; if he had done so, the students themselves would have driven him from the college."
While at Washington College, Lee told a colleague that the greatest mistake of his life was taking a military education.
President Johnson's amnesty pardons.
On May 29, 1865, President Andrew Johnson issued a Proclamation of Amnesty and Pardon to persons who had participated in the rebellion against the United States. There were fourteen excepted classes, though, and members of those classes had to make special application to the President. Lee sent an application to Grant and wrote to President Johnson on June 13, 1865:
Being excluded from the provisions of amnesty & pardon contained in the proclamation of the 29th Ulto; I hereby apply for the benefits, & full restoration of all rights & privileges extended to those included in its terms. I graduated at the Mil. Academy at West Point in June 1829. Resigned from the U.S. Army April '61. Was a General in the Confederate Army, & included in the surrender of the Army of N. Virginia 9 April '65.
On October 2, 1865, the same day that Lee was inaugurated as president of Washington College in Lexington, Virginia, he signed his Amnesty Oath, thereby complying fully with the provision of Johnson's proclamation. Lee was not pardoned, nor was his citizenship restored.
Three years later, on December 25, 1868, Johnson proclaimed a second amnesty which removed previous exceptions, such as the one that affected Lee.
Postwar politics.
Lee, who had opposed secession and remained mostly indifferent to politics before the Civil War, supported President Johnson's plan of Presidential Reconstruction that took effect in 1865–66. However, he opposed the Congressional Republican program that took effect in 1867. In February 1866, he was called to testify before the Joint Congressional Committee on Reconstruction in Washington, where he expressed support for President Andrew Johnson's plans for quick restoration of the former Confederate states, and argued that restoration should return, as far as possible, the status quo ante in the Southern states' governments (with the exception of slavery).
Lee told the Committee, "...every one with whom I associate expresses kind feelings towards the freedmen. They wish to see them get on in the world, and particularly to take up some occupation for a living, and to turn their hands to some work." Lee also expressed his "willingness that blacks should be educated, and ... that it would be better for the blacks and for the whites." Lee forthrightly opposed allowing blacks to vote: "My own opinion is that, at this time, they [black Southerners] cannot vote intelligently, and that giving them the [vote] would lead to a great deal of demagogism, and lead to embarrassments in various ways." Lee also recommended the deportation of African Americans from Virginia and even mentioned that Virginians would give aid in the deportation. "I think it would be better for Virginia if she could get rid of them [African Americans]. ... I think that everyone there would be willing to aid it."
In an interview in May 1866, Lee said, "The Radical party are likely to do a great deal of harm, for we wish now for good feeling to grow up between North and South, and the President, Mr. Johnson, has been doing much to strengthen the feeling in favor of the Union among us. The relations between the Negroes and the whites were friendly formerly, and would remain so if legislation be not passed in favor of the blacks, in a way that will only do them harm."
In 1868, Lee's ally Alexander H. H. Stuart drafted a public letter of endorsement for the Democratic Party's presidential campaign, in which Horatio Seymour ran against Lee's old foe Republican Ulysses S. Grant. Lee signed it along with thirty-one other ex-Confederates. The Democratic campaign, eager to publicize the endorsement, published the statement widely in newspapers. Their letter claimed paternalistic concern for the welfare of freed Southern blacks, stating that "The idea that the Southern people are hostile to the negroes and would oppress them, if it were in their power to do so, is entirely unfounded. They have grown up in our midst, and we have been accustomed from childhood to look upon them with kindness." However, it also called for the restoration of white political rule, arguing that "It is true that the people of the South, in common with a large majority of the people of the North and West, are, for obvious reasons, inflexibly opposed to any system of laws that would place the political power of the country in the hands of the negro race. But this opposition springs from no feeling of enmity, but from a deep-seated conviction that, at present, the negroes have neither the intelligence nor the other qualifications which are necessary to make them safe depositories of political power."
In his public statements and private correspondence, Lee argued that a tone of reconciliation and patience would further the interests of white Southerners better than hotheaded antagonism to federal authority or the use of violence. Lee repeatedly expelled white students from Washington College for violent attacks on local black men, and publicly urged obedience to the authorities and respect for law and order. In 1869–70 he was a leader in successful efforts to establish state-funded schools for blacks. He privately chastised fellow ex-Confederates such as Jefferson Davis and Jubal Early for their frequent, angry responses to perceived Northern insults, writing in private to them as he had written to a magazine editor in 1865, that "It should be the object of all to avoid controversy, to allay passion, give full scope to reason and to every kindly feeling. By doing this and encouraging our citizens to engage in the duties of life with all their heart and mind, with a determination not to be turned aside by thoughts of the past and fears of the future, our country will not only be restored in material prosperity, but will be advanced in science, in virtue and in religion."
Illness and death.
On September 28, 1870, Lee suffered a stroke. He died two weeks later, shortly after 9 a.m. on October 12, 1870, in Lexington, Virginia, from the effects of pneumonia. According to one account, his last words on the day of his death, were "Tell Hill he must come up. Strike the tent", but this is debatable because of conflicting accounts and because Lee's stroke had resulted in aphasia, possibly rendering him unable to speak.
At first no suitable coffin for the body could be located. An undertaker had ordered three from Richmond that had reached Lexington, but due to unprecedented flooding from long-continued heavy rains, the caskets were washed away. Two neighborhood boys, C.G. Chittum and Robert E. Hillis, found one of the coffins that had been swept ashore. Undamaged, it was used for the General's body, though it was a bit short for him. As a result, Lee was buried without shoes. He was buried underneath Lee Chapel at Washington and Lee University, where his body remains.
Legacy.
Among Southerners, Lee came to be even more revered after his surrender than he had been during the war, when Stonewall Jackson had been the great Confederate hero. In an address before the Southern Historical Society in Atlanta, Georgia in 1874, Benjamin Harvey Hill described Lee in this way:
He was a foe without hate; a friend without treachery; a soldier without cruelty; a victor without oppression, and a victim without murmuring. He was a public officer without vices; a private citizen without wrong; a neighbour without reproach; a Christian without hypocrisy, and a man without guile. He was a Caesar, without his ambition; Frederick, without his tyranny; Napoleon, without his selfishness, and Washington, without his reward.
His reputation continued to grow. By the end of the 19th century, his popularity had spread to the North. Lee's admirers have pointed to his character and devotion to duty, and his brilliant tactical successes in battle after battle against a stronger foe.
According to my notion of military history there is as much instruction both in strategy and in tactics to be gleaned from General Lee's operations of 1862 as there is to be found in Napoleon's campaigns of 1796.—Field Marshal Garnet Wolseley
Military historians continue to pay attention to his battlefield tactics and maneuvering, though many think he should have designed better strategic plans for the Confederacy. However, it should be noted that he was not given full direction of the Southern war effort until late in the conflict.
Robert E. Lee has been commemorated on U.S. postage stamps at least five times, the first one being a commemorative stamp that also honored Stonewall Jackson, issued in 1936. A second 'regular issue' stamp was issued in 1955. He was commemorated with a 32-cent stamp issued in the American Civil War Issue of June 29, 1995. His horse Traveller is pictured in the background. Explanatory text is imprinted on the back of each stamp, issued in a sheet of 20 commemorative Civil War stamps. Stamp Ventures printed the stamps in the gravure process. An image of the stamp is available at Arago online at the link in the footnote.
Washington and Lee University in Lexington, Virginia was commemorated on its 200th anniversary on November 23, 1948, with a 3-cent postage. The central design is a view of the university, flanked by portraits of generals George Washington and Robert E. Lee. Lee was again commemorated on a commemorative stamp in 1970, along with Jefferson Davis and Thomas J. "Stonewall" Jackson, depicted on horseback on the 6-cent Stone Mountain Memorial commemorative issue, modeled after the actual Stone Mountain Memorial carving in Georgia. The stamp was issued on September 19, 1970 in conjunction with the dedication of the Stone Mountain Confederate Memorial in Georgia on May 9, 1970. The design of the stamp replicates the memorial, the largest high relief sculpture in the world. It is carved on the side of Stone Mountain 400 feet above the ground.
On September 29, 2007, General Lee's three Civil War-era letters were sold for $61,000 at auction by Thomas Willcox, much less than the record of $630,000 for a Lee item in 2002. The auction included more than 400 documents of Lee's from the estate of the parents of Willcox that had been in the family for generations. South Carolina sued to stop the sale on the grounds that the letters were official documents and therefore property of the state, but the court ruled in favor of Willcox.
On January 30, 1975, Senate Joint Resolution 23, "A joint resolution to restore posthumously full rights of citizenship to General R. E. Lee" was introduced into the Senate by Senator Harry F. Byrd, Jr. (I-VA), the result of a five-year campaign to accomplish this. The resolution, which enacted Public Law 94-67, was passed, and the bill was signed by President Gerald Ford on September 5.
Monuments, memorials and commemorations.
Since it was built in 1884, the most prominent monument in New Orleans has been a 60 ft-tall monument to General Lee. A 16.5 ft statue of Lee stands tall upon a towering column of white marble in the middle of Lee Circle. The statue of Lee, which weighs more than 7,000 pounds, faces the North. Lee Circle is situated along New Orleans' famous St. Charles Avenue. The New Orleans streetcars roll past Lee Circle and New Orleans' best Mardi Gras parades go around Lee Circle (the spot is so popular that bleachers are set up annually around the perimeter for Mardi Gras). Around the corner from Lee Circle is New Orleans' Confederate Museum, which contains the second largest collection of Confederate memorabilia in the world. In a tribute to Lee Circle (which had formerly been known as Tivoli Circle), former Confederate soldier George Washington Cable wrote:
Arlington House, The Robert E. Lee Memorial, also known as the Custis-Lee Mansion, is a Greek revival mansion in Arlington, Virginia, that was once Lee's home. It overlooks the Potomac River and the National Mall in Washington, D.C. During the Civil War, the grounds of the mansion were selected as the site of Arlington National Cemetery, in part to ensure that Lee would never again be able to return to his home. The United States designated the mansion as a National Memorial to Lee in 1955, a mark of widespread respect for him in both the North and South.
In Richmond, Virginia, a large equestrian statue of Lee by French sculptor Jean Antonin Mercié is the centerpiece of the city's famous Monument Avenue, which boasts four other statues to famous Confederates. This monument to Lee was unveiled on May 29, 1890; over 100,000 people attended this dedication. Lee is also shown mounted on Traveller in Gettysburg National Military Park on top of the Virginia Monument; he is facing roughly in the direction of Pickett's Charge. Lee's portrayal on a mural on Richmond's Flood Wall on the James River, considered offensive by some, was removed in the late 1990s, but currently is back on the flood wall. Also in Virginia, the Robert Edward Lee Sculpture at Charlottesville was listed on the National Register of Historic Places in 1997.
In Baltimore's Wyman Park, a large double equestrian statue of Lee and Jackson is located directly across from the Baltimore Museum of Art. Designed by Laura Gardin Fraser and dedicated in 1948, Lee is depicted astride his horse Traveller next to Stonewall Jackson who is mounted on "Little Sorrel." Architect John Russell Pope created the base, which was dedicated on the anniversary of the eve of the Battle of Chancellorsville.
An equestrian statue of Lee is located in Robert E. Lee Park, in Dallas, Texas; and in Austin, a statue of Lee is on display at the main mall of The University of Texas at Austin. A statue of Robert E. Lee is one of two statues (the other is Washington) representing Virginia in Statuary Hall in the Capitol in Washington, D.C. Lee is one of the figures depicted in bas-relief carved into Stone Mountain near Atlanta, Georgia. Accompanying him on horseback in the relief are Stonewall Jackson and Jefferson Davis.
The birthday of Robert E. Lee is celebrated or commemorated in several states. In Virginia, Lee-Jackson Day is celebrated on the Friday preceding Martin Luther King, Jr. Day which is the third Monday in January. In Texas, he is celebrated as part of Confederate Heroes Day on January 19, Lee's birthday. In Alabama, Arkansas, and Mississippi, his birthday is celebrated on the same day as Martin Luther King, Jr. Day, while in Georgia, this occurs on the day after Thanksgiving.
One United States college and one junior college are named for Lee: Washington and Lee University in Lexington, Virginia; and Lee College in Baytown, Texas, respectively. Lee Chapel at Washington and Lee University marks Lee's final resting place. Throughout the South, many primary and secondary schools were also named for him as well as private schools such as Robert E. Lee Academy in Bishopville, South Carolina.
In 1900, Lee was one of the first 29 individuals selected for the Hall of Fame for Great Americans (the first Hall of Fame in the United States), designed by Stanford White, on the Bronx, New York, campus of New York University, now a part of Bronx Community College.
In 1862, the newly formed Confederate Navy purchased a 642-ton iron-hulled side-wheel gunboat, built in at Glasgow, Scotland, and gave her the name of CSS "Robert E. Lee" in honor of this Confederate General. During the next year, she became one of the South's most famous Confederate blockade runners, successfully making more than twenty runs through the Union blockade.
The Mississippi River steamboat "Robert E. Lee" was named for Lee after the Civil War. It was the participant in an 1870 St. Louis – New Orleans race with the "Natchez VI", which was featured in a Currier and Ives lithograph. The "Robert E. Lee" won the race. The steamboat inspired the 1912 song "Waiting for the Robert E. Lee" by Lewis F. Muir and L. Wolfe Gilbert. In more modern times, the USS "Robert E. Lee", a George Washington class ballistic missile submarine built in 1958, was named for Lee, as was the M3 Lee tank, produced in 1941 and 1942.
The Commonwealth of Virginia issues an optional license plate honoring Lee, making reference to him as 'The Virginia Gentleman'. In February 2014, a road on Fort Bliss previously named for Lee was renamed to honor Buffalo Soldiers.
In popular culture.
Lee serves as a main character in the Shaara novels "The Killer Angels", "Gods and Generals", and "The Last Full Measure", as well as the film adaptations of "The Killer Angels" and "Gods and Generals". He is played by Martin Sheen in the former and his descendent Robert Duvall in the latter. Lee is portrayed as a hero in the historical children's novel "Lee and Grant at Appomattox" by MacKinlay Kantor. He is a major character in Harry Turtledove's alternate history novel, "The Guns of the South", in which he ends up as President of a victorious Confederacy. 
His part in the Civil War is told from the perspective of his horse in Richard Adams' book Traveller.
On September 18, 1960, the American actor George Macready portrayed Lee in the episode "Johnny Yuma at Appomattox" of the ABC television series "The Rebel", starring Nick Adams in the title role.
Robert Symonds played Lee in the 1982 miniseries "The Blue and the Gray" .
In the 1986 TV series "North and South Book II" , Lee was portrayed by actor "William Schallert" .
The Dodge Charger featured in the CBS television series "The Dukes of Hazzard" was named The General Lee.

</doc>
<doc id="25742" url="http://en.wikipedia.org/wiki?curid=25742" title="Raster graphics">
Raster graphics

In computer graphics, a raster graphics image is a dot matrix data structure representing a generally rectangular grid of pixels, or points of color, viewable via a monitor, paper, or other display medium. Raster images are stored in image files with varying formats.
A bitmap, a single-bit raster, corresponds bit-for-bit with an image displayed on a screen, generally in the same format used for storage in the display's video memory, or maybe as a device-independent bitmap. A raster is technically characterized by the width and height of the image in pixels and by the number of bits per pixel (a color depth, which determines the number of colors it can represent).
The printing and prepress industries know raster graphics as contones (from "continuous tones"). The opposite to contones is "line work", usually implemented as vector graphics in digital systems.
Etymology.
The word "raster" has its origins in the latin "rastrum" (a rake), which is derived from "radere" (to scrape). It originates from the raster scan of cathode ray tube (CRT) video monitors, which paint the image line by line by magnetically steering a focused electron beam. By association, it came also to refer to a rectangular grid of pixels. The word rastrum is now used to refer to a device for drawing musical staff lines.
Applications.
Computer displays.
Most modern computers have bitmapped displays, where each on-screen pixel directly corresponds to a small number of bits in memory. The screen is refreshed simply by scanning through pixels and coloring them according to each set of bits. In other cases, a graphics processing unit is used instead. An early scanned display with raster computer graphics was invented in the late 1960s by A. Michael Noll at Bell Labs, but its patent application filed February 5, 1970 was abandoned at the Supreme Court in 1977 over the issue of the patentability of computer software.
Image storage.
Most computer images are stored in raster graphics formats or compressed variations, including GIF, JPEG, and PNG, which are popular on the World Wide Web.
Three-dimensional voxel raster graphics are employed in video games such as the Comanche series by Novalogic, and are also used in medical imaging such as MRI scanners.
Resolution.
Raster graphics are resolution dependent, meaning they cannot scale up to an arbitrary resolution without loss of apparent quality. This property contrasts with the capabilities of vector graphics, which easily scale up to the quality of the device rendering them. Raster graphics deal more practically than vector graphics with photographs and photo-realistic images, while vector graphics often serve better for typesetting or for graphic design. Modern computer-monitors typically display about 72 to 130 pixels per inch (PPI), and some modern consumer printers can resolve 2400 dots per inch (DPI) or more; determining the most appropriate image resolution for a given printer-resolution can pose difficulties, since printed output may have a greater level of detail than a viewer can discern on a monitor. Typically, a resolution of 150 to 300 PPI works well for 4-color process (CMYK) printing.
However, for printing technologies that perform color mixing through dithering (halftone) rather than through overprinting (virtually all home/office inkjet and laser printers), printer DPI and image PPI have a very different meaning, and this can be misleading. Because, through the dithering process, the printer builds a single image pixel out of several printer dots to increase color depth, the printer's DPI setting must be set far higher than the desired PPI to ensure sufficient color depth without sacrificing image resolution. Thus, for instance, printing an image at 250 PPI may actually require a printer setting of 1200 DPI.
Raster-based image editors.
Raster-based image editors, such as Painter, Photoshop, Paint.NET, MS Paint, and GIMP, revolve around editing pixels, unlike vector-based image editors, such as Xfig, CorelDRAW, Adobe Illustrator, or Inkscape, which revolve around editing lines and shapes (vectors). When an image is rendered in a raster-based image editor, the image is composed of millions of pixels. At its core, a raster image editor works by manipulating each individual pixel. Most pixel-based image editors work using the RGB color model, but some also allow the use of other color models such as the CMYK color model.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="25745" url="http://en.wikipedia.org/wiki?curid=25745" title="Rerun">
Rerun

A rerun or repeat is a rebroadcast of an episode of a radio or television program. There are two types of reruns – those that occur during a hiatus, and those that occur when a program is syndicated. Reruns can also be, as the case with more popular shows, when a show is aired outside of its timeslot (for example, in the afternoon).
In the United Kingdom, the word "repeat" refers only to a single episode; "rerun" or "rerunning" is the preferred term for an entire series/season. "Repeat" is also used to refer to programs shown less than a week after the original broadcast, before the next episode of the series.
The term "rerun" can also be used in some respects as a synonym for "reprint", the equivalent term for print items; this is especially true for print items that are part of ongoing series (such as comic strips; "Peanuts", for instance, has been in reruns since the retirement and death of creator Charles M. Schulz).
Reruns in the United States.
In the United States, most television shows from the late 1940s and early 1950s were performed live, and in many cases they were never recorded. However, television networks in the United States began making kinescope recordings of shows broadcast live from the East Coast. This allowed the show to be broadcast later for the West Coast. These kinescopes, along with pre-filmed shows, and later, videotape, paved the way for extensive reruns of syndicated television series.
During hiatus.
In the United States, currently running shows will rerun older episodes from the same season to fill the time slot with the same program. This is often done for headliner shows because the length of the year (52 weeks) is far more than the length of a pick-up (from six to 13 episodes – usually one per week) or a full season (usually from 22 to 24 episodes or weeks). Shows will tend to start rerunning episodes after the November sweeps period and usually show only reruns from mid-December until mid-January or even February sweeps (where a show will return to airing new episodes in order to increase its ratings, which will determine the cost of a commercial run during that time slot). This winter (or "mid-season") phase is also used to try out new shows that did not make it onto the fall schedule to see how they fare with the public. These series usually run six to 13 episodes. If they do well with the public, they may get a renewal for a half (13 weeks) or full season in the new fall schedule. Major shows that are already a hit with audiences will return from February sweeps until the end of the season (which sometimes ends before May sweeps) with only limited reruns used. These are more frequently referred to as "repeats," with "reruns" the term more commonly used for syndicated programs.
Television specials.
Often, if a television special such as "Peter Pan" or a network television broadcast of a classic film like "The Wizard of Oz" is especially well-received, it will be rerun from time to time. Before the VCR era, this would be the only opportunity audiences had of seeing a program more than once.
Seasonal programming such as "How the Grinch Stole Christmas", "It's A Wonderful Life" or the Charlie Brown television specials are normally re-shown once (or occasionally twice) each year, in the appropriate timeframe.
Syndication.
A television program goes into syndication when many episodes of the program are sold as a package for a large sum of money. Generally the buyer is either a cable channel or an owner of local television stations. Often, programs are not particularly profitable until they are sold for syndication. Since local television stations often need to sell more commercial airtime than network affiliates, syndicated shows are usually edited to make room for extra commercials. Often about 100 episodes (four to five seasons' worth) are required for a weekly series to be rerun in daily syndication (at least four times a week). Very popular series running more than four seasons may start daily reruns of the first seasons, while production and airings continue of the current season's episodes; until approximately the early 1980s, shows that aired in syndication while still in production had the reruns aired under an alternate name (or multiple alternate names, as was the case with "Death Valley Days") to differentiate the reruns from the first-run episodes.
Few people anticipated the long life that a popular television series would eventually see in syndication, so most performers signed contracts that limited residual payments to about six repeats. After that, the actors received nothing and the production company would keep 100% of any income until the copyright expired; many shows did not even have their copyrights renewed and others were systematically destroyed to recycle valuable film, such was the lack of awareness of the potential for revenue from them. This situation went unchanged until the mid-1970s, when contracts for new shows extended residual payments for the performers, regardless of the number of reruns, while tape recycling effectively came to an end (rapid advancements in digital video in the 1990s made preservation far more economical) and the Copyright Act of 1976 extended copyright terms to much longer lengths, eliminating the need for renewal.
Once a series is no longer performing well enough to be sold in syndication, it may still remain in "barter" syndication, in which television stations are offered the program for free in exchange for a requirement to air additional advertisements (without compensation) bundled with the free program during other shows (barter syndication is far more common, if not the norm, in radio, where only the most popular programs charge rights fees). The Program Exchange is the most prominent barter syndicator in United States television, offering mostly older series from numerous network libraries. Barter syndicated series may be seen on smaller, independent stations with small budgets or as short-term filler on larger stations; they tend not to be as widely syndicated as programs syndicated with a rights fee.
26 Classic television.
With the growing availability of cable and satellite television channels as well as over-the-air digital subchannels, combined with a growing body of available post-syndication programming, a handful of specialty channels have been built solely or primarily to run former network programming which otherwise would no longer be in syndication. Branded as "classic television", these often carry reruns of programming dating back to the monochrome television era and are promoted as nostalgia. The corresponding radio format would be that of an oldies, classic rock, classic hits or adult standards station. Depending on the programs chosen for a classic network, running the format can be very inexpensive, due to many shows beginning to fall into the public domain.
On cable and satellite, channels that devote at least some of their program schedule to post-syndication reruns include Nick at Nite, TV Land, TBS, USA Network, WGN America, TVGN, Hub Network, Game Show Network, Boomerang, Nicktoons, INSP, RFD-TV, and the Hallmark Channel. Equity Media Holdings had been using low-power television stations to carry its own Retro Television Network in various markets; those stations were, as a result of Equity going bankrupt, sold to religious broadcaster Daystar Television Network. Since the early 2010s, the growth of digital subchannel networks has allowed for increasing specialization of these classic networks: in addition to general-interest program networks such as Me-TV, Retro TV and Antenna TV, there exist networks solely for sitcoms (Laff), game shows (Buzzr TV), black-oriented programs (Bounce TV), children's programming (PBJ, qubo), true crime and court programming (Justice Network), and feature films (Movies!, getTV and This TV).
Traditionally, shows most likely to be rerun in this manner are scripted comedies and dramas. Game shows, variety shows, Saturday morning cartoons and, to a lesser extent, newsmagazines and late-night talk shows (often in edited form) have been seen less commonly in reruns. Most variants of reality television have proven to be a comparative failure in reruns, due to the competitive nature of the format and the lack of the element of surprise. Despite this, reruns of sports broadcasts have found a niche, and networks such as MSG Network, ESPN Classic and NFL Network currently have a significant portion of programming time devoted to reruns of live sportscasts.
DVD retail.
With the rise of the DVD video format, box sets featuring season or series runs of television series have become an increasingly important retail item. Some view this development as a rising new idea in the industry of reruns as an increasingly major revenue source in themselves instead of the standard business model as a draw for audiences for advertising. While there were videotape releases of television series before DVD, the format's limited content capacity, large size and reliance on mechanical winding made it impractical as a widespread retail item. Many series which continue to air first-run episodes (such as "Modern Family" and "Grey's Anatomy") may release DVD sets of the prior season between the end of that season and the beginning of the next.
Some television programs that are released on DVD (particularly those that have been out of production for several years) may not have all of the seasons released, either due to poor overall sales or prohibitive costs for obtaining rights to music used in the program; one such incidence is "Perfect Strangers", which has seldom been in wide syndication since the late 1990s primarily due to lack of demand, which had only a DVD set of the first and second seasons released due to the expensiveness of relicensing songs used in later seasons of the series that are performed by the show's two lead characters. In some cases, series whose later season releases have been held up for these reasons may have the remaining seasons made available on DVD, often after a distributor that does not hold syndication rights to the program (such as Shout! Factory) secures the rights for future DVD releases.
TV listings.
TV Guide originally used the term "rerun" to designate rebroadcast programs, but abruptly changed to "repeat" in the early 1970s.
Other TV listings services and publications, including local newspapers, would often indicate reruns as "(R)"; since the early 2000s, many listing services now only provide a notation only if an episode is new ("(N)"), with reruns getting no notation.
Repeats in the United Kingdom.
In the United Kingdom, most drama and comedy series run for shorter seasons – typically six, seven or thirteen episodes – and are then replaced by others. An exception is soap operas, which are either on all year round (for example, "EastEnders" and "Coronation Street"), or are on for a season similar to the American format.
As in the U.S., fewer new episodes are made during the summer. Until recently it was also common practice for the BBC, ITV and Channel 4 to repeat classic shows from their archives, but this has more or less dried up in favor of newer (and cheaper) formats like reality shows, except on the BBC where older BBC shows, especially sitcoms like "Dad's Army" and "Fawlty Towers", are frequently repeated.
Syndication did not exist as such in United Kingdom until the arrival of satellite, cable and later, from 1998 on, digital television, although it could be argued that many ITV programs up to the early 1990s, particularly imported programming was syndicated in the sense that each ITV region bought some programs independently of the ITV Network, and in particular many programs out of prime time made by smaller ITV stations were "part-networked" where some regions would show them and others would not. Nowadays there are many channels in the UK (for example, Gold) which repackage and rebroadcast "classic" programming from both sides of the Atlantic. Some of these channels, like their U.S. counterparts, make commercial timing cuts; others get around this by running shows in longer time slots, and critics of timing cuts see no reason why all channels should not do the same. 
Early on in the history of British television, agreements with the actors' union Equity and other trade bodies limited the number of times a single program could be broadcast, usually only twice, and these showings were limited to within a set time period such as five years. This was due to the unions' fear that the channels filling their schedules with repeats could put actors and other production staff out of work as fewer new shows would be made. It also had the unintentional side effect of causing many programs to be junked after their repeat rights had expired, as they were considered to be of no further use by the broadcasters. Although these agreements changed during the 1980s and beyond, it is still expensive to repeat archive television series on British terrestrial television, as new contracts have to be drawn up and payments made to the artists concerned. Repeats on multi-channel television are cheaper, as are re-showings of newer programs covered by less strict repeat clauses. However, programs are no longer destroyed, as the historical and cultural reasons for keeping them have now been seen, even if the programs have little or no repeat value.

</doc>
<doc id="25748" url="http://en.wikipedia.org/wiki?curid=25748" title="Router (computing)">
Router (computing)

A router is a networking device that forwards data packets between computer networks. A router is connected to two or more data lines from different networks (as opposed to a network switch, which connects data lines from one single network). When a data packet comes in on one of the lines, the router reads the address information in the packet to determine its ultimate destination. Then, using information in its routing table or routing policy, it directs the packet to the next network on its journey. This creates an overlay internetwork. Routers perform the "traffic directing" functions on the Internet. A data packet is typically forwarded from one router to another through the networks that constitute the internetwork until it reaches its destination node.
The most familiar type of routers are home and small office routers that simply pass data, such as web pages, email, IM, and videos between the home computers and the Internet. An example of a router would be the owner's cable or DSL router, which connects to the Internet through an ISP. More sophisticated routers, such as enterprise routers, connect large business or ISP networks up to the powerful core routers that forward data at high speed along the optical fiber lines of the Internet backbone. Though routers are typically dedicated hardware devices, use of software-based routers has grown increasingly common.
Applications.
When multiple routers are used in interconnected networks, the routers exchange information about destination addresses using a dynamic routing protocol. Each router builds up a table listing the preferred routes between any two systems on the interconnected networks. A router has interfaces for different physical types of network connections, such as copper cables, fibre optic, or wireless transmission. It also contains firmware for different networking communications protocol standards. Each network interface uses this specialized computer software to enable data packets to be forwarded from one protocol transmission system to another.
Routers may also be used to connect two or more logical groups of computer devices known as subnets, each with a different sub-network address. The subnet addresses recorded in the router do not necessarily map directly to the physical interface connections.
A router has two stages of operation called planes:
Routers may provide connectivity within enterprises, between enterprises and the Internet, or between internet service providers' (ISPs) networks. The largest routers (such as the Cisco CRS-1 or Juniper T1600) interconnect the various ISPs, or may be used in large enterprise networks. Smaller routers usually provide connectivity for typical home and office networks. Other networking solutions may be provided by a backbone Wireless Distribution System (WDS), which avoids the costs of introducing networking cables into buildings.
All sizes of routers may be found inside enterprises. The most powerful routers are usually found in ISPs, academic and research facilities. Large businesses may also need more powerful routers to cope with ever increasing demands of intranet data traffic. A three-layer model is in common use, not all of which need be present in smaller networks.
Access.
Access routers, including 'small office/home office' (SOHO) models, are located at customer sites such as branch offices that do not need hierarchical routing of their own. Typically, they are optimized for low cost. Some SOHO routers are capable of running alternative free Linux-based firmwares like Tomato, OpenWrt or DD-WRT.
Distribution.
Distribution routers aggregate traffic from multiple access routers, either at the same site, or to collect the data streams from multiple sites to a major enterprise location. Distribution routers are often responsible for enforcing quality of service across a WAN, so they may have considerable memory installed, multiple WAN interface connections, and substantial onboard data processing routines. They may also provide connectivity to groups of file servers or other external networks.
Security.
External networks must be carefully considered as part of the overall security strategy. A router may include a firewall, VPN handling, and other security functions, or these may be handled by separate devices. Many companies produced security-oriented routers, including Cisco Systems' PIX and ASA5500 series, Juniper's Netscreen, Watchguard's Firebox, Barracuda's variety of mail-oriented devices, and many others. Routers also commonly perform network address translation, which allows multiple devices on a network to share a single public IP address.
Core.
In enterprises, a core router may provide a "collapsed backbone" interconnecting the distribution tier routers from multiple buildings of a campus, or large enterprise locations. They tend to be optimized for high bandwidth, but lack some of the features of Edge Routers.
Internet connectivity and internal use.
Routers intended for ISP and major enterprise connectivity usually exchange routing information using the Border Gateway Protocol (BGP). RFC 4098 standard defines the types of BGP routers according to their functions:
Historical and technical information.
The very first device that had fundamentally the same functionality as a router does today, was the Interface Message Processor (IMP); IMPs were the devices that made up the ARPANET, the first packet network. The idea for a router (called "gateways" at the time) initially came about through an international group of computer networking researchers called the International Network Working Group (INWG). Set up in 1972 as an informal group to consider the technical issues involved in connecting different networks, later that year it became a subcommittee of the International Federation for Information Processing.
These devices were different from most previous packet networks in two ways. First, they connected dissimilar kinds of networks, such as serial lines and local area networks. Second, they were connectionless devices, which had no role in assuring that traffic was delivered reliably, leaving that entirely to the hosts (this particular idea had been previously pioneered in the CYCLADES network).
The idea was explored in more detail, with the intention to produce a prototype system, as part of two contemporaneous programs. One was the initial DARPA-initiated program, which created the TCP/IP architecture in use today.
The other was a program at Xerox PARC to explore new networking technologies, which produced the PARC Universal Packet system; due to corporate intellectual property concerns it received little attention outside Xerox for years.
Some time after early 1974 the first Xerox routers became operational. The first true IP router was developed by Virginia Strazisar at BBN, as part of that DARPA-initiated effort, during 1975-1976. By the end of 1976, three PDP-11-based routers were in service in the experimental prototype Internet.
The first multiprotocol routers were independently created by staff researchers at MIT and Stanford in 1981; the Stanford router was done by William Yeager, and the MIT one by Noel Chiappa; both were also based on PDP-11s.
Virtually all networking now uses TCP/IP, but multiprotocol routers are still manufactured. They were important in the early stages of the growth of computer networking, when protocols other than TCP/IP were in use. Modern Internet routers that handle both IPv4 and IPv6 are multiprotocol, but are simpler devices than routers processing AppleTalk, DECnet, IP and Xerox protocols.
From the mid-1970s and in the 1980s, general-purpose mini-computers served as routers. Modern high-speed routers are highly specialized computers with extra hardware added to speed both common routing functions, such as packet forwarding, and specialised functions such as IPsec encryption.
There is substantial use of Linux and Unix software based machines, running open source routing code, for research and other applications. Cisco's operating system was independently designed. Major router operating systems, such as those from Juniper Networks and Extreme Networks, are extensively modified versions of Unix software.
Forwarding.
For pure Internet Protocol (IP) forwarding function, a router is designed to minimize the state information associated with individual packets. The main purpose of a router is to connect multiple networks and forward packets destined either for its own networks or other networks. A router is considered a Layer 3 device because its primary forwarding decision is based on the information in the Layer 3 IP packet, specifically the destination IP address. This process is known as routing. When each router receives a packet, it searches its routing table to find the best match between the destination IP address of the packet and one of the network addresses in the routing table. Once a match is found, the packet is encapsulated in the Layer 2 data link frame for that outgoing interface. A router does not look into the actual data contents that the packet carries, but only at the layer 3 addresses to make a forwarding decision, plus optionally other information in the header for hints on, for example, quality of service (QoS). Once a packet is forwarded, the router does not retain any historical information about the packet, but the forwarding action can be collected into the statistical data, if so configured.
The routing table itself can contain information derived from a variety of sources, such as a default or static route that is configured manually, or dynamic routing protocols where the router learns routes from other routers. A default route is one that is used to route all traffic whose destination does not otherwise appear in the routing table; this is common – even necessary – in small networks, such as a home or small business where the default route simply sends all non-local traffic to the Internet service provider. The default route can be manually configured (as a static route), or learned by dynamic routing protocols, or be obtained by DHCP. (A router can serve as a DHCP client or as a DHCP server.) A router can run more than one routing protocol at a time, particularly if it serves as an autonomous system border router between parts of a network that run different routing protocols; if it does so, then may be used (usually selectively) to share information between the different protocols running on the same router.
Forwarding decisions can involve decisions at layers other than layer 3. A function that forwards based on layer 2 information is properly called a bridge. This function is referred to as layer 2 bridging, as the addresses it uses to forward the traffic are layer 2 addresses (e.g. MAC addresses on Ethernet).
Yet another function a router performs is called policy-based routing where special rules are constructed to override the rules derived from the routing table when a packet forwarding decision is made.
Besides making decision as to which interface a packet is forwarded to, which is handled primarily via the routing table, a router also has to manage congestion when packets arrive at a rate higher than the router can process. Three policies commonly used in the Internet are tail drop, random early detection (RED), and weighted random early detection (WRED). Tail drop is the simplest and most easily implemented; the router simply drops packets once the length of the queue exceeds the size of the buffers in the router. RED probabilistically drops datagrams early when the queue exceeds a pre-configured portion of the buffer, until a pre-determined max, when it becomes tail drop. WRED requires a weight on the average queue size to act upon when the traffic is about to exceed the pre-configured size, so that short bursts will not trigger random drops.
Another function a router performs is to decide which packet should be processed first when multiple queues exist. This is managed through QoS, which is critical when Voice over IP is deployed, so that delays between packets do not exceed 150ms to maintain the quality of voice conversations.
These functions may be performed through the same internal paths that the packets travel inside the router. Some of the functions may be performed through an application-specific integrated circuit (ASIC) to avoid overhead caused by multiple CPU cycles, and others may have to be performed through the CPU as these packets need special attention that cannot be handled by an ASIC.

</doc>
<doc id="25750" url="http://en.wikipedia.org/wiki?curid=25750" title="Routing">
Routing

 
Routing is the process of selecting best paths in a network. In the past, the term routing was also used to mean forwarding network traffic among networks. However this latter function is much better described as simply forwarding. Routing is performed for many kinds of networks, including the telephone network (circuit switching), electronic data networks (such as the Internet), and transportation networks. This article is concerned primarily with routing in electronic data networks using packet switching technology.
In packet switching networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.
In case of overlapping/equal routes, the following elements are considered in order to decide which routes get installed into the routing table (sorted by priority):
Routing, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.
Delivery semantics.
Routing schemes differ in their delivery semantics:
Unicast is the dominant form of message delivery on the Internet. This article focuses on unicast routing algorithms.
Topology distribution.
In static routing (or non-dynamic routing), small networks may use manually configured routing tables. Larger networks have complex topologies that can change rapidly, making the manual construction of routing tables unfeasible. Nevertheless, most of the public switched telephone network (PSTN) uses pre-computed routing tables, with fallback routes if the most direct route becomes blocked (see routing in the PSTN). Dynamic routing attempts to solve this problem by constructing routing tables automatically, based on information carried by routing protocols, allowing the network to act nearly autonomously in avoiding network failures and blockages.
Examples of dynamic-routing algorithms are the Routing Information Protocol (RIP) and the Open-Shortest-Path-First protocol (OSPF). Dynamic routing dominates the Internet. However, the configuration of the routing protocols often requires a skilled touch; networking technology has not developed to the point of the complete automation of routing.
Distance vector algorithms.
Distance vector algorithms use the Bellman–Ford algorithm. This approach assigns a "cost" number to each of the links between each node in the network. Nodes will send information from point A to point B via the path that results in the lowest "total cost" (i.e. the sum of the costs of the links between the nodes used).
The algorithm operates in a very simple manner. When a node first starts, it only knows of its immediate neighbours, and the direct cost involved in reaching them. (This information — the list of destinations, the total cost to each, and the "next hop" to send data to get there — makes up the routing table, or "distance table".) Each node, on a regular basis, sends to each neighbour node its own current assessment of the total cost to get to all the destinations it knows of. The neighbouring nodes examine this information and compare it to what they already 'know'; anything that represents an improvement on what they already have, they insert in their own routing table(s). Over time, all the nodes in the network will discover the best next hop for all destinations, and the best total cost.
When one network node goes down, any nodes that used it as their next hop discard the entry, and create new routing-table information. These nodes convey the updated routing information to all adjacent nodes, which in turn repeat the process. Eventually all the nodes in the network receive the updates, and discover new paths to all the destinations they can still "reach".
Link-state algorithms.
When applying link-state algorithms, a graphical map of the network is the fundamental data used for each node. To produce its map, each node floods the entire network with information about the other nodes it can connect to. Each node then independently assembles this information into a map. Using this map, each router independently determines the least-cost path from itself to every other node using a standard shortest paths algorithm such as Dijkstra's algorithm. The result is a tree graph rooted at the current node, such that the path through the tree from the root to any other node is the least-cost path to that node. This tree then serves to construct the routing table, which specifies the best next hop to get from the current node to any other node.
Optimised Link State Routing algorithm.
A link-state routing algorithm optimised for mobile ad hoc networks is the "Optimised Link State Routing Protocol (OLSR)". OLSR is proactive; it uses Hello and Topology Control (TC) messages to discover and disseminate link state information through the mobile ad hoc network. Using Hello messages, each node discovers 2-hop neighbor information and elects a set of "multipoint relays" (MPRs). MPRs distinguish OLSR from other link state routing protocols.
Path vector protocol.
Distance vector and link state routing are both intra-domain routing protocols. They are used inside an autonomous system, but not between autonomous systems. Both of these routing protocols become intractable in large networks and cannot be used in Inter-domain routing. Distance vector routing is subject to instability if there are more than a few hops in the domain. Link state routing needs huge amount of resources to calculate routing tables. It also creates heavy traffic due to flooding.
Path vector routing is used for inter-domain routing. It is similar to distance vector routing. In path vector routing we assume there is one node (there can be many) in each autonomous system which acts on behalf of the entire autonomous system. This node is called the speaker node. The speaker node creates a routing table and advertises it to neighboring speaker nodes in neighboring autonomous systems. The idea is the same as distance vector routing except that only speaker nodes in each autonomous system can communicate with each other. The speaker node advertises the path, not the metric, of the nodes in its autonomous system or other autonomous systems.
Path vector routing is discussed in RFC 1322; the path vector routing algorithm is somewhat similar to the distance vector algorithm in the sense that each border router advertises the destinations it can reach to its neighboring router. However, instead of advertising networks in terms of a destination and the distance to that destination, networks are advertised as destination addresses and path descriptions to reach those destinations. A route is defined as a pairing between a destination and the attributes of the path to that destination, thus the name, path vector routing, where the routers receive a vector that contains paths to a set of destinations.
The path, expressed in terms of the domains (or confederations) traversed so far, is carried in a special path attribute that records the sequence of routing domains through which the reachability information has passed.
Path selection.
Path selection involves applying a routing metric to multiple routes, in order to select (or predict) the best route.
In computer networking, the metric is computed by a routing algorithm, and can cover information such as bandwidth, network delay, hop count, path cost, load, MTU (maximum transmission unit), reliability, and communication cost (see e.g. for a list of proposed routing metrics). The routing table stores only the best possible routes, while link-state or topological databases may store all other information as well.
Because a routing metric is specific to a given routing protocol, multi-protocol routers must use some external heuristic in order to select between routes learned from different routing protocols. Cisco routers, for example, attribute a value known as the administrative distance to each route, where smaller administrative distances indicate routes learned from a supposedly more reliable protocol.
A local network administrator, in special cases, can set up host-specific routes to a particular device which provides more control over network usage, permits testing and better overall security. This can come in handy when debugging network connections or routing tables.
In some small systems, a single central device decides ahead of time the complete path of every packet.
In some other small systems, whichever edge device injects a packet into the network decides ahead of the time complete path of that particular packet.
In both of these systems, that route-planning device needs to know a lot of information about what devices are connected to the network and how they are connected to each other.
Once it has this information, it can use an algorithm such as A* search algorithm to find the best path.
In high-speed systems, there are so many packets transmitted every second that it is infeasible for a single device to calculate the complete path for each and every packet. Early high-speed systems dealt with this by setting up a circuit switching relay channel once for the first packet between some source and some destination; later packets between that same source and that same destination continue to follow the same path without recalculating until the channel teardown. Later high-speed systems inject packets into the network without any one device ever calculating a complete path for that packet -- multiple agents.
In large systems, there are so many connections between devices, and those connections change so frequently, that it is infeasible for any one device to even know how all the devices are connected to each other, much less calculate a complete path through them.
Such systems generally use next-hop routing.
Most systems use a deterministic dynamic routing algorithm:
When a device chooses a path to a particular final destination, that device will always choose the same path to that destination until that device receives information that makes it think some other path is the "best" path.
A few routing algorithms do not use a deterministic algorithm to find the "best" link for a packet to get from its original source to its final destination.
Instead, in order to avoid congestion in switched systems or network hot spots in packet systems, a few algorithms use a randomized algorithm -- Valiant's paradigm --that routes a path to a randomly-picked intermediate destination, and from there to its true final destination.
In many early telephone switches, a randomizer was often used to select the start of a path through a multistage switching fabric.
Multiple agents.
In some networks, routing is complicated by the fact that no single entity is responsible for selecting paths; instead, multiple entities are involved in selecting paths or even parts of a single path. Complications or inefficiency can result if these entities choose paths to optimize their own objectives, which may conflict with the objectives of other participants.
A classic example involves traffic in a road system, in which each driver picks a path which minimizes their own travel time. With such routing, the equilibrium routes can be longer than optimal for all drivers. In particular, Braess paradox shows that adding a new road can "lengthen" travel times for all drivers.
In another model, for example, used for routing automated guided vehicles (AGVs) on a terminal, reservations are made for each vehicle to prevent simultaneous use of the same part of an infrastructure. This approach is also referred to as context-aware routing.
The Internet is partitioned into autonomous systems (ASs) such as internet service providers (ISPs), each of which has control over routes involving its network, at multiple levels. First, AS-level paths are selected via the BGP protocol, which produces a sequence of ASs through which packets will flow. Each AS may have multiple paths, offered by neighboring ASs, from which to choose. Its decision often involves business relationships with these neighboring ASs, which may be unrelated to path quality or latency. Second, once an AS-level path has been selected, there are often multiple corresponding router-level paths, in part because two ISPs may be connected in multiple locations. In choosing the single router-level path, it is common practice for each ISP to employ hot-potato routing: sending traffic along the path that minimizes the distance through the ISP's own network—even if that path lengthens the total distance to the destination.
Consider two ISPs, "A" and "B", which each have a presence in New York, connected by a fast link with latency 5 ms; and which each have a presence in London connected by a 5 ms link. Suppose both ISPs have trans-Atlantic links connecting their two networks, but "A"'s link has latency 100 ms and B's has latency 120 ms. When routing a message from a source in "A"'s London network to a destination in "B"'s New York network, "A" may choose to immediately send the message to "B" in London. This saves "A" the work of sending it along an expensive trans-Atlantic link, but causes the message to experience latency 125 ms when the other route would have been 20 ms faster.
A 2003 measurement study of Internet routes found that, between pairs of neighboring ISPs, more than 30% of paths have inflated latency due to hot-potato routing, with 5% of paths being delayed by at least 12 ms. Inflation due to AS-level path selection, while substantial, was attributed primarily to BGP's lack of a mechanism to directly optimize for latency, rather than to selfish routing policies. It was also suggested that, were an appropriate mechanism in place, ISPs would be willing to cooperate to reduce latency rather than use hot-potato routing.
Such a mechanism was later published by the same authors, first for the case of two ISPs and then for the global case.
Route analytics.
As the Internet and IP networks become mission critical business tools, there has been increased interest in techniques and methods to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation, flapping and/or downtime. Monitoring routing in a network is achieved using route analytics tools and techniques.
Further reading.
</dl>

</doc>
<doc id="25751" url="http://en.wikipedia.org/wiki?curid=25751" title="RIP">
RIP

RIP or R.I.P. is an abbreviation of Rest in peace or (in Latin) "requiescat in pace", often used in epitaphs.
It may also refer to:

</doc>
<doc id="25754" url="http://en.wikipedia.org/wiki?curid=25754" title="Resistor">
Resistor

A resistor is a passive two-terminal electrical component that implements electrical resistance as a circuit element. Resistors act to reduce current flow, and, at the same time, act to lower voltage levels within circuits. In electronic circuits resistors are used to limit current flow, to adjust signal levels, bias active elements, terminate transmission lines among other uses. High-power resistors that can dissipate many watts of electrical power as heat may be used as part of motor controls, in power distribution systems, or as test loads for generators. 
Fixed resistors have resistances that only change slightly with temperature, time or operating voltage. Variable resistors can be used to adjust circuit elements (such as a volume control or a lamp dimmer), or as sensing devices for heat, light, humidity, force, or chemical activity.
Resistors are common elements of electrical networks and electronic circuits and are ubiquitous in electronic equipment. Practical resistors as discrete components can be composed of various compounds and forms. Resistors are also implemented within integrated circuits.
The electrical function of a resistor is specified by its resistance: common commercial resistors are manufactured over a range of more than nine orders of magnitude. The nominal value of the resistance will fall within a manufacturing tolerance.
Electronic symbols and notation.
Two typical schematic diagram symbols are as follows;
The notation to state a resistor's value in a circuit diagram varies, too. The European notation BS 1852 avoids using a decimal separator, and replaces the decimal separator with the SI prefix symbol for the particular value. For example, "8k2" in a circuit diagram indicates a resistor value of 8.2 kΩ. Additional zeros imply tighter tolerance, for example "15M0". When the value can be expressed without the need for an SI prefix, an 'R' is used instead of the decimal separator. For example, "1R2" indicates 1.2 Ω, and "18R" indicates 18 Ω. The use of a SI prefix symbol or the letter 'R' circumvents the problem that decimal separators tend to 'disappear' when photocopying a printed circuit diagram.
Theory of operation.
Ohm's law.
The behavior of an ideal resistor is dictated by the relationship specified by Ohm's law:
Ohm's law states that the voltage (V) across a resistor is proportional to the current (I), where the constant of proportionality is the resistance (R). For example, if a 300 ohm resistor is attached across the terminals of a 12 volt battery, then a current of 12 / 300 = 0.04 amperes flows through that resistor.
Practical resistors also have some inductance and capacitance which will also affect the relation between voltage and current in alternating current circuits.
The ohm (symbol: Ω) is the SI unit of electrical resistance, named after Georg Simon Ohm. An ohm is equivalent to a volt per ampere. Since resistors are specified and manufactured over a very large range of values, the derived units of milliohm (1 mΩ = 10−3 Ω), kilohm (1 kΩ = 103 Ω), and megohm (1 MΩ = 106 Ω) are also in common usage.
Series and parallel resistors.
The total resistance of resistors connected in series is the sum of their individual resistance values. 
The total resistance of resistors connected in parallel is the reciprocal of the sum of the reciprocals of the individual resistors. 
So, for example, a 10 ohm resistor connected in parallel with a 5 ohm resistor and a 15 ohm resistor will produce the inverse of 1/10+1/5+1/15 ohms of resistance, or 1/(.1+.2+.067)=2.725 ohms.
A resistor network that is a combination of parallel and series connections can be broken up into smaller parts that are either one or the other. Some complex networks of resistors cannot be resolved in this manner, requiring more sophisticated circuit analysis. Generally, the Y-Δ transform, or matrix methods can be used to solve such problems.
Power dissipation.
At any instant of time, the power "P" (watts) consumed by a resistor of resistance "R" (ohms) is calculated as:
formula_4
where "V" (volts) is the voltage across the resistor and "I" (amps) is the current flowing through it. Using Ohm's law, the two other forms can be derived. This power is converted into heat which must be dissipated by the resistor's package before its temperature rises excessively.
Resistors are rated according to their maximum power dissipation. Most discrete resistors in solid-state electronic systems absorb much less than a watt of electrical power and require no attention to their power rating. Such resistors in their discrete form, including most of the packages detailed below, are typically rated as 1/10, 1/8, or 1/4 watt.
Resistors required to dissipate substantial amounts of power, particularly used in power supplies, power conversion circuits, and power amplifiers, are generally referred to as "power resistors"; this designation is loosely applied to resistors with power ratings of 1 watt or greater. Power resistors are physically larger and may not use the preferred values, color codes, and external packages described below.
If the average power dissipated by a resistor is more than its power rating, damage to the resistor may occur, permanently altering its resistance; this is distinct from the reversible change in resistance due to its temperature coefficient when it warms. Excessive power dissipation may raise the temperature of the resistor to a point where it can burn the circuit board or adjacent components, or even cause a fire. There are flameproof resistors that fail (open circuit) before they overheat dangerously.
Since poor air circulation, high altitude, or high operating temperatures may occur, resistors may be specified with higher rated dissipation than will be experienced in service.
All resistors have a maximum voltage rating; this may limit the power dissipation for higher resistance values.
Nonideal properties.
Practical resistors have a series inductance and a small parallel capacitance; these specifications can be important in high-frequency applications. In a low-noise amplifier or pre-amp, the noise characteristics of a resistor may be an issue.
The temperature coefficient of the resistance may also be of concern in some precision applications.
The unwanted inductance, excess noise, and temperature coefficient are mainly dependent on the technology used in manufacturing the resistor. They are not normally specified individually for a particular family of resistors manufactured using a particular technology. A family of discrete resistors is also characterized according to its form factor, that is, the size of the device and the position of its leads (or terminals) which is relevant in the practical manufacturing of circuits using them.
Practical resistors are also specified as having a maximum power rating which must exceed the anticipated power dissipation of that resistor in a particular circuit: this is mainly of concern in power electronics applications.
Resistors with higher power ratings are physically larger and may require heat sinks. In a high-voltage circuit, attention must sometimes be paid to the rated maximum working voltage of the resistor. While there is no minimum working voltage for a given resistor, failure to account for a resistor's maximum rating may cause the resistor to incinerate when current is run through it.
Fixed resistor.
Lead arrangements.
Through-hole components typically have "leads" (pronounced to rhyme with "reeds") leaving the body "axially," that is, on a line parallel with the part's longest axis. Others have leads coming off their body "radially" instead. Other components may be SMT (surface mount technology), while high power resistors may have one of their leads designed into the heat sink.
Carbon composition.
Carbon composition resistors consist of a solid cylindrical resistive element with embedded wire leads or metal end caps to which the lead wires are attached. The body of the resistor is protected with paint or plastic. Early 20th-century carbon composition resistors had uninsulated bodies; the lead wires were wrapped around the ends of the resistance element rod and soldered. The completed resistor was painted for color-coding of its value.
The resistive element is made from a mixture of finely ground (powdered) carbon and an insulating material (usually ceramic). A resin holds the mixture together. The resistance is determined by the ratio of the fill material (the powdered ceramic) to the carbon. Higher concentrations of carbon— a good conductor— result in lower resistance. Carbon composition resistors were commonly used in the 1960s and earlier, but are not so popular for general use now as other types have better specifications, such as tolerance, voltage dependence, and stress (carbon composition resistors will change value when stressed with over-voltages). Moreover, if internal moisture content (from exposure for some length of time to a humid environment) is significant, soldering heat will create a non-reversible change in resistance value. Carbon composition resistors have poor stability with time and were consequently factory sorted to, at best, only 5% tolerance.
These resistors, however, if never subjected to overvoltage nor overheating were remarkably reliable considering the component's size.
Carbon composition resistors are still available, but comparatively quite costly. Values ranged from fractions of an ohm to 22 megohms. Due to their high price, these resistors are no longer used in most applications. However, they are used in power supplies and welding controls.
Carbon pile.
A carbon pile resistor is made of a stack of carbon disks compressed between two metal contact plates. Adjusting the clamping pressure changes the resistance between the plates. These resistors are used when an adjustable load is required, for example in testing automotive batteries or radio transmitters. A carbon pile resistor can also be used as a speed control for small motors in household appliances (sewing machines, hand-held mixers) with ratings up to a few hundred watts. A carbon pile resistor can be incorporated in automatic voltage regulators for generators, where the carbon pile controls the field current to maintain relatively constant voltage. The principle is also applied in the carbon microphone.
Carbon film.
A carbon film is deposited on an insulating substrate, and a helix is cut in it to create a long, narrow resistive path. Varying shapes, coupled with the resistivity of amorphous carbon (ranging from 500 to 800 μΩ m), can provide a wide range of resistance values. Compared to carbon composition they feature low noise, because of the precise distribution of the pure graphite without binding. Carbon film resistors feature a power rating range of 0.125 W to 5 W at 70 °C. Resistances available range from 1 ohm to 10 megohm. The carbon film resistor has an operating temperature range of −55 °C to 155 °C. It has 200 to 600 volts maximum working voltage range. Special carbon film resistors are used in applications requiring high pulse stability.
Printed carbon resistor.
Carbon composition resistors can be printed directly onto printed circuit board (PCB) substrates as part of the PCB manufacturing process. Although this technique is more common on hybrid PCB modules, it can also be used on standard fibreglass PCBs. Tolerances are typically quite large, and can be in the order of 30%. A typical application would be non-critical pull-up resistors.
Thick and thin film.
Thick film resistors became popular during the 1970s, and most SMD (surface mount device) resistors today are of this type. The resistive element of thick films is 1000 times thicker than thin films, but the principal difference is how the film is applied to the cylinder (axial resistors) or the surface (SMD resistors).
Thin film resistors are made by sputtering (a method of vacuum deposition) the resistive material onto an insulating substrate. The film is then etched in a similar manner to the old (subtractive) process for making printed circuit boards; that is, the surface is coated with a photo-sensitive material, then covered by a pattern film, irradiated with ultraviolet light, and then the exposed photo-sensitive coating is developed, and underlying thin film is etched away.
Thick film resistors are manufactured using screen and stencil printing processes.
Because the time during which the sputtering is performed can be controlled, the thickness of the thin film can be accurately controlled. The type of material is also usually different consisting of one or more ceramic (cermet) conductors such as tantalum nitride (TaN), ruthenium oxide (RuO2), lead oxide (PbO), bismuth ruthenate (Bi2Ru2O7), nickel chromium (NiCr), or bismuth iridate (Bi2Ir2O7).
The resistance of both thin and thick film resistors after manufacture is not highly accurate; they are usually trimmed to an accurate value by abrasive or laser trimming. Thin film resistors are usually specified with tolerances of 0.1, 0.2, 0.5, or 1%, and with temperature coefficients of 5 to 25 ppm/K. They also have much lower noise levels, on the level of 10–100 times less than thick film resistors.
Thick film resistors may use the same conductive ceramics, but they are mixed with sintered (powdered) glass and a carrier liquid so that the composite can be screen-printed. This composite of glass and conductive ceramic (cermet) material is then fused (baked) in an oven at about 850 °C.
Thick film resistors, when first manufactured, had tolerances of 5%, but standard tolerances have improved to 2% or 1% in the last few decades. Temperature coefficients of thick film resistors are high, typically ±200 or ±250 ppm/K; a 40 kelvin (70 °F) temperature change can change the resistance by 1%.
Thin film resistors are usually far more expensive than thick film resistors. For example, SMD thin film resistors, with 0.5% tolerances, and with 25 ppm/K temperature coefficients, when bought in full size reel quantities, are about twice the cost of 1%, 250 ppm/K thick film resistors.
Metal film.
A common type of axial-leaded resistor today is the metal-film resistor. Metal Electrode Leadless Face (MELF) resistors often use the same technology, and are also cylindrically shaped but are designed for surface mounting. Note that other types of resistors (e.g., carbon composition) are also available in MELF packages.
Metal film resistors are usually coated with nickel chromium (NiCr), but might be coated with any of the cermet materials listed above for thin film resistors. Unlike thin film resistors, the material may be applied using different techniques than sputtering (though this is one of the techniques). Also, unlike thin-film resistors, the resistance value is determined by cutting a helix through the coating rather than by etching. (This is similar to the way carbon resistors are made.) The result is a reasonable tolerance (0.5%, 1%, or 2%) and a temperature coefficient that is generally between 50 and 100 ppm/K. Metal film resistors possess good noise characteristics and low non-linearity due to a low voltage coefficient. Also beneficial are their tight tolerance, low temperature coefficient and long-term stability.
Metal oxide film.
Metal-oxide film resistors are made of metal oxides such as tin oxide. This results in a higher operating temperature and greater stability/reliability than Metal film. They are used in applications with high endurance demands.
Wire wound.
Wirewound resistors are commonly made by winding a metal wire, usually nichrome, around a ceramic, plastic, or fiberglass core. The ends of the wire are soldered or welded to two caps or rings, attached to the ends of the core. The assembly is protected with a layer of paint, molded plastic, or an enamel coating baked at high temperature. These resistors are designed to withstand unusually high temperatures of up to 450 °C. Wire leads in low power wirewound resistors are usually between 0.6 and 0.8 mm in diameter and tinned for ease of soldering. For higher power wirewound resistors, either a ceramic outer case or an aluminum outer case on top of an insulating layer is used – if the outer case is ceramic, such resistors are sometimes described as "cement" resistors, though they do not actually contain any traditional cement. The aluminum-cased types are designed to be attached to a heat sink to dissipate the heat; the rated power is dependent on being used with a suitable heat sink, e.g., a 50 W power rated resistor will overheat at a fraction of the power dissipation if not used with a heat sink. Large wirewound resistors may be rated for 1,000 watts or more.
Because wirewound resistors are coils they have more undesirable inductance than other types of resistor, although winding the wire in sections with alternately reversed direction can minimize inductance. Other techniques employ bifilar winding, or a flat thin former (to reduce cross-section area of the coil). For the most demanding circuits, resistors with Ayrton-Perry winding are used.
Applications of wirewound resistors are similar to those of composition resistors with the exception of the high frequency. The high frequency response of wirewound resistors is substantially worse than that of a composition resistor.
Foil resistor.
The primary resistance element of a foil resistor is a special alloy foil several micrometers thick. Since their introduction in the 1960s, foil resistors have had the best precision and stability of any resistor available. One of the important parameters influencing stability is the temperature coefficient of resistance (TCR). The TCR of foil resistors is extremely low, and has been further improved over the years.
One range of ultra-precision foil resistors offers a TCR of 0.14 ppm/°C, tolerance ±0.005%, long-term stability (1 year) 25 ppm, (3 years) 50 ppm (further improved 5-fold by hermetic sealing), stability under load (2000 hours) 0.03%, thermal EMF 0.1 μV/°C, noise −42 dB, voltage coefficient 0.1 ppm/V, inductance 0.08 μH, capacitance 0.5 pF.
Ammeter shunts.
An ammeter shunt is a special type of current-sensing resistor, having four terminals and a value in milliohms or even micro-ohms. Current-measuring instruments, by themselves, can usually accept only limited currents. To measure high currents, the current passes through the shunt across which the voltage drop is measured and interpreted as current. A typical shunt consists of two solid metal blocks, sometimes brass, mounted on an insulating base. Between the blocks, and soldered or brazed to them, are one or more strips of low temperature coefficient of resistance (TCR) manganin alloy. Large bolts threaded into the blocks make the current connections, while much smaller screws provide volt meter connections. Shunts are rated by full-scale current, and often have a voltage drop of 50 mV at rated current. Such meters are adapted to the shunt full current rating by using an appropriately marked dial face; no change need to be made to the other parts of the meter.
Grid resistor.
In heavy-duty industrial high-current applications, a grid resistor is a large convection-cooled lattice of stamped metal alloy strips connected in rows between two electrodes. Such industrial grade resistors can be as large as a refrigerator; some designs can handle over 500 amperes of current, with a range of resistances extending lower than 0.04 ohms. They are used in applications such as dynamic braking and load banking for locomotives and trams, neutral grounding for industrial AC distribution, control loads for cranes and heavy equipment, load testing of generators and harmonic filtering for electric substations.
The term "grid resistor" is sometimes used to describe a resistor of any type connected to the control grid of a vacuum tube. This is not a resistor technology; it is an electronic circuit topology.
Variable resistors.
Adjustable resistors.
A resistor may have one or more fixed tapping points so that the resistance can be changed by moving the connecting wires to different terminals. Some wirewound power resistors have a tapping point that can slide along the resistance element, allowing a larger or smaller part of the resistance to be used.
Where continuous adjustment of the resistance value during operation of equipment is required, the sliding resistance tap can be connected to a knob accessible to an operator. Such a device is called a rheostat and has two terminals.
Potentiometers.
A common element in electronic devices is a three-terminal resistor with a continuously adjustable tapping point controlled by rotation of a shaft or knob. These variable resistors are known as potentiometers when all three terminals are present, since they act as a continuously adjustable voltage divider. A common example is a volume control for a radio receiver.
Accurate, high-resolution panel-mounted potentiometers (or "pots") have resistance elements typically wirewound on a helical mandrel, although some include a conductive-plastic resistance coating over the wire to improve resolution. These typically offer ten turns of their shafts to cover their full range. They are usually set with dials that include a simple turns counter and a graduated dial. Electronic analog computers used them in quantity for setting coefficients, and delayed-sweep oscilloscopes of recent decades included one on their panels.
Resistance decade boxes.
A resistance decade box or resistor substitution box is a unit containing resistors of many values, with one or more mechanical switches which allow any one of various discrete resistances offered by the box to be dialed in. Usually the resistance is accurate to high precision, ranging from laboratory/calibration grade accuracy of 20 parts per million, to field grade at 1%. Inexpensive boxes with lesser accuracy are also available. All types offer a convenient way of selecting and quickly changing a resistance in laboratory, experimental and development work without needing to attach resistors one by one, or even stock each value. The range of resistance provided, the maximum resolution, and the accuracy characterize the box. For example, one box offers resistances from 0 to 100 megohms, maximum resolution 0.1 ohm, accuracy 0.1%.
Special devices.
There are various devices whose resistance changes with various quantities. The resistance of NTC thermistors exhibit a strong negative temperature coefficient, making them useful for measuring temperatures. Since their resistance can be large until they are allowed to heat up due to the passage of current, they are also commonly used to prevent excessive current surges when equipment is powered on. Similarly, the resistance of a humistor varies with humidity. One sort of photodetector, the photoresistor, has a resistance which varies with illumination.
The strain gauge, invented by Edward E. Simmons and Arthur C. Ruge in 1938, is a type of resistor that changes value with applied strain. A single resistor may be used, or a pair (half bridge), or four resistors connected in a Wheatstone bridge configuration. The strain resistor is bonded with adhesive to an object that will be subjected to mechanical strain. With the strain gauge and a filter, amplifier, and analog/digital converter, the strain on an object can be measured.
A related but more recent invention uses a Quantum Tunnelling Composite to sense mechanical stress. It passes a current whose magnitude can vary by a factor of 1012 in response to changes in applied pressure.
Measurement.
The value of a resistor can be measured with an ohmmeter, which may be one function of a multimeter. Usually, probes on the ends of test leads connect to the resistor. A simple ohmmeter may apply a voltage from a battery across the unknown resistor (with an internal resistor of a known value in series) producing a current which drives a meter movement. The current, in accordance with Ohm's law, is inversely proportional to the sum of the internal resistance and the resistor being tested, resulting in an analog meter scale which is very non-linear, calibrated from infinity to 0 ohms. A digital multimeter, using active electronics, may instead pass a specified current through the test resistance. The voltage generated across the test resistance in that case is linearly proportional to its resistance, which is measured and displayed. In either case the low-resistance ranges of the meter pass much more current through the test leads than do high-resistance ranges, in order for the voltages present to be at reasonable levels (generally below 10 volts) but still measurable.
Measuring low-value resistors, such as fractional-ohm resistors, with acceptable accuracy requires four-terminal connections. One pair of terminals applies a known, calibrated current to the resistor, while the other pair senses the voltage drop across the resistor. Some laboratory quality ohmmeters, especially milliohmmeters, and even some of the better digital multimeters sense using four input terminals for this purpose, which may be used with special test leads. Each of the two so-called Kelvin clips has a pair of jaws insulated from each other. One side of each clip applies the measuring current, while the other connections are only to sense the voltage drop. The resistance is again calculated using Ohm's Law as the measured voltage divided by the applied current.
Standards.
Production resistors.
Resistor characteristics are quantified and reported using various national standards. In the US, MIL-STD-202 contains the relevant test methods to which other standards refer.
There are various standards specifying properties of resistors for use in equipment:
There are other United States military procurement MIL-R- standards.
Resistance standards.
The primary standard for resistance, the "mercury ohm" was initially defined in 1884 in as a column of mercury 106.3 cm long and 1 square millimeter in cross-section, at 0 degrees Celsius. Difficulties in precisely measuring the physical constants to replicate this standard result in variations of as much as 30 ppm. From 1900 the mercury ohm was replaced with a precision machined plate of manganin. Since 1990 the international resistance standard has been based on the quantized Hall effect discovered by Klaus von Klitzing, for which he won the Nobel Prize in Physics in 1985.
Resistors of extremely high precision are manufactured for calibration and laboratory use. They may have four terminals, using one pair to carry an operating current and the other pair to measure the voltage drop; this eliminates errors caused by voltage drops across the lead resistances, because no charge flows through voltage sensing leads. It is important in small value resistors (100–0.0001 ohm) where lead resistance is significant or even comparable with respect to resistance standard value.
Resistor marking.
Most axial resistors use a pattern of colored stripes to indicate resistance, which also indicate tolerance, and may also be extended to show temperature coefficient and reliability class. Cases are usually tan, brown, blue, or green, though other colors are occasionally found such as dark red or dark gray. The power rating is not usually marked and is deduced from the size.
The color bands of the carbon resistors can be three, four, five or, six bands. The first two bands represent first two digits to measure their value in ohms. The third band of a three- or four-banded resistor represents multiplier; a fourth band denotes tolerance (which if absent, denotes ±20%). For five and six color-banded resistors, the third band is a third digit, fourth band multiplier and fifth is tolerance. The sixth band represents temperature co-efficient in a six-banded resistor.
Surface-mount resistors are marked numerically, if they are big enough to permit marking; more-recent small sizes are impractical to mark.
Early 20th century resistors, essentially uninsulated, were dipped in paint to cover their entire body for color-coding. A second color of paint was applied to one end of the element, and a color dot (or band) in the middle provided the third digit. The rule was "body, tip, dot", providing two significant digits for value and the decimal multiplier, in that sequence. Default tolerance was ±20%. Closer-tolerance resistors had silver (±10%) or gold-colored (±5%) paint on the other end.
Preferred values.
Early resistors were made in more or less arbitrary round numbers; a series might have 100, 125, 150, 200, 300, etc. Resistors as manufactured are subject to a certain percentage tolerance, and it makes sense to manufacture values that correlate with the tolerance, so that the actual value of a resistor overlaps slightly with its neighbors. Wider spacing leaves gaps; narrower spacing increases manufacturing and inventory costs to provide resistors that are more or less interchangeable.
A logical scheme is to produce resistors in a range of values which increase in a geometric progression, so that each value is greater than its predecessor by a fixed multiplier or percentage, chosen to match the tolerance of the range. For example, for a tolerance of ±20% it makes sense to have each resistor about 1.5 times its predecessor, covering a decade in 6 values. In practice the factor used is 1.4678, giving values of 1.47, 2.15, 3.16, 4.64, 6.81, 10 for the 1–10-decade (a decade is a range increasing by a factor of 10; 0.1–1 and 10–100 are other examples); these are rounded in practice to 1.5, 2.2, 3.3, 4.7, 6.8, 10; followed, by 15, 22, 33, … and preceded by … 0.47, 0.68, 1. This scheme has been adopted as the E6 series of the IEC 60063 preferred number values. There are also E12, E24, E48, E96 and E192 series for components of progressively finer resolution, with 12, 24, 96, and 192 different values within each decade. The actual values used are in the IEC 60063 lists of preferred numbers.
A resistor of 100 ohms ±20% would be expected to have a value between 80 and 120 ohms; its E6 neighbors are 68 (54–82) and 150 (120–180) ohms. A sensible spacing, E6 is used for ±20% components; E12 for ±10%; E24 for ±5%; E48 for ±2%, E96 for ±1%; E192 for ±0.5% or better. Resistors are manufactured in values from a few milliohms to about a gigaohm in IEC60063 ranges appropriate for their tolerance. Manufacturers may sort resistors into tolerance-classes based on measurement. Accordingly a selection of 100 ohms resistors with a tolerance of ±10%, might not lie just around 100 ohm (but no more than 10% off) as one would expect (a bell-curve), but rather be in two groups – either between 5 to 10% too high or 5 to 10% too low (but not closer to 100 ohm than that) because any resistors the factory had measured as being less than 5% off would have been marked and sold as resistors with only ±5% tolerance or better. When designing a circuit, this may become a consideration.
Earlier power wirewound resistors, such as brown vitreous-enameled types, however, were made with a different system of preferred values, such as some of those mentioned in the first sentence of this section.
SMT resistors.
Surface mounted resistors are printed with numerical values in a code related to that used on axial resistors. Standard-tolerance surface-mount technology (SMT) resistors are marked with a three-digit code, in which the first two digits are the first two significant digits of the value and the third digit is the power of ten (the number of zeroes). For example:
Resistances less than 100 ohms are written: 100, 220, 470. The final zero represents ten to the power zero, which is 1. For example:
Sometimes these values are marked as "10" or "22" to prevent a mistake.
Resistances less than 10 ohms have 'R' to indicate the position of the decimal point (radix point). For example:
Precision resistors are marked with a four-digit code, in which the first three digits are the significant figures and the fourth is the power of ten. For example:
"000" and "0000" sometimes appear as values on surface-mount zero-ohm links, since these have (approximately) zero resistance.
More recent surface-mount resistors are too small, physically, to permit practical markings to be applied.
Industrial type designation.
Format:" [two letters]<space>[resistance value (three digit)]<nospace>[tolerance code(numerical – one digit)]
Electrical and thermal noise.
In amplifying faint signals, it is often necessary to minimize electronic noise, particularly in the first stage of amplification. As a dissipative element, even an ideal resistor will naturally produce a randomly fluctuating voltage or "noise" across its terminals. This Johnson–Nyquist noise is a fundamental noise source which depends only upon the temperature and resistance of the resistor, and is predicted by the fluctuation–dissipation theorem. Using a larger value of resistance produces a larger voltage noise, whereas with a smaller value of resistance there will be more current noise, at a given temperature.
The thermal noise of a practical resistor may also be larger than the theoretical prediction and that increase is typically frequency-dependent. Excess noise of a practical resistor is observed only when current flows through it. This is specified in unit of μV/V/decade – μV of noise per volt applied across the resistor per decade of frequency. The μV/V/decade value is frequently given in dB so that a resistor with a noise index of 0 dB will exhibit 1 μV (rms) of excess noise for each volt across the resistor in each frequency decade. Excess noise is thus an example of 1/"f" noise. Thick-film and carbon composition resistors generate more excess noise than other types at low frequencies. Wire-wound and thin-film resistors are often used for their better noise characteristics. Carbon composition resistors can exhibit a noise index of 0 dB while bulk metal foil resistors may have a noise index of −40 dB, usually making the excess noise of metal foil resistors insignificant. Thin film surface mount resistors typically have lower noise and better thermal stability than thick film surface mount resistors. Excess noise is also size-dependent: in general excess noise is reduced as the physical size of a resistor is increased (or multiple resistors are used in parallel), as the independently fluctuating resistances of smaller components will tend to average out.
While not an example of "noise" per se, a resistor may act as a thermocouple, producing a small DC voltage differential across it due to the thermoelectric effect if its ends are at different temperatures. This induced DC voltage can degrade the precision of instrumentation amplifiers in particular. Such voltages appear in the junctions of the resistor leads with the circuit board and with the resistor body. Common metal film resistors show such an effect at a magnitude of about 20 µV/°C. Some carbon composition resistors can exhibit thermoelectric offsets as high as 400 µV/°C, whereas specially constructed resistors can reduce this number to 0.05 µV/°C. In applications where the thermoelectric effect may become important, care has to be taken to mount the resistors horizontally to avoid temperature gradients and to mind the air flow over the board.
Failure modes.
The failure rate of resistors in a properly designed circuit is low compared to other electronic components such as semiconductors and electrolytic capacitors. Damage to resistors most often occurs due to overheating when the average power delivered to it (as computed above) greatly exceeds its ability to dissipate heat (specified by the resistor's "power rating"). This may be due to a fault external to the circuit, but is frequently caused by the failure of another component (such as a transistor that shorts out) in the circuit connected to the resistor. Operating a resistor too close to its power rating can limit the resistor's lifespan or cause a significant change in its resistance. A safe design generally uses overrated resistors in power applications to avoid this danger.
Low-power thin-film resistors can be damaged by long-term high-voltage stress, even below maximum specified voltage and below maximum power rating. This is often the case for the startup resistors feeding the SMPS integrated circuit.
When overheated, carbon-film resistors may decrease or increase in resistance.
Carbon film and composition resistors can fail (open circuit) if running close to their maximum dissipation. This is also possible but less likely with metal film and wirewound resistors.
There can also be failure of resistors due to mechanical stress and adverse environmental factors including humidity. If not enclosed, wirewound resistors can corrode.
Surface mount resistors have been known to fail due to the ingress of sulfur into the internal makeup of the resistor. This sulfur chemically reacts with the silver layer to produce non-conductive silver sulfide. The resistor's impedance goes to infinity. Sulfur resistant and anti-corrosive resistors are sold into automotive, industrial, and military applications. ASTM B809 is an industry standard that tests a part's susceptibility to sulfur.
An alternative failure mode can be encountered where large value resistors are used (hundreds of kilohms and higher). Resistors are not only specified with a maximum power dissipation, but also for a maximum voltage drop. Exceeding this voltage will cause the resistor to degrade slowly reducing in resistance. The voltage dropped across large value resistors can be exceeded before the power dissipation reaches its limiting value. Since the maximum voltage specified for commonly encountered resistors is a few hundred volts, this is a problem only in applications where these voltages are encountered.
Variable resistors can also degrade in a different manner, typically involving poor contact between the wiper and the body of the resistance. This may be due to dirt or corrosion and is typically perceived as "crackling" as the contact resistance fluctuates; this is especially noticed as the device is adjusted. This is similar to crackling caused by poor contact in switches, and like switches, potentiometers are to some extent self-cleaning: running the wiper across the resistance may improve the contact. Potentiometers which are seldom adjusted, especially in dirty or harsh environments, are most likely to develop this problem. When self-cleaning of the contact is insufficient, improvement can usually be obtained through the use of contact cleaner (also known as "tuner cleaner") spray. The crackling noise associated with turning the shaft of a dirty potentiometer in an audio circuit (such as the volume control) is greatly accentuated when an undesired DC voltage is present, often indicating the failure of a DC blocking capacitor in the circuit.

</doc>
<doc id="25755" url="http://en.wikipedia.org/wiki?curid=25755" title="Republicanism">
Republicanism

Republicanism is the ideology of governing a society or state as a republic (la. "res publica"), where the head of state is a representative of the people who hold popular sovereignty rather than the people being subjects of the head of state. The head of state is typically appointed by means other than heredity, often through elections.
The exact meaning of republicanism varies depending on the cultural and historical context. In general it implies the absence of monarchy, but it may indicate anything from 'rule by many people and by law', through oligarchy, to arbitrary rule by one person. Republicanism existed as an identifiable movement in the Roman Republic, where the founder of the Republic, Lucius Junius Brutus, denounced the former Roman Kingdom and had the Roman people declare a solemn oath never to allow a monarchy to return again.
Historical Development of Republicanism.
Classical antecedents.
Ancient Greece.
In Ancient Greece, several philosophers and historians analysed and described elements we now recognize as classical republicanism. Some scholars have translated the Greek concept of "politeia" as "republic," but most modern scholars reject this idea. There is no single written expression or definition from this era that exactly corresponds with a modern understanding of the term "republic." However, most of the essential features of the modern definition are present in the works of Plato, Aristotle, and Polybius. These include theories of mixed government and of civic virtue. For example, Plato's dialogue on the ideal state, "The Republic", (although misnamed by the standards of modern political theory) places great emphasis on the importance of civic virtue (aiming for the good of the whole city) together with personal virtue ('just man') on the part of the ideal rulers, modeled after the character of his teacher, Socrates. Indeed in the famous passage in Book V, Plato asserts that until rulers have the nature of philosophers (Socrates) or philosophers become the rulers, there will be no civic peace or happiness.
A number of Ancient Greek states such as Athens and Sparta have been classified as "classical republics", because they featured extensive participation by the citizens in legislation and political decision-making. Aristotle considered Carthage to have been a republic as it had a political system similar to that of some of the Greek cities, notably Sparta, but avoided some of the defects that affected them (e.g., the Spartan common meal without any state subsidy, which undermined the ostensible purpose of the practice).
Ancient Rome.
Both Livy (in Latin, living in Augustus' time) and Plutarch (in Greek, a century later), described how Rome had developed its legislation, notably the transition from a "kingdom" to a "republic", by following the example of the Greeks. Some of this history, composed more than 500 years after the events, with scant written sources to rely on, may be fictitious reconstruction. Nonetheless, the influence of Greek ideas on governance is evident in the organisation of the Roman Republic.
The Greek historian Polybius, writing more than a 50 centuries before Livy, became one of the first to describe the importance of the emergence of the Roman Republic. Polybius exerted a great influence on Cicero as he wrote his politico-philosophical works in the 1st century BC. In one of these works, "De re publica", Cicero linked the Roman concept of "res publica" to the Greek "politeia".
However, the modern term "republic", despite its derivation, is not synonymous with the Roman "res publica". Among the several meanings of the term "res publica", it is most often translated "republic" where the Latin expression refers to the Roman state, and its form of government, between the era of the Kings and the era of the Emperors. This Roman Republic would, by a modern understanding of the word, still be defined as a true republic, even if not coinciding entirely. Thus, Enlightenment philosophers saw the Roman Republic as an ideal system, because it included features like a systematic separation of powers.
Romans still called their state "Res Publica" in the era of the early emperors because, on the surface, the organization of the state had been preserved by the first emperors without significant alteration. Several offices from the republican era, held by individuals, were combined under the control of a single person. These changes became permanent, and gradually conferred sovereignty on the Emperor.
Cicero's description of the ideal state, in "De re publica", does not equate to a modern day "republic"; it is more like enlightened absolutism. His philosophical works were influential when Enlightenment philosophers such as Voltaire developed their political concepts.
In its classical meaning, a republic was any stable well-governed political community. Both Plato and Aristotle identified three forms of government: democracy, aristocracy, and monarchy. However, mixed government was considered ideal. First Plato and Aristotle, and then Polybius and Cicero, developed the notion that the ideal republic is a mixture of these three forms of government. The writers of the Renaissance embraced this notion.
Cicero expressed reservations concerning the republican form of government. While in his "theoretical" works he defended monarchy, or at least a mixed monarchy/oligarchy, in his own political life, he generally opposed men, like Julius Caesar, Mark Antony and Octavian, who were trying to realise such ideals. Eventually, that opposition led to his death and Cicero can be seen as a victim of his own republican ideals.
Tacitus, a contemporary of Plutarch, was not concerned with whether a form of government could be analysed as a "republic" or a "monarchy". He analyzed how the powers accumulated by the early Julio-Claudian dynasty were all given by a State that was still notionally a republic. Nor was the Roman Republic "forced" to give away these powers: it did so freely and reasonably, certainly in Augustus' case, because of his many services to the state, freeing it from civil wars and disorder.
Tacitus was one of the first to ask whether such powers were given to the head of state because the citizens wanted to give them, or whether they were given for other reasons (for example, because one had a deified ancestor). The latter case led more easily to abuses of power. In Tacitus' opinion, the trend away from a true republic was "irreversible" only when Tiberius established power, shortly after Augustus' death in AD 14 (much later than most historians place the start of the Imperial form of government in Rome). By this time, too many principles defining some powers as "untouchable" had been implemented.
Renaissance republicanism.
In Europe, republicanism was revived in the late Middle Ages when a number of states, which arose from medieval communes, embraced a republican system of government. These were generally small but wealthy trading states in which the merchant class had risen to prominence. Haakonssen notes that by the Renaissance, Europe was divided, such that those states controlled by a landed elite were monarchies, and those controlled by a commercial elite were republics. The latter included the Italian city states of Florence, Genoa, and Venice and members of the Hanseatic League.
Building upon concepts of medieval feudalism, Renaissance scholars used the ideas of the ancient world to advance their view of an ideal government. Thus the republicanism developed during the Renaissance is known as 'classical republicanism' because it relied on classical models. This terminology was developed by Zera Fink in the 1960s but some modern scholars, such as Brugger, consider it confuses the "classical republic" with the system of government used in the ancient world. 'Early modern republicanism' has been proposed as an alternative term. It is also sometimes called civic humanism.
Beyond simply a non-monarchy, early modern thinkers conceived of an "ideal" republic, in which mixed government was an important element, and the notion that virtue and the common good were central to good government. Republicanism also developed its own distinct view of liberty.
Renaissance authors who spoke highly of republics were rarely critical of monarchies. While Niccolò Machiavelli's "Discourses on Livy" is the period's key work on republics, he also wrote "The Prince" on how best to run a monarchy. The early modern writers did not see the republican model as universally applicable; most thought that it could be successful only in very small and highly urbanized city-states. Jean Bodin in "Six Books of the Commonwealth" identified monarchy with republic.
Classical writers like Tacitus, and Renaissance writers like Machiavelli, tried to avoid an outspoken preference for one government system or another. Enlightenment philosophers, on the other hand, expressed a clear opinion. Thomas More, writing before the Age of Enlightenment, was too outspoken for the reigning king's taste, even though he coded his political preferences in a Utopian allegory.
In England a type of republicanism evolved that was not wholly opposed to monarchy; thinkers such as Thomas More and Sir Thomas Smith saw a monarchy, firmly constrained by law, as compatible with republicanism.
Dutch Republic.
Anti-monarchism became more strident in the Dutch Republic during and after the Eighty Years' War, which began in 1568. This anti-monarchism was more propaganda than a political philosophy; most of the anti-monarchist works appeared in the form of widely distributed pamphlets. This evolved into a systematic critique of monarchy, written by men such as Johan Uytenhage de Mist, Radboud Herman Scheel, Lieven de Beaufort and the brothers Johan and Peter de la Court. These writers saw all monarchies as illegitimate tyrannies that were inherently corrupt. These authors were more concerned with preventing the position of Stadholder from evolving into a monarchy, than with attacking their former rulers. Dutch republicanism also influenced on French Huguenots during the Wars of Religion. In the other states of early modern Europe republicanism was more moderate.
Polish-Lithuanian Commonwealth.
In the Polish-Lithuanian Commonwealth republicanism was an influential ideology. After the establishment of the Commonwealth of Two Nations, republicans supported the status quo, of having a very weak monarch, and opposed those who thought a stronger monarchy was needed. These mostly Polish republicans, such as Łukasz Górnicki, Andrzej Wolan, and Stanisław Konarski, were well read in classical and Renaissance texts and firmly believed that their state was a republic on the Roman model, and started to call their state the Rzeczpospolita. Atypically, Polish-Lithuanian republicanism was not the ideology of the commercial class, but rather of the landed aristocracy, which would lose power if the monarchy were expanded. This resulted in an oligarchy of the great magnates.
Enlightenment republicanism.
England.
Oliver Cromwell set up a republic called the Commonwealth of England (1649–1660) and ruled as a near dictator after the overthrow of King Charles I. James Harrington was then a leading philosopher of republicanism. The collapse of the Commonwealth of England in 1660 and the restoration of the monarchy under Charles II discredited republicanism among England's ruling circles. However they welcomed the liberalism, and emphasis on rights, of John Locke, which played a major role in the Glorious Revolution of 1688. Even so, republicanism flourished in the "country" party of the early 18th century, which denounced the corruption of the "court" party, producing a political theory that heavily influenced the American colonists. In general the English ruling classes of the 18th century vehemently opposed republicanism, typified by the attacks on John Wilkes, and especially on the American Revolution and the French Revolution.
French and Swiss thought.
French and Swiss Enlightenment thinkers, such as Montesquieu and later Rousseau, expanded upon and altered the ideas of what an ideal republic should be: some of their new ideas were scarcely traceable to antiquity or the Renaissance thinkers. Concepts they contributed, or heavily elaborated, were social contract, positive law, and mixed government. They also borrowed from, and distinguished republicanism from, the ideas of liberalism that were developing at the same time.
Liberalism and republicanism were frequently conflated during this period, because they were both opposed to absolute monarchy. Modern scholars see them as two distinct streams that both contributed to the democratic ideals of the modern world. An important distinction is that, while republicanism continued to stress the importance of civic virtue and the common good, liberalism was based on economics and individualism. It is clearest in the matter of private property which, according to some, may be maintained only under protection of established positive law.
Jules Ferry, Prime Minister of France from 1880 to 1885, followed both these schools of thought and eventually enacted the Ferry Laws which were intended to overturn the Falloux Laws, by embracing the anti-clerical thinking of the "Philosophs". These laws ended the Catholic Church's involvement with many government institutions in late 19th-century France, including schools.
In the history of French politics, republicanism has faced many challenges. There were those who claimed to be republicans but who in actual practice promoted rule by a minority (sometimes a minority of one), including those Jacobins responsible for the Great Terror, and the emperors Napoleon I and Napoleon III; the imposition of monarchies in 1815 and 1830; the threat of dictatorship in the 1880s; the proto-fascism of the Vichy regime in 1940–44; the threatened coups of 1957 and 1968. But the republican system has survived in France.
Republicanism in the United States.
In recent years a debate has developed over the role of republicanism in the American Revolution and in the British radicalism of the 18th century. For many decades the consensus was that liberalism, especially that of John Locke, was paramount and that republicanism had a distinctly secondary role.
The new interpretations were pioneered by J.G.A. Pocock who argued in "The Machiavellian Moment" (1975) that, at least in the early 18th century, republican ideas were just as important as liberal ones. Pocock's view is now widely accepted. Bernard Bailyn and Gordon Wood pioneered the argument that the American founding fathers were more influenced by republicanism than they were by liberalism. Cornell University professor Isaac Kramnick, on the other hand, argues that Americans have always been highly individualistic and therefore Lockean.
In the decades before the American Revolution (1776), the intellectual and political leaders of the colonies studied history intently, looking for models of good government. They especially followed the development of republican ideas in England. Pocock explained the intellectual sources in America:
The Whig canon and the neo-Harringtonians, John Milton, James Harrington and Sidney, Trenchard, Gordon and Bolingbroke, together with the Greek, Roman, and Renaissance masters of the tradition as far as Montesquieu, formed the authoritative literature of this culture; and its values and concepts were those with which we have grown familiar: a civic and patriot ideal in which the personality was founded in property, perfected in citizenship but perpetually threatened by corruption; government figuring paradoxically as the principal source of corruption and operating through such means as patronage, faction, standing armies (opposed to the ideal of the militia), established churches (opposed to the Puritan and deist modes of American religion) and the promotion of a monied interest — though the formulation of this last concept was somewhat hindered by the keen desire for readily available paper credit common in colonies of settlement. A neoclassical politics provided both the ethos of the elites and the rhetoric of the upwardly mobile, and accounts for the singular cultural and intellectual homogeneity of the Founding Fathers and their generation.
The commitment of most Americans to these republican values made the American Revolution inevitable. Britain was increasingly seen as corrupt and hostile to republicanism, and as a threat to the established liberties the Americans enjoyed.
Leopold von Ranke in 1848 claimed that American republicanism played a crucial role in the development of European liberalism:
By abandoning English constitutionalism and creating a new republic based on the rights of the individual, the North Americans introduced a new force in the world. Ideas spread most rapidly when they have found adequate concrete expression. Thus republicanism entered our Romanic/Germanic world... Up to this point, the conviction had prevailed in Europe that monarchy best served the interests of the nation. Now the idea spread that the nation should govern itself. But only after a state had actually been formed on the basis of the theory of representation did the full significance of this idea become clear. All later revolutionary movements have this same goal... This was the complete reversal of a principle. Until then, a king who ruled by the grace of God had been the center around which everything turned. Now the idea emerged that power should come from below... These two principles are like two opposite poles, and it is the conflict between them that determines the course of the modern world. In Europe the conflict between them had not yet taken on concrete form; with the French Revolution it did.
"Républicanisme".
Republicanism, especially that of Rousseau, played a central role in the French Revolution and foreshadowed modern republicanism. The revolutionaries, after overthrowing the French monarchy in the 1790s, began by setting up a republic; Napoleon converted it into an Empire with a new aristocracy. In the 1830s Belgium adopted some of the innovations of the progressive political philosophers of the Enlightenment.
"Républicanisme" is a French version of modern republicanism. It is a form of social contract, deduced from Jean-Jacques Rousseau's idea of a general will. Ideally, each citizen is engaged in a direct relationship with the state, removing the need for identity politics based on local, religious, or racial identification.
"Républicanisme", in theory, makes anti-discrimination laws unnecessary, but some critics argue that colour-blind laws serve to perpetuate discrimination.
Republicanism in Ireland.
Inspired by the American and French Revolutions, the Society of United Irishmen was founded in 1791 in Belfast and Dublin. The inaugural meeting of the United Irishmen in Belfast which took place on 18 October 1791 approved a declaration of the society's objectives identifying the central grievance that Ireland had no national government: "...we are ruled by Englishmen, and the servants of Englishmen, whose object is the interest of another country, whose instrument is corruption, and whose strength is the weakness of Ireland..." Three central positions were adopted: (i) to seek out a cordial union among all the people of Ireland, to maintain that balance essential to preserve liberties and extend commerce; (ii) that the sole constitutional mode by which English influence can be opposed, is by a complete and radical reform of the representation of the people in Parliament; (iii) that no reform is practicable or efficacious, or just which shall not include Irishmen of every religious persuasion. The declaration, then, urged constitutional reform, union among Irish people and the removal of all religious disqualifications.
The event that above all influenced men's thoughts at that time was the French Revolution. Public interest, already strongly aroused, was brought to a pitch by the publication in 1790 of Edmund Burke's "Reflections on the Revolution in France", and Thomas Paine's response, "Rights of Man", in February 1791. Theobald Wolfe Tone wrote later that "this controversy, and the gigantic event which gave rise to it, changed in an instant the politics of Ireland. Paine himself was aware of this commenting on sales of Part I of "Rights of Man" in November 1791, only eight months after publication of the first edition, he informed a friend that in England " "almost sixteen thousand has gone off - and in Ireland above forty thousand ". Paine my have been inclined to talk up sales of his works but what is striking in this context is that Paine believed that Irish sales were so far ahead of English ones before Part II had appeared. On 5 June 1792, Thomas Paine, author of the "Rights of Man" was proposed for honorary membership of the Dublin Society of the United Irishmen.
The fall of the Bastille was to be celebrated in Belfast on 14 July 1791 by a Volunteer meeting. At the request of Thomas Russell, Tone drafted suitable resolutions for the occasion, including one favouring the inclusion of Catholics in any reforms. In a covering letter to Russell, Tone wrote, "I have not said one word that looks like a wish for separation, though I give it to you and your friends as my most decided opinion that such an event would be a regeneration of their country"". By 1795, Tone's Republicanism and that of the society had openly crystallized when he tells us: ""I remember particularly two days thae we passed on Cave Hill. On the first Russell, Neilson, Simms, McCracken and one or two more of us, on the summit of McArt's fort, took a solemn obligation...never to desist in our efforts until we had subverted the authority of England over our country and asserted her independence.""
The culmination was an uprising against British rule in Ireland lasting from May to September 1798 - the Irish Rebellion of 1798 - with military support from revolutionary France in August and again October 1798. After the failure of the rising of 1798 the United Irishman, John Daly Burk, an émigré in the United States in his " The History of the Late War in Ireland" written in 1799, was most emphatic in its identification of the Irish, French and American causes
Modern republicanism.
During the Enlightenment, anti-monarchism extended beyond the civic humanism of the Renaissance. Classical republicanism, still supported by philosophers such as Rousseau and Montesquieu, was only one of several theories seeking to limit the power of monarchies rather than directly opposing them. New forms of anti-monarchism, such as liberalism and later socialism, quickly overtook classical republicanism as the leading republican ideologies. Republicanism gained support and monarchies were challenged throughout Europe.
Radicalism.
Radicalism arose in European states in the 19th century. All 19th century "radicals" supported a constitutional republic and universal suffrage, while European "liberals" were at the time in favor of constitutional monarchy and census suffrage. Most radical parties later favored economic liberalism and capitalism. This distinction between radicalism and liberalism had not totally disappeared in the 20th century, although many radicals simply joined liberal parties. For example, the Radical Party of the Left in France or the (originally Italian) Transnational Radical Party, which exist today, are more focussed on republicanism than on simple liberalism.
Liberalism, was represented in France by the Orleanists who rallied to the Third Republic only in the late 19th century, after the comte de Chambord's 1883 death and the 1891 papal encyclical "Rerum novarum".
But the early Republican, Radical and Radical-Socialist Party in France, and Chartism in Britain, were closer to republicanism, and the left-wing. Radicalism remained close to republicanism in the 20th century, at least in France, where they governed several times with other left-wing parties (participating in both the Cartel des gauches coalitions as well as the Popular Front).
Discredited after the Second World War, French radicals split into a left-wing party – the Radical Party of the Left, an associate of the Socialist Party – and the Radical Party "valoisien", an associate party of the conservative Union for a Popular Movement (UMP) and its Gaullist predecessors. Italian radicals also maintained close links with republicanism, as well as with socialism, with the "Partito radicale" founded in 1955, which became the Transnational Radical Party in 1989.
United States.
Republicanism became the dominant political value of Americans during and after the American Revolution. The "Founding Fathers" were strong advocates of republican values, especially Thomas Jefferson, Samuel Adams, Patrick Henry, Thomas Paine, Benjamin Franklin, John Adams, James Madison and Alexander Hamilton.
British Empire and Commonwealth of Nations.
In some countries of the British Empire, later the Commonwealth of Nations, republicanism taken a variety of forms.
In Barbados the government gave the promise of a referendum on becoming a republic in August 2008, but it was postponed due to the change of government in the 2008 election.
In Jamaica, Prime Minister Portia Simpson-Miller announced during her inaugural address, that Jamaica would begin the process of becoming a republic.
In South Africa, republicanism in the 1960s was identified with the supporters of apartheid, who resented British interference in their treatment of the country's black population.
Australia.
In Australia, the debate between republicans and monarchists is still active and Julia Gillard, the former Australian Prime Minister, has expressed her wish for Australia to begin the transition phase to a republic on the death of Elizabeth II.
United Kingdom.
Republican groups are also active in the United Kingdom. The major organisation campaigning for a republic in the United Kingdom is 'Republic'.
Sweden.
In Sweden, a major promoter of republicanism is the Swedish Republican Association, which advocates the abolition of the Monarchy of Sweden.
Spain.
There is a renewed interest in republicanism in Spain after two earlier attempts: the First Spanish Republic (1873–1874) and the Second Spanish Republic (1931–1939). Movements such as "Ciudadanos Por la República", Citizens for the Republic in Spanish, have emerged, and parties like United Left (Spain) and the Republican Left of Catalonia increasingly refer to republicanism. In a survey conducted in 2007 reported that 69% of the population prefer the monarchy to continue, compared with 22% opting for a Republic. In a 2008 survey, 57.9% of Spanish citizens were indifferent, 16.2% favored a Republic, 15.7% were monarchists, and 7% claimed to be "Juancarlistas" (supporters of continued monarchy under King Juan Carlos I, without a common position for the fate of the monarchy after his death). In the last years republicanism has been rising, especially among the young people.
Neo-republicanism.
Prominent theorists in this movement are Philip Pettit and Cass Sunstein, who have each written several works defining republicanism and how it differs from liberalism. Michael Sandel, a late convert to republicanism from communitarianism, advocates replacing or supplementing liberalism with republicanism, as outlined in his "Democracy's Discontent: America in Search of a Public Philosophy." However, these theorists have had little impact on government. John W. Maynor, argues that Bill Clinton was interested in these notions and that he integrated some of them into his 1995 "new social compact" State of the Union Address.
This revival also has its critics. David Wootton, for instance, argues that throughout history the meanings of the term "republicanism" have been so diverse, and at times contradictory, that the term is all but meaningless and any attempt to build a cogent ideology based on it will fail.
Democracy.
In the late 18th century there was convergence of democracy and republicanism. Republicanism is a system that replaces or accompanies inherited rule. There is an emphasis on liberty, and a rejection of corruption. It strongly influenced the American Revolution and the French Revolution in the 1770s and 1790s, respectively. Republicans, in these two examples, tended to reject inherited elites and aristocracies, but two questions were left open: whether a republic, in order to restrain unchecked majority rule, should have an unelected upper chamber, with members perhaps being appointed meritorious experts, and whether it should have a constitutional monarch.
Although conceptually separate from democracy, republicanism included the key principles of rule by the consent of the governed and sovereignty of the people. In effect republicanism held that kings and aristocracies were not the real rulers, but rather the whole people were. Exactly "how" the people were to rule was an issue of democracy – republicanism itself did not specify how. In the United States, the solution was the creation of political parties that reflected the votes of the people, and which controlled the government (see Republicanism in the United States). Many exponents of republicanism, such as Benjamin Franklin, Thomas Paine, and Thomas Jefferson were strong promoters of representative democracy. However, other supporters of republicanism, such as John Adams and Alexander Hamilton, were more distrustful of majority rule and sought a government with more power for elites. There were similar debates in many other democratizing nations.
Democracy and republic.
In contemporary usage, the term "democracy" refers to a government chosen by the people, whether it is direct or representative. Today the term "republic" usually refers to a representative democracy with an elected head of state, such as a president, who serves for a limited term; in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies, with an elected or appointed head of government such as a prime minister.
The Founding Fathers of the United States rarely praised and often criticized democracy, which in their time tended to specifically mean direct democracy; James Madison argued that what distinguished a "democracy" from a "republic" was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure. What was critical to American values, John Adams insisted, was that the government should be "bound by fixed laws, which the people have a voice in making, and a right to defend."
Constitutional monarchs and upper chambers.
Some countries (such as the United Kingdom, the Netherlands, Belgium, Luxembourg, the Scandinavian countries, and Japan) turned powerful monarchs into constitutional ones with limited, or eventually merely symbolic, powers. Often the monarchy was abolished along with the aristocratic system, whether or not they were replaced with democratic institutions (such as in France, China, Iran, Russia, Germany, Austria, Hungary, Italy, Greece, Turkey and Egypt). In Australia, New Zealand, Canada, Papua New Guinea, and some other countries the monarch, or its representative, is given supreme executive power, but by convention acts only on the advice of his or her ministers. Many nations had elite upper houses of legislatures, the members of which often had lifetime tenure, but eventually these houses lost much power (as the UK House of Lords), or else became elective and remained powerful. 

</doc>
<doc id="25756" url="http://en.wikipedia.org/wiki?curid=25756" title="Repetitive strain injury">
Repetitive strain injury

A repetitive strain injury (RSI) is an "injury to the musculoskeletal and nervous systems that may be caused by repetitive tasks, forceful exertions, vibrations, mechanical compression, or sustained or awkward positions." RSIs are also known as "cumulative trauma disorders", "repetitive stress injuries", "repetitive motion injuries or disorders", "musculoskeletal disorders", and "occupational or sports overuse syndromes".
Definition.
Repetitive strain injury (RSI) and associative trauma disorders are umbrella terms used to refer to several discrete conditions that can be associated with repetitive tasks, forceful exertions, vibrations, mechanical compression, or sustained/awkward positions. Examples of conditions that may sometimes be attributed to such causes include edema, tendinosis (or less often tendinitis), carpal tunnel syndrome, cubital tunnel syndrome, De Quervain syndrome, thoracic outlet syndrome, intersection syndrome, golfer's elbow (medial epicondylitis), tennis elbow (lateral epicondylitis), trigger finger (so-called stenosing tenosynovitis), radial tunnel syndrome, and focal dystonia.
Since the 1970s there has been a worldwide increase in RSIs of the arms, hands, neck, and shoulder attributed to the widespread use of typewriters/computers in the workplace that require long periods of repetitive motions in a fixed posture.
Popular terms.
Specific sources of discomfort have been popularly referred to by terms such as Blackberry thumb, iPod finger, PlayStation thumb, Rubik's wrist or "cuber's thumb", stylus finger, raver's wrist, and Emacs pinky, among others.
History.
Although seemingly a modern phenomenon, RSIs have long been documented in the medical literature. In 1700, the Italian physician Bernardino Ramazzini first described RSI in more than 20 categories of industrial workers in Italy, including musicians and clerks. Carpal tunnel syndrome was first identified by the British surgeon James Paget in 1854. The Swiss surgeon Fritz de Quervain first identified De Quervain’s tendinitis in Swiss factory workers in 1895. The French neurologist Jules Tinel (1879-1952) developed his percussion test for compression of the median nerve in 1900. The American surgeon George Phalen improved the understanding of the aetiology of carpal tunnel syndrome with his clinical experience of several hundred patients during the 1950s and 1960s.
Assessment.
RSIs are assessed using a number of objective clinical measures. These include effort-based tests such as grip and pinch strength, diagnostic tests such as Finkelstein's test for Dequervain's tendinitis, Phalen's Contortion, Tinel's Percussion for carpal tunnel syndrome, and nerve conduction velocity tests that show nerve compression in the wrist. Various imaging techniques can also be used to show nerve compression such as x-ray for the wrist, and MRI for the thoracic outlet and cervico-brachial areas.
Treatment.
The most-often prescribed treatments for early-stage RSIs include drug therapies such as anti-inflammatory medications combined with passive forms of physical therapy such as rest, splinting, massage and the like. Low-grade RSIs can sometimes resolve themselves if treatments begin shortly after the onset of symptoms. However, some RSIs may require more aggressive intervention including surgery and can persist for years.
General exercise has been shown to decrease the risk of developing RSI. Doctors sometimes recommend that RSI sufferers engage in specific strengthening exercises, for example to improve sitting posture, reduce excessive kyphosis, and potentially thoracic outlet syndrome. Modifications of posture and arm use (human factors and ergonomics) are often recommended.
The Active Release Technique has also been studied as a possible treatment for RSI's.

</doc>
<doc id="25757" url="http://en.wikipedia.org/wiki?curid=25757" title="Robertson Davies">
Robertson Davies

William Robertson Davies, CC, OOnt, FRSC, FRSL (August 28, 1913 – December 2, 1995) was a Canadian novelist, playwright, critic, journalist, and professor. He was one of Canada's best known and most popular authors, and one of its most distinguished "men of letters", a term Davies is variously said to have gladly accepted for himself and to have detested. Davies was the founding Master of Massey College, a graduate residential college associated with the University of Toronto.
Biography.
Early life.
Robertson Davies was born in Thamesville, Ontario to William Rupert Davies and Florence Sheppard McKay. Growing up, Davies was surrounded by books and lively language. His father, Senator Davies, was a newspaperman from Welshpool in Wales, and both parents were voracious readers. He, similarly, read everything he could. He also participated in theatrical productions as a child, when he developed a lifelong interest in drama.
He attended Upper Canada College in Toronto from 1926 to 1932 and while there attended services at the Church of St. Mary Magdalene. He would later leave the Presbyterian Church and join Anglicanism over objections to Calvinist theology. Davies later used his experience of the ceremonial of High Mass at St Mary Magdalene's in his novel "The Cunning Man".
After Upper Canada College, he studied at Queen's University at Kingston, Ontario from 1932 until 1935. At Queen's, he was enrolled as a special student not working towards a degree, and wrote for the student paper, "The Queen's Journal". He left Canada to study at Balliol College, Oxford, where he received a BLitt degree in 1938. The next year he published his thesis, "Shakespeare's Boy Actors", and embarked on an acting career outside London. In 1940, he played small roles and did literary work for the director at the Old Vic Repertory Company in London. Also that year, Davies married Australian Brenda Mathews, whom he had met at Oxford, and who was then working as stage manager for the theatre. They spent their honeymoon in the Welsh countryside at Fronfraith Hall, Abermule, Montgomery; the family house owned by Rupert Davies.
Davies' early life provided him with themes and material to which he would often return in his later work, including the theme of Canadians returning to England to finish their education, and the theatre.
Middle years.
Davies and his new bride returned to Canada in 1940, where he took the position of literary editor at the magazine "Saturday Night". Two years later, he became editor of the "Peterborough Examiner" in the small city of Peterborough, Ontario, northeast of Toronto. Again he was able to mine his experiences here for many of the characters and situations which later appeared in his novels and plays.
Davies, along with family members William Rupert Davies and Arthur Davies, purchased several media outlets. Along with the "Examiner" newspaper, they owned the "Kingston Whig-Standard" newspaper, CHEX-AM, CKWS-AM, CHEX-TV, and CKWS-TV.
During his tenure as editor of the "Examiner", which lasted from 1942 to 1955 (and which he subsequently served as publisher from 1955 to 1965), Davies published a total of 18 books, produced several of his own plays and wrote articles for various journals. For example, Davies set out his theory of acting in his "Shakespeare for Young Players" (1947) and then put theory into practice when he wrote "Eros at Breakfast", a one-act play which was named best Canadian play of the year by the 1948 Dominion Drama Festival.
"Eros at Breakfast" was followed in close succession by "Fortune, My Foe" in 1949 and "At My Heart's Core", a three-act play, in 1950. Meanwhile, Davies was writing humorous essays in the "Examiner" under the pseudonym Samuel Marchbanks. Some of these were collected and published in "The Diary of Samuel Marchbanks" (1947), "The Table Talk of Samuel Marchbanks" (1949), and later in "Samuel Marchbanks' Almanack" (1967). (An omnibus edition of the three Marchbanks books, with new notes by the author, was published under the title "The Papers of Samuel Marchbanks" in 1985.)
During the 1950s, Davies played a major role in launching the Stratford Shakespearean Festival of Canada. He served on the Festival's board of governors and collaborated with the Festival's director, Sir Tyrone Guthrie, in publishing three books about the Festival's early years.
Although his first love was drama and he had achieved some success with his occasional humorous essays, Davies found his greatest success in fiction. His first three novels, which later became known as The Salterton Trilogy, were "Tempest-Tost" (1951), "Leaven of Malice" (1954) (which won the Stephen Leacock Award for Humour), and "A Mixture of Frailties" (1958). These novels explored the difficulty of sustaining a cultural life in Canada, and life on a small-town newspaper, subjects of which Davies had first-hand knowledge.
1960s.
In 1960, Davies joined Trinity College at the University of Toronto, where he would teach literature until 1981. The following year he published a collection of essays on literature, "A Voice From the Attic", and was awarded the Lorne Pierce Medal for his literary achievements.
In 1963, he became the Master of Massey College, the University of Toronto's new graduate college. During his stint as Master, he initiated a tradition of writing and telling ghost stories at the yearly Christmas celebrations. His stories were later collected in his book, "High Spirits" (1982).
1970s.
Davies drew on his interest in Jungian psychology to create "Fifth Business" (1970), a novel that relies heavily on Davies' own experiences, his love of myth and magic and his knowledge of small-town mores. The narrator, like Davies, is of immigrant Canadian background, with a father who runs the town paper. The book's characters act in roles that roughly correspond to Jungian archetypes according to Davies' belief in the predominance of spirit over the things of the world.
Davies built on the success of "Fifth Business" with two more novels: "The Manticore" (1972), a novel cast largely in the form of a Jungian analysis (for which he received that year's Governor General's Literary Award), and "World of Wonders" (1975). Together these three books came to be known as The Deptford Trilogy.
1980s and 1990s.
When Davies retired from his position at the university, his seventh novel, a satire of academic life, "The Rebel Angels" (1981), was published, followed by "What's Bred in the Bone" (1985). "The Lyre of Orpheus" follows these two books in what became known as The Cornish Trilogy.
During his retirement from academe he continued to write novels which further established him as a major figure in the literary world: "The Lyre of Orpheus" (1988), "Murther and Walking Spirits" (1991) and "The Cunning Man" (1994). A third novel in what would have been a further trilogy — the Toronto Trilogy — was in progress at Davies' death. He also realized a long-held dream when he penned the libretto to an opera: "The Golden Ass", based on "The Metamorphoses" of Lucius Apuleius, just like that written by one of the characters in Davies' 1958 "A Mixture of Frailties". The opera was performed by the Canadian Opera Company at the Hummingbird Centre in Toronto, in April, 1999, several years after Davies' death.
Davies was a fine public speaker—deft, often humorous, and unafraid to be unfashionable. Often asked if he used a computer, Davies said in 1987: "I don't want a word-processor. I process my own words. Helpful people assure me that a word-processor would save me a great deal of time. But I don't want to save time. I want to write the best book I can, and I have whatever time it takes to make that attempt." In its obituary, The Times wrote: 'Davies encompassed all the great elements of life...His novels combined deep seriousness and psychological inquiry with fantasy and exuberant mirth.' He remained close friends with John Kenneth Galbraith, attending Galbraith's eighty-fifth birthday party in Boston in 1993, and became so close a friend and colleague of the American novelist John Irving that Irving gave one of the scripture readings at Davies' funeral in Trinity College, Toronto chapel. He also wrote in support of Salman Rushdie when the latter was threatened by a "fatwā" from Ayatollah Ruhollah Khomeini of Iran in reaction to supposed anti-Islam expression in his novel "The Satanic Verses".
Works.
Essays.
Fictional essays
edited by the author into:
Criticism
Collections.
</dl>

</doc>
<doc id="25758" url="http://en.wikipedia.org/wiki?curid=25758" title="RNA">
RNA

Ribonucleic acid (RNA) is a polymeric molecule. It is implicated in various biological roles in coding, decoding, regulation, and expression of genes. DNA and RNA are nucleic acids, and, along with proteins and carbohydrates, constitute the three major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA it is more often found in nature as a single-strand folded unto itself, rather than a paired double-strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the letters G, U, A, and C to denote the nitrogenous bases guanine, adenine, uracil and cytosine) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.
Some RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function whereby mRNA molecules direct the assembly of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) links amino acids together to form proteins.
Comparison with DNA.
The chemical structure of RNA is very similar to that of DNA, but differs in three main ways:
Like DNA, most biologically active RNAs, including mRNA, tRNA, rRNA, snRNAs, and other non-coding RNAs, contain self-complementary sequences that allow parts of the RNA to fold and pair with itself to form double helices. Analysis of these RNAs has revealed that they are highly structured. Unlike DNA, their structures do not consist of long double helices, but rather collections of short helices packed together into structures akin to proteins.
In this fashion, RNAs can achieve chemical catalysis, like enzymes. For instance, determination of the structure of the ribosome—an enzyme that catalyzes peptide bond formation—revealed that its active site is composed entirely of RNA.
Structure.
Each nucleotide in RNA contains a ribose sugar, with carbons numbered 1' through 5'. A base is attached to the 1' position, in general, adenine (A), cytosine (C), guanine (G), or uracil (U). Adenine and guanine are purines, cytosine and uracil are pyrimidines. A phosphate group is attached to the 3' position of one ribose and the 5' position of the next. The phosphate groups have a negative charge each at physiological pH, making RNA a charged molecule (polyanion). The bases form hydrogen bonds between cytosine and guanine, between adenine and uracil and between guanine and uracil. However, other interactions are possible, such as a group of adenine bases binding to each other in a bulge,
or the GNRA tetraloop that has a guanine–adenine base-pair.
An important structural feature of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2' position of the ribose sugar. The presence of this functional group causes the helix to adopt the A-form geometry rather than the B-form most commonly observed in DNA. This results in a very deep and narrow major groove and a shallow and wide minor groove. A second consequence of the presence of the 2'-hydroxyl group is that in conformationally flexible regions of an RNA molecule (that is, not involved in formation of a double helix), it can chemically attack the adjacent phosphodiester bond to cleave the backbone.
RNA is transcribed with only four bases (adenine, cytosine, guanine and uracil), but these bases and attached sugars can be modified in numerous ways as the RNAs mature. Pseudouridine (Ψ), in which the linkage between uracil and ribose is changed from a C–N bond to a C–C bond, and ribothymidine (T) are found in various places (the most notable ones being in the TΨC loop of tRNA). Another notable modified base is hypoxanthine, a deaminated adenine base whose nucleoside is called inosine (I). Inosine plays a key role in the wobble hypothesis of the genetic code.
There are more than 100 other naturally occurring modified nucleosides, The greatest structural diversity of modifications can be found in tRNA, while pseudouridine and nucleosides with 2'-O-methylribose often present in rRNA are the most common. The specific roles of many of these modifications in RNA are not fully understood. However, it is notable that, in ribosomal RNA, many of the post-transcriptional modifications occur in highly functional regions, such as the peptidyl transferase center and the subunit interface, implying that they are important for normal function.
The functional form of single-stranded RNA molecules, just like proteins, frequently requires a specific tertiary structure. The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule. This leads to several recognizable "domains" of secondary structure like hairpin loops, bulges, and internal loops. Since RNA is charged, metal ions such as Mg2+ are needed to stabilise many secondary and tertiary structures.
The naturally occurring enantiomer of RNA is -RNA composed of -ribonucleotides. All chirality centers are located in the -ribose. By the use of -ribose or rather -ribonucleotides, -RNA can be synthesized. -RNA is much more stable against degradation by RNase.
Synthesis.
Synthesis of RNA is usually catalyzed by an enzyme—RNA polymerase—using DNA as a template, a process known as transcription. Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA (usually found "upstream" of a gene). The DNA double helix is unwound by the helicase activity of the enzyme. The enzyme then progresses along the template strand in the 3’ to 5’ direction, synthesizing a complementary RNA molecule with elongation occurring in the 5’ to 3’ direction. The DNA sequence also dictates where termination of RNA synthesis will occur.
Primary transcript RNAs are often modified by enzymes after transcription. For example, a poly(A) tail and a 5' cap are added to eukaryotic pre-mRNA and introns are removed by the spliceosome.
There are also a number of RNA-dependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA. For instance, a number of RNA viruses (such as poliovirus) use this type of enzyme to replicate their genetic material. Also, RNA-dependent RNA polymerase is part of the RNA interference pathway in many organisms.
Types of RNA.
Overview.
Messenger RNA (mRNA) is the RNA that carries information from DNA to the ribosome, the sites of protein synthesis (translation) in the cell. The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced. However, many RNAs do not code for protein (about 97% of the transcriptional output is non-protein-coding in eukaryotes).
These so-called non-coding RNAs ("ncRNA") can be encoded by their own genes (RNA genes), but can also derive from mRNA introns. The most prominent examples of non-coding RNAs are transfer RNA (tRNA) and ribosomal RNA (rRNA), both of which are involved in the process of translation. There are also non-coding RNAs involved in gene regulation, RNA processing and other roles. Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules, and the catalysis of peptide bond formation in the ribosome; these are known as ribozymes.
In translation.
Messenger RNA (mRNA) carries information about a protein sequence to the ribosomes, the protein synthesis factories in the cell. It is coded so that every three nucleotides (a codon) correspond to one amino acid. In eukaryotic cells, once precursor mRNA (pre-mRNA) has been transcribed from DNA, it is processed to mature mRNA. This removes its introns—non-coding sections of the pre-mRNA. The mRNA is then exported from the nucleus to the cytoplasm, where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA. In prokaryotic cells, which do not have nucleus and cytoplasm compartments, mRNA can bind to ribosomes while it is being transcribed from DNA. After a certain amount of time the message degrades into its component nucleotides with the assistance of ribonucleases.
Transfer RNA (tRNA) is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation. It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding.
Ribosomal RNA (rRNA) is the catalytic component of the ribosomes. Eukaryotic ribosomes contain four different rRNA molecules: 18S, 5.8S, 28S and 5S rRNA. Three of the rRNA molecules are synthesized in the nucleolus, and one is synthesized elsewhere. In the cytoplasm, ribosomal RNA and protein combine to form a nucleoprotein called a ribosome. The ribosome binds mRNA and carries out protein synthesis. Several ribosomes may be attached to a single mRNA at any time. Nearly all the RNA found in a typical eukaryotic cell is rRNA.
Transfer-messenger RNA (tmRNA) is found in many bacteria and plastids. It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling.
Regulatory RNAs.
Several types of RNA can downregulate gene expression by being complementary to a part of an mRNA or a gene's DNA. MicroRNAs (miRNA; 21-22 nt) are found in eukaryotes and act through RNA interference (RNAi), where an effector complex of miRNA and enzymes can cleave complementary mRNA, block the mRNA from being translated, or accelerate its degradation.
While small interfering RNAs (siRNA; 20-25 nt) are often produced by breakdown of viral RNA, there are also endogenous sources of siRNAs. siRNAs act through RNA interference in a fashion similar to miRNAs. Some miRNAs and siRNAs can cause genes they target to be methylated, thereby decreasing or increasing transcription of those genes. Animals have Piwi-interacting RNAs (piRNA; 29-30 nt) that are active in germline cells and are thought to be a defense against transposons and play a role in gametogenesis.
Many prokaryotes have CRISPR RNAs, a regulatory system similar to RNA interference. Antisense RNAs are widespread; most downregulate a gene, but a few are activators of transcription. One way antisense RNA can act is by binding to an mRNA, forming double-stranded RNA that is enzymatically degraded. There are many long noncoding RNAs that regulate genes in eukaryotes, one such RNA is Xist, which coats one X chromosome in female mammals and inactivates it.
An mRNA may contain regulatory elements itself, such as riboswitches, in the 5' untranslated region or 3' untranslated region; these cis-regulatory elements regulate the activity of that mRNA. The untranslated regions can also contain elements that regulate other genes.
In RNA processing.
Many RNAs are involved in modifying other RNAs.
Introns are spliced out of pre-mRNA by spliceosomes, which contain several small nuclear RNAs (snRNA), or the introns can be ribozymes that are spliced by themselves.
RNA can also be altered by having its nucleotides modified to other nucleotides than A, C, G and U.
In eukaryotes, modifications of RNA nucleotides are in general directed by small nucleolar RNAs (snoRNA; 60-300 nt), found in the nucleolus and cajal bodies. snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA. These enzymes then perform the nucleotide modification. rRNAs and tRNAs are extensively modified, but snRNAs and mRNAs can also be the target of base modification. RNA can also be methylated.
RNA genomes.
Like DNA, RNA can carry genetic information. RNA viruses have genomes composed of RNA that encodes a number of proteins. The viral genome is replicated by some of those proteins, while other proteins protect the genome as the virus particle moves to a new host cell. Viroids are another group of pathogens, but they consist only of RNA, do not encode any protein and are replicated by a host plant cell's polymerase.
In reverse transcription.
Reverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA; these DNA copies are then transcribed to new RNA. Retrotransposons also spread by copying DNA and RNA from one another, and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes.
Double-stranded RNA.
Double-stranded RNA (dsRNA) is RNA with two complementary strands, similar to the DNA found in all cells. dsRNA forms the genetic material of some viruses (double-stranded RNA viruses). Double-stranded RNA such as viral RNA or siRNA can trigger RNA interference in eukaryotes, as well as interferon response in vertebrates.
Key discoveries in RNA biology.
Research on RNA has led to many important biological discoveries and numerous Nobel Prizes. Nucleic acids were discovered in 1868 by Friedrich Miescher, who called the material 'nuclein' since it was found in the nucleus. It was later discovered that prokaryotic cells, which do not have a nucleus, also contain nucleic acids. The role of RNA in protein synthesis was suspected already in 1939. Severo Ochoa won the 1959 Nobel Prize in Medicine (shared with Arthur Kornberg) after he discovered an enzyme that can synthesize RNA in the laboratory. However, the enzyme discovered by Ochoa (polynucleotide phosphorylase) was later shown to be responsible for RNA degradation, not RNA synthesis. In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by X-ray crystallography.
The sequence of the 77 nucleotides of a yeast tRNA was found by Robert W. Holley in 1965, winning Holley the 1968 Nobel Prize in Medicine (shared with Har Gobind Khorana and Marshall Nirenberg).
In 1967, Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life (self-replicating molecules) could have relied on RNA both to carry genetic information and to catalyze biochemical reactions—an RNA world.
During the early 1970s, retroviruses and reverse transcriptase were discovered, showing for the first time that enzymes could copy RNA into DNA (the opposite of the usual route for transmission of genetic information). For this work, David Baltimore, Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975.
In 1976, Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome, that of bacteriophage MS2.
In 1977, introns and RNA splicing were discovered in both mammalian viruses and in cellular genes, resulting in a 1993 Nobel to Philip Sharp and Richard Roberts.
Catalytic RNA molecules (ribozymes) were discovered in the early 1980s, leading to a 1989 Nobel award to Thomas Cech and Sidney Altman. In 1990, it was found in "Petunia" that introduced genes can silence similar genes of the plant's own, now known to be a result of RNA interference.
At about the same time, 22 nt long RNAs, now called microRNAs, were found to have a role in the development of "C. elegans".
Studies on RNA interference gleaned a Nobel Prize for Andrew Fire and Craig Mello in 2006, and another Nobel was awarded for studies on the transcription of RNA to Roger Kornberg in the same year. The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA, such as siRNA, to silence genes.
Evolution.
In March 2015, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, were reported formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

</doc>
<doc id="25762" url="http://en.wikipedia.org/wiki?curid=25762" title="Russian Revolution">
Russian Revolution

The Russian Revolution is the collective term for a pair of revolutions in Russia in 1917, which dismantled the Tsarist autocracy and led to the creation of the Russian SFSR. The Emperor was forced to abdicate and the old regime was replaced by a provisional government during the first revolution of February 1917 (March in the Gregorian calendar; the older Julian calendar was in use in Russia at the time). In the second revolution, during October, the Provisional Government was removed and replaced with a Bolshevik (Communist) government.
The February Revolution (March 1917) was a revolution focused around Petrograd (now Saint Petersburg). In the chaos, members of the Imperial parliament or Duma assumed control of the country, forming the Russian Provisional Government. The army leadership felt they did not have the means to suppress the revolution and Nicholas II, the last Emperor of Russia, abdicated. The Soviets (workers' councils), which were led by more radical socialist factions, initially permitted the Provisional Government to rule, but insisted on a prerogative to influence the government and control various militias. The February Revolution took place in the context of heavy military setbacks during the First World War (1914–18), which left much of the Russian army in a state of mutiny.
A period of dual power ensued, during which the Provisional Government held state power while the national network of Soviets, led by socialists, had the allegiance of the lower classes and the political left. During this chaotic period there were frequent mutinies, protests and many strikes. When the Provisional Government chose to continue fighting the war with Germany, the Bolsheviks and other socialist factions campaigned for stopping the conflict. The Bolsheviks turned workers militias under their control into the Red Guards (later the Red Army) over which they exerted substantial control.
In the October Revolution (November in the Gregorian calendar), the Bolshevik party, led by Vladimir Lenin, and the workers' Soviets, overthrew the Provisional Government in Petrograd. The Bolsheviks appointed themselves as leaders of various government ministries and seized control of the countryside, establishing the Cheka to quash dissent. To end Russia’s participation in the First World War, the Bolshevik leaders signed the Treaty of Brest-Litovsk with Germany in March 1918.
Civil war erupted among the "Reds" (Bolsheviks), the "Whites" (anti-socialist factions), and non-Bolshevik socialists. It continued for several years, during which the Bolsheviks defeated both the Whites and all rival socialists. In this way, the Revolution paved the way for the creation of the Union of Soviet Socialist Republics (USSR) in 1922. While many notable historical events occurred in Moscow and Petrograd, there was also a visible movement in cities throughout the state, among national minorities throughout the empire and in the rural areas, where peasants took over and redistributed land.
Background.
The Russian Revolution of 1905 was said to be a major factor to the February Revolutions of 1917. The events of Bloody Sunday triggered a line of protests. A council of workers called the St. Petersburg Soviet was created in all this chaos, and the beginning of a communist political protest had begun.
World War I prompted a Russian outcry directed at Tsar Nicholas II. It was another major factor contributing to the retaliation of the Russian Communists against their royal opponents. After the entry of the Ottoman Empire on the side of the Central Powers in October 1914, Russia was deprived of a major trade route through Ottoman Empire, which followed with a minor economic crisis, in which Russia became incapable of providing munitions to their army in the years leading to 1917. However, the problems were merely administrative, and not industrial as Germany was producing great amounts of munitions whilst constantly fighting on two major battlefronts.
The war also developed a weariness in the city, owing to a lack of food in response to the disruption of agriculture. Food scarcity had become a considerable problem in Russia, but the cause of this did not lie in any failure of the harvests, which had not been significantly altered during war-time. The indirect reason was that the government, in order to finance the war, had been printing off millions of ruble notes, and by 1917 inflation had made prices increase up to four times what they had been in 1914. The peasantry were consequently faced with the higher cost of purchases, but made no corresponding gain in the sale of their own produce, since this was largely taken by the middlemen on whom they depended. As a result they tended to hoard their grain and to revert to subsistence farming. Thus the cities were constantly short of food. At the same time rising prices led to demands for higher wages in the factories, and in January and February 1916 revolutionary propaganda, aided by German funds, led to widespread strikes. The outcome of all this, however, was a growing criticism of the government rather than any war-weariness. The original fever of patriotic excitement, which had caused the name of St. Petersburg to be changed to the less German sounding "Petrograd," may have subsided a little in the subsequent years, but it had not turned to defeatism and during the initial risings in Petrograd in February 1917, the crowds in the streets clearly objected to the banners proclaiming "down with the war." Heavy losses during the war also strengthened thoughts that Tsar Nicholas II was unfit to rule.
The Liberals were now better placed to voice their complaints, since they were participating more fully through a variety of voluntary organizations. Local industrial committees proliferated. In July 1915, a Central War Industries Committee was established under the chairmanship of a prominent Octobrist, Guchkov, and including ten workers' representatives. The Petrograd Mensheviks agreed to join despite the objections of their leaders abroad. All this activity gave renewed encouragement to political ambitions, and, in September 1915, a combination of Octobrists and Kadets in the Duma demanded the forming of a responsible government. The Tsar rejected these proposals. He had now taken over the position of commander-in-chief of the armed forces and, during his absence from Petrograd while at his military headquarters at Mogilev, he left most of the day-to-day government in the hands of the Empress.She was intensely unpopular, owing, in part, to her German origin and to the influence that Rasputin, an unsavoury "monk", exercised over her.
All these factors had given rise to a sharp loss of confidence in the regime by 1916. Early in that year, Guchkov had been taking soundings among senior army officers and members of the Central War Industries Committee about a possible coup to force the abdication of the Tsar. In November, Pavel Milyukov in the Duma openly accused the government of contemplating peace negotiations with Germany. In December, a small group of nobles assassinated Rasputin, and in January 1917 the Tsar's uncle, Grand Duke Nicholas, was asked indirectly by Prince Lvov whether he would be prepared to take over the throne from his nephew, Tsar Nicholas II. None of these incidents were in themselves the immediate cause of the February Revolution, but they do help to explain why the monarchy survived only a few days after it had broken out.
Meanwhile, the Social Democrat leaders in exile, now mostly in Switzerland, had been the glum spectators of the collapse of international socialist solidarity. French and German Social Democrats had voted in favour of their respective governments. Georgi Plekhanov in Paris had adopted a violently anti-German stand, while Parvus supported the German war effort as the best means of ensuring a revolution in Russia. The Mensheviks largely maintained that Russia had the right to defend herself against Germany, although Martov (a prominent Menshevik), now on the left of his group, demanded an end to the war and a settlement on the basis of national self-determination, with no annexations or indemnities.
It was these views of Martov that predominated in a manifesto drawn up by Leon Trotsky (a major Bolshevik revolutionary) at a conference in Zimmerwald, attended by thirty-five Socialist leaders in September 1915. Inevitably Vladimir Lenin, supported by Zinoviev and Radek, strongly contested them. Their attitudes became known as the Zimmerwald Left. Lenin rejected both the defence of Russia and the cry for peace. Since the autumn of 1914, he had insisted that "from the standpoint of the working class and of the labouring masses from the lesser evil would be the defeat of the Tsarist Monarchy"; the war must be turned into a civil war of the proletarian soldiers against their own governments, and if a proletarian victory should emerge from this in Russia, then their duty would be to wage a revolutionary war for the liberation of the masses throughout Europe. Thus, Lenin remained the "enfant terrible" of the Russian Social Democratic Labour Party, although at this point in the war his following in Russia was as few as 10,000 and he must have seemed no more than the leader of an extremist wing of a bankrupt organization. Lenin, however, then executed the protests of Petrograd which set off the 1917 Russian Revolution.
Economic and social changes.
An elementary theory of property, believed by many peasants, was that land should belong to those who work on it. At the same time, peasant life and culture was changing constantly. Change was facilitated by the physical movement of growing numbers of peasant villagers who migrated to and from industrial and urban environments, but also by the introduction of city culture into the village through material goods, the press, and word of mouth.
Workers also had good reasons for discontent: overcrowded housing with often deplorable sanitary conditions, long hours at work (on the eve of the war a 10-hour workday six days a week was the average and many were working 11–12 hours a day by 1916), constant risk of injury and death from very poor safety and sanitary conditions, harsh discipline (not only rules and fines, but foremen’s fists), and inadequate wages (made worse after 1914 by steep war-time increases in the cost of living). At the same time, urban industrial life was full of benefits, though these could be just as dangerous, from the point of view of social and political stability, as the hardships. There were many encouragements to expect more from life. Acquiring new skills gave many workers a sense of self-respect and confidence, heightening expectations and desires. Living in cities, workers encountered material goods such as they had never seen while in the village. Most important, living in cities, they were exposed to new ideas about the social and political order.
The social causes of the Russian Revolution mainly came from centuries of oppression of the lower classes by the Tsarist regime, and Nicholas's failures in World War I. While rural agrarian peasants had been emancipated from serfdom in 1861, they still resented paying redemption payments to the state, and demanded communal tender of the land they worked. The problem was further compounded by the failure of Sergei Witte's land reforms of the early 20th century. Increasing peasant disturbances and sometimes actual revolts occurred, with the goal of securing ownership of the land they worked. Russia consisted mainly of poor farming peasants, with 1.5% of the population owning 25% of the land.
The rapid industrialization of Russia also resulted in urban overcrowding and poor conditions for urban industrial workers (as mentioned above). Between 1890 and 1910, the population of the capital, Saint Petersburg, swelled from 1,033,600 to 1,905,600, with Moscow experiencing similar growth. This created a new 'proletariat' which, due to being crowded together in the cities, was much more likely to protest and go on strike than the peasantry had been in previous times. In one 1904 survey, it was found that an average of sixteen people shared each apartment in Saint Petersburg, with six people per room. There was also no running water, and piles of human waste were a threat to the health of the workers. The poor conditions only aggravated the situation, with the number of strikes and incidents of public disorder rapidly increasing in the years shortly before World War I. Because of late industrialization, Russia's workers were highly concentrated. By 1914 40% of Russian workers were employed in factories of +1,000 workers (32% in 1901). 42% worked in 100–1,000 worker enterprises, 18% in 1–100 worker businesses (in the USA, 1914, the figures were 18, 47 and 35 respectively).
World War I only added to the chaos. Conscription swept up the unwilling in all parts of Russia. The vast demand for factory production of war supplies and workers caused many more labor riots and strikes. Conscription stripped skilled workers from the cities, who had to be replaced with unskilled peasants, and then, when famine began to hit due to the poor railway system, workers abandoned the cities in droves to look for food. Finally, the soldiers themselves, who suffered from a lack of equipment and protection from the elements, began to turn against the Tsar. This was mainly because, as the war progressed, many of the officers who were loyal to the Tsar were killed, and were replaced by discontented conscripts from the major cities, who had little loyalty to the Tsar.
Political issues.
Many sections of the country had reason to be dissatisfied with the existing autocracy. Nicholas II was a deeply conservative ruler and maintained a strict authoritarian system. Individuals and society in general were expected to show self-restraint, devotion to community, deference to the social hierarchy and a sense of duty to the country. Religious faith helped bind all of these tenets together as a source of comfort and reassurance in the face of difficult conditions and as a means of political authority exercised through the clergy. Perhaps more than any other modern monarch, Nicholas II attached his fate and the future of his dynasty to the notion of the ruler as a saintly and infallible father to his people.
This idealized vision of the Romanov monarchy blinded him to the actual state of his country. With a firm belief that his power to rule was granted by Divine Right, Nicholas assumed that the Russian people were devoted to him with unquestioning loyalty. This ironclad belief rendered Nicholas unwilling to allow the progressive reforms that might have alleviated the suffering of the Russian people. Even after the 1905 revolution spurred the Tsar to decree limited civil rights and democratic representation, he worked to limit even these liberties in order to preserve the ultimate authority of the crown.
Despite constant oppression, the desire of the people for democratic participation in government decisions was strong. Since the Age of Enlightenment, Russian intellectuals had promoted Enlightenment ideals such as the dignity of the individual and the rectitude of democratic representation. These ideals were championed most vociferously by Russia’s liberals, although populists, Marxists, and anarchists also claimed to support democratic reforms. A growing opposition movement had begun to challenge the Romanov monarchy openly well before the turmoil of World War I.
Dissatisfaction with Russian autocracy culminated in the huge national upheaval that followed the Bloody Sunday massacre of January 1905, in which hundreds of unarmed protesters were shot by the Tsar's troops. Workers responded to the massacre with a crippling general strike, forcing Nicholas to put forth the October Manifesto, which established a democratically elected parliament (the State Duma). The Tsar undermined this promise of reform but a year later with Article 87 of the 1906 Fundamental State Laws, and subsequently dismissed the first two Dumas when they proved uncooperative. Unfulfilled hopes of democracy fueled revolutionary ideas and violent outbursts targeted at the monarchy.
One of the Tsar’s principal rationales for risking war in 1914 was his desire to restore the prestige that Russia had lost amid the debacles of the Russo-Japanese war. Nicholas also sought to foster a greater sense of national unity with a war against a common and ancient enemy. The Russian Empire was an agglomeration of diverse ethnicities that had shown significant signs of disunity in the years before the First World War. Nicholas believed in part that the shared peril and tribulation of a foreign war would mitigate the social unrest over the persistent issues of poverty, inequality, and inhuman working conditions. Instead of restoring Russia's political and military standing, World War I led to the horrifying slaughter of Russian troops and military defeats that undermined both the monarchy and society in general to the point of collapse.
World War I.
The outbreak of war in August 1914 initially served to quiet the prevalent social and political protests, focusing hostilities against a common external enemy, but this patriotic unity did not last long. As the war dragged on inconclusively, war-weariness gradually took its toll. More important, though, was a deeper fragility: although many ordinary Russians joined anti-German demonstrations in the first few weeks of the war, the most widespread reaction appears to have been skepticism and fatalism. Hostility toward the Kaiser and the desire to defend their land and their lives did not necessarily translate into enthusiasm for the Tsar or the government.
Russia's first major battle of the war was a disaster: in the 1914 Battle of Tannenberg, over 30,000 Russian troops were killed or wounded and 90,000 captured, while Germany suffered just 12,000 casualties. However, Austro-Hungarian forces allied to Germany were driven back deep into the Galicia region by the end of the year. In the autumn of 1915, Nicholas had taken direct command of the army, personally overseeing Russia's main theatre of war and leaving his ambitious but incapable wife Alexandra in charge of the government. Reports of corruption and incompetence in the Imperial government began to emerge, and the growing influence of Grigori Rasputin in the Imperial family was widely resented. In the eyes of Lynch, a revisionist historian who focuses on the role of the people, Rasputin was a "fatal disease" to the Tsarist regime.
In 1915, things took a critical turn for the worse when Germany shifted its focus of attack to the Eastern front. The superior German army – better led, better trained and better supplied – was terrifyingly effective against the ill-equipped Russian forces, driving the Russians out of Galicia, as well as Russian Poland, during the Gorlice–Tarnów Offensive campaign. By the end of October 1916, Russia had lost between 1,600,000 and 1,800,000 soldiers, with an additional 2,000,000 prisoners of war and 1,000,000 missing, all making up a total of nearly 5,000,000 men.
These staggering losses played a definite role in the mutinies and revolts that began to occur. In 1916, reports of fraternizing with the enemy started to circulate. Soldiers went hungry, and lacked shoes, munitions, and even weapons. Rampant discontent lowered morale, which was further undermined by a series of military defeats.
Casualty rates were the most vivid sign of this disaster. Already, by the end of 1914, only five months into the war, around 390,000 Russian men had lost their lives and nearly 1,000,000 were injured. Far sooner than expected, barely trained recruits had to be called up for active duty, a process repeated throughout the war as staggering losses continued to mount. The officer class also saw remarkable changes, especially within the lower echelons, which were quickly filled with soldiers rising up through the ranks. These men, usually of peasant or working-class backgrounds, were to play a large role in the politicization of the troops in 1917.
The huge losses on the battlefields were not limited to men. The army quickly ran short of rifles and ammunition (as well as uniforms and food), and, by mid-1915, men were being sent to the front bearing no arms. It was hoped that they could equip themselves with the arms that they recovered from fallen soldiers, of both sides, on the battlefields. With good reason, the soldiers did not feel that they were being treated as human beings, or even as valuable soldiers, but rather as raw materials to be squandered for the purposes of the rich and powerful.
By the spring of 1915, the army was in steady retreat, which was not always orderly; desertion, plunder and chaotic flight were not uncommon. By 1916, however, the situation had improved in many respects. Russian troops stopped retreating, and there were even some modest successes in the offensives that were staged that year, albeit at great loss of life. Also, the problem of shortages was largely solved by a major effort to increase domestic production. Nevertheless, by the end of 1916, morale among soldiers was even worse than it had been during the great retreat of 1915. The fortunes of war may have improved, but the fact of the war, still draining away strength and lives from the country and its many individuals and families, remained an oppressive inevitability. The crisis in morale (as was argued by Allan Wildman, a leading historian of the Russian army in war and revolution) "was rooted fundamentally in the feeling of utter despair that the slaughter would ever end and that anything resembling victory could be achieved."
The war devastated not only soldiers. By the end of 1915, there were manifold signs that the economy was breaking down under the heightened strain of wartime demand. The main problems were food shortages and rising prices. Inflation dragged incomes down at an alarmingly rapid rate, and shortages made it difficult to buy even what one could afford. These shortages were a problem especially in the capital, St. Petersburg, where distance from supplies and poor transportation networks made matters particularly bad. Shops closed early or entirely for lack of bread, sugar, meat and other provisions, and lines lengthened massively for what remained. It became increasingly difficult both to afford and actually buy food.
Not surprisingly, strikes increased steadily from the middle of 1915, and so did crime; but, for the most part, people suffered and endured, scouring the city for food. Working class women in St. Petersburg reportedly spent about forty hours a week in food lines, begging, turning to prostitution or crime, tearing down wooden fences to keep stoves heated for warmth, grumbling about the rich, and wondering when and how this would all come to an end.
Government officials responsible for public order worried about how long people's patience would last. A report by the St. Petersburg branch of the security police, the Okhrana, in October 1916, warned bluntly of "the possibility in the near future of riots by the lower classes of the empire enraged by the burdens of daily existence."
Nicholas was blamed for all of these crises, and what little support he had left began to crumble. As discontent grew, the State Duma issued a warning to Nicholas in November 1916. It stated that, inevitably, a terrible disaster would grip the country unless a constitutional form of government was put in place. In typical fashion, however, Nicholas ignored them, and Russia's Tsarist regime collapsed a few months later during the February Revolution of 1917. One year later, the Tsar and his entire family were executed. Ultimately, Nicholas's inept handling of his country and the war destroyed the Tsar and ended up costing him both his reign and his life.
February Revolution.
At the beginning of February, Petrograd workers began several strikes and demonstrations. On 7 March [O.S. 22 February], workers at Putilov, Petrograd's largest industrial plant, announced a strike.
The next day, a series of meetings and rallies were held for International Women's Day, which gradually turned into economic and political gatherings. Demonstrations were organised to demand bread, and these were supported by the industrial working force who considered them a reason for continuing the strikes. The women workers marched to nearby factories bringing out over 50,000 workers on strike. By 10 March [O.S. 25 February], virtually every industrial enterprise in Petrograd had been shut down, together with many commercial and service enterprises. Students, white-collar workers and teachers joined the workers in the streets and at public meetings.
To quell the riots, the Tsar looked to the army. At least 180,000 troops were available in the capital, but most were either untrained or injured. Historian Ian Beckett suggests around 12,000 could be regarded as reliable, but even these proved reluctant to move in on the crowd, since it included so many women. It was for this reason that when, on 11 March [O.S. 26 February], the Tsar ordered the army to suppress the rioting by force, troops began to mutiny. Although few actively joined the rioting, many officers were either shot or went into hiding; the ability of the garrison to hold back the protests was all but nullified, symbols of the Tsarist regime were rapidly torn down around the city, and governmental authority in the capital collapsed – not helped by the fact that Nicholas had prorogued the Duma that morning, leaving it with no legal authority to act. The response of the Duma, urged on by the liberal bloc, was to establish a Temporary Committee to restore law and order; meanwhile, the socialist parties establish the Petrograd Soviet to represent workers and soldiers. The remaining loyal units switched allegiance the next day.
The Tsar took a train back towards Petrograd, which was stopped on 14 March [O.S. 1 March], by a group of disloyal troops. When the Tsar finally reached his destination, the Army Chiefs and his remaining ministers (those who had not fled under pretense of a power-cut) suggested in unison that he abdicate the throne. He did so on 15 March [O.S. 2 March], on behalf of himself, and then, having taken advice, on behalf of his son, the Tsarevich. Nicholas nominated his brother, the Grand Duke Michael Alexandrovich, to succeed him. But the Grand Duke realised that he would have little support as ruler, so he declined the crown on 16 March [O.S. 3 March], stating that he would take it only if that was the consensus of democratic action. Six days later, Nicholas, no longer Tsar and addressed with contempt by the sentries as "Nicholas Romanov", was reunited with his family at the Alexander Palace at Tsarskoye Selo. He was placed under house arrest with his family by the Provisional Government.
The immediate effect of the February Revolution was a widespread atmosphere of elation and excitement in Petrograd. On 16 March [O.S. 3 March], a provisional government was announced. The center-left was well represented, and the government was initially chaired by a liberal aristocrat, Prince Georgy Yevgenievich Lvov, a member of the Constitutional Democratic party (KD). The socialists had formed their rival body, the Petrograd Soviet (or workers' council) four days earlier. The Petrograd Soviet and the Provisional Government competed for power over Russia.
Between February and throughout October: "Dual Power" ("dvoevlastie").
The effective power of the Provisional Government was challenged by the authority of an institution that claimed to represent the will of workers and soldiers and could, in fact, mobilize and control these groups during the early months of the revolution – the Petrograd Soviet [Council] of Workers' Deputies. The model for the soviet were workers' councils that had been established in scores of Russian cities during the 1905 revolution. In February 1917, striking workers elected deputies to represent them and socialist activists began organizing a citywide council to unite these deputies with representatives of the socialist parties. On 27 February, socialist Duma deputies, mainly Mensheviks and Socialist Revolutionaries, took the lead in organizing a citywide council. The Petrograd Soviet met in the Tauride Palace, the same building where the new government was taking shape.
The leaders of the Petrograd Soviet believed that they represented particular classes of the population, not the whole nation. They also believed Russia was not ready for socialism. So they saw their role as limited to pressuring hesitant "bourgeoisie" to rule and to introduce extensive democratic reforms in Russia (the replacement of the monarchy by a republic, guaranteed civil rights, a democratic police and army, abolition of religious and ethnic discrimination, preparation of elections to a constituent assembly, and so on). They met in the same building as the emerging Provisional Government not to compete with the Duma Committee for state power but to best exert pressure on the new government, to act, in other words, as a popular democratic lobby.
The relationship between these two major powers was complex from the beginning and would shape the politics of 1917. The representatives of the Provisional Government agreed to "take into account the opinions of the Soviet of Workers' Deputies", though they were also determined to prevent "interference in the actions of the government", which would create "an unacceptable situation of dual power." In fact, this was precisely what was being created, though this "dual power" (dvoevlastie) was the result less of the actions or attitudes of the leaders of these two institutions than of actions outside their control, especially the ongoing social movement taking place on the streets of Russia’s cities, in factories and shops, in barracks and in the trenches, and in the villages.
A series of political crises – see the chronology below – in the relationship between population and government and between the Provisional Government and the soviets (which developed into a nationwide movement with a national leadership, The All-Russian Central Executive Committee of Soviets (VTsIK)) undermined the authority of the Provisional Government but also of the moderate socialist leaders of the Soviet. Although the Soviet leadership initially refused to participate in the "bourgeois" Provisional Government, Alexander Kerensky, a young and popular lawyer and a member of the Socialist Revolutionary Party (SRP), agreed to join the new cabinet, and became an increasingly central figure in the government, eventually taking leadership of the Provisional Government. As minister of war and later Prime Minister, Kerensky promoted freedom of speech, released thousands of political prisoners, did his very best to continue the war effort and even organised another offensive (which, however, was no more successful than its predecessors). Nevertheless, Kerensky still faced several great challenges, highlighted by the soldiers, urban workers and peasants, who claimed that they had gained nothing by the revolution:
The political group that proved most troublesome for Kerensky, and would eventually overthrow him, was the Bolshevik Party, led by Vladimir Lenin. Lenin had been living in exile in neutral Switzerland and, due to democratization of politics after the February Revolution, which legalized formerly banned political parties, he perceived the opportunity for his Marxist revolution. Although return to Russia had become a possibility, the war made it logistically difficult. Eventually, German officials arranged for Lenin to pass through their territory, hoping that his activities would weaken Russia or even – if the Bolsheviks came to power – lead to Russia's withdrawal from the war. Lenin and his associates, however, had to agree to travel to Russia in a sealed train: Germany would not take the chance that he would foment revolution in Germany. After passing through the front, he arrived in Petrograd in April 1917.
With Lenin's arrival, the popularity of the Bolsheviks increased steadily. Over the course of the spring, public dissatisfaction with the Provisional Government and the war, in particular among workers, soldiers and peasants, pushed these groups to radical parties. Despite growing support for the Bolsheviks, buoyed by maxims that called most famously for "all power to the Soviets," the party held very little real power in the moderate-dominated Petrograd Soviet. In fact, historians such as Sheila Fitzpatrick have asserted that Lenin's exhortations for the Soviet Council to take power were intended to arouse indignation both with the Provisional Government, whose policies were viewed as conservative, and the Soviet itself, which was viewed as subservient to the conservative government. By some historians' accounts, Lenin and his followers were unprepared for how their groundswell of support, especially among influential worker and soldier groups, would translate into real power in the summer of 1917.
On 18 June, the Provisional Government launched an attack against Germany that failed miserably. Soon after, the government ordered soldiers to go to the front, reneging on a promise. The soldiers refused to follow the new orders. The arrival of radical Kronstadt sailors – who had tried and executed many officers, including one admiral – further fueled the growing revolutionary atmosphere. The sailors and soldiers, along with Petrograd workers, took to the streets in violent protest, calling for "all power to the Soviets." The revolt, however, was disowned by Lenin and the Bolshevik leaders and dissipated within a few days. In the aftermath, Lenin fled to Finland under threat of arrest while Trotsky, among other prominent Bolsheviks, was arrested. The July Days confirmed the popularity of the anti-war, radical Bolsheviks, but their unpreparedness at the moment of revolt was an embarrassing gaffe that lost them support among their main constituent groups: soldiers and workers.
The Bolshevik failure in the July Days proved temporary. The Bolsheviks had undergone a spectacular growth in membership. Whereas, in February 1917, the Bolsheviks were limited to only 24,000 members, by September 1917 there were 200,000 members of the Bolshevik faction. Previously, the Bolsheviks had been in the minority in the two leading cities of Russia—St. Petersburg and Moscow behind the Mensheviks and the Socialist Revolutionaries, by September the Bolsheviks were in the majority in both cities. Furthermore, the Bolshevik-controlled Moscow Regional Bureau of the Party also controlled the Party organizations of the thirteen (13) provinces around Moscow. These thirteen provinces held 37% of Russia's population and 20% of the membership of the Bolshevik faction.
In August, poor or misleading communication led General Lavr Kornilov, the recently appointed Supreme Commander of Russian military forces, to believe that the Petrograd government had already been captured by radicals, or was in serious danger thereof. In response, he ordered troops to Petrograd to pacify the city. To secure his position, Kerensky had to ask for Bolshevik assistance. He also sought help from the Petrograd Soviet, which called upon armed Red Guards to "defend the revolution". The Kornilov Affair failed largely due to the efforts of the Bolsheviks, whose influence over railroad and telegraph workers proved vital in stopping the movement of troops. With his coup failing, Kornilov surrendered and was relieved of his position. The Bolsheviks' role in stopping the attempted coup further strengthened their position.
In early September, the Petrograd Soviet freed all jailed Bolsheviks and Trotsky became chairman of the Petrograd Soviet. Growing numbers of socialists and lower-class Russians viewed the government less and less as a force in support of their needs and interests. The Bolsheviks benefited as the only major organized opposition party that had refused to compromise with the Provisional Government, and they benefited from growing frustration and even disgust with other parties, such as the Mensheviks and Socialist Revolutionaries, who stubbornly refused to break with the idea of national unity across all classes.
In Finland, Lenin had worked on his book "State and Revolution" and continued to lead his party, writing newspaper articles and policy decrees. By October, he returned to Petrograd (St. Petersburg), aware that the increasingly radical city presented him no legal danger and a second opportunity for revolution. Recognising the strength of the Bolsheviks, Lenin began pressing for the immediate overthrow of the Kerensky government by the Bolsheviks. Lenin was of the opinion that taking power should occur in both St. Petersburg and Moscow simultaneously, parenthetically stating that it made no difference which city rose up first, but expressing his opinion that Moscow may well rise up first. The Bolshevik Central Committee drafted a resolution, calling for the dissolution of the Provisional Government in favor of the Petrograd Soviet. The resolution was passed 10–2 (Lev Kamenev and Grigory Zinoviev prominently dissenting) and the October Revolution began.
October Revolution.
The October Revolution was led by Vladimir Lenin and was based upon Lenin's writing on the ideas of Karl Marx, a political ideology often known as Marxism-Leninism. It marked the beginning of the spread of communism in the 20th century. It was far less sporadic than the revolution of February and came about as the result of deliberate planning and coordinated activity to that end.
Though Lenin was the leader of the Bolshevik Party, it has been argued that since Lenin was not present during the actual takeover of the Winter Palace, it was really Trotsky's organization and direction that led the revolution, merely spurred by the motivation Lenin instigated within his party. Critics on the Right have long argued that the financial and logistical assistance of German intelligence via their key agent, Alexander Parvus was a key component as well, though historians are divided, since there is little evidence supporting that claim.
On 7 November 1917, Bolshevik leader Vladimir Lenin led his leftist revolutionaries in a revolt against the ineffective Provisional Government (Russia was still using the Julian Calendar at the time, so period references show a 25 October date). The October revolution ended the phase of the revolution instigated in February, replacing Russia's short-lived provisional parliamentary government with government by soviets, local councils elected by bodies of workers and peasants. Liberal and monarchist forces, loosely organized into the White Army, immediately went to war against the Bolsheviks' Red Army, in a series of battles that would become known as the Russian Civil War.
Soviet membership was initially freely elected, but many members of the Socialist-Revolutionary Party, anarchists, and other leftists created opposition to the Bolsheviks through the soviets themselves. When it became clear that the Bolsheviks had little support outside of the industrialized areas of Saint Petersburg and Moscow, they simply barred non-Bolsheviks from membership in the soviets. Not surprisingly, this caused mass domestic tension with many individuals who called for another series of political reform, revolting, and calling for "a third Russian revolution," a movement that received a significant amount of support. The most notable instances of this anti-Bolshevik mentality were expressed in the Tambov rebellion, 1919–1921, and the Kronstadt rebellion in March 1921. These movements, which made a wide range of demands and lacked effective coordination, were eventually defeated along with the White Army during the Civil War.
Civil war.
The Russian Civil War, which broke out in 1918 shortly after the revolution, brought death and suffering to millions of people regardless of their political orientation. The war was fought mainly between the Red Army ("Reds"), consisting of the uprising majority led by the Bolshevik minority, and the "Whites" – army officers and cossacks, the "bourgeoisie", and political groups ranging from the far Right to the Socialist Revolutionaries who opposed the drastic restructuring championed by the Bolsheviks following the collapse of the Provisional Government to the soviets (under clear Bolshevik dominance). The Whites had backing from nations such as Great Britain, France, USA and Japan, while the Reds possessed internal support which proved to be much more effective. Though the Allied nations, using external interference, provided substantial military aid to the loosely knit anti-Bolshevik forces, they were ultimately defeated.
The Bolsheviks firstly assumed power in Petrograd, expanding their rule outwards. They eventually reached the Easterly Siberian Russian coast in Vladivostok, 4 years after the war began, an occupation that is believed to have ended all significant military campaigns in the nation. Less than one year later the last area controlled by the White Army, the Ayano-Maysky District, directly to the north of the Krai containing Vladivostok, was given up when General Anatoly Pepelyayev capitulated in 1923.
Several revolts were initiated against the Bolsheviks and their army near the end of the war, notably the Kronstadt Rebellion. This was a naval mutiny engineered by Soviet Baltic sailors, former Red Army soldiers, and the people of Kronstadt. This armed uprising was fought against the antagonizing Bolshevik economic policies that farmers were subjected to, including seizures of grain crops by the Communists. This all amounted to large-scale discontent. When delegates representing the Kronstadt sailors arrived at Petrograd for negotiations, they raised 15 demands primarily pertaining to the Russian right to freedom The Government firmly denounced the rebellions and labelled the requests as a reminder of the Social Revolutionaries, a political party that was popular among Soviets before Lenin, but refused to cooperate with the Bolshevik Army. The Government then responded with an armed suppression of these revolts and suffered 10 thousand casualties before entering the city of Kronstadt. This ended the rebellions fairly quickly, causing many of the rebels to flee to political exile.
During the Civil War, Nestor Makhno led a Ukrainian anarchist movement, the Black Army allied to the Bolsheviks thrice, one of the powers ending the alliance each time. However, a Bolshevik force under Mikhail Frunze destroyed the Makhnovist movement, when the Makhnovists refused to merge into the Red Army. In addition, the so-called "Green Army" (peasants defending their property against the opposing forces) played a secondary role in the war, mainly in the Ukraine.
Execution of the imperial family.
The Bolsheviks executed the tsar and his family on 16 July 1918. In early March, the Provisional Government placed Nicholas and his family under house arrest in the Alexander Palace at Tsarskoe Selo, 15 mi south of Petrograd. In August 1917 the Kerensky government evacuated the Romanovs to Tobolsk in the Urals, to protect them from the rising tide of revolution during the Red Terror. After the Bolsheviks came to power in October 1917, the conditions of their imprisonment grew stricter and talk of putting Nicholas on trial increased. As the counter revolutionary White movement gathered force, leading to full-scale civil war by the summer, the Romanovs were moved during April and May 1918 to Yekaterinburg, a militant Bolshevik stronghold.
During the early morning of 16 July, Nicholas, Alexandra, their children, their physician, and several servants were taken into the basement and shot. According to Edvard Radzinsky and Dmitrii Volkogonov, the order came directly from Lenin and Sverdlov in Moscow. That the order came from the top has long been believed, although there is a lack of hard evidence. The execution may have been carried out on the initiative of local Bolshevik officials, or it may have been an option pre-approved in Moscow should White troops approach Yekaterinburg. Radzinsky noted that Lenin's bodyguard personally delivered the telegram ordering the execution and that he was ordered to destroy the evidence.
The Russian revolution and the world.
Leon Trotsky said that the goal of socialism in Russia would not be realized without the success of the world revolution. Indeed, a revolutionary wave caused by the Russian Revolution lasted until 1923. Despite initial hopes for success in the German Revolution of 1918–1919, in the short-lived Hungarian Soviet Republic and others like it, no other Marxist movement at the time succeeded in keeping power in its hands.
This issue is subject to conflicting views on the communist history by various Marxist groups and parties. Joseph Stalin later rejected this idea, stating that socialism was possible in one country.
The confusion regarding Stalin's position on the issue stems from the fact that he, after Lenin's death in 1924, successfully used Lenin's argument – the argument that socialism's success needs the workers of other countries in order to happen – to defeat his competitors within the party by accusing them of betraying Lenin and, therefore, the ideals of the October Revolution.
Historiography.
Few events in historical research have been as conditioned by political influences as the October Revolution. The historiography of the Revolution generally divides into three camps: the Soviet-Marxist view, the Western-Totalitarian view, and the Revisionist view. Since the fall of Communism in Russia in 1991, the Western-Totalitarian view has again become dominant and the Soviet-Marxist view has practically vanished.
Lenin's biographer Robert Service, says he, "laid the foundations of dictatorship and lawlessness. Lenin had consolidated the principle of state penetration of the whole society, its economy and its culture. Lenin had practised terror and advocated revolutionary amoralism."
Chronologies.
Chronology of events leading to the Revolution of 1917.
"Dates are correct for the Julian calendar, which was used in Russia until 1918. It was twelve days behind the Gregorian calendar during the 19th century and thirteen days behind it during the 20th century."
Forecast.
The revolution was foreseen by the Russian writer Fyodor Dostoyevsky who died more than 30 years before it. He wrote:
"Ungodly atheism is near, our children will see it. International decided that the European revolution will start in Russia and it will as we do not have reliable resistance in administration nor society. The mutiny will start with atheism and robbery of all riches they will start to depose the religion, destroy temples and turn them into barracks and barns, engulf the world in blood and then themselves get frightened".
Cultural portrayal.
George Orwell's classic novella "Animal Farm" is an allegory of the Russian Revolution. It describes the dictator Stalin as a big Berkshire boar by the name of Napoleon. Trotsky is represented by a pig called Snowball who is a brilliant talker and makes magnificent speeches. However, Napoleon overthrows Snowball as Stalin overthrew Trotsky and Napoleon took over the farm on which the animals were living on. Napoleon became a tyrant and used force and propaganda to oppress the animals.
Film.
The Russian Revolution has been portrayed in several films.

</doc>
<doc id="25764" url="http://en.wikipedia.org/wiki?curid=25764" title="Raven Software">
Raven Software

Raven Software (or Raven Entertainment Software, Inc.) is an American video game developing company based in Wisconsin and founded in 1990. In 1997, Raven made an exclusive publishing deal with Activision and was subsequently acquired by them. After the acquisition, much of the studio's original developers, largely responsible for creating the "Heretic" and "" games, left to form Human Head Studios.
History.
Raven Software was founded in 1990 by brothers Brian and Steve Raffel. The company was independent until 1997 when it looked for a buyer; eventually being bought by Activision due to their happiness to leave the studio relatively untouched. When Raven Software was acquired it lost many employees who distrusted Activision and were afraid that it would ruin their corporate culture.
Raven has a history of working with id Software: After using id's engines for many of their games (from "Heretic" in 1994), they took over development of id's "Quake" franchise for "Quake 4" and the new iteration of id's "Wolfenstein" series.
The company started off with three development teams, cut to two from the major layoffs, of 30-35 staff, which occurred in August 2009 following the poor performance and possible over-budget of "Wolfenstein". The amount of teams reduced to one as a result of more layoffs in October 2010 after delays with "Singularity" in which as many as 40 staff were released.
In a recent twit, they have announced for the possible sequel of the Singularity, despite objection and denying from Activision.
Games.
In 2012, Raven began hiring employees for a next generation game, and were announced as collaborating with Infinity Ward on "" in May 2013.
On April 3, 2013 following the closure of LucasArts, Raven Software released the source codes for ' and ' on Kotaku.
As of April 2014, they are the lead developer of the free-to-play Chinese "Call of Duty" title, "".

</doc>
<doc id="25765" url="http://en.wikipedia.org/wiki?curid=25765" title="RNA world">
RNA world

The RNA world refers to the self-replicating ribonucleic acid (RNA) molecules that were precursors to all current life on Earth. It is generally accepted that current life on Earth descends from an RNA world, although RNA-based life may not have been the first life to exist.
RNA stores genetic information like DNA, and catalyzes chemical reactions like an enzyme protein. It may, therefore, have played a major step in the evolution of cellular life. The RNA world would have eventually been replaced by the DNA, RNA and protein world of today, likely through an intermediate stage of ribonucleoprotein enzymes such as the ribosome and ribozymes, since proteins large enough to self-fold and have useful activities would only have come about after RNA was available to catalyze peptide ligation or amino acid polymerization. DNA is thought to have taken over the role of data storage due to its increased stability, while proteins, through a greater variety of monomers (amino acids), replaced RNA's role in specialized biocatalysis.
The RNA world hypothesis is supported by many independent lines of evidence, such as the observations that RNA is central to the translation process and that small RNAs can catalyze all of the chemical group and information transfers required for life. The structure of the ribosome has been called the "smoking gun," as it showed that the ribosome is a ribozyme, with a central core of RNA and no amino acid side chains within 18 angstroms of the active site where peptide bond formation is catalyzed. Many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. This would mean that the RNA and nucleotide cofactors in modern cells are an evolutionary remnant of an RNA-based enzymatic system that preceded the protein-based one seen in all extant life.
Evidence suggests chemical conditions (including the presence of boron, molybdenum and oxygen) for initially producing RNA molecules may have been better on the planet Mars than those on the planet Earth. If so, life-suitable molecules, originating on Mars, may have later migrated to Earth via panspermia or similar process.
History.
One of the challenges in studying abiogenesis is that the system of reproduction and metabolism utilized by all extant life involves three distinct types of interdependent macromolecules (DNA, RNA, and protein). This suggests that life could not have arisen in its current form, and mechanisms have then been sought whereby the current system might have arisen from a simpler precursor system. The concept of RNA as a primordial molecule can be found in papers by Francis Crick and Leslie Orgel, as well as in Carl Woese's 1967 book "The Genetic Code". In 1962 the molecular biologist Alexander Rich, of the Massachusetts Institute of Technology, had posited much the same idea in an article he contributed to a volume issued in honor of Nobel-laureate physiologist Albert Szent-Györgyi. Hans Kuhn in 1972 laid out a possible process by which the modern genetic system might have arisen from a nucleotide-based precursor, and this led Harold White in 1976 to observe that many of the cofactors essential for enzymatic function are either nucleotides or could have been derived from nucleotides. He proposed that these nucleotide cofactors represent "fossils of nucleic acid enzymes". The phrase "RNA World" was first used by Nobel laureate Walter Gilbert in 1986, in a commentary on how recent observations of the catalytic properties of various forms of RNA fit with this hypothesis.
Properties of RNA.
The properties of RNA make the idea of the RNA world hypothesis conceptually plausible, though its general acceptance as an explanation for the origin of life requires further evidence. RNA is known to form efficient catalysts and its similarity to DNA makes its ability to store information clear. Opinions differ, however, as to whether RNA constituted the ﬁrst autonomous self-replicating system or was a derivative of a still-earlier system. One version of the hypothesis is that a different type of nucleic acid, termed "pre-RNA", was the first one to emerge as a self-reproducing molecule, to be replaced by RNA only later. On the other hand, the recent ﬁnding that activated pyrimidine ribonucleotides can be synthesized under plausible prebiotic conditions means that it is premature to dismiss the RNA-ﬁrst scenarios. Suggestions for 'simple' "pre-RNA" nucleic acids have included Peptide nucleic acid (PNA), Threose nucleic acid (TNA) or Glycol nucleic acid (GNA). Despite their structural simplicity and possession of properties comparable with RNA, the chemically plausible generation of "simpler" nucleic acids under prebiotic conditions has yet to be demonstrated.
RNA as an enzyme.
RNA enzymes, or ribozymes, are found in today's DNA-based life and could be examples of living fossils. Ribozymes play vital roles, such as those in the ribosome, which is vital for protein synthesis. Many other ribozyme functions exist; for example, the hammerhead ribozyme performs self-cleavage and an RNA polymerase ribozyme can synthesize a short RNA strand from a primed RNA template.
Among the enzymatic properties important for the beginning of life are:
RNA in information storage.
RNA is a very similar molecule to DNA, and only has two chemical differences. The overall structure of RNA and DNA are immensely similar—one strand of DNA and one of RNA can bind to form a double helical structure. This makes the storage of information in RNA possible in a very similar way to the storage of information in DNA. However RNA is less stable.
Comparison of DNA and RNA structure.
The major difference between RNA and DNA is the presence of a hydroxyl group at the 2'-position of the ribose sugar in RNA ("illustration, right"). This group makes the molecule less stable because when not constrained in a double helix, the 2' hydroxyl can chemically attack the adjacent phosphodiester bond to cleave the phosphodiester backbone. The hydroxyl group also forces the ribose into the C3'-"endo" sugar conformation unlike the C2'-"endo" conformation of the deoxyribose sugar in DNA. This forces an RNA double helix to change from a B-DNA structure to one more closely resembling A-DNA.
RNA also uses a different set of bases than DNA—adenine, guanine, cytosine and uracil, instead of adenine, guanine, cytosine and thymine. Chemically, uracil is similar to thymine, differing only by a methyl group, and its production requires less energy. In terms of base pairing, this has no effect. Adenine readily binds uracil or thymine. Uracil is, however, one product of damage to cytosine that makes RNA particularly susceptible to mutations that can replace a GC base pair with a GU (wobble) or AU base pair.
RNA is thought to have preceded DNA, because of their ordering in the biosynthetic pathways. The deoxyribonucleotides used to make DNA are made from ribonucleotides, the building blocks of RNA, by removing the 2'-hydroxyl group. As a consequence a cell must have the ability to make RNA before it can make DNA.
Limitations of information storage in RNA.
The chemical properties of RNA make large RNA molecules inherently fragile, and they can easily be broken down into their constituent nucleotides through hydrolysis. These limitations do not make use of RNA as an information storage system impossible, simply energy intensive (to repair or replace damaged RNA molecules) and prone to mutation. While this makes it unsuitable for current 'DNA optimised' life, it may have been acceptable for more primitive life.
RNA as a regulator.
Riboswitches have been found to act as regulators of gene expression, particularly in bacteria, but also in plants and archaea. Riboswitches alter their secondary structure in response to the binding of a metabolite. This change in structure can result in the formation or disruption of a terminator, truncating or permitting transcription respectively. Alternatively, riboswitches may bind or occlude the Shine-Dalgarno sequence, affecting translation. It has been suggested that these originated in an RNA-based world. In addition, RNA thermometers regulate gene expression in response to temperature changes.
Support and difficulties.
The RNA world hypothesis is supported by RNA's ability to store, transmit, and duplicate genetic information, as DNA does. RNA can act as a ribozyme, a special type of enzyme. Because it can perform the tasks of both DNA and enzymes, RNA is believed to have once been capable of supporting independent life forms. Some viruses use RNA as their genetic material, rather than DNA. Further, while nucleotides were not found in Miller-Urey's origins of life experiments, their formation in prebiotically plausible conditions has now been reported, as noted above; the purine base known as adenine is merely a pentamer of hydrogen cyanide. Experiments with basic ribozymes, like Bacteriophage Qβ RNA, have shown that simple self-replicating RNA structures can withstand even strong selective pressures (e.g., opposite-chirality chain terminators).
Since there were no known chemical pathways for the abiogenic synthesis of nucleotides from pyrimidine nucleobases cytosine and uracil under prebiotic conditions, it is thought by some that nucleic acids did not contain these nucleobases seen in life's nucleic acids. The nucleoside cytosine has a half-life in isolation of 19 days at 100 °C and 17,000 years in freezing water, which some argue is too short on the geologic time scale for accumulation. Others have questioned whether ribose and other backbone sugars could be stable enough to find in the original genetic material, and have raised the issue that all ribose molecules would have had to be the same enantiomer, as any nucleotide of the wrong chirality acts as a chain terminator.
Pyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesised by a sequence of reactions that by-pass free sugars and assemble in a stepwise fashion by going against the dogma that nitrogenous and oxygenous chemistries should be avoided. In a series of publications, The "Sutherland Group" at the School of Chemistry, University of Manchester have demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater, of possible interest towards biological homochirality. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallised out from a mixture of the other pentose aminooxazolines. Aminooxazolines can react with cyanoacetylene in a mild and highly efficient manner, controlled by inorganic phosphate, to give the cytidine ribonucleotides. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry, one problem with this chemistry is the selective phosphorylation of alpha-cytidine at the 2' position. However, in 2009 they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerise into RNA. This was hailed as strong evidence for the RNA world. The paper also highlighted the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates. A potential weakness of these routes is the generation of enantioenriched glyceraldehyde, or its 3-phosphate derivative (glyceraldehyde prefers to exist as its keto tautomer dihydroxyacetone).
On August 8, 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of RNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. On August 29, 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
"Molecular biologist's dream".
"Molecular biologist's dream" is a phrase coined by Gerald Joyce and Leslie Orgel to refer to the problem of emergence of self-replicating RNA molecules, as any movement towards an RNA world on a properly modeled prebiotic early Earth would have been continuously suppressed by destructive reactions. It was noted that many of the steps needed for the nucleotides formation do not proceed efficiently in prebiotic conditions. Joyce and Orgel specifically referred the molecular biologist's dream to "a magic catalyst" that could "convert the activated nucleotides to a random ensemble of polynucleotide sequences, a subset of which had the ability to replicate".
Joyce and Orgel further argued that nucleotides cannot link unless there is some activation of the phosphate group, whereas the only effective activating groups for this are "totally implausible in any prebiotic scenario", particularly adenosine triphosphate. According to Joyce and Orgel, in case of the phosphate group activation, the basic polymer product would have 5',5'-pyrophosphate linkages, while the 3',5'-phosphodiester linkages, which are present in all known RNA, would be much less abundant. The associated molecules would have been also prone to addition of incorrect nucleotides or to reactions with numerous other substances likely to have been present. The RNA molecules would have been also continuously degraded by such destructive process as spontaneous hydrolysis, present on the early Earth. Joyce and Orgel proposed to reject "the myth of a self-replicating RNA molecule that arose "de novo" from a soup of random polynucleotides" and hypothesised a scenario where the prebiotic processes furnish pools of enantiopure beta-D-ribonucleosides.
Prebiotic RNA synthesis.
Nucleotides are the fundamental molecules that combine in series to form RNA. They consist of a nitrogenous base attached to a sugar-phosphate backbone. RNA is made of long stretches of specific nucleotides arranged so that their sequence of bases carries information. The RNA world hypothesis holds that in the primordial soup (or sandwich), there existed free-floating nucleotides. These nucleotides regularly formed bonds with one another, which often broke because the change in energy was so low. However, certain sequences of base pairs have catalytic properties that lower the energy of their chain being created, enabling them to stay together for longer periods of time. As each chain grew longer, it attracted more matching nucleotides faster, causing chains to now form faster than they were breaking down.
These chains have been proposed by some as the first, primitive forms of life. In an RNA world, different sets of RNA strands would have had different replication outputs, which would have increased or decreased their frequency in the population, i.e. natural selection. As the fittest sets of RNA molecules expanded their numbers, novel catalytic properties added by mutation, which benefitted their persistence and expansion, could accumulate in the population. Such an autocatalytic set of ribozymes, capable of self replication in about an hour, has been identified. It was produced by molecular competition ("in vitro" evolution) of candidate enzyme mixtures.
Competition between RNA may have favored the emergence of cooperation between different RNA chains, opening the way for the formation of the first protocell. Eventually, RNA chains developed with catalytic properties that help amino acids bind together (a process called peptide-bonding). These amino acids could then assist with RNA synthesis, giving those RNA chains that could serve as ribozymes the selective advantage. The ability to catalyze one step in protein synthesis, aminoacylation of RNA, has been demonstrated in a short (five-nucleotide) segment of RNA.
One of the problems with the RNA world hypothesis is to discover the pathway by which RNA became upgraded to the DNA system. Ken Stedman of Portland State University in Oregon, may have found the solution. While filtering virus-sized particles from a hot acidic lake in Lassen Volcanic National Park, California, he discovered 400,000 pieces of viral DNA. Some of these, however, contained a protein coat of reverse transcriptase enzyme normally associated with RNA based retroviruses. This lack of respect for biochemical boundaries virologists like Luis Villareal of the University of California Irvine believe would have been a characteristic of a pre RNA virus world up to 4 billion years ago. This finding bolsters the argument for the transfer of information from the RNA world to the emerging DNA world before the emergence of the Last Universal Common Ancestor. From the research, the diversity of this virus world is still with us.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under conditions found only in outer space, using starting chemicals, like pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in giant red stars or in interstellar dust and gas clouds, according to the scientists.
Viroids.
Additional evidence supporting the concept of an RNA world has resulted from research on viroids, the first representatives of a novel domain of "subviral pathogens."
Viroids are mostly plant pathogens, which consist of short stretches (a few hundred nucleobases) of highly complementary, circular, single-stranded, and non-coding RNA without a protein coat. Compared with other infectious plant pathogens, viroids are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection are about 2,000 nucleobases long.
In 1989, Diener proposed that, based on their characteristic properties, viroids are more plausible "living relics" of the RNA world than are introns or other RNAs then so considered. If so, viroids have attained potential significance beyond plant pathology to evolutionary biology, by representing the most plausible macromolecules known capable of explaining crucial intermediate steps in the evolution of life from inanimate matter (see: abiogenesis).
Apparently, Diener's hypothesis lay dormant until 2014, when Flores et al. published a review paper, in which Diener's evidence supporting his hypothesis was summarized. In the same year, a New York Times science writer published a popularized version of Diener's proposal, in which, however, he mistakenly credited Flores et al. with the hypothesis' original conception.
Pertinent viroid properties listed in 1989 are:
1. their small size, imposed by error-prone replication;
2. their high guanine and cytosine content, which increases stability and replication fidelity;
3. their circular structure, which assures complete replication without genomic tags;
4. existence of structural periodicity, which permits modular assembly into enlarged genomes;
5. their lack of protein-coding ability, consistent with a ribosome-free habitat; and
6. replication mediated in some by ribozymes—the fingerprint of the RNA world.
The existence, in extant cells, of RNAs with molecular properties predicted for RNAs of the RNA World constitutes an additional argument supporting the RNA World hypothesis.
Origin of sex.
Eigen et al. and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene).
Vulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. However for such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number. The protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells.
A cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with a damage in an essential RNA gene would simply die.
This model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, “mating” can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as “multiplicity reactivation”. Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation.
Further developments.
Patrick Forterre has been working on a novel hypothesis, called "three viruses, three domains": that viruses were instrumental in the transition from RNA to DNA and the evolution of Bacteria, Archaea, and Eukaryota. He believes the last common ancestor (specifically, the "last universal cellular ancestor") was RNA-based and evolved RNA viruses. Some of the viruses evolved into DNA viruses to protect their genes from attack. Through the process of viral infection into hosts the three domains of life evolved. Another interesting proposal is the idea that RNA synthesis might have been driven by temperature gradients, in the process of thermosynthesis.
Single nucleotides have been shown to catalyze organic reactions.
Alternative hypotheses.
The hypothesized existence of an RNA world does not exclude a "Pre-RNA world", where a metabolic system based on a different nucleic acid is proposed to pre-date RNA. A candidate nucleic acid is peptide nucleic acid (PNA), which uses simple peptide bonds to link nucleobases. PNA is more stable than RNA, but its ability to be generated under prebiological conditions has yet to be demonstrated experimentally.
Threose nucleic acid (TNA) has also been proposed as a starting point, as has glycol nucleic acid (GNA), and like PNA, also lack experimental evidence for their respective abiogenesis.
An alternative—or complementary— theory of RNA origin is proposed in the PAH world hypothesis, whereby polycyclic aromatic hydrocarbons (PAHs) mediate the synthesis of RNA molecules. PAHs are the most common and abundant of the known polyatomic molecules in the visible Universe, and are a likely constituent of the primordial sea. PAHs, along with fullerenes (also implicated in the origin of life), have been recently detected in nebulae.
The iron-sulfur world theory proposes that simple metabolic processes developed before genetic materials did, and these energy-producing cycles catalyzed the production of genes.
Some of the difficulties over producing the precursors on earth are bypassed by another alternative or complementary theory for their origin, panspermia. It discusses the possibility that the earliest life on this planet was carried here from somewhere else in the galaxy, possibly on meteorites similar to the Murchison meteorite. This does not invalidate the concept of an RNA world, but posits that this world or its precursors originated not on Earth but rather another, probably older, planet.
There are hypotheses that are in direct conflict to the RNA world hypothesis. The relative chemical complexity of the nucleotide and the unlikelihood of it spontaneously arising, along with the limited number of combinations possible among four base forms as well as the need for RNA polymers of some length before seeing enzymatic activity have led some to reject the RNA world hypothesis in favor of a metabolism-first hypothesis, where the chemistry underlying cellular function arose first, and the ability to replicate and facilitate this metabolism. Another proposal is that the dual molecule system we see today, where a nucleotide-based molecule is needed to synthesize protein, and a protein-based molecule is needed to make nucleic acid polymers, represents the original form of life. This theory is called the Peptide-RNA world, and offers a possible explanation for the rapid evolution of high-quality replication in RNA (since proteins are catalysts), with the disadvantage of having to postulate the formation of two complex molecules, an enzyme (from peptides) and a RNA (from nucleotides). In this Peptide-RNA World scenario, RNA would have contained the instructions for life while peptides (simple protein enzymes) would have accelerated key chemical reactions to carry out those instructions. The study leaves open the question of exactly how those primitive systems managed to replicate themselves — something neither the RNA World hypothesis nor the Peptide-RNA World theory can yet explain, unless polymerases —enzymes that rapidly assemble the RNA molecule— played a role.
Implications of the RNA world.
The RNA world hypothesis, if true, has important implications for the definition of life.
For most of the time that followed Watson and Crick's elucidation of DNA structure in 1953, life was largely defined in terms of DNA and proteins: DNA and proteins seemed the dominant macromolecules in the living cell, with RNA only aiding in creating proteins from the DNA blueprint.
The RNA world hypothesis places RNA at center-stage when life originated. This has been accompanied by many studies in the last ten years that demonstrate important aspects of RNA function not previously known—and supports the idea of a critical role for RNA in the mechanisms of life. The RNA world hypothesis is supported by the observations that ribosomes are ribozymes: the catalytic site is composed of RNA, and proteins hold no major structural role and are of peripheral functional importance. This was confirmed with the deciphering of the 3-dimensional structure of the ribosome in 2001. Specifically, peptide bond formation, the reaction that binds amino acids together into proteins, is now known to be catalyzed by an adenine residue in the rRNA.
Other interesting discoveries demonstrate a role for RNA beyond a simple message or transfer molecule. These include the importance of small nuclear ribonucleoproteins (snRNPs) in the processing of pre-mRNA and RNA editing, RNA interference (RNAi), and reverse transcription from RNA in eukaryotes in the maintenance of telomeres in the telomerase reaction.
Further reading.
</dl>

</doc>
<doc id="25766" url="http://en.wikipedia.org/wiki?curid=25766" title="Ribosome">
Ribosome

The ribosome is a large and complex molecular machine, found within all living cells, that serves as the site of biological protein synthesis (translation). Ribosomes link amino acids together in the order specified by messenger RNA (mRNA) molecules. Ribosomes consist of two major components — the small ribosomal subunit which reads the RNA, and the large subunit which joins amino acids to form a polypeptide chain. Each subunit is composed of one or more ribosomal RNA (rRNA) molecules and a variety of proteins. The ribosomes and associated molecules are also known as the "translational apparatus".
The sequence of DNA encoding for a protein may be copied many times into RNA chains of a similar sequence. Ribosomes can bind to an RNA chain and use it as a template for determining the correct sequence of amino acids in a particular protein. Amino acids are selected, collected and carried to the ribosome by transfer RNA (tRNA molecules), which enter one part of the ribosome and bind to the messenger RNA chain. The attached amino acids are then linked together by another part of the ribosome. Once the protein is produced, it can then 'fold' to produce a specific functional three-dimensional structure.
A ribosome is made from complexes of RNAs and proteins and is therefore a ribonucleoprotein. Each ribosome is divided into two subunits: a smaller subunit which binds to the mRNA pattern, and a larger subunit which binds to the tRNA and the amino acids. When a ribosome finishes reading an mRNA molecule, these two subunits split apart. Ribosomes are ribozymes, because the catalytic peptidyl transferase activity that links amino acids together is performed by the ribosomal RNA.
Ribosomes from bacteria, archaea and eukaryotes (the three domains of life on Earth) differ in their size, sequence, structure, and the ratio of protein to RNA. The differences in structure allow some antibiotics to kill bacteria by inhibiting their ribosomes, while leaving human ribosomes unaffected. In bacteria and archaea, more than one ribosome may move along a single mRNA chain at one time, each "reading" its sequence and producing a corresponding protein molecule. The ribosomes in the mitochondria of eukaryotic cells functionally resemble many features of those in bacteria, reflecting the likely evolutionary origin of mitochondria.
Discovery.
Ribosomes were first observed in the mid-1950s by Romanian cell biologist George Emil Palade using an electron microscope as dense particles or granules for which, in 1974, he would win a Nobel Prize. The term "ribosome" was proposed by scientist Richard B. Roberts in 1958:
 During the course of the symposium a semantic difficulty became apparent. To some of the participants, "microsomes" mean the ribonucleoprotein particles of the microsome fraction contaminated by other protein and lipid material; to others, the microsomes consist of protein and lipid contaminated by particles. The phrase “microsomal particles” does not seem adequate, and “ribonucleoprotein particles of the microsome fraction” is much too awkward. During the meeting, the word "ribosome" was suggested, which has a very satisfactory name and a pleasant sound. The present confusion would be eliminated if “ribosome” were adopted to designate ribonucleoprotein particles in sizes ranging from 35 to 100S.
 — Roberts, R. B., "Microsomal Particles and Protein Synthesis"
Albert Claude, Christian de Duve, and George Emil Palade were jointly awarded the Nobel Prize in Physiology or Medicine, in 1974, for the discovery of the ribosomes. The Nobel Prize in Chemistry 2009 was awarded to Venkatraman Ramakrishnan, Thomas A. Steitz and Ada E. Yonath for determining the detailed structure and mechanism of the ribosome.
Structure.
The ribosome is responsible for the synthesis of proteins in cells and is found in all cellular organisms. It serves to convert the instructions found in messenger RNA (mRNA, which itself is made from instructions in DNA) into the chains of amino-acids that make up proteins.
The ribosome is a cellular machine which is highly complex. It is made up of dozens of distinct proteins (the exact number varies a little bit between species) as well as a few specialized RNA molecules known as ribosomal RNA (rRNA). Note – these rRNAs do not carry instructions to make specific proteins like mRNAs. The ribosomal proteins and rRNAs are arranged into two distinct ribosomal pieces of different size, known generally as the large and small subunit of the ribosome. Ribosomes consist of two subunits that fit together (Figure 2) and work as one to translate the mRNA into a polypeptide chain during protein synthesis (Figure 1). Because they are formed from two subunits of non-equal size, they are slightly longer in the axis than in diameter. Prokaryotic ribosomes are around 20 nm (200 Å) in diameter and are composed of 65% rRNA and 35% ribosomal proteins. Eukaryotic ribosomes are between 25 and 30 nm (250–300 Å) in diameter with an rRNA to protein ratio that is close to 1. Bacterial ribosomes are composed of one or two rRNA strands. Eukaryotic ribosomes contain one or three very large rRNA molecules and multiple smaller protein molecules. Crystallographic work has shown that there are no ribosomal proteins close to the reaction site for polypeptide synthesis. This proves that the protein components of ribosomes do not directly participate in peptide bond formation catalysis, but rather suggests that these proteins act as a scaffold that may enhance the ability of rRNA to synthesize protein (See: Ribozyme).
The ribosomal subunits of prokaryotes and eukaryotes are quite similar.
The unit of measurement is the Svedberg unit, a measure of the rate of sedimentation in centrifugation rather than size. This accounts for why fragment names do not add up: for example, prokaryotic 70S ribosomes are made of 50S and 30S subunits.
Prokaryotes have 70S ribosomes, each consisting of a small (30S) and a large (50S) subunit. Their small subunit has a 16S RNA subunit (consisting of 1540 nucleotides) bound to 21 proteins. The large subunit is composed of a 5S RNA subunit (120 nucleotides), a 23S RNA subunit (2900 nucleotides) and 31 proteins. Affinity label for the tRNA binding sites on the E. coli ribosome allowed the identification of A and P site proteins most likely associated with the peptidyltransferase activity; labelled proteins are L27, L14, L15, L16, L2; at least L27 is located at the donor site, as shown by E. Collatz and A.P. Czernilofsky. Additional research has demonstrated that the S1 and S21 proteins, in association with the 3'-end of 16S ribosomal RNA, are involved in the initiation of translation.
Eukaryotes have 80S ribosomes, each consisting of a small (40S) and large (60S) subunit. Their 40S subunit has an 18S RNA (1900 nucleotides) and 33 proteins. The large subunit is composed of a 5S RNA (120 nucleotides), 28S RNA (4700 nucleotides), a 5.8S RNA (160 nucleotides) subunits and 46 proteins. During 1977, Czernilofsky published research that used affinity labeling to identify tRNA-binding sites on rat liver ribosomes. Several proteins, including L32/33, L36, L21, L23, L28/29 and L13 were implicated as being at or near the peptidyl transferase center.
The ribosomes found in chloroplasts and mitochondria of eukaryotes also consist of large and small subunits bound together with proteins into one 70S particle. These organelles are believed to be descendants of bacteria (see Endosymbiotic theory) and as such their ribosomes are similar to those of bacteria.
The various ribosomes share a core structure, which is quite similar despite the large differences in size. Much of the RNA is highly organized into various tertiary structural motifs, for example pseudoknots that exhibit coaxial stacking. The extra RNA in the larger ribosomes is in several long continuous insertions, such that they form loops out of the core structure without disrupting or changing it. All of the catalytic activity of the ribosome is carried out by the RNA; the proteins reside on the surface and seem to stabilize the structure.
The differences between the bacterial and eukaryotic ribosomes are exploited by pharmaceutical chemists to create antibiotics that can destroy a bacterial infection without harming the cells of the infected person. Due to the differences in their structures, the bacterial 70S ribosomes are vulnerable to these antibiotics while the eukaryotic 80S ribosomes are not. Even though mitochondria possess ribosomes similar to the bacterial ones, mitochondria are not affected by these antibiotics because they are surrounded by a double membrane that does not easily admit these antibiotics into the organelle.
High-resolution structure.
The general molecular structure of the ribosome has been known since the early 1970s. In the early 2000s the structure has been achieved at high resolutions, on the order of a few Å.
The first papers giving the structure of the ribosome at atomic resolution were published almost simultaneously in late 2000. The 50S (large prokaryotic) subunit was determined from the archaeon "Haloarcula marismortui" and the bacterium "Deinococcus radiodurans", and the structure of the 30S subunit was determined from "Thermus thermophilus". These structural studies were awarded the Nobel Prize in Chemistry in 2009. Early the next year (May 2001) these coordinates were used to reconstruct the entire "T. thermophilus" 70S particle at 5.5 Å resolution.
Two papers were published in November 2005 with structures of the "Escherichia coli" 70S ribosome. Ribosomes are responsible for creating the proteins in a eukaryotic cell. The structures of a vacant ribosome were determined at 3.5-Å resolution using x-ray crystallography. Then, two weeks later, a structure based on cryo-electron microscopy was published, which depicts the ribosome at 11–15Å resolution in the act of passing a newly synthesized protein strand into the protein-conducting channel.
The first atomic structures of the ribosome complexed with tRNA and mRNA molecules were solved by using X-ray crystallography by two groups independently, at 2.8 Å and at 3.7 Å. These structures allow one to see the details of interactions of the "Thermus thermophilus" ribosome with mRNA and with tRNAs bound at classical ribosomal sites. Interactions of the ribosome with long mRNAs containing Shine-Dalgarno sequences were visualized soon after that at 4.5- to 5.5-Å resolution.
In 2011, the first complete atomic structure of the eukaryotic 80S ribosome from the yeast "Saccharomyces cerevisiae" was obtained by crystallography. The model reveals the architecture of eukaryote-specific elements and their interaction with the universally conserved core. At the same time, the complete model of a eukaryotic 40S ribosomal structure in "Tetrahymena thermophila" was published and described the structure of the 40S subunit as well as much about the 40S subunit's interaction with eIF1 during translation initiation. Similarly, the eukaryotic 60S subunit structure was also determined from "Tetrahymena thermophila" in complex with eIF6.
Function.
Translation.
Ribosomes are the workplaces of protein biosynthesis, the process of translating mRNA into protein. The mRNA comprises a series of codons that dictate to the ribosome the sequence of the amino acids needed to make the protein. Using the mRNA as a template, the ribosome traverses each codon (3 nucleotides) of the mRNA, pairing it with the appropriate amino acid provided by an aminoacyl-tRNA. aminoacyl-tRNA contains a complementary anticodon on one end and the appropriate amino acid on the other. For fast and accurate recognition of the appropriate tRNA, the ribosome utilizes large conformational changes (conformational proofreading) 
The small ribosomal subunit, typically bound to an aminoacyl-tRNA containing the amino acid methionine, binds to an AUG codon on the mRNA and recruits the large ribosomal subunit. The ribosome contains three RNA binding sites, designated A, P and E. The A site binds an aminoacyl-tRNA; the P site binds a peptidyl-tRNA (a tRNA bound to the peptide being synthesized); and the E site binds a free tRNA before it exits the ribosome. Protein synthesis begins at a start codon AUG near the 5' end of the mRNA. mRNA binds to the P site of the ribosome first. The ribosome is able to identify the start codon by use of the Shine-Dalgarno sequence of the mRNA in prokaryotes and Kozak box in eukaryotes.
Although catalysis of the peptide bond involves the C2 hydroxyl of RNA's P-site adenosine in a protein shuttle mechanism, other steps in protein synthesis (such as translocation) are caused by changes in protein conformations. Since their catalytic core is made of RNA, ribosomes are classified as "ribozymes," and it is thought that they might be remnants of the RNA world.
In Figure 5, both ribosomal subunits (small and large) assemble at the start codon (towards the 5' end of the RNA). The ribosome uses RNA that matches the current codon (triplet) on the mRNA to append an amino acid to the polypeptide chain. This is done for each triplet on the RNA, while the ribosome moves towards the 3' end of the mRNA. Usually in bacterial cells, several ribosomes are working parallel on a single RNA, forming what is called a "polyribosome" or "polysome".
Addition of translation-independent amino acids.
Presence of a ribosome quality control protein Rqc2 is associated with mRNA-independent protein elongation. This elongation is a result of ribosomal addition (via tRNAs brought by Rqc2) of CAT tails": ribosomes extend the C-terminus of a stalled protein with random, translation-independent sequences of alanines and t"hreonines.
Ribosome locations.
Ribosomes are classified as being either "free" or "membrane-bound".
Free and membrane-bound ribosomes differ only in their spatial distribution; they are identical in structure. Whether the ribosome exists in a free or membrane-bound state depends on the presence of an ER-targeting signal sequence on the protein being synthesized, so an individual ribosome might be membrane-bound when it is making one protein, but free in the cytosol when it makes another protein.
Ribosomes are sometimes referred to as organelles, but the use of the term "organelle" is often restricted to describing sub-cellular components that include a phospholipid membrane, which ribosomes, being entirely particulate, do not. For this reason, ribosomes may sometimes be described as "non-membranous organelles".
Free ribosomes.
Free ribosomes can move about anywhere in the cytosol, but are excluded from the cell nucleus and other organelles. Proteins that are formed from free ribosomes are released into the cytosol and used within the cell. Since the cytosol contains high concentrations of glutathione and is, therefore, a reducing environment, proteins containing disulfide bonds, which are formed from oxidized cysteine residues, cannot be produced within it.
Membrane-bound ribosomes.
When a ribosome begins to synthesize proteins that are needed in some organelles, the ribosome making this protein can become "membrane-bound". In eukaryotic cells this happens in a region of the endoplasmic reticulum (ER) called the "rough ER". The newly produced polypeptide chains are inserted directly into the ER by the ribosome undertaking vectorial synthesis and are then transported to their destinations, through the secretory pathway. Bound ribosomes usually produce proteins that are used within the plasma membrane or are expelled from the cell via "exocytosis".
Biogenesis.
In bacterial cells, ribosomes are synthesized in the cytoplasm through the transcription of multiple ribosome gene operons. In eukaryotes, the process takes place both in the cell cytoplasm and in the nucleolus, which is a region within the cell nucleus. The assembly process involves the coordinated function of over 200 proteins in the synthesis and processing of the four rRNAs, as well as assembly of those rRNAs with the ribosomal proteins.
External links.
 This article incorporates public domain material from the document .

</doc>
<doc id="25767" url="http://en.wikipedia.org/wiki?curid=25767" title="Real-time computing">
Real-time computing

In computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a "real-time constraint", for example operational deadlines from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as "deadlines". Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually "guarantee" a response within any timeframe, although "actual" or "expected" response times may be given.
A real-time system has been described as one which "controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time." The term "real-time" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean "without significant delay".
Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.
Systems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding. Real-time processing "fails" if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.
History.
The term "real-time" derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. 
Minicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as CAT scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operatings System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real Time operating systems would also be used for time-sharing multi-user duties, for example Data General Business Basic could run in the foreground or background of RDOS (and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.
Once when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Commodore Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the Programmable Interrupt Controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems thanks to this facility. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.
Criteria for real-time computing.
A system is said to be "real-time" if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline:
Thus, the goal of a "hard real-time system" is to ensure that all deadlines are met, but for "soft real-time systems" the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high priority tasks meeting their deadlines.
Hard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). For example, a car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Other examples of hard real-time embedded systems include medical systems such as heart pacemakers and industrial process controllers. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems such as the Atari 2600 and Cinematronics vector graphics had hard real-time requirements because of the nature of the graphics and timing hardware.
In the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). Other scheduling algorithms include earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.
Soft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. An example can be software that maintains and updates the flight plans for commercial airliners: the flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time; violation of constraints results in degraded quality, but the system can continue to operate and also recover in the future using workload prediction and reconfiguration methodologies.
Real-time in digital signal processing.
In a real-time digital signal processing (DSP) process, the analyzed (input) and generated (output) samples can be processed (or generated) continuously in the time it takes to input and output the same set of samples "independent" of the processing delay. It means that the processing delay must be bounded even if the processing continues for an unlimited time. That means that the mean processing time per sample is no greater than the sampling period, which is the reciprocal of the sampling rate. This is the criterion whether the samples are grouped together in large segments and processed as blocks or are processed individually and whether there are long, short, or non-existent input and output buffers.
Consider an audio DSP example; if a process requires 2.01 seconds to analyze, synthesize, or process 2.00 seconds of sound, it is not real-time. If it takes 1.99 seconds, it is or can be made into a real-time DSP process.
A common life analog is standing in a line or queue waiting for the checkout in a grocery store. If the line asymptotically grows longer and longer without bound, the checkout process is not real-time. If the length of the line is bounded, customers are being "processed" and output as rapidly, on average, as they are being inputted and that process "is" real-time. The grocer might go out of business or must at least lose business if they cannot make their checkout process real-time; thus, it is fundamentally important that this process is real-time.
A signal processing algorithm that cannot keep up with the flow of input data with output falling farther and farther behind the input is not real-time. But if the delay of the output (relative to the input) is bounded regarding a process that operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.
Real-time signal processing is necessary, but not sufficient in and of itself, for live signal processing such as what is required in live event support. Live audio digital signal processing requires both real-time operation and a sufficient limit to throughput delay so as not to be noticeable by listeners also watching the performers. Tolerable limits to delay for live, real-time processing is a subject of debate but is estimated to be between 6 and 20 milliseconds.
Real-time and high-performance.
Real-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification. For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect) but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. Therefore, the most important requirement of a real-time system is predictability and not performance.
Some kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, while real-time is the ability to get done with the processing to yield a useful output in the available time.
Near real-time.
The term ""near real-time" or "nearly real-time"" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.
The distinction between the terms "near real time" and "real time" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as "real-time" would be more accurately described as "near real-time".
Near real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so that they can sync/share common data in "near real-time" with each other.
The distinction between "near real-time" and "real-time" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically of the order of several seconds to several minutes.
Design methods.
Several methods exist to aid the design of real-time systems, an example of which is MASCOT, an old but very successful method which represents the concurrent structure of the system. Other examples are HOOD, Real-Time UML, AADL, the Ravenscar profile, and Real-Time Java.

</doc>
<doc id="25768" url="http://en.wikipedia.org/wiki?curid=25768" title="Ruby (programming language)">
Ruby (programming language)

Ruby is a dynamic, reflective, object-oriented, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.
According to its authors, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp. It supports multiple programming paradigms, including functional, object-oriented, and imperative. It also has a dynamic type system and automatic memory management.
History.
Early concept.
Ruby was conceived on February 24, 1993. In a 1999 post to the "ruby-talk" mailing list, Ruby author Yukihiro Matsumoto describes some of his early ideas about the language:
I was talking with my colleague about the possibility of an object-oriented scripting language. I knew Perl (Perl4, not Perl5), but I didn't like it really, because it had the smell of a toy language (it still has). The object-oriented language seemed very promising. I knew Python then. But I didn't like it, because I didn't think it was a true object-oriented language — OO features appeared to be add-on to the language. As a language maniac and OO fan for 15 years, I really wanted a genuine object-oriented, easy-to-use scripting language. I looked for but couldn't find one. So I decided to make it.
Matsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.
The name "Ruby".
The name "Ruby" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language. Initially two names were proposed: "Coral" and "Ruby". Matsumoto chose the latter in a later e-mail to Ishitsuka. Matsumoto later noted a factor in choosing the name "Ruby" – it was the birthstone of one of his colleagues.
First publication.
The first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995. Subsequently three more versions of Ruby were released in two days. The release coincided with the launch of the Japanese-language "ruby-list" mailing list, which was the first mailing list for the new language.
Already present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.
Early releases.
Following the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:
In 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.
In 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.
In 1999, the first English language mailing list "ruby-talk" began, which signaled a growing interest in the language outside Japan. In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, "The Object-oriented Scripting Language Ruby" (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.
By 2000, Ruby was more popular than Python in Japan. In September 2000, the first English language book "Programming Ruby" was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language "ruby-talk" mailing list was receiving more messages than the Japanese-language "ruby-list", demonstrating Ruby's increasing popularity in the English-speaking world.
Ruby 1.8.
Ruby 1.8 was initially released in August 2003, was stable for a long time, and was retired June 2013. Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.
Ruby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011 and an international standard (ISO/IEC 30170) in 2012.
Around 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web application framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.
Ruby 1.9.
Ruby 1.9 was released in December 2007. Effective with Ruby 1.9.3, released October 31, 2011, Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license. Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.
Ruby 1.9 introduces many significant changes over the 1.8 series. Examples:
Ruby 1.9 is obsolete since February 23 2015 and it will no longer receive bug and security fixes. Users are recommended to upgrade to a more recent version.
Ruby 2.0.
Ruby 2.0 added several new features, including:
Ruby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.
Ruby 2.1.
Ruby 2.1.0 was released on Christmas Day in 2013. The release includes speed-ups, bugfixes, and library updates. Starting with 2.1.0, Ruby is using semantic versioning.
Ruby 2.2.
Ruby 2.2.0 was released on Christmas Day in 2014. The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling - an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), support for Unicode 7.0.
Features now obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger as well as various C API functions.
Philosophy.
Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design. At a Google Tech Talk in 2008 Matsumoto further stated, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language." He stresses that systems design needs to emphasize human, rather than computer, needs:
Often people, especially computer engineers, focus on the machines. They think, "By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something." They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.
Ruby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby, but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.
Matsumoto defined it this way in an interview:
Everyone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least "my" surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.
Semantics.
Ruby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and "null"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become members of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as "global" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.
Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.
According to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk but it differs greatly from Python."
Syntax.
The syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. One difference from C and Perl is that keywords are typically used to define logical code blocks, without braces, in other words a pair of { and }. For practical purposes there is no distinction between expressions and statements. Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.
One of the differences of Ruby compared to Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (codice_8, codice_9, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.
Python's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare codice_10 and codice_11 methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.
See the Examples section below for samples of code demonstrating Ruby syntax.
Differences from other languages.
Some features that differ notably from languages such as C or Perl:
Some features that differ notably from other languages:
A list of so-called gotchas may be found in Hal Fulton's book "The Ruby Way", 2nd ed (ISBN 0-672-32884-4), Section 1.5. A similar list in the 1st edition pertained to an older version of Ruby (version 1.6), some problems of which have been fixed in the meantime. For example, codice_49 now works with codice_50, codice_51, and codice_52, as well as with iterators.
Interaction.
The Ruby official distribution also includes codice_53, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using codice_53:
Examples.
The following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing codice_55.
Classic Hello world example:
Some basic Ruby code:
Conversions:
Strings.
There are a variety of ways to define strings in Ruby.
The following assignments are equivalent:
Strings support variable interpolation:
The following assignments are equivalent and produce raw strings:
Collections.
Constructing and using an array:
Constructing and using an [[associative array]] (in Ruby, called a "hash"):
Control structures.
If statement:
Blocks and iterators.
The two syntaxes for creating a code block:
A code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:
Parameter-passing a block to be a [[Closure (computer science)|closure]]:
Creating an [[anonymous function]]:
Returning [[Closure (computer science)|closures]] from a method:
Yielding the flow of program control to a block that was provided at calling time:
Iterating over enumerations and arrays using blocks:
A method such as codice_56 can accept both a parameter and a block. The codice_56 method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the codice_58 function in [[functional programming languages]]. For example:
On the first pass, the block receives 10 (the argument to inject) as codice_59, and 1 (the first element of the array) as codice_60. This returns 11, which then becomes codice_59 on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.
Using an enumeration and a block to square the numbers 1 to 10 (using a "range"):
Or invoke a method on each item (codice_62 is a synonym for codice_63):
Classes.
The following code defines a class named codice_64. In addition to codice_65, the usual constructor to create new objects, it has two methods: one to override the codice_66 comparison operator (so codice_67 can sort by age) and the other to override the codice_68 method (so codice_69 can format its output). Here, codice_9 is an example of metaprogramming in Ruby: codice_71 defines getter and setter methods of instance variables, but codice_9 only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit codice_73 statement.
The preceding code prints three names in reverse age order:
codice_64 is a constant and is a reference to a codice_12 object.
Open classes.
In Ruby, classes are never closed: methods can always be added to an existing class. This applies to "all" classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's codice_76 class:
Adding methods to previously defined classes is often called [[monkey patch|monkey-patching]]. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.
Exceptions.
An exception is raised with a codice_77 call:
An optional message can be added to the exception:
Exceptions can also be specified by the programmer:
Alternatively, an exception instance can be passed to the codice_77 method:
This last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:
Exceptions are handled by the codice_79 clause. Such a clause can catch exceptions that inherit from codice_80. Other flow control keywords that can be used when handling exceptions are codice_81 and codice_82:
It is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:
Or catch particular exceptions:
It is also possible to specify that the exception object be made available to the handler clause:
Alternatively, the most recent exception is stored in the magic global codice_83.
Several exceptions can also be caught:
Metaprogramming.
Ruby code can programmatically modify, at [[Run time (program lifecycle phase)|runtime]], aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of [[metaprogramming]] can be used to write more concise code and effectively extend the language.
For example, the following Ruby code generates new methods for the built-in codice_84 class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.
The generated methods could then be used like this:
To implement the equivalent in many other languages, the programmer would have to write each method (codice_85, codice_86, codice_87, etc.) separately.
Some other possible uses for Ruby metaprogramming include:
More examples.
More sample Ruby code is available as algorithms in the following article:
Implementations.
Matz's Ruby Interpreter.
The official Ruby [[interpreter (computer software)|interpreter]] often referred to as the [[Ruby MRI|Matz's Ruby Interpreter]] or MRI. This implementation is written in C and uses its own Ruby-specific [[virtual machine]].
The standardized and retired Ruby 1.8 [[Ruby MRI|implementation]] was written in [[C (programming language)|C]], as a single-pass [[interpreted language]].
Starting with Ruby 1.9, and continuing with Ruby 2.0 and 2.1, the official Ruby interpreter has been [[YARV]] ("Yet Another Ruby VM"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.
Alternate implementations.
s of 2010[ [update]], there are a number of alternative implementations of Ruby, including [[JRuby]], [[Rubinius]], [[MagLev (software)|MagLev]], [[IronRuby]], [[MacRuby]] (and its iOS counterpart, [[RubyMotion]]), [[mruby]], [[HotRuby]], [[Topaz (Ruby implementation)|Topaz]] and [[Opal (Ruby implementation)|Opal]]. Each takes a different approach, with IronRuby, JRuby, MacRuby and Rubinius providing [[just-in-time compilation]] and MacRuby and mruby also providing [[ahead-of-time compilation]].
Ruby 1.9 has two major alternate implementations:
Other Ruby implementations include:
Other now defunct Ruby implementations were:
The maturity of Ruby implementations tends to be measured by their ability to run the [[Ruby on Rails]] (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called "the Rails singularity". The reference implementation [[Ruby MRI|(MRI)]], [[JRuby]], and [[Rubinius]] are all able to run Rails unmodified in a production environment. [[IronRuby]] is starting to be able to run Rails test cases, but is still far from being production-ready.
Platform support.
Matsumoto originally did Ruby development on the [[BSD|4.3BSD]]-based [[Sony NEWS|Sony NEWS-OS]] 3.x, but later migrated his work to [[SunOS]] 4.x, and finally to [[Linux]].
By 1999, Ruby was known to work across many different [[operating system]]s, including NEWS-OS, SunOS, [[AIX]], [[SVR4]], [[Sun Solaris|Solaris]], [[NEC]] [[UP-UX]], [[NeXTSTEP]], BSD, Linux, [[Mac OS]], [[DOS]], [[Microsoft Windows|Windows]], and [[BeOS]].
Modern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, [[Mac OS X]], Windows, [[Windows Phone]], [[Windows CE]], [[Symbian OS]], BeOS, and [[IBM i]].
Repositories and libraries.
[[RubyGems]] is Ruby's package manager. A Ruby package is called a "gem" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as [[integrated development environment|IDEs]]. There are over 70,000 Ruby gems hosted on .
Many new and existing Ruby libraries are hosted on [[GitHub]], a service that offers [[Revision control|version control]] repository hosting for [[Git (software)|Git]].
Further reading.
</dl>
External links.
[[Category:Commons category without a link on Wikidata]]
[[Category:Class-based programming languages]]
[[Category:Dynamically typed programming languages]]
[[Category:Scripting languages]]
[[Category:Object-oriented programming languages]]
[[Category:Articles with example Ruby code]]
[[Category:Ruby (programming language)| ]]
[[Category:Free software programmed in C]]
[[Category:Programming languages created in 1995]]
[[Category:Text-oriented programming languages]]
[[Category:Software using the BSD license]]
[[Category:ISO standards]]

</doc>
<doc id="25774" url="http://en.wikipedia.org/wiki?curid=25774" title="Render farm">
Render farm

A render farm is a high performance computer system, e.g. a computer cluster, built to render computer-generated imagery (CGI), typically for film and television visual effects.
This is different from a render wall, which is a networked, tiled display used for real-time rendering. The rendering of images is a highly parallelizable activity, as frames and sometimes tiles can be calculated independently of the others, with the main communication between processors being the upload of the initial source material, such as models and textures, and the download of the finished images.
Over the decades, advances in computer power would allow an image to take less time to render. However, the increased computation is instead used to meet demands to achieve state-of-the-art image quality. While simple images can be produced rapidly, more realistic and complicated higher-resolution images can now be produced in more reasonable amounts of time. The time spent producing images can be limited by production time-lines and deadlines, and the desire to create high-quality work drives the need for increased computing power, rather than simply wanting the same images created faster.
To manage large farms, one must introduce a "queue manager" that automatically distributes processes to the many processors. Each "process" could be the rendering of one full image, a few images, or even a sub-section (or "tile") of an image. The software is typically a client–server package that facilitates communication between the processors and the queue manager, although some queues have no central manager. Some common features of queue managers are: re-prioritization of the queue, management of software licenses, and algorithms to best optimize throughput based on various types of hardware in the farm. Software licensing handled by a queue manager might involve dynamic allocation of licenses to available CPUs or even cores within CPUs.
A tongue-in-cheek job title for systems engineers who work primarily in the maintenance and monitoring of a render farm is a "render wrangler" to further the "farm" theme. This job title can be seen in film credits.

</doc>
<doc id="25775" url="http://en.wikipedia.org/wiki?curid=25775" title="Render">
Render

Render, rendered, or rendering may refer to:

</doc>
<doc id="25776" url="http://en.wikipedia.org/wiki?curid=25776" title="Robert Borden">
Robert Borden

Sir Robert Laird Borden, PC GCMG KC (June 26, 1854 – June 10, 1937) was a Canadian lawyer and politician. He served as the eighth Prime Minister of Canada from October 10, 1911, to July 10, 1920, and was the third Nova Scotian to hold this office. After retiring from public life, he served as the chancellor of Queen's University. His portrait appears on Canadian $100 bills produced since 1976.
Early life and career.
Robert Laird Borden was born and educated in Grand-Pré, Nova Scotia, a farming community at the eastern end of the Annapolis Valley, where his great-grandfather Perry Borden, Sr. of Tiverton, Rhode Island had taken up Acadian land in 1760. Perry had accompanied his father, Samuel Borden, the chief surveyor chosen by the government of Massachusetts to survey the former Acadian land and draw up new lots for the Planters in Nova Scotia. Robert Borden was the last Canadian Prime Minister born before Confederation.
Borden's father Andrew Borden was judged by his son to be "a man of good ability and excellent judgement", of a "calm, contemplative and philosophical" turn of mind, but "he lacked energy and had no great aptitude for affairs". His mother Eunice Jane Laird was more driven, possessing "very strong character, remarkable energy, high ambition and unusual ability". Her ambition was transmitted to her first-born child, who applied himself to his studies while assisting his parents with the farm work he found so disagreeable. His cousin Sir Frederick Borden was a prominent Liberal politician.
Lawyer.
From 1868 to 1874, he worked as a teacher in Grand-Pré and Matawan, New Jersey. Seeing no future in teaching, he returned to Nova Scotia in 1874. Despite have no formal university education, he went to article for four years at a Halifax law firm. In August 1878, he was called to the Nova Scotia Bar, placing first in the bar examinations. Borden went to Kentville, Nova Scotia as the junior partner of the Conservative lawyer John P. Chipman. In 1880, he was inducted into the Freemasons - St Andrew's lodge #1.
In 1882, he was asked by Wallace Graham to move to Halifax and join the Conservative law firm headed by Graham and 
Charles Hibbert Tupper. In the Autumn of 1889, when he was only 35, Borden became the senior partner following the departure of Graham and Tupper for the bench and politics, respectively. His financial future guaranteed, on September 25, 1889, he married Laura Bond (1863–1940), the daughter of a Halifax hardware merchant. They would have no children. In 1894, he bought a large property and home on the south side of Quinpool Road, which the couple called "Pinehurst". In 1893, Borden successfully argued the first of two cases which he took to the Judicial Committee of the Privy Council. He represented many of the important Halifax businesses, and sat on the boards of Nova Scotian companies including the Bank of Nova Scotia and the Crown Life Insurance Company. In 1896, he became President of the Nova Scotia Barristers' Society, and took the initiative in organizing the founding meetings of the Canadian Bar Association in Montreal within the same year. By the time he was prevailed upon to enter politics, Borden had what some judged to be the largest legal practice in the Maritime Provinces, and had become a wealthy man.
Conservative Party in opposition.
Borden was a Liberal until he broke with the party in 1891 over the issue of Reciprocity.
He was elected to Parliament in the 1896 federal election as a Conservative and in 1901 was selected by the Conservative caucus to succeed Sir Charles Tupper as leader of the Conservative Party. He was defeated in his Halifax seat in the 1904 federal election and re-entered the House of Commons the next year via a by-election in Carleton. Over the next decade he worked to rebuild the party and establish a reform policy, the Halifax Platform of 1907 which he described as "the most advanced and progressive policy ever put forward in Federal affairs". It called for reform of the Senate and the civil service, a more selective immigration policy, free rural mail delivery, and government regulation of telegraphs, telephones, and railways and eventually national ownership of telegraphs and telephones. Despite his efforts, his party lost the 1908 federal election to Wilfrid Laurier's Liberals. His party's fortunes turned around in the 1911 federal election, however, when the Conservatives successfully campaigned against Laurier's proposals for a Reciprocity (free trade) agreement with the United States. Borden countered with a revised version of John A. Macdonald's National Policy and appeals of loyalty to the British Empire and ran on the slogan "Canadianism or Continentalism". In British Columbia, the party ran on the slogan "A White Canada," playing to the fears of British Columbians that resented the increasing presence of cheap Asian labour and the resulting depression in wages. In Quebec, concurrently, Henri Bourassa led a campaign against what he saw as Laurier's capitulation to British imperialism, playing a part in the defeat of Laurier's government and the election of Borden's Tories.
Prime Minister (1911-1920).
First World War.
As Prime Minister of Canada during the First World War, he transformed his government to a wartime administration, passing the "War Measures Act" in 1914. Borden committed Canada to provide half a million soldiers for the war effort. However, volunteers had quickly dried up when Canadians realized there would be no quick end to the war. Borden's determination to meet that huge commitment led to the "Military Service Act" and the Conscription Crisis of 1917, which split the country on linguistic lines. In 1917 Borden recruited members of the Liberals (with the notable exception of leader Wilfrid Laurier) to create a Unionist government. The 1917 election saw the "Government" candidates (including a number of Liberal-Unionists) crush the Opposition "Laurier Liberals" in English Canada resulting in a large parliamentary majority for Borden.
Sir Robert Borden pledged himself during the campaign to equal suffrage for women. With his return to power, he introduced a bill in 1918 for extending the franchise to women. This passed without division.
The war effort also enabled Canada to assert itself as an independent power. Borden wanted to create a single Canadian army, rather than have Canadian soldiers split up and assigned to British divisions as had happened during the Boer War. Sam Hughes, the Minister of Militia, generally ensured that Canadians were well-trained and prepared to fight in their own divisions, although with mixed results such as the Ross Rifle. Arthur Currie provided sensible leadership for the Canadian divisions in Europe, although they were still under overall British command. Nevertheless, Canadian troops proved themselves to be among the best in the world, fighting at the Somme, Ypres, Passchendaele, and especially at the Battle of Vimy Ridge.
During Borden's first term as prime minister, the National Research Council of Canada was established in 1916.
Borden and the Treaty of Versailles.
In world affairs, Borden played a crucial role in transforming the British Empire into a partnership of equal states, the Commonwealth of Nations, a term that was first discussed at an Imperial Conference in London during the war. Borden also introduced the first Canadian income tax, which at the time was meant to be temporary, but was never repealed.
Convinced that Canada had become a nation on the battlefields of Europe, Borden demanded that it have a separate seat at the Paris Peace Conference. This was initially opposed not only by Britain but also by the United States, who perceived such a delegation as an extra British vote. Borden responded by pointing out that since Canada had lost a far larger proportion of its men compared to the U.S. in the war (although not more in absolute numbers), Canada at least had the right to the representation of a "minor" power. British Prime Minister David Lloyd George eventually relented, and convinced the reluctant Americans to accept the presence of separate Canadian, Indian, Australian, Newfoundland, New Zealand and South African delegations. Despite this, Borden boycotted the opening ceremony, protesting at the precedence given to the prime minister of the much smaller Newfoundland over him.
Not only did Borden's persistence allow him to represent Canada in Paris as a nation, it also ensured that each of the dominions could sign the Treaty of Versailles in its own right, and receive a separate membership in the League of Nations. During the conference Borden tried to act as an intermediary between the United States and other members of the British Empire delegation, particularly Australia and New Zealand over the issue of Mandates. Borden also discussed with Lloyd George, the possibility of Canada taking over the administration of Belize and the West Indies, but no agreement was reached.
At Borden's insistence, the treaty was ratified by the Canadian Parliament. Borden was the last prime minister to be knighted after the House of Commons indicated its desire for the discontinuation of the granting of any future titles to Canadians in 1919 with the adoption of the Nickle Resolution
Post-war government.
That same year, Borden approved the use of troops to put down the Winnipeg General Strike, which was feared to be the result of Bolshevik agitation from the Soviet Union.
Post-political career.
Sir Robert Borden retired from office in 1920. He was the Chancellor of Queen's University from 1924 to 1930 and also was Chancellor of McGill University from 1918 to 1920 while still Prime Minister. Borden also served as Vice-President of The Champlain Society between 1923 and 1925. He was the Society's first Honorary President between 1925 and 1938. Borden's successor Arthur Meighen was defeated by the new Liberal leader William Lyon Mackenzie King in the 1921 election. Nevertheless, Borden would go on to represent Canada once more on the international stage when he attended the Washington Naval Conference in 1922 and signed the resulting arms reduction treaty on Canada's behalf.
At the time of his death, Borden stood as president of two financial institutions: Barclays Bank of Canada and the Crown Life Insurance Company. Borden died on June 10, 1937, in Ottawa and is buried in the Beechwood Cemetery marked by a simple stone cross.
Family.
Robert Laird Borden married Laura Bond, youngest daughter of the late T. H. Bond, September 1889. She served as president of the Local Council of Women of Halifax, until her resignation in 1901. She served as President of the Aberdeen Association, Vice-President of the Women's Work Exchange in Halifax, and Corresponding Secretary of the Associated Charities of the United States. He is a distant relative of American accused murderer Lizzie Borden.
Supreme Court appointments.
Borden chose the following jurists to sit as justices of the Supreme Court of Canada:
References.
By Sir Robert
By others
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="25781" url="http://en.wikipedia.org/wiki?curid=25781" title="Robot">
Robot

A robot is a mechanical or virtual artificial agent, usually an electro-mechanical machine that is guided by a computer program or electronic circuitry. Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's "Advanced Step in Innovative Mobility" (ASIMO) and TOSY's "TOSY Ping Pong Playing Robot" (TOPIO) to industrial robots, collectively programmed "swarm" robots, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own.
The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: Soft robotics.
From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control. 
The word 'robot' was first used to denote fictional humanoid in a 1921 play "R.U.R." by the Czech writer, Karel Čapek. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948. The first digital and programmable robot was invented by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.
Robots have replaced humans in the assistance of performing those repetitive and dangerous tasks which humans prefer not to do, or are unable to do due to size limitations, or even those such as in outer space or at the bottom of the sea where humans could not survive the extreme environments.
There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.
Summary.
The word "robot" can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to do some or all of the following: move around, operate a mechanical limb, sense and manipulate their environment, and exhibit intelligent behavior — especially behavior which mimics humans or other animals. In practical terms, "robot" usually refers to a machine which can be electronically programmed to carry out a variety of physical tasks or actions.
There is no one definition of "robot" that satisfies everyone and many people have their own. For example Joseph Engelberger, a pioneer in industrial robotics, once remarked: "I can't define a robot, but I know one when I see one." The two ways that robots differ from actual beings are, simply stated, in the domain of cognition, and in the domain of biological form. The general consensus is that a "robot" is a machine and not a being simply because it is not intelligent (it requires programming to function), regardless of how human-like it may appear. In contrast, an imaginary "machine" or "artificial life form" (as in science fiction) that could think near or above human intelligence, and had a sensory body, would no longer be a "robot" but would be some kind of "artificial being" or "cognitive robot", (see also cyborg).
According to the "Encyclopaedia Britannica", a robot is "any automatically operated machine that replaces human effort, though it may not resemble human beings in appearance or perform functions in a humanlike manner." Merriam-Webster describes a robot as a "machine that looks like a human being and performs various complex acts (as walking or talking) of a human being", or a "device that automatically performs complicated often repetitive tasks", or a "mechanism guided by automatic controls".
History.
The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the "Lie Zi".
Early beginnings.
Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.
In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures." In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.
The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. 
In ancient China, the 3rd century text of the "Lie Zi" describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the "Han Fei Zi" and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds ("ma yuan") that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.
The beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.
In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his "Vitruvian Man". It is not known whether he attempted to build it.
In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century "Karakuri zui" ("Illustrated Machinery", 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the "Butai karakuri", which were used in theatre, the "Zashiki karakuri", which were small and used in homes, and the "Dashi karakuri" which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.
In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.
Remote-controlled systems.
Remotely operated vehicles were demonstrated in the late 19th Century in the form of several types of remotely controlled torpedos. The early 1870s saw remotely controlled torpedos by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).
The Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first "practical" guided missile". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.
Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.
Humanoid robots.
The term 'robot' was first used to denote fictional automata in a 1921 play "R.U.R." by the Czech writer, Karel Čapek. The word 'robot' is of Czech origin.
In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot Eric's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.
Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.
Modern autonomous robots.
The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named "Elmer" and "Elsie", were constructed between 1948 and 1949 and were often described as "tortoises" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.
Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's "turtles" may be found in the form of BEAM robotics.
The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol’s patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.
The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.
Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.
Etymology.
The word "robot" was introduced to the public by the Czech interwar writer Karel Čapek in his play "R.U.R. (Rossum's Universal Robots)", published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called "robots." The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieves self-awareness and incites robots all around the world to rise up against the humans).
Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the "Oxford English Dictionary" in which he named his brother, the painter and writer Josef Čapek, as its actual originator.
In an article in the Czech journal "Lidové noviny" in 1933, he explained that he had originally wanted to call the creatures "laboři" ("workers", from Latin "labor"). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word "robota" means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as "robot" in Hungarian). Traditionally the "robota" (Hungarian "robot") was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) "rabota" "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root "*orbh-". "Robot" is cognate with the German root "Arbeit" (work).
The word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the "Three Laws of Robotics" which are a recurring theme in his books. These have since been used by many others to define laws used in fact and fiction.
Modern robots.
Mobile robot.
Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the "automated guided vehicle" or "automatic guided vehicle" (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.
Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.
Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.
Industrial robots (manipulating).
Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.
The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:
"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."
This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.
Service robot.
Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."
Educational robot.
Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.
There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.
There have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.
Modular robot.
Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots.
Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These “ANAT robots” can be designed with “n” DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases.
Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.
Modular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.
Collaborative robots.
A "collaborative robot" or "cobot" is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.
The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.
Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. s of May 2014[ [update]], 190 companies in the US have bought Baxters and they are being used commercially in the UK.
Robots in society.
Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.
Autonomy and ethical questions.
As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called "Plug & Pray".
Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.
In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns.
Military robots.
Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots.
The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.
One robot in particular, the EATR, has generated public concerns
 over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.
Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.
Relationship to unemployment.
A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.
Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to revise redundancy laws.
Contemporary uses.
At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.
Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. Of course, all robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.
General-purpose autonomous robots.
General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.
Factory robots.
Car production.
Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.
Packaging.
Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.
Electronics.
Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.
Automated guided vehicles (AGVs).
Mobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.
Early AGV-style robots.
Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.
Interim AGV technologies.
Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.
Intelligent AGVs (i-AGVs).
Such as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.
Dirty, dangerous, dull or inaccessible tasks.
There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.
Space probes.
Almost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, and others.
Telerobots.
Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).
Automated fruit harvesting machines.
Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.
Domestic robots.
Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.
Military robots.
Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.
Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.
The AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.
Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies." Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.
Mining robots.
Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous vehicle fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia.
Drilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.
Healthcare.
Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.
Home automation for the elderly and disabled.
Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.
The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.
FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.
Pharmacies.
Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient’s medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it’s the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.
McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.
Research robots.
While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.
Bionic and biomimetic robots.
One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.
Nanorobots.
Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.
Reconfigurable robots.
A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.
Soft robots.
Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids), controlled using fuzzy logic and neural networks, look and feel different from robots with rigid skeletons, and can have different behaviors.
Swarm robots.
Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.
Haptic interface robots.
Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.
Entertainment.
Some robots are used for entertainment and as a demonstration of the newest technology. This nimble automoton is a perfect example of this process. Being the main attractions at Ce-BIT, the world’s biggest IT trade fair in Hanover, Germany.
Future development.
Technological trends.
Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions.
Technological development.
Overall trends.
Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.
As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.
New functions and abilities.
Caterpillar Inc. is making a dump truck which can drive itself without any human operator.
Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.
As early as 1982 people were confident that someday robots would:
1. clean parts by removing molding flash
2. spray paint automobiles with absolutely no human presence
3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes
4. make electrical cable harness
5. load trucks with boxes—a packing problem
6. handle soft goods, such as garments and shoes
7. shear sheep
8. prosthesis
9. cook fast food and work in other service industries
10. household robot.
Generally such predictions are overly optimistic in timescale.
Reading robot.
A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.
Robots in popular culture.
Literature.
Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.
The first reference in Western literature to mechanical servants appears in Homer's "Iliad". In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." Of course, the words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).
Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.
According to the "Oxford English Dictionary," the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word "robotics". Asimov was not initially aware of this; he assumed the word already existed by analogy with "mechanics," "hydraulics," and other similar terms denoting branches of applied knowledge.
Problems depicted in popular culture.
Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. (See "The Terminator, Runaway, RoboCop", the Replicators in "Stargate", the Cylons in "Battlestar Galactica", the Cybermen in "Doctor Who", "The Matrix", "Enthiran" and "I, Robot".) Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are "", "Red Planet" and "Enthiran". Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely. "Frankenstein" (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or monster advancing beyond its creator. In the TV show, Futurama, the robots are portrayed as humanoid figures that live alongside humans, not as robotic butlers. They still work in industry, but these robots carry out daily lives. Other problems may include events pertaining to robot surrogates (e.g. the movie "Surrogates") where tissue of living organisms is interchanged with robotic systems. These problems can leave many possibilities where electronic viruses or an electro magnetic pulse (EMP) can destroy not only the robot but kill the host/operator as well.

</doc>
<doc id="25783" url="http://en.wikipedia.org/wiki?curid=25783" title="R. B. Bennett">
R. B. Bennett

Richard Bedford Bennett, 1st Viscount Bennett, PC KC FRSA (3 July 1870 – 26 June 1947) was a Canadian lawyer, businessman, politician, and philanthropist. He served as the 11th Prime Minister of Canada from 7 August 1930, to 23 October 1935, during the worst of the Great Depression years. Following his defeat as prime minister, Bennett moved to England, and was elevated to the peerage as Viscount Bennett.
Early life.
R. B. Bennett was born on 3 July 1870, when his mother, Henrietta Stiles, was visiting at her parents' home in Hopewell Hill, New Brunswick, Canada. He grew up nearby at the home of his father, Henry John Bennett, at Hopewell Cape, the shire town of Albert County, then a town of 1,800 people.
His father was descended from English ancestors who had emigrated to Connecticut in the 17th century. His great-great-grandfather Bennett migrated from Connecticut to Nova Scotia c. 1765, before the American Revolution, taking the lands forcibly removed from the deported Acadians during the Great Upheaval.
R. B. Bennett's family was poor, subsisting mainly on the produce of a small farm. His early days inculcated a lifelong habit of thrift. The driving force in his family was his mother. She was a Wesleyan Methodist and passed this faith and the Protestant ethic on to her son. His principle ever after was: work as hard as you can, earn all you can, save all you can, and then give all you can. Bennett's father does not appear to have been a good provider for his family, though the reason is unclear. He operated a general store for a while and tried to develop some gypsum deposits.
The Bennetts had previously been a relatively prosperous family, operating a shipyard in Hopewell Cape, but the change to steam-powered vessels in the mid-19th century meant the gradual winding down of their business. However, the household was a literate one, subscribing to three newspapers. They were strong Conservatives; indeed one of the largest and last ships launched by the Bennett shipyard (in 1869) was the "Sir John A. Macdonald".
Educated in the local school, Bennett was a good student, but something of a loner. In addition to his Protestant faith, Bennett grew up with an abiding love of the British Empire, then at its apogee.
Some important friendships.
One day, while Bennett was crossing the Miramichi River on the ferry boat, a well-dressed lad about nine years younger came over to him and struck up a conversation. This was the beginning of an improbable but important friendship with Max Aitken, later the industrialist and British press baron, Lord Beaverbrook. The agnostic Aitken liked to tease the Methodist Bennett, whose fiery temper contrasted with Aitken's ability to turn away wrath with a joke. This friendship would become important to his success later in life, as would his friendship with the Chatham lawyer, Lemuel J. Tweedie, a prominent Conservative politician. He began to study law with Tweedie on weekends and during summer holidays. Another important friendship was with the prominent Shirreff family of Chatham, the father being High Sheriff of Northumberland County for 25 years. The son, Harry, joined the E.B. Eddy Company, a large pulp and paper industrial concern, and was transferred to Halifax. His sister moved there to study nursing, and soon Bennett joined them to study law at Dalhousie University. Their friendship was renewed there, and became crucial to his later life when Jennie Shirreff married the head of the Eddy Company. She later made Bennett the lawyer for her extensive interests.
University, early legal career.
Bennett started at Dalhousie University in 1890, graduating in 1893 with a law degree. He worked his way through with a job as assistant in the library, being recommended by Dr. R. C. Weldon.
He was then a partner in the Chatham law firm of Tweedie and Bennett. Max Aitken (later known as Lord Beaverbrook) was his office boy, while articling as a lawyer, acting as a stringer for the Montreal Gazette, and selling life insurance. Aitken persuaded him to run for alderman in the first Town Council of Chatham, and managed his campaign. Bennett was elected by one vote, and was later furious with Aitken when he heard all the promises he had made on Bennett's behalf.
Moving west.
Despite his election to the Chatham town council, Bennett's days in the town were numbered. He was ambitious and saw that the small community was too narrow a field for him. He was already negotiating with Sir James Lougheed to move to the North-West Territories and become his law partner in Calgary. Lougheed was Calgary's richest man and most successful lawyer.
Bennett moved to Calgary in 1897. A lifelong bachelor and teetotaler (although Bennett was known by select associates to occasionally drink alcohol when the press was not around to observe this), he led a rather lonely life in a hotel and later, in a boarding house. For a while a younger brother roomed with him. He ate his noon meal on workdays at the Alberta Hotel. Social life, such as it was, centred on church. There was, however, no scandal attached to his personal life. Bennett worked hard and gradually built up his legal practice. In 1908 he was one of five people appointed to the first Library Board for the city of Calgary and was instrumental in establishing the Calgary Public Library.
In 1910, Bennett became a director of Calgary Power Ltd. (now formally TransAlta Corporation) and just a year later he became President. During his leadership projects completed included the first storage reservoir at Lake Minnewanka, a second transmission line to Calgary and the construction of the Kananaskis Falls hydro station. At that time, he was also director of Rocky Mountains Cement Company and Security Trust.
Bennett developed an extensive legal practice in Calgary. In 1929-30, he served as national President of the Canadian Bar Association. His successor in that office was Louis St. Laurent, another future Prime Minister.
Early political career.
He was elected to the Legislative Assembly of the North-West Territories in the 1898 general election, representing the riding of West Calgary. He was re-elected to a second term in office in 1902 as an Independent in the North-West Territories legislature.
In 1905, when Alberta was carved out of the territories and made a province, Bennett became the first leader of the Alberta Conservative Party. In 1909, he won a seat in the provincial legislature, before switching to federal politics.
Elected to the Canadian House of Commons in 1911, Bennett returned to the provincial scene to again lead the Alberta Tories in the 1913 provincial election, but kept his federal seat in Ottawa when his Tories failed to take power in the province; such practice was later forbidden.
At age 44, he tried to enlist in the Canadian military once World War I broke out, but was turned down as being medically unfit. In 1916, Bennett was appointed director general of the National Service Board, which was in charge of identifying the number of potential recruits in the country.
While Bennett supported the Conservatives, he opposed Prime Minister Robert Borden's proposal for a Union Government that would include both Conservatives and Liberals, fearing that this would ultimately hurt the Conservative Party. While he campaigned for Conservative candidates in the 1917 federal election he did not stand for re-election himself.
Cabinet minister, Conservative party leader.
Nevertheless, Borden's successor, Arthur Meighen appointed Bennett Minister of Justice in his government, as it headed into the 1921 federal election in which both the government and Bennett were defeated. Bennett won the seat of Calgary West in the 1925 federal election and was returned to government as Minister of Finance in Meighen's short-lived government in 1926. The government was defeated in the 1926 federal election. Meighen stepped down as Tory leader, and Bennett became the party's leader in 1927 at the first Conservative leadership convention.
As Opposition leader, Bennett faced off against the more experienced Liberal Prime Minister William Lyon Mackenzie King in Commons debates, and took some time to acquire enough experience to hold his own with King. In 1930, King blundered badly when he made overly partisan statements in response to criticism over his handling of the economic downturn, which was hitting Canada very hard. King's worst error was in stating that he "would not give Tory provincial governments a five-cent piece!" This serious mistake, which drew wide press coverage, gave Bennett his needed opening to attack King, which he did successfully in the election campaign which followed.
Prime minister (1930-1935).
Confronting the Depression.
By defeating William Lyon Mackenzie King in the 1930 federal election, he had the misfortune of taking office during the Great Depression. Bennett tried to combat the depression by increasing trade within the British Empire and imposing tariffs for imports from outside the Empire, promising that his measures would "blast" Canadian exports into world markets. His success was limited however, and his own wealth (often openly displayed) and impersonal style alienated many struggling Canadians.
When his "Imperial Preference" policy failed to generate the desired result, Bennett's government had no real contingency plan. The party's pro-business and pro-banking inclinations provided little relief to the millions of increasingly desperate and agitated unemployed. Despite the economic crisis, "laissez-faire" persisted as the guiding economic principle of Conservative Party ideology. Government relief to the unemployed was considered a disincentive to individual initiative, and was therefore only granted in the most minimal amounts and attached to work programs. An additional concern of the federal government was that large numbers of disaffected unemployed men concentrating in urban centres created a volatile situation. As an "alternative to bloodshed on the streets," the stop-gap solution for unemployment chosen by the Bennett government was to establish military-run and -styled relief camps in remote areas throughout the country, where single unemployed men toiled for twenty cents a day. Any relief beyond this was left to provincial and municipal governments, many of which were either insolvent or on the brink of bankruptcy, and which railed against the inaction of other levels of government. Partisan differences began to sharpen on the question of government intervention in the economy, since lower levels of government were largely in Liberal hands, and protest movements were beginning to send their own parties into the political mainstream, notably the Cooperative Commonwealth Federation and William Aberhart's Social Credit Party in Alberta.
Hosts, dominates 1932 Imperial Conference.
Bennett hosted the 1932 Imperial Economic Conference in Ottawa; this was the first time Canada had hosted the meetings. It was attended by the leaders of the independent dominions of the British Empire (which later became the Commonwealth of Nations). Bennett dominated the meetings, which were ultimately unproductive, due to the inability of leaders to agree on policies, mainly to combat the economic woes dominating the world at the time.
Anti-Communism.
A nickname that would stick with Bennett for the remainder of his political career, "Iron Heel Bennett," came from a 1932 speech he gave in Toronto that ironically, if unintentionally, alluded to Jack London's socialist novel:
What do they offer you in exchange for the present order? Socialism, Communism, dictatorship. They are sowing the seeds of unrest everywhere. Right in this city such propaganda is being carried on and in the little out of the way places as well. And we know that throughout Canada this propaganda is being put forward by organizations from foreign lands that seek to destroy our institutions. And we ask that every man and woman put the iron heel of ruthlessness against a thing of that kind. 
Reacting to fears of Communist subversion, Bennett invoked the controversial Section 98 of the Criminal Code of Canada. Enacted in the aftermath of the Winnipeg General Strike, Section 98 dispensed with the presumption of innocence in outlawing potential threats to the state: specifically, anyone belonging to an organization that officially advocated the violent overthrow of the government. Even if the accused had never committed an act of violence or personally supported such an action, they could be incarcerated merely for attending meetings of such an organization, publicly speaking in its defense, or distributing its literature. Despite the broad power authorized under Section 98, it targeted specifically the Communist Party of Canada. Eight of the top party leaders, including Tim Buck, were arrested and convicted under Section 98 in 1931. This plan to stamp out communism backfired, however, and proved to be a damaging embarrassment for the government, especially after Buck was the target of an apparent assassination attempt. While confined to his cell during a prison riot, despite not participating in the riot, shots were fired into his cell. When an agit-prop play depicting these events, "Eight Men Speak", was suppressed by the Toronto police, a protest meeting was held where activist A.E. Smith repeated the play's allegations, and he was consequently arrested for sedition. This created a storm of public protest, compounded when Buck was called as a witness to the trial and repeated the allegations in open court. Although the remarks were stricken from the record, they still discredited the prosecution's case and Smith was acquitted. As a result, the government's case against Buck lost any credibility, and Buck and his comrades were released early and fêted as heroic champions of civil liberties.
A 2001 book by Quebec nationalist writer Normand Lester, "Le Livre noir du Canada anglais" (later translated as "The Black Book of English Canada") accused Bennett of having a political affiliation with, and of having provided financial support to, fascist Quebec writer Adrien Arcand. This is based on a series of letters sent to Bennett following his election as Prime Minister by Arcand, his colleague Ménard and two Conservative caucus members asking for financial support for Arcand's antsemitic newspaper "Le Goglu". The book also claims that in a 1936 letter to Bennett, A. W. Reid, a Conservative organizer, estimated that Conservative Party members gave Arcand a total of $27,000 (the modern equivalent $359,284).
Relief camp protest.
Having survived Section 98, and benefiting from the public sympathy wrought by persecution, Communist Party members set out to organize workers in the relief camps. Camp workers laboured on a variety of infrastructure projects, including such things as municipal airports, roads, and park facilities, along with a number of make-work schemes. Conditions in the camps were abhorrent, not only because of the low pay, but the lack of recreational facilities, isolation from family and friends, poor quality food, and the use of military discipline, which made the camps feel like penal colonies. Communists thus had ample grounds on which to organize camp inmates. The Relief Camp Workers' Union was formed and affiliated with the Workers' Unity League, the trade union umbrella of the Communist Party. Camp workers in BC struck on 4 April 1935, and, after two months of protesting in Vancouver, began the On-to-Ottawa Trek to bring their grievances to Bennett's doorstep. The Prime Minister and his Minister of Justice, Hugh Guthrie, treated the trek as an attempted insurrection, and ordered it to be stopped. The Royal Canadian Mounted Police (RCMP) halted the Trek in Regina on 1 July 1935, by attacking a crowd of 3,000 strikers and their supporters, resulting in two deaths and dozens of injured. All told, Bennett's anti-Communist policy would not bode well for his political career.
Bennett's New Deal.
Following the lead of President Roosevelt's New Deal in the United States, Bennett, under the advice of William Duncan Herridge, who was Canada's Envoy to the United States, the government eventually began to follow the Americans' lead. In a series of five speeches to the nation in January 1935, Bennett introduced a Canadian version of the "New Deal," involving unprecedented public spending and federal intervention in the economy. Progressive income taxation, a minimum wage, a maximum number of working hours per week, unemployment insurance, health insurance, an expanded pension programme, and grants to farmers were all included in the plan.
In one of his addresses to the nation, Bennett said:
"In the last five years great changes have taken place in the world... The old order is gone. We are living in conditions that are new and strange to us. Canada on the dole is like a young and vigorous man in the poorhouse ... If you believe that things should be left as they are, you and I hold contrary and irreconcilable views. I am for reform. And in my mind, reform means government intervention. It means government control and regulation. It means the end of laissez-faire."
Bennett's conversion, however, was seen as too little too late, and he faced criticism that his reforms either went too far, or did not go far enough, including from one of his cabinet ministers H.H. Stevens, who bolted the government to form the Reconstruction Party of Canada. Some of the measures were alleged to have encroached on provincial jurisdictions laid out in Section 92 of the British North America Act. The courts, including the Judicial Committee of the Privy Council, agreed and eventually struck down virtually all of Bennett's reforms. However, some of Bennett's initiatives, such as the Bank of Canada, which he founded in 1934, remain in place to this day, and the Canadian Wheat Board remained in place until 2011 when the government of Stephen Harper abolished it.
Defeat.
Although there was no unity among the motley political groups that constituted Bennett's opposition, a consensus emerged that his handling of the economic crisis was insufficient and inappropriate, even from Conservative quarters. Bennett personally became a symbol of the political failings underscoring the depression. Car owners, for example, who could no longer afford gasoline, had horses pull their vehicles, named them Bennett Buggies. Unity in his own administration suffered, notably by the defection of his Minister of Trade, Henry Herbert Stevens. Stevens left the Conservatives and formed the Reconstruction Party of Canada, after Bennett refused to implement Stevens' plan for drastic economic reform to deal with the economic crisis.
The beneficiary of the overwhelming opposition during Bennett's tenure was the Liberal Party. The Tories were decimated in the October 1935 general election, winning only 40 seats to 173 for Mackenzie King's Liberals. The Tories would not form a majority government again in Canada until 1958. King's government soon implemented its own moderate reforms, including the replacement of relief camps with a scaled down provincial relief project scheme, and the repeal of Section 98. King had earlier outlined his plans with his 1918 book "Industry and Economy". Many of King's other reforms continue today, including the Canadian Broadcasting Corporation, the nationalized Bank of Canada, versions of minimum wage, maximum hours of work, pension, and unemployment insurance legislation. Ultimately, Canada pulled out of the depression as a result of government-funded jobs associated with the preparation for and onset of the Second World War.
Coat of arms.
Bennett's Coat of Arms was designed by Alan Beddoe "Argent within two bendlets Gules three maple leaves proper all between two demi-lions rampant couped gules. Crest, a demi-lion Gules grapsing in the dexter paw a battle axe in bend sinister Or and resting the sinister paw on an escallop also Gules. Supporters, Dexter a buffalo, sinister a moose, both proper. Motto, To be Pressed not Oppressed." 
Legacy.
While Bennett was, and is still, often criticized for lack of compassion for the impoverished masses, he stayed up through many nights reading and responding to personal letters from ordinary citizens asking for his help, and often dipped into his personal fortune to send a five-dollar bill to a starving family. The total amount he gave personally is uncertain, although he personally estimated that between the years of 1927-37 he spent well over 2.3 million dollars. Bennett was a controlling owner of the E.B. Eddy match company, which was the largest safety match manufacturer in Canada, and he was one of the richest Canadians at that time. Bennett helped put many poor, struggling young men through university. Relative to the times he lived in, he was likely the wealthiest Canadian to become prime minister.
Bennett worked an exhausting schedule throughout his years as prime minister, often more than 14 hours per day, and dominated his government, usually holding several cabinet posts. He lived in a suite in the Chateau Laurier hotel, a short walk from Parliament Hill. The respected author Bruce Hutchison wrote that had the economic times been more normal, Bennett would likely have been regarded as a good, perhaps great, Canadian prime minister.
Bennett was also a noted talent spotter. He took note of and encouraged the young Lester Pearson in the early 1930s, and appointed Pearson to significant roles on two major government inquiries: the 1931 Royal Commission on Grain Futures, and the 1934 Royal Commission on Price Spreads. Bennett saw that Pearson was recognized with an O.B.E. after he shone in that work, arranged a bonus of $1,800, and invited him to a London conference. Former Prime Minister John Turner, who as a child knew Bennett while he was prime minister, praised Bennett's promotion of Turner's economist mother to the highest civil service post held by a Canadian woman to that time.
Retirement and death.
Bennett retired to Britain in 1938, and, on 12 June 1941, became the first and only former Canadian Prime Minister to be elevated to the peerage as Viscount Bennett of Mickleham in the County of Surrey and of Calgary and Hopewell in the Dominion of Canada. Bennet's interest increasing public awareness and accessibility to Canada's historical records, led him to serving as Vice-President of The Champlain Society from 1933 until his death.
He died after suffering a heart attack while taking a bath on 26 June 1947, at Mickleham. He was exactly one week shy of his 77th birthday. He is buried there in St. Michael's Churchyard, Mickleham (the tomb and Government Canada marker outside and steps from the front doors of the church). He is the only deceased former Canadian Prime Minister not buried in Canada. Unmarried, Bennett was survived by nephews William Herridge, Jr., and Robert Coats and by brother Ronald V. Bennett. The viscountcy became extinct on his death.
Bennett was ranked #12 by a survey of Canadian historians out of the then 20 Prime Ministers of Canada through Jean Chrétien. The results of the survey were included in the book "Prime Ministers: Ranking Canada's Leaders" by J.L. Granatstein and Norman Hillmer.
Supreme Court appointments.
Bennett chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:
Other appointments.
Bennett was the Honorary Colonel of The Calgary Highlanders from the year of their designation as such in 1921 to his death in 1947. He visited the Regiment in England during the Second World War, and always ensured the 1st Battalion had a turkey dinner at Christmas every year they were overseas, including the Christmas of 1944 when the battalion was holding front line positions in the Nijmegen Salient.
Bennett served as the Rector of Queen's University in Kingston, Ontario from 1935 to 1937, even while he was still prime minister. At the time, this role covered mediation for significant disputes between Queen's students and the university administration.
Electoral record.
Northwest Territories (West Calgary).
The by-election was caused by the resignation of Richard Bennett, who resigned his seat to run for the Canadian House of Commons in the 1900 Canadian federal election.
Federal elections.
Calgary West.
 
Further reading.
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="25784" url="http://en.wikipedia.org/wiki?curid=25784" title="Renewable energy">
Renewable energy

Renewable energy is generally defined as energy that comes from resources which are naturally replenished on a human timescale such as sunlight, wind, rain, tides, waves and geothermal heat. Renewable energy replaces conventional fuels in four distinct areas: electricity generation, hot water/space heating, motor fuels, and rural (off-grid) energy services.
Based on REN21's 2014 report, renewables contributed 19 percent to our energy consumption and 22 percent to our electricity generation in 2012 and 2013, respectively. Both, modern renewables, such as hydro, wind, solar and biofuels, as well as traditional biomass, contributed in about equal parts to the global energy supply. Worldwide investments in renewable technologies amounted to more than US$214 billion in 2013, with countries like China and the United States heavily investing in wind, hydro, solar and biofuels.
Renewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond.
While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.
Overview.
Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:
Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources.
Wind power is growing at the rate of 30% annually, with a worldwide installed capacity of 282,482 megawatts (MW) at the end of 2012, and is widely used in Europe, Asia, and the United States. At the end of 2012 the photovoltaic (PV) capacity worldwide was 100,000 MW, and PV power stations are popular in Germany and Italy. Solar thermal energy stations operate in the USA and Spain, and the largest of these is the 354 MW Solar Energy Generating Systems power plant in the Mojave Desert. The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the USA.
Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits.
Renewable energy replaces conventional fuels in four distinct areas: electricity generation, hot water/space heating, motor fuels, and rural (off-grid) energy services:
As of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.
At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.
Climate change and global warming concerns, coupled with high oil prices, peak oil, and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors. According to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment.
Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the sun is expected to make the surface of the earth too hot for liquid water to exist.
History.
Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from 790,000 years ago. Use of biomass for fire did not become commonplace until many hundreds of thousands of years later, sometime between 200,000 and 400,000 years ago.
Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships on the Nile.
Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass. A graph of energy use in the United States up until 1900 shows oil and natural gas with about the same importance in 1900 as wind and solar played in 2010.
By 1873, concerns of running out of coal prompted experiments with using solar energy. Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 "Scientific American" article: "in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race".
The theory of peak oil was published in 1956. In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.
The IEA 2014 World Energy Outlook projects a growth of renewable energy supply from 1700 gigawatts in 2014 to 4550 gigawatts in 2040. Fossil fuels received about $550 billion in subsidies in 2013, compared to $120 billion for all renewable energies.
Mainstream technologies.
Wind power.
Airflows can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 5 MW of rated power, although turbines with rated output of 1.5–3 MW have become the most common for commercial use; the power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. Areas where winds are stronger and more constant, such as offshore and high altitude sites, are preferred locations for wind farms. Typical capacity factors are 20-40%, with values at the upper end of the range in particularly favourable sites.
Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land stationed turbines.
Hydropower.
Energy in water can be harnessed and used. Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:
Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.
Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, has currently no economic feasibility.
Solar energy.
Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power, solar architecture and artificial photosynthesis.
Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.
Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).
A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect. Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP. Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.
In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".
Biomass.
Biomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: "thermal", "chemical", and "biochemical" methods.
Wood remains the largest biomass energy source today; examples include forest residues (such as dead trees, branches and tree stumps), yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).
Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. Some examples of these plants are wheat, which typically yield 7.5–8 tonnes of grain per hectare, and straw, which typically yield 3.5–5 tonnes per hectare in the UK. The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first generation biofuel.
Biomass can be converted to other usable forms of energy like methane gas or transportation fuels like ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas – also called "landfill gas" or "biogas". Crops, such as corn and sugar cane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products like vegetable oils and animal fats. Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research.
There is a great deal of research involving algal, or algae-derived, biomass due to the fact that it's a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen.
The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the UK.
Biofuel.
Biofuels include a wide range of fuels which are derived from biomass. The term covers solid biofuels, liquid biofuels, and gaseous biofuels. Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas.
Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugar cane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.
With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns.
Biodiesel is made from vegetable oils, animal fats or recycled greases. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe.
Biofuels provided 2.7% of the world's transport fuel in 2010.
Geothermal energy.
Geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet (20%) and from radioactive decay of minerals (80%). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective "geothermal" originates from the Greek roots "geo", meaning earth, and "thermos", meaning heat.
The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 4,000 mi down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 700 F.
From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation.
Commercialization.
Growth of renewables.
From the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. For wind power and many other renewable technologies, growth accelerated in 2009 relative to the previous four years. More wind power capacity was added during 2009 than any other renewable technology. However, grid-connected PV increased the fastest of all renewables technologies, with a 60% annual average growth rate. In 2010, renewable power constituted about a third of the newly built power generation capacities.
Projections vary, but scientists have advanced a plan to power 100% of the world's energy with wind, hydroelectric, and solar power by the year 2030.
According to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment. Cedric Philibert, senior analyst in the renewable energy division at the IEA said: "Photovoltaic and solar-thermal plants may meet most of the world's demand for electricity by 2060 – and half of all energy needs – with wind, hydropower and biomass plants supplying much of the remaining generation". "Photovoltaic and concentrated solar power together can become the major source of electricity", Philibert said.
Economic trends.
Renewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2011 IEA report said: "A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support," and added that "cost reductions in critical technologies, such as wind and solar, are set to continue."
Hydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. 
Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where "oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today".
A series of studies by the US National Renewable Energy Laboratory modeled the "grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power." In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of "between $0.47 and $1.28 to each MegaWatt hour generated"; however, the savings in the cost of the fuels saved "adds up to $7 billion, meaning the added costs are, at most, two percent of the savings."
Hydroelectricity.
The Three Gorges Dam in Hubei, China, has the world's largest instantaneous generating capacity (22,500 MW), with the Itaipu Dam in Brazil/Paraguay in second place (14,000 MW). The Three Gorges Dam is operated jointly with the much smaller Gezhouba Dam (3,115 MW). s of 2012[ [update]], the total generating capacity of this two-dam complex is 25,615 MW. In 2008, this complex generated 98 TWh of electricity (81 TWh from the Three Gorges Dam and 17 TWh from the Gezhouba Dam), which is 3% more power in one year than the 95 TWh generated by Itaipu in 2008.
Wind power development.
From 2004 to 2014, worldwide installed capacity of Wind power has been growing from 47 GW to 369 GW—a more than sevenfold increase within 10 years with 2014 breaking a new record in global installations (51 GW). Wind power is widely used in Europe, Asia, and the United States. Several countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, 14% in Ireland and 9% in Germany in 2010 and have since continued to expand their installed capacity More than 80 countries around the world are using wind power on a commercial basis.
Solar thermal.
The United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.
The oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California. The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW. The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about 70 mi southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.
The solar thermal power industry is growing rapidly with 1.3 GW under construction in 2012 and more planned. Spain is the epicenter of solar thermal power development with 873 MW under construction, and a further 271 MW under development. In the United States, 5,600 MW of solar thermal power projects have been announced. Several power plants have been constructed in the Mojave Desert, Southwestern United States. The Ivanpah Solar Power Facility being the most recent. In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.
Photovoltaic development.
Worldwide Growth of Solar PV
in MW grouped by region
      Europe
      Asia-Pacific
      Americas
      China
      Middle East and Africa
Photovoltaics (PV) uses solar cells assembled into solar panels to convert sunlight into electricity. It's a fast-growing technology doubling its worldwide installed capacity every couple of years. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale solar plants. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time (EPBT), and has reached grid parity in at least 30 different markets by 2014. Financial institutions are predicting a second solar "gold rush" in the near future.
At the end of 2014, worldwide PV capacity reached at least 177,000 megawatts. Photovoltaics grew fastest in China (+10.6 GW), followed by Japan (+9.7 GW) and the United States (+6.2 GW), while Germany remains the world's largest overall producer of photovoltaic power with a total capacity of 38.2 GW, contributing about 7.0 percent to the overall electricity generation. Italy meets 7.9 percent of its electricity demands with photovoltaic power—the highest share worldwide.
For 2015, global cumulative capacity is forecasted to increase by more than 50 gigawatts (GW). By 2018, worldwide capacity is projected to reach as much as 430 gigawatts. This corresponds to a tripling within five years. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics (PV) and concentrated solar power (CSP) contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.
Photovoltaic power stations.
Many solar photovoltaic power stations have been built, mainly in Europe. As of May 2012, the largest photovoltaic (PV) power plants in the world are the Agua Caliente Solar Project (USA, 247 MW), Charanka Solar Park (India, 214 MW), Golmud Solar Park (China, 200 MW), Perovo Solar Park (Ukraine, 100 MW), Sarnia Photovoltaic Power Plant (Canada, 97 MW), Brandenburg-Briest Solarpark (Germany, 91 MW), Solarpark Finow Tower (Germany, 84.7 MW), Montalto di Castro Photovoltaic Power Station (Italy, 84.2 MW), and the Eggebek Solar Park (Germany, 83.6 MW).
There are also many large plants under construction. The Desert Sunlight Solar Farm is a 550 MW solar power plant under construction in Riverside County, California, that will use thin-film solar photovoltaic modules made by First Solar. The Topaz Solar Farm is a 550 MW photovoltaic power plant, being built in San Luis Obispo County, California. The Blythe Solar Power Project is a 500 MW photovoltaic station under construction in Riverside County, California. The California Valley Solar Ranch (CVSR) is a 250 MW solar photovoltaic power plant, which is being built by SunPower in the Carrizo Plain, northeast of California Valley. The 230 MW Antelope Valley Solar Ranch is a First Solar photovoltaic project which is under construction in the Antelope Valley area of the Western Mojave Desert, and due to be completed in 2013.
Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems. There are no fuel costs or emissions during operation of the power stations.
However, when it comes to renewable energy systems and PV, it is not just large systems that matter. Building-integrated photovoltaics or "onsite" PV systems use existing land and structures and generate power close to where it is consumed.
Carbon-neutral and negative fuels.
Carbon-neutral fuels are synthetic fuels (including methane, gasoline, diesel fuel, jet fuel or ammonia) produced by hydrogenating waste carbon dioxide recycled from power plant flue-gas emissions, recovered from automotive exhaust gas, or derived from carbonic acid in seawater. Such fuels are considered carbon-neutral because they do not result in a net increase in atmospheric greenhouse gases. To the extent that synthetic fuels displace fossil fuels, or if they are produced from waste carbon or seawater carbonic acid, and their combustion is subject to carbon capture at the flue or exhaust pipe, they result in negative carbon dioxide emission and net carbon dioxide removal from the atmosphere, and thus constitute a form of greenhouse gas remediation.
Such renewable fuels alleviate the costs and dependency issues of imported fossil fuels without requiring either electrification of the vehicle fleet or conversion to hydrogen or other fuels, enabling continued compatible and affordable vehicles. Carbon-neutral fuels offer relatively low cost energy storage, alleviating the problems of wind and solar intermittency, and they enable distribution of wind, water, and solar power through existing natural gas pipelines. Nighttime wind power is considered the most economical form of electrical power with which to synthesize fuel, because the load curve for electricity peaks sharply during the warmest hours of the day, but wind tends to blow slightly more at night than during the day, so, the price of nighttime wind power is often much less expensive than any alternative. Germany has built a 250 kilowatt synthetic methane plant which they are scaling up to 10 megawatts.
The George Olah carbon dioxide recycling plant in Grindavík, Iceland has been producing 2 million liters of methanol transportation fuel per year from flue exhaust of the Svartsengi Power Station since 2011. It has the capacity to produce 5 million liters per year.
Biofuel development.
Biofuels provided 3% of the world's transport fuel in 2010. Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.
Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump.
Nearly all the gasoline sold in the United States today is mixed with 10% ethanol, a mix known as E10, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell "flexible-fuel" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol (E85). By mid-2006, there were approximately 6 million E85-compatible vehicles on U.S. roads. The challenge is to expand the market for biofuels beyond the farm states where they have been most popular to date. Flex-fuel vehicles are assisting in this transition because they allow drivers to choose different fuels based on price and availability. The "Energy Policy Act of 2005", which calls for 7.5 e9USgal of biofuels to be used annually by 2012, will also help to expand the market.
Geothermal development.
Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.
The International Geothermal Association (IGA) has reported that 10,715 MW of geothermal power in 24 countries is online, which is expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in geothermal power online capacity since 2005. IGA projects this will grow to 18,500 MW by 2015, due to the large number of projects presently under consideration, often in areas previously assumed to have little exploitable resource.
In 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants; the largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines follows the US as the second highest producer of geothermal power in the world, with 1,904 MW of capacity online; geothermal power makes up approximately 18% of the country's electricity generation.
Developing countries.
Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.
Technology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who don't have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene. An estimated 3 million households get power from small solar PV systems. Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 12 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.
Micro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulostic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.
Renewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.
Industry and policy trends.
U.S. President Barack Obama's American Recovery and Reinvestment Act of 2009 includes more than $70 billion in direct spending and tax credits for clean energy and associated transportation programs. Clean Edge suggests that the commercialization of clean energy will help countries around the world pull out of the current economic malaise. Leading renewable energy companies include First Solar, Gamesa, GE Energy, Q-Cells, Sharp Solar, Siemens, SunOpta, Suntech Power, and Vestas.
The military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.
The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed on 26 January 2009, by 75 countries signing the charter of IRENA. As of March 2010, IRENA has 143 member states who all are considered as founding members, of which 14 have also ratified the statute.
As of 2011, 119 countries have some form of national renewable energy policy target or renewable support policy. National targets now exist in at least 98 countries. There is also a wide range of policies at state/provincial and local levels.
United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. In October 2011, he "announced the creation of a high-level group to drum up support for energy access, energy efficiency and greater use of renewable energy. The group is to be co-chaired by Kandeh Yumkella, the chair of UN Energy and director general of the UN Industrial Development Organisation, and Charles Holliday, chairman of Bank of America".
100% renewable energy.
The incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of “stabilization wedges” that can allow us to maintain our quality of life while avoiding catastrophic climate change, and "renewable energy sources," in aggregate, constitute the largest number of their "wedges." 
Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University and director of its Atmosphere and Energy Program says producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be "primarily social and political, not technological or economic". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs.
Similarly, in the United States, the independent National Research Council has noted that “sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand." .
The most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 "Post Carbon Pathways" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.
Emerging technologies.
Other renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy. These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.
There are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.
Multiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.
Cellulosic ethanol.
Companies such as Iogen, POET, and Abengoa are building refineries that can process biomass and turn it into ethanol, while companies such as the Verenium Corporation, Novozymes, and are producing enzymes which could enable a cellulosic ethanol future. The shift from food crop feedstocks to waste residues and native grasses offers significant opportunities for a range of players, from farmers to biotechnology firms, and from project developers to investors.
Marine energy.
Marine energy (also sometimes referred to as ocean energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries.
The term marine energy encompasses both wave power – power from surface waves, and tidal power – obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.
The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.
Enhanced geothermal systems.
Enhanced geothermal systems are a new type of geothermal power technologies that do not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock. EGS technologies "enhance" and/or create geothermal resources in this "hot dry rock (HDR)" through hydraulic stimulation.
EGS / HDR technologies, like hydrothermal geothermal, are expected to be baseload resources which produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR / EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km) layer of insulating sediments which slow heat loss. There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S. and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW.
Experimental solar power.
Concentrated photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electricity generation. Thermoelectric, or "thermovoltaic" devices convert a temperature difference between dissimilar materials into an electric current.
Artificial photosynthesis.
Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol. Researchers in this field are striving to design molecular mimics of photosynthesis that utilize a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells). However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012, because it offers few savings over other ways to make hydrogen from sunlight.
Debate.
Renewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being variable or intermittent. However, the International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.
There have been "not in my back yard" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction. In the USA, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.
A recent UK Government document states that "projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake". In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.
The market for renewable energy technologies has continued to grow. Climate change concerns, coupled with high oil prices, peak oil, and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.
Bibliography.
</dl>

</doc>
<doc id="25789" url="http://en.wikipedia.org/wiki?curid=25789" title="Romulus Augustulus">
Romulus Augustulus

Romulus Augustus (born perhaps around 461 – died after 476, and was apparently still alive as late as 507) was an Emperor (alleged usurper) reigning over the Western Roman Empire from 31 October 475 until 4 September 476. His deposition by Odoacer traditionally marks the end of the western empire, the fall of ancient Rome, and the beginning of the Middle Ages in Western Europe.
He is mostly known by his nickname "Romulus Augustulus", though he ruled officially as Romulus Augustus. The Latin suffix "-ulus" is a diminutive; hence, "Augustulus" effectively means "Little Augustus".
The historical record contains few details of Romulus's life. He was proclaimed as emperor by his father Orestes, the "magister militum" (master of soldiers) of the Roman army after forcing Emperor Julius Nepos to quit Italy. Romulus, little more than a child, acted as a figurehead for his father's rule. Reigning for only ten months, his legitimacy and authority disputed beyond Italy, Romulus was then deposed by Odoacer, who had defeated and executed Orestes, and sent to live in the Castellum Lucullanum in Campania; afterwards he disappears from the historical record.
Life.
Romulus' father Orestes was a Roman citizen, originally from Pannonia, who had served as a secretary and diplomat for Attila the Hun and later rose through the ranks of the Roman army. The future emperor was named "Romulus" after his maternal grandfather, a nobleman from Poetovio in Noricum. Many historians have noted the coincidence that the last western emperor bore the names of both Romulus, the legendary founder and first king of Rome, and Augustus, its first emperor.
Orestes was appointed Magister militum by Julius Nepos in 475. Shortly after his appointment, Orestes launched a rebellion and captured Ravenna, the capital of the Western Roman Empire since 402, on 28 August 475. Nepos fled to Dalmatia, where his uncle had ruled a semi-autonomous state in the 460s. Orestes, however, refused to become emperor, "from some secret motive", according to historian Edward Gibbon. Instead, he installed his son on the throne on 31 October 475.
The empire Augustus ruled was a shadow of its former self and had shrunk significantly over the previous 80 years. Imperial authority had retreated to the Italian borders and parts of southern Gaul: Italia and Gallia Narbonensis, respectively. The Eastern Roman Empire treated its western counterpart as a client state. The Eastern Emperor Leo, who died in 474, had appointed the western emperors Anthemius and Julius Nepos, and Constantinople never recognized the new government. Neither Zeno nor Basiliscus, the two generals fighting for the eastern throne at the time of Romulus' accession, accepted him as ruler.
As a proxy for his father, Romulus made no decisions and left no monuments, though coins bearing his name were minted in Rome, Milan, Ravenna, and Gaul. Several months after Orestes took power, a coalition of Heruli, Scirian and Turcilingi mercenaries demanded that he give them a third of the land in Italy. When Orestes refused, the tribes revolted under the leadership of the Scirian chieftain Odoacer. Orestes was captured near Piacenza on 28 August 476 and swiftly executed.
Odoacer advanced on Ravenna, capturing the city and the young emperor. Romulus was compelled to abdicate the throne on 4 September 476. This act has been cited as the end of the Western Roman Empire, although Romulus' deposition did not cause any significant disruption at the time. Rome had already lost its hegemony over the provinces, Germanics dominated the Roman army and Germanic generals like Odoacer had long been the real powers behind the throne. Italy would suffer far greater devastation in the next century when Emperor Justinian I re-conquered it.
After the abdication of Romulus, the Roman Senate, on behalf of Odoacer, sent representatives to the Eastern Roman Emperor Zeno, whom it asked to formally reunite the two halves of the Empire: "the west… no longer required an emperor of its own: one monarch sufficed for the world". He was also asked to make Odoacer a Patrician, and administrator of Italy in Zeno's name. Zeno pointed out that the Senate should rightfully have first requested that Julius Nepos take the throne once more, but he nonetheless agreed to their requests. Odoacer then ruled Italy in Zeno's name.
Later life.
The ultimate fate of Romulus is a mystery. The "Anonymus Valesianus" wrote that Odoacer, "taking pity on his youth", spared Romulus' life and granted him an annual pension of 6,000 solidi before sending him to live with relatives in Campania. Jordanes and Marcellinus Comes say Odoacer exiled Romulus to Campania but do not mention any financial support from the Germanic king.
The sources do agree that Romulus took up residence in the Lucullan Villa, an ancient castle originally built by Lucullus in Campania. From here, contemporary histories fall silent. In the "History of the Decline and Fall of the Roman Empire", Edward Gibbon notes that the disciples of Saint Severinus of Noricum were invited by a "Neapolitan lady" to bring his body to the villa in 488; Gibbon conjectures from this that Augustulus "was probably no more." The villa was converted into a monastery before 500 to hold the saint's remains.
Cassiodorus, then a secretary to Theodoric the Great, wrote a letter to a "Romulus" in 507 confirming a pension. Thomas Hodgkin, a translator of Cassiodorus' works, wrote in 1886 that it was "surely possible" the Romulus in the letter was the same person as the last western emperor. The letter would match the description of Odoacer's coup in the "Anonymus Valesianus", and Romulus could have been alive in the early sixth century. But Cassiodorus does not supply any details about his correspondent or the size and nature of his pension, and Jordanes, whose history of the period abridges an earlier work by Cassiodorus, makes no mention of a pension.
Last emperor.
As Romulus was an alleged usurper, Julius Nepos claimed to hold legally the title of emperor when Odoacer took power. However, few of Nepos' contemporaries were willing to support his cause after he ran away to Dalmatia. Some historians regard Julius Nepos, who ruled in Dalmatia until being murdered in 480, as the last lawful Western Roman Emperor.
Following Odoacer's coup, the Roman Senate sent a letter to Zeno stating that "the majesty of a sole monarch is sufficient to pervade and protect, at the same time, both the East and the West". While Zeno told the Senate that Nepos was their lawful sovereign, he did not press the point, and he accepted the imperial insignia brought to him by the senate.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="25791" url="http://en.wikipedia.org/wiki?curid=25791" title="List of Roman emperors">
List of Roman emperors

The Roman Emperors were men who ruled the Roman Empire and wielded power over its citizens and military. The empire was developed as the Roman Republic invaded and occupied most of Europe and portions of northern Africa and western Asia. Under the republic, regions of the empire were ruled by provincial governors answerable to and authorised by the "Senate and People of Rome". Rome and its senate were ruled by a variety of magistrates – of whom the consuls were the most powerful. The republic ended, and the emperors were created, when these magistrates became legally and practically subservient to one citizen with power over all other magistrates. Augustus, the first emperor, was careful to maintain the facade of republican rule, taking no specific title for his position and calling the concentration of magisterial power "Princeps Senatus" (the first man of the senate). This style of government lasted for 300 years, and is thus called the "Principate" era. The modern word 'emperor' derives from the title "imperator", which was granted by an army to a successful general; during the initial phase of the empire, it still had to be earned by the 'Princeps'. The term emperor is a modern construction, used when describing rulers of the Roman Empire because it emphasises the strong links between the ruler and the army (on whose support the ruler's power depended), and does not discriminate between the personal styles of rule and titles in different phases of the Empire.
In the late 3rd century, after the Crisis of the Third Century, Diocletian formalised and embellished the recent manner of imperial rule, establishing the so-called 'Dominate' period of the Roman Empire. This was characterised by the explicit increase of authority in the person of the Emperor, and the use of the style 'Dominus Noster' ('Our Lord'). The rise of powerful Barbarian tribes along the borders of the empire and the challenge they posed to defense of far-flung borders and unstable imperial succession led Diocletian to experiment with sharing imperial titles and responsibilities among several individuals - a partial reversion to pre-Augustian Roman traditions. For nearly two centuries thereafter there was often more than one emperor at a time, frequently dividing the administration of the vast territories between them. As Henry Moss warned, "Yet it is important to remember that in the eyes of contemporaries the Empire was still one and indivisible. It is false to the ideas of this time to speak of 'the Eastern and Western Empire'; the two halves of Empire were thought of as 'the Eastern, or Western parts' ("partes orientis vel occidentis".)" However, after the death of Theodosius I (395), the split became firmly entrenched (see: Western and Eastern) The last pretense of such division was formally ended by Zeno after the death of Julius Nepos in 480. For the remaining thousand years of Roman history there would only ever be one legitimate senior emperor, ruling from Constantinople and maintaining claim to the increasingly unstable territories in the west. After 480, multiple claims to be the imperial title of "Augustus" (or "Basileus" for Greek speakers) necessarily meant civil war, although the experiment with designating junior emperors (called now "Caesars"), usually to indicate the intended successor, occasionally reappeared.
The Empire and chain of emperors continued until the death of Constantine XI and the capture of Constantinople by the Ottoman Empire in 1453. The use of the terms "Byzantium," "Byzantine Empire," and "Byzantine Emperor" to refer to the medieval period of the Empire has been common, but not universal, among Western scholars since the 18th century, and continues to be a subject of specialist debate today.
Legitimacy.
The emperors listed in this article are those generally agreed to have been 'legitimate' emperors, and who appear in published regnal lists. The word 'legitimate' is used by most authors, but usually without clear definition, perhaps not surprisingly, since the emperorship was itself rather vaguely defined legally. In Augustus' original formulation, the "princeps" was selected by either the Senate or "the people" of Rome, but quite quickly the legions became an acknowledged stand-in for "the people." A person could be proclaimed as emperor by their troops or by "the mob" in the street, but in theory needed to be confirmed by the Senate. The coercion that frequently resulted was implied in this formulation. Furthermore, a sitting emperor was empowered to name a successor and take him on as apprentice in government and in that case the Senate had no role to play, although it sometimes did when a successor lacked the power to inhibit bids by rival claimants. By the medieval (or "Byzantine") period, the very definition of the Senate became vague as well, adding to the complication.
Lists of legitimate emperors are therefore partly influenced by the subjective views of those compiling them, and also partly by historical convention. Many of the 'legitimate' emperors listed here acceded to the position by usurpation, and many 'illegitimate' claimants had a legitimate claim to the position. Historically, the following criteria have been used to derive emperor lists:
So for instance, Aurelian, though acceding to the throne by usurpation, was the sole and undisputed monarch between 270–275 AD, and thus was a legitimate emperor. Gallienus, though not in control of the whole Empire, and plagued by other claimants, was the legitimate heir of (the legitimate emperor) Valerian. Claudius Gothicus, though acceding illegally, and not in control of the whole Empire, was the only claimant accepted by the Senate, and thus, for his reign, was the legitimate emperor. Equally, during the Year of the Four Emperors, all claimants, though not undisputed, were at some point accepted by the Senate and are thus included; conversely, during the Year of the Five Emperors neither Pescennius Niger nor Clodius Albinus were accepted by the Senate, and are thus not included. There are a few examples where individuals were made co-emperor, but never wielded power in their own right (typically the child of an emperor); these emperors are legitimate, but are not included in regnal lists, and in this article are listed together with the 'senior' emperor.
Emperors after 395.
After 395, the list of emperors in the East is based on the same general criteria, with the exception that the emperor only had to be in undisputed control of the Eastern part of the empire, or be the legitimate heir of the Eastern emperor.
The situation in the West is more complex. Throughout the final years of the Western Empire (395–480) the Eastern emperor was considered the senior emperor, and a Western emperor was only legitimate if recognized as such by the Eastern emperor. Furthermore, after 455 the Western emperor ceased to be a relevant figure and there was sometimes no claimant at all. For the sake of historical completeness, all Western Emperors after 455 are included in this list, even if they were not recognized by the Eastern Empire; some of these technically illegitimate emperors are included in regnal lists, while others are not. For instance, Romulus Augustulus was technically a usurper who ruled only the Italian peninsula and was never legally recognized. However, he was traditionally considered the "last Roman Emperor" by 18th and 19th century western scholars and his overthrow by Odoacer used as the marking point between historical epochs, and as such he is usually included in regnal lists. However, modern scholarship has confirmed that Romulus Augustulus' predecessor, Julius Nepos continued to rule as Emperor in the other Western holdings and as a figurehead for Odoacer's rule in Italy until Nepos' death in 480. Since the question of what constitutes an emperor can be ambiguous, and dating the "fall of the Western Empire" arbitrary, this list includes details of both figures.

</doc>
<doc id="25792" url="http://en.wikipedia.org/wiki?curid=25792" title="Roman calendar">
Roman calendar

The Roman calendar changed its form several times between the founding of Rome and the fall of the Roman Empire. This article generally discusses the early Roman or pre-Julian calendars. The calendar used after 46 BC is discussed under Julian calendar. The common calendar widely used today known as the Gregorian calendar is a refinement of the Julian calendar where the length of the year has been adjusted from 365.25 days to 365.2425 days (a 0.002% change).
History.
The original Roman calendar is believed to have been a lunar calendar, which may have been based on one of the Greek lunar calendars. As the time between new moons averages 29.5 days, its months were constructed to be either hollow (29 days) or full (30 days). Full months were considered powerful and therefore auspicious; hollow months (like odd numbers) were unlucky. Unlike currently used dates, which are numbered sequentially from the beginning of the month, the Romans counted backwards from three fixed points: the Nones, the Ides and the Kalends of the following month. This system originated in the practice of "calling" the new month when the lunar crescent was first observed in the west after sunset. From the shape and orientation of the new moon, the number of days remaining to the nones would be proclaimed. At some point in history dates of months ceased to be connected with lunar phases, but it is unknown when it happened.
Calendar of Romulus.
Roman writers attributed the original Roman calendar to Romulus, the mythical founder of Rome around 753 BC. Romulus' calendar had ten months with the spring equinox in the first month:
The regular calendar year consisted of 304 days, with the winter days after the end of December and before the beginning of the following March not being assigned to any month.
Martius was named in honour of Mars, the god of war. Aprilis is from "aperire", to open: Earth opens to receive seed. Maia was the goddess of growth ("maior", elder). Iunius is from "iunior" (younger). The names of the months from the fifth month on were based on their position in the calendar: "Quintilis" comes from Latin "quinque" meaning five; "Sextilis" from "sex" meaning six; "September" from "septem" meaning seven; "October" from "octo" meaning eight; "November" from "novem" meaning nine; and "December" from "decem" meaning ten. 
Calendar of Numa.
Numa Pompilius, the second of the seven traditional kings of Rome, reformed the calendar of Romulus around 713 BC. The Romans considered even numbers to be unlucky, so Numa took one day from each of the six months with 30 days, reducing the number of days in the 10 previously defined months by a total of six days.
There were 51 previously unallocated winter days, to which were added the six days from the reductions in the days in the months, making a total of 57 days. These he made into two months, January and February, which he prefixed to the previous 10 months. January was given 29 days, while February had the unlucky number of 28 days, suitable for the month of purification ("Februa", the Roman festival of purification). This made a regular year (of 12 lunar months) 355 days long in place of the previous 304 days of the Romulus calendar. Of the 11 months with an odd number of days, four had 31 days each and seven had 29 days each:
February consisted of two parts, each with an odd number of days. The first part ended with the "Terminalia" on the 23rd, which was considered the end of the religious year, and the five remaining days formed the second part. To keep the calendar year roughly aligned with the solar year, a leap month, called the "Mensis Intercalaris", was added from time to time between these two parts of February. The second part of February was incorporated in the intercalary month as its last five days, with no change either in their dates or the festivals observed on them. This follows naturally from the fact that the days after the Ides of February (in an ordinary year) or the Ides of Intercalaris (in an intercalary year) both counted down to the Kalends of March. The nones and ides of Intercalaris occupied the normal positions of the 5th and 13th of the month. 
The third - century writer Censorinus says:
When it was thought necessary to add (every two years) an intercalary month of 22 or 23 days, so that the civil year should correspond to the natural (solar) year, this intercalation was in preference made in February, between Terminalia [23rd] and Regifugium [24th]. 
The fifth - century writer Macrobius says that the Romans intercalated 22 and 23 days in alternate years ("Saturnalia", 1.13.12), the intercalation was placed after 23 February and the remaining five days of February followed ("Saturnalia", 1.13.15). To avoid the nones falling on a nundine, where necessary an intercalary day was inserted "in the middle of the Terminalia, where they placed the intercalary month". ("Saturnalia", 1.13.16, 1.13.19). 
This is historically correct. In 167 BC Intercalaris began on the day after 23 February and in 170 BC it began on the second day after 23 February. Varro, writing in the first century BC, says "the twelfth month was February, and when intercalations take place the five last days of this month are removed." Since all the days after the Ides of Intercalaris were counted down to the beginning of March Intercalaris had either 27 days (making 377 for the year) or 28 (making 378 for the year).
There is another theory which says that in intercalary years February had 23 or 24 days and Intercalaris had 27. No date is offered for the Regifugium in 378 - day years. 
The "Pontifex Maximus" determined when an intercalary month was to be inserted. On average, this happened in alternate years. The system of aligning the year through intercalary months broke down at least twice: the first time was during and after the Second Punic War. It led to the reform of the Lex Acilia in 191 BC, the details of which are unclear, but it appears to have successfully regulated intercalation for over a century. The second breakdown was in the middle of the first century BC and may have been related to the increasingly chaotic and adversarial nature of Roman politics at the time. The position of Pontifex Maximus was not a full-time job; it was held by a member of the Roman elite, who would almost invariably be involved in the machinations of Roman politics. Because the term of office of elected Roman magistrates was defined in terms of a Roman calendar year, a Pontifex Maximus would have reason to lengthen a year in which he or his allies were in power, or shorten a year in which his political opponents held office. For example, Julius Caesar made the year of his third consulship in 46 BC 445 days long.
Julian calendar.
Julius Caesar, as Pontifex Maximus, reformed the calendar in 46 BC. The new calendar became known as the Julian calendar. Quintilis was renamed as Iulius (July) in honour of Julius Caesar in 44 BC by Mark Antony. The calendar reforms were completed during the reign of his successor Augustus, when the Senate renamed Sextilis as Augustus (August) in 8 BC. (Lex Pacuvia de mense augusto—see Macrobius, "Saturnalia", 1.12). Some documents state that the date of the change of the name started between 26 and 23 BC but the date of the Lex Pacuvia is certain.
Months.
In the earliest times, the three reference dates were probably declared publicly, when appropriate lunar conditions were observed. After the reforms of Numa, they occurred on fixed days.
The day preceding the Kalends, Nones, or Ides was "Pridie", e.g., "Prid. Id. Mart." = 14 March. Other days were denoted by ordinal number, counting back from a named reference day. The reference day itself counted as the first, so that two days before was denoted the third day. Dates were written as "a.d. NN", an abbreviation for "ante diem NN", meaning "on the Nth (Numerus) day before the named reference day ("Nomen")", e.g., "a.d. III Kal. Nov." = on the third day before the November Kalends = 30 October. The value two was not used to denote a day before the fixed point, because second was the same as "pridie". Further examples of date equivalence are: "a.d. IV Non. Jan." = 2 January; "a.d. VI Non. Mai." = 2 May; "a.d. VIII Id. Apr." = 6 April; "a.d. VIII Id. Oct." = 8 October; "a.d. XVII Kal. Nov." = 16 October.
In detail, the system worked as follows:
Months were grouped in days such that the "Kalends" was the first day of the month, the "Ides" was the 13th day of short months, or the 15th day of long months, and the "Nones" was the 9th day (counted inclusively) before the "Ides" (i.e., the fifth or seventh day of the month). All other days of the month were counted backward (inclusively) from these three dates. In both long and short months (except February and the "mensis intercalaris") there were 16 days between the "Ides" of the month and the "Kalends" of the next month, and the date referred to the name of the next month, not that of the current month; thus, for example, the date of the 16th day of March was "a.d. XVII Kal. Apr". In intercalary years, the first part of February was terminated on the 23rd, i.e., the day of the "Terminalia", and the festivals normally held in the last five days of February were held instead in the last five days of the intercalary month, immediately before the "Kalends" of March. The first 22 or 23 days of the intercalary month were inserted between these two parts.
So:
Some dates were also sometimes known by the name of a festival that occurred on them, or shortly afterwards. Examples of such dates are recorded for the "Feralia, Quirinalia", and the "Terminalia", though not yet for the "Lupercalia". The known examples of such dates are all after the Ides of February, which suggests they are connected with resolving an ambiguity that could arise in intercalary years: dates of the form "a.d. [N] Kal. Mart." were dates in late February in regular years, but were a month later in intercalary years. However, it is much debated whether there was a fixed rule for using festival-based dates. It has been variously proposed that a date like "a.d. X Terminalia" (known from an inscription in 94 BC) implied that its year 'was', 'was not', or 'might have been' intercalary.
When Julius Caesar added days to some months, he added them to the end of the month, so as not to disturb the dates of festivals in those months. This increased the count of all days after the Ides in those months, and had some odd effects. For example, the emperor Augustus was born in 63 BC on the 23rd day of September. In the pre-Julian calendar, this is seven days before the Kalends of October (or, in Roman style, counting inclusively, "a.d. VIII Kal. Oct."), but in the Julian calendar, it is eight days ("a.d. IX Kal. Oct."). Because of this ambiguity, his birthday was sometimes celebrated on both dates. See discussion in Julian calendar.
Nundinal cycle.
The Romans of the Republic, like the Etruscans, used a "market week" of eight days, marked as A to H in the calendar. A "nundinum" was the market day; etymologically, the word is related to "novem", "nine", because the Roman system of counting was inclusive. The market "week" is the nundinal cycle. Since the length of the year was not a multiple of eight days, the letter for the market day (known as a "nundinal letter") changed every year. For example, if the letter for market days in some year was A and the year was 355 days long, then the letter for the next year would be F.
The nundinal cycle formed one rhythm of day-to-day Roman life; the market day was the day when country people would come to the city, and the day when city people would buy their eight days' worth of groceries. For this reason, a law was passed in 287 BC (the "Lex Hortensia") that forbade the holding of meetings of the "comitia" (for example to hold elections) on market days, but permitted the holding of legal actions. In the late republic, a superstition arose that it was unlucky to start the year with a market day (i.e., for the market day to fall on 1 January, with a letter A), and the pontiffs, who regulated the calendar, took steps to avoid it.
Because the nundinal cycle was absolutely fixed at eight days under the Republic, information about the dates of market days is one of the most important tools used for working out the Julian equivalent of a Roman date in the pre-Julian calendar. In the early Empire, the Roman market day was occasionally changed. The details of this are not clear, but one likely explanation is that it would be moved by one day if it fell on the same day as the festival of "Regifugium", an event that could occur at intervals of three years. The reason for this is not explained.
The nundinal cycle was eventually replaced by the modern seven-day week, which first came into use in Italy during the early imperial period, after the Julian calendar had come into effect in 45 BC. The system of nundinal letters was also adapted for the week, see dominical letter. For a while, the week and the nundinal cycle coexisted, but by the time the week was officially adopted by Constantine in AD 321, the nundinal cycle had fallen out of use. For further information on the week, see week and days of the week.
Character of the day.
Each day of the Roman calendar was marked on the "fasti" with a letter that designated its religious and legal character. These were:
Years.
The calendar year originally began on 1 March, as is shown by the names of the six months following June (Quintilis = fifth month, Sextilis = sixth month, September = seventh month, etc.). It is not known when the start of the calendar year was changed to 1 January. Ancient authors attributed it to Numa Pompilius. Varro states that, according to M. Fulvius Nobilior (consul in 189 BC), who had composed a commentary on a "fasti" preserved in the temple of Hercules Musarum, January was named after Janus because the god faced both ways, which implies the calendar year started in January in his time, before the consular year started beginning on 1 January in 153 BC. A surviving calendar from the late Republic proves the calendar year started in January before the Julian reform.
How years were identified during the Roman monarchy is not known. During the Roman Republic, years were named after the consuls, who were elected annually (see List of Republican Roman Consuls). Thus, the name of the year identified a consular term of office, not a calendar year. For example, 205 BC was "The year of the consulship of Publius Cornelius Scipio Africanus and Publius Licinius Crassus", who took office on 15 March of that year, and their consular year ran until 14 March 204 BC. Lists of consuls were maintained in the "fasti".
The first day of the consular term changed several times during Roman history. It became 1 January in 153 BC. Before then, it was 15 March. Earlier changes are a little less certain. There is good reason to believe it was 1 May for most of the third century BC, until 222 BC. Livy mentions earlier consular years starting on 1 Sextilis (August), 15 May, 15 December, 1 October and 1 Quintilis (July).
In the later Republic, historians and scholars began to count years from the founding of the city of Rome. Different scholars used different dates for this event. The date most widely used today is that calculated by Varro, 753 BC, but other systems varied by up to several decades. Dates given by this method are numbered "ab urbe condita" (meaning "from the founding of the city", and abbreviated AUC), and correspond to consular years. When reading ancient works using AUC dates, care must be taken to determine the epoch used by the author before translating the date into a Julian year.
Converting pre-Julian dates.
The fact that the modern world uses the same month names as the Romans can lead to an erroneous assumption that a Roman date occurred on the same Julian date as its modern equivalent. Even early Julian dates, before the leap year cycle was stabilised, are not quite what they appear to be. For example, Macrobius says 45 BC was not a leap year.
Finding the exact Julian equivalent of a pre-Julian date is complex. As there exists an essentially complete list of the consuls, a Julian year can be found to correspond to the pre-Julian year.
However, the sources rarely reveal which years were regular, which were intercalary, and how long an intercalary year was. Nevertheless, the pre-Julian calendar could be substantially out of alignment with the Julian calendar. Two precise astronomical synchronisms given by Livy show that in 168 BC, the two calendars were misaligned by more than two months, and in 190 BC, they were four months out of alignment.
A number of other clues are available to reconstruct the Julian equivalent of pre-Julian dates. First, the precise Julian date for the start of the Julian calendar is known, although some uncertainty occurs even about that. Detailed sources for the previous decade or so are found, mostly in the letters and speeches of Cicero. Combining these with what is known about how the calendar worked, especially the nundinal cycle, an accurate conversion of Roman dates after 58 BC relative to the start of the Julian calendar can be performed.
The histories of Livy give exact Roman dates for two eclipses in 190 BC and 168 BC, and a few loose synchronisms to dates in other calendars provide rough (and sometimes exact) solutions for the intervening period. Before 190 BC, the alignment between the Roman and Julian years is determined by clues such as the dates of harvests mentioned in the sources.
Combining these sources of data, an estimate can be computed for approximate Julian equivalents of Roman dates back to the start of the First Punic War in 264 BC. However, while there are enough data to make such reconstructions, the number of years before 45 BC for which pre-Julian Roman dates can be converted to Julian dates with certainty is very small, and several different reconstructions of the pre-Julian calendar are possible. A detailed reconstruction giving conversions from pre-Julian dates into Julian dates is available.

</doc>
<doc id="25794" url="http://en.wikipedia.org/wiki?curid=25794" title="Revolver">
Revolver

A revolver is a repeating firearm that has a revolving cylinder containing multiple chambers and at least one barrel for firing. The term "revolver" refers to a handgun, but other weapons may also have a revolving chamber. These include some models of grenade launchers, shotguns, and rifles.
Though the original name was "revolving gun", the short-hand "revolver" is universally used. (Cannons using this mechanism are known as revolver cannons.) Nearly all early revolvers and many modern ones have six chambers in the cylinder, giving rise to the slang term "six-shooter"; however, revolvers with 3 to 24 chambers have been made, with most modern revolvers having 5 or 6 chambers.
The revolver allows the user to fire multiple rounds without reloading. Each time the user cocks the hammer, the cylinder revolves to align the next chamber and round with the hammer and barrel, which gives this type of firearm its name. In a single-action revolver, the user pulls the hammer back with his free hand or thumb; the trigger pull only releases the hammer. In a double-action revolver, pulling the trigger moves the hammer back, then releases it, which requires a longer and heavier trigger pull than single-action. Loading and unloading a double action revolver requires the operator to swing out the cylinder and insert the proper ammunition all while keeping the gun pointed in a safe direction.
The first guns with multichambered cylinders that revolved to feed one barrel were made in the late 1500s in Europe. They were expensive and rare curiosities. Not until the 1800s would revolvers become practical weapons for non-rich owners. One of the first was a flintlock revolver made by Elisha Collier in 1814. The first percussion cap revolver was invented by the Italian Francesco Antonio Broccu 1833. He received a prize of 300 francs for his invention, although he did not patent it, his revolver was shown to the King Charles Albert of Sardinia. However, in 1835 a similar handgun was patented by Samuel Colt, who would go on to make the first mass-produced revolver.
The first cartridge revolvers were produced around 1856 by Smith & Wesson.
Revolvers soon became standard for nearly all uses. In the early 20th century, semi-automatic pistols were developed, which can hold more rounds, and are faster to reload. "Automatic" pistols also have a flat profile, more suitable for concealed carry. Semi-auto pistols were not considered reliable enough for serious police work or self-defense until the later half of the century, however, and revolvers were the dominant handgun for police and civilians until modern pistols such as the Beretta 92 and Glock 17 were developed in the 70s and 80s.
Automatic pistols have almost completely replaced revolvers in military and law enforcement use (in military use, from 1910-1960; in law enforcement, in the 1980s and 1990s).
Revolvers still remain popular as back-up and off-duty handguns among American law enforcement officers and security guards. Also, revolvers are still common in the American private sector as defensive and sporting/hunting firearms. Famous police and military revolvers include the Webley, the Colt Single Action Army, the Colt Police Special, the Smith & Wesson Model 36, the Smith & Wesson Model 10, the Smith & Wesson 1917, the Smith & Wesson Model 3 the Nagant M1895.
History.
In the development of firearms, an important limiting factor was the time it took to reload the weapon after it was fired. While the user was reloading, the weapon was useless, and an adversary might be able to take advantage of the situation and kill or wound the user. Several approaches to the problem of increasing the rate of fire were developed, the earliest being multi-barrelled weapons which allowed two or more shots without reloading.
Later multibarreled guns, called pepper-box pistols, used a revolving cylinder containing multiple chamber-with-barrel passages. Many early true revolvers (multichamber cylinder revolving to feed one barrel) were partly an attempt to improve on pepper-boxes. Firing through a single barrel saved the expense and weight of having the multiple barrels of the pepper-box.
The earliest examples of what today is called a revolver were made in Europe in the late 1500s. One is a shoulder-gun-length weapon made in Nuremberg, Germany, circa 1580. Another is a revolving arquebus, produced by 
Hans Stopler of Nuremberg in 1597. Another early specimen, now in the Tower of London armouries, is dated to the middle 17th century and attributed to John Dafte of London. This example, a flintlock, uses a single lock, with a flash pan for each of the six chambers. The cylinder is rotated by hand, and locks in place for firing. This was still not perfected, however, as it was apparently destroyed by a misfire. Two revolver-type weapons were found in the armoury of the French King Louis XIII. One was a combined sword and revolver of Spanish origin while the other was a flintlock revolver designed by the Russian gunsmith Pervusha Issayev in about 1625 with six shots.
An early 17th-century flintlock revolver of French or Flemish origin with a remarkably similar design to Samuel Colt's original design was purchased by Samuel Colt from the Tower of London in order to prevent any infringement upon his patents.
James Puckle patented a revolving chamber gun in 1718. This gun, which had a 1.25 inch bore (30 mm), was tripod mounted, and the 11-shot cylinder was operated by a hand crank. It is often cited as the first machine gun. By changing cylinders to reload (an early example of a speedloader), the gun was fired and reloaded to fire a total of 63 rounds in seven minutes.
Elisha Collier patented a flintlock revolver in Britain in 1818, and significant numbers were being produced in London by 1822.
Francesco Antonio Broccu developed a percussion cap revolver in 1833, which he received a prize of 300 francs for, though he didn't patent it.
In 1836, Samuel Colt patented a revolver mechanism that led to the widespread use of the revolver. According to Samuel Colt, he came up with the idea for the revolver while at sea, inspired by the capstan, which had a ratchet and pawl mechanism on it, a version of which was used in his guns to rotate the cylinder. Revolvers proliferated largely due to Colt's ability as a salesman. But his influence spread in other ways as well; the build quality of his company's guns became famous, and its armories in America and England trained several seminal generations of toolmakers and other machinists, who had great influence in other manufacturing efforts of the next half century.
Early revolvers were caplocks and loaded like muskets: the user poured powder into a chamber, rammed down a bullet, then placed percussion caps between the hammer and cylinder. After firing a shot, the user would raise his pistol vertically as he cocked the hammer back so as to let the fragments of the percussion cap fall out and not jam the mechanism.
A more ambitious idea that had features of both revolvers and prefigured belt-fed machine guns circulated in the 1850s and 1860s. Instead of a simple cylinder, these guns used a larger capacity, somewhat flexible circular chain of chambers that was indexed by a slightly more sophisticated mechanism involving one or more sprockets. Examples include the 14-round Treeby "chain gun" developed in England, the 20-round Josselyn "chain revolver" developed in the US which is now part of the Smithsonian collection. None of these achieved much commercial success. George Chinn commented that "It is relatively easy to imagine what embarrassment might be experienced by a man who, in defense of his person, is required to extract from his pocket a gun with a foot or so of loose chain attached. Nevertheless, although the gun no doubt does not represent the most convenient hand arm, the basic idea is sound from the mechanical viewpoint and might even have proved to be useful in a machine gun. At any rate, it serves to demonstrate that very few stones were left unturned in the search for the ideal form of the multiple chamber mechanism."
Revolvers have remained popular to the present day in many areas, although in the military and law enforcement they have largely been supplanted by magazine-fed semi-automatic pistols such as the Beretta M9, especially in circumstances where reload time and higher cartridge capacity are deemed important.
Patents.
Elisha Collier of Boston, Massachusetts patented a flintlock revolver in Britain in 1818, and significant numbers were being produced in London by 1822. The origination of this invention is in doubt, as similar designs were patented in the same year by Artemus Wheeler in the United States and by Cornelius Coolidge in France. Samuel Colt submitted a British patent for his revolver in 1835 and an American patent (number 138) on February 25, 1836 for a "Revolving gun", and made the first production model on March 5 of that year.
Another revolver patent was issued to Samuel Colt on August 29, 1839. The February 25, 1836 patent was then reissued as U.S. Patent entitled "Revolving gun" to Samuel Colt on October 24, 1848. This was followed by U.S. Patent on September 3, 1850 for a "Revolver", and by U.S. Patent on September 10, 1850 for a "Revolver". U.S. Patent was issued to Roger C. Field for an economical device for minimizing the flash gap of a revolver between the barrel and the cylinder. In 1855, Rollin White patented the bored-through cylinder entitled "Improvement in revolving fire-arms" U.S. Patent . In 1856 Horace Smith & Daniel Wesson formed a partnership (S&W), developed and manufactured a revolver chambered for a self-contained metallic cartridge.
Design.
A revolver works by having several firing chambers arranged in a circle in a cylindrical block that are brought into alignment with the firing mechanism and barrel one at a time. In contrast, other repeating firearms, such as lever-action, pump-action, and semi-automatic, have a single firing chamber and a mechanism to load and extract cartridges into it.
A single-action revolver requires the hammer to be pulled back by hand before each shot, which also revolves the cylinder. This leaves the trigger with just one "single action" left to perform - releasing the hammer to fire the shot - so the force and distance required to pull the trigger can be minimal. In contrast, with a self-cocking revolver, one long squeeze of the trigger pulls back the hammer and revolves the cylinder, then finally fires the shot. They can generally be fired faster than a single-action, but with reduced accuracy in the hands of most shooters.
Most modern revolvers are "traditional double-action", which means they may operate either in single-action or self-cocking mode. The accepted meaning of "double-action" has, confusingly, come to be the same as "self-cocking", so modern revolvers that cannot be pre-cocked are called "double-action-only". These are intended for concealed carry, because the hammer of a traditional design is prone to snagging on clothes when drawn. Most revolvers do not come with accessory rails, which are used for mounting lights and lasers, except for the Smith & Wesson M&P R8 (.357 Magnum), Smith & Wesson Model 325 Thunder Ranch (.45 ACP), and all versions of the Chiappa Rhino (.357 Magnum, 9mm Parabellum, .40 S&W, or 9x21mm) except for the 2" model, respectively. However, certain revolvers, such as the Taurus Judge and Charter Arms revolvers, can be fitted with accessory rails.
Most commonly, such revolvers have a 5 or 6 shot capacity, hence the common names of "six-gun" or "six-shooter". However, some revolvers have a 7 to 10 shot capacity, often depending on the caliber, and at least one revolver has a 12 shot capacity (the US Fire Arms Model 12/22). Each chamber has to be reloaded manually, which makes reloading a revolver a much slower procedure than reloading a semi-automatic pistol.
Compared to autoloading handguns, a revolver is often much simpler to operate and may have greater reliability. For example, should a semiautomatic pistol fail to fire, clearing the chamber requires manually cycling the action to remove the errant round, as cycling the action normally depends on the energy of a cartridge firing. With a revolver, this is not necessary as none of the energy for cycling the revolver comes from the firing of the cartridge, but is supplied by the user either through cocking the hammer or, in a double action design, by just squeezing the trigger. Another significant advantage of revolvers is superior ergonomics, particularly for users with small hands. A revolver's grip does not hold a magazine, and it can be designed or customized much more than the grip of a typical semi-automatic. Partially because of these reasons, revolvers still hold significant market share as concealed carry and home-defense weapons.
A revolver can be kept loaded and ready to fire without fatiguing any springs and is not very dependent on lubrication for proper firing. Additionally, in the case of double-action-only revolvers there is no risk of accidental discharge from dropping alone, as the hammer is cocked by the trigger pull. However, the revolver's clockwork-like internal parts are relatively delicate and can become misaligned after a severe impact, and its revolving cylinder can become jammed by excessive dirt or debris.
Over the long period of development of the revolver, many calibers have been used. Some of these have proved more durable during periods of standardization and some have entered general public awareness. Among these are the .22 rimfire, a caliber popular for target shooting and teaching novice shooters; .38 Special and .357 Magnum, known for police use; the .44 Magnum, famous from Clint Eastwood's "Dirty Harry" films; and the .45 Colt, used in the Colt revolver of the Wild West. Introduced in 2003, the Smith & Wesson Model 500 is one of the most powerful revolvers, utilizing the .500 S&W Magnum cartridge.
Because the rounds in a revolver are headspaced on the rim, some revolvers are capable of chambering more than one type of ammunition. The .44 Magnum round will chamber the shorter .44 Special and shorter .44 Colt, likewise the .357 Magnum will safely chamber .38 Special and .38 Colt. In 1996 a revolver known as the Medusa M47 was made that could chamber 25 different cartridges with bullet diameters between .355" and .357".
Revolver technology lives on in other weapons used by the military. Some autocannons and grenade launchers use mechanisms similar to revolvers, and some riot shotguns use spring-loaded cylinders holding up to 12 rounds. In addition to serving as backup guns, revolvers still fill the specialized niche role as a shield gun; law enforcement personnel using a "bulletproof" ballistic shield (Gun shield) sometimes opt for a revolver instead of a self-loading pistol, because the slide of a pistol may strike the front of the shield when fired. Revolvers do not suffer from this disadvantage. A second revolver may be secured behind the shield to provide a quick means of continuity of fire. Many police also still use revolvers as their duty weapon due to their relative mechanical simplicity and user friendliness.
With the advancement of technology and design in 2010 major revolver manufacturers are coming out with polymer frame revolvers like the Ruger LCR, Smith & Wesson Bodyguard 38, and Taurus Protector Polymer. The new innovative design incorporates advanced polymer technology that lowers weight significantly, helps absorbs recoil, and strong enough to handle +P and .357 Magnum loads. The polymer is only used on the lower frame and joined to a metal alloy upper frame, barrel, and cylinder. Polymer technology is considered one of the major advancements in revolver history because the frame has always been metal alloy and mostly one piece frame design.
Another recent development in revolver technology is the Rhino, a revolver introduced by Italian manufacturer Chiappa in 2009 and first sold in the U.S. in 2010. The Rhino, built with the U.S. concealed carry market in mind, is designed so that the bullet fires from the bottom chamber of the cylinder instead of the top chamber as in standard revolvers. This is intended to reduce muzzle flip, allowing for faster and more accurate repeat shots. In addition, the cylinder cross-section is hexagonal instead of circular, further reducing the weapon's profile.
Loading and unloading.
Front loading.
The first revolvers were "front loading", and were a bit like muskets in that the powder and bullet were loaded separately. These were caplocks or "cap and ball" revolvers, because the caplock method of priming was the first to be compact enough to make a practical revolver feasible. When loading, each chamber in the cylinder was rotated out of line with the barrel, and charged from the front with loose powder and an oversized bullet. Next, the chamber was aligned with the ramming lever underneath the barrel. Pulling the lever would drive a rammer into the chamber, pushing the ball securely in place. Finally, the user would place percussion caps on the nipples on the rear face of the cylinder.
After each shot, a user was advised to raise his revolver vertically while cocking back the hammer so as to allow the fragments of the spent percussion cap to fall out safely. Otherwise, the fragments could fall into the revolver's mechanism and jam it. Caplock revolvers were vulnerable to "chain fires", wherein hot gas from a shot ignited the powder in the other chambers. This could be prevented by sealing the chambers with cotton, wax, or grease.
Loading a cylinder in this manner was a slow and awkward process and generally could not be done in the midst of battle. Some soldiers solved this by carrying multiple revolvers in the field. Another solution was to use a revolver with a detachable cylinder design. These revolvers allowed the shooter to quickly remove a cylinder and replace it with a full one.
Fixed cylinder designs.
In many of the first generation of cartridge revolvers (especially those that were converted after manufacture), the base pin on which the cylinder revolved was removed, and the cylinder taken from the revolver for loading. Most revolvers using this method of loading are single-action revolvers, although Iver Johnson produced double-action models with removable cylinders. The removable-cylinder design is employed in some modern "micro-revolvers" (usually in .22 caliber), in order to simplify their design. These weapons are small enough to fit in the palm of the hand.
Later single-action revolver models with a fixed cylinder used a loading gate at the rear of the cylinder that allowed insertion of one cartridge at a time for loading, while a rod under the barrel could be pressed rearward to eject the fired case.
The loading gate on the original Colt designs (and on nearly all single-action revolvers since, such as the famous Colt Single Action Army) is on the right side, which was done to facilitate loading while on horseback; with the revolver held in the left hand with the reins of the horse, the cartridges can be ejected and loaded with the right hand.
Because the cylinders in these types of revolvers are firmly attached at the front and rear of the frame, and the frame is typically full thickness all the way around, fixed cylinder revolvers are inherently strong designs. Accordingly, many modern large caliber hunting revolvers tend to be based on the fixed cylinder design. Fixed cylinder revolvers can fire the strongest and most powerful cartridges, but at the price of being the slowest to load and reload and they cannot use speedloaders or moon clips for loading, as only one chamber is exposed at a time to the loading gate.
Top break.
In a top-break revolver, the frame is hinged at the bottom front of the cylinder. Releasing the lock and pushing the barrel down exposes the rear face of the cylinder. In most top-break revolvers, this act also operates an extractor that pushes the cartridges in the chambers back far enough that they will fall free, or can be removed easily. Fresh rounds are then inserted into the cylinder. The barrel and cylinder are then rotated back and locked in place, and the revolver is ready to fire.
Top break revolvers can be loaded more rapidly than fixed-frame revolvers, especially with the aid of a speedloader or moon clip. However, this design is much weaker and cannot handle high pressure rounds. While this design is mostly obsolete today, supplanted by the stronger yet equally convenient swing-out design, manufacturers have begun making reproductions of late 19th century designs for use in cowboy action shooting.
The most commonly found top-break revolvers were manufactured by Smith & Wesson, Iver Johnson, Harrington & Richardson, Manhattan Fire Arms, Meriden Arms and Forehand & Wadsworth.
Tip up.
The tip-up was the first revolver design for use with metallic cartridges in the Smith & Wesson Model 1. It is similar to the break-open design that had a hinge on the top rear of the frame, but in the case of the tip-up, the barrel release catch is located on the side of the frame in front of the trigger. Smith & Wesson discontinued it in the third series of the Smith & Wesson Model 1 1/2 but it was fairly widely used in Europe in the 19th century, after a patent by Spirlet in 1870, which also included an ejector.
Swing out cylinder.
The most modern method of loading and unloading a revolver is by means of the "swing out cylinder". The cylinder is mounted on a pivot that is parallel to the chambers, and the cylinder swings out and down (to the left in most cases). An extractor is fitted, operated by a rod projecting from the front of the cylinder assembly. When pressed, it will push all fired rounds free simultaneously (as in top break models, the travel is designed to not completely extract longer, unfired rounds). The cylinder may then be loaded, singly or again with a speedloader, closed, and latched in place.
The pivoting part that supports the cylinder is called the crane; it is the weak point of swing-out cylinder designs. Using the method often portrayed in movies and television of flipping the cylinder open and closed with a flick of the wrist can in fact cause the crane to bend over time, throwing the cylinder out of alignment with the barrel. Lack of alignment between chamber and barrel is a dangerous condition, as it can impede the bullet's transition from chamber to barrel. This gives rise to higher pressures in the chamber, bullet damage, and the potential for an explosion if the bullet becomes stuck.
The shock of firing can exert a great deal of stress on the crane, as in most designs the cylinder is only held closed at one point, the rear of the cylinder. Stronger designs, such as the Ruger Super Redhawk, use a lock in the crane as well as the lock at the rear of the cylinder. This latch provides a more secure bond between cylinder and frame, and allows the use of larger, more powerful cartridges. Swing out cylinders are rather strong, but not as strong as fixed cylinders, and great care must be taken with the cylinder when loading, so as not to damage the crane.
Action.
Single-action.
In a single-action revolver, the hammer is manually cocked, usually with the thumb of the firing or supporting hand. This action advances the cylinder to the next round and locks the cylinder in place with the chamber aligned with the barrel. The trigger, when pulled, releases the hammer, which fires the round in the chamber. To fire again, the hammer must be manually cocked again. This is called "single-action" because the trigger only performs a single action, of releasing the hammer. Because only a single action is performed and trigger pull is lightened, firing a revolver in this way allows most shooters to achieve greater accuracy. Additionally, the need to cock the hammer manually acts as a safety. The Colt Paterson Revolver, the Walker Colt, the Colt's Dragoon and the Colt Single Action Army pistol of the American Frontier era are all good examples of this system.
Double-action.
In double-action (DA), the stroke of the trigger pull generates three actions: 
Thus, DA means that a cocking action separate from the trigger pull is unnecessary; every trigger pull will result in a complete cycle. This allows uncocked carry, while also allowing draw-and-fire using only the trigger. A longer and harder trigger stroke is the trade-off. However, this drawback can also be viewed as a safety feature, as the gun is safer against accidental discharges from being dropped.
Most double-action revolvers may be fired in two ways.
Certain revolvers, called "double action only" (DAO) or, more correctly but less commonly, "self cocking", lack the latch that enables the hammer to be locked to the rear, and thus can only be fired in the double action mode. With no way to lock the hammer back, DAO designs tend to have "bobbed" or "spurless" hammers, and may even have the hammer completely covered by the revolver's frame (i.e., shrouded or hooded). These are generally intended for concealed carrying, where a hammer spur could snag when the revolver is drawn. The potential reduction in accuracy in aimed fire is offset by the increased capability for concealment.
DA and DAO revolvers were the standard-issue sidearm of countless police departments for many decades. Only in the 1990s did the semiautomatic pistol begin to make serious inroads after the advent of safe actions. The reasons for these choices are the modes of carry and use. Double action is good for high-stress situations because it allows a mode of carry in which "draw and pull the trigger" is the only requirement—no safety catch release nor separate cocking stroke is required.
Other.
In the cap-and-ball days of the mid 19th century, two revolver models, the English Tranter and the American Savage “Figure Eight”, used a method whereby the hammer was cocked by the shooter’s middle finger pulling on a second trigger below the main trigger.
Iver Johnson made an unusual model from 1940 to 1947, called the "Trigger Cocking Double Action". If the hammer was down, pulling the trigger would cock the hammer. If the trigger was pulled with the hammer cocked, it would then fire. This meant that to fire the revolver from a hammer down state, the trigger must be pulled twice.
3D printed revolver.
The Zig zag revolver is a 3D printed .38 Revolver made public in May 2014. It was created using an $500 plastic 3D-printer used, the name of the printer was not revealed by the creator. It was created by a Japanese citizen from Kawasaki named Yoshitomo Imura He was arrested in May 2014 after he had posted a video online of himself firing a 3D printed Zig Zag revolver. It is the first 3D printed Japanese gun in the world which can discharge live cartridges.
Use with suppressors.
As a general rule, revolvers cannot be effective with a sound suppressor ("silencer"), as there is usually a small gap between the revolving cylinder and the barrel which a bullet must traverse or jump when fired. From this opening, a rather loud report is produced. A suppressor can only suppress noise coming from the muzzle.
A suppressible revolver design does exist in the Nagant M1895, a Belgian designed revolver used by Imperial Russia and later the Soviet Union from 1895 through World War II. This revolver uses a unique cartridge whose case extends beyond the tip of the bullet, and a cylinder that moves forward to place the end of the cartridge inside the barrel when ready to fire. This bridges the gap between the cylinder and the barrel, and expands to seal the gap when fired. While the tiny gap between cylinder and barrel on most revolvers is insignificant to the internal ballistics, the seal is especially effective when used with a suppressor, and a number of suppressed Nagant revolvers have been used since its invention.
There is a modern revolver of Russian design, the OTs-38, which uses ammunition that incorporates the silencing mechanism into the cartridge case, making the gap between cylinder and barrel irrelevant as far as the suppression issue is concerned. The OTs-38 does need an unusually close and precise fit between the cylinder and barrel due to the shape of bullet in the special ammunition (Soviet SP-4), which was originally designed for use in a semi-automatic.
Additionally, the US Military experimented with designing a special version of the Smith & Wesson Model 29 for Tunnel Rats, called the Quiet Special Purpose Revolver or QSPR. Using special .40 caliber ammunition, it never entered official service.
Automatic revolvers.
The term "automatic revolver" has two different meanings, the first being used in the late nineteenth and early twentieth centuries when "automatic" referred not to the operational mechanism of firing, but of extraction and ejection of spent casings. An "automatic revolver" in this context is one which extracts empty fired cases "automatically," i.e., upon breaking open the action, rather than requiring manual extraction of each case individually with a sliding rod or pin (as in the Colt Single Action Army design). This term was widely used in the advertising of the period as a way to distinguish such revolvers from the far more common rod-extraction types.
In the second sense, "automatic revolver" refers to the mechanism of firing rather than extraction. Double-action revolvers use a long trigger pull to cock the hammer, thus negating the need to manually cock the hammer between shots. The disadvantage of this is that the long, heavy pull cocking the hammer makes the double-action revolver much harder to shoot accurately than a single-action revolver (although cocking the hammer of a double action reduces the length and weight of the trigger pull). A rare class of revolvers, called automatic for its firing design, attempts to overcome this restriction, giving the high speed of a double-action with the trigger effort of a single-action. The Webley-Fosbery Automatic Revolver is the most famous commercial example. It was recoil-operated, and the cylinder and barrel recoiled backwards to cock the hammer and revolve the cylinder. Cam grooves were milled on the outside of the cylinder to provide a means of advancing to the next chamber—half a turn as the cylinder moved back, and half a turn as it moved forward. .38 caliber versions held eight shots, .455 caliber versions six. At the time, the few available automatic pistols were larger, less reliable, and more expensive. The automatic revolver was popular when it first came out, but was quickly superseded by the creation of reliable, inexpensive semi-automatic pistols.
In 1997, the Mateba company developed a type of recoil-operated automatic revolver, commercially named the Mateba Autorevolver, which uses the recoil energy to auto-rotate a normal revolver cylinder holding six or seven cartridges, depending on the model. The company has made several versions of its Autorevolver, including longer-barrelled and carbine variations, chambered for .357 Magnum, .44 Magnum and .454 Casull.
The Pancor Jackhammer is a combat shotgun based on a similar mechanism to an automatic revolver. It uses a Blow-Forward action to move the barrel forward (which unlocks it from the cylinder) and then rotate the cylinder and cock the hammer.
Revolving long guns.
Revolvers were not limited to handguns and as a longer barrelled arm is more useful in military applications than a sidearm, the idea was applied to both rifles and shotguns throughout the history of the revolver mechanism with mixed degrees of success.
Rifles.
Revolving rifles were an attempt to increase the rate of fire of rifles by combining them with the revolving firing mechanism that had been developed earlier for revolving pistols. Colt began experimenting with revolving rifles in the early 19th century, making them in a variety of calibers and barrel lengths. Colt revolving rifles were the first repeating rifles adopted by the U.S. Government, but they had their problems. They were officially given to soldiers because of their rate of fire. But after firing six shots, the shooter had to take an excessive amount of time to reload. Also, on occasion Colt rifles discharged all their rounds at once, endangering the shooter. Even so, an early model was used in the Seminole Wars in 1838. During the Civil War a LeMat Carbine was made based on the LeMat revolver.
Shotguns.
Colt briefly manufactured several revolving shotguns that were met with mixed success. The Colt Model 1839 Shotgun was manufactured between 1839 and 1841. Later, the Colt Model 1855 Shotgun, based on the Model 1855 revolving rifle, was manufactured between 1860 and 1863. Because of their low production numbers and age they are among the rarest of all Colt firearms.
The Armsel Striker was a modern take on the revolving shotgun that held 10 rounds of 12 Gauge ammunition in its cylinder. It was copied by Cobray as the Streetsweeper.
Taurus manufactures a carbine variant of the Taurus Judge revolver along with its Australian partner company, Rossi known as the "Taurus/Rossi Circuit Judge". It comes in the original combination chambering of .410 bore and .45 Long Colt, as well as the .44 Remington Magnum chambering. The rifle has small blast shields attached to the cylinder to protect the shooter from hot gases escaping between the cylinder and barrel.
Six gun.
A Six Gun is a revolver that holds six cartridges. The cylinder in a six gun is often called a 'wheel', and the six gun is itself often called a 'wheel gun'. Although a "Six Gun" can refer to any six-chambered revolver, it is typically a reference to the Colt Single Action Army, or its modern look-alikes such as the Ruger Vaquero and Beretta Stampede.
Until the 1970s, when older-design revolvers such as Colt Single Action Armys and Ruger Blackhawks were re-engineered with drop safeties (such as firing pin blocks, hammer blocks, or transfer bars) that prevent the firing pin from contacting the cartridge's primer unless the trigger is pulled, safe carry required the hammer being positioned over an empty chamber, reducing the available cartridges from six to five, or, on some models, in between chambers on either a pin or in a groove for that purpose, thus keeping the full six rounds available. This kept the uncocked hammer from resting directly on the primer of a cartridge. If not used in this manner, the hammer rests directly on a primer and unintentional firing may occur if the gun is dropped or the hammer is struck. Some holster makers provided a thick leather thong to place underneath the hammer that both allowed the carry of a gun fully loaded with all six rounds and secured the gun in the holster to help prevent its accidental loss. Human nature being what it has always been, some people simply took the risk and carried the guns fully loaded with no provisions for prevention of accidental discharges.
Six guns are used commonly by Single-Action Shooting enthusiasts in shooting competitions, designed to mimic the gunfights of the Old West, and for general target shooting, hunting and personal defense.

</doc>
<doc id="25795" url="http://en.wikipedia.org/wiki?curid=25795" title="Robert Freitas">
Robert Freitas

Robert A. Freitas Jr. (born 1952) is a Senior Research Fellow, one of four researchers at the nonprofit foundation Institute for Molecular Manufacturing in Palo Alto, California.
Career.
Freitas holds a 1974 Bachelor's degree majoring in both physics and psychology from Harvey Mudd College, and a 1978 Juris Doctor (J.D.) degree from Santa Clara University. He has written more than 150 technical papers, book chapters, or popular articles on a diverse set of scientific, engineering, and legal topics. He co-edited the 1980 NASA feasibility analysis of self-replicating space factories and later authored the first detailed technical design study of a hypothetical medical nanorobot, the respirocyte, ever published in a refereed medical journal.
In 1977-78 Robert Freitas created the concept sentience quotient (SQ) as a way to describe the information processing rate in living organisms or computers. Freitas is authoring the multi-volume text "Nanomedicine", the first book-length technical discussion of the potential medical applications of hypothetical molecular nanotechnology and medical nanorobotics. Volume I was published in October 1999 by Landes Bioscience while Freitas was a Research Fellow at the Institute for Molecular Manufacturing . He published Volume IIA in October 2003 with Landes Bioscience while serving as a research scientist at Zyvex Corp., a nanotechnology company headquartered in Richardson, Texas, during 2000-2004.
Also in 2004, Robert Freitas and Ralph Merkle coauthored and published "Kinematic Self-Replicating Machines", the first complete survey of the field of physical and hypothetical self-replicating machines. In 2006, Freitas and Merkle co-founded the Nanofactory Collaboration, a research program to develop the first working diamondoid nanofactory.
He received the 2007 Foresight Prize in Communication from the Foresight Institute. In 2009, Freitas was awarded the Feynman Prize in Nanotechnology for Theory.
In 2010, Freitas was granted a patent for what was at the time (2004) the first patent application ever filed on diamond mechanosynthesis.

</doc>
<doc id="25797" url="http://en.wikipedia.org/wiki?curid=25797" title="Robert Morris">
Robert Morris

Robert or Bob Morris may refer to:

</doc>
<doc id="25798" url="http://en.wikipedia.org/wiki?curid=25798" title="Reykjavík">
Reykjavík

Reykjavík (], ) is the capital and largest city of Iceland. Its latitude, at 64°08' N, makes it the world's northernmost capital of a sovereign state and a popular tourist destination. It is located in southwestern Iceland, on the southern shore of the Faxaflói Bay. With a population of around 120,000 (and over 200,000 in the Capital Region), it is the heart of Iceland's cultural, economic and governmental activity.
Reykjavík is believed to be the location of the first permanent settlement in Iceland, which Ingólfur Arnarson is said to have established around AD 870. Until the 18th century, there was no urban development in the city location. The city was founded in 1786 as an official trading town and grew steadily over the next decades, as it transformed into a regional and later national center of commerce, population, and governmental activities. It is among the cleanest, greenest, and safest cities in the world.
History.
The first permanent settlement in Iceland by Norsemen is believed to have been established in Reykjavík by Ingólfur Arnarson from Norway around AD 870; this is described in "Landnámabók", or the Book of Settlement. Ingólfur Arnarson is said to have decided the location of his settlement using a traditional Viking method; he cast his high seat pillars (Öndvegissúlur) into the ocean when he saw the coastline, then settled where the pillars came to shore. Steam from hot springs in the region is said to have inspired Reykjavík's name, which loosely translates to Smoke Cove (the city is often referred to as the Bay of Smokes or Bay of Smoke) The original name was Reykja"r"vík with an additional "r" that vanished around 1300.
Reykjavík is not mentioned in any medieval sources except as farmland, but the 18th century saw the beginning of urban concentration there. The Danish rulers of Iceland backed the idea of domestic industry in Iceland that would stimulate much-needed progress on the island. In 1752, the King of Denmark, Frederik V, donated the estate of Reykjavík to the Innréttingar Corporation; the name comes from Danish "indretninger", meaning institution. The leader of this movement was Skúli Magnússon. In the 1750s several houses were built to house the wool industry that was to be Reykjavík's most important employer for a few decades and the original reason for its existence. Other crafts were also practised by the Innréttingar, such as fisheries, sulphur mining, agriculture, and shipbuilding.
The Danish Crown abolished monopoly trading in 1786 and granted six communities around the country an exclusive trading charter, Reykjavík was one of them and the only one to hold on to the charter permanently. The year 1786 is regarded as the date of the city's founding; its 200th anniversary was celebrated in 1986. Trading rights were still limited to the subjects of the Danish Crown, and Danish traders continued to dominate trade in Iceland. Over the following decades, their business in Iceland expanded. After 1880, free trade was expanded to all nationalities and the influence of Icelandic merchants started to grow.
Rise of nationalism.
Icelandic nationalist sentiment gained influence in the 19th century and the idea of Icelandic independence became widespread. Reykjavík, as Iceland's only city, was the melting pot of such ideas. Advocates of an independent Iceland realized that a strong Reykjavík was fundamental to that objective. All the important events in the history of the independence struggle are important for Reykjavík as well. In 1845, Alþingi, the general assembly formed in AD 930, was re-established in Reykjavík; it had been suspended a few decades earlier when it was located at Þingvellir. At the time it functioned only as an advisory assembly, advising the King about Icelandic affairs. The location of Alþingi in Reykjavík effectively established the city as the capital of Iceland.
In 1874 Iceland was given a constitution; with it, Alþingi gained some limited legislative powers and in essence became the institution that it is today. The next step was to move most of the executive power to Iceland: Home Rule was granted in 1904 when the office of minister for Iceland was established in Reykjavík. The biggest step towards an independent Iceland was taken on 1 December 1918 when Iceland became a sovereign country under the Crown of Denmark, the Kingdom of Iceland.
In the 1920s and 1930s most of the growing Icelandic fishing trawler fleet sailed from Reykjavík and salt-cod production was the main industry, but the Great Depression hit Reykjavík hard with unemployment and labour union struggles that sometimes became violent.
World War II.
On the morning of 10 May 1940, following the German occupation of Denmark and Norway on 9 April 1940, four British warships approached Reykjavík and anchored in the harbour. In a few hours, the allied occupation of Reykjavík was complete. There was no armed resistance, and taxi and truck drivers even assisted the invasion force, which initially had no motor vehicles. The Icelandic government had received many requests from the British government to consent to the occupation, but it always declined on the basis of the Neutrality Policy. For the remaining years of World War II, British and later American soldiers occupied camps in Reykjavík, and the number of foreign soldiers in Reykjavík became about the same as the local population of the town. The Royal Regiment of Canada (RREGTC) formed part of the garrison in Iceland during the early part of the war.
The economic effects of the occupation were positive for Reykjavík: the unemployment of the depression years vanished and construction work began. The British built Reykjavík Airport, which is still in service today, mostly serving domestic flights. The Americans, meanwhile, built Keflavík Airport, situated 50 km west of Reykjavík, which would become Iceland's primary international airport. In 1944 the Republic of Iceland was founded and a president, elected by the people, replaced the King; the office of the president was placed in Reykjavík.
Post-war development.
In the post-war years the growth of Reykjavík accelerated. A mass exodus from the rural countryside began, largely due to improved technology in agriculture that reduced the need for manpower, and because of the population boom resulting from better living conditions in the country. A once primitive village was rapidly transformed into a modern city. Private cars became common and modern apartment complexes rose in the expanding suburbs. Much of Reykjavík lost its village feel. In 1972, Reykjavík hosted the world chess championship between Bobby Fischer and Boris Spassky.
Reykjavík has in the last three decades become a significant player in the global community. The 1986 Reykjavík Summit between Ronald Reagan and Mikhail Gorbachev underlined Reykjavík's new-found international status. Deregulation in the financial sector and the computer revolution of the 1990s again transformed Reykjavík. The financial sector and information technology are now significant employers in the city. The city has fostered some world-famous talents in recent years, such as Björk, Ólafur Arnalds and bands Múm, Sigur Rós, and Of Monsters and Men, and poet Sjón.
Geography.
Reykjavík is located in southwest Iceland. The Reykjavík area coastline is characterized by peninsulas, coves, straits, and islands.
During the Ice Age (up to 10,000 years ago) a large glacier covered parts of the city area, reaching as far out as Álftanes. Other parts of the city area were covered by sea water. In the warm periods and at the end of the Ice Age, some hills like Öskjuhlíð were islands. The former sea level is indicated by sediments (with clams) reaching (at Öskjuhlíð, for example) as far as 43 m above the current sea level. The hills of Öskjuhlíð and Skólavörðuholt appear to be the remains of former shield volcanoes which were active during the warm periods of the Ice Age.
After the Ice Age, the land rose as the heavy load of the glaciers fell away, and began to look as it does today.
The capital city area continued to be shaped by earthquakes and volcanic eruptions, like the one 4,500 years ago in the mountain range Bláfjöll, when the lava coming down the Elliðaá valley reached the sea at the bay of Elliðavogur.
The largest river to run through Reykjavík is the Elliðaá River, which is non-navigable. It is one of the best salmon fishing rivers in the country. Mount Esja, at 914 m, is the highest mountain in the vicinity of Reykjavík.
The city of Reykjavík is mostly located on the Seltjarnarnes peninsula, but the suburbs reach far out to the south and east. Reykjavík is a spread-out city: most of its urban area consists of low-density suburbs, and houses are usually widely spaced. The outer residential neighbourhoods are also widely spaced from each other; in between them are the main traffic arteries and a lot of empty space.
Panorama of Reykjavík seen from Perlan with the mountains Akrafjall (middle) and Esja (right) in the background
Panorama of Reykjavík seen from Perlan in summer during sunset.
Climate.
Temperatures very rarely drop below -15 C in the winter. This is because the Icelandic coastal weather in winter is moderated by the warm waters of the Gulf Stream. The climate is subpolar oceanic (Koppen: "Cfc "), and the city is on the northern edge of the temperate zone. The city's coastal location does make it prone to wind, however, and gales are common in winter. Summers are cool, with temperatures fluctuating between 10 and, sometimes exceeding 20 °C. Reykjavík is not a particularly wet city, but it nevertheless averages 148 days with measurable precipitation every year. Droughts are uncommon although they occur in some summers. In the summer of 2007, no rain was measured for one month. Spring tends to be the sunniest season, May particularly. Annual sunshine hours in Reykjavík are around 1,300, which is comparable with other places in Northern and North-Eastern Europe, such as Glasgow, Scotland. The highest ever recorded temperature in Reykjavík was 26.2 °C, recorded on 30 July 2008, while the lowest ever recorded temperature was -24.5 C, recorded on 21 January 1918. The temperature has not dropped to below -20 °C since 30 January 1971.
Cityscape.
Panorama of the northern seashore of Reykjavík, as seen from Örfirisey.
City administration.
The Reykjavik City Council governs the city of Reykjavík according to law number 45/1998 and is directly elected by those aged over 18 domiciled in the city. The council has 15 members who are elected using the open list method for 4 year terms.
The council selects members of boards, and each board controls a different field under the city council's authority. The most important board is the City Board that wields the executive rights along with the City Mayor. The City Mayor is the senior public official and also the director of city operations. Other public officials control city institutions under the mayor's authority. Thus the administration consists of two different parts:
Political control.
The Independence Party was traditionally the ruling party for the city, having an overall majority from its establishment in 1929 until 1978, when it was narrowly lost. From 1978 to 1982, a three party coalition composed of the People's Alliance, the Social Democratic Party and the Progressive Party formed the majority of the council. In 1982, the Independence Party regained an overall majority of the seats which it held for three consecutive terms. In 1994, Icelandic socialist parties formed an alliance called the Reykjavíkurlistinn (R-list) which was led by Ingibjörg Sólrún Gísladóttir to victory. The alliance stood for election for three consecutive city council elections and won a majority in all of them, until it was dissolved for the city council election of 2006 when five different parties were on the ballot. The Independence Party obtained 7 members of the council, and thus failed to gain overall control, but together with the Progressive Party, and its one council member, they were able to form a new majority in the council which took over in June 2006. In October 2007 a new majority was formed on the council, consisting of members of the Progressive Party (1), the Social Democratic Alliance (4), the Left-Greens (2) and the F-list (1) (liberals and independents), after controversy regarding REI, a subsidiary of OR, the city's energy company. However three months later the leader of the F-list formed a new majority together with the Independence Party. Ólafur F. Magnússon, the leader of the F-list, was elected mayor on 24 January 2008, and in March 2009 the Independence Party was due to appoint a new mayor. This changed once again on 14 August 2008 when the fourth majority of the term was formed, when the Independence Party and the Social Democratic Alliance formed a majority, with Hanna Birna Kristjánsdóttir becoming mayor. The City Council election in May 2010 saw a new political party, The Best Party, win 6 of 15 seats and they formed a coalition with the Social Democratic Alliance with comedian Jón Gnarr becoming mayor. At the 2014 election, the Social Democratic Alliance had its best showing yet gaining 5 seats in the council, while Bright future (successor to the Best Party) received 2 seats and the two parties formed a coalition with the Left-Green movement and the Pirate party both of which received 1 councilor each. The Independence Party received its worst election with only four seats in the council.
Mayor.
The mayor is appointed by the city council; usually one of the council members is chosen but they may also appoint a mayor who is not a member of the council.
The post was created in 1907 and advertised in 1908. Two applications were received, from Páll Einarsson, sheriff and town mayor of Hafnarfjörður and from Knud Zimsen, town councillor in Reykjavík. Páll was appointed on 7 May and was mayor for six years. At that time the city mayor received a salary of 4500 ISK per year and 1500 ISK for office expenses. The current mayor is Dagur B. Eggertsson.
Demographics.
Reykjavík is the largest and most populous settlement in Iceland. Present-day Reykjavík is a city with people from at least 100 countries. The most common ethnic minorities are Poles, Lithuanians, and Danes. In 2009, foreign-born individuals made up 8% of the total population. Children of foreign origin, many of whom are adopted, form a more considerable minority in the city's schools: as many as a third in places. The city is also visited by thousands of tourists, students and other temporary residents, at times outnumbering natives in the city centre.
The population of Reykjavík in 2011 was 119,848, and the combined population of the Capital Region was about 202,341. Six of the municipalities of Iceland are in the capital city area:
Districts.
Reykjavík is divided into 10 districts.
Economy.
Borgartún is the financial centre of Reykjavík, hosting a large number of companies and three investment banks.
Reykjavík has been at the centre of Iceland's economic growth and subsequent economic contraction over the last decade, a period referred to in foreign media as the "Nordic Tiger" years, or "Iceland's Boom Years". The economic boom led to a sharp increase in construction, with large redevelopment projects such as Harpa concert hall and conference centre and others.
In 2009, Reykjavík was listed as the richest city in the world in 2007 by The Economist Group.
Infrastructure.
Roads.
Per capita car ownership in Iceland is among the highest in the world at roughly 522 vehicles per 1,000 residents, though Reykjavík is not severely affected by congestion. Several multi-lane highways (mainly dual carriageways) run between the most heavily populated areas and most frequently driven routes. Parking spaces are also plentiful in most areas. Public transportation consists of a bus system called Strætó bs. Route 1 (the Ring Road) runs through the city outskirts and connects the city to the rest of Iceland.
Airports and seaports.
Reykjavík Airport, the second largest airport in the country (after Keflavík International Airport), is positioned inside the city, just south of the city centre. It is mainly used for domestic flights, as well as flights to Greenland and the Faroe Islands. It was built there by the British occupation force during World War II, when it was on the outskirts of the then much smaller Reykjavík. Since 1962 there has been some controversy regarding the location of the airport, since it takes up a lot of valuable space in central Reykjavík.
Reykjavík has two seaports, the old harbour near the city centre which is mainly used by fishermen and cruise ships and "Sundahöfn" in the east city which is the largest cargo port in the country.
Railways.
There are no public railways in Iceland, due to its sparse population, but the locomotives used to build the docks are on display.
District heating.
Volcanic activity provides Reykjavík with geothermal heating systems for both residential and industrial districts. In 2008, natural hot water was used to heat roughly 90% of all buildings in Iceland. Of total annual use of geothermal energy of 39 PJ, space heating accounted for 48%.
Most of the district heating in Iceland comes from three main geothermal power plants:
Cultural heritage.
The "Culture House" was opened in 1909 and has a number of important exhibits. Originally built for to house the National Library and National Archives and also previously the location of the National Museum and Natural History Museum, in 2000 it was re-modelled to promote the Icelandic national heritage. Many of Iceland's national treasures are on display, such as the Poetic Edda, and the Sagas, in their original manuscripts. There are also changing exhibitions on various topics.
Lifestyle.
Nightlife.
Reykjavík is famous for its weekend nightlife. Icelanders tend to go out late, so bars that look rather quiet can fill up suddenly—usually after midnight on a weekend.
Alcohol is relatively expensive at bars. People tend to drink at home before going out. Beer was banned in Iceland until 1 March 1989, but has since become popular among many Icelanders as their alcoholic drink of choice.
There are over 100 different bars and clubs in Reykjavík; most of them are located on Laugavegur and its side streets. It is very common for an establishment that is a café before dinner to turn into a bar in the evening. Closing time is usually around 4:30 am at weekends and 1 am during the week. The Iceland Airwaves music festival is annually staged in October.
New Year's Eve.
The arrival of the new year is a particular cause for celebration to the people of Reykjavík. Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. As a result, every New Year's Eve the city is lit up with fireworks displays.
Twin towns and sister cities.
In July 2013, mayor Jón Gnarr filed a motion before the city council to terminate the city's relationship with Moscow, in response to a trend of anti-gay legislation in Russia. According to "The Daily Telegraph", "Mr Gnarr has long been an advocate for gay rights, appearing in Gay Pride parades in drag"; in 2009, Iceland was the first modern country to have an openly LGBT head of government (Jóhanna Sigurðardóttir, who is a lesbian), and the Alþingi unanimously legalized same-sex marriage in 2010.
References.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Reykjavík" article dated 2008-06-23, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="25799" url="http://en.wikipedia.org/wiki?curid=25799" title="Retrovirus">
Retrovirus

"Retroviridae" is a family of enveloped viruses that replicate in a host cell through the process of reverse transcription. A retrovirus is a single-stranded positive sense RNA virus with a DNA intermediate and, as an obligate parasite, targets a host cell. Once inside the host cell cytoplasm, the virus uses its own reverse transcriptase enzyme to produce DNA from its RNA genome, the reverse of the usual pattern, thus "retro" (backwards). This new DNA is then incorporated into the host cell genome by an integrase enzyme, at which point the retroviral DNA is referred to as a provirus. The host cell then treats the viral DNA as part of its own genome, translating and transcribing the viral genes along with the cell's own genes, producing the proteins required to assemble new copies of the virus. It is difficult to detect the virus until it has infected the host. At that point, the infection will persist indefinitely.
In most viruses, DNA is transcribed into RNA, and then RNA is translated into protein. However, retroviruses function differently – their RNA is reverse-transcribed into DNA, which is integrated into the host cell's genome (when it becomes a provirus), and then undergoes the usual transcription and translational processes to express the genes carried by the virus. So, the information contained in a retroviral gene is used to generate the corresponding protein via the sequence: RNA → DNA → RNA → polypeptide. This extends the fundamental process identified by Francis Crick, (one gene-one peptide), in which the sequence is: DNA → RNA → peptide, (proteins are made of one or more polypeptide chain e.g. haemoglobin is a four chain peptide).
Retroviruses are proving to be valuable research tools in molecular biology and have been used successfully in gene delivery systems.
Structure.
Virions of retroviruses consist of enveloped particles about 100 nm in diameter. The virions also contain two identical single-stranded RNA molecules 7–10 kilobases in length. Although virions of different retroviruses do not have the same morphology or biology, all the virion components are very similar.
The main virion components are:
Multiplication.
When retroviruses have integrated their own genome into the germ line, their genome is passed on to a following generation. These endogenous retroviruses (ERVs), contrasted with exogenous ones, now make up 5-8% of the human genome. Most insertions have no known function and are often referred to as "junk DNA". However, many endogenous retroviruses play important roles in host biology, such as control of gene transcription, cell fusion during placental development in the course of the germination of an embryo, and resistance to exogenous retroviral infection. Endogenous retroviruses have also received special attention in the research of immunology-related pathologies, such as autoimmune diseases like multiple sclerosis, although endogenous retroviruses have not yet been proven to play any causal role in this class of disease.
While transcription was classically thought to occur only from DNA to RNA, reverse transcriptase transcribes RNA into DNA. The term "retro" in retrovirus refers to this reversal (making DNA from RNA) of the central dogma of molecular biology. Reverse transcriptase activity outside of retroviruses has been found in almost all eukaryotes, enabling the generation and insertion of new copies of retrotransposons into the host genome. These inserts are transcribed by enzymes of the host into new RNA molecules that enter the cytosol. Next, some of these RNA molecules are translated into viral proteins. For example, the "gag" gene is translated into molecules of the capsid protein, the "pol" gene is translated into molecules of reverse transcriptase, and the "env" gene is translated into molecules of the envelope protein. It is important to note that a retrovirus must "bring" its own reverse transcriptase in its capsid, otherwise it is unable to utilize the enzymes of the infected cell to carry out the task, due to the unusual nature of producing DNA from RNA.
Industrial drugs that are designed as protease and reverse transcriptase inhibitors are made such that they target specific sites and sequences within their respective enzymes. However these drugs can quickly become ineffective due to the fact that the gene sequences that code for the protease and the reverse transcriptase quickly mutate. These changes in bases cause specific codons and sites with the enzymes to change and thereby avoid drug targeting by losing the sites that the drug actually targets.
Because reverse transcription lacks the usual proofreading of DNA replication, a retrovirus mutates very often. This enables the virus to grow resistant to antiviral pharmaceuticals quickly, and impedes the development of effective vaccines and inhibitors for the retrovirus.
One difficulty faced with some retroviruses, such as the Moloney retrovirus, involves the requirement for cells to be actively dividing for transduction. As a result, cells such as neurons are very resistant to infection and transduction by retroviruses. This gives rise to a concern that insertional mutagenesis due to integration into the host genome might lead to cancer or leukemia. This is unlike "Lentivirus", a genus of "Retroviridae", which are able to integrate their RNA into the genome of non-dividing host cells.
Genes.
Retrovirus genomes commonly contain these three open reading frames that encode for proteins that can be found in the mature virus:
Provirus.
This DNA can be incorporated into host genome as a provirus that can be passed on to progeny cells. The retrovirus DNA is inserted at random into the host genome. Because of this, it can be inserted into oncogenes. In this way some retroviruses can convert normal cells into cancer cells. Some provirus remains latent in the cell for a long period of time before it is activated by the change in cell environment.
Early evolution.
Studies of retroviruses led to the first demonstrated synthesis of DNA from RNA templates, a fundamental mode for transferring genetic material that occurs in both eukaryotes and prokaryotes. It has been speculated that the RNA to DNA transcription processes used by retroviruses may have first caused DNA to be used as genetic material. In this model, the RNA world hypothesis, cellular organisms adopted the more chemically stable DNA when retroviruses evolved to create DNA from the RNA templates.
Gene therapy.
Gammaretroviral and lentiviral vectors for gene therapy have been developed that mediate stable genetic modification of treated cells by chromosomal integration of the transferred vector genomes. This technology is of use, not only for research purposes, but also for clinical gene therapy aiming at the long-term correction of genetic defects, e.g., in stem and progenitor cells. Retroviral vector particles with tropism for various target cells have been designed. Gammaretroviral and lentiviral vectors have so far been used in more than 300 clinical trials, addressing treatment options for various diseases. Retro viral mutations can be developed to make transgenic mouse models to study various cancers and their metastatic models.
Cancer.
Retroviruses that cause tumor growth include "Rous sarcoma virus" and "Mouse mammary tumor virus". Cancer can be triggered by proto-oncogenes that were mistakenly incorporated into proviral DNA or by the disruption of cellular proto-oncogenes. Rous sarcoma virus contains the src gene that triggers tumor formation. Later it was found that a similar gene in cells is involved in cell signaling, which was most likely excised with the proviral DNA. Nontransforming viruses can randomly insert their DNA into proto-oncogenes, disrupting the expression of proteins that regulate the cell cycle. The promoter of the provirus DNA can also cause over expression of regulatory genes.
Classification.
Exogenous.
These are infectious RNA-containing viruses which are transmitted from human to human.
The following genera are included here:
These were previously divided into three subfamilies ("Oncovirinae", "Lentivirinae", and "Spumavirinae"), but are now divided into two: "Orthoretrovirinae" and "Spumaretrovirinae". The term oncovirus is now commonly used to describe a cancer-causing virus.
Retroviruses were in 2 groups of the Virus classification#Baltimore classification.
Group VI viruses.
All members of Group VI use virally encoded reverse transcriptase, an RNA-dependent DNA polymerase, to produce DNA from the initial virion RNA genome. This DNA is often integrated into the host genome, as in the case of retroviruses and pseudoviruses, where it is replicated and transcribed by the host.
Group VI includes:
Group VII viruses.
Both families in Group VII have DNA genomes contained within the invading virus particles. The DNA genome is transcribed into both mRNA, for use as a transcript in protein synthesis, and pre-genomic RNA, for use as the template during genome replication. Virally encoded reverse transcriptase uses the pre-genomic RNA as a template for the creation of genomic DNA.
Group VII includes:
Endogenous.
Endogenous retroviruses are not formally included in this classification system, and are broadly classified into three classes, on the basis of relatedness to exogenous genera:
Treatment.
Antiretroviral drugs are medications for the treatment of infection by retroviruses, primarily HIV. Different classes of antiretroviral drugs act on different stages of the HIV life cycle. Combination of several (typically three or four) antiretroviral drugs is known as highly active anti-retroviral therapy (HAART).
Treatment of veterinary retroviruses.
"Feline leukemia virus" and "Feline immunodeficiency virus" infections are treated with biologics, including the only immunomodulator currently licensed for sale in the United States, Lymphocyte T-Cell Immune Modulator (LTCI).

</doc>
<doc id="25801" url="http://en.wikipedia.org/wiki?curid=25801" title="Round (music)">
Round (music)

A round or perpetual canon is a musical composition, a limited type of canon, in which a minimum of three voices sing exactly the same melody at the unison (and may continue repeating it indefinitely), but with each voice beginning at different times so that different parts of the melody coincide in the different voices, but nevertheless fit harmoniously together . It is one of the easiest forms of part singing, as only one line of melody need be learned by all parts, and is part of a popular musical tradition. They were particularly favoured in glee clubs, which combined amateur singing with regular drinking (, especially at 21: "Catch-singing is unthinkable without a supply of liquor to hand..."). The earliest known rounds date from the 12th century.
"Row, Row, Row Your Boat" is a well-known children's round for four voices. Other well-known examples are "Frère Jacques" and "Three Blind Mice" , and "London's Burning" (  ). However, not all rounds are nursery rhymes. Serious composers who turned their hand to the round format include Thomas Arne, John Blow, William Byrd, Henry Purcell, Louis Hardin, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Benjamin Britten (for example, "Old Joe Has Gone Fishing", sung by the villagers in the pub to keep the peace, at the end of act 1 of "Peter Grimes") . (   by Joseph Haydn)
A catch is a round in which a phrase that is not apparent in a single line of lyrics emerges when the lyrics are split between the different voices.
History.
The term "round" first appears in English in the early 16th century, though the form was found much earlier. In medieval England, they were called rota or rondellus. Later, an alternative term was "roundel" (e.g., David Melvill's manuscript "Ane Buik off Roundells", Aberdeen, 1612). Special types of rounds are the "catch" (a comic English form found from about 1580 to 1800), and a specialized use of the word "canon", in 17th- and 18th-century England designating rounds with religious texts . The oldest surviving round in English is "Sumer Is Icumen In"   , which is for 4 voices, plus 2 bass voices singing a ground (that is, a never-changing repeating part), also in canon. However, the earliest known rounds are two works found in the eleventh fascicle of the Notre Dame manuscript Pluteo 29.1. They are "Leto leta concio" (a two-voice round) and "O quanto consilio" (a four-voice round). The former dates from before 1180 and may be of German origin . The first published rounds in English were printed by Thomas Ravenscroft in 1609... "Three Blind Mice"    appears in this collection, although in a somewhat different form from today's children's round:
Many of the rounds printed by Ravenscroft also appear in a 1580 manuscript (KC 1), and several are mentioned in Shakespeare's plays, so these little ditties seem to have been quite popular.
Mechanics.
The canon, or rule, of a simple round is that each voice enters after a set interval of time, at the same pitch, using the same notes.
What makes a round work is that after the work is divided into equal-sized blocks of a few measures each, corresponding notes in each block either are the same, or are different notes in the same chord. This is easiest with one chord, as in "Row, Row, Row Your Boat"  :
A new part can join the singing by starting at the beginning whenever another part reaches any asterisk in the above music. If one ignores the sixteenth notes that pass between the main chords, every single note is in the tonic triad—in this case, a C, E, or G.
Many rounds involve more than one chord, as in "Frère Jacques"   :
The texture is simpler, but it uses a few more notes; this can perhaps be more easily seen if all four parts are run together into the same two measures:
The second beat of each measure does not sketch out a tonic triad, it outlines a dominant seventh chord (or "V7 chord").
Many different chord progressions are theoretically possible in a round, but it can be very challenging to keep each part sounding different and yet still melodic as they trace out the appropriate chords.

</doc>
<doc id="25806" url="http://en.wikipedia.org/wiki?curid=25806" title="Reincarnation">
Reincarnation

Reincarnation is the religious or philosophical concept that the soul or spirit, after biological death, can begin a new life in a new body. This doctrine is a central tenet of the Indian religions. It is also a common belief of various ancient and modern religions such as Spiritism, Theosophy, and Eckankar is found in many tribal societies around the world, in places such as Siberia, West Africa, North America, and Australia.
Although the majority of sects within the Abrahamic religions of Judaism, Christianity, and Islam do not believe that individuals reincarnate, particular groups within these religions do refer to reincarnation; these groups include the mainstream historical and contemporary followers of Kabbalah, the Cathars, the Druze and the Rosicrucians. The historical relations between these sects and the beliefs about reincarnation that were characteristic of Neoplatonism, Orphism, Hermeticism, Manicheanism and Gnosticism of the Roman era, as well as the Indian religions, has been the subject of recent scholarly research.
In recent decades, many Europeans and North Americans have developed an interest in reincarnation. Contemporary films, books, and popular songs frequently mention reincarnation.
Conceptual definitions.
The word "reincarnation" derives from Latin, literally meaning, "entering the flesh again". The Greek equivalent "metempsychosis" (μετεμψύχωσις) roughly corresponds to the common English phrase "transmigration of the soul" and also usually connotes reincarnation after death, as either human, animal, though emphasising the continuity of the soul, not the flesh. The term has been used by modern philosophers such as Kurt Gödel and has entered the English language. Another Greek term sometimes used synonymously is "palingenesis", "being born again".
There is no word corresponding exactly to the English terms "rebirth", "metempsychosis", "transmigration" or "reincarnation" in the traditional languages of Pāli and Sanskrit. The entire universal process that gives rise to the cycle of death and rebirth, governed by karma, is referred to as "Samsara" while the state one is born into, the individual process of being born or coming into the world in any way, is referred to simply as "birth" ("jāti"). "Devas" (gods) may also die and live again. Here the term "reincarnation" is not strictly applicable, yet Hindu gods are said to have reincarnated ("see Avatar"): Lord Vishnu is known for his ten incarnations, the "Dashavatars". Celtic religion seems to have had reincarnating gods also. Many Christians regard Jesus as a divine incarnation. Some Christians and Muslims believe he and some prophets may incarnate again. Most Christians, however, believe that Jesus will come again in the Second Coming at the end of the world, although this is not a reincarnation. Some ghulat Shi'a Muslim sects also regard their founders as in some special sense divine incarnations ("hulul").
Philosophical and religious beliefs regarding the existence or non-existence of an unchanging "self" have a direct bearing on how reincarnation is viewed within a given tradition. The Buddha lived at a time of great philosophical creativity in India when many conceptions of the nature of life and death were proposed. Some were materialist, holding that there was no existence and that the self is annihilated upon death. Others believed in a form of cyclic existence, where a being is born, lives, dies and then is reborn, but in the context of a type of determinism or fatalism in which karma played no role. Others were "eternalists", postulating an eternally existent self or soul comparable to that in Judaic monotheism: the ātman survives death and reincarnates as another living being, based on its karmic inheritance. This is the idea that has become dominant (with certain modifications) in modern Hinduism.
The Buddhist concept of reincarnation differs from others in that there is no eternal "soul", "spirit" or "self" but only a "stream of consciousness" that links life with life. The actual process of change from one life to the next is called "punarbhava" (Sanskrit) or "punabbhava" (Pāli), literally "becoming again", or more briefly "bhava", "becoming", and some English-speaking Buddhists prefer the term "rebirth" or "re-becoming" to render this term as they take "reincarnation" to imply a fixed entity that is reborn. Popular Jain cosmology and Buddhist cosmology as well as a number of schools of Hinduism posit rebirth in many worlds and in varied forms. In Buddhist tradition the process occurs across five or six realms of existence, including the human, any kind of animal and several types of supernatural being. It is said in Tibetan Buddhism that it is very rare for a person to be reborn in the immediate next life as a human.
"Gilgul", "Gilgul neshamot" or "Gilgulei Ha Neshamot" (Heb. גלגול הנשמות) refers to the concept of reincarnation in Kabbalistic Judaism, found in much Yiddish literature among Ashkenazi Jews. "Gilgul" means "cycle" and "neshamot" is "souls". The equivalent Arabic term is "tanasukh": the belief is found among Shi'a ghulat Muslim sects.
History.
Origins.
The origins of the notion of reincarnation are obscure. Discussion of the subject appears in the philosophical traditions of India (including the Indus Valley). The Greek Pre-Socratics discussed reincarnation, and the Celtic Druids are also reported to have taught a doctrine of reincarnation.
The ideas associated with reincarnation may have arisen independently in different regions, or they might have spread as a result of cultural contact. Proponents of cultural transmission have looked for links between Iron Age Celtic, Greek and Vedic philosophy and religion, some even suggesting that belief in reincarnation was present in Proto-Indo-European religion. In ancient European, Iranian and Indian agricultural cultures, the life cycles of birth, death, and rebirth were recognized as a replica of natural agricultural cycles.
Early Jainism, Hinduism and Buddhism.
Patrick Olivelle asserts that the origin of the concept of the cycle of birth and death, the concept of samsara, and the concept of liberation in the Indian tradition, were in part the creation of the non-Vedic Shramana tradition. Another possibility are the prehistoric Dravidian traditions of South India. Some scholars suggest that the idea is original to the Buddha.
In Jainism, the soul and matter are considered eternal, not created and perpetual. There is a constant interplay between the two, resulting in bewildering cosmic manifestations in material, psychic and emotional spheres around us. This led to the theories of transmigration and rebirth. Changes but not total annihilation of spirit and matter is the basic postulate of Jain philosophy. The life as we know now, after death therefore moves on to another form of life based on the merits and demerits it accumulated in its current life. The path to becoming a supreme soul is to practice non-violence and be truthful.
In Hinduism's Rigveda, the oldest extant Indo-Aryan text, numerous references are made to transmigration, rebirth (punarjanma), and redeath (punarmrtyu) in the Brahmanas. One verse reads, "Each death repeats the death of the primordial man (purusa), which was also the first sacrifice" (RV 10:90). Another excerpt from the Rig Veda states (10: 16. 1-4):
Burn him not up, nor quite consume him, Agni: let not his body or his skin be scattered. O Jatavedas, when thou hast matured him, then send him on his way unto the Fathers... let thy fierce flame, thy glowing splendour, burn him With thine auspicious forms, o Jatavedas, bear this man to the region of the pious... Again, O Agni, to the Fathers send him who, offered in thee, goes with our oblations. Wearing new life let him increase his offspring: let him rejoin a body, Jatavedas.
Indian discussion of reincarnation enters the historical record from about the 6th century BCE, with the development of the Advaita Vedanta tradition in the early Upanishads (around the middle of the first millennium BCE), Gautama Buddha (623–543 BCE) as well as Mahavira, the 24th Tirthankara of Jainism.
The systematic attempt to attain first-hand knowledge of past lives has been developed in various ways in different places.
The early Buddhist texts discuss techniques for recalling previous births, predicated on the development of high levels of meditative concentration. The later Yoga Sutras of Patanjali, which incorporated elements of Buddhist thought, give similar instructions on how to attain the ability. The Buddha reportedly warned that this experience can be misleading and should be interpreted with care. Tibetan Buddhism has developed a unique "science" of death and rebirth, a good deal of which is set down in what is popularly known as "The Tibetan Book of the Dead".
Early Greece.
Early Greek discussion of the concept likewise dates to the 6th century BCE. An early Greek thinker known to have considered rebirth is Pherecydes of Syros (fl. 540 BCE). His younger contemporary Pythagoras (c. 570–c. 495 BCE), its first famous exponent, instituted societies for its diffusion. Plato (428/427–348/347 BCE) presented accounts of reincarnation in his works, particularly the "Myth of Er".
Authorities have not agreed on how the notion arose in Greece: sometimes Pythagoras is said to have been Pherecydes' pupil, sometimes to have introduced it with the doctrine of Orphism, a Thracian religion that was to be important in the diffusion of reincarnation, or else to have brought the teaching from India. In "Phaedo", Plato makes his teacher Socrates, prior to his death, state: "I am confident that there truly is such a thing as living again, and that the living spring from the dead." However Xenophon does not mention Socrates as believing in reincarnation and Plato may have systematised Socrates' thought with concepts he took directly from Pythagoreanism or Orphism.
Classical Antiquity.
The Orphic religion, which taught reincarnation, first appeared in Thrace in north-eastern Greece and Bulgaria, about the 6th century BC, organized itself into mystery schools at Eleusis and elsewhere, and produced a copious literature. Orpheus, its legendary founder, is said to have taught that the immortal soul aspires to freedom while the body holds it prisoner. The wheel of birth revolves, the soul alternates between freedom and captivity round the wide circle of necessity. Orpheus proclaimed the need of the grace of the gods, Dionysus in particular, and of self-purification until the soul has completed the spiral ascent of destiny to live for ever.
An association between Pythagorean philosophy and reincarnation was routinely accepted throughout antiquity. In the "Republic" Plato makes Socrates tell how Er, the son of Armenius, miraculously returned to life on the twelfth day after death and recounted the secrets of the other world. There are myths and theories to the same effect in other dialogues, in the Chariot allegory of the Phaedrus, in the Meno, Timaeus and Laws. The soul, once separated from the body, spends an indeterminate amount of time in "formland" (see The Allegory of the Cave in "The Republic") and then assumes another body.
In later Greek literature the doctrine is mentioned in a fragment of Menander and satirized by Lucian. In Roman literature it is found as early as Ennius, who, in a lost passage of his "Annals", told how he had seen Homer in a dream, who had assured him that the same soul which had animated both the poets had once belonged to a peacock. Persius in his satires (vi. 9) laughs at this, it is referred to also by Lucretius and Horace.
Virgil works the idea into his account of the Underworld in the sixth book of the Aeneid. It persists down to the late classic thinkers, Plotinus and the other Neoplatonists. In the Hermetica, a Graeco-Egyptian series of writings on cosmology and spirituality attributed to Hermes Trismegistus/Thoth, the doctrine of reincarnation is central.
In Greco-Roman thought, the concept of metempsychosis disappeared with the rise of Early Christianity, reincarnation being incompatible with the Christian core doctrine of salvation of the faithful after death. It has been suggested that some of the early Church Fathers, especially Origen still entertained a belief in the possibility of reincarnation, but evidence is tenuous, and the writings of Origen as they have come down to us speak explicitly against it.
Some early Christian Gnostic sects professed reincarnation. The Sethians and followers of Valentinus believed in it. The followers of Bardaisan of Mesopotamia, a sect of the 2nd century deemed heretical by the Catholic Church, drew upon Chaldean astrology, to which Bardaisan's son Harmonius, educated in Athens, added Greek ideas including a sort of metempsychosis. Another such teacher was Basilides (132–? CE/AD), known to us through the criticisms of Irenaeus and the work of Clement of Alexandria. "(see also Neoplatonism and Gnosticism and Buddhism and Gnosticism)"
In the third Christian century Manichaeism spread both east and west from Babylonia, then within the Sassanid Empire, where its founder Mani lived about 216–276. Manichaean monasteries existed in Rome in 312 AD. Noting Mani's early travels to the Kushan Empire and other Buddhist influences in Manichaeism, Richard Foltz attributes Mani's teaching of reincarnation to Buddhist influence. However the inter-relation of Manicheanism, Orphism, Gnosticism and neo-Platonism is far from clear.
The Celts.
In the 1st century BCE Alexander Cornelius Polyhistor wrote:
The Pythagorean doctrine prevails among the Gauls' teaching that the souls of men are immortal, and that after a fixed number of years they will enter into another body.
Julius Caesar recorded that the druids of Gaul, Britain and Ireland had metempsychosis as one of their core doctrines:
The principal point of their doctrine is that the soul does not die and that after death it passes from one body into another... the main object of all education is, in their opinion, to imbue their scholars with a firm belief in the indestructibility of the human soul, which, according to their belief, merely passes at death from one tenement to another; for by such doctrine alone, they say, which robs death of all its terrors, can the highest form of human courage be developed.
Judaism.
In Judaism, the Zohar, first publicized in the 13th century, discusses reincarnation at length, especially in the Torah portion "Balak." The most comprehensive kabbalistic work on reincarnation, "Shaar HaGilgulim", was written by Rabbi Chaim Vital, based on the teachings of his mentor, the 16th century kabbalist Rabbi Isaac Luria, who was said to know the past lives of each person through his semi-prophetic abilities. The 18th century Lithuanian master scholar and kabbalist, Rabbi Elijah, known as the Vilna Gaon (Elijah of Vilna), authored a commentary on the biblical Book of Jonah as an allegory of reincarnation.
According to the "Jewish Encyclopedia", the philosophy of metempsychosis entered Judaism during the eighth century, under the influences of Islamic mysticism.
There is an extensive literature of Jewish folk and traditional stories that refer to reincarnation.
Taoism.
Taoist documents from as early as the Han Dynasty claimed that Lao Tzu appeared on earth as different persons in different times beginning in the legendary era of Three Sovereigns and Five Emperors. The (ca. 3rd century BC) "Chuang Tzu" states: "Birth is not a beginning; death is not an end. There is existence without limitation; there is continuity without a starting-point. Existence without limitation is Space. Continuity without a starting point is Time. There is birth, there is death, there is issuing forth, there is entering in."
Middle Ages.
Around the 11–12th century several reincarnationist movements were persecuted as heresies, through the establishment of the Inquisition in the Latin west. These included the Cathar, Paterene or Albigensian church of western Europe, the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria.
Christian sects such as the Bogomils and the Cathars, who professed reincarnation and other gnostic beliefs, were referred to as "Manichean", and are today sometimes described by scholars as "Neo-Manichean". As there is no known Manichaean mythology or terminology in the writings of these groups there has been some dispute among historians as to whether these groups truly were descendants of Manichaeism.
Norse mythology.
Reincarnation also appears in Norse mythology, in the "Poetic Edda". The editor of the "Poetic Edda" says that Helgi Hjörvarðsson and his mistress, the valkyrie Sváfa, whose love story is told in the poem "Helgakviða Hjörvarðssonar", were reborn as Helgi Hundingsbane and the valkyrie Sigrún. Helgi and Sigrún's love story is the matter of a part of the "Völsunga saga" and the lays "Helgakviða Hundingsbana I and II". They were reborn a second time as Helgi Haddingjaskati and the valkyrie Kára, but unfortunately their story, "Káruljóð", only survives in a probably modified form in the "Hrómundar saga Gripssonar".
The belief in reincarnation may have been commonplace among the Norse since the annotator of the "Poetic Edda" wrote that people formerly used to believe in it:
Sigrun was early dead of sorrow and grief. It was believed in olden times that people were born again, but that is now called old wives' folly. Of Helgi and Sigrun it is said that they were born again; he became Helgi Haddingjaskati, and she Kara the daughter of Halfdan, as is told in the Lay of Kara, and she was a Valkyrie.
Renaissance and Early Modern period.
While reincarnation has been a matter of faith in some communities from an early date it has also frequently been argued for on principle, as Plato does when he argues that the number of souls must be finite because souls are indestructible, Benjamin Franklin held a similar view. Sometimes such convictions, as in Socrates' case, arise from a more general personal faith, at other times from anecdotal evidence such as Plato makes Socrates offer in the "Myth of Er".
During the Renaissance translations of Plato, the Hermetica and other works fostered new European interest in reincarnation. Marsilio Ficino argued that Plato's references to reincarnation were intended allegorically, Shakespeare made fun but Giordano Bruno was burned at the stake by authorities after being found guilty of heresy by the Roman Inquisition for his teachings. But the Greek philosophical works remained available and, particularly in north Europe, were discussed by groups such as the Cambridge Platonists.
19th to 20th centuries.
By the 19th century the philosophers Schopenhauer and Nietzsche could access the Indian scriptures for discussion of the doctrine of reincarnation, which recommended itself to the American Transcendentalists Henry David Thoreau, Walt Whitman and Ralph Waldo Emerson and was adapted by Francis Bowen into "Christian Metempsychosis".
By the early 20th century, interest in reincarnation had been introduced into the nascent discipline of psychology, largely due to the influence of William James, who raised aspects of the philosophy of mind, comparative religion, the psychology of religious experience and the nature of empiricism. James was influential in the founding of the American Society for Psychical Research (ASPR) in New York City in 1885, three years after the British Society for Psychical Research (SPR) was inaugurated in London, leading to systematic, critical investigation of paranormal phenomena.
At this time popular awareness of the idea of reincarnation was boosted by the Theosophical Society's dissemination of systematised and universalised Indian concepts and also by the influence of magical societies like The Golden Dawn. Notable personalities like Annie Besant, W. B. Yeats and Dion Fortune made the subject almost as familiar an element of the popular culture of the west as of the east. By 1924 the subject could be satirised in popular children's books.
Théodore Flournoy was among the first to study a claim of past-life recall in the course of his investigation of the medium Hélène Smith, published in 1900, in which he defined the possibility of cryptomnesia in such accounts.
Carl Gustav Jung, like Flournoy based in Switzerland, also emulated him in his thesis based on a study of cryptomnesia in psychism. Later Jung would emphasise the importance of the persistence of memory and ego in psychological study of reincarnation: "This concept of rebirth necessarily implies the continuity of personality... (that) one is able, at least potentially, to remember that one has lived through previous existences, and that these existences were one's own..." Hypnosis, used in psychoanalysis for retrieving forgotten memories, was eventually tried as a means of studying the phenomenon of past life recall.
Reincarnation research.
Psychiatrist Ian Stevenson, from the University of Virginia, investigated many reports of young children who claimed to remember a past life. He conducted more than 2,500 case studies over a period of 40 years and published twelve books, including "Twenty Cases Suggestive of Reincarnation" and "Where Reincarnation and Biology Intersect". Stevenson methodically documented each child's statements and then identified the deceased person the child identified with, and verified the facts of the deceased person's life that matched the child's memory. He also matched birthmarks and birth defects to wounds and scars on the deceased, verified by medical records such as autopsy photographs, in "Reincarnation and Biology".
Stevenson searched for disconfirming evidence and alternative explanations for the reports, and believed that his strict methods ruled out all possible "normal" explanations for the child’s memories. However, a significant majority of Stevenson's reported cases of reincarnation originated in Eastern societies, where dominant religions often permit the concept of reincarnation. Following this type of criticism, Stevenson published a book on "European Cases of the Reincarnation Type". Other people who have undertaken reincarnation research include Jim B. Tucker, Antonia Mills, Satwant Pasricha, Godwin Samararatne, and Erlendur Haraldsson.
Skeptics such as Paul Edwards have analyzed many of these accounts, and called them anecdotal, while also suggesting that claims of evidence for reincarnation originate from selective thinking and from the false memories that often result from one's own belief system and basic fears, and thus cannot be counted as empirical evidence. Carl Sagan referred to examples apparently from Stevenson's investigations in his book "The Demon-Haunted World" as an example of carefully collected empirical data, though he rejected reincarnation as a parsimonious explanation for the stories. Sam Harris cited Stevenson's works in his book "The End of Faith" as part of a body of data that seems to attest to the reality of psychic phenomena.
Stevenson claimed there were a handful of cases that suggested evidence of xenoglossy. These included two where a subject under hypnosis could allegedly converse with people speaking the foreign language, instead of merely being able to recite foreign words. Sarah Thomason, a linguist at the University of Michigan, reanalyzed these cases, concluding that "the linguistic evidence is too weak to provide support for the claims of xenoglossy."
Ian Wilson argued that a large number of Stevenson’s cases consisted of poor children remembering wealthy lives or belonging to a higher caste. He speculated that such cases may represent a scheme to obtain money from the family of the alleged former incarnation. The philosopher Keith Augustine has written "the vast majority of Stevenson's cases come from countries where a religious belief in reincarnation is strong, and rarely elsewhere, seems to indicate that cultural conditioning (rather than reincarnation) generates claims of spontaneous past-life memories." According to the research of Robert Baker many of the alleged past-life experiences investigated by Stevenson and other parapsychologists can be explained in terms of known psychological factors. Baker has written the recalling of past lives is a mixture of cryptomnesia and confabulation. The philosopher Paul Edwards noted that reincarnation invokes assumptions and is inconsistent with modern science.
Objections to claims of reincarnation include the facts that the vast majority of people do not remember previous lives and there is no mechanism known to modern science that would enable a personality to survive death and travel to another body, barring the idea of biocentrism. Researchers such as Stevenson have acknowledged these limitations.
Reincarnation in the West.
During recent decades, many people in the West have developed an interest in reincarnation. Feature films, such as "The Reincarnation of Peter Proud", "Dead Again", "Kundun", "Fluke", "What Dreams May Come", "The Mummy" and "Birth", and "Chances Are", contemporary books by authors such as Carol Bowman and Vicki Mackenzie, as well as popular songs, deal with reincarnation.
Recent studies have indicated that some Westerners accept the idea of reincarnation including certain contemporary Christians, modern Neopagans, followers of Spiritism, Theosophists and students of esoteric philosophies such as Kabbalah, and Gnostic and Esoteric Christianity as well as of Indian religions. Demographic survey data from 1999–2002 shows a significant minority of people from Europe and America, where there is reasonable freedom of thought and access to ideas but no outstanding recent reincarnationist tradition, believe we had a life before we were born, will survive death and be born again physically. The mean for the Nordic countries is 22%. The belief in reincarnation is particularly high in the Baltic countries, with Lithuania having the highest figure for the whole of Europe, 44%. The lowest figure is in East Germany, 12%. In Russia, about one-third believes in reincarnation. The effect of communist anti-religious ideas on the beliefs of the populations of Eastern Europe seems to have been rather slight, if any, except apparently in East Germany. Overall, 22% of respondents in Western Europe believe in reincarnation. According to a 2005 Gallup poll 20 percent of U.S. adults believe in reincarnation. Recent surveys by the Barna Group, a Christian research nonprofit organization, have found that a quarter of U.S. Christians, including 10 percent of all born-again Christians, embrace the idea.
Skeptic Carl Sagan asked the Dalai Lama what he would do if a fundamental tenet of his religion (reincarnation) were definitively disproved by science. The Dalai Lama answered, "If science can disprove reincarnation, Tibetan Buddhism would abandon reincarnation... but it's going to be mighty hard to disprove reincarnation."
Ian Stevenson reported that belief in reincarnation is held (with variations in details) by adherents of almost all major religions except Christianity and Islam. In addition, between 20 and 30 percent of persons in western countries who may be nominal Christians also believe in reincarnation.
One 1999 study by Walter and Waterhouse reviewed the previous data on the level of reincarnation belief and performed a set of thirty in-depth interviews in Britain among people who did not belong to a religion advocating reincarnation. The authors reported that surveys have found about one fifth to one quarter of Europeans have some level of belief in reincarnation, with similar results found in the USA. In the interviewed group, the belief in the existence of this phenomenon appeared independent of their age, or the type of religion that these people belonged to, with most being Christians. The beliefs of this group also did not appear to contain any more than usual of "new age" ideas (broadly defined) and the authors interpreted their ideas on reincarnation as "one way of tackling issues of suffering", but noted that this seemed to have little effect on their private lives.
Waterhouse also published a detailed discussion of beliefs expressed in the interviews. She noted that although most people "hold their belief in reincarnation quite lightly" and were unclear on the details of their ideas, personal experiences such as past-life memories and near-death experiences had influenced most believers, although only a few had direct experience of these phenomena. Waterhouse analyzed the influences of second-hand accounts of reincarnation, writing that most of the people in the survey had heard other people's accounts of past-lives from regression hypnosis and dreams and found these fascinating, feeling that there "must be something in it" if other people were having such experiences.
Contemporary religious philosophies.
Hinduism.
Reincarnation – known as Punarjanma – it is one of the core beliefs of Hinduism that is generally accepted by many of its practitioners.
Reincarnation is the natural process of birth, death and rebirth. Hindus believe that the Jiva or Atman (soul) is intrinsically pure. However, because of the layers of I-ness and My-ness, the jiva goes through transmigration in the cycle of births and deaths. Death destroys the physical body, but not the jiva. The jiva is eternal. It takes on another body with respect to its karmas. Every karma produces a result which must be experienced either in this or some future life. As long as the jiva is enveloped in ignorance, it remains attached to material desires and subject to the cycles of births and deaths (Samsara).
There is no permanent heaven or hell in Hinduism. After services in the afterlife, the "jiva" enters the karma and rebirth system, reborn as an animal, a human or a divinity. This reincarnation continues until "mokṣa", the final release, is gained.
The Bhagavad Gita states;
Never was there a time when I did not exist, nor you, nor all these kings; nor in the future shall any of us cease to be. As the embodied soul continuously passes, in this body, from childhood to youth to old age, the soul similarly passes into another body at death. A sober person is not bewildered by such a change. (2:12–13)
and,
Worn-out garments are shed by the body; Worn-out bodies are shed by the dweller within the body. New bodies are donned by the dweller, like garments. (2:22)
According to the Hindu sage Adi Shankaracharya, the world – as we ordinarily understand it – is like a dream: fleeting and illusory. To be trapped in samsara (the cycle of birth and death) is a result of ignorance of the true nature of our existence. It is ignorance ("avidya") of one's true self that leads to ego-consciousness, grounding one in desire and a perpetual chain of reincarnation. The idea is intricately linked to action ("karma"), a concept first recorded in the Upanishads. Every action has a reaction and the force determines one's next incarnation. One is reborn through desire: a person "desires" to be born because he or she wants to enjoy a body, which can never bring deep, lasting happiness or peace ("ānanda"). After many births every person becomes dissatisfied and begins to seek higher forms of happiness through spiritual experience. When, after spiritual practice (sādhanā), a person realizes that the true "self" is the immortal soul rather than the body or the ego all desires for the pleasures of the world will vanish since they will seem insipid compared to spiritual "ānanda". When all desire has vanished the person will not be born again. When the cycle of rebirth thus comes to an end, a person is said to have attained liberation ("moksha"). All schools agree this implies the cessation of worldly desires and freedom from the cycle of birth and death, though the exact definition differs. Followers of the Advaita Vedanta school believe they will spend eternity absorbed in the perfect peace and happiness of the realization that all existence is One "Brahman" of which the soul is part. Dvaita schools perform worship with the goal of spending eternity in a spiritual world or heaven ("loka") in the blessed company of the Supreme Being.
Reasons for Reincarnation.
Hindus provide several reasons why the jiva takes on various physical bodies:
Jainism.
Jainism is historically connected with the "sramana" tradition with which the earliest mentions of reincarnation are associated.
Karma forms a central and fundamental part of Jain faith, being intricately connected to other of its philosophical concepts like transmigration, reincarnation, liberation, non-violence ("ahiṃsā") and non-attachment, among others. Actions are seen to have consequences: some immediate, some delayed, even into future incarnations. So the doctrine of karma is not considered simply in relation to one life-time, but also in relation to both future incarnations and past lives. "Uttarādhyayana-sūtra" 3.3–4 states: "The "jīva" or the soul is sometimes born in the world of gods, sometimes in hell. Sometimes it acquires the body of a demon; all this happens on account of its karma. This "jīva" sometimes takes birth as a worm, as an insect or as an ant." The text further states (32.7): "Karma is the root of birth and death. The souls bound by karma go round and round in the cycle of existence."
Actions and emotions in the current lifetime affect future incarnations depending on the nature of the particular karma. For example, a good and virtuous life indicates a latent desire to experience good and virtuous themes of life. Therefore, such a person attracts karma that ensures that his future births will allow him to experience and manifest his virtues and good feelings unhindered. In this case, he may take birth in heaven or in a prosperous and virtuous human family. On the other hand, a person who has indulged in immoral deeds, or with a cruel disposition, indicates a latent desire to experience cruel themes of life. As a natural consequence, he will attract karma which will ensure that he is reincarnated in hell, or in lower life forms, to enable his soul to experience the cruel themes of life.
There is no retribution, judgment or reward involved but a natural consequences of the choices in life made either knowingly or unknowingly. Hence, whatever suffering or pleasure that a soul may be experiencing in its present life is on account of choices that it has made in the past. As a result of this doctrine, Jainism attributes supreme importance to pure thinking and moral behavior.
The Jain texts postulate four "gatis", that is states-of-existence or birth-categories, within which the soul transmigrates. The four "gatis" are: "deva" (demi-gods), "manuṣya" (humans), "nāraki" (hell beings) and "tiryañca" (animals, plants and micro-organisms). The four "gatis" have four corresponding realms or habitation levels in the vertically tiered Jain universe: demi-gods occupy the higher levels where the heavens are situated; humans, plants and animals occupy the middle levels; and hellish beings occupy the lower levels where seven hells are situated.
Single-sensed souls, however, called "nigoda", and element-bodied souls pervade all tiers of this universe. "Nigodas" are souls at the bottom end of the existential hierarchy. They are so tiny and undifferentiated, that they lack even individual bodies, living in colonies. According to Jain texts, this infinity of "nigodas" can also be found in plant tissues, root vegetables and animal bodies. Depending on its karma, a soul transmigrates and reincarnates within the scope of this cosmology of destinies. The four main destinies are further divided into sub-categories and still smaller sub-sub-categories. In all, Jain texts speak of a cycle of 8.4 million birth destinies in which souls find themselves again and again as they cycle within "samsara".
In Jainism, God has no role to play in an individual's destiny; one's personal destiny is not seen as a consequence of any system of reward or punishment, but rather as a result of its own personal karma. A text from a volume of the ancient Jain canon, "Bhagvati sūtra" 8.9.9, links specific states of existence to specific karmas. Violent deeds, killing of creatures having five sense organs, eating fish, and so on, lead to rebirth in hell. Deception, fraud and falsehood lead to rebirth in the animal and vegetable world. Kindness, compassion and humble character result in human birth; while austerities and the making and keeping of vows lead to rebirth in heaven.
Each soul is thus responsible for its own predicament, as well as its own salvation. Accumulated karma represent a sum total of all unfulfilled desires, attachments and aspirations of a soul. It enables the soul to experience the various themes of the lives that it desires to experience. Hence a soul may transmigrate from one life form to another for countless of years, taking with it the karma that it has earned, until it finds conditions that bring about the required fruits. In certain philosophies, heavens and hells are often viewed as places for eternal salvation or eternal damnation for good and bad deeds. But according to Jainism, such places, including the earth are simply the places which allow the soul to experience its unfulfilled karma.
Buddhism.
The early Buddhist texts make it clear that there is no permanent consciousness that moves from life to life. Gautama Buddha taught a distinct concept of rebirth constrained by the concepts of anattā, that there is no irreducible ātman or "self" tying these lives together (which serves as a contrast to Hinduism, where everything is connected, and in a sense, "everything is everything"), and anicca, that all compounded things are subject to dissolution, including all the components of the human person and personality.
In Buddhist doctrine the evolving consciousness (Pali: "samvattanika-viññana") or stream of consciousness (Pali: "viññana-sotam", Sanskrit: "vijñāna-srotām, vijñāna-santāna", or "citta-santāna") upon death (or "the dissolution of the aggregates" (P. "khandha"s, S. "skandha"s)), becomes one of the contributing causes for the arising of a new aggregation. At the death of one personality, a new one comes into being, much as the flame of a dying candle can serve to light the flame of another. The consciousness in the new person is neither identical to nor entirely different from that in the deceased but the two form a causal continuum or stream. Transmigration is the effect of "karma" ("kamma") or volitional action. The basic cause is the abiding of consciousness in ignorance (Pali: "avijja", Sanskrit: "avidya"): when ignorance is uprooted, rebirth ceases.
The Buddha's detailed conception of the connections between action (karma), rebirth and causality is set out in the twelve links of dependent origination. The empirical, changing self does not only affect the world about it, it also generates, consciously and unconsciously, a subjective image of the world in which it lives as "reality". It "tunes in" to a particular level of consciousness which has a particular range of objects, selectively notices such objects and forms a partial model of reality in which the ego is the crucial reference point. Vipassana meditation uses "bare attention" to mind-states without interfering, owning or judging. Observation reveals each moment as an experience of an individual mind-state such as a thought, a memory, a feeling or a perception that arises, exists and ceases. This limits the power of desire, which, according to the second noble truth of Buddhism, is the cause of suffering ("dukkha"), and leads to "Nirvana" ("nibbana", vanishing (of the self-idea)) in which self-oriented models are transcended and "the world stops". Thus consciousness is a continuous birth and death of mind-states: rebirth is the persistence of this process.
Buddhist traditions vary in precise views on rebirth. The Tibetan schools hold to the notion of a bardo (intermediate state) that can last up to forty-nine days. An accomplished or realized practitioner (by maintaining conscious awareness during the death process) can choose to return to samsara. They believe many lamas choose to be born again and again as humans and are called "tulkus" or incarnate lamas. The Sarvastivada school believed that between death and rebirth there is a sort of limbo in which beings do not yet reap the consequences of their previous actions but may still influence their rebirth. The death process and this intermediate state were believed to offer a uniquely favourable opportunity for spiritual awakening. Theravada Buddhism generally denies there is an intermediate state—though some early Buddhist texts seem to support the idea-- but asserts that rebirth is immediate.
Within Japanese Zen, reincarnation is accepted by some, but wholly rejected by others. A distinction can be drawn between "folk Zen", as in the Zen practiced by devotional lay people, and "philosophical Zen". Folk Zen generally accepts the various supernatural elements of Buddhism such as rebirth. Philosophical Zen, however, places such emphasis on the present moment that rebirth may be considered irrelevant because, even if it does exist, it can never be consciously experienced. Specifically, in Zen the past and future are considered to be merely ideas which are held in the present. Because as living beings rebirth can only be viewed as something which may have happened in the past or that might happen in the future, we must essentially reject the present moment, or Dharma, in order to even consider it. For this reason, rebirth is often either rejected or considered unknowable in Zen and therefore a distraction. Dōgen Zenji, the founder of Japanese Sōtō Zen, writes the following regarding reincarnation: 
According to that non-Buddhist view, there is one spiritual intelligence existing within our bodies. When this body dies, however, the spirit casts off the skin and is reborn. If we learn this view as the Buddha's Dharma we are even more foolish than a person who grasps a tile or pebble thinking it to be a golden treasure.—Dōgen Zenji, Shōbōgenzō
Some schools conclude that karma continues to exist and adhere to the person until it works out its consequences. For the Sautrantika school, each act "perfumes" the individual or "plants a seed" that later germinates. Tibetan Buddhism stresses the state of mind at the time of death. To die with a peaceful mind will stimulate a virtuous seed and a fortunate rebirth; a disturbed mind will stimulate a non-virtuous seed and an unfortunate rebirth. The medieval Pali scholar Buddhaghosa labeled the consciousness that constitutes the condition for a new birth as described in the early texts "rebirth-linking consciousness" ("patisandhi").
Still other Buddhists regard samsara as merely a metaphor of the human condition.
Sant mystics and Sikhism.
Reincarnation remained a tenet of the Sant Bhakti movement and of related mystics on the frontiers of Islam and Hinduism such as the Baul minstrels, the Kabir panth and the Sikh Panth. Sikhs believe the soul is passed from one body to another until Liberation. If we perform good deeds and actions and remember the Creator, we attain a better life while, if we carry out evil actions and sinful deeds, we will be incarnated in “lower” life forms. God may pardon wrongs and release us. Otherwise reincarnation is due to the law of cause and effect but does not create any caste or differences among people. Some scholars consider Eckankar a Western presentation of Sant mysticism. It teaches that Soul is eternal and either chooses an incarnation for growth or else an incarnation is imposed or agreed to because of Karma. Soul is perfected through a series of incarnations until it arrives at a level of spiritual development that obviates the need for further experience in what are described as the "lower worlds" of experience in time and space.
African Vodun.
The Yoruba believe in reincarnation within the family. The names Babatunde (Father returns), Yetunde (Mother returns), Babatunji (Father wakes once again) and Sotunde (The wise man returns) all offer vivid evidence of the Ifa concept of familial or lineal rebirth. There is no simple guarantee that your grandfather or great uncle will "come back" in the birth of your child, however.
Whenever the time arrives for a spirit to return to Earth (otherwise known as The Marketplace) through the conception of a new life in the direct bloodline of the family, one of the component entities of a person's being returns, while the other remains in Heaven (Ikole Orun). The spirit that returns does so in the form of a Guardian Ori. One's Guardian Ori, which is represented and contained in the crown of the head, represents not only the spirit and energy of one's previous blood relative, but the accumulated wisdom he or she has acquired through a myriad of lifetimes. This is not to be confused with one’s spiritual Ori, which contains personal destiny, but instead refers to the coming back to The Marketplace of one's personal blood Ori through one's new life and experiences.
Islam.
The idea of reincarnation is accepted by a few Muslim sects, particularly of the Ghulat, and by other sects in the Muslim world. Historically, South Asian Isma'ilis performed chantas yearly, one of which is for seeking forgiveness of sins committed in past lives. Alawites belonging to Shia denomination of Islam hold that they were originally stars or divine lights that were cast out of heaven through disobedience and must undergo repeated reincarnation (or metempsychosis) before returning to heaven. They can be reincarnated as Christians or others through sin and as animals if they become infidels.
Reincarnation was also accepted by some streams of Sufism. Modern Sufis who embrace the idea include Bawa Muhaiyadeen. However Hazrat Inayat Khan has criticized the idea as unhelpful to the spiritual seeker.
Druze.
Reincarnation is a paramount tenet in the Druze faith. There is an eternal duality of the body and the soul and it is impossible for the soul to exist without the body. Therefore, reincarnations occur instantly at one's death. While in the Hindu and Buddhist belief system a soul can be transmitted to any living creature, in the Druze belief system this is not possible and a human soul will only transfer to a human body. Furthermore, a male Druze can only be reincarnated as another male Druze and a female Druze can only be reincarnated as another female Druze. Additionally, souls cannot be divided and the number of souls existing is finite.
Very few Druzes are able to recall their past but, if they are able to they are called a "Nateq". Typically souls who have died violent deaths in their previous incarnation will be able to recall memories. Since death is seen as a quick transient state, mourning is discouraged. Unlike other Abrahamic faiths, heaven and hell as spiritual. Heaven is the ultimate happiness received when soul escapes the cycle of rebirths and reunites with the Creator. While hell is conzeputalized the bitterness of being unable to reunite with the Creator and escape from the cycle of rebirth.
Judaism.
Reincarnation is one of the thirteen principles of Maimonides 13 Principles of Faith (number 13) which states "I believe with a perfect faith that the Holy One... in the future will bring the dead back to life...". It is also a core element in the tale of the Ten Martyrs in the "Yom Kippur" liturgy, who were killed by Romans to atone for the souls of the ten brothers of Joseph, is read in Ashkenazi Orthodox Jewish communities. However it is not a topic widely discussed in the classical rabbinical works (Mishnah and Talmud), or Maimonides'. Medieval Jewish Rationalist philosophers discussed the issue, often in rejection.
However, Jewish mystical texts (the Kabbalah), from their classic Medieval canon onwards, teach a belief in "Gilgul Neshamot" (Hebrew for metempsychosis of souls: literally "soul cycle", plural "gilgulim"). It is a common belief in contemporary Hasidic Judaism, which regards the Kabbalah as sacred and authoritative, though unstressed in favour of a more innate psychological mysticism. Kabbalah also teaches that "The soul of Moses is reincarnated in every generation." Other, Non-Hasidic, Orthodox Jewish groups while not placing a heavy emphasis on reincarnation, do acknowledge it as a valid teaching. Its popularisation entered modern secular Yiddish literature and folk motif.
The 16th century mystical renaissance in communal Safed replaced scholastic Rationalism as mainstream traditional Jewish theology, both in scholarly circles and in the popular imagination. References to "gilgul" in former Kabbalah became systemised as part of the metaphysical purpose of creation. Isaac Luria (the Ari) brought the issue to the centre of his new mystical articulation, for the first time, and advocated identification of the reincarnations of historic Jewish figures that were compiled by Haim Vital in his Shaar HaGilgulim. "Gilgul" is contrasted with the other processes in Kabbalah of Ibbur ("pregnancy"), the attachment of a second soul to an individual for (or by) good means, and Dybuk ("possession"), the attachment of a spirit, demon, etc. to an individual for (or by) "bad" means.
In Lurianic Kabbalah, reincarnation is not retributive or fatalistic, but an expression of Divine compassion, the microcosm of the doctrine of cosmic rectification of creation. "Gilgul" is a heavenly agreement with the individual soul, conditional upon circumstances. Luria's radical system focused on rectification of the Divine soul, played out through Creation. The true essence of anything is the divine spark within that gives it existence. Even a stone or leaf possesses such a soul that "came into this world to receive a rectification". A human soul may occasionally be exiled into lower inanimate, vegetative or animal creations. The most basic component of the soul, the nefesh, must leave at the cessation of blood production. There are four other soul components and different nations of the world possess different forms of souls with different purposes. Each Jewish soul is reincarnated in order to fulfil each of the 613 Mosaic commandments that elevate a particular spark of holiness associated with each commandment. Once all the Sparks are redeemed to their spiritual source, the Messianic Era begins. Non-Jewish observance of the 7 Laws of Noah assists the Jewish people, though Biblical adversaries of Israel reincarnate to oppose.
Among the many rabbis who accepted reincarnation are Nahmanides (the Ramban) and Rabbenu Bahya ben Asher, Levi ibn Habib (the Ralbah), Shelomoh Alkabez, Moses Cordovero, Moses Chaim Luzzatto, the Baal Shem Tov and later Hasidic masters, DovBer Pinson and the Mitnagdic Vilna Gaon and Chaim Volozhin and their school, Ben Ish Chai of Baghdad and the Baba Sali. Rabbis who have rejected the idea include Saadia Gaon, David Kimhi, Hasdai Crescas, Joseph Albo, Abraham ibn Daud, Leon de Modena, Solomon ben Aderet, Maimonides and Asher ben Jehiel. Among the Geonim, Hai Gaon argued in favour of "gilgulim".
Native American nations.
Reincarnation is an intrinsic part of many Native American and Inuit traditions. In the now heavily Christian Polar North (now mainly parts of Greenland and Nunavut), the concept of reincarnation is enshrined in the Inuit language.
The following is a story of human-to-human reincarnation as told by Thunder Cloud, a Winnebago (Ho-Chunk tribe) shaman referred to as T. C. in the narrative. Here T. C. talks about his two previous lives and how he died and came back again to this his third lifetime. He describes his time between lives, when he was “blessed” by Earth Maker and all the abiding spirits and given special powers, including the ability to heal the sick.
T. C.'s Account of His Two Reincarnations:
I "(my ghost)" was taken to the place where the sun sets "(the west)". ... While at that place, I thought I would come back to earth again, and the old man with whom I was staying said to me, “My son, did you not speak about wanting to go to the earth again?” I had, as a matter of fact, only thought of it, yet he knew what I wanted. Then he said to me, “You can go, but you must ask the chief first.” Then I went and told the chief of the village of my desire, and he said to me, “You may go and obtain your revenge upon the people who killed your relatives and you.” Then I was brought down to earth. ... There I lived until I died of old age. ... As I was lying [in my grave], someone said to me, “Come, let us go away.” So then we went toward the setting of the sun. There we came to a village where we met all the dead. ... From that place I came to this earth again for the third time, and here I am". (Radin, 1923)
Christianity.
Though the major Christian denominations reject the concept of reincarnation, a large number of Christians profess the belief. In a survey by the Pew Forum in 2009, 24% of American Christians expressed a belief in reincarnation. In a 1981 Survey in Europe 31% of regular churchgoing Catholics expressed a belief in reincarnation.
Geddes MacGregor, an Episcopalian priest and professor of Philosophy, makes a case for the compatibility of Christian doctrine and reincarnation.
There is evidence that the writing of Origen, a Church father in early Christian times, was mistranslated into Latin due to religious bias and that he taught reincarnation in his lifetime. One of the epistles written by St. Jerome, "To Avitus" (Letter 124 ; Ad Avitum. Epistula CXXIV), asserts that Origen's "On First Principles" (Latin: "De Principiis"; Greek: Περὶ Ἀρχῶν) was mistranscribed from Greek into Latin:
 About ten years ago that saintly man Pammachius sent me a copy of a certain person's [ Rufinus's ] rendering, or rather misrendering, of Origen's "First Principles"; with a request that in a Latin version I should give the true sense of the Greek and should set down the writer's words for good or for evil without bias in either direction. When I did as he wished and sent him the book, he was shocked to read it and locked it up in his desk lest being circulated it might wound the souls of many.
Under the impression that Origen was a heretic like Arius, St. Jerome criticizes ideas described in "On First Principles". Further in "To Avitus" (Letter 124), St. Jerome writes about "convincing proof" that Origen teaches reincarnation in the original version of the book:
The following passage is a convincing proof that he holds the transmigration of the souls and annihilation of bodies. 'If it can be shown that an incorporeal and reasonable being has life in itself independently of the body and that it is worse off in the body than out of it; then beyond a doubt bodies are only of secondary importance and arise from time to time to meet the varying conditions of reasonable creatures. Those who require bodies are clothed with them, and contrariwise, when fallen souls have lifted themselves up to better things, their bodies are once more annihilated. They are thus ever vanishing and ever reappearing.'
The original text of "On First Principles" has almost completely disappeared. It remains extant as "De Principiis" in fragments faithfully translated into Latin by St. Jerome and in "the not very reliable Latin translation of Rufinus."
New religious and spiritual movements.
Theosophy.
The Theosophical Society draws much of its inspiration from India. The idea is, according to a recent Theosophical writer, "the master-key to modern problems", including heredity. In the Theosophical world-view reincarnation is the vast rhythmic process by which the soul, the part of a person which belongs to the formless non-material and timeless worlds, unfolds its spiritual powers in the world and comes to know itself. It descends from sublime, free, spiritual realms and gathers experience through its effort to express itself in the world. Afterwards there is a withdrawal from the physical plane to successively higher levels of reality, in death, a purification and assimilation of the past life. Having cast off all instruments of personal experience it stands again in its spiritual and formless nature, ready to begin its next rhythmic manifestation, every lifetime bringing it closer to complete self-knowledge and self-expression. However it may attract old mental, emotional, and energetic "karma" patterns to form the new personality.
Modern Astrology.
Inspired by Helena Blavatsky's major works, including "Isis Unveiled" and "The Secret Doctrine", astrologers in the early twentieth-century integrated the concepts of karma and reincarnation into the practice of Western astrology. Notable astrologers who advanced this development included Alan Leo, Charles E. O. Carter, Marc Edmund Jones, and Dane Rudhyar. A new synthesis of East and West resulted as Hindu and Buddhist concepts of reincarnation were fused with Western astrology's deep roots in Hermeticism and Neoplatonism. In the case of Rudhyar, this synthesis was enhanced with the addition of Jungian depth psychology. This dynamic integration of astrology, reincarnation and depth psychology has continued into the modern era with the work of astrologers Steven Forrest and Jeffrey Wolf Green. Their respective schools of Evolutionary Astrology are based on "an acceptance of the fact that human beings incarnate in a succession of lifetimes."
Anthroposophy.
Anthroposophy describes reincarnation from the point of view of Western philosophy and culture. The ego is believed to transmute transient soul experiences into universals that form the basis for an individuality that can endure after death. These universals include ideas, which are intersubjective and thus transcend the purely personal (spiritual consciousness), intentionally formed human character (spiritual life), and becoming a fully conscious human being (spiritual humanity). Rudolf Steiner described both the general principles he believed to be operative in reincarnation, such as that one's will activity in one life forms the basis for the thinking of the next, and a number of successive lives of various individualities.
Eckankar.
Awareness of past lives, dreams, and soul travel are spiritual disciplines practiced by students of Eckankar. Eckankar teaches that each person is Soul, which transcends time and space. Soul travel is a term specific to Eckankar that refers to a shift in consciousness. Eckists believe the purpose of being aware of past lives is to help with understanding personal conditions in the present. Practicing students of Eckankar can become aware of past lives, through dreams, soul travel, and spiritual exercises called contemplations. This form of contemplation is the active, unconditional practice of going within to connect with the "Light and Sound of God" known as the divine life current or Holy Spirit.
Scientology.
Past reincarnation, usually termed "past lives", is a key part of the principles and practices of the Church of Scientology. Scientologists believe that the human individual is actually a "thetan", an immortal spiritual entity, that has fallen into a degraded state as a result of past-life experiences. Scientology auditing is intended to free the person of these past-life traumas and recover past-life memory, leading to a higher state of spiritual awareness. This idea is echoed in their highest fraternal religious order, the Sea Organization, whose motto is "Revenimus" or "We Come Back", and whose members sign a "billion-year contract" as a sign of commitment to that ideal. L. Ron Hubbard, the founder of Scientology, does not use the word "reincarnation" to describe its beliefs, noting that: "The common definition of reincarnation has been altered from its original meaning. The word has come to mean 'to be born again in different life forms' whereas its actual definition is 'to be born again into the flesh of another body.' Scientology ascribes to this latter, original definition of reincarnation."
The first writings in Scientology regarding past lives date from around 1951 and slightly earlier. In 1960, Hubbard published a book on past lives entitled "Have You Lived Before This Life". In 1968 he wrote "Mission into Time", a report on a five-week sailing expedition to Sardinia, Sicily and Carthage to see if specific evidence could be found to substantiate L. Ron Hubbard's recall of incidents in his own past, centuries ago.
Meher Baba.
The Indian spiritual teacher Meher Baba stated that reincarnation occurs due to desires and once those desires are extinguished the ego-mind ceases to reincarnate:
The power that keeps the individual soul bound to the wheel of life and death is its thirst for separate existence, which is a condition for a host of cravings connected with objects and experiences of the world of duality. It is for the fulfillment of cravings that the ego-mind keeps on incarnating itself. When all forms of craving disappear, the impressions which create and enliven the ego-mind disappear. With the disappearance of these impressions, the ego-mind itself is shed with the result that there is only the realisation of the one eternal, unchanging Oversoul or God, Who is the only reality. God-realisation is the end of the incarnations of the ego-mind because it is the end of its very existence. As long as the ego-mind exists in some form, there is an inevitable and irresistible urge for incarnations. When there is cessation of the ego-mind, there is cessation of incarnations in the final fulfillment of Self-realisation. (1967)
Spiritism.
Spiritism is a Christian philosophy codified in the 19th century by the French educator Allan Kardec. Spiritism soon spread to other countries, having today 35 countries represented in the International Spiritist Council. In countries like Brazil the movement had spread and became widely accepted, mostly due to Chico Xavier's works. Today the official spiritist community has about 20 million adepts, though due to local syncretism, it is accepted and somehow practiced by three times as many across the country. Some statistics even mention an adherence to Spiritist practices by 40 million people in Brazil.
Spiritism teaches reincarnation or rebirth into human life after death. This basically distinguishes Spiritism from Spiritualism. According to the Spiritist doctrine, reincarnation explains the moral and intellectual differences among men. It also provides the path to man's moral and intellectual perfection by amending for his mistakes and increasing his knowledge in successive lives. For this reason Spiritism does not accept rebirth in animals as this would be retrogressive. Reincarnation is the natural method of the perfection process through which the Spirit faces countless different situations, problems and obstacles, and needs to learn how to deal with them. The central tenet of Spiritist doctrine is the belief in spiritual life. The spirit is eternal, and evolves through a series of incarnations in the material world. The true life is the spiritual one; life in the material world is just a short-termed stage, where the spirit has the opportunity to learn and develop its potentials. Reincarnation is the process where the spirit, once free in the spiritual world, comes back to the world for further learning.
In popular culture.
Reincarnation has been the theme of , including "Madhumati" (1958), which was one of the earliest Bollywood films of the theme. The 2010 Thai film, "Uncle Boonmee Who Can Recall His Past Lives", won the Palme d'Or at the 2010 Cannes Film Festival. The John Craigie song "So Many Lives" has been called a "reincarnation love song" and features a character that goes from being a caterpillar, to a bumble bee, to a sperm whale, to a chimpanzee.
 In the 1974 movie Sonar Kella,directed by Satyajit Ray, the character of Mukul is believed to be a reincarnation, and this becomes the main theme of the plot.
The song Highwayman written originally by Jimmy Webb in 1977, is based entirely upon the idealism of reincarnation. [Highwayman] is about a soul with incarnations in four different places in time and history: as a highwayman, a sailor, a dam builder on the Hoover Dam, and finally as an astronaut. In 1985, the song became the inspiration for the naming of the supergroup The Highwaymen, which featured Johnny Cash, Waylon Jennings, Willie Nelson, and Kris Kristofferson, who brought the song back to popularity with their rendition of the highwayman on the supergroups debut album.

</doc>
<doc id="25808" url="http://en.wikipedia.org/wiki?curid=25808" title="Robert Noyce">
Robert Noyce

Robert Norton Noyce (December 12, 1927 – June 3, 1990), nicknamed "the Mayor of Silicon Valley," co-founded Fairchild Semiconductor in 1957 and Intel Corporation in 1968. He is also credited (along with Jack Kilby) with the realization of the first integrated circuit or microchip which fueled the personal computer revolution and gave Silicon Valley its name.
Biography.
Active all his life, Noyce enjoyed reading Hemingway, flying his own airplane, hang gliding, and scuba diving. Noyce believed that microelectronics would continue to advance in complexity and sophistication well beyond its current state, leading to the question of what use society would make of the technology. In his last interview, Noyce was asked what he would do if he were "emperor" of the United States. He said that he would, among other things, "…make sure we are preparing our next generation to flourish in a high-tech age. And that means education of the lowest and the poorest, as well as at the graduate school level."
Early life.
Noyce was born on December 12, 1927, in Burlington, Iowa as the third of four sons of the Rev. Ralph Brewster Noyce. His father had graduated from Doane College (1915), Oberlin College (1920), and the Chicago Theological Seminary (1923). He was also nominated for a Rhodes Scholarship. The Reverend Noyce worked as a Congregational clergyman and as the associate superintendent of the Iowa Conference of Congregational Churches in the 1930s and 1940s.
His mother, Harriet May Norton, was the daughter of the Rev. Milton J. Norton, a Congregational clergyman, and of Louise Hill. She graduated from Oberlin College in 1921 and had dreamed of becoming a missionary prior to her marriage. She has been described as an intelligent woman with a commanding will.
Bob Noyce had three siblings: Donald Sterling Noyce, Gaylord Brewster Noyce and Ralph Harold Noyce. His earliest childhood memory involved beating his father at ping pong and feeling absolutely shocked when his mother reacted to the thrilling news of his victory with a distracted "Wasn't that nice of Daddy to let you win?" Even at the age of five, Noyce felt offended by the notion of intentionally losing at anything. "That's not the game", he sulked to his mother. "If you're going to play, play to win!"
In the summer of 1940, at the age of 12, he built a boy-sized aircraft with his brother, which they used to fly from the roof of the Grinnell College stables. Later he built a radio from scratch and motorized his sled by welding a propeller and an engine from an old washing machine to the back of it. His parents were both religious but Noyce became an agnostic and irreligious in later life.
Education.
He grew up in Grinnell, Iowa, and attended the local schools. He exhibited a talent for mathematics and science while in high school and took the Grinnell College freshman physics course in his senior year. He graduated from Grinnell High School in 1945 and entered Grinnell College in the fall of that year. He was the star diver on the 1947 Midwest Conference Championship swim team. While at Grinnell College, Noyce sang, played the oboe and acted. In Noyce’s junior year, he got in trouble for stealing a 25 pound pig from the mayor of Grinnell’s farm and roasting it at a school luau. The mayor sent a letter home to Noyce’s parents stating that “In the agricultural state of Iowa, stealing a domestic animal is a felony which carries a minimum penalty of a year in prison and a fine of one thousand dollars.” So essentially, Noyce would have to be expelled from Grinnell College. Grant Gale, Noyce’s physics professor and the President of Grinnell College, did not want to lose a student like Robert who had so much potential. They were able to compromise with the mayor so that the college would compensate him for the pig, Noyce would only be suspended for one semester, and no further charges would be pressed. He returned to Grinnell in February 1949.He graduated Phi Beta Kappa with a BA in physics and mathematics from Grinnell College in 1949. He also received a signal honor from his classmates: the Brown Derby Prize, which recognized "the senior man who earned the best grades with the least amount of work".
While an undergraduate, Noyce attended a physics course of the professor Grant Gale and was fascinated by the physics. Gale got hold of two of the very first transistors ever to come out of Bell Labs and showed them off to his class and Noyce was hooked. Grant Gale suggested that he apply to the doctoral program in physics at MIT, which he did.
Noyce had a mind so quick that his graduate school friends called him "Rapid Robert." He received his doctorate in physics from Massachusetts Institute of Technology in 1953.
Career.
After graduating from the Massachusetts Institute of Technology in 1953, he took his first job as a research engineer at the Philco Corporation in Philadelphia. He left in 1956 for the Shockley Semiconductor Laboratory in Mountain View, California.
He joined William Shockley, a co-inventor of the transistor and eventual Nobel Prize winner, at the Shockley Semiconductor Laboratory, a division of Beckman Instruments.
Noyce left with the "traitorous eight" in 1957, upon having issues with respect to the quality of its management, and co-founded the influential Fairchild Semiconductor corporation. According to Sherman Fairchild, Noyce's impassioned presentation of his vision was the reason Fairchild had agreed to create the semiconductor division for the traitorous eight.
Noyce and Gordon Moore founded Intel in 1968 when they left Fairchild Semiconductor. Arthur Rock, the chairman of Intel's board and a major investor in the company said that for Intel to succeed, Intel needed Noyce, Moore and Andrew Grove. And it needed them in that order. Noyce: the visionary, born to inspire; Moore: the virtuoso of technology; and Grove: the technologist turned management scientist. The relaxed culture that Noyce brought to Intel was a carry-over from his style at Fairchild Semiconductor. He treated employees as family, rewarding and encouraging teamwork. His follow-your-bliss management style set the tone for many Valley success stories. Noyce's management style could be called a "roll up your sleeves" style. He shunned fancy corporate cars, reserved parking spaces, private jets, offices, and furnishings in favor of a less-structured, relaxed working environment in which everyone contributed and no one received lavish benefits. By declining the usual executive perks he stood as a model for future generations of Intel CEOs.
At Intel, he oversaw Ted Hoff's invention of the microprocessor, which was his second revolution.
Death.
Noyce suffered a heart attack at home on June 3, 1990, and later died at the Seton Medical Center in Austin, Texas.
Personal life.
In 1953, Noyce married Elizabeth Bottomley. She was a 1951 graduate of Tufts University. During this time, the couple lived in Los Altos, California. They had four children: William B., Pendred, Priscilla, and Margaret. Elizabeth loved New England, so the family acquired a 50-acre coastal summer home in Bremen, Maine. Elizabeth and the children would summer there. Robert would visit during the summer, but he continued working at Intel during the summer. The couple divorced in 1974.
On November 27, 1974, Noyce married Ann Schmeltz Bowers. Bowers, a 1959 graduate of Cornell University, also received an honorary Ph.D. from Santa Clara University, where she was a trustee for nearly 20 years. She was the first Director of Personnel for Intel Corporation and the first Vice President of Human Resources for Apple Inc. She currently serves as Chair of the Board and the founding trustee of the Noyce Foundation.
Awards and honors.
In July 1959, he filed for U.S. Patent "Semiconductor Device and Lead Structure", a type of integrated circuit. This independent effort was recorded only a few months after the key findings of inventor Jack Kilby. For his co-invention of the integrated circuit and its world-transforming impact, three presidents of the United States honored him.
Noyce was a holder of many honors and awards. President Ronald Reagan awarded him the National Medal of Technology in 1987. Two years later, he was inducted into the U.S. Business Hall of Fame sponsored by Junior Achievement, during a black tie ceremony keynoted by President George H. W. Bush. In 1990 Noyce – along with, among others, Jack Kilby and transistor inventor John Bardeen – received a "Lifetime Achievement Medal" during the bicentennial celebration of the Patent Act.
Noyce received the Franklin Institute's Stuart Ballantine Medal in 1966. He was awarded the IEEE Medal of Honor in 1978 "for his contributions to the silicon integrated circuit, a cornerstone of modern electronics." In 1979, he was awarded the National Medal of Science. Noyce was elected a Fellow of the American Academy of Arts and Sciences in 1980. The National Academy of Engineering awarded him its 1989 Charles Stark Draper Prize.
The science building at his alma mater, Grinnell College, is named after him.
On December 12, 2011, Noyce was honored with a Google Doodle celebrating the 84th anniversary of his birth.
December 8, 2000 According to the book 'The Innovators' Noyce was mentioned/credited as the honorary co-recipient in the Nobel Prize acceptance speech given by Kilby http://www.nobelprize.org/nobel_prizes/physics/laureates/2000/kilby-lecture.html
Legacy.
The Noyce Foundation was founded in 1991 by his family. The foundation is dedicated to improving public education in mathematics and science in grades K-12.
Patents.
Noyce was granted 15 patents.
</dl>
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="25809" url="http://en.wikipedia.org/wiki?curid=25809" title="Riemann zeta function">
Riemann zeta function

The Riemann zeta function or Euler–Riemann zeta function, "ζ"("s"), is a function of a complex variable "s" that analytically continues the sum of the infinite series 
which converges when the real part of "s" is greater than 1. More general representations of "ζ"("s") for all "s" are given below. The Riemann zeta function plays a pivotal role in analytic number theory and has applications in physics, probability theory, and applied statistics.
This function, as a function of a real argument, was introduced and studied by Leonhard Euler in the first half of the eighteenth century without using complex analysis, which was not available at that time. Bernhard Riemann in his article "On the Number of Primes Less Than a Given Magnitude" published in 1859 extended the Euler definition to a complex variable, proved its meromorphic continuation and functional equation and established a relation between its zeros and the distribution of prime numbers.
The values of the Riemann zeta function at even positive integers were computed by Euler. The first of them, "ζ"(2), provides a solution to the Basel problem. In 1979 Apéry proved the irrationality of "ζ"(3). The values at negative integer points, also found by Euler, are rational numbers and play an important role in the theory of modular forms. Many generalizations of the Riemann zeta function, such as Dirichlet series, Dirichlet L-functions and L-functions, are known.
Definition.
The Riemann zeta function "ζ"("s") is a function of a complex variable "s" = "σ" + "it". (The notation with "s", "σ", and "t" is traditionally used in the study of the "ζ"-function, following Riemann.)
The following infinite series converges for all complex numbers "s" with real part greater than 1, and defines "ζ"("s") in this case:
It can also be defined by the integral
where Γ("s") is the gamma function.
The Riemann zeta function is defined as the analytic continuation of the function defined for σ > 1 by the sum of the preceding series.
Leonhard Euler considered the above series in 1740 for positive integer values of "s", and later Chebyshev extended the definition to real "s" > 1.
The above series is a prototypical Dirichlet series that converges absolutely to an analytic function for "s" such that "σ" > 1 and diverges for all other values of "s". Riemann showed that the function defined by the series on the half-plane of convergence can be continued analytically to all complex values "s" ≠ 1. For "s" = 1 the series is the harmonic series which diverges to +∞, and
Thus the Riemann zeta function is a meromorphic function on the whole complex "s"-plane, which is holomorphic everywhere except for a simple pole at "s" = 1 with residue 1.
Specific values.
For any positive even integer "2n":
where "B"2"n" is a Bernoulli number.
For negative integers, one has
for "n" ≥ 1, so in particular "ζ" vanishes at the negative even integers because "B""m" = 0 for all odd "m" other than 1. For odd positive integers, no such simple expression is known.
Via analytic continuation, one can show that 
Euler product formula.
The connection between the zeta function and prime numbers was discovered by Euler, who proved the identity
where, by definition, the left hand side is "ζ"("s") and the infinite product on the right hand side extends over all prime numbers "p" (such expressions are called Euler products):
Both sides of the Euler product formula converge for Re("s") > 1. The proof of Euler's identity uses only the formula for the geometric series and the fundamental theorem of arithmetic. Since the harmonic series, obtained when "s" = 1, diverges, Euler's formula (which becomes formula_19) implies that there are infinitely many primes.
The Euler product formula can be used to calculate the asymptotic probability that "s" randomly selected integers are set-wise coprime. Intuitively, the probability that any single number is divisible by a prime (or any integer), "p" is 1/"p". Hence the probability that "s" numbers are all divisible by this prime is 1/"p""s", and the probability that at least one of them is "not" is 1 − 1/"p""s". Now, for distinct primes, these divisibility events are mutually independent because the candidate divisors are coprime (a number is divisible by coprime divisors "n" and "m" if and only if it is divisible by "nm", an event which occurs with probability 1/("nm")). Thus the asymptotic probability that "s" numbers are coprime is given by a product over all primes,
The functional equation.
The Riemann zeta function satisfies the functional equation (known as the Riemann functional equation or Riemann's functional equation)
where Γ("s") is the gamma function, which is an equality of meromorphic functions valid on the whole complex plane. This equation relates values of the Riemann zeta function at the points "s" and 1 − "s". Owing to the zeros of the sine function, the functional equation implies that "ζ"("s") has a simple zero at each even negative integer "s" = −2"n" — these are known as the trivial zeros of "ζ"("s"). When "s" is an even positive integer, the product <br> sin("πs"/2)Γ(1−"s") on the right is regular and non-zero because Γ(1−"s") has a simple pole: the functional equation thus relates the values of the Riemann zeta function at odd negative integers and even positive integers.
The functional equation was established by Riemann in his 1859 paper "On the Number of Primes Less Than a Given Magnitude" and used to construct the analytic continuation in the first place. An equivalent relationship had been conjectured by Euler over a hundred years earlier, in 1749, for the Dirichlet eta function (alternating zeta function)
Incidentally, this relation is interesting also because it actually exhibits "ζ"("s") as a Dirichlet series (of the "η"-function) which is convergent (albeit non-absolutely) in the larger half-plane "σ" > 0 (not just "σ" > 1), up to an elementary factor.
Riemann also found a symmetric version of the functional equation (which he assigned the letter "ξ" [small xi]), given by first defining
The functional equation is then given by
Zeros, the critical line, and the Riemann hypothesis.
The functional equation shows that the Riemann zeta function has zeros at −2, −4, ... . These are called the trivial zeros. They are trivial in the sense that their existence is relatively easy to prove, for example, from sin(π"s"/2) being 0 in the functional equation. The non-trivial zeros have captured far more attention because their distribution not only is far less understood but, more importantly, their study yields impressive results concerning prime numbers and related objects in number theory. It is known that any non-trivial zero lies in the open strip {"s" ∈ C : 0 < Re("s") < 1}, which is called the critical strip. The Riemann hypothesis, considered one of the greatest unsolved problems in mathematics, asserts that any non-trivial zero "s" has Re("s") = 1/2. In the theory of the Riemann zeta function, the set {"s" ∈ C : Re("s") = 1/2} is called the critical line. For the Riemann zeta function on the critical line, see Z-function.
The Hardy–Littlewood conjectures.
In 1914, Godfrey Harold Hardy proved that formula_25 has infinitely many zeros.
Hardy and John Edensor Littlewood formulated two conjectures on the density and distance between the zeros of formula_25 on intervals of large positive real numbers. In the following, formula_27 is the total number of real zeros and formula_28 the total number of zeros of odd order of the function formula_25 lying in the interval formula_30.
These two conjectures opened up new directions in the investigation of the Riemann zeta function.
Other results.
The location of the Riemann zeta function's zeros is of great importance in the theory of numbers. The prime number theorem is equivalent to the fact that there are no zeros of the zeta function on the Re("s") = 1 line. A better result that follows from an effective form of Vinogradov's mean-value theorem is that "ζ"("σ" + i"t") ≠ 0 whenever | "t" | ≥ 3 and
The strongest result of this kind one can hope for is the truth of the Riemann hypothesis, which would have many profound consequences in the theory of numbers.
It is known that there are infinitely many zeros on the critical line. Littlewood showed that if the sequence (γ"n") contains the imaginary parts of all zeros in the upper half-plane in ascending order, then
The critical line theorem asserts that a positive percentage of the nontrivial zeros lies on the critical line.
In the critical strip, the zero with smallest non-negative imaginary part is 1/2 + "i"14.13472514... ( ). Directly from the functional equation one sees that the non-trivial zeros are symmetric about the axis Re("s") = 1/2. Furthermore, the fact that formula_44 for all complex "s" ≠ 1 implies that the zeros of the Riemann zeta function are symmetric about the real axis.
Various properties.
For sums involving the zeta-function at integer and half-integer values, see rational zeta series.
Reciprocal.
The reciprocal of the zeta function may be expressed as a Dirichlet series over the Möbius function μ("n"):
for every complex number "s" with real part > 1. There are a number of similar relations involving various well-known multiplicative functions; these are given in the article on the Dirichlet series.
The Riemann hypothesis is equivalent to the claim that this expression is valid when the real part of "s" is greater than 1/2.
Universality.
The critical strip of the Riemann zeta function has the remarkable property of universality. This zeta-function universality states that there exists some location on the critical strip that approximates any holomorphic function arbitrarily well. Since holomorphic functions are very general, this property is quite remarkable.
Estimates of the maximum of the modulus of the zeta function.
Let the functions formula_46 and formula_47 be defined by the equalities
Here formula_49 is a sufficiently large positive number, formula_50, formula_51, formula_52, formula_53. Estimating the values formula_54 and formula_55 from below shows, how large (in modulus) values formula_56 can take on short intervals of the critical line or in small neighborhoods of points lying in the critical strip formula_57.
The case formula_58 was studied by Ramachandra; the case formula_59, where formula_60 is a sufficiently large constant, is trivial.
Karatsuba proved, in particular, that if the values formula_61 and formula_62 exceed certain sufficiently small constants, then the estimates
hold, where formula_64 are certain absolute constants.
The argument of the Riemann zeta-function.
The function formula_65 is called the argument of the Riemann zeta function.
Here formula_66 is the increment of an arbitrary continuous branch of formula_67 along the broken line joining the points formula_68 and formula_69
There are some theorems on properties of the function formula_70. Among those results are the mean value theorems for formula_70 and its first integral formula_72 on intervals of the real line, and also the theorem claiming that every interval formula_35 for formula_74 contains at least
points where the function formula_70 changes sign. Earlier similar results were obtained by Atle Selberg for the case
formula_77.
Representations.
Dirichlet series.
An extension of the area of convergence can be obtained by rearranging the original series. The series
converges for formula_79,
while 
converges even for formula_81. In this way, the area of convergence can be extended to formula_82 for any formula_83.
Mellin transform.
The Mellin transform of a function "ƒ"("x") is defined as
in the region where the integral is defined. There are various expressions for the zeta-function as a Mellin transform. If the real part of "s" is greater than one, we have
where Γ denotes the Gamma function. By modifying the contour, Riemann showed that
for all "s", where the contour C starts and ends at +∞ and circles the origin once.
We can also find expressions which relate to prime numbers and the prime number theorem. If π("x") is the prime-counting function, then
for values with Re("s") > 1.
A similar Mellin transform involves the Riemann prime-counting function "J"("x"), which counts prime powers "p""n" with a weight of 1/"n", so that
Now we have
These expressions can be used to prove the prime number theorem by means of the inverse Mellin transform. Riemann's prime-counting function is easier to work with, and π("x") can be recovered from it by Möbius inversion.
Theta functions.
The Riemann zeta function can be given formally by a divergent Mellin transform
in terms of Jacobi's theta function
However this integral does not converge for any value of "s" and so needs to be regularized: this gives the following expression for the zeta function:
Laurent series.
The Riemann zeta function is meromorphic with a single pole of order one at
"s" = 1. It can therefore be expanded as a Laurent series about "s" = 1;
the series development then is
The constants γ"n" here are called the Stieltjes constants and can be defined
by the limit
The constant term γ0 is the Euler–Mascheroni constant.
Integral.
For all formula_95 the integral relation (cf. Abel–Plana formula)
holds true, which may be used for a numerical evaluation of the zeta-function.
Rising factorial.
Another series development using the rising factorial valid for the entire complex plane is
This can be used recursively to extend the Dirichlet series definition to all complex numbers.
The Riemann zeta function also appears in a form similar to the Mellin transform in an integral over the Gauss–Kuzmin–Wirsing operator acting on "x""s"−1; that context gives rise to a series expansion in terms of the falling factorial.
Hadamard product.
On the basis of Weierstrass's factorization theorem, Hadamard gave the infinite product expansion 
where the product is over the non-trivial zeros "ρ" of "ζ" and the letter "γ" again denotes the Euler–Mascheroni constant. A simpler infinite product expansion is
This form clearly displays the simple pole at "s" = 1, the trivial zeros at −2, −4, ... due to the gamma function term in the denominator, and the non-trivial zeros at "s" = "ρ". (To ensure convergence in the latter formula, the product should be taken over "matching pairs" of zeroes, i.e. the factors for a pair of zeroes of the form "ρ" and 1 − "ρ" should be combined.)
Logarithmic derivative on the critical strip.
where formula_101 is the density of zeros of "ζ" on the critical strip 0 < Re("s") < 1 ("δ" is the Dirac delta distribution, and the sum is over the nontrivial zeros "ρ" of "ζ").
Globally convergent series.
A globally convergent series for the zeta function, valid for all complex numbers "s" except for some integer "n", was conjectured by Konrad Knopp and proved by Helmut Hasse in 1930 (cf. Euler summation):
The series only appeared in an appendix to Hasse's paper, and did not become generally known until it was rediscovered more than 60 years later (see Sondow, 1994).
Hasse also proved the globally converging series
in the same publication.
Peter Borwein has shown a very rapidly convergent series suitable for high precision numerical calculations. The algorithm, making use of Chebyshev polynomials, is described in the article on the Dirichlet eta function.
Series representation at positive integers via the primorial.
Here "pn#" is the primorial sequence and "Jk" is Jordan's totient function.
Applications.
The zeta function occurs in applied statistics (see Zipf's law and Zipf–Mandelbrot law).
Zeta function regularization is used as one possible means of regularization of divergent series and divergent integrals in quantum field theory. In one notable example, the Riemann
zeta-function shows up explicitly in the calculation of the Casimir effect. The zeta function is also useful for the analysis of dynamical systems.
Infinite series.
The zeta function evaluated at positive integers appears in infinite series representations of a number of constants. There are more formulas in the article Harmonic number.
Some zeta series evaluate to more complicated expressions
Generalizations.
There are a number of related zeta functions that can be considered to be generalizations of the Riemann zeta function. These include the Hurwitz zeta function
(the convergent series representation was given by Helmut Hasse in 1930, cf. Hurwitz zeta function), which coincides with the Riemann zeta function when "q" = 1 (note that the lower limit of summation in the Hurwitz zeta function is 0, not 1), the Dirichlet L-functions and the Dedekind zeta-function. For other related functions see the articles Zeta function and L-function.
The polylogarithm is given by
which coincides with the Riemann zeta function when "z" = 1.
The Lerch transcendent is given by
which coincides with the Riemann zeta function when "z" = 1 and "q" = 1 (note that the lower limit of summation in the Lerch transcendent is 0, not 1).
The Clausen function Cl"s"("θ") that can be chosen as the real or imaginary part of Li"s"("e" "iθ").
The multiple zeta functions are defined by
One can analytically continue these functions to the "n"-dimensional complex space. The special values of these functions are called multiple zeta values by number theorists and have been connected to many different branches in mathematics and physics.

</doc>
