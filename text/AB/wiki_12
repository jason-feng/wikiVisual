<doc id="14884" url="http://en.wikipedia.org/wiki?curid=14884" title="Intermediate value theorem">
Intermediate value theorem

In mathematical analysis, the intermediate value theorem states that if a continuous function "f" with an interval ["a", "b"] as its domain takes values "f"("a") and "f"("b") at each end of the interval, then it also takes any value between "f"("a") and "f"("b") at some point within the interval. 
This has two important specializations: 1) If a continuous function has values of opposite sign inside an interval, then it has a root in that interval (Bolzano's theorem). 2) The image of a continuous function over an interval is itself an interval.
Motivation.
This captures an intuitive property of continuous functions: given "f" continuous on [1, 2] with the known values "f"(1) = 3 and "f"(2) = 5. Then the graph of "y" = "f"("x") must pass through the horizontal line "y" = 4 while "x" moves from 1 to 2. It represents the idea that the graph of a continuous function on a closed interval can be drawn without lifting your pencil from the paper.
Theorem.
The intermediate value theorem states the following: Consider
an interval "I" = ["a", "b"] in the real numbers ℝ and a continuous function "f" : "I" → ℝ. Then,
Remark: "Version II" states that the set of function values has no gap. For any two function values "c"<"d", even if they are outside the interval between "f"("a") and "f"("b"), all points in the interval ["c", "d"] are also function values,
A subset of the real numbers with no internal gap is an interval. "Version I" is obviously contained in "Version II".
Relation to completeness.
The theorem depends on, and is equivalent to, the completeness of the real numbers. The intermediate value theorem does not apply to the rational numbers ℚ because gaps exists between rational numbers; irrational numbers fill those gaps. For example, the function "f"("x") = "x"2 − 2 for "x" ∈ ℚ satisfies "f"(0) = −2 and "f"(2) = 2. However there is no rational number "x" such that "f"("x") = 0, because √2 is an irrational number.
Proof.
The theorem may be proved as a consequence of the completeness property of the real numbers as follows:
We shall prove the first case "f"("a") < "u" < "f"("b"); the second is similar.
Let "S" be the set of all "x" in ["a", "b"] such that "f"("x") < "u". Then "S" is non-empty since "a" is an element of "S", and "S" is bounded above by "b". Hence, by completeness, the supremum "c" = sup "S" exists. That is, "c" is the lowest number that is greater than or equal to every member of S. We claim that "f"("c") = "u". 
Fix some "ε" > 0. Since "f" is continuous, there is a "δ" > 0 such that | "f"("x") − "f"("c") | < ε whenever | "x" − "c" | < "δ". This means that
for all "x" between "c" − "δ" and "c" + "δ". By the properties of the supremum, there exist "a*" between "c" − "δ" and "c" that is contained in "S", so that for that "a*"
Choose "a**" between "c" and "c" + "δ" that will obviously not contained in "S", so we have
Both inequalities
are valid for all "ε" > 0, from which we deduce "f"("c") = "u" as the only possible value, as stated.
An alternative proof may be found at non-standard calculus.
History.
For "u" = 0 above, the statement is also known as "Bolzano's theorem." This theorem was first proved by Bernard Bolzano in 1817. Augustin-Louis Cauchy provided a proof in 1821. Both were inspired by the goal of formalizing the analysis of functions and the work of Joseph-Louis Lagrange. The idea that continuous functions possess the intermediate value property has an earlier origin. Simon Stevin proved the intermediate value theorem for polynomials (using a cubic as an example) by providing an algorithm for constructing the decimal expansion of the solution. The algorithm iteratively subdivides the interval into 10 parts, producing an additional decimal digit at each step of the iteration. Before the formal definition of continuity was given, the intermediate value property was given as part of the definition of a continuous function. Proponents include Louis Arbogast, who assumed the functions to have no jumps, satisfy the intermediate value property and have increments whose sizes corresponded to the sizes of the increments of the variable.
Earlier authors held the result to be intuitively obvious, and requiring no proof. The insight of Bolzano and Cauchy was to define a general notion of continuity (in terms of infinitesimals in Cauchy's case, and using real inequalities in Bolzano's case), and to provide a proof based on such definitions.
Generalizations.
The intermediate value theorem can be seen as a consequence of the following two statements from topology:
The intermediate value theorem generalizes in a natural way: Suppose that "X" is a connected topological space and ("Y", <) is a totally ordered set equipped with the order topology, and let "f" : "X" → "Y" be a continuous map. If "a" and "b" are two points in "X" and "u" is a point in "Y" lying between "f"("a") and "f"("b") with respect to <, then there exists "c" in "X" such that "f"("c") = "u". The original theorem is recovered by noting that ℝ is connected and that its natural topology is the order topology.
The Brouwer fixed-point theorem is a related theorem that, in one dimension gives a special case of the intermediate value theorem.
Converse is false.
A "Darboux function" is a real-valued function "f" that has the "intermediate value property", i.e., that satisfies the conclusion of the intermediate value theorem: for any two values "a" and "b" in the domain of "f", and any "y" between "f"("a") and "f"("b"), there is some "c" between "a" and "b" with "f"("c") = "y". The intermediate value theorem says that every continuous function is a Darboux function. However, not every Darboux function is continuous; i.e., the converse of the intermediate value theorem is false. 
As an example, take the function "f" : [0, ∞) → [−1, 1] defined by "f"("x") = sin(1/"x") for "x" > 0 and f(0) = 0. This function is not continuous at "x" = 0 because the limit of "f"("x") as "x" tends to 0 does not exist; yet the function has the intermediate value property. Another, more complicated example is given by the Conway base 13 function.
Historically, this intermediate value property has been suggested as a "definition" for continuity of real-valued functions; this definition was not adopted.
Darboux's theorem states that all functions that result from the differentiation of some other function on some interval have the intermediate value property (even though they need not be continuous).
Implications of theorem in real world.
The theorem implies that on any great circle around the world, the temperature, pressure, elevation, carbon dioxide concentration, or any other similar scalar quantity which varies continuously, there will always exist two antipodal points that share the same value for that variable.
"Proof:" Take "f" to be any continuous function on a circle. Draw a line through the center of the circle, intersecting it at two opposite points "A" and "B". Let "d" be defined by the difference "f"("A") − "f"("B"). If the line is rotated 180 degrees, the value −"d" will be obtained instead. Due to the intermediate value theorem there must be some intermediate rotation angle for which "d" = 0, and as a consequence "f"("A") = "f"("B") at this angle.
This is a special case of a more general result called the Borsuk–Ulam theorem.
Another generalization for which this holds is for any closed convex n (n > 1) dimensional shape. Specifically, for any continuous function whose domain is the given shape, and any point inside the shape (not necessarily its center), there exist two antipodal points with respect to the given point whose functional value is the same. The proof is identical to the one given above.
The theorem also underpins the explanation of why rotating a wobbly table will bring it to stability (subject to certain easily met constraints).

</doc>
<doc id="14889" url="http://en.wikipedia.org/wiki?curid=14889" title="Iran–Iraq War">
Iran–Iraq War

The Iran–Iraq War was an armed conflict between the Islamic Republic of Iran and the Republic of Iraq lasting from September 1980 to August 1988, making it the 20th century's longest conventional war. It was initially referred to in English as the "Gulf War" prior to the Persian Gulf War of the early 1990s.
The Iran–Iraq War began when Iraq invaded Iran via air and land on 22 September 1980. It followed a long history of border disputes, and was motivated by fears that the Iranian Revolution in 1979 would inspire insurgency among Iraq's long-suppressed Shia majority as well as Iraq's desire to replace Iran as the dominant Persian Gulf state. Although Iraq hoped to take advantage of Iran's revolutionary chaos and attacked without formal warning, they made only limited progress into Iran and were quickly repelled; Iran regained virtually all lost territory by June 1982. For the next six years, Iran was on the offensive. A number of proxy forces participated in the war, most notably the Iranian Mujahedin-e-Khalq siding with Ba'athist Iraq and Iraqi Kurdish militias of Kurdistan Democratic Party and Patriotic Union of Kurdistan siding with Iran—all suffering a major blow by the end of the conflict.
Despite calls for a ceasefire by the United Nations Security Council, hostilities continued until 20 August 1988. The war finally ended with Resolution 598, a U.N.-brokered ceasefire which was accepted by both sides. At the war's conclusion, it took several weeks for Iranian armed forces to evacuate Iraqi territory to honour pre-war international borders set by the 1975 Algiers Agreement. The last prisoners of war were exchanged in 2003.
The war cost both sides in lives and economic damage: half a million Iraqi and Iranian soldiers, with an equivalent number of civilians, are believed to have died, with many more injured; however, the war brought neither reparations nor changes in borders. The conflict has been compared to World War I:171 in terms of the tactics used, including large-scale trench warfare with barbed wire stretched across trenches, manned machine-gun posts, bayonet charges, human wave attacks across a no-man's land, and extensive use of chemical weapons such as mustard gas by the Iraqi government against Iranian troops, civilians, and Iraqi Kurds. The United States, alongside regional and international powers, supported Iraq with loans, military equipment and satellite imagery during Iraqi gas attacks against Iranian targets. At the time of the conflict, the U.N. Security Council issued statements that "chemical weapons had been used in the war." U.N. statements never clarified that only Iraq was using chemical weapons, and according to retrospective authors "the international community remained silent as Iraq used weapons of mass destruction against Iranian[s] as well as Iraqi Kurds."
Terminology.
The Iran–Iraq War was originally referred to as the "Gulf War" until the Persian Gulf War of 1990 and 1991, after which it was referred to as the "First Persian Gulf War" The Iraq–Kuwait conflict. What was originally known as the "Second Persian Gulf War" eventually became known simply as the "Gulf War". The Iraq War from 2003 to 2011 has since been called the "Second Persian Gulf War". It is now most commonly called the Iran-Iraq war.
In Iran, the war is known as the "Imposed War" (جنگ تحمیلی, "Jang-e Tahmīlī") and the "Holy Defense" (دفاع مقدس, "Defā'-e Moqaddas"). In Iraq, Saddam Hussein had initially dubbed the conflict the "Whirlwind War".:219 It was also referred to as "Saddām's Qādisiyyah" (قادسية صدام, "Qādisiyyat Ṣaddām"), in reference to the Battle of al-Qādisiyyah.
Origins.
Iran–Iraq relations.
Since the Ottoman–Persian Wars of the 16th and 17th centuries, Iran (known as Persia prior to 1935) and the Ottomans fought over Iraq (then known as Mesopotamia) and full control of the Shatt al-Arab/Arvand Roud waterway until the signing of the Treaty of Zuhab in 1639 which established the final borders between Iran and Iraq.:4 The Shatt al-Arab was considered an important channel for both states' oil exports, and in 1937, Iran and the newly independent Iraq signed a treaty to settle the dispute. In the same year, Iran and Iraq both joined the Saadabad Pact, and relations between the two states remained good for decades afterwards.
The 1937 treaty recognised the Iran–Iraq border to be along the low-water mark on the Shatt al-Arab's eastern side, except at Abadan and Khorramshahr, where the frontier ran along the deep water line ("thalweg"). This gave Iraq control of most of the waterway and required Iran to pay tolls whenever its ships used it.
In 1955, both nations joined the Baghdad Pact. However, the overthrow of the Hashemites in Iraq in 1958 brought a nationalist government to power which promptly abandoned the pact. On 18 December 1959, Iraq's new leader, General Abdul Karim Qassim, declared: "We do not wish to refer to the history of Arab tribes residing in al-Ahwaz and Mohammareh [Khorramshahr]. The Ottomans handed over Mohammareh, which was part of Iraqi territory, to Iran." The Iraqi government's dissatisfaction with Iran's possession of the oil-rich Khuzestan province (which the Iraqis called "Arabistan") that had a large Arabic-speaking population was not limited to rhetorical statements. Iraq began supporting secessionist movements in Khuzestan, and raised the issue of its territorial claims at an Arab League meeting, though unsuccessfully.
Iraq showed reluctance in fulfilling existing agreements with Iran—especially after Egyptian President Gamal Abdel Nasser's death in 1970 and the Iraqi Ba’ath Party's rise which took power in a 1968 coup, leading Iraq to take on the self-appointed role of "leader of the Arab world". At the same time, by the late 1960s, the build-up of Iranian power under Shah Mohammad Reza Pahlavi, who had gone on a military spending spree, led Iran to take a more assertive stance in the region.
In April 1969, Iran abrogated the 1937 treaty over the Shatt al-Arab, and as such, ceased paying tolls to Iraq when its ships used the waterway. The Shah justified his move by arguing that almost all river borders around the world ran along the "thalweg", and by claiming that because most of the ships that used the waterway were Iranian, the 1937 treaty was unfair to Iran.:37 Iraq threatened war over the Iranian move, but when, on 24 April 1969, an Iranian tanker escorted by Iranian warships sailed down the river, Iraq—being the militarily weaker state—did nothing.
Iran's abrogation of the treaty marked the beginning of a period of acute Iraqi-Iranian tension that was to last until the Algiers Accords of 1975. In 1969, Saddam Hussein, Iraq's deputy prime minister, stated: "Iraq's dispute with Iran is in connection with Khuzestan, which is part of Iraq's soil and was annexed to Iran during foreign rule." Soon, Iraqi radio stations began exclusively broadcasting into "Arabistan", encouraging Arabs living in Iran and even Balūchīs to revolt against the Shah's government. Basra TV stations began showing Iran's Khuzestan province as part of Iraq's new province "Nasiriyah", renaming all its cities with Arabic names.
In 1971, Iraq (now under Saddam's effective rule) broke diplomatic relations with Iran after claiming sovereignty rights over the islands of Abu Musa, Greater Tunb, and Lesser Tunb in the Persian Gulf following the withdrawal of the British. As retaliation for Iraq's claims to Khuzestan, Iran became the main patron of Iraq's Kurdish rebels in the early 1970s, giving the Iraqi Kurds bases in Iran and arming the Kurdish groups. In addition to Iraq fomenting separatism in Iran's Khuzestan and Balochistan provinces, both states encouraged separatist activities by Kurdish nationalists in the other state. From March 1974 to March 1975, Iran and Iraq fought border wars over Iran's support of Iraqi Kurds. In 1975, the Iraqis launched an offensive into Iran using tanks, though the Iranians defeated them. Several other attacks took place; however, Iran had the world's fifth most powerful military at the time and easily defeated the Iraqis with their air force. As a result, Iraq decided against continuing the war, choosing instead to make concessions to Tehran to end the Kurdish rebellion.
In the 1975 Algiers Agreement, Iraq made territorial concessions—including the Shatt al-Arab waterway—in exchange for normalised relations. In return for Iraq recognising that the frontier on the waterway ran along the entire "thalweg", Iran ended its support of Iraq's Kurdish guerrillas. Iraqis viewed the Algiers Agreement as humiliating.:260 However, the agreement meant the end of Iranian and American support for the Peshmerga, who were defeated by Iraq's government in a short campaign that claimed 20,000 lives.:298 The British journalist Patrick Brogan wrote that "...the Iraqis celebrated their victory in the usual manner, by executing as many of the rebels as they could lay their hands on.":298
The relationship between the governments of Iran and Iraq briefly improved in 1978, when Iranian agents in Iraq discovered plans for a pro-Soviet coup d'état against Iraq's government. When informed of this plot, Saddam ordered the execution of dozens of his army's officers and in a sign of reconciliation, expelled Ruhollah Khomeini, an exiled leader of clerical opposition to the Shah, from Iraq. Despite that, Saddam merely considered the Algiers Agreement to be a truce, rather than a definite settlement and waited for the opportunity to contest it.
After the Iranian Revolution.
Tensions between Iraq and Iran were fueled by Iran's Islamic revolution and its appearance of being a Pan-Islamic force, in contrast to Iraq's Arab nationalism. Despite Iraq's goals of regaining the Shatt al-Arab, the Iraqi government seemed to initially welcome Iran's Revolution, which overthrew Iran's Shah, who was seen as a common enemy. It is difficult to pinpoint when tensions began to build, but there were frequent cross-border skirmishes, largely at Iran's instigation.
Ayatollah Ruhollah Khomeini called on Iraqis to overthrow the Ba'ath government, and it was received with considerable anger in Baghdad. On 17 July 1979, despite Khomeini's call, Saddam gave a speech praising the Iranian Revolution and called for an Iraqi-Iranian friendship based on non-interference in each other's internal affairs. When Khomeini rejected Saddam's overture by calling for Islamic revolution in Iraq, Saddam was alarmed. Iran's new Islamic administration was regarded in Baghdad as an irrational, existential threat to the Ba'ath government, especially because the Ba'ath party, having a secular nature, discriminated and posed a threat to the Shia movement in Iraq, whose clerics were Iran's allies within Iraq and whom Khomeini saw as oppressed.
Saddam's primary interest in war may have also stemmed from his desire to right the supposed "wrong" of the Algiers Agreement, in addition to finally achieving his desire of annexing Khuzestan and becoming the regional superpower. Saddam's goal was to replace Egypt as the "leader of the Arab world" and to achieve hegemony over the Persian Gulf. He saw Iran's increased weakness due to revolution, sanctions, and international isolation. Saddam had heavily invested in Iraq's military since his defeat against Iran in 1975, buying large amounts of weaponry from the Soviet Union and France. By 1980, Iraq possessed 200,000 soldiers, 2,000 tanks and 450 aircraft.:1 Watching the powerful Iranian army that frustrated him in 1974–1975 disintegrate, he saw an opportunity to attack, using the threat of Islamic Revolution as a pretext.
A successful invasion of Iran would enlarge Iraq's petroleum reserves and make Iraq the region's dominant power. With Iran engulfed in chaos, an opportunity for Iraq to annex the oil-rich Khuzestan Province materialized.:261 In addition, Khuzestan's large ethnic Arab population would allow Saddam to pose as a liberator for Arabs from Persian rule.:260 Fellow Gulf states such as Saudi Arabia and Kuwait (despite being hostile to Iraq) encouraged Iraq to attack, as they feared that an Islamic revolution would take place within their own borders. Certain Iranian exiles also helped convince Saddam that if he invaded, the fledgling Islamic republic would quickly collapse.
In 1979–80, Iraq was the beneficiary of an oil boom that saw it take in US$33 billion, which allowed Iraq's government to go on a spending spree on both civilian and military projects. On several occasions, Saddam the Islamic conquest of Iran in promoting his position against Iran. For example, on 2 April 1980, half a year before the war's outbreak, in a visit to Baghdad's al-Mustansiriya University, he drew parallels to Persia's defeat at the 7th century Battle of al-Qādisiyyah:In your name, brothers, and on behalf of the Iraqis and Arabs everywhere we tell those Persian cowards and dwarfs who try to avenge al-Qadisiyah that the spirit of al-Qadisiyah as well as the blood and honor of the people of al-Qadisiyah who carried the message on their spearheads are greater than their attempts.
In 1979–1980, anti-Ba'ath riots arose in the Iraq's Shia areas by groups who were working toward an Islamic revolution in their country. Saddam and his deputies believed that the riots had been inspired by the Iranian Revolution and instigated by Iran's government. On 10 March 1980, when Iraq declared Iran's ambassador persona non-grata, and demanded his withdrawal from Iraq by 15 March, Iran replied by downgrading its diplomatic ties to the charge d'affaires level, and demanded that Iraq withdraw their ambassador from Iran. In April 1980, Grand Ayatollah Mohammad Baqir al-Sadr and his sister Amina Haydar (better known as Bint al-Huda) were hanged as part of a crackdown to restore Saddam's control. The execution of Iraq's most senior Ayatollah caused outrage throughout the Islamic world, especially among Shias.
Iraq soon after expropriated the properties of 70,000 civilians believed to be of Iranian origin and expelled them from its territory. Many, if not most, of those expelled were in fact Arabic-speaking Iraqi Shias who had little to no family ties with Iran. This caused tensions between the two nations to increase further.
In April 1980, Shia militants assassinated 20 Ba'ath officials, and Deputy Prime Minister Tariq Aziz was almost assassinated on 1 April; Aziz survived, but 11 students were killed in the attack. Three days later, the funeral procession being held to bury the students was bombed. Iraqi Information Minister Latif Nusseif al-Jasim also barely survived assassination by Shia militants. The Shias' repeated calls for the overthrow of the Ba'ath party and the support they allegedly received from Iran's new government led Saddam to increasingly perceive Iran as a threat that, if ignored, might one day overthrow him; he thus used the attacks as pretext for attacking Iran later that September, though skirmishes along the Iran–Iraq border had already become a daily event by May that year.
Iraq also helped to instigate riots among Iranian Arabs in Khuzestan province, supporting them in their labor disputes, and turning uprisings into armed battles between Iran's Revolutionary Guards and militants, killing over 100 on both sides. At times, Iraq also supported armed rebellion by the Kurdish Democratic Party of Iran in Kurdistan. The most notable of such events was the Iranian Embassy siege in London, in which six armed Khuzestani Arab insurgents took the Iranian Embassy's staff as hostages, resulting in an armed siege that was finally ended by Britain's Special Air Service.
According to former Iraqi general Ra'ad al-Hamdani, the Iraqis believed that in addition to the Arab revolts, the Revolutionary Guards would be drawn out of Tehran, leading to a counter-revolution in Iran that would cause Khomeini's government to collapse and thus ensure Iraqi victory. However, rather than turning against the revolutionary government as experts had predicted, Iran's people (including Iranian Arabs) rallied in support of their country and put up a stiff resistance.
Iraqi preparations.
Iraq began planning offensives, confident that they would succeed. Iran lacked both cohesive leadership and spare parts for their American-made equipment. Iraq, on the other hand, possessed a fully equipped and trained modern military, consisting of 200,000 men, 2,000 tanks, and 320 aircraft. The Iraqis could mobilise up to 12 mechanised divisions, and morale was running high. Through the 1970s, Saddam had armed his forces with the latest military hardware available from the Soviet Union.
In addition, the area around the Shatt al-Arab posed no obstacle for the Iraqis, as they possessed river crossing equipment. Iraq correctly deduced that Iran's defences at the crossing points around the Kharkeh and Karoun Rivers were undermanned and that the rivers could be easily crossed. Iraqi intelligence was also informed that the Iranian forces in Khuzestan (which consisted of two divisions prior to the revolution) now only consisted of several ill-equipped and under-strength battalions. Only a handful of company-sized tank units remained operational.
The only qualms the Iraqis had were over the Islamic Republic of Iran Air Force (formerly the Imperial Iranian Air Force). Despite the purge of several key pilots and commanders as well as the lack of spare parts, the air force showed its power during local uprisings and rebellions. They were also active after the failed U.S. attempt to rescue its hostages, Operation Eagle Claw. As such, Iraq's leaders decided to carry out a surprise airstrike against the Iranian air force's infrastructure prior to the main invasion.
Iranian preparations.
In Iran, severe officer purges (including numerous executions ordered by Sadegh Khalkhali, the new Revolutionary Court judge), and shortages of spare parts for Iran's U.S.-made equipment had crippled Iran's once-mighty military. Between February and September 1979, Iran's government executed 85 senior generals and forced all major-generals and most brigadier-generals into early retirement. By September 1980, the government had purged 12,000 army officers. These purges resulted in a drastic decline in the Iranian military's operational capacities. Their regular army (which, in 1978, was considered the world's fifth most powerful) had been badly weakened by purges and lack of spare parts. The desertion rate had reached 60%, and the officer corps was devastated. The most highly skilled soldiers and aviators were exiled, imprisoned, or executed. Throughout the war, Iran never managed to fully recover from this flight of human capital. Continuous sanctions prevented Iran from acquiring many heavy weapons, such as tanks and aircraft. When the invasion occurred, many pilots and officers were released from prison, or had their executions commuted to combat the Iraqis. In addition, many junior officers were promoted to generals, resulting in the army being more integrated as a part of the regime by the war's end, as it is today. Iran still had at least 1,000 operational tanks and several hundred functional aircraft, and could cannibalize equipment to procure spare parts.
Meanwhile, a new paramilitary organisation gained prominence in Iran, the Islamic Revolutionary Guard Corps (often shortened to "Revolutionary Guards", and known in Iran as the "Sepah-e-Pasdaran"), which intended to protect the new regime and counterbalance the decaying army. Despite having been trained as a paramilitary organisation, after the Iraqi invasion, they were forced to act as a regular army. Initially, they refused to fight alongside the army, which resulted in many defeats, but, by 1982, the two groups began carrying out combined operations. Another paramilitary militia was founded in response to the invasion, the "Army of 20 Million", commonly known as the Basij. The Basij were poorly armed and had members as young as 12 and as old as 70. They often acted in conjunction with the Revolutionary Guard, launching so-called human wave attacks and other campaigns against the Iraqis. They were subordinate to the Revolutionary Guards, and they made up most of the manpower that was used in the Revolutionary Guard's attacks.
Border conflicts leading to war.
By September, skirmishes between Iran and Iraq were increasing in number. Iraq began to grow bolder, both shelling and launching border incursions in disputed territories. Iran responded by shelling several Iraqi border towns and posts, though this did little to alter the situation on the ground. By 10 September, Saddam declared that the Iraqi Army had "liberated" all disputed territories within Iran. With the conclusion of the "liberating operations", on 17 September, in a statement addressed to Iraq's parliament, Saddam stated:The frequent and blatant Iranian violations of Iraqi sovereignty...have rendered the 1975 Algiers Agreement null and void... This river [Shatt al-Arab]...must have its Iraqi-Arab identity restored as it was throughout history in name and in reality with all the disposal rights emanating from full sovereignty over the river...We in no way wish to launch war against Iran.
Despite Saddam's claim that Iraq did not want war with Iran, the next day his forces proceeded to attack Iranian border posts in preparation for the planned invasion. Iraq's 7th Mechanised and 4th Infantry Divisions attacked the Iranian border posts leading to the cities of Fakkeh and Bostan, opening the route for future armoured thrusts into Iran. Weakened by internal chaos, Iran was unable to repel the attacks; which in turn led to Iraq becoming more confident in its military edge over Iran and prompting them to believe in a quick victory.
Geographic analysis.
The mountainous border between Iran and Iraq made a deep ground invasion almost impossible, and air strikes were used instead. The invasion's first waves were a series of air strikes targeted at Iranian airfields. Iraq also attempted to bomb Tehran, Iran's capital and command centre, into submission.
Course of the war.
1980: Iraqi invasion.
Iraq launched a full-scale invasion of Iran on 22 September 1980. The Iraqi Air Force launched surprise air strikes on ten Iranian airfields with the objective of destroying the Iranian Air Force. The attack damaged some of Iran's airbase infrastructure, but failed to destroy a significant number of aircraft: the Iraqi Air Force was only able to strike in depth with a few MiG-23BN, Tu-22, and Su-20 aircraft. Three MiG-23s managed to attack Tehran, striking its airport but destroyed only a few aircraft.
The next day, Iraq launched a ground invasion of Iran along a front measuring 644 km in three simultaneous attacks. The invasion's purpose, according to Saddam, was to blunt the edge of Khomeini's movement and to thwart his attempts to export his Islamic revolution to Iraq and the Persian Gulf states. Saddam hoped that by annexing Khuzestan, he would send such a blow to Iran's prestige that it would lead to the new government's downfall, or, at the very least, end Iran's calls for his overthrow.
Of Iraq's six divisions that were invading by ground, four were sent to Khuzestan, which was located near the border's southern end, to cut off the Shatt al-Arab from the rest of Iran and to establish a territorial security zone.:22 The other two divisions invaded across the northern and central part of the border to prevent an Iranian counter-attack. Two of the four Iraqi divisions, one mechanised and one armoured, operated near the southern end and began a siege of the strategically important port cities of Abadan and Khorramshahr.:22
The other two divisions, both armoured, secured the territory bounded by the cities of Khorramshahr, Ahvaz, Susangerd, and Musian.:22 On the central front, the Iraqis occupied Mehran, advanced towards the foothills of the Zagros Mountains, and were able to block the traditional Tehran–Baghdad invasion route by securing territory forward of Qasr-e Shirin, Iran.:23 On the northern front, the Iraqis attempted to establish a strong defensive position opposite Suleimaniya to protect the Iraqi Kirkuk oil complex.:23 Iraqi hopes of an uprising by the ethnic Arabs of Khuzestan failed to materialise, as most of the ethnic Arabs remained loyal to Iran. The Iraqi troops advancing into Iran in 1980 were described by Patrick Brogan as "badly led and lacking in offensive spirit".:261 The first known chemical weapons attack by Iraq on Iran probably took place during the fighting around Susangerd.
Though the Iraqi air invasion surprised the Iranians, the Iranian air force retaliated with an attack against Iraqi military bases and infrastructure in Operation "Kaman" 99 (Bow 99). Groups of F-4 Phantom and F-5 Tiger fighter jets attacked targets throughout Iraq, such as oil facilities, dams, petrochemical plants, and oil refineries, and included Mosul Airbase, Baghdad, and the Kirkuk oil refinery. Iraq was taken by surprise at the strength of the retaliation, as Iran took few losses while the Iraqis took heavy defeats and economic disruptions.
The Iranian force of AH-1 Cobra helicopter gunships began attacks on the advancing Iraqi divisions, along with F-4 Phantoms armed with Maverick missiles; they destroyed numerous armoured vehicles and impeded the Iraqi advance, though not completely halting it. Iran had discovered that a group of two or three low-flying F-4 Phantoms could hit targets almost anywhere in Iraq. Meanwhile, Iraqi air attacks on Iran were repulsed by Iran's F-14 Tomcat interceptor fighter jets, using Phoenix missiles, which downed a dozen of Iraq's Soviet-built fighters in the first two days of battle. 
The Iranian regular military, police forces, volunteer Basij, and Revolutionary Guards all conducted their operations separately; thus, the Iraqi invading forces did not face coordinated resistance. However, on 24 September, the Iranian Navy attacked Basra, Iraq, destroying two oil terminals near the Iraqi port Faw, which reduced Iraq's ability to export oil. The Iranian ground forces (primarily consisting of the Revolutionary Guard) retreated to the cities, where they set up defences against the invaders.
On 30 September, Iran's air force launched Operation Scorch Sword, striking and badly damaging the Osirak nuclear reactor near Baghdad.
By 1 October, Baghdad had been subjected to eight air attacks.:29 In response, Iraq launched aerial strikes against Iranian targets.
First Battle of Khorramshahr.
On 22 September, a prolonged battle began in the city of Khorramshahr, eventually leaving 7,000 dead on each side. Reflecting the bloody nature of the struggle, Iranians came to call Khorramshahr "City of Blood" (خونین شهر, "Khunin shahr").
The battle began with Iraqi air raids against key points and mechanised divisions advancing on the city in a crescent-like formation. They were slowed by Iranian air attacks and Revolutionary Guard troops with recoilless rifles, rocket-propelled grenades, and Molotov cocktails. The Iranians flooded the marsh areas around the city, forcing the Iraqis to traverse through narrow strips of land. Iraqi tanks launched attacks with no infantry support, and many tanks were lost to Iranian anti-tank teams. However, by 30 September, the Iraqis had managed to clear the Iranians from the outskirts of the city. The next day, the Iraqis launched infantry and armoured attacks into the city. After heavy house-to-house fighting, the Iraqis were repelled. On 14 October, the Iraqis launched a second offensive. The Iranians launched a controlled withdrawal from the city, street by street. By 24 October, most of the city was captured, and the Iranians evacuated across the Karun River. Some partisans remained, and fighting continued until 10 November.
Iraqi advance stalls.
The people of Iran, rather than turning against their still-weak Islamic Republic, rallied around their country to resist invasion. An estimated 200,000 fresh troops had arrived at the front by November, many of them ideologically committed volunteers.
Though Khorramshahr was finally captured, the battle had delayed the Iraqis enough to allow the large-scale deployment of the Iranian military. In November, Saddam ordered his forces to advance towards Dezful and Ahvaz, and lay sieges to both cities. However, the Iraqi offensive had been badly damaged by Iranian militias and air power. Iran's air force had destroyed Iraq's army supply depots and fuel supplies, and was strangling the country through an aerial siege. On the other hand, Iran's supplies had not been exhausted, despite sanctions, and they often cannibalised spare parts from other equipment and began searching for more parts on the black market. On 28 November, Iran launched Operation "Morvarid" (Pearl), a combined air and sea attack which destroyed 80% of Iraq's navy and all of their radar sites in the southern portion of the country. When Iraq laid siege to Abadan and dug their troops in around the city, they were unable to blockade the port, which allowed Iran to resupply Abadan by sea.
Iraq's strategic reserves had been depleted, and by now they lacked the power to go on any major offensives until nearly the end of the war. On 7 December, Hussein announced that Iraq was going on the defensive. By the end of 1980 the Iraqis had destroyed about 500 of Western-built Iranian tanks and captured 100 others.
For the next eight months, both sides were to be on a defensive footing (with the exception of the Battle of Dezful), as the Iranians needed more time to reorganise their forces and the damage inflicted by the purge of 1979–80. During this period, fighting consisted mainly of artillery duels and raids. Iraq had mobilised 21 divisions for the invasion, while Iran countered with only 13 regular army divisions and one brigade. Of the regular divisions, only seven were deployed to the border. The war bogged down into World War I-style trench warfare with tanks and modern late-20th century weapons. Due to the power of anti-tank weapons such as the RPG-7, armored maneuver by the Iraqis was very costly, and they consequently entrenched their tanks into static positions.
Iraq also began firing Scud missiles into the cities of Dezful and Ahvaz and used terror bombing to bring the war to the Iranian civilian population. Iran launched dozens of human wave assaults.
1981: Stalemate.
Battle of Dezful.
On 5 January 1981, Iran had reorganised its forces enough to launch a large-scale offensive, Operation "Nasr" (Victory). The Iranians launched their major armoured offensive from Dezful in the direction of Susangerd, consisting of the 16th "Qazvin" and the 77th "Khorasan" armoured divisions, and broke through Iraqi lines.:32 However, the Iranian tanks had raced through Iraqi lines with their flanks unprotected and with no infantry support; as a result, they were cut off by Iraqi tanks. In the ensuing Battle of Dezful, the Iranian division was nearly wiped out in one of the biggest tank battles of the war. When the Iranian tanks tried to manoeuvre, they became stuck in the mud of the marshes, and many tanks were abandoned. The Iraqis lost 45 T-62 tanks, while the Iranians lost 100-200 Chieftain and M-60 tanks. Reporters counted roughly 150 destroyed or deserted Iranian tanks, and also 40 Iraqi tanks. 141 Iranians were killed during this battle.
The battle had been ordered by Iranian president Abulhassan Banisadr, who was hoping that a victory might shore up his deteriorating political position; instead, the failure hastened his fall.:71 Many of Iran's problems took place because of political infighting between President Banisadr, who supported the regular army, and the hardliners who supported the IRGC. Once he was impeached and the competition ended, the performance of the Iranian military improved. Iran was further distracted by internal fighting between the regime and the Islamic Marxist "Mujaheddin e-Khalq" (MEK) on the streets of Iran's major cities in June 1981 and again in September.:250–251 After the end of these battles, the MEK gradually leaned towards Saddam Hussein, completely taking his side by the mid-1980s. Approximate clarification : People's Mujahedin of Iran started to take the side of Saddam in 1984 or 1986(Mid of 80s decade). Clearly, in 1986 Rajavi moved from Paris to Iraq and set up a base on the Iranian border. The Battle of Dezful became a critical battle in Iranian military thinking. Less emphasis was placed on the Army with its conventional tactics, and more emphasis was placed on the Revolutionary Guard with its unconventional tactics.
Attack on H3.
The Iraqi Air Force, badly damaged by the Iranians, was moved to the H-3 Airbase in Western Iraq, near the Jordanian border and away from Iran. However, on 3 April 1981, the Iranian air force used eight F-4 Phantom fighter bombers, four F-14 Tomcats, three Boeing 707 refuelling tankers, and one Boeing 747 command plane to launch a surprise attack on H3, destroying 27–50 Iraqi fighter jets.
Despite the successful H-3 airbase attack (in addition to other air attacks), in April, the Iranian Air Force was forced to cancel its successful 180-day air offensive against Iraq. In addition, they gave up trying to hold total control of Iranian airspace. Due to the heavy toll of sanctions and pre-war purges, the Iranian air force could not suffer further heavy attrition, and made the decision in order to limit their losses. They were also damaged by a fresh purge, after the impeachment crisis of President Banisadr. The Iranian air force would fight heavily on the defensive, trying to hold back the Iraqis rather than engaging them. While throughout 1981–1982 the Iraqi air force would remain weak, within the next few years they would rearm and expand again, and begin to regain the strategic initiative.
Iran introduces the human wave attack.
Since the Iranians suffered from a shortage of heavy weapons:225 but had a large number of devoted volunteer troops, they began using human wave attacks against the Iraqis. Typically, an Iranian assault would consist of the following: First, the poorly trained Basij would launch the primary human wave assaults to swamp the weakest portions of the Iraqi lines en masse (on some occasions even bodily clearing minefields). They would be followed up by the more experienced Revolutionary Guard infantry, who would breach the weakened Iraqi lines. Afterwards, the regular army using mechanized forces would maneuver through the breach and encircle and defeat the enemy.
According to historian Stephen C. Pelletiére, the idea of Iranian "human wave attacks" were a misconception. Instead, the Iranian tactics consisted of using groups of 22 man infantry squads which moved forward to attack specific objectives. As the squads surged forward to execute their missions, that gave the impression of a "human wave attack". Nevertheless, the idea of "human wave attacks" remained virtually synonymous with any large-scale infantry frontal assault Iran carried out. Large amounts of troops would be used, aimed at overwhelming the Iraqi lines (usually the weakest portion manned by the Iraqi Popular Army) regardless of losses.
According to the former Iraqi general Ra'ad al-Hamdani, the Iranian human waves charges consisted of armed "civilians" who carried most of their necessary equipment themselves into battle and often lacked command and control and logistics. However, Iranian tactics also were sophisticated as well. Operations were often carried out during the night, and deception operations, infiltrations, and maneuvers became more common.
The Iranians attempted to add the element of surprise to most of their attacks, differing them from those in World War I. During 1982, Iran used the same marshes that proved fatal to their tank forces during the Battle of Dezful to infiltrate to the rear of the Iraqi lines. The Iranians would reinforce the infiltrating forces with new units to keep up their momentum. Once a weak point was found, the Iranians would concentrate all of their forces into that area in an attempt to break through with human wave attacks.
The human wave attacks, while extremely bloody (tens of thousands of troops died in the process), when used in combination with infiltration and surprise caused major Iraqi defeats. As the Iraqis would dig in their tanks and infantry into static, entrenched positions, the Iranians would manage to break through the lines and encircle entire divisions. Merely the fact that the Iranian forces used maneuver warfare using their light infantry against static Iraqi defenses was often the decisive factor in the battle. However, lack of coordination between the Army and IRGC and shortages of heavy weaponry did play a detrimental role, with most of the infantry not supported by artillery and armor.
Operation Eighth Imam.
For about a year after the Iraqi offensive stalled in March 1981, there was little change in the front other than Iran retaking the high ground above Susangerd in May. However, by late 1981, Iran returned to the offensive and the Iraqi military was forced to retreat. Iran launched a new operation, Operation "Samen-ol-A'emeh" (The Eighth Imam), ending the Iraqi Siege of Abadan on 27–29 September 1981.:9 The Iranians used a combined force of regular army artillery with small groups of armor, supported by Pasdaran and Basij infantry. Iranians lost 150 M-48A tanks on 29 September. On 15 October, after the end of the siege, a large Iranian convoy was ambushed by Iraqi tanks. During tank battle between T-55s tanks and Chieftains, Iranians lost 20 Chieftains and other armored vehicles and withdrew.
Operation Tariq al-Qods.
By the fall of 1981, serious problems with morale had developed in the Iraqi Army, with many soldiers seeing no point to the invasion.
On 29 November 1981, Iran began Operation "Tariq al-Qods" with three army brigades and seven Revolutionary Guard brigades. The Iraqis failed to properly patrol their occupied areas, and the Iranians constructed a 14 km road through the unguarded sand dunes, infiltrating in and launching their attack from the Iraqi rear. The battle saw the town of Bostan being retaken from Iraqi divisions by 7 December.:10 Operation Tariq al-Qods also saw the first use of the Iranian "human wave" tactics, where the Revolutionary Guard light infantry charged at Iraqi positions repeatedly, oftentimes without the support of armour or air power. The fall of Bostan exacerbated the Iraqis' logistical problems, forcing them to use a roundabout route from Ahvaz far to the south to resupply its troops. 6,000 Iranians and over 2,000 Iraqis were killed in the operation.
1982: Iraqi retreat, Iranian offensive.
The Iraqis, realising that the Iranians were planning to attack, decided to preempt them with Operation "al-Fawz al-'Azim" (Supreme Success) on 19 March. Using a large number of tanks, helicopters, and fighter jets, they attacked the Iranian buildup around the Roghabiyeh pass. Though Saddam and his generals assumed they had succeeded, in reality the Iranian forces remained fully intact. The Iranians had concentrated much of their forces by bringing them directly from the cities and towns throughout Iran via trains, buses, and private cars. The concentration of forces did not resemble a traditional military buildup, and although the Iraqis detected a population buildup near the front, they failed to realise that this was an attacking force. As a result, Saddam's army was unprepared for the Iranian offensives to come.
Operation Undeniable Victory.
Iran's next major offensive, led by General Ali Sayad Shirazi, was Operation "Fath-ol-Mobeen" (Undeniable Victory). On 22 March 1982, Iran launched an attack which took the Iraqi forces by surprise: using Chinook helicopters, they landed behind Iraqi lines, silenced their artillery, and captured an Iraqi headquarter. The Iranian Basij then launched human wave attacks, consisting of 1,000 fighters per wave. Though they took heavy losses, they eventually broke through Iraqi lines.
The Revolutionary Guard and regular army followed up by surrounding the Iraqi 9th and 10th Armoured and 1st Mechanised divisions that had camped close to the Iranian town of Shush. The Iraqis launched a counter-attack using their 12th Armoured division to break the encirclement and rescue the surrounded divisions. Iraqi tanks came under attack by 95 Iranian F-4 Phantom and F-5 Tiger fighter jets, effectively destroying the entire division.
Operation Undeniable Victory ended decisively in Iran's favour, and Iraqi forces were driven away from, Shush, Dezful and Ahvaz. The Iranian armed forces destroyed 320-400 Iraqi tanks and armored vehicles in combat, but the price they paid for it was high. In just the first day of the battle the Iranians lost 196 tanks. By this time, most of the Khuzestan province had returned to Iran's hands.
Operation Beit ol-Moqaddas.
In preparation for Operation "Beit ol-Moqaddas", the Iranians had launched numerous air raids against Iraq air bases, destroying 47 jets (including Iraq's brand new Mirage F-1 fighter jets from France); this gave the Iranians air superiority over the battlefield while allowing them to monitor Iraqi troop movements.
On 29 April, Iran launched the offensive. 70,000 Revolutionary Guard and Basij members struck on several axes – Bostan, Susangerd, the west bank of the Karun River, and Ahvaz. The Basij launched human wave attacks, which were followed up by the regular army and Revolutionary Guard support along with tanks and helicopters. Under heavy Iranian pressure, the Iraqi forces retreated. By 12 May, Iran had driven out all Iraqi forces from the Susangerd area.:36 The Iranians captured several thousand Iraqi troops and a large number of tanks. Nevertheless, the Iranians took many losses as well, especially among the Basij.
The Iraqis retreated to the Karun River, with only Khorramshahr and a few outlying areas remaining in their possession. Saddam ordered 70,000 troops to be placed around the city of Khorramshahr. The Iraqis created a hastily constructed defence line around the city and outlying areas. To discourage airborne commando landings, the Iraqis also placed metal spikes and destroyed cars in areas likely to be used as troop landing zones. Saddam Hussein even visited Khorramshahr in a dramatic gesture, swearing that the city would never be relinquished. However, Khorramshahr's only resupply point was across the Shatt al-Arab, and the Iranian air force began bombing the supply bridges to the city, while their artillery zeroed in on the besieged garrison.
Liberation of Khorramshahr (Second Battle of Khorramshahr).
In the early morning hours of 23 May 1982 the Iranians began the drive towards Khorramshahr across the Karun River. This part of Operation Beit ol-Moqaddas was spearheaded by the 77th Khorasan division with tanks along with the Revolutionary Guard and Basij. The Iranians hit the Iraqis with destructive air strikes and massive artillery barrages, crossed the Karun River, captured bridgeheads, and launched human wave attacks towards the city. Saddam's defensive barricade collapsed; in less than 48 hours of fighting, the city fell and 19,000 Iraqis surrendered to the Iranians. A total of 10,000 Iraqis were killed or wounded in Khorramshahr, while the Iranians suffered 30,000 casualties. During the whole of Operation Beit ol-Moqaddas, 33,000 Iraqi soldiers were taken prisoner by the Iranians.
State of Iraqi armed forces.
The fighting had battered the Iraqi military: its strength fell from 210,000 to 150,000 troops; over 20,000 Iraqi soldiers were killed and over 30,000 captured; two out of four active armoured divisions and at least three mechanised divisions fell to less than a brigade's strength; and the Iranians had captured over 450 tanks and armoured personnel carriers.
The Iraqi Air Force was also left in poor shape: after losing up to 55 aircraft since early December 1981, they had only 100 intact fighter-bombers and interceptors. A defector who flew his MiG-21 to Syria in June 1982 revealed that the Iraqi Air Force had only three squadrons of fighter-bombers left that were capable of mounting offensive operations into Iran. The Iraqi Army Air Corps was in slightly better shape, and could still operate more than 70 helicopters. Despite that, the Iraqis still held 3,000 tanks, while Iran held 1,000.
At this point, Saddam believed that his army was too demoralised and damaged to hold onto Khuzestan and major swaths of territory in Iran, and withdrew his remaining armed forces from those areas. He redeployed them along the border between Iraq and Iran as a means of defence. However, his troops continued to occupy some key border areas of Iran, and continued to hold onto the disputed territories that prompted his invasion, including the Shatt al-Arab waterway which was the primary cause of the war. In response to their failures against the Iranians in Khorramshahr, Saddam ordered the executions of General Juwad Shitnah, General Salah al-Qadhi, and Colonel Masa abd al-Jalil. At least a dozen high-ranking officers were also executed during this time. This became increasingly common punishment for those who failed him in battle.
International response in 1982.
In April 1982, the rival Baathist regime in Syria, one of the few nations that supported Iran, closed the Kirkuk–Banias pipeline that had allowed Iraqi oil to reach tankers on the Mediterranean, reducing the Iraqi budget by US$5 billion per month. Journalist Patrick Brogan wrote, "It appeared for a while that Iraq would be strangled economically before it was defeated militarily.":263 Syria's closure of the Kirkuk-Banis pipeline left Iraq with the pipeline to Turkey as the only mean of exporting oil. However, that pipeline had a capacity of only 500000 oilbbl/d, which was insufficient to pay for the war.:160 However, Saudi Arabia, Kuwait, and the other Gulf states saved Iraq from bankruptcy by providing it with an average of $60 billion in subsidies per year.:263 Though Iraq had previously been hostile towards other Gulf states, "the threat of Persian fundamentalism was far more feared.":162–163:263 They were especially inclined to fear Iranian victory after Ayatollah Khomeini declared monarchies to be illegitimate and an un-Islamic form of government. Khomeini's statement was widely received as a call to overthrow the Gulf monarchies. Journalists John Bulloch and Harvey Morris wrote:The virulent Iranian campaign, which at its peak seemed to be making the overthrow of the Saudi regime a war aim on a par with the defeat of Iraq, did have an effect on the Kingdom [of Saudi Arabia], but not the one the Iranians wanted: instead of becoming more conciliatory, the Saudis became tougher, more self-confident, and less prone to seek compromise.:163 Saudi Arabia was said to provide Iraq with $1 billion per month starting mid-1982.:160
Iraq began receiving support from the United States and west European countries as well. Saddam Hussein was given diplomatic, monetary, and military support by the U.S., including massive loans, political clout, and intelligence on Iranian deployments gathered using American spy satellites, which allowed them to coordinate attacks against the Iranians. The Iraqis relied heavily on American satellite footage and radar planes to detect Iranian troop movements, and they enabled Iraq to move troops to the site before the battle.
With Iranian success on the battlefield, the U.S. made its backing of Iraq more pronounced, supplying intelligence, economic aid, and dual-use equipment and vehicles, as well as normalizing their intergovernmental relations (which had been broken during the 1967 Six-Day War). President Ronald Reagan decided that the United States "could not afford to allow Iraq to lose the war to Iran", and that the United States "would do whatever was necessary to prevent Iraq from losing the war with Iran". President Reagan formalised this policy by issuing a National Security Decision Directive to this effect in June 1982.
In 1982, Reagan removed Iraq from the list of countries "supporting terrorism" and sold weapons such as howitzers to Iraq via Jordan and Israel. France sold Iraq millions of dollars worth of weapons, including Gazelle helicopters, Mirage F-1 fighters, and Exocet missiles. Both the United States and West Germany sold Iraq dual-use pesticides and poisons that would be used to create chemical and other weapons, such as Roland missiles.
At the same time, the Soviet Union, angered with Iran for purging and destroying the Tudeh Party (Iran's national communist party), sent large shipments of weapons to Iraq. The Iraqi Air Force was rearmed with Soviet and French fighter jets and helicopters. Iraq also bought weapons such as AK-47s and rocket-propelled grenades from the Chinese. The depleted tank forces were replenished with Soviet tanks, and the Iraqis were reinvigorated in the face of the coming Iranian onslaught. Iran was portrayed as the aggressor, and would be seen as such until the 1990–1991 Persian Gulf War, when Iraq would be condemned.
Iran did not have the same financial capability to purchase arms to the same extent as Iraq. Iran could count on China, North Korea, Libya, Syria and Japan for supplying anything from weapons and munitions to logistical and engineering equipment. There were also clandestine purchases from certain elements within Israel and the United States, who also bought small arms from China, via North Korea.
Ceasefire proposal.
On 20 June 1982 Saddam announced that he wanted to sue for peace and proposed an immediate ceasefire. Khomeini rejected the Iraqi peace offer because an immediate ceasefire would mean that Iraqi troops would remain on Iran's borders in the disputed territory. He proclaimed that Iran would invade Iraq and would not stop until the Ba'ath regime was replaced by an Islamic republic. Iran supported a government in exile for Iraq, the Supreme Council of the Islamic Revolution in Iraq, led by exiled Iraqi cleric Mohammad Baqer al-Hakim, was dedicated to overthrowing the Ba'ath party. They recruited dissidents, exiles, and Shias to join the Badr Brigade, the military wing of the organisation.
The decision to invade Iraq was taken after much debate within the Iranian government. One faction, comprising Prime Minister Mir-Hossein Mousavi, Foreign Minister Ali Akbar Velayati, President Ali Khamenei, and Army Chief of Staff General Ali Sayad Shirazi, wanted to accept the ceasefire, as most of Iranian soil had been recaptured. In particular, General Shirazi was opposed to the invasion of Iraq on logistical grounds, and stated he would consider resigning if "unqualified people continued to meddle with the conduct of the war.":38 Of the opposing view was a hardline faction led by the clerics on the Supreme Defence Council, whose leader was the politically powerful speaker of the "Majlis", Akbar Hashemi Rafsanjani.
The most important factor for continuing the war (as Rafsanjani argued) was that despite Iran having foiled Iraq's major territorial ambitions, they still held nearly 3000 sqmi of Iranian territory, areas such as Shalamcheh, Mehran, the Naft Shahr oil fields, and many of the pre-war disputed areas (ex. Shatt al-Arab). In the event of an immediate ceasefire, the Iraqis would remain in those territories and the fear was they wouldn't relinquish those areas, but instead reinforce them for a future invasion. Iran understood that it was internationally isolated and unlikely to receive foreign support to pressure Iraq to withdraw, nor receive compensation, nor get an international condemnation of Iraq, making it unlikely that they could gain an advantageous peace unless they scored a major military victory. While western sources often believe that because Saddam's ceasefire plea of 1982 served as a basis for the 1988 ceasefire, they blame Khomeini's decision for extending the war for the next six years;:11,147 Iranian sources point out that Saddam's ceasefire plea would have Iraqi troops occupying Iran's border areas and Iran would receive no compensation, nor would Iraq be found guilty for starting the war, while the 1988 UN ceasefire ordered a return to the pre-war borders, and allowed a commission to determine war guilt and compensation, meaning that continuing the war was advantageous for Iran after all (although very bloody and costly).
While many officials wanted to fight the war until total victory, according to a 2003 interview with Rafsanjani (the architect of Iran's strategy against Iraq), Iran's main strategy was to occupy key portions of Iraqi territory to use as bargaining chips to force a diplomatic and political solution to the war (possibly in an international court), primarily getting Iraq to withdraw from the remaining areas of Iranian territory and to accept Iranian rights, have Iraq recognized as the aggressor, and pay compensation. The areas Rafsanjani had in mind were the Al-Faw Peninsula and the major port of Umm Qasr (cutting Iraq off from the sea), isolating and capturing Basra (the second largest city of Iraq), and capturing part of the Tigris River and Highway 8 (Baghdad-Basra Highway), which would effectively split Iraq in two and sever the Iraqi government from their main oil fields in the south. They also wanted to capture Darbandikhan Dam in northern Iraq, which supplied most of Iraq's water. They also hoped that their attacks would ignite a revolt against Saddam's rule by the Shia and Kurdish population of Iraq, possibly resulting in his downfall (or at least forcing him to the negotiation table). They were successful in doing so with the Kurdish population, but not the Shia. Iran had captured large quantities of Iraqi equipment (enough to create several tank battalions, Iran once again had 1,000 tanks) and also managed to clandestinely procure spare parts as well.
At a cabinet meeting in Baghdad, Minister of Health Riyadh Ibrahim Hussein suggested that Saddam could step down temporarily as a way of easing Iran towards a ceasefire, and then afterwards would come back to power.:147 Saddam, annoyed, asked if anyone else in the Cabinet agreed with the Health Minister's idea. When no one raised their hand in support, he escorted Riyadh Hussein to the next room, closed the door and shot him with his pistol.:147 Saddam returned to the room and continued with his meeting.
Iran invades Iraq.
Iraqi tactics against Iranian invasion.
For most part, Iraq remained on the defensive for the next six years of war, unable and unwilling to launch any major offensives, while Iran launched no fewer than 70 offensives against the Iraqis. Iraq's strategy changed from holding territory in Iran to denying Iran any major gains in Iraq (as well as holding onto disputed territories and Iran's border areas). Saddam commenced a policy of total war, gearing most of his country towards defending against Iran. By 1988, Iraq was spending 40–75% of their GDP on military equipment. Saddam had also more than doubled the size of the Iraqi army, from 200,000 soldiers (12 divisions and 3 independent brigades) to 500,000 (23 divisions and nine brigades). They also began launching air raids against Iranian border cities, greatly increasing the practice by 1984. By the end of 1982, Iraq had been resupplied with new Soviet materiel, and the ground war entered a new phase. Iraq used newly acquired T-55, T-62 and T-72 tanks, BM-21 truck-mounted rocket launchers, and Mi-24 helicopter gunships to prepare a Soviet-type three-line defence, replete with obstacles such as barbed wire, minefields, fortified positions and bunkers. The Combat Engineer Corps built bridges across water obstacles, laid minefields, erected earthen revetments, dug trenches, built machinegun nests, and prepared new defence lines and fortifications.:2
Iraq began to focus on using defense in depth to defeat the Iranians. Iraq created multiple static defense lines to bleed the Iranians through sheer size. When faced against large Iranian attack, where human waves would overrun Iraq's entrenched infantry defences, the Iraqis would often retreat, but their static defences would bleed the Iranians and channel them into certain directions, drawing them into a trap. Afterwards, Iraqi air and artillery attacks would pin the Iranians down, while tanks and mechanised infantry attacks using mobile warfare would push them back. Sometimes, the Iraqis would launch "probing attacks" into the Iranian lines to provoke them into launching their attacks sooner. Chemical weapons were used as well, and were a major source of Iranian infantry casualties. While Iranian human wave attacks were successful against the dug in Iraqi forces in Khuzestan, they had trouble breaking through Iraq's defense in depth lines. Iraq had a logistical advantage in their defence: the front was located near the main Iraqi bases and arms depots, allowing their army to be efficiently supplied.:260,265 By contrast, the front in Iran was a considerable distance away from the main Iranian bases and arms depots, and as such, Iranian troops and supplies had to travel through roads across mountain ranges before arriving at the front.:260
In addition, Iran's military power was weakened once again by large purges in 1982, resulting from another supposedly attempted coup.
Operation Ramadan (First Battle of Basra).
The Iranian generals wanted to launch an all-out attack on Baghdad and seize it before the weapon shortages continued to manifest further. Instead, that was rejected as being unfeasable, and the decision was made to capture one area of Iraq after the other in the hopes that a series of blows delivered foremost by the Revolutionary Guards Corps would force a political solution to the war (including Iraq withdrawing completely from disputed territories of Iran).
The Iranians planned their attack in southern Iraq, near Basra, the second most important city in Iraq. Called Operation Ramadan, it involved over 180,000 troops from both sides, and was one of the largest land battles since World War II.:3 Iranian strategy dictated that they launch their primary attack on the weakest point of the Iraqi lines; however, the Iraqis were informed of Iran's battle plans and moved all of their forces to the area the Iranians planned to attack. The Iraqis were equipped with tear gas to use against the enemy, which would be first major use of chemical warfare during the conflict, throwing an entire attacking division into chaos.
Over 100,000 Revolutionary Guards and Basij volunteer forces charged towards the Iraqi lines. The Iraqi troops had entrenched themselves in formidable defences, and had set up a network of bunkers and artillery positions. The Basij used human waves, and were even used to bodily clear the Iraqi minefields and allow the Revolutionary Guards to advance. Combatants came so close to one another that Iranians were able to board Iraqi tanks and throw grenades inside the hulls. By the eighth day, the Iranians had gained 16 km inside Iraq and had taken several bridges. Iran's Revolutionary Guards also used the T-55 tanks they had captured in earlier battles.
However, the attacks came to a halt and the Iranians turned to defensive measures. Seeing this, Iraq used their Mi-25 helicopters, along with French-built Gazelle helicopters armed with Euromissile HOT, against columns of Iranian mechanised infantry and tanks. These "hunter-killer" teams of helicopters, which had been formed with the help of East German advisors, proved to be very costly for Iranians. Aerial dogfights occurred between Iraqi MiGs and Iranian F-4 Phantoms.
On 16 July, Iran tried again further north and managed to push the Iraqis back. However, only 13 km from Basra, the poorly equipped Iranian forces were surrounded on three sides by Iraqis with heavy weaponry. Some were captured, while many were killed. Only a last-minute attack by Iranian AH-1 Cobra helicopters stopped the Iraqis from routing the Iranians. Three more similar attacks occurred around the Khorramshar-Baghdad road area towards the end of the month, but none were significantly successful. Iraq had concentrated three armoured divisions, the 3rd, 9th, and 10th, as a counter-attack force to attack any penetrations. They were successful in defeating the Iranian breakthroughs, but suffered heavy losses. The 9th Armoured Division in particular had to be disbanded, and was never reformed. 80,000 soldiers from both sides were killed. 400 Iranian tanks and armored vehicles were destroyed or abandoned, while Iraq lost 370 tanks.
Fighting during the rest of 1982.
After Iran's defeat in Operation Ramadan, they carried out only a few smaller attacks. Iran launched two limited offensives aimed at liberating the Sumar Hills and isolating the Iraqi pocket at Naft Shahr near the Iraqi border, both of which were Iranian territory still under Iraq occupation. They then aimed to capture the Iraqi border city of Mandali. They planned to take the Iraqis by surprise using Basij militiamen, army helicopters, and some armoured forces, then stretch their defences and possibly break through them to open a road to Baghdad for future exploitation. During Operation "Muslim ibn Aqil" (1–7 October), Iran recovered 150 km2 of its own territory and reached the outskirts of Mandali before being stopped by Iraqi helicopter and armoured attacks. During Operation "Muharram" (1–21 November), the Iranians captured part of the Bayat oilfield with their fighter jets and helicopters, destroying 105 Iraqi tanks, 70 APCs, and 7 planes with few losses. They nearly breached the Iraqi lines but failed to capture Mandali after the Iraqis sent reinforcements, including brand new T-72 tanks, which possessed armour that could not be pierced from the front by Iranian TOW missiles. The Iranian advance was also impeded by heavy rains. 3,500 Iraqis and an unknown number of Iranians died, with only minor gains for Iran.
1983–84: Strategic stalemate and war of attrition.
After the failure of the 1982 summer offensives, Iran believed that a major effort along the entire breadth of the front would yield the victory. During the course of 1983, the Iranians launched five major assaults along the front, though none achieved substantial success, as the Iranians staged more massive "human wave" attacks. By this time, it was estimated that no more than 70 Iranian fighter aircraft were still operational at any given time; Iran had their own helicopter repair facilities, left over from before the revolution, and thus often used helicopters for close air support. While Iranian fighter pilots had superior training compared to their Iraqi counterparts, and would continue to dominate in combat, due to shortages of aircraft, the size of defended territory and American intelligence supplied to Iraq, the Iraqis could exploit gaps in Iranian airspace. The Iraqis were able to gain air superiority towards the end of the war. Iraqi air campaigns met little opposition, striking over half of Iran.
Operation Before the Dawn.
Operation "Fajr al-Nasr" (Before the Dawn/Dawn of Victory), launched 6 February 1983, saw the Iranians shift focus from the southern to the central and northern sectors. Iran, using 200,000 "last reserve" Revolutionary Guard troops, attacked along a 40 km stretch near al-Amarah, Iraq about 200 km southeast of Baghdad, in an attempt to reach the highways connecting northern and southern Iraq. The attack was stalled by 60 km of hilly escarpments, forests, and river torrents blanketing the way to al-Amarah, but the Iraqis could not force the Iranians back. Iran directed artillery on Basra and Al Amarah, and Mandali.
The Iranians suffered a large number of casualties clearing minefields and breaching Iraqi anti-tank mines, which Iraqi engineers were unable to replace. After this battle, Iran reduced its use of human wave attacks, though they still remained a key tactic as the war went on.
The Mandali–Baghdad northcentral sector also witnessed fighting in April 1983, as Iranian attacks were stopped by Iraqi mechanised and infantry divisions. Casualties were high, and by the end of 1983, an estimated 120,000 Iranians and 60,000 Iraqis had been killed. Iran, however, held the advantage in the war of attrition.:2
Dawn Operations.
From early 1983–1984, Iran launched a series of four "Valfajr" (Dawn) Operations (that eventually numbered to 10). During Operation Dawn-1, in early February 1983, 50,000 Iranian forces attacked westward from Dezful and were confronted by 55,000 Iraqi forces. The Iranian objective was to cut off the road from Basra to Baghdad in the central sector. The Iraqis carried out 150 air sorties against the Iranians, and even bombed Dezful, Ahvaz, and Khorramshahr in retribution. The Iraqi counterattack was broken up by Iran's 92nd Armoured Division.
During Operation Dawn-2, the Iranian's directed insurgency operations by proxy in April 1983 by supporting the Kurds in the north. With Kurdish support, the Iranians attacked on 23 July 1983, capturing the Iraqi town of Haj Omran and maintaining it against an Iraqi poison gas counteroffensive. This operation incited Iraq to later conduct indiscriminate chemical attacks against the Kurds. The Iranians attempted to further exploit activities in the north on 30 July 1983, during Operation Dawn-3. Iran saw an opportunity to sweep away Iraqi forces controlling the roads between the Iranian mountain border towns of Mehran, Dehloran and Elam. Iraq launched airstrikes, and equipped attack helicopters with chemical warheads; while ineffective, it demonstrated both the Iraqi general staff's and Saddam's increasing interest in using chemical weapons. In the end, 17,000 had been killed on both sides, with no gain for either country.
The focus of Operation Dawn-4 in September 1983 was the northern sector in Iranian Kurdistan. Three Iranian regular divisions, the Revolutionary Guard, and Kurdistan Democratic Party (KDP) elements amassed in Marivan and Sardasht in a move to threaten the major Iraqi city Suleimaniyah. Iran's strategy was to press Kurdish tribes to occupy the Banjuin Valley, which was within 45 km of Suleimaniyah and 140 km from the oilfields of Kirkuk. To stem the tide, Iraq deployed Mi-8 attack helicopters equipped with chemical weapons and executed 120 sorties against the Iranian force, which stopped them 15 km into Iraqi territory. 5,000 Iranians and 2,500 Iraqis died. Iran gained 110 km2 of its territory back in the north, gained 15 km2 of Iraqi land, and captured 1,800 Iraqi prisoners while Iraq abandoned large quantities of valuable weapons and war materiel in the field. Iraq responded to these losses by firing a series of SCUD-B missiles into the cities of Dezful, Masjid Soleiman, and Behbehan. Iran's use of artillery against Basra while the battles in the north raged created multiple fronts, which effectively confused and wore down Iraq.
Iran's change in tactics.
Previously, the Iranians had outnumbered the Iraqis on the battlefield, but Iraq expanded their military draft (pursuing a policy of total war), and by 1984, the armies were equal in size. By 1986, Iraq had twice as many soldiers as Iran. By 1988, Iraq would have 1 million soldiers, giving it the fourth largest army in the world. Some of their equipment, such as tanks, outnumbered the Iranians' by at least five to one. Iranian commanders, however, remained more tactically skilled.
After the Dawn Operations, Iran attempted to change tactics. In the face of increasing Iraqi defense in depth, as well as increased armaments and manpower, Iran could no longer rely on simple human wave attacks. Iranian offensives became more complex and involved extensive maneuver warfare using primarily light infantry. Iran launched frequent, and sometimes smaller offensives to slowly gain ground and deplete the Iraqis through attrition. They wanted to drive Iraq into economic failure by wasting money on weapons and war mobilization, and to deplete their smaller population by bleeding them dry, in addition to creating an anti-government insurgency (they were successful in Kurdistan, but not southern Iraq). Iran also kept its goal of capturing important territory to force Iraq to negotiate. Iran also supported their attacks with heavy weaponry when possible and with better planning (although the brunt of the battles still fell to the infantry). The Army and Revolutionary Guards worked together better as their tactics improved. Human wave attacks became less frequent (although still used). To defeat Iraqi defense in depth, static positions, and heavy firepower, Iran began to focus on fighting in areas that the Iraqis could not use their heavy weaponry, such as marshes, valleys, and mountains, and frequently using infiltration tactics.
Iran began training troops in infiltration, patrolling, night-fighting, marsh warfare, and mountain warfare. They also began training thousands of Revolutionary Guard commandos in amphibious warfare, as southern Iraq is marshy and filled with wetlands. Iran used speedboats to cross the marshes and rivers in southern Iraq and landed troops on the opposing banks, where they would dig and set up pontoon bridges across the rivers and wetlands to allow heavy troops and supplies to cross. Iran also learned to integrate foreign guerrilla units as part of their military operations. On the northern front, Iran began working heavily with the Peshmerga, Kurdish guerrillas. Iranian military advisors organised the Kurds into raiding parties of 12 guerrillas, which would attack Iraqi command posts, troop formations, infrastructure (including roads and supply lines), and government buildings. The oil refineries of Kirkuk became a favourite target, and were often hit by homemade Peshmerga rockets.
Battle of the Marshes.
By 1984, the Iranian ground forces were reorganised well enough for the Revolutionary Guard to start Operation "Kheibar" (named after Kheibar, Saudi Arabia), which lasted from 24 February to 19 March.:171 On 15 February 1984, the Iranians began launching attacks against the central section of the front, where the Second Iraqi Army Corps was deployed: 250,000 Iraqis faced 250,000 Iranians. The goal of this new major offensive was the capture of Basra-Baghdad Highway, cutting off of Basra from Baghdad and setting the stage for an eventual attack upon the city. The Iraqi high command had assumed the marshlands above Basra were natural barriers to attack, and had not reinforced them. The marshes negated Iraqi advantage in armor, and absorbed artillery and bombs.
Prior to the attack, Iranian commandos on helicopters had landed behind Iraqi lines and destroyed Iraqi artillery. Iran launched two preliminary attacks prior to the main offensive, Operation Dawn 5 and Dawn 6. They saw the Iranians attempting to capture Kut al-Imara, Iraq and sever the highway connecting Baghdad to Basra, which would impede Iraqi coordination of supplies and defences. Iranian troops crossed the river on motorboats in a surprise attack, though only came within 24 km of the highway.
Operation Kheibar began on 24 February with Iranian infantrymen crossing the Hawizeh Marshes using motorboats and transport helicopters in an amphibious assault. The Iranians attacked the vital oil-producing Majnoon Island by landing troops via helicopters onto the islands and severing the communication lines between Amareh and Basra. They then continued the attack towards Qurna. By 27 February, they had captured the island, but suffered catastrophic helicopter losses to IRAF. On that day, a massive array of Iranian helicopters transporting Pasdaran troops were intercepted by Iraqi combat aircraft (MiGs, Mirages and Sukhois). In what was essentially an aerial slaughter, Iraqi jets shot down 49 of 50 Iranian helicopters. At times, fighting took place in waters over 2 m deep. Iraq ran live electrical cables through the water, electrocuting numerous Iranian troops and then displaying their corpses on state television.
By 29 February, the Iranians had reached the outskirts of Qurna and were closing in on the Baghdad–Basra highway. They had broken out of the marshes and returned to open terrain, where they were confronted by conventional Iraqi weapons, including artillery, tanks, air power, and mustard gas. 1,200 Iranian soldiers were killed in the counter-attack. The Iranians retreated back to the marshes, though they still held onto them along with Majnoon Island.:44
The Battle of the Marshes saw an Iraqi defence that had been under continuous strain since 15 February; they were relieved by their use of chemical weapons and defence-in-depth, where they layered defensive lines: even if the Iranians broke through the first line, they were usually unable to break through the second due to exhaustion and heavy losses.:171 They also largely relied on Mi-24 Hind to "hunt" the Iranian troops in the marshes, and at least 20,000 Iranians were killed in the marsh battles. Iran used the marshes as a springboard for future attacks/infiltrations.
Four years into the war, the human cost to Iran had been 170,000 combat fatalities and 340,000 wounded. Iraqi combat fatalities were estimated at 80,000 with 150,000 wounded.
The "Tanker War" and the "War of the Cities".
Unable to launch successful ground attacks against Iran, Iraq used their now expanded air force to carry out strategic bombing against Iranian shipping, economic targets, and cities in order to damage Iran's economy and morale. Iraq also wanted to provoke Iran into doing something that would cause the superpowers to be directly involved in the conflict on the Iraqi side.
Attacks on shipping.
The so-called "Tanker War" started when Iraq attacked the oil terminal and oil tankers at Kharg Island in early 1984. Saddam's aim in attacking Iranian shipping was to provoke the Iranians to retaliate with extreme measures, such as closing the Strait of Hormuz to all maritime traffic, thereby bringing American intervention: the United States had threatened several times to intervene if the Strait of Hormuz were closed. As such, the Iranians limited their retaliatory attacks to Iraqi shipping, leaving the strait open to general passage.
Iraq declared that all ships going to or from Iranian ports in the northern zone of the Persian Gulf were subject to attack. They used air power, primarily helicopters, F-1 Mirage, and MiG-23 fighters armed with Exocet anti-ship missiles, to enforce their threats. Iraq began to repeatedly bomb Iran's main oil export facility on Kharg Island, causing increasingly heavy damage. After these attacks, Iran attacked a Kuwaiti tanker carrying Iraqi oil near Bahrain on 13 May 1984, as well as a Saudi tanker in Saudi waters on 16 May. Because Iraq had become landlocked during the invasion, they had to rely on their Arab allies, primarily Kuwait, to transport their oil. Iran attacked tankers carrying Iraqi oil from Kuwait, later attacking tankers from any Persian Gulf state supporting Iraq. Attacks on ships of noncombatant nations in the Persian Gulf sharply increased thereafter, with both nations attacking oil tankers and merchant ships of neutral nations in an effort to deprive their opponent of trade. The Iranian attacks against Saudi shipping led to Saudi F-15s shooting down a pair of F-4 Phantom II on 5 June 1984.
The air and small-boat attacks, however, did little damage to Persian Gulf state economies, and Iran moved its shipping port to Larak Island in the Strait of Hormuz.
The Iranian Navy imposed a naval blockade of Iraq, using its British-built frigates to stop and inspect any ships thought to be trading with Iraq. They operated with virtual impunity, as Iraqi pilots had little training in hitting naval targets. Some Iranian warships attacked tankers with ship-to-ship missiles, while others used their radars to guide land-based anti-ship missiles to their targets. Iran began to rely on its new Revolutionary Guard's navy, which used Boghammar speedboats: fitted with rocket launchers, RPGs, and heavy machine guns, these speedboats would launch surprise attacks against tankers and cause substantial damage. Iran also used aircraft and helicopters to launch Maverick missiles and unguided rockets at tankers.
A U.S. Navy ship, the "Stark", was struck on 17 May 1987 by two Exocet anti-ship missiles fired from an Iraqi F-1 Mirage plane. The missiles had been fired at about the time the plane was given a routine radio warning by the "Stark". The frigate did not detect the missiles with radar, and warning was given by the lookout only moments before they struck. Both missiles hit the ship, and one exploded in crew quarters, killing 37 sailors and wounding 21.
Lloyd's of London, a British insurance market, estimated that the Tanker War damaged 546 commercial vessels and killed about 430 civilian sailors. The largest portion of the attacks was directed by Iraq against vessels in Iranian waters, with the Iraqis launching three times as many attacks as the Iranians.:3 But Iranian speedboat attacks on Kuwaiti shipping led Kuwait to formally petition foreign powers on 1 November 1986 to protect its shipping. The Soviet Union agreed to charter tankers starting in 1987, and the United States Navy offered to provide protection for foreign tankers reflagged and flying the U.S. flag starting 7 March 1987 in Operation Earnest Will. Neutral tankers shipping to Iran were unsurprisingly not protected by Earnest Will, resulting in reduced foreign tanker traffic to Iran, since they risked Iraqi air attack. Iran accused the US of helping Iraq.
During the course of the war, Iran attacked two Soviet Navy ships which were protecting Kuwaiti tankers. Notably, the "Seawise Giant", the largest ship ever built, was struck and damaged by Iraqi Exocet missiles as it was carrying Iranian crude oil out of the Gulf.
Attacks on cities.
Meanwhile, Iraq's air force also began carrying out strategic bombing raids against Iranian cities. While Iraq had launched numerous attacks with aircraft and missiles against border cities from the beginning of the war and sporadic raids on Iran's main cities, this was the first systematic strategic bombing that Iraq carried out during the war. This would become known as the "War of the Cities". With the help of the USSR and the west, Iraq's air force had been rebuilt and expanded. Meanwhile Iran, due to sanctions and lack of spare parts, had heavily curtailed their air force operations. Iraq used Tu-22 Blinder and Tu-16 Badger strategic bombers to carry out long-range high-speed raids on Iranian cities, including Tehran. Fighter-bombers such as the Mig-25 Foxbat and Su-22 Fitter were used against smaller or shorter range targets, as well as escorting the strategic bombers. Civilian and industrial targets were hit by the raids, and each successful raid inflicted economic damage from regular strategic bombing.
In response, the Iranians deployed their F-4 Phantoms to combat the Iraqis, and eventually they deployed F-14s as well. Most of the Iraqi air raids were intercepted by the Iranian fighter jets and air defense, but some also successfully hit their targets, becoming a major headache for Iran. By 1986, Iran also expanded their air defense network heavily to take the load of the fighting off the air force. By later in the war, Iraqi raids primarily consisted of indiscriminate missile attacks while air attacks were used only on fewer, more important targets. Starting in 1987, Saddam also ordered several chemical attacks on civilian targets in Iran, such as the town of Sardasht.
Iran also launched several retaliatory air raids on Iraq, while primarily shelling border cities such as Basra. Iran also bought some Scud missiles from Libya, and launched them against Baghdad. These too inflicted damage upon Iraq.
On 7 February 1984, (during the first war of the cities) Saddam ordered his air force to attack eleven Iranian cities; bombardments ceased on 22 February 1984. Though Saddam had aimed for the attacks to demoralise Iran and force them to negotiate, they had little effect, and Iran quickly repaired the damage. Iraq's air force took heavy losses, however, and Iran struck back, hitting Baghdad and other Iraqi cities. Nevertheless, the attacks resulted in tens of thousands of civilian casualties on both sides, and became known as the first "war of the cities". It was estimated that 1,200 Iranian civilians were killed during the raids in February alone. There would be five such major exchanges throughout the course of the war, and multiple minor ones. While interior cities such as Tehran, Tabriz, Qom, Isfahan and Shiraz did receive numerous raids, it was the cities of western Iran that suffered the most death and destruction.
Strategic situation in 1984.
By 1984, Iran's losses were estimated to be 300,000 soldiers, while Iraq's losses were estimated to be 150,000.:2 Foreign analysts agreed that both Iran and Iraq failed to use their modern equipment properly, and both sides failed to carry out modern military assaults that could win the war. Both sides also abandoned equipment in the battlefield because their technicians were unable to carry out repairs. Iran and Iraq showed little internal coordination on the battlefield, and in many cases units were left to fight on their own. As a result, by the end of 1984, the war was a stalemate.:2
 One limited offensive Iran launched (Dawn 7) took place from 18–25 October 1984, when they recaptured the Iranian city of Mehran, which had been occupied by the Iraqis from the beginning of the war.
1985–86: Offensives and retreats.
By 1985, Iraqi armed forces were receiving financial support from Saudi Arabia, Kuwait, and other Persian Gulf states, and were making substantial arms purchases from the Soviet Union, China, and France. For the first time since early 1980, Saddam launched new offensives.
On 6 January 1986, the Iraqis launched an offensive attempting to retake Majnoon Island. However, they were quickly bogged down into a stalemate against 200,000 Iranian infantrymen, reinforced by amphibious divisions. However, they managed to gain a foothold in the southern part of the island.
Iraq also carried out another "war of the cities" between 12–14 March, hitting up to 158 targets in over 30 towns and cities, including Tehran. Iran responded by launching 14 Scud missiles for the first time, purchased from Libya. More Iraqi air attacks were carried out in August, resulting in hundreds of additional civilian casualties. Iraqi attacks against both Iranian and neutral oil tankers in Iranian waters continued, with Iraq carrying out 150 airstrikes using French bought Super Etendard and Mirage F-1 jets as well as Super Frelon helicopters, using Exocet missiles.
Operation Badr.
The Iraqis attacked again on 28 January 1985; they were defeated, and the Iranians retaliated on 11 March 1985 with a major offensive directed against the Baghdad-Basra highway (one of the few major offensives conducted in 1985), codenamed Operation "Badr" (after the Battle of Badr, Muhammad's first military victory in Mecca). Ayatollah Khomeini urged Iranians on, declaring:It is our belief that Saddam wishes to return Islam to blasphemy and polytheism...if America becomes victorious...and grants victory to Saddam, Islam will receive such a blow that it will not be able to raise its head for a long time...The issue is one of Islam versus blasphemy, and not of Iran versus Iraq. This operation was similar to Operation Kheibar, though it invoked more planning. Iran used 100,000 troops, with 60,000 more in reserve. They assessed the marshy terrain, plotted points where they could land tanks, and constructed pontoon bridges across the marshes. The Basij forces were also equipped with anti-tank weapons.
The ferocity of the Iranian offensive broke through the Iraqi lines. The Revolutionary Guard, with the support of tanks and artillery, broke through north of Qurna on 14 March. That same night 3,000 Iranian troops reached and crossed the Tigris River using pontoon bridges and captured part of the Baghdad–Basra Highway 8, which they had failed to achieve in Operations Dawn 5 and 6.
Saddam responded by launching chemical attacks against the Iranian positions along the highway and by initiating the aforementioned second "war of the cities", with an air and missile campaign against twenty to thirty Iranian population centres, including Tehran. Under General Sultan Hashim Ahmad al-Tai and General Jamal Zanoun (both considered to be among Iraq's the most skilled commanders), the Iraqis launched air attacks against the Iranian positions and pinned them down. They then launched a pincer attack using mobile infantry and heavy artillery. Chemical weapons were used, and the Iraqis also flooded Iranian trenches with specially constructed pipes delivering water from the Tigris River.
The Iranians retreated back to the Hoveyzeh marshes while being attacked by helicopters, and the highway was recaptured by the Iraqis. Operation Badr resulted in 10,000–12,000 Iraqi casualties and 15,000 Iranian ones.
Strategic situation at the beginning of 1986.
The failure of the human wave attacks in earlier years had prompted Iran to develop a better working relationship between the Army and the Revolutionary Guard and to mould the Revolutionary Guard units into a more conventional fighting force. To combat Iraq's use of chemical weapons, Iran began producing an antidote. They also created and fielded their own homemade drones, the Mohajer 1's, fitted with six RPG-7's to launch attacks. They were primarily used in observation, being used for up to 700 sorties.
For the rest of 1986, and until the spring of 1988, the Iranian Air Force's efficiency in air defence increased, with weapons being repaired or replaced and new tactical methods being used. For example, the Iranians would loosely integrate their SAM sites and interceptors to create "killing fields" in which dozens of Iraqi planes were lost (which was reported in the West as the Iranian Air Force using F-14s as "mini-AWACs"). The Iraqi Air Force reacted by increasing the sophistication of its equipment, incorporating modern electronic countermeasure pods, decoys such as chaff and flare, and anti-radiation missiles. Due to the heavy losses in the last war of the cities, Iraq reduced their use of aerial attacks on Iranian cities. Instead, they would launch Scud missiles, which the Iranians could not stop. Since the range of the Scud missile was too short to reach Tehran, they converted them to al-Hussein missiles with the help of East German engineers, cutting up their Scuds into three chunks and attaching them together. Iran responded to these attacks by using their own Scud missiles.
Aside from extensive foreign help to Iraq, Iranian attacks were severely hampered by their shortages of weaponry, including heavy weaponry. Large portions of them had been lost during the last several years. Iran still managed to maintain 1,000 tanks (often by capturing Iraqi ones) and additional artillery, but many needed repairs to be operational. But by this time Iran managed to procure spare parts from various sources, helping them to restore some weapons. They secretly imported some weapons, such as RBS-70 anti-aircraft MANPADS. In an exception to the US's support for Iraq, in exchange for Iran using its influence to help free western hostages in Lebanon, the US secretly sold Iran some limited supplies (in the Ayatollah Rafsanjani's postwar interview, he stated that during the period when Iran was succeeding, for a short time the US supported Iran, then shortly after began helping Iraq again). Iran managed to get some advanced weapons, such as anti-tank TOW missiles, which worked better than rocket-propelled grenades. Iran later reverse-engineered and produced those weapons on their own as well. All of these almost certainly helped increase the effectiveness of Iran, although it did not reduce the human cost of their attacks.
First Battle of al-Faw.
On the night of 10–11 February 1986, the Iranians launched Operation Dawn 8, in which 30,000 troops comprising five Army divisions and men from the Revolutionary Guard and the Basij advanced in a two-pronged offensive to capture the al-Faw peninsula in southern Iraq, the only area touching the Persian Gulf. The capture of Al Faw and Umm Qasr was a major goal for Iran. Iran began with a feint attack against Basra, which was stopped by the Iraqis; Meanwhile, an amphibious strike force landed at the foot of the peninsula. The resistance, consisting of several thousand poorly trained soldiers of the Iraqi Popular Army, fled or were defeated, and the Iranian forces set up pontoon bridges crossing the Shatt al-Arab, allowing 30,000 soldiers to cross in a short period of time. They drove north along the peninsula almost unopposed, capturing it after only 24 hours of fighting.:240 Afterwards they dug in and set up defenses.
The sudden capture of al-Faw took the Iraqis by shock, since they had thought it impossible for the Iranians to cross the Shatt al-Arab. On 12 February 1986, the Iraqis began a counter-offensive to retake al-Faw, which failed after a week of heavy fighting. On 24 February 1986, Saddam sent one of his best commanders, General Maher Abd al-Rashid, and the Republican Guard to begin a new offensive to recapture al-Faw. A new round of heavy fighting took place. However, their attempts again ended in failure, costing them many tanks and aircraft: their 15th mechanised division was almost completely wiped out. The capture of al-Faw and the failure of the Iraqi counter-offensives were blows to the Ba'ath regime's prestige, and led the Gulf countries to fear that Iran might win the war. Kuwait in particular felt menaced with Iranian troops only 16 km away, and increased its support of Iraq accordingly.:241
In March 1986, the Iranians tried to follow up their success by attempting to take Umm Qasr, which would have completely severed Iraq from the Gulf and placed Iranian troops on the border with Kuwait. However, the offensive failed due to Iranian shortages of armor. By this time, 10,000 Iraqis and 30,000 Iranians were casualties. The First Battle of al-Faw ended in March, but heavy combat operations lasted on the peninsula into 1988, with neither side being able to displace the other. The battle bogged down into a World War I-style stalemate in the marshes of the peninsula. 53,000 Iraq troops and an unknown number of Iranian troops were killed.
Battle of Mehran.
Immediately after the Iranian capture of al-Faw, Saddam declared a new offensive against Iran, designed to drive deep into the state. The Iranian border city of Mehran, on the foot of the Zagros Mountains, was selected as the first target. On 15–19 May, Iraqi Army's Second Corps, supported by helicopter gunships, attacked and captured the city. Saddam then offered the Iranians to exchange Mehran for al-Faw. The Iranians rejected the offer. Iraq then continued the attack, attempting to push deeper into Iran. However, Iraq's attack was quickly warded off by Iranian AH-1 Cobra helicopters with TOW missiles, which destroyed numerous Iraqi tanks and vehicles.
The Iranians built up their forces on the heights surrounding Mehran. On 30 June, using mountain warfare tactics they launched their attack, recapturing the city by 3 July. Saddam ordered the Republican Guard to retake the city on 4 July, but their attack was ineffective. Iraqi losses were heavy enough to allow the Iranians to also capture territory inside Iraq, and depleted the Iraqi military enough to prevent them from launching a major offensive for the next two years. Iraq's defeats at al-Faw and at Mehran were severe blows to the prestige of the Iraqi regime, and western powers, including the U.S., became more determined to prevent an Iraqi loss.
Strategic situation at the end of 1986.
Through the eyes of international observers, Iran was prevailing in the war by the end of 1986. In the northern front, the Iranians began launching attacks toward the city of Suleimaniya with the help of Kurdish fighters, taking the Iraqis by surprise. They came within 16 km of the city before being stopped by chemical and army attacks. Iran's army had also reached the Meimak Hills, only 113 km from Baghdad. Iraq managed to contain Iran's offensives in the south, but was under serious pressure, as the Iranians were slowly overwhelming them.
Iraq responded by launching another "war of the cities". In one attack, Tehran's main oil refinery was hit, and in another instance, Iraq damaged Iran's Assadabad satellite dish, disrupting Iranian overseas telephone and telex service for almost two weeks. Civilian areas were also hit, resulting in many casualties. Iraq continued to attack oil tankers via air. Iran responded by launching Scud missiles and air attacks at Iraqi targets.
Iraq continued to attack Kharg Island and the oil tankers and facilities as well. Iran created a tanker shuttle service of 20 tankers to move oil from Kharg to Larak Island, escorted by Iranian fighter jets. Once moved to Larak, the oil would be moved to oceangoing tankers (usually neutral). They also rebuilt the oil terminals damaged by Iraqi air raids and moved shipping to Larak Island, while attacking foreign tankers that carried Iraqi oil (as Iran had blocked Iraq's access to the open sea with the capture of al-Faw). By now they almost always used the armed speedboats of the IRGC navy, and attacked many tankers. The tanker war escalated drastically, with attacks nearly doubling in 1986 (the majority carried out by Iraq). Iraq got permission from the Saudi government to use its airspace to attack Larak Island, although due to the distance attacks were less frequent there. The escalating tanker war in the Gulf became an ever increasing concern to foreign powers, especially the United States.
In April 1986, Ayatollah Khomeini issued a fatwa declaring that the war must be won by March 1987. The Iranians increased recruitment efforts, obtaining 650,000 volunteers. The animosity between the Army and the Revolutionary Guard arose again, with the Army wanting to use more refined, limited military attacks while the Revolutionary Guard wanted to carry out major offensives. Iran, confident in its successes, began planning their largest offensives of the war, which they called their "final offensives."
Iraq's Dynamic Defense Strategy.
Faced with their recent defeats in al-Faw and Mehran, Iraq appeared to be losing the war. Iraq's generals, angered by Saddam's interference, threatened a full-scale mutiny against the Ba'ath Party unless they were allowed to conduct operations freely. In one of the few times during his career, Saddam gave in to the demands of his generals. Up to this point, Iraqi strategy was to ride out Iranian attacks. However, the defeat at al-Faw led Saddam to declare the war to be "Al-Defa al Mutahharakkha" (The Dynamic Defense), and announcing that all civilians had to take part in the war effort. The universities were closed and all of the male students were drafted into the military. Civilians were instructed to clear marshlands to prevent Iranian amphibious infiltrations and to help build fixed defenses.
The government tried to integrate the Shias into the war effort by recruiting many as part of the Ba'ath Party. In an attempt to counterbalance the religious fervor of the Iranians and gain support from the devout masses, the regime also began to promote religion and, on the surface, Islamization, despite the fact that Iraq was run by a socialist regime. Scenes of Saddam praying and making pilgrimages to shrines became common on state-run television. While Iraqi morale had been low throughout the war, the attack on al-Faw raised patriotic fervor, as the Iraqis feared invasion. Saddam also recruited volunteers from other Arab countries into the Republican Guard, and received much technical support from foreign nations as well. While Iraqi military power had been depleted in recent battles, through heavy foreign purchases and support, they were able to expand their military even to much larger proportions by 1988.
At the same time, Saddam ordered the genocidal al-Anfal Campaign in an attempt to crush the Kurdish resistance, who were now allied with the Iranians. The result was the deaths of several hundred thousand Iraqi Kurds, and the destruction of villages, towns, and cities.
Iraq began to try to perfect their maneuver tactics. The Iraqis began to prioritize the professionalization of their military. Prior to 1986, the conscription-based Iraqi regular army and the volunteer-based Iraqi Popular Army conducted the bulk of the operations in the war, to little effect. The Republican Guard, formerly an elite praetorian guard, was expanded as a volunteer army and filled with Iraq's best generals. Loyalty to the state was no longer a primary requisite for joining. However, due to Saddam's paranoia, the former duties of the Republican Guard were transferred to a new unit, the Special Republican Guard. Full-scale war games against hypothetical Iranian positions were carried out in the western Iraqi desert against mock targets, and they were repeated over the course of a full year until the forces involved fully memorized their attacks. Iraq built its military massively, eventually possessing the 4th largest in the world, in order to overwhelm the Iranians through sheer size.
1987–88: Towards a ceasefire.
Meanwhile, as the Iraqis were planning their strike, the Iranians continued to attack. 1987 saw a renewed series of major Iranian human wave offensives in both northern and southern Iraq. The Iraqis had constructed heavy static fortifications around the city. They built 5 defensive rings, supported by natural waterways such as the Shatt-al-Arab, and manmade ones, such as "Fish Lake" and the Jasim River, along with manmade earth barriers. Fish Lake was a massive lake filled with mines, underwater barbed wire, electrodes, and various sensors. In addition, behind each waterway and defensive line was radar-guided artillery, ground attack aircraft, and combat helicopters; all capable of firing poison gas in addition to conventional munitions.
Iran's strategy was to penetrate through these massive defensive lines, and encircle Basra, cutting off the city as well as the Al-Faw peninsula from the rest of Iraq. The Iranians hoped that the capture of Basra would be such a major blow to Iraq that they would be forced to negotiate a settlement favorable to Iran. Iran's plan was for three assaults: a diversionary attack near Basra, the main offensive, and another diversionary attack using Iranian armor in the north to have Iraqi heavy armor diverted away from Basra. For these battles, Iran had re-expanded their military by recruiting many new Basij and Pasdaran volunteers. Iran brought 150,000–200,000 total troops into the battles.
Karbala Operations.
Operation Karbala-4.
On 25 December 1986, Iran launched Operation Karbala-4 ("Karbala" referring to Hussein ibn Ali's Battle of Karbala). According to General Ra'ad al-Hamdani, this was a diversionary attack. The Iranians launched an amphibious assault against the Iraqi island of Umm al-Rassas which lie in the Shatt-Al-Arab river parallel to Khoramshahr; they then set up a pontoon bridge and continued the attack, eventually capturing it after taking many casualties and failing to advance further; they had taken 60,000 casualties, while the Iraqis took 9,500. The Iraqi commanders exaggerated Iran's losses to Saddam, and it was assumed that the main Iranian attack on Basra had been fully defeated and that the Iranians were depleted for six months. Therefore, when the main Iranian attack, Operation Karbala 5 began, many Iraqi troops had gone on leave.
Operation Karbala-5 (Second Battle of Basra).
Operation Karbala-5 began midnight 8 January 1987, when a strike force of 35,000 Pasdaran crossed Fish Lake, while 4 Iranian divisions attacked at the southern end shore of the lake, overrunning the Iraqi forces and capturing Duaiji, an irrigation canal. They used their bridgehead at Duaiji as a springboard to recapture the Iranian town of Shalamcheh. Between 9–10 January, the Iranians broke through the first and second defense lines of Basra at the north of Fish Lake with tanks. The Iranians rapidly reinforced their forces with 60,000 troops and began to clear the remaining Iraqis in the area.
As early as 9 January, the Iraqis began their counterattack, supported by newer Su-25 and Mig-29 aircraft, by the 10th the Iraqis were throwing every available heavy weapon in a bid to eject the Iranians. Despite being outnumbered 10–1 in the air, Iran's air defense system downed many Iraqi aircraft (50–60 jets total; 10% of Iraq's air force), allowing Iran to provide close air support with their smaller air force, which also proved superior in dogfighting, causing the Iraqis to temporarily stop providing their troops air support. Iraqi tanks floundered in the marshland and were defeated by Cobra helicopters and TOW missile-equipped anti-tank commandos. Later in the battle, after their ground forces taking heavy losses due to the lack of air support, the Iraqi aircraft came back to the battlefield once again, facing their Iranian counterparts.
However, despite superior Iranian infantry tactics, it was the sheer size of the Iraqi defensive lines that prevented the Iranians from achieving a victory. On 19–24 January, Iran launched another major infantry offensive, breaking the third line and driving the Iraqis across the Jasim river. The battle became a contest of which side could bring more reinforcements. By 29 January, the Iranians launched a new attack from the west of the Jasim river, breaking through the fourth line. They were within 12 km of the city. At this point, the battle became a stalemate. Iranian TV broadcast footage of the outskirts of Basra, but the Iranians pushed no further. Iranian losses were so severe that Iraq took the offensive and pushed them back to their original positions. Despite that, the fighting continued, and 30,000 Iranians still held positions around Fish Lake. The battle bogged down into a trench war, where neither side could displace the other. Iran launched several more unsuccessful attacks. Karbala-5 officially ended by the end of February, but heavy combat operations continued, and Iran continued to besiege the city.
Among those killed was Iranian commander Hossein Kharrazi. Possibly 65,000 Iranians and 20,000 Iraqis were casualties because of Operation Karbala-5. Basra was largely destroyed, and Iraq's army had taken many material losses. The fighting during this operation was the heaviest and bloodiest during the war, with the area around Shalamcheh becoming known as the "Somme of the Iran-Iraq War". At one point, the situation had declined to the point that Saddam ordered several of his officers to be executed. With Iranian aircraft fighting at Basra, the Iraqis bombed Iranian supply routes with chemical weapons, as well as Iranian cities with conventional bombs, including Tehran, Isfahan, and Qom. It is believed that around 3,000 Iranian civilians were killed in these attacks. Iran retaliated by firing eleven long-range missiles further into Iraqi territory, inflicting heavy casualties among civilians and killing at least 300.
Operation Karbala-6.
At the same time as Operation Karbala 5, Iran also launched Operation Karbala-6 against the Iraqis in Qasr-e Shirin in central Iran to prevent the Iraqis from rapidly transferring units down to defend against the Karbala-5 attack. The attack was carried out by Basij infantry and the Revolutionary Guard's 31st "Ashura" and the Army's 77th "Khorasan" armored divisions. The Basij attacked the Iraqi lines, forcing the Iraqi infantry to retreat. An Iraqi armored counter-attack surrounded the Basij in a pincer movement, but the Iranian tank divisions attacked, breaking the encirclement. The Iranian attack was finally stopped by mass Iraqi chemical weapons attacks.
Iran's increasing war-weariness.
Operation Karbala-5 was a severe blow to Iran's military and morale. To foreign observers, it appeared that Iran was continuing to strengthen. By 1988, Iran had become self-sufficient in many areas, such as anti-tank TOW missiles, Scud ballistic missiles (Shahab-1), Silkworm anti-ship missiles, Oghab tactical rockets, and producing spare parts for their weaponry. Iran had also beefed up their air defenses with smuggled surface to air missiles.
Iran even was producing UAV's and the Pilatus PC-7 propellor aircraft for observation.
Iran also doubled their stocks of artillery, and was self-sufficient in manufacture of ammunition and small arms.
But, while it was not obvious to foreign observers, the Iranian public had become increasingly war-weary and disillusioned with the fighting, and relatively few volunteers joined the fight in 1987–88. Because the Iranian war effort relied on popular mobilization, their military strength actually declined, and Iran was unable to launch any major offensives after Karbala-5. As a result, for the first time since 1982, the momentum of the fighting shifted towards the regular army. Since the regular army was conscription based, it made the war even less popular. Many Iranians began to try to escape the conflict. As early as May 1985, anti-war demonstrations took place in 74 cities throughout Iran; however, they were crushed by the regime, resulting in some protesters being shot and killed. By 1987, draft-dodging had become a serious problem, and the Revolutionary Guards and police set up roadblocks throughout cities to capture those who tried to evade conscription. However, other people (including the more nationalistic and religious) as well as the clergy, the Revolutionary Guards, and the regular army wanted to continue the war to achieve their goals.
The leadership acknowledged that the war was a stalemate, and began to plan accordingly. There were no more "final offensives" planned. The head of the Supreme Defense Council Hashemi Rafsanjani announced during a news conference to finally end the use of human wave attacks. Mohsen Rezaee, head of the IRGC, announced that Iran would focus exclusively on limited attacks/infiltrations, while arming and supporting opposition groups inside of Iraq (such as the Kurds and Badr Brigade).
On the Iranian home front, the combination of sanctions, declining oil prices, and Iraqi attacks on Iranian oil facilities and shipping took a heavy toll on the economy. While the attacks themselves were not as destructive as some analysts believed, the US-led Operation Earnest Will (which protected Iraqi and allied oil tankers, but not Iranian ones) led many neutral countries to stop trading with Iran because of rising insurance and fear of air attack. Iranian oil and non-oil exports fell by 55%, inflation reached 50% by 1987, and the unemployment rate skyrocketed. At the same time, Iraq was experiencing crushing debt and shortages of workers, encouraging its leadership to try to end the war quicker.
Strategic Situation in late 1987.
By the end of 1987, Iraq possessed 5,550 tanks (outnumbering the Iranians five to one) and 900 fighter aircraft (outnumbering the Iranians ten to one). However, after Operation Karbala-5, Iraq only had 100 qualified fighter pilots remaining; therefore, Iraq began to invest in recruiting foreign pilots from countries such as Belgium, Australia, South Africa, both East and West Germany, and the Soviet Union. They replenished their manpower by integrating volunteers from other Arab countries into their army (for example, Iran eventually captured 3,000 Egyptian soldiers). Iraq also became self-sufficient in chemical weapons and some conventional ones and received much equipment from abroad. Foreign support helped Iraq bypass its economic troubles and massive debt to continue the war and increase the size of its military.
While the southern and central fronts were at a stalemate, Iran began to focus on carrying out offensives in northern Iraq with the help of the Peshmerga (Kurdish insurgents). The capture of Iraq's northern oil fields, and dams remained an important goal for Iran to force Iraq to negotiate, and with the help of the Peshmerga, there was a good chance it could succeed. The Iranians used a combination of semi-guerrilla and infiltration tactics in the Kurdish mountains with the Peshmerga. During Operation Karbala-9 in early April, Iran captured territory near Suleimaniya, provoking a severe poison gas counterattack. During Operation Karbala-10, Iran attacked near the same area, capturing more territory. During Operation Nasr-4, the Iranians surrounded the city of Suleimaniya, and with the help of the Peshmerga infiltrated over 140 kilometers into Iraq and raided and threatened to capture the oil-rich city of Kirkuk and other northern oilfields. Nasr-4 was considered to be Iran's most successful individual operation of the war. However, Iranian forces were unable to consolidate their gains and continue their advance, and while these offensives coupled with the Kurdish uprising sapped Iraqi strength, losses in the north would not mean a catastrophic failure for Iraq.
On 20 July, the UN Security Council passed the US-sponsored Resolution 598, which called for an end to the fighting and a return to pre-war boundaries. This resolution was noted by Iran for being the first resolution to call for a return to the pre-war borders, and setting up a commission to determine the aggressor and compensation. This was in contrast to previous settlements that only allowed a ceasefire (leaving Iraq in possession of disputed Iranian territories). The head of the IRGC, Mohsen Rezaee believed that it was a direct result of the capture of Al-Faw and Iran began to consider the ceasefire. While Iraq accepted the resolution, according to Iran's then foreign minister Ali Akbar Velayati, Iraq refused to set a timetable for withdrawing their troops from Iran, and thus Iran did not adopt the ceasefire yet, although they seriously considered it. The resolution was very ambiguous as well. Velayati stated that as a result, the western nations claimed that it was Iran that refused to accept peace.
Air and Tanker War in 1987.
With the stalemate on land, the air/tanker war began to play an increasingly major role in the conflict.
The Iranian air force had become very small, containing only 20 F-4 Phantoms, 20 F-5 Tigers, and 15 F-14 Tomcats in operation. Despite that, Iran managed to restore some damaged planes into service. The Iranian Air force, despite its once sophisticated equipment, lacked enough equipment and personnel to sustain the war of attrition that had arisen, and was unable to lead an outright onslaught against Iraq. The Iraqi Air Force, however, had originally lacked modern equipment and experienced pilots, but after pleas from Iraqi military leaders, Saddam decreased political influence on everyday operations and left the fighting to his combatants. In addition, the Soviets began delivering more advanced aircraft and weapons to Iraq, while the French improved training for flying crews and technical personnel and continually introduced new methods for countering Iranian weapons and tactics. However, at the same time, Iran's ground air defense downed many Iraqi aircraft.
The main Iraqi air effort had shifted to the destruction of Iranian war-fighting capability (primarily Persian Gulf oil fields, tankers, and Kharg Island), and starting late 1986 the Iraqi Air Force moved on a comprehensive campaign against the Iranian economic infrastructure. By late 1987, the Iraqi Air Force could count on direct American support for conducting long-range operations against Iranian infrastructural targets and oil installations deep in the Persian Gulf. U.S. Navy ships actively tracked and reported movements of Iranian shipping and defences. They supplied targeting information on several occasions in February and March 1988; when they failed to warn Iraqi aircraft of Iranian interceptors' presence, the Iraqis suffered considerable losses. The massive Iraqi air strike against Kharg Island, flown on 18 March 1988, was one such occasion: the Iraqis destroyed two supertankers but lost five aircraft to Iranian F-14 Tomcats, including two Tupolev Tu-22Bs and one Mikoyan MiG-25RB. The U.S. Navy was now becoming more involved in the fight in the Persian Gulf, launching Operations Earnest Will and Prime Chance against the Iranians.
The attacks on oil tankers continued. Both Iran and Iraq carried frequent attacks during the first four months of the year. Iran was effectively waging a naval guerrilla war with its IRGC navy speedboats, while Iraq attacked with its aircraft. In 1987, Kuwait asked to reflag its tankers to the US flag. They did so in March, and the US navy began Operation Earnest Will to escort the tankers. The result of Earnest Will would be that while oil tankers shipping Iraqi/Kuwaiti oil were protected, Iranian tankers, and neutral tankers shipping to Iran would be unprotected, resulting in both losses for Iran and the undermining of its trade with foreign countries, damaging Iran's economy further. Iran also deployed Silkworm missiles to attack some ships, but only a few were actually fired. Both the US and Iran jockeyed for influence in the Gulf. To discourage the US from escorting tankers, Iran secretly mined some areas in the Gulf. The US began to escort the reflagged tankers, but one of them was destroyed by a mine while under escort. While being a public-relations victory for Iran, the US increased its reflagging efforts. While Iran mined the Persian Gulf, their speedboat attacks were reduced, primarily attacking unflagged tankers shipping in the area.
On 24 September, US Navy SEALS captured the Iranian mine-laying ship Iran Ajr, a diplomatic disaster for the already isolated Iranians. On 8 October, the US Navy destroyed four Iranian speedboats, and in response to Iranian Silkworm missile attacks on Kuwaiti oil tankers, launched Operation Nimble Archer, destroying two Iranian oil rigs in the Persian Gulf. During November and December, the Iraqi air force launched a bid to destroy all Iranian airbases in Khuzestan and the remaining Iranian air force. However, Iran managed to shoot down 30 Iraqi fighters with fighter jets, anti-aircraft guns, and missiles, allowing the Iranian air force to survive to the end of the war.
On 28 June, Iraqi fighter bombers attacked the Iranian town of Sardasht near the border, using chemical mustard gas bombs. While many towns and cities had been bombed before, and troops attacked with gas, this was the first time that the Iraqis had attacked a civilian area with poison gas. One quarter of the town's then population of 20,000 was burned and stricken, and 113 were killed immediately, with many more dying and suffering health effects over the next decades. Saddam ordered the attack in order to test the effects of the newly developed "dusty mustard" gas, which was designed to be even more crippling than traditional mustard gas. While little known outside of Iran (unlike the later Halabja chemical attack), the Sardasht bombing (and future similar attacks) had a tremendous effect on the Iranian people's psyche.
1988: Iraqi offensives and the UN ceasefire.
By 1988, with massive equipment imports and reduced Iranian volunteers, Iraq was ready to launch major offensives against Iran. On February 1988, Saddam began the fifth and most deadly "war of the cities". Over the next two months, Iraq launched over 200 al-Hussein missiles at 37 Iranian cities. Saddam also threatened to use chemical weapons in his missiles, which caused 30% of Tehran's population to leave the city. Iran retaliated, launching at least 104 missiles against Iraq in 1988 and shelling Basra. This event was nicknamed the "Scud Duel" in the foreign media. In all, Iraq launched 520 Scuds and al-Husseins against Iran and Iran fired 177 at them. However, the Iranian attacks were too few in number to deter Iraq from launching their attacks. Iraq also increased their airstrikes against Kharg Island and Iranian oil tankers. With their allies tankers protected by US warships, they could operate with virtual impunity. To make matters worse, the West supplied Iraq's air force with laser-guided smart bombs, allowing them to attack economic targets while evading anti-aircraft defenses. These attacks began to have a major toll on the Iranian economy, morale, and caused many casualties as well.
Iran's Kurdistan Operations.
In March 1988, the Iranians carried out Operation Dawn 10, Operation "Beit-ol-Moqaddas" 2, and Operation "Zafar" 7 (Victory 7) in Iraqi Kurdistan with the aim of capturing the Darbandikhan Dam and the power plant at Lake Dukan, which supplied Iraq with much of its electricity and water, as well as the city of Suleimaniya.:264 Iran hoped that the capture of these areas would bring more favorable terms to the ceasefire agreement. This infiltration offensive was carried out in conjunction with the Peshmerga. Iranian airborne commandos landed behind the Iraqi lines and Iranian helicopters hit Iraqi tanks with TOW missiles. The Iraqis were taken by surprise, and Iranian F-5E Tiger fighter jets even damaged the Kirkuk oil refinery. Iraq carried out executions of multiple officers for these failures in March–April 1988, including Colonel Jafar Sadeq. The Iranians used infiltration tactics in the Kurdish mountains, captured the town of Halabja and began to fan out across the province.
Though the Iranians advanced to within sight of Dukan, and captured around 1040 km2 and 4,000 Iraqi troops, the offensive failed due to the Iraqi use of chemical warfare.:264 The Iraqis launched the deadliest chemical weapons attacks of the war. The Republican Guard launched 700 chemical shells, while the other artillery divisions launched 200–300 chemical shells each, unleashing a chemical cloud over the Iranians, killing or wounding 60% of them, the blow was felt particurarly by the Iranian 84th infantry division and 55th paratrooper division. The Iraqi special forces then stopped the remains of the Iranian force. In retaliation for Kurdish collaboration with the Iranians, Iraq launched a massive poison gas attack against Kurdish civilians in Halabja, recently taken by the Iranians, killing thousands of civilians. Iran airlifted foreign journalists to the ruined city, and the images of the dead were shown throughout the world. However, Western mistrust of Iran and collaboration with Iraq led them to also blame Iran for the attack. At one point, the United States claimed that Iran had launched the attack and then tried to blame Iraq for it.
Second Battle of al-Faw.
On 17 April 1988, Iraq launched Operation Ramadan "Mubarak" (Blessed Ramadan), a surprise attack against the 15,000 Basij troops on the peninsula. The attack on al-Faw was preceded by Iraqi diversionary attacks in northern Iraq, with a massive artillery and air barrage of Iranian front lines. Key areas, such as supply lines, command posts, and ammunition depots, were hit by a storm of mustard gas and nerve gas, as well as by conventional explosives. Helicopters landed Iraqi commandos behind Iranian lines while the main Iraqi force attacked in a frontal assault. Within 48 hours, all of the Iranian forces had been killed or cleared from the al-Faw Peninsula. The day was celebrated in Iraq as Faw Liberation Day throughout Saddam's rule. The Iraqis had planned the offensive well. Prior to the attack the Iraqi soldiers gave themselves poison gas antidotes to shield themselves from the effect of the saturation of gas. The heavy and well executed use of chemical weapons was the decisive factor in the Iraqi victory. Iraqi losses were relatively light, especially compared to Iran's casualties. The Iranians eventually managed to halt the Iraqi drive as they pushed towards Khuzestan.
To the shock of the Iranians, rather than breaking off the offensive, the Iraqis kept up their drive, and a new force attacked the Iranian positions around Basra. Following this, the Iraqis launched a sustained drive to clear the Iranians out of all of southern Iraq.:264
One of the most successful Iraqi tactics was the "one-two punch" attack using chemical weapons. Using artillery, they would saturate the Iranian front line with rapidly dispersing cyanide and nerve gas, while longer-lasting mustard gas was launched via fighter-bombers and rockets against the Iranian rear, creating a "chemical wall" that blocked reinforcement.
Operation Praying Mantis.
The same day as Iraq's attack on al-Faw peninsula, the United States Navy launched Operation Praying Mantis in retaliation against Iran for damaging a warship with a mine. Iran lost oil platforms, destroyers, and frigates in this battle, which ended only when President Reagan decided that the Iranian navy had been put down enough. In spite of this, the Revolutionary Guard's navy continued their speedboat attacks against oil tankers. However, the combined defeats at al-Faw and in the Persian Gulf nudged Iranian leadership towards quitting the war, especially when facing the prospect of fighting the Americans.
Iranian counteroffensive.
Faced with such losses, Khomeini appointed the cleric Hashemi Rafsanjani as the Supreme Commander of the Armed Forces, though he had in actuality occupied that position for months. Rafsanjani ordered a surprise counter-attack into Iraq, which was launched 13 June 1988. The Iranians infiltrated through the Iraqi trenches and moved 10 km into Iraq, and managed to strike Saddam's presidential palace in Baghdad using fighter aircraft. After 10 hours of fighting, the decimated Iranians were driven back to their original positions again as the Iraqis launched 650 helicopter and 300 aircraft sorties.
Operation Forty Stars.
On 18 June, Iraq launched Operation Forty Stars (چل چراغ "chehel cheragh") in conjunction to the Mujahideen-e-Khalq (MEK) around Mehran. With 530 aircraft sorties and heavy use of nerve gas, they crushed the Iranian forces in the area, killing 3,500, and nearly destroying a Revolutionary Guard division. Mehran was captured once again and occupied by the MEK. Iraq also launched air raids on Iranian population centers and economic targets, setting 10 oil installations on fire.
Tawakalna ala Allah Operations.
On 25 May 1988, Iraq launched the first of four "Tawakalna ala Allah" (Trust in God) Operations, consisting of one of the largest artillery barrages in history, coupled with chemical weapons. The marshes had been dried by drought, allowing the Iraqis to use tanks to bypass Iranian field fortifications, expelling the Iranians from the border town of Shalamcheh after less than 10 hours of combat.:11:265
On 25 June, Iraq launched the second Tawakal ala Allah operation against the Iranians on Majnoon Island. Iraqi commandos used amphibious craft to block the Iranian rear, then used hundreds of tanks with heavy conventional and chemical artillery barrages to recapture the island after 8 hours of combat. Saddam appeared live on Iraqi television to "lead" the charge against the Iranians. The majority of the Iranian defenders were killed during the quick assault. The final two Tawakal ala Allah operations took place near al-Amarah and Khaneqan. By 12 July, the Iraqis had captured the city of Dehloran, 30 km inside Iran, along with 2,500 troops and much armour and material, which took four days to transport to Iraq. These losses included more than 570 of the 1,000 remaining Iranian tanks, over 430 armored vehicles, 45 self-propelled artillery, 620 towed artillery and antiaircraft guns. The Iraqis withdrew from Dehloran soon after, claiming that they had "no desire to conquer Iranian territory." Historian Kaveh Farrokh considered this to be Iran's greatest military disaster during the war. Pelletier notes that "Tawakal ala Allah … resulted in the absolute destruction of Iran’s military machine."
During the 1988 battles, the Iranians put up little resistance to the Iraqi offensives, having been worn out by nearly eight years of war.:253 They lost large amounts of equipment; however, they managed to rescue most of their troops from being captured by the Iraqis, leaving Iraq with relatively few prisoners. On 2 July, Iran belatedly set up a joint central command which unified the Revolutionary Guard, Army, and Kurdish rebels, and dispelled the rivalry between the Army and the Revolutionary Guard. However, this came too late, and Iran was believed to have fewer than 200 remaining tanks on the southern front, faced against thousands of Iraqi ones. The only area where the Iranians were not suffering major defeats was in Kurdistan.
Iran accepts the ceasefire.
Saddam sent a warning to Khomeini in mid-1988, threatening to launch a full-scale invasion and attack Iranian cities with weapons of mass destruction. Shortly afterwards, Iraqi aircraft bombed the Iranian town of Oshnavieh with poison gas, immediately killing and wounding over 2,000 civilians. The fear of an all out chemical attack against Iran's largely unprotected civilian population weighed heavily on the Iranian leadership, and they realized that the international community had no intention of restraining Iraq. The lives of the civilian population of Iran were becoming very disrupted, with a third of the urban population evacuating major cities in fear of the seemingly imminent chemical war. Meanwhile, Iraqi conventional bombs and missiles continuously hit towns and cities as well as destroyed vital civilian and military infrastructure, and the death toll increased. Iran did reply with missile and air attacks as well, but not enough to deter the Iraqis from attacking.
Under the threat of a new and even more powerful invasion, Commander-in-Chief Akbar Rafsanjani ordered the Iranians to retreat from Haj Omran, Kurdistan on 14 July. The Iranians did not publicly describe this as a retreat, instead called it a "temporary withdrawal". By July, Iran's army inside Iraq (except Kurdistan) had largely disintegrated. Iraq put up a massive display of captured Iranian weapons in Baghdad, claiming they "captured" 1,298 tanks, 5,550 recoil-less rifles, and thousands of other weapons. However, Iraq had taken heavy losses as well, and the battles were very costly.
On July 1988, Iraqi aeroplanes dropped cyanide bombs on the Iranian Kurdish village of Zardan (as they had done four months earlier on their own Kurdish village of Halabja). Dozens of villages, and some larger towns, such as Marivan, were attacked with poison gas, resulting in even heavier civilian casualties. About the same time, the USS "Vincennes" shot down Iran Air Flight 655, killing 290 passengers. The lack of international sympathy disturbed the Iranian leadership, and they came to the conclusion that the United States was on the verge of waging a full-scale war against them, and that Iraq was on the verge of unleashing its entire chemical arsenal upon their major cities.
At this point, elements of the Iranian leadership, led by Ali Akbar Hashemi Rafsanjani (who had pushed for the war initially), persuaded Khomeini to accept the UN ceasefire. They stated that in order to win the war, Iran's military budget would have to be increased by 700% and the war would last until 1993. Many officials also pointed out to Khomeini that while Resolution 598 would not give all of the gains that Iran wanted, it was better than anything else that had been proposed before, and likely would be proposed afterward, considering that it provided a return to the pre-war status quo, rather than having Iraq occupy Iranian border areas like previous agreements. While many parts were ambiguous, it could be negotiated with Iraq.
On 20 July 1988, Iran accepted Resolution 598, showing its willingness to accept a ceasefire.:11 A statement from Khomeini was read out in a radio address, and he expressed deep displeasure and reluctance about accepting the ceasefire:Happy are those who have departed through martyrdom. Happy are those who have lost their lives in this convoy of light. Unhappy am I that I still survive and have drunk the poisoned chalice...:1
The news of the end of the war was greeted with celebration in Baghdad, with people dancing in the streets; in Tehran, however, the end of the war was greeted with a somber mood.:1
Operation Mersad and end of the war.
Operation "Mersad" (مرصاد "ambush") was the last major military operation of the war. Both Iran and Iraq had accepted Resolution 598. But despite the ceasefire, after seeing Iraqi victories in the previous months, MEK decided to launch an attack of its own and wished to advance all the way to Teheran. Saddam and the Iraqi high command decided on a two pronged offensive across the border: central Iran, and Iranian Kurdistan. Shortly after Iran accepted the ceasefire the MEK army began its offensive, attacking into Ilam province under cover of Iraqi air power.
In the north, Iraq also launched an attack into Iraqi Kurdistan, which was blunted by the Iranians.
On 26 July 1988, the Mujahadeen-e-Khalq (MEK), with the support of the Iraqi army, started their campaign, Operation "Forough Javidan" (Eternal Light) in central Iran. The MEK supported by Iraq attacked western Iran, advancing towards Kermanshah. The Iranians had withdrawn their remaining soldiers to Khuzestan in fear of a new Iraqi invasion attempt, and as a result the Mujahedeen advanced rapidly, seizing Qasr-e Shirin, Sarpol-e Zahab, Kerend-e Gharb, and Islamabad-e-Gharb, and towards Kermanshah. The MEK expected the Iranian population to rise up and support their advance; however, the uprising never materialised, but they reached 145 km deep into Iran. In response, the Iranian military launched its counter-attack, Operation Mersad, under Lieutenant General Ali Sayyad Shirazi. Iranian paratroopers landed behind the MEK lines while the Iranian Air Force and helicopters launched an air attack, destroying much of the enemy columns. The Iranians defeated the MEK in the city of Kerend-e Gharb on 29 July 1988. On 31 July, Iran drove the MEK out of Qasr-e-Shirin and Sarpol Zahab, though MEK claimed to have "voluntarily withdrawn" from the towns. Iran estimated that 4,500 MEK were killed, while 400 Iranian soldiers died.
The last notable combat actions of the war took place on 3 August 1988, in the Persian Gulf when the Iranian navy fired on a freighter and Iraq launched chemical attacks on Iranian civilians, killing an unknown number of them and wounding 2,300.
Iraq came under heavy international pressure to end the war. Resolution 598 became effective on 8 August 1988, ending all combat operations between the two countries. By 20 August 1988, peace with Iran was restored. UN peacekeepers belonging to the UNIIMOG mission took the field, remaining on the Iran–Iraq border until 1991. The majority of Western analysts believe that the war had no winners while some believed that Iraq emerged as the victor of the war, based on Iraq’s overwhelming successes between April and July 1988. While the war was now over, Iraq spent the rest of August and early September clearing the Kurdish resistance. Using 60,000 troops along with helicopter gunships, chemical weapons (poison gas), and mass executions, Iraq hit 15 villages, killing rebels and civilians, and forced tens of thousands of Kurds to relocate to settlements. Many Kurdish civilians immigrated to Iran. By 3 September 1988, the anti-Kurd campaign ended, and all resistance had been crushed. 400 Iraqi soldiers and 50,000 Kurdish civilians and soldiers had been killed.
Aftermath.
The Iran–Iraq War was the deadliest conventional war ever fought between regular armies of developing countries. Iraqi casualties are estimated at 105,000–200,000 killed, while about 400,000 had been wounded and some 70,000 taken prisoner. Thousands of civilians on both sides died in air raids and ballistic missile attacks. Prisoners taken by both countries began to be released in 1990, though some were not released until more than 10 years after the end of the conflict. Cities on both sides had also been considerably damaged. While revolutionary Iran had been bloodied, Iraq was left with a large military and was a regional power, albeit with severe debt, financial problems, and labor shortages.
According to Iranian government sources, the war cost Iran an estimated 200,000–220,000 killed, or up to 262,000 according to the conservative Western estimates. This includes 123,220 combatants, 60,711 MIA and 11,000-16,000 civilians. Combatants include 79,664 members of the Revolutionary Guard Corps and additional 35,170 soldiers from regular military. In addition, prisoners of war comprise 42,875 Iranian casualties, they were captured and kept in Iraqi detention centers from 2.5 to more than 15 years after the war was over. According to the Janbazan Affairs Organization, 398,587 Iranians sustained injuries that required prolonged medical and health care following primary treatment, including 52,195 (13%) injured due to the exposure to chemical warfare agents. From 1980 to 2012, 218,867 Iranians died due to war injuries and mean age of combatants was 23 years old. This includes 33,430 civilians, mostly women and children. More than 144,000 Iranian children were orphaned as a consequence of these deaths. Other estimates put Iranian casualties up to 600,000.
Both Iraq and Iran manipulated loss figures to suit their purposes. At the same time, Western analysts accepted improbable estimates. By April 1988, such casualties were estimated at between 150,000 to 340,000 Iraqis dead, and 450,000 to 730,000 Iranians. Shortly after the end of the war, it was thought that Iran suffered even more than a million dead. Considering the style of fighting on the ground and the fact that neither side penetrated deeply into the other's territory, USMC analysts believe events do not substantiate the high casualties claimed. Iraqi government has claimed 800,000 Iranians were killed in conflict, four times more than Iranian official figures. Iraqi losses were also revised downwards over time.
Peace Talks and Postwar Situation.
With the ceasefire in place, and UN peacekeepers monitoring the border, Iran and Iraq sent their representatives to Geneva, Switzerland, to negotiate a peace agreement on the terms of the ceasefire. However, peace talks stalled. Iraq, in violation of the UN ceasefire, refused to withdraw its troops from 3000 sqmi of Iranian territory unless the Iranians accepted Iraq's full sovereignty over the Shatt al-Arab waterway (as Iran had feared in 1982). Foreign powers continued to support Iraq, which wanted to gain at the negotiating table what they failed to achieve on the battlefield, and Iran was portrayed as the one who was not wanting peace. Iran, in response, refused to release 70,000 Iraqi prisoners of war (twice as many compared to Iranian prisoners of war in Iraq). They also continued to carry out a naval blockade of Iraq, although its effects were mitigated by Iraqi trade with its Arab neighbors. Iran also began to improve relations with many of the states that opposed it during the war. Because of Iranian actions, by 1990, Saddam had become more conciliatory, and in a letter to the now President Rafsanjani, he became more open to the idea of a peace agreement, although he still insisted on full sovereignty over the Shatt al-Arab.
By 1990, Iran was undergoing military rearmament and reorganization, purchasing from the USSR and China $10 billion worth of heavy weaponry, including aircraft, tanks, and missiles. Rafsanjani reversed Iran's self-imposed ban on chemical weapons, and ordered the manufacture and stockpile of them (in 1993, Iran ratified the Chemical Weapons Convention, and subsequently destroyed them). Saddam realized that if Iran attempted to expel the Iraqis from their territory, it was likely they would succeed. As war with the western powers loomed, Iraq became concerned about Iran mending their relations with the west in order to attack Iraq. Iraq had lost their support from the West, and their position in Iran was increasingly untenable. Shortly after his invasion of Kuwait, Saddam wrote a letter to Rafsanjani stating that Iraq recognised Iranian rights over the eastern half of the Shatt al-Arab, a reversion to the "status quo ante bellum" that he had repudiated a decade earlier, and that he would accept Iran's demands and withdraw Iraq's military from the disputed territories at the border. A peace agreement was signed finalizing the terms of the UN resolution, diplomatic relations was restored, and by late 1990-early 1991, the Iraqi military withdrew from the disputed territories. The UN peacekeepers withdrew from the border shortly afterward. Most of the prisoners of war were released in 1990, although some remained as late as 2003. Iranian politicians declared it to be the "greatest victory in the history of the Islamic Republic of Iran".
Most historians and analysts consider the war to be a stalemate. Certain analysts believe that Iraq won, on the basis of the success of their 1988 offensives which thwarted Iran's major territorial ambitions in Iraq and persuaded Iran to accept the ceasefire. Iranian analysts believe that they won the war because although they did not succeed in defeating Iraq militarily, they did manage to gain their political goals of driving Iraq entirely from their territory (which was an important purpose of the post 1982 invasion of Iraq, to force the Iraqis to negotiate a withdrawal from Iran's border areas). They also cite the fact that Iran achieved its goals against Iraq's superior military, they thwarted Iraq's major territorial ambitions in Iran, and that, 2 years after the war had ended, Iraq permanently gave up its claims to the Shatt al-Arab as well.
On 9 December 1991, Javier Pérez de Cuéllar, UN Secretary General at the time, reported that Iraq's initiation of the war was unjustified, as was its occupation of Iranian territory and use of chemical weapons against civilians:
That [Iraq's] explanations do not appear sufficient or acceptable to the international community is a fact...[the attack] cannot be justified under the charter of the United Nations, any recognized rules and principles of international law, or any principles of international morality, and entails the responsibility for conflict. Even if before the outbreak of the conflict there had been some encroachment by Iran on Iraqi territory, such encroachment did not justify Iraq's aggression against Iran—which was followed by Iraq's continuous occupation of Iranian territory during the conflict—in violation of the prohibition of the use of force, which is regarded as one of the rules of jus cogens...On one occasion I had to note with deep regret the experts' conclusion that "chemical weapons ha[d] been used against Iranian civilians in an area adjacent to an urban center lacking any protection against that kind of attack".
He also stated that had the UN accepted this fact earlier, the war would have almost certainly not lasted as long as it did. Iran, encouraged by the announcement, sought reparations from Iraq, but never received any.
Throughout the 1990s and early 2000s, Iran and Iraq relations remained at a limbo between a cold war and a cold peace. Despite renewed and somewhat thawed relations, both sides continued to have low level conflicts with each other. Iraq continued to host and support the Mujahedeen-e-Khalq, which carried out multiple attacks throughout Iran up until the 2003 invasion of Iraq (including the assassination of Iranian general Ali Sayyad Shirazi in 1998, cross border raids, and mortar attacks). Iran carried out several airstrikes and missile attacks against Mujahedeen targets inside of Iraq (the largest taking place in 2001, when Iran fired 56 Scud missiles at Mujahedeen targets). In addition, according to General Hamdani, Iran continued to carry out low-level infiltrations of Iraqi territory, using Iraqi dissidents and anti-government activists rather than Iranian troops, in order to incite revolts. After the fall of Saddam in 2003, Hamdani claimed that Iranian agents infiltrated and created numerous militias in Iraq and built an intelligence system operating within the country.
In 2005, the new government of Iraq apologised to Iran for starting the war. The Iraqi government also commemorated the war with various monuments, including the Hands of Victory and the al-Shaheed Monument, both in Baghdad. The war also helped to create a forerunner for the Coalition of the Gulf War, when the Gulf Arab states banded together early in the war to form the Gulf Cooperation Council to help Iraq fight Iran.
With the 2003 invasion of Iraq and Iran's involvement in Iraq's new government and backing of proxy militias, many observers believe that Iran has effectively gained influence over Iraq.
Financial situation.
The financial loss at the time was believed to exceed US$500 billion for each country ($1.2 trillion total). In addition, economic development stalled and oil exports were disrupted. Iran, having used bloodier but economically cheaper tactics during the war, only incurred a small debt, in contrast to the large ones incurred by Iraq. Iraq had accrued more than $130 billion of international debt, excluding interest, and was also weighed down by a slowed GDP growth. Iraq's debt to Paris Club amounted to $21 billion, 85% of which had originated from the combined inputs of Japan, the USSR, France, Germany, the United States, Italy and the United Kingdom. The largest portion of Iraq's debt, amounting to $130 billion, was to its former Arab backers, with $67 billion loaned by Kuwait, Saudi Arabia, Qatar, UAE, and Jordan.
After the war, Iraq accused Kuwait of slant drilling and stealing oil, inciting its invasion of Kuwait, which in turn worsened Iraq's financial situation: the United Nations Compensation Commission mandated Iraq to pay reparations of more than $200 billion to victims of the invasion, including Kuwait and the United States. To enforce payment Iraq was put under a complete international embargo, which put further strain on the Iraqi economy and pushed its external debt and international liabilities to private and public sectors to more than $500 billion by the end of Saddam's rule. Combined with Iraq's negative economic growth after prolonged international sanctions, this produced a debt-to-GDP ratio of more than 1,000%, making Iraq the most indebted developing country in the world. The unsustainable economic situation compelled the new Iraqi government to request that a considerable portion of debt incurred during the Iran–Iraq war be written off. Consequently the effects of the Iran–Iraq War led to the Iraqi invasion of Kuwait and the subsequent Persian Gulf War two years later.
Much of the oil industry in both countries was damaged in air raids. 10 million shells had landed in Iraq's oil fields at Basra, seriously damaging their oil production. Iraq's production capacity has yet to fully recover from the damages of the war.
Science and technology.
The war had a marked effect on the scientific and technological advancement of the involved countries: Iraq's productivity in the field collapsed and has not yet recovered, and Kuwait's scientific output was initially slowed and later became stagnant.
The war had its impact on medical science: a surgical intervention for comatose patients with penetrating brain injuries was created by Iranian physicians treating wounded soldiers, later establishing neurosurgery guidelines to treat civilians who had suffered blunt or penetrating skull injuries. Iranian physicians' experience in the war reportedly helped U.S. congresswoman Gabrielle Giffords recover after the 2011 Tucson shooting.
In addition to helping trigger the Persian Gulf War, the Iran–Iraq War also contributed to Iraq's defeat in the Persian Gulf War. Iraq's military was accustomed to fighting the slow moving Iranian infantry formations with artillery and static defenses, while using mostly unsophisticated tanks to gun down and shell the infantry and overwhelm the smaller Iranian tank force; in addition to being dependent on weapons of mass destruction to help secure victories. Therefore, they were rapidly overwhelmed by the high-tech, quick-maneuvering US forces using modern doctrines such as AirLand Battle.
Home front.
Iraq.
At first, Saddam attempted to ensure that the Iraqi population suffered from the war as little as possible. There was rationing, but civilian projects begun before the war continued. At the same time, the already extensive personality cult around Saddam reached new heights of adulation while the regime tightened its control over the military.
After the Iranian victories of the spring of 1982 and the Syrian closure of Iraq's main pipeline, Saddam did a volte-face on his policy towards the home front: a policy of austerity and total war was introduced, with the entire population being mobilised for the war effort. All Iraqis were ordered to donate blood and around 100,000 Iraqi civilians were ordered to clear the reeds in the southern marshes. Mass demonstrations of loyalty towards Saddam became more common. Saddam also began implementing a policy of discrimination against Iraqis of Iranian origin.
In the summer of 1982, Saddam began a campaign of terror. More than 300 Iraqi Army officers were executed for their failures on the battlefield. In 1983, a major crackdown was launched on the leadership of the Shia community. Ninety members of the al-Hakim family, an influential family of Shia clerics whose leading members were the émigrés Mohammad Baqir al-Hakim and Abdul Aziz al-Hakim, were arrested, and 6 were hanged. The crackdown on Kurds saw 8,000 members of the Barzani clan, whose leader (Massoud Barzani) also led the Kurdistan Democratic Party, summarily executed. From 1983 onwards, a campaign of increasingly brutal repression was started against the Iraqi Kurds, characterised by Israeli historian Efraim Karsh as having "assumed genocidal proportions" by 1988. The al-Anfal Campaign was intended to "pacify" Iraqi Kurdistan permanently.
Gaining civilian support.
To secure the loyalty of the Shia population, Saddam allowed more Shias into the Ba'ath Party and the government, and improved Shia living standards, which had been lower than those of the Iraqi Sunnis. Saddam had the state pay for restoring Imam Ali's tomb with white marble imported from Italy. The Baathists also increased their policies of repression against the Shia. The most infamous event was the massacre of 148 civilians of the Shia town of Dujail.
Despite the costs of the war, the Iraqi regime made generous contributions to Shia "waqf" (religious endowments) as part of the price of buying Iraqi Shia support.:75–76 The importance of winning Shia support was such that welfare services in Shia areas were expanded during a time in which the Iraqi regime was pursuing austerity in all other non-military fields.:76 During the first years of the war in the early 1980s, the Iraqi government tried to accommodate the Kurds in order to focus on the war against Iran. In 1983, the Patriotic Union of Kurdistan agreed to cooperate with Baghdad, but the Kurdistan Democratic Party (KDP) remained opposed. In 1983, Saddam signed an autonomy agreement with Jalal Talabani of the Patriotic Union of Kurdistan (PUK), though Saddam later reneged on the agreement. By 1985, the PUK and KDP had joined forces, and Iraqi Kurdistan saw widespread guerrilla warfare up to the end of the war.
Iran.
The Iranian government saw the outbreak of war as chance to strengthen its position and consolidate the Islamic revolution: the war was presented to the Iranian people as a glorious "jihad" and a test of Iranian national character. The Iranian regime followed a policy of total war from the beginning, and attempted to mobilise the nation as a whole. They established a group known as the Reconstruction Campaign, whose members were exempted from conscription and were instead sent into the countryside to work on farms to replace the men serving at the front.
Iranian workers had a day's pay deducted from their pay cheques every month to help finance the war, and mass campaigns were launched to encourage the public to donate food, money, and blood for the soldiers. To further help finance the war, the Iranian government banned the import of all non-essential items, and launched a major effort to rebuild the damaged oil plants.
Civil unrest.
In June 1981, street battles broke out between the Revolutionary Guard and the left-wing Mujaheddin e-Khalq (MEK), continuing for several days and killing hundreds on both sides.:250 In September, more unrest broke out on the streets of Iran as the MEK attempted to seize power. Thousands of left-wing Iranians (many of whom were not associated with the MEK) were shot and hanged by the government in the aftermath.:251 The MEK began an assassination campaign that killed hundreds of regime officials by the fall of 1981.:251 On 28 June 1981, they assassinated the secretary-general of the Islamic Republican Party, Mohammad Beheshti and on 30 August, killed Iran's president, Mohammad-Ali Rajai.:251 The government responded with mass executions of suspected MEK members, a practice that lasted until 1985.
In addition to the open civil conflict with the MEK, the Iranian government was faced with Iraqi-supported rebellions in Iranian Kurdistan, which were gradually put down through a campaign of systematic repression. 1985 also saw student anti-war demonstrations, which were crushed by government activists.
Economy.
The war furthered the decline of the Iranian economy that had begun with the revolution in 1978–79. Between 1979 and 1981, foreign exchange reserves fell from US$14.6 billion to $1 billion. As a result of the war, living standards dropped dramatically,:252 and Iran was described by British journalists John Bulloch and Harvey Morris as "a dour and joyless place" ruled by a harsh regime that "seemed to have nothing to offer but endless war.":239 Though Iran was becoming bankrupt, Khomeini interpreted Islam's prohibition of usury to mean they could not borrow against future oil revenues to meet war expenses. As a result, Iran funded the war by the income from oil exports after cash had run out. The revenue from oil dropped from $20 billion in 1982 to $5 billion in 1988.:252
In January 1985, former prime minister and anti-war Islamic Liberation Movement co-founder Mehdi Bazargan criticised the war in a telegram to the United Nations, calling it un-Islamic and illegitimate and arguing that Khomeini should have accepted Saddam's truce offer in 1982 instead of attempting to overthrow the Ba'ath. He added, "Since 1986, you have not stopped proclaiming victory, and now you are calling upon population to resist until victory. Is that not an admission of failure on your part?":252 Khomeini was annoyed by Bazargan's telegram, and issued a lengthy public rebuttal in which he defended the war as both Islamic and just.
By 1987, Iranian morale had begun to crumble, reflected in the failure of government campaigns to recruit "martyrs" for the front. Israeli historian Efraim Karsh points to the decline in morale in 1987–88 as being a major factor in Iran's decision to accept the ceasefire of 1988.
Not all saw the war in negative terms. The Islamic Revolution of Iran was strengthened and radicalised. The Iranian government-owned "Etelaat" newspaper wrote, "There is not a single school or town that is excluded from the happiness of 'holy defence' of the nation, from drinking the exquisite elixir of martyrdom, or from the sweet death of the martyr, who dies in order to live forever in paradise."
Comparison of Iraqi and Iranian military strength.
At the beginning of the war, Iraq held a clear advantage in armour, while both nations were roughly equal in terms of artillery. The gap only widened as the war went on. Iran started with a stronger air force, but over time, the balance of power reversed in Iraq's favour. Estimates for 1980 and 1987 were:
Foreign support to Iraq and Iran.
During the war, Iraq was regarded by the West and the Soviet Union as a counterbalance to post-revolutionary Iran.:119 The Soviet Union, Iraq's main arms supplier during the war, did not wish for the end of its alliance with Iraq, and was alarmed by Saddam's threats to find new arms suppliers in the West and China if the Kremlin did not provide him with the weapons he wanted.:119, 198–199 The Soviet Union hoped to use the threat of reducing arms supplies to Iraq as leverage for forming a Soviet-Iranian alliance.:197
During the early years of the war, the United States lacked meaningful relations with either Iran or Iraq, the former due to the Iranian Revolution and the Iran hostage crisis and the latter because of Iraq's alliance with the Soviet Union and hostility towards Israel. Following Iran's success of repelling the Iraqi invasion and Khomeini's refusal to end the war in 1982, the U.S. made an outreach to Iraq, beginning with the restoration of diplomatic relations in 1984. The United States wished to both keep Iran away from Soviet influence and protect other Gulf states from any threat of Iranian expansion. As a result, it began to provide limited support to Iraq.:142–143 In 1982, Henry Kissinger, former Secretary of State, outlined U.S. policy towards Iran:The focus of Iranian pressure at this moment is Iraq. There are few governments in the world less deserving of our support and less capable of using it. Had Iraq won the war, the fear in the Gulf and the threat to our interest would be scarcely less than it is today. Still, given the importance of the balance of power in the area, it is in our interests to promote a ceasefire in that conflict; through not a cost that will preclude an eventual rapprochement with Iran either if a more moderate regime replaces Khomenini's or if the present rulers wake up to geopolitical reality that the historic threat to Iran's independence has always come from the country with which it shares a border of 1500 mi: the Soviet Union. A rapprochement with Iran, of course, must await at a minimum Iran's abandonment of hegemonic aspirations in the Gulf.:142–143 Richard Murphy, Assistant Secretary of State during the war, testified to Congress in 1984 that the Reagan administration believed a victory for either Iran or Iraq was "neither militarily feasible nor strategically desirable.":178
Support to Iraq was given via technological aid, intelligence, the sale of chemical and biological warfare technology and military equipment, and satellite intelligence. While there was direct combat between Iran and the United States, it is not universally agreed that the fighting between the U.S. and Iran was specifically to benefit Iraq, or for separate issues between the U.S. and Iran. American official ambiguity towards which side to support was summed up by Henry Kissinger when he remarked, "It's a pity they both can't lose." The Americans and the British also either blocked or watered down UN resolutions that condemned Iraq for using chemical weapons against the Iranians and their own Kurdish citizens.
More than 30 countries provided support to Iraq, Iran, or both; most of the aid went to Iraq. Iran had a complex clandestine procurement network to obtain munitions and critical materials. Iraq had an even larger clandestine purchasing network, involving 10–12 allied countries, to maintain ambiguity over their arms purchases and to circumvent "official restrictions". Arab mercenaries and volunteers from Egypt and Jordan formed the Yarmouk Brigade and participated in the war alongside Iraqis.
Iraq.
According to the Stockholm International Peace Institute, the Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.
The United States pursued policies in favour of Iraq by reopening diplomatic channels, lifting restrictions on the export of dual-use technology, overseeing the transfer of third-party military hardware, and providing operational intelligence on the battlefield. France, which from the 1970s onward had been one of Iraq's closest allies, was a major supplier of military hardware to Iraq.:184–185 The French sold weapons equal to the sum of US$5 billion, which comprised well over a quarter of Iraq's total arms stockpile.:184–185 China, which had no direct stake in the victory of either side and whose interests in the war were entirely commercial, freely sold arms to both sides.:185, 187, 188, 192–193
Iraq also made extensive use of front companies, middlemen, secret ownership of all or part of companies all over the world, forged end-user certificates, and other methods to hide what it was acquiring. Some transactions may have involved people, shipping, and manufacturing in as many as 10 countries. Support from Great Britain exemplified the methods by which Iraq would circumvent export controls. Iraq bought at least one British company with operations in the United Kingdom and the United States, and had a complex relationship with France and the Soviet Union, its major suppliers of actual weapons.
The United Nations Security Council initially called for a cease-fire after a week of fighting while Iraq was occupying Iranian territory, and renewed the call on later occasions. However, the UN did not come to Iran's aid to repel the Iraqi invasion, and the Iranians thus interpreted the UN as subtly biased in favour of Iraq.
Financial support.
Iraq's main financial backers were the oil-rich Persian Gulf states, most notably Saudi Arabia ($30.9 billion), Kuwait ($8.2 billion), and the United Arab Emirates ($8 billion). In all, Iraq received $35 billion in loans from the West and between $30 and $40 billion from the Persian Gulf states during the 1980s.
The Iraqgate scandal revealed that a branch of Italy's largest bank, Banca Nazionale del Lavoro (BNL), in Atlanta, Georgia, U.S. relied partially on U.S. taxpayer-guaranteed loans to funnel $5 billion to Iraq from 1985 to 1989. In August 1989, when FBI agents raided the Atlanta branch of BNL, branch manager Christopher Drogoul was charged with making unauthorised, clandestine, and illegal loans to Iraq – some of which, according to his indictment, were used to purchase arms and weapons technology.
According to the "Financial Times", the companies involved in the scandal by shipping militarily useful technology to Iraq were Hewlett-Packard, Tektronix and Matrix Churchill's branch in Ohio, U.S.
Iran.
While the United States directly fought Iran, citing freedom of navigation as a major "casus belli", it also indirectly supplied some weapons to Iran as part of a complex and illegal programme that became known as the Iran-Contra affair. These secret sales were partly to help secure the release of hostages held in Lebanon, and partly to make money to help the Contras rebel group in Nicaragua. This arms for hostages agreement turned into a major scandal.
North Korea was a major arms supplier to Iran, often acting as a third party in arms deals between Iran and the Communist bloc. Support included domestically manufactured arms and Eastern-Bloc weapons, for which the major powers wanted deniability. Other arms suppliers and supporters of Iran included Libya and China.
Both countries.
Besides the United States and the Soviet Union, Yugoslavia also sold weapons to both countries for the entire duration of the conflict. Likewise, Portugal helped both countries;:8 it was not unusual to see Iranian and Iraqi flagged ships moored side-by-side at the Port of Sines.
From 1980 to 1987 Spain sold €458 million in weapons to Iran and €172 million in weapons to Iraq. Weapons sold to Iraq included 4x4 vehicles, BO-105 helicopters, explosives, and ammunition. A research party later discovered that an unexploded chemical Iraqi warhead in Iran was manufactured in Spain.:8
Although neither side acquired any weapons from Turkey, both sides enjoyed Turkish civilian trade during the conflict, although the Turkish government remained neutral and refused to support the trade embargo imposed by the U.S. on Iran. Turkey's export market jumped from $220 million in 1981 to $2 billion in 1985, making up 25% of Turkey's overall exports. Turkish construction projects in Iraq totaled $2.5 billion between 1974 and 1990. Trading with both countries helped Turkey to offset its ongoing economic crisis, though the benefits decreased as the war neared its end and accordingly disappeared entirely with Iraq's invasion of Kuwait and the resulting Iraq sanctions Turkey imposed in response.
U.S. involvement.
U.S. Embargo.
A key element of US political–military and energy–economic planning occurred in early 1983. The Iran–Iraq war had been going on for three years and there were significant casualties on both sides, reading hundreds of thousands. Within the Reagan National Security Council concern was growing that the war could spread beyond the boundaries of the two belligerents. A National Security Planning Group meeting was called chaired by Vice President George Bush to review US options. It was determined that there was a high likelihood that the conflict would spread into Saudi Arabia and other Gulf states, but that the US had little capability to defend the region. Furthermore it was determined that a prolonged war in the region would induce much higher oil prices and threaten the fragile world recovery which was just beginning to gain momentum. On 22 May 1984, President Reagan was briefed on the project conclusions in the Oval Office by William Flynn Martin who had served as the head of the NSC staff that organized the study. The full declassified presentation can be seen here. The conclusions were threefold: first oil stocks needed to be increased among members of the International Energy Agency and, if necessary, released early in the event of oil market disruption; second the United States needed to beef up the security of friendly Arab states in the region and thirdly an embargo should be placed on sales of military equipment to Iran and Iraq. The Plan was approved by the President and later affirmed by the G-7 leaders headed by Margaret Thatcher in the London Summit of 1984.
Iraqi attack on U.S. warship.
On 17 May 1987, an Iraqi Mirage F1 attack aircraft launched two Exocet missiles at the USS "Stark", a "Perry" class frigate. The first struck the port side of the ship and failed to explode, though it left burning propellant in its wake; the second struck moments later in approximately the same place and penetrated through to crew quarters, where it exploded, killing 37 crew members and leaving 21 injured. Whether or not Iraqi leadership authorised the attack is still unknown. Initial claims by the Iraqi government (that "Stark" was inside the Iran–Iraq War zone) were shown to be false, and the motives and orders of the pilot remain unanswered. Though American officials claimed that the pilot who attacked "Stark" had been executed, an ex-Iraqi Air Force commander since stated he had not been punished, and was still alive at the time. The attack remains the only successful anti-ship missile strike on an American warship. Due to the extensive political and military cooperation between the Iraqis and Americans by 1987, the attack had little effect on relations between the two countries.
U.S. military actions toward Iran.
U.S. attention was focused on isolating Iran as well as maintaining freedom of navigation. It criticised Iran's mining of international waters, and sponsored , which passed unanimously on 20 July, under which the U.S. and Iranian forces skirmished during Operation Earnest Will. During Operation Nimble Archer in October 1987, the U.S. attacked Iranian oil platforms in retaliation for an Iranian attack on the U.S.-flagged Kuwaiti tanker "Sea Isle City".
On 14 April 1988, the frigate USS "Samuel B. Roberts" was badly damaged by an Iranian mine, and 10 sailors were wounded. U.S. forces responded with Operation Praying Mantis on 18 April, the U.S. Navy's largest engagement of surface warships since World War II. Two Iranian oil platforms were damaged, and five Iranian warships and gunboats were sunk. An American helicopter also crashed. This fighting manifested in the International Court of Justice as Oil Platforms case (Islamic Republic of Iran v. United States of America), which was eventually dismissed in 2003.
U.S. shoots down civilian airliner.
In the course of escorts by the U.S. Navy, the cruiser USS "Vincennes" shot down Iran Air Flight 655 on 3 July 1988, killing all 290 passengers and crew on board. The American government claimed that the "Vincennes" was in international waters at the time (which was later proven to be untrue), that the civilian airliner had been mistaken for an Iranian F-14 Tomcat, and the "Vincennes" feared that it was under attack.:260–273 The Iranians maintain that the "Vincennes" was in their own waters, and that the passenger jet was turning away and increasing altitude after take-off. U.S. Admiral William J. Crowe later admitted on "Nightline" that the "Vincennes" was in Iranian territorial waters when it launched the missiles. At the time of the attack, Admiral Crowe claimed that the Iranian plane did not identify itself and sent no response to warning signals he had sent.
Reagan’s vice president, George H. W. Bush, defended the "Vincennes" crew's action saying "I will never apologize for the United States. Ever. I don't care what the facts are". In 1996, the U.S. expressed regret for the event and the civilian deaths it caused.
Use of chemical weapons by Iraq.
In a declassified 1991 report, the CIA estimated that Iran had suffered more than 50,000 casualties from Iraq's use of several chemical weapons, though current estimates are more than 100,000 as the long-term effects continue to cause casualties. The official CIA estimate did not include the civilian population contaminated in bordering towns or the children and relatives of veterans, many of whom have developed blood, lung and skin complications, according to the Organization for Veterans of Iran. According to a 2002 article in the "Star-Ledger", 20,000 Iranian soldiers were killed on the spot by nerve gas. As of 2002, 5,000 of the 80,000 survivors continue to seek regular medical treatment, while 1,000 are hospital inpatients.
According to Iraqi documents, assistance in developing chemical weapons was obtained from firms in many countries, including the United States, West Germany, the Netherlands, the United Kingdom, and France. A report stated that Dutch, Australian, Italian, French and both West and East German companies were involved in the export of raw materials to Iraqi chemical weapons factories. Declassified CIA documents show that the United States was providing reconnaissance intelligence to Iraq around 1987–88 which was then used to launch chemical weapon attacks on Iranian troops and that CIA fully knew that chemical weapons would be deployed and sarin attacks followed.
On 21 March 1986, the United Nations Security Council made a declaration stating that "members are profoundly concerned by the unanimous conclusion of the specialists that chemical weapons on many occasions have been used by Iraqi forces against Iranian troops, and the members of the Council strongly condemn this continued use of chemical weapons in clear violation of the Geneva Protocol of 1925, which prohibits the use in war of chemical weapons." The United States was the only member who voted against the issuance of this statement. A mission to the region in 1988 found evidence of the use of chemical weapons, and was condemned in Security Council Resolution 612.
According Walter Lang, senior defence intelligence officer for the U.S. Defense Intelligence Agency at the time, "the use of gas on the battlefield by the Iraqis was not a matter of deep strategic concern" to Reagan and his aides, because they "were desperate to make sure that Iraq did not lose." He claimed that the Defense Intelligence Agency "would have never accepted the use of chemical weapons against civilians, but the use against military objectives was seen as inevitable in the Iraqi struggle for survival". The Reagan administration did not stop aiding Iraq after receiving reports of the use of poison gas on Kurdish civilians.
The U.S. has accused Iran of using chemical weapons as well,:214 though the allegations have been disputed. Joost Hiltermann, the principal researcher for Human Rights Watch between 1992 and 1994, conducted a two-year study that included a field investigation in Iraq, and obtained Iraqi government documents in the process. According to Hiltermann, the literature on the Iran–Iraq War reflects allegations of chemical weapons used by Iran, but they are "marred by a lack of specificity as to time and place, and the failure to provide any sort of evidence".:153
Analysts Gary Sick and Lawrence Potter have called the allegations against Iran "mere assertions" and stated, "No persuasive evidence of the claim that Iran was the primary culprit [of using chemical weapons] was ever presented.":156 Policy consultant and author Joseph Tragert stated, "Iran did not retaliate with chemical weapons, probably because it did not possess any at the time".
At his trial in December 2006, Saddam said he would take responsibility "with honour" for any attacks on Iran using conventional or chemical weapons during the 1980–1988 war, but he took issue with charges he ordered attacks on Iraqis. A medical analysis of the effects of Iraqi mustard gas is described in a U.S. military textbook and contrasted effects of World War I gas.
Dissimilarities from other conflicts.
Iran's attack on the Osirak nuclear reactor in September 1980 was the first attack on a nuclear reactor and one of only six military attacks on nuclear facilities in history. It was also the first instance of a pre-emptive attack on a nuclear reactor to forestall the development of a nuclear weapon, though it did not achieve its objective, as France repaired the reactor after the attack. (It took a second pre-emptive strike by the Israeli Air Force in June 1981 to disable the reactor, killing a French engineer in the process and causing France to pull out of Osirak. The decommissioning of Osirak has been cited as causing a substantial delay to Iraqi acquisition of nuclear weapons.)
The Iran–Iraq War was the first and only conflict in the history of warfare in which both forces used ballistic missiles against each other. The war also saw the only confirmed air-to-air helicopter battles in the history of warfare, with Iraqi Mi-25s flying against Iranian AH-1J SeaCobras on numerous occasions. The first instance of these helicopter "dogfights" occurred on the first day of the war (22 September 1980): two Iranian SeaCobras crept up on two Mi-25s and hit them with TOW wire-guided anti-tank missiles. One Mi-25 went down immediately, the other was badly damaged and crashed before reaching base. The Iranians won a similar air battle on 24 April 1981, destroying two Mi-25s without incurring losses themselves. According to unclassified documents, Iranian pilots achieved a 10-to-1 kill ratio over the Iraqi helicopter pilots during these engagements and even engaged Iraqi fixed-wing aircraft. The Iraqis hit back, claiming the destruction of a SeaCobra on 14 September 1983 (with a YaKB machine gun), then three SeaCobras on 5 February 1984[23] and three more on 25 February 1984 (two with Falanga missiles, one with S-5 rockets).[19] After a lull in helicopter losses, each side lost a gunship on 13 February 1986.[19] Later, a Mi-25 claimed a SeaCobra shot down with a YaKB gun on 16 February, and a SeaCobra claimed a Mi-25 shot down with rockets on 18 February.[19] The last engagement between the two types was on 22 May 1986, when Mi-25s shot down a SeaCobra. The final claim tally was 10 SeaCobras destroyed and 6 Mi-25s destroyed. The relatively small numbers and the inevitable disputes over actual kill numbers makes it unclear if one gunship had a real technical superiority over the other. Iraqi Mi-25s also claimed a total of 43 kills against other Iranian helicopters, such as Agusta-Bell UH-1 Hueys.[23]. Both sides, especially Iraq, also carried out air and missile attacks against population centers.
In October 1986 Iraqi aircraft began to attack civilian passenger trains and aircraft on Iranian soil, including an Iran Air Boeing 737 unloading passengers at Shiraz International Airport. In retaliation for the Iranian Operation Karbala 5, Iraq attacked 65 cities in 226 sorties over 42 days, bombing civilian neighbourhoods. Eight Iranian cities came under attack from Iraqi missiles. The bombings killed 65 children in an elementary school in Borujerd. The Iranians responded with Scud missile attacks on Baghdad and struck a primary school there. These events became known as the "war of the cities".
Despite both sides fighting a war with each other, Iran and Iraq maintained diplomatic relations and embassies in each other's countries until mid-1987.
Iran's government used human waves to attack enemy troops and even in some cases to clear minefields. Children volunteered as well. Some reports mistakenly have the Basijis marching into battle while marking their expected entry to heaven by wearing "plastic keys to paradise" around their necks, although other analysts regard this story as a hoax involving a misinterpretation of the carrying of a prayer book called "The Keys to Paradise"(Mafatih al-Janan) by Sheikh Abbas Qumi given to all volunteers.
According to journalist Robin Wright,
During the Fateh offensive [in February 1987], I toured the southwest front on the Iranian side and saw scores of boys, aged anywhere from nine to sixteen, who said with staggering and seemingly genuine enthusiasm that they had volunteered to become martyrs. Regular army troops, the paramilitary Revolutionary Guards and mullahs all lauded these youths, known as baseeji [Basij], for having played the most dangerous role in breaking through Iraqi lines. They had led the way, running over fields of mines to clear the ground for the Iranian ground assault. Wearing white headbands to signify the embracing of death, and shouting "Shaheed, shaheed" (Martyr, martyr) they literally blew their way into heaven. Their numbers were never disclosed. But a walk through the residential suburbs of Iranian cities provided a clue. Window after window, block after block, displayed black-bordered photographs of teenage or preteen youths.
See also.
General:
Persons:
Memoirs
Stories
Relevant conflicts

</doc>
<doc id="14891" url="http://en.wikipedia.org/wiki?curid=14891" title="Incremental reading">
Incremental reading

Incremental reading is a method for learning and retaining information from reading that might otherwise be forgotten. It is particularly targeted to people who are trying to learn a large amount of information at once, particularly if that information is varied.
Incremental reading works by breaking up key points of articles, often dozens or thousands of articles, into flashcards, which are then learned and reviewed over an extended period. Concretely, when reading an article (in a web browser), the reader selects extracts (similar to underlining or highlighting a paper article), which are then converted to question-answer format, often by cloze deletion, and then scheduled for learning and review by the learning software. This flashcard creation process is semi-automated – the reader chooses which material to learn and edits the precise wording of the questions, while the software assists in making the flashcards and does the scheduling.
Incremental reading is based on psychological principles of long-term memory storage and retrieval, in particular the spacing effect.
Information is broken into chunks, and an algorithm (usually computer software) organises the user's reading and calculates the ideal time for the reader to review each chunk. The method itself is often credited to the Polish software developer Piotr Wozniak.
Until recently, Wozniak's SuperMemo was the only implementation of incremental reading (as opposed to simply spaced repetition of questions and cloze deletions etc.). Anki has an implementation available as an add-on.
There is also incremental reading support for the text editors Emacs and Yi.
Method.
With incremental reading, a load of material is subdivided into articles and its extracts. All articles and extracts are processed according to the rules of spaced repetition. This means that all processed pieces of information are presented at increasing intervals. Individual articles are read in portions proportional to the attention span, which depends on the user, their mood, the article, etc.
The name "incremental" comes from "reading in portions". Without the use of spaced repetition, the reader would quickly get lost in the glut of information when studying dozens of subjects at the same time. However, spaced repetition makes it possible to retain traces of the processed material in memory. Incremental reading makes it possible to read hundreds of articles at the same time with a substantial gain to attention.
For incremental reading to leave a permanent mark in long-term memory, the processed material must be gradually converted into material based on active recall. This means that extracts such as "George Washington was the first U.S. President" must be changed to questions such as "Who was the first U.S. President?", "Who was George Washington?", etc.

</doc>
<doc id="14892" url="http://en.wikipedia.org/wiki?curid=14892" title="Intelligence quotient">
Intelligence quotient

An intelligence quotient (IQ) is a score derived from one of several standardized tests designed to assess human intelligence. The abbreviation "IQ" was coined by the psychologist William Stern for the German term "Intelligenz-quotient", his term for a scoring method for intelligence tests he advocated in a 1912 book. When current IQ tests are developed, the median raw score of the norming sample is defined as IQ 100 and scores each standard deviation (SD) up or down are defined as 15 IQ points greater or less, although this was not always so historically. By this definition, approximately two-thirds of the population scores between IQ 85 and IQ 115, and about 5 percent of the population scores above 125.
IQ scores have been shown to be associated with such factors as morbidity and mortality, parental social status, and, to a substantial degree, biological parental IQ. While the heritability of IQ has been investigated for nearly a century, there is still debate about the significance of heritability estimates and the mechanisms of inheritance.
IQ scores are used for educational placement, assessment of intellectual disability, and evaluating job applicants. In research contexts they have been studied as predictors of job performance, and income. They are also used to study distributions of psychometric intelligence in populations and the correlations between it and other variables. Raw scores on IQ tests for many populations have been rising at an average rate that scales to three IQ points per decade since the early 20th century, a phenomenon called the Flynn effect. Investigation of different patterns of increases in subtest scores can also inform current research on human intelligence.
History.
Precursors to IQ testing.
Historically, even before IQ tests were invented, there were attempts to classify people into intelligence categories by observing their behavior in daily life. Those other forms of behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of "intelligence" used in a particular case and on the reliability and error of estimation in the classification procedure.
The English statistician Francis Galton made the first attempt at creating a standardized test for rating a person's intelligence. A pioneer of psychometrics and the application of statistical methods to the study of human diversity and the study of inheritance of human traits, he believed that intelligence was largely a product of heredity (by which he did not mean genes, although he did develop several pre-Mendelian theories of particulate inheritance). He hypothesized that there should exist a correlation between intelligence and other observable traits such as reflexes, muscle grip, and head size. He set up the first mental testing centre in the world in 1882 and he published "Inquiries into Human Faculty and Its Development" in 1883, in which he set out his theories. After gathering data on a variety of physical variables, he was unable to show any such correlation, and he eventually abandoned this research.
French psychologist Alfred Binet, together with Victor Henri and Théodore Simon had more success in 1905, when they published the Binet-Simon test, which focused on verbal abilities. It was intended to identify mental retardation in school children, but in specific contradistinction to claims made by psychiatrists that these children were "sick" (not "slow") and should therefore be removed from school and cared-for in asylums. The score on the Binet-Simon scale would reveal the child's mental age. For example, a six-year-old child who passed all the tasks usually passed by six-year-olds—but nothing beyond—would have a mental age that matched his chronological age, 6.0. (Fancher, 1985). Binet thought that intelligence was multifaceted, but came under the control of practical judgement.
In Binet's view, there were limitations with the scale and he stressed what he saw as the remarkable diversity of intelligence and the subsequent need to study it using qualitative, as opposed to quantitative, measures (White, 2000). American psychologist Henry H. Goddard published a translation of it in 1910. American psychologist Lewis Terman at Stanford University revised the Binet-Simon scale, which resulted in the Stanford-Binet Intelligence Scales (1916). It became the most popular test in the United States for decades.
General factor ("g").
The many different kinds of IQ tests include a wide variety of item content. Some test items are visual, while many are verbal. Test items vary from being based on abstract-reasoning problems to concentrating on arithmetic, vocabulary, or general knowledge.
The British psychologist Charles Spearman in 1904 made the first formal factor analysis of correlations between the tests. He observed that children's school grades across seemingly unrelated school subjects were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. He suggested that all mental performance could be conceptualized in terms of a single general ability factor and a large number of narrow task-specific ability factors. Spearman named it "g" for "general factor" and labeled the specific factors or abilities for specific tasks "s". In any collection of test items that make up an IQ test, the score that best measures "g" is the composite score that has the highest correlations with all the item scores. Typically, the ""g"-loaded" composite score of an IQ test battery appears to involve a common strength in abstract reasoning across the test's item content. Therefore, Spearman and others have regarded "g" as closely related to the essence of human intelligence.
Spearman's argument proposing a general factor of human intelligence is still accepted in principle by many psychometricians. Today's factor models of intelligence typically represent cognitive abilities as a three-level hierarchy, where there are a large number of narrow factors at the bottom of the hierarchy, a handful of broad, more general factors at the intermediate level, and at the apex a single factor, referred to as the "g" factor, which represents the variance common to all cognitive tasks. However, this view is not universally accepted; other factor analyses of the data, with different results, are possible. Some psychometricians regard "g" as a statistical artifact.
United States military selection in World War I.
During World War I, a way was needed to evaluate and assign Army recruits to appropriate tasks. This led to the rapid development of several mental tests. The testing generated controversy and much public debate in the United States. Nonverbal or "performance" tests were developed for those who could not speak English or were suspected of malingering. After the war, positive publicity promoted by army psychologists helped to make psychology a respected field. Subsequently, there was an increase in jobs and funding in psychology in the United States. Group intelligence tests were developed and became widely used in schools and industry.
L.L. Thurstone argued for a model of intelligence that included seven unrelated factors (verbal comprehension, word fluency, number facility, spatial visualization, associative memory, perceptual speed, reasoning, and induction). While not widely used, Thurstone's model influenced later theories.
David Wechsler produced the first version of his test in 1939. It gradually became more popular and overtook the Stanford-Binet in the 1960s. It has been revised several times, as is common for IQ tests, to incorporate new research. One explanation is that psychologists and educators wanted more information than the single score from the Binet. Wechsler's ten or more subtests provided this. Another is that Stanford-Binet test reflected mostly verbal abilities, while the Wechsler test also reflected nonverbal abilities. The Stanford-Binet has also been revised several times and is now similar to the Wechsler in several aspects, but the Wechsler continues to be the most popular test in the United States.
Cattell–Horn–Carroll theory.
Raymond Cattell (1941) proposed two types of cognitive abilities in a revision of Spearman's concept of general intelligence. Fluid intelligence (Gf) was hypothesized as the ability to solve novel problems by using reasoning, and crystallized intelligence (Gc) was hypothesized as a knowledge-based ability that was very dependent on education and experience. In addition, fluid intelligence was hypothesized to decline with age, while crystallized intelligence was largely resistant to the effects of aging. The theory was almost forgotten, but was revived by his student John L. Horn (1966) who later argued Gf and Gc were only two among several factors, and who eventually identified nine or ten broad abilities. The theory continued to be called Gf-Gc theory.
John B. Carroll (1993), after a comprehensive reanalysis of earlier data, proposed the three stratum theory, which is a hierarchical model with three levels. The bottom stratum consists of narrow abilities that are highly specialized (e.g., induction, spelling ability). The second stratum consists of broad abilities. Carroll identified eight second-stratum abilities. Carroll accepted Spearman's concept of general intelligence, for the most part, as a representation of the uppermost, third stratum.
In 1999, a merging of the Gf-Gc theory of Cattell and Horn with Carroll's Three-Stratum theory has led to the Cattell–Horn–Carroll theory (CHC Theory). It has greatly influenced many of the current broad IQ tests.
In CHC theory, a hierarchy of factors is used; "g" is at the top. Under it are ten broad abilities that in turn are subdivided into seventy narrow abilities. The broad abilities are:
Modern tests do not necessarily measure all of these broad abilities. For example, Gq and Grw may be seen as measures of school achievement and not IQ. Gt may be difficult to measure without special equipment. "g" was earlier often subdivided into only Gf and Gc, which were thought to correspond to the nonverbal or performance subtests and verbal subtests in earlier versions of the popular Wechsler IQ test. More recent research has shown the situation to be more complex. Modern comprehensive IQ tests don't stop at reporting a single IQ score. Although they still give an overall score, they now also give scores for many of these more restricted abilities, identifying particular strengths and weaknesses of an individual.
Other theories.
J.P. Guilford's Structure of Intellect (1967) model used three dimensions which when combined yielded a total of 120 types of intelligence. It was popular in the 1970s and early 1980s, but faded owing to both practical problems and theoretical criticisms.
Alexander Luria's earlier work on neuropsychological processes led to the PASS theory (1997). It argued that only looking at one general factor was inadequate for researchers and clinicians who worked with learning disabilities, attention disorders, intellectual disability, and interventions for such disabilities. The PASS model covers four kinds of processes (planning process, attention/arousal process, simultaneous processing, and successive processing). The planning processes involve decision making, problem solving, and performing activities and requires goal setting and self-monitoring. The attention/arousal process involves selectively attending to a particular stimulus, ignoring distractions, and maintaining vigilance. Simultaneous processing involves the integration of stimuli into a group and requires the observation of relationships. Successive processing involves the integration of stimuli into serial order. The planning and attention/arousal components comes from structures located in the frontal lobe, and the simultaneous and successive processes come from structures located in the posterior region of the cortex. It has influenced some recent IQ tests, and been seen as a complement to the Cattell-Horn-Carroll theory described above.
Current tests.
There are a variety of individually administered IQ tests in use in the English-speaking world. The most commonly used individual IQ test series is the Wechsler Adult Intelligence Scale for adults and the Wechsler Intelligence Scale for Children for school-age test-takers. Other commonly used individual IQ tests (some of which do not label their standard scores as "IQ" scores) include the current versions of the Stanford-Binet, Woodcock-Johnson Tests of Cognitive Abilities, the Kaufman Assessment Battery for Children, the Cognitive Assessment System, and the Differential Ability Scales.
IQ scales are ordinally scaled. While one standard deviation is 15 points, and two SDs are 30 points, and so on, this does not imply that mental ability is linearly related to IQ, such that IQ 50 means half the cognitive ability of IQ 100. In particular, IQ points are not percentage points.
Reliability and validity.
Psychometricians generally regard IQ tests as having high statistical reliability. A high reliability implies that—although test-takers may have varying scores when taking the same test on differing occasions, and they may have varying scores when taking different IQ tests at the same age—the scores generally agree with one another and across time. Like all statistical quantities, any particular estimate of IQ has an associated standard error that measures uncertainty about the estimate. For modern tests, the standard error of measurement is about three points. Clinical psychologists generally regard IQ scores as having sufficient statistical validity for many clinical purposes.
Flynn effect.
Since the early 20th century, raw scores on IQ tests have increased in most parts of the world. When a new version of an IQ test is normed, the standard scoring is set so performance at the population median results in a score of IQ 100. The phenomenon of rising raw score performance means if test-takers are scored by a constant standard scoring rule, IQ test scores have been rising at an average rate of around three IQ points per decade. This phenomenon was named the Flynn effect in the book "The Bell Curve" after James R. Flynn, the author who did the most to bring this phenomenon to the attention of psychologists.
Researchers have been exploring the issue of whether the Flynn effect is equally strong on performance of all kinds of IQ test items, whether the effect may have ended in some developed nations, whether there are social subgroup differences in the effect, and what possible causes of the effect might be. A 1998 textbook, IQ and Human Intelligence, by N. J. Mackintosh, noted that before Flynn published his major papers, many psychologists mistakenly believed that there were dysgenic trends gradually reducing the level of intelligence in the general population. They also believed that no environmental factor could possibly have a strong effect on IQ. Mackintosh noted that Flynn's observations have prompted much new research in psychology and "demolish some long-cherished beliefs, and raise a number of other interesting issues along the way."
Age.
IQ can change to some degree over the course of childhood. However, in one longitudinal study, the mean IQ scores of tests at ages 17 and 18 were correlated at r=0.86 with the mean scores of tests at ages five, six, and seven and at r=0.96 with the mean scores of tests at ages 11, 12, and 13.
For decades practitioners' handbooks and textbooks on IQ testing have reported IQ declines with age after the beginning of adulthood. However, later researchers pointed out this phenomenon is related to the Flynn effect and is in part a cohort effect rather than a true aging effect. A variety of studies of IQ and aging have been conducted since the norming of the first Wechsler Intelligence Scale drew attention to IQ differences in different age groups of adults. Current consensus is that fluid intelligence generally declines with age after early adulthood, while crystallized intelligence remains intact. Both cohort effects (the birth year of the test-takers) and practice effects (test-takers taking the same form of IQ test more than once) must be controlled to gain accurate data. It is unclear whether any lifestyle intervention can preserve fluid intelligence into older ages.
The exact peak age of fluid intelligence or crystallized intelligence remains elusive. Cross-sectional studies usually show that especially fluid intelligence peaks at a relatively young age (often in the early adulthood) while longitudinal data mostly show that intelligence is stable until the mid adulthood or later. Subsequently, intelligence seems to decline slowly.
Genetics and environment.
Environmental and genetic factors play a role in determining IQ. Their relative importance has been the subject of much research and debate.
Heritability.
Heritability is defined as the proportion of variance in a trait which is attributable to genotype within a defined population in a specific environment. A number of points must be considered when interpreting heritability. Heritability measures the proportion of 'variation' in a trait that can be attributed to genes, and not the proportion of a trait caused by genes. The value of heritability can change if the impact of environment (or of genes) in the population is substantially altered. A high heritability of a trait does not mean environmental effects, such as learning, are not involved. Since heritability increases during childhood and adolescence, one should be cautious drawing conclusions regarding the role of genetics and environment from studies where the participants are not followed until they are adults.
Twin studies have found the heritability of IQ in adult twins to be 0.7 to 0.8 and in child twins 0.45 in the Western world. It may seem reasonable to expect genetic influences on traits like IQ to become less important as one gains experiences with age. However, the opposite occurs. Heritability measures in infancy are as low as 0.2, around 0.4 in middle childhood, and as high as 0.8 in adulthood. One proposed explanation is that people with different genes tend to reinforce the effects of those genes, for example by seeking out different environments. Debate is ongoing about whether these heritability estimates are too high, owing to inadequate consideration of various factors—such as the environment being relatively more important in families with low socioeconomic status, or the effect of the maternal (fetal) environment.
Research shows that molecular genetics of psychology and social science requires approaches that go beyond the examination of candidate genes.
Shared family environment.
Family members have aspects of environments in common (for example, characteristics of the home). This shared family environment accounts for 0.25–0.35 of the variation in IQ in childhood. By late adolescence, it is quite low (zero in some studies). The effect for several other psychological traits is similar. These studies have not looked at the effects of extreme environments, such as in abusive families.
Non-shared family environment and environment outside the family.
Although parents treat their children differently, such differential treatment explains only a small amount of nonshared environmental influence. One suggestion is that children react differently to the same environment because of different genes. More likely influences may be the impact of peers and other experiences outside the family.
Individual genes.
A very large proportion of the over 17,000 human genes are thought to have an effect on the development and functionality of the brain. While a number of individual genes have been reported to be associated with IQ, none have a strong effect. Deary and colleagues (2009) reported that no finding of a strong gene effect on IQ has been replicated. Most reported associations of genes with intelligence are false positive results. Recent findings of gene associations with normally varying intelligence differences in adults continue to show weak effects for any one gene; likewise in children.
Gene-environment interaction.
David Rowe reported an interaction of genetic effects with socioeconomic status, such that the heritability was high in high-SES families, but much lower in low-SES families. This has been replicated in infants, children and adolescents 
in the US, though not outside the US, for instance a reverse result was reported in the UK.
Dickens and Flynn (2001) have argued that genes for high IQ initiate environment-shaping feedback, as genetic effects cause bright children to seek out more stimulating environments that further increase IQ. In their model, environment effects decay over time (the model could be adapted to include possible factors, like nutrition in early childhood, that may cause permanent effects). The Flynn effect can be explained by a generally more stimulating environment for all people. The authors suggest that programs aiming to increase IQ would be most likely to produce long-term IQ gains if they caused children to persist in seeking out cognitively demanding experiences.
Interventions.
In general, educational interventions, as those described below, have shown short-term effects on IQ, but long-term follow-up is often missing. For example, in the US very large intervention programs such as the Head Start Program have not produced lasting gains in IQ scores. More intensive, but much smaller projects such as the Abecedarian Project have reported lasting effects, often on socioeconomic status variables, rather than IQ.
Recent studies have shown that training in using one's working memory may increase IQ. A study on young adults published in April 2008 by a team from the Universities of Michigan and Bern supports the possibility of the transfer of fluid intelligence from specifically designed working memory training. Further research will be needed to determine nature, extent and duration of the proposed transfer. Among other questions, it remains to be seen whether the results extend to other kinds of fluid intelligence tests than the matrix test used in the study, and if so, whether, after training, fluid intelligence measures retain their correlation with educational and occupational achievement or if the value of fluid intelligence for predicting performance on other tasks changes. It is also unclear whether the training is durable of extended periods of time.
Music.
Musical training in childhood has been found to correlate with higher than average IQ. Multiple attempted replications (e.g.) have shown that this is at best a short-term effect (lasting no longer than 10 to 15 minutes), and is not related to IQ-increase.
Music lessons.
In 2004, Schellenberg devised an experiment to test his hypothesis that music lessons can enhance the IQ of children. He had 144 samples of 6 year old children which were put into 4 groups; keyboard lessons, vocal lessons, drama lessons or no lessons at all, for 36 weeks. The samples' IQ was measured both before and after the lessons had taken place using the Wechsler Intelligence Scale for Children–Third Edition, Kaufman Test of Educational Achievement and Parent Rating Scale of the Behavioral Assessment System for Children. All four groups had increases in IQ, most likely resulted by the entrance of grade school. The notable difference with the two music groups compared to the two controlled groups was a slightly higher increase in IQ. The children in the control groups on average had an increase in IQ of 4.3 points, while the increase in IQ of the music groups was 7.0 points. Though the increases in IQ were not dramatic, one can still conclude that musical lessons do have a positive effect for children, if taken at a young age. It is hypothesized that improvements in IQ occur after musical lessons because the music lessons encourage multiple experiences which generates progression in a wide range of abilities for the children. Testing this hypothesis however, has proven difficult.
Brain anatomy.
Several neurophysiological factors have been correlated with intelligence in humans, including the ratio of brain weight to body weight and the size, shape and activity level of different parts of the brain. Specific features that may affect IQ include the size and shape of the frontal lobes, the amount of blood and chemical activity in the frontal lobes, the total amount of gray matter in the brain, the overall thickness of the cortex and the glucose metabolic rate.
Health.
Health is important in understanding differences in IQ test scores and other measures of cognitive ability. Several factors can lead to significant cognitive impairment, particularly if they occur during pregnancy and childhood when the brain is growing and the blood–brain barrier is less effective. Such impairment may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. A cohort study confers the relationship between familial inbreeding and modest cognitive impairments among children, providing the evidence for inbreeding depression on intellectual behaviors on comparing with environmental and socioeconomic variables.
Since about 2010 researchers such as Eppig, Hassel and MacKenzie have found a very close and consistent link between IQ scores and infectious diseases, especially in the infant and preschool populations and the mothers of these children. They have postulated that fighting infectious diseases strains the child's metabolism and prevents full brain development. Hassel postulated that it is by far the most important factor in determining population IQ. However they also found that subsequent factors such as good nutrition, regular quality schooling can offset early negative effects to some extent.
Developed nations have implemented several health policies regarding nutrients and toxins known to influence cognitive function. These include laws requiring fortification of certain food products and laws establishing safe levels of pollutants (e.g. lead, mercury, and organochlorides). Improvements in nutrition, and in public policy in general, have been implicated in worldwide IQ increases. 
Cognitive epidemiology is a field of research that examines the associations between intelligence test scores and health. Researchers in the field argue that intelligence measured at an early age is an important predictor of later health and mortality differences.
Social correlations.
School performance.
The American Psychological Association's report "Intelligence: Knowns and Unknowns" states that wherever it has been studied, children with high scores on tests of intelligence tend to learn more of what is taught in school than their lower-scoring peers. The correlation between IQ scores and grades is about .50. This means that the explained variance is 25%. Achieving good grades depends on many factors other than IQ, such as "persistence, interest in school, and willingness to study" (p. 81).
It has been found that the correlation of IQ scores with school performance depends on the IQ measurement used. For undergraduate students, the Verbal IQ as measured by WAIS-R has been found to correlate significantly (0.53) with the GPA of the last 60 hours. In contrast, Performance IQ correlation with the same GPA was only 0.22 in the same study.
Some measures of educational aptitude correlate highly with IQ tests – for instance, Frey and Detterman (2004) reported a correlation of 0.82 between "g" (general intelligence factor) and SAT scores; another research found a correlation of 0.81 between "g" and GCSE scores, with the explained variance ranging "from 58.6% in Mathematics and 48% in English to 18.1% in Art and Design".
Job performance.
According to Schmidt and Hunter, "for hiring employees without previous experience in the job the most valid predictor of future performance is general mental ability." The validity of IQ as a predictor of job performance is above zero for all work studied to date, but varies with the type of job and across different studies, ranging from 0.2 to 0.6. The correlations were higher when the unreliability of measurement methods was controlled for. While IQ is more strongly correlated with reasoning and less so with motor function, IQ-test scores predict performance ratings in all occupations. That said, for highly qualified activities (research, management) low IQ scores are more likely to be a barrier to adequate performance, whereas for minimally-skilled activities, athletic strength (manual strength, speed, stamina, and coordination) are more likely to influence performance. It is largely through the quicker acquisition of job-relevant knowledge that higher IQ mediates job performance.
In establishing a causal direction to the link between IQ and work performance, longitudinal studies by Watkins and others suggest that IQ exerts a causal influence on future academic achievement, whereas academic achievement does not substantially influence future IQ scores. Treena Eileen Rohde and Lee Anne Thompson write that general cognitive ability, but not specific ability scores, predict academic achievement, with the exception that processing speed and spatial ability predict performance on the SAT math beyond the effect of general cognitive ability.
The US military has minimum enlistment standards at about the IQ 85 level. There have been two experiments with lowering this to 80 but in both cases these men could not master soldiering well enough to justify their costs.
Income.
While it has been suggested that "in economic terms it appears that the IQ score measures something with decreasing marginal value. It is important to have enough of it, but having lots and lots does not buy you that much.", large scale longitudinal studies indicate an increase in IQ translates into an increase in performance at all levels of IQ: i.e., that ability and job performance are monotonically linked at all IQ levels. Charles Murray, coauthor of "The Bell Curve," found that IQ has a substantial effect on income independently of family background.
The link from IQ to wealth is much less strong than that from IQ to job performance. Some studies indicate that IQ is unrelated to net worth.
The American Psychological Association's 1995 report "Intelligence: Knowns and Unknowns" stated that IQ scores accounted for (explained variance) about a quarter of the social status variance and one-sixth of the income variance. Statistical controls for parental SES eliminate about a quarter of this predictive power. Psychometric intelligence appears as only one of a great many factors that influence social outcomes.
Some studies claim that IQ only accounts for (explains) a sixth of the variation in income because many studies are based on young adults, many of whom have not yet reached their peak earning capacity, or even their education. On pg 568 of "", Arthur Jensen claims that although the correlation between IQ and income averages a moderate 0.4 (one sixth or 16% of the variance), the relationship increases with age, and peaks at middle age when people have reached their maximum career potential. In the book, "A Question of Intelligence", Daniel Seligman cites an IQ income correlation of 0.5 (25% of the variance).
A 2002 study further examined the impact of non-IQ factors on income and concluded that an individual's location, inherited wealth, race, and schooling are more important as factors in determining income than IQ.
Crime.
The American Psychological Association's 1995 report "Intelligence: Knowns and Unknowns" stated that the correlation between IQ and crime was −0.2. It was −0.19 between IQ scores and number of juvenile offenses in a large Danish sample; with social class controlled, the correlation dropped to −0.17. A correlation of 0.20 means that the explained variance is 4%. It is important to realize that the causal links between psychometric ability and social outcomes may be indirect. Children with poor scholastic performance may feel alienated. Consequently, they may be more likely to engage in delinquent behavior, compared to other children who do well.
In his book "" (1998), Arthur Jensen cited data which showed that, regardless of race, people with IQs between 70 and 90 have higher crime rates than people with IQs below or above this range, with the peak range being between 80 and 90.
The 2009 "Handbook of Crime Correlates" stated that reviews have found that around eight IQ points, or 0.5 SD, separate criminals from the general population, especially for persistent serious offenders. It has been suggested that this simply reflects that "only dumb ones get caught" but there is similarly a negative relation between IQ and self-reported offending. That children with conduct disorder have lower IQ than their peers "strongly argues" for the theory.
A study of the relationship between US county-level IQ and US county-level crime rates found that higher average IQs were associated with lower levels of property crime, burglary, larceny rate, motor vehicle theft, violent crime, robbery, and aggravated assault. These results were not "confounded by a measure of concentrated disadvantage that captures the effects of race, poverty, and other social disadvantages of the county."
The American Psychological Association's 1995 report "Intelligence: Knowns and Unknowns" stated that the correlations for most "negative outcome" variables are typically smaller than 0.20, which means that the explained variance is less than 4%.
Tambs "et al." found that occupational status, educational attainment, and IQ are individually heritable; and further found that "genetic variance influencing educational attainment ... contributed approximately one-fourth of the genetic variance for occupational status and nearly half the genetic variance for IQ." In a sample of U.S. siblings, Rowe "et al." report that the inequality in education and income was predominantly due to genes, with shared environmental factors playing a subordinate role.
Real-life accomplishments.
There is considerable variation within and overlap among these categories. People with high IQs are found at all levels of education and occupational categories. The biggest difference occurs for low IQs with only an occasional college graduate or professional scoring below 90.
Group differences.
Among the most controversial issues related to the study of intelligence is the observation that intelligence measures such as IQ scores vary between ethnic and racial groups and sexes. While there is little scholarly debate about the "existence" of some of these differences, their "causes" remain highly controversial both within academia and in the public sphere.
Sex.
Most IQ tests are constructed so that there are no overall score differences between females and males. Popular IQ batteries such as the WAIS and the WISC-R are also constructed in order to eliminate sex differences. In a paper presented at the International Society for Intelligence Research in 2002, it was pointed out that because test constructors and the Educational Testing Service (which developed the SAT) often eliminate items showing marked sex differences in order to reduce the perception of bias, the "true sex" difference is masked. Items like the MRT and RT tests that show a male advantage in IQ are often removed.
Race and ethnicity.
The 1996 Task Force investigation on Intelligence sponsored by the American Psychological Association concluded that there are significant variations in IQ across races. The problem of determining the causes underlying this variation relates to the question of the contributions of "nature and nurture" to IQ. Psychologists such as Alan S. Kaufman and Nathan Brody and statisticians such as Bernie Devlin argue that there are insufficient data to conclude that this is because of genetic influences. A review article published in 2012 by leading scholars on human intelligence concluded, after reviewing the prior research literature, that group differences in IQ are best understood as environmental in origin.
Public policy.
In the United States, certain public policies and laws regarding military service,
education, public benefits,
capital punishment, and employment incorporate an individual's IQ into their decisions. However, in the case of Griggs v. Duke Power Co. in 1971, for the purpose of minimizing employment practices that disparately impacted racial minorities, the U.S. Supreme Court banned the use of IQ tests in employment, except when linked to job performance via a job analysis. Internationally, certain public policies, such as improving nutrition and prohibiting neurotoxins, have as one of their goals raising, or preventing a decline in, intelligence.
A diagnosis of intellectual disability is in part based on the results of IQ testing. Borderline intellectual functioning is a categorization where a person has below average cognitive ability (an IQ of 71–85), but the deficit is not as severe as intellectual disability (70 or below).
In the United Kingdom, the eleven plus exam which incorporated an intelligence test has been used from 1945 to decide, at eleven years of age, which type of school a child should go to. They have been much less used since the widespread introduction of comprehensive schools.
Criticism and views.
Relation with intelligence.
IQ is the most researched attempt at measuring intelligence and by far the most widely used in practical setting. However, although IQ attempts to measure some notion of intelligence, it may fail to act as an accurate measure of "intelligence" in its broadest sense. IQ tests only examine particular areas embodied by the broadest notion of "intelligence", failing to account for certain areas which are also associated with "intelligence" such as creativity or emotional intelligence.
There are critics such as Keith Stanovich who do not dispute the stability of IQ test scores or the fact that they predict certain forms of achievement rather effectively. They do argue, however, that to base a concept of intelligence on IQ test scores alone is to ignore many important aspects of mental ability.
Criticism of "g".
Some scientists dispute IQ entirely. In "The Mismeasure of Man" (1996), paleontologist Stephen Jay Gould criticized IQ tests and argued that they were used for scientific racism. He argued that "g" was a mathematical artifact and criticized:
...the abstraction of intelligence as a single entity, its location within the brain, its quantification as one number for each individual, and the use of these numbers to rank people in a single series of worthiness, invariably to find that oppressed and disadvantaged groups—races, classes, or sexes—are innately inferior and deserve their status.(pp. 24–25)
Arthur Jensen responded:
...what Gould has mistaken for "reification" is neither more nor less than the common practice in every science of hypothesizing explanatory models to account for the observed relationships within a given domain. Well known examples include the heliocentric theory of planetary motion, the Bohr atom, the electromagnetic field, the kinetic theory of gases, gravitation, quarks, Mendelian genes, mass, velocity, etc. None of these constructs exists as a palpable entity occupying physical space.
Jensen also argued that even if "g" were replaced by a model with several intelligences this would change the situation less than expected. He argues that all tests of cognitive ability would continue to be highly correlated with one another and there would still be a black-white gap on cognitive tests.
Psychologist Peter Schönemann persistently criticized IQ, calling it "the IQ myth". He argued that "g" is a flawed theory and that the high heritability estimates of IQ are based on false assumptions.
Robert Sternberg, another significant critic of "g" as the main measure of human cognitive abilities, argued that reducing the concept of intelligence to the measure of "g" does not fully account for the different skills and knowledge types that produce success in human society.
Test bias.
The American Psychological Association's report "Intelligence: Knowns and Unknowns" stated that in the United States IQ tests as predictors of social achievement are not biased against African Americans since they predict future performance, such as school achievement, similarly to the way they predict future performance for Caucasians. While agreeing that IQ tests predict performance equally well for all racial groups (Except Asian Americans), Nicholas Mackintosh also points out that there may still be a bias inherent in IQ testing if the education system is also systematically biased against African Americans, in which case educational performance may in fact also be an underestimation of African American children's cognitive abilities. Earl Hunt points out that while this may be the case that would not be a bias of the test, but of society.
However, IQ tests may well be biased when used in other situations. A 2005 study stated that "differential validity in prediction suggests that the WAIS-R test may contain cultural influences that reduce the validity of the WAIS-R as a measure of cognitive ability for Mexican American students," indicating a weaker positive correlation relative to sampled white students. Other recent studies have questioned the culture-fairness of IQ tests when used in South Africa. Standard intelligence tests, such as the Stanford-Binet, are often inappropriate for autistic children; the alternative of using developmental or adaptive skills measures are relatively poor measures of intelligence in autistic children, and may have resulted in incorrect claims that a majority of autistic children are mentally retarded.
Outdated methodology.
According to a 2006 article by the National Center for Biotechnology Information, contemporary psychological researches often did not reflect substantial recent developments in psychometrics and "bears an uncanny resemblance to the psychometric state of the art as it existed in the 1950s."
"Intelligence: Knowns and Unknowns".
In response to the controversy surrounding "The Bell Curve", the American Psychological Association's Board of Scientific Affairs established a task force in 1995 to write a report on the state of intelligence research which could be used by all sides as a basis for discussion, "". The full text of the report is available through several websites.
In this paper the representatives of the association regret that IQ-related works are frequently written with a view to their political consequences: "research findings were often assessed not so much on their merits or their scientific standing as on their supposed political implications".
The task force concluded that IQ scores do have high predictive validity for individual differences in school achievement. They confirm the predictive validity of IQ for adult occupational status, even when variables such as education and family background have been statistically controlled. They stated that individual differences in intelligence are substantially influenced by both genetics and environment.
The report stated that a number of biological factors, including malnutrition, exposure to toxic substances, and various prenatal and perinatal stressors, result in lowered psychometric intelligence under at least some conditions. The task force agrees that large differences do exist between the average IQ scores of blacks and whites, saying:
The cause of that differential is not known; it is apparently not due to any simple form of bias in the content or administration of the tests themselves. The Flynn effect shows that environmental factors can produce differences of at least this magnitude, but that effect is mysterious in its own right. Several culturally based explanations of the Black/ White IQ differential have been proposed; some are plausible, but so far none has been conclusively supported. There is even less empirical support for a genetic interpretation. In short, no adequate explanation of the differential between the IQ means of Blacks and Whites is presently available.
The APA journal that published the statement, "American Psychologist," subsequently published eleven critical responses in January 1997, several of them arguing that the report failed to examine adequately the evidence for partly genetic explanations.
Dynamic assessment.
A notable and increasingly influential alternative to the wide range of standard IQ tests originated in the writings of psychologist Lev Vygotsky (1896-1934) of his most mature and highly productive period of 1932-1934. The notion of the zone of proximal development that he introduced in 1933, roughly a year before his death, served as the banner for his proposal to diagnose development as the level of actual development that can be measured by the child's independent problem solving and, at the same time, the level of proximal, or potential development that is measured in the situation of moderately assisted problem solving by the child. The maximum level of complexity and difficulty of the problem that the child is capable to solve under some guidance indicates the level of potential development. Then, the difference between the higher level of potential and the lower level of actual development indicates the zone of proximal development. Combination of the two indexes—the level of actual and the zone of the proximal development—according to Vygotsky, provides a significantly more informative indicator of psychological development than the assessment of the level of actual development alone.
The ideas on the zone of development were later developed in a number of psychological and educational theories and practices. Most notably, they were developed under the banner of dynamic assessment that focuses on the testing of learning and developmental potential (for instance, in the work of Reuven Feuerstein and his associates, who has criticized standard IQ testing for its putative assumption or acceptance of "fixed and immutable" characteristics of intelligence or cognitive functioning). Grounded in developmental theories of Vygotsky and Feuerstein, who maintained that human beings are not static entities but are always in states of transition and transactional relationships with the world, dynamic assessment received also considerable support in the recent revisions of cognitive developmental theory by Joseph Campione, Ann Brown, and John D. Bransford and in theories of multiple intelligences by Howard Gardner and Robert Sternberg.
Classification.
IQ classification is the practice by IQ test publishers of designating IQ score ranges as various categories with labels such as "superior" or "average." IQ classification was preceded historically by attempts to classify human beings by general ability based on other forms of behavioral observation. Those other forms of behavioral observation are still important for validating classifications based on IQ tests.
High IQ societies.
There are social organizations, some international, which limit membership to people who have scores as high as or higher than the 98th percentile on some IQ test or equivalent. Mensa International is perhaps the best known of these. There are other groups requiring a score above the 99th percentile.
Bibliography.
 

</doc>
<doc id="14894" url="http://en.wikipedia.org/wiki?curid=14894" title="Indian Institute of Technology Kanpur">
Indian Institute of Technology Kanpur

The Indian Institute of Technology Kanpur (commonly known as "IIT Kanpur" or "IITK") is a public research college located in Kanpur, Uttar Pradesh. It was declared to be Institute of National Importance by Government of India under IIT Act.
Established in 1959 as one of the first Indian Institutes of Technology, the institute was created with the assistance of a consortium of nine US research universities as part of the Kanpur Indo-American Programme (KIAP).
History.
IIT Kanpur was established by an Act of Parliament in 1959. The institute was started in December 1959 in a room in the canteen building of the Harcourt Butler Technological Institute at Agricultural Gardens in Kanpur. In 1963, the institute moved to its present location, on the Grand Trunk Road near the village of Kalyanpur in Kanpur district.
During the first ten years of its existence, a consortium of nine US universities (namely M.I.T, University of California, Berkeley, California Institute of Technology, Princeton University, Carnegie Institute of Technology, University of Michigan, Ohio State University,
Case Institute of Technology and Purdue University) helped set up IIT Kanpur's research laboratories and academic programmes under the Kanpur Indo-American Programme (KIAP). The first Director of the Institute was P. K. Kelkar (after whom the Central Library was renamed in 2002).
Under the guidance of economist John Kenneth Galbraith, IIT Kanpur was the first institute in India to offer Computer Science education. The earliest computer courses were started at IIT Kanpur in August 1963 on an IBM 1620 system. The initiative for computer education came from the Electrical Engineering department, then under the chairmanship of Prof. H.K. Kesavan, who was concurrently the chairman of Electrical Engineering and head of the Computer Centre. Prof. Harry Huskey of the University of California, Berkeley, who preceded Kesavan, helped with the computer activity at IIT-Kanpur. In 1971, the institute began an independent academic program in Computer Science and Engineering, leading to M.Tech. and Ph.D. degrees.
In 1972 the KIAP program ended, in part because of tensions due to the U.S. support of Pakistan. Government funding was also reduced as a reaction to the sentiment that the IIT's were contributing to the brain drain.
Campus.
IIT Kanpur panorama from Hall 7
IIT Kanpur is located on the Grand Trunk Road, 15 km west of Kanpur City and measures close to 420 ha. This land was donated by the Government of Uttar Pradesh in 1960 and by March 1963 the Institute had moved to its current location.
The institute has around 6478 students with 3938 undergraduate students and 2540 postgraduate students and about 500 research associates.
Noida Extension centre.
IIT Kanpur is to open an extension centre in Noida with the plan of making a small convention centre there for supporting outreach activities. Its foundation was laid on 4 December 2012 on 5 acres of land allocated by Uttar Pradesh state government in the of Noida city, which is less than an hour`s journey from New Delhi and the Indira Gandhi International Airport. The cost of construction is estimated to be about 25 crores. The new campus will have an auditorium, seminar halls for organising national and international conferences and an International Relations Office along with a 7-storey guest house. Several short-term management courses and refresher courses meant for distance learning will be available at the extension center. 
Academic Area.
Institute's Academic Area comprises academic buildings and facilities such as the PK Kelkar Library, Computer Centre, National Wind Tunnel Facility and SIDBI Innovation and Incubation Centre. It also houses faculty offices, laboratories and administrative buildings. The academic area is connected by a corridor which links all the major buildings.
IIT Kanpur has 10 boys' hostels, 2 girls' hostels and one girls' residence tower all together called the 'Halls of Residence'.
Students' Activity Centre (SAC).
SAC is the hub for most of the major extra-curricular activities. There are two SAC buildings, the older building(Old-SAC) and the other (New-SAC), which was recently built. New-SAC has an open-air theatre (OAT) with a capacity of over 1400 people. The SAC is maintained and managed by the Students' Gymkhana at IIT Kanpur.
Helicopter service.
On 1 June 2013, a helicopter ferry service was started at IIT Kanpur run by Pawan Hans Helicopters Limited. In its initial run the service connects IIT Kanpur to Lucknow, but it is planned to later extend it to New Delhi. Currently there are two flights daily to and from Lucknow Airport with a duration of 25 minutes. Lucknow Airport operates both international and domestic flights to major cities. IIT Kanpur is the first academic institution in the country to provide such a service.
Admissions.
Undergraduate admissions until 2012 were being done through the national-level Indian Institute of Technology Joint Entrance Examination (IIT-JEE). Following the Ministry of Human Resource Development's decision to replace IIT-JEE with a common engineering entrance examination, IIT Kanpur's admissions are now based on JEE (Joint Entrance Examination) -Advanced level along with other IITs.
Postgraduate admissions are made through the Graduate Aptitude Test in Engineering.
Student life.
Students' Gymkhana.
The Students' Gymkhana is the students' government organization of IIT Kanpur, established in 1962.
The Students' Gymkhana functions mainly through the Students' Senate, an elected student representative body composed of Senators elected from each batch and the five elected executives:
The number of Senators in the Students' Senate is around 50-55. A senator is elected for every 100 students of IIT Kanpur.
The meetings of the Students' Senate are chaired by the Convener, Students' Senate, who is elected by the Senate.
The Senate lays down the guidelines for the functions of the executives, their associated councils, the Gymkhana Festivals and other matters pertaining to the Student body at large.
The Students' Senate has a say in the policy and decision making bodies of the institute. The President, Students' Gymkhana and the Convener, Students' Senate are special Invitees to the Institute Academic Senate. The President is usually invited to the meetings of the Board of Governors when matters affecting students are being discussed. Nominees of the Students' Senate are also members of the various standing Committees of the Institute Senate including the disciplinary committee, the Undergraduate and Postgraduate committee, the scholarship committee etc. All academic departments have Departmental Undergraduate and Post Graduate Committees consisting of members of the faculty and nominees of the Students' Gymkhana.
Student publications.
The students have a student magazine called 'The Campus Magazine', a monthly newspaper, "Vox Populi" and "NERD", Scientia, a science and technology magazine.
Student voluntary services.
The students participate voluntarily in social services and organisations, "Prayas" being one of them, where they involve local underprivileged children in educational activities during their free time, conduct awareness programs and provide for campus visits.Another one is Vivekananda Samiti, being one of the oldest clubs under the Students' Gymkhana functioning for over 40 years, thus spreading the message of Swami Vivekananda among the campus residents.
Convocation.
Every year convocation is held in the month of June, where degrees are awarded to graduating students.The latest was held on 18 June 2014.
Rankings.
Internationally, IIT Kanpur has been ranked 300th in the QS World University Rankings of 2014, and 52nd in the QS Asian University Rankings of 2013. In India rankings, among engineering colleges, it was ranked first in 2014, 2012, 2011 & 2010, and second in 2013 by "India Today", and fourth by "Outlook India" in 2012, though it achieved highest points in academic excellence category in "Outlook India" survey . In the "Mint" Government Colleges survey of 2009 it ranked second.
Academic Bodies and Activities.
IIT Kanpur offers various courses on management and social sciences.
Undergraduate.
IIT Kanpur offers four-year B.Tech programs in Aerospace Engineering, Biological Sciences and Bio-engineering, Chemical Engineering, Civil Engineering, Computer Science and Engineering, Electrical Engineering, Materials Science and Engineering and Mechanical Engineering. The admission to these programs is procured through Joint Entrance Examination. IITK offers admission only to Bachelor's degree now (discontinuing the integrated course programs), but it can be extended by 1 year to make it integrated, depending on the choice of student and based on his/her performance there at undergraduate level. IIT Kanpur also offers four-year B.S. Programs in Pure and Applied Sciences (Mathematics, Physics and Chemistry in particular) and in Economics.
New academic system.
From 2011, IIT Kanpur has started offering a four-year BS program in sciences and has kept its B.Tech Program intact. Entry to the five-year M.Tech/MBA programs and Dual degree programme will be done based on the CPI of students instead of JEE rank. In order to reduce the number of student exams, IIT Kanpur has also abolished the earlier system of conducting two mid-term examinations. Instead, only two examinations (plus two quizzes in most courses depending on the instructor-in-charge, one before mid-semesters and the other after the mid-semesters and before the end-semesters examination), one between the semester and other towards the end of it would be held from the academic session starting July 2011 onward as per Academic Review Committee's recommendations.
Postgraduate.
Postgraduate courses in Engineering offer Master of Technology (M.Tech) and Ph.D. degrees. The institute also offers two tier M.Sc. courses in areas of basic sciences in which students are admitted through JAM exam. The institute also offers M.Des. (2 years), M.B.A. (2 years) and M.Sc. (2 years) degrees. Admissions to M. Tech is made once a year through Graduate Aptitude Test in Engineering. Admissions to M. Des are made once a year through both Graduate Aptitude Test in Engineering(GATE) and Common Entrance Exam for Design(CEED). Until 2011, admissions to the M.B.A. program were accomplished through the Joint Management Entrance Test (JMET), held yearly, and followed by a Group Discussion/Personal Interview process. In 2011, JMET was replaced by Common Admission Test (CAT).
Departments.
The academic departments at IIT Kanpur are:
Laboratories and other facilities.
The campus is spread over an area of 4.3 km2. Facilities include the National Wind Tunnel Facility. Other large research centres include the Advanced Centre for Material Science, a Bio-technology centre, the Advanced Centre for Electronic Systems, and the Samtel Centre for Display Technology, Centre for Mechatronics, Centre for Laser Technology, Prabhu Goel Research Centre for Computer and Internet Security, Facility for Ecological and Analytical Testing. The departments have their own libraries.
The institute has its own airfield, for flight testing and gliding.
PK Kelkar Library (formerly Central Library) is an academic library of the institute with a collection of more than 300,000 volumes, and subscriptions to more than 1,000 periodicals. The library was renamed to its present name in 2003 after Dr. P K Kelkar, the first director of the institute. It is housed in a three-story building, with a total floor area of 6973 square metres. The Abstracting and Indexing periodicals, Microform and CD-ROM databases, technical reports, Standards and thesis are in the library. Each year, about 4,500 books and journal volumes are added to the library.
The Computer Centre has about 200-300 Linux terminals and more than 100 Windows-NT terminals supported by the PARAM 10000 supercomputer, and is continuously available to the students for academic work and recreation. It hosts about 50 SUN workstations. 
IIT Kanpur has set up the SIDBI Innovation and Incubation Centre(SIIC) in collaboration with the Small Industries development Bank of India (SIDBI) aiming to aid innovation, research, and entrepreneurial activities in technology-based areas. SIIC helps business Start-ups to develop their ideas into commercially viable products. SIIC conducted the Nettech Summer Program 2007 for engineering students from all over India at IIT Kanpur Campus.
A team of students, working under the guidance of faculty members of the institute and scientists of Indian Space Research Organisation (ISRO) have designed and built India's first nano satellite Jugnu, which was successfully launched in orbit on 12 Oct 2011 by ISRO's PSLV-C18.
Students' research related activity.
Research is controlled by the Office of the Dean of Research and Development. Under the aegis of the Office the students publish the quarterly NERD Magazine (Notes on Engineering Research and Development) which publishes scientific and technical content created by students. Articles may be original work done by students in the form of hobby projects, term projects, internships, or theses. Articles of general interest which are informative but do not reflect original work are also accepted.
Along with the magazine a student research organisation, PoWER (Promotion of Work Experience and Research) has been started. Under it several independent student groups are working on projects like the Lunar Rover for ISRO, alternate energy solutions under the Group for Environment and Energy Engineering, ICT solutions through a group Young Engineers, solution for diabetes, green community solutions through ideas like zero water and zero waste quality air approach. Through BRaIN (Biological Research and Innovation Network) students interested in solving biological problems get involved in research projects like genetically modifying fruit flies to study molecular systems and developing bio-sensors to detect alcohol levels. A budget of Rs 1.5 to 2 crore has been envisaged to support student projects that demonstrate technology.
The students of IIT Kanpur made a nano satellite called Jugnu, which was given by president Pratibha Patil to ISRO for launch. Jugnu is a remote sensing satellite which will be operated by the Indian Institute of Technology Kanpur. It is a nanosatellite which will be used to provide data for agriculture and disaster monitoring. It is a 3-kilogram (6.6 lb) spacecraft, which measures 34 centimetres (13 in) in length by 10 centimetres (3.9 in) in height and width. Its development programme cost around 25 million rupees. It has a design life of one year.
Jugnu's primary instrument is the Micro Imaging System, a near infrared camera which will be used to observe vegetation. It also carries a GPS receiver to aid tracking, and is intended to demonstrate a microelectromechanical inertial measurement unit.
IITK motorsports is the biggest and most comprehensive student initiative of the college, founded in January 2011. It is a group of students from varied disciplines who aim at designing and fabricating a Formula-style race car for international Formula SAE 
(Society of Automotive Engineers) events. Most of the components of the car, except the engine, tyres and wheel rims, are designed and manufactured by the team members themselves. The car is designed to provide maximum performance under the constraints of the event, while ensuring the driveability, reliability, driver safety and aesthetics of the car are not compromised.
The institute is part of the European Research and Education Collaboration with Asia (EURECA) programme since 2008.
External links.
 

</doc>
<doc id="14895" url="http://en.wikipedia.org/wiki?curid=14895" title="Insulin">
Insulin

Insulin "(from the Latin, insula meaning island)" is a peptide hormone produced by beta cells in the pancreas. It regulates the metabolism of carbohydrates and fats by promoting the absorption of glucose from the blood to skeletal muscles and fat tissue and by causing fat to be stored rather than used for energy. Insulin also inhibits the production of glucose by the liver.
Except in the presence of the metabolic disorder diabetes mellitus and metabolic syndrome, insulin is provided within the body in a constant proportion to remove excess glucose from the blood, which otherwise would be toxic. When blood glucose levels fall below a certain level, the body begins to use stored glucose as an energy source through glycogenolysis, which breaks down the glycogen stored in the liver and muscles into glucose, which can then be utilized as an energy source. As a central metabolic control mechanism, its status is also used as a control signal to other body systems (such as amino acid uptake by body cells). In addition, it has several other anabolic effects throughout the body.
When control of insulin levels fails, diabetes mellitus can result. As a consequence, insulin is used medically to treat some forms of diabetes mellitus. Patients with type 1 diabetes depend on external insulin (most commonly injected subcutaneously) for their survival because the hormone is no longer produced internally. Patients with type 2 diabetes are often insulin resistant and, because of such resistance, may suffer from a "relative" insulin deficiency. Some patients with type 2 diabetes may eventually require insulin if dietary modifications or other medications fail to control blood glucose levels adequately. Over 40% of those with Type 2 diabetes require insulin as part of their diabetes management plan.
Insulin is a very old protein that may have originated more than a billion years ago. The molecular origins of insulin go at least as far back as the simplest unicellular eukaryotes. Apart from animals, insulin-like proteins are also known to exist in Fungi and Protista kingdoms. The human insulin protein is composed of 51 amino acids, and has a molecular weight of 5808 Da. It is a dimer of an A-chain and a B-chain, which are linked together by disulfide bonds. Insulin's structure varies slightly between species of animals. Insulin from animal sources differs somewhat in "strength" (in carbohydrate metabolism control effects) from that in humans because of those variations. Porcine insulin is especially close to the human version.
Gene.
The preproinsulin precursor of insulin is encoded by the "INS" gene.
Alleles.
A variety of mutant alleles with changes in the coding region have been identified. A read-through gene, INS-IGF2, overlaps with this gene at the 5' region and with the IGF2 gene at the 3' region.
Regulation.
Several regulatory sequences in the promoter region of the human insulin gene bind to transcription factors. In general, the A-boxes bind to Pdx1 factors, E-boxes bind to NeuroD, C-boxes bind to MafA, and cAMP response elements to CREB. There are also silencers that inhibit transcription.
Protein structure.
Within vertebrates, the amino acid sequence of insulin is strongly conserved. Bovine insulin differs from human in only three amino acid residues, and porcine insulin in one. Even insulin from some species of fish is similar enough to human to be clinically effective in humans. Insulin in some invertebrates is quite similar in sequence to human insulin, and has similar physiological effects. The strong homology seen in the insulin sequence of diverse species suggests that it has been conserved across much of animal evolutionary history. The C-peptide of proinsulin (discussed later), however, differs much more among species; it is also a hormone, but a secondary one.
The primary structure of bovine insulin was first determined by Frederick Sanger in 1951. After that, this polypeptide was synthesized independently by several groups. The 3-dimensional structure of insulin was determined by X-ray crystallography in Dorothy Hodgkin's laboratory in 1969 (PDB file 1ins).
Insulin is produced and stored in the body as a hexamer (a unit of six insulin molecules), while the active form is the monomer. The hexamer is an inactive form with long-term stability, which serves as a way to keep the highly reactive insulin protected, yet readily available. The hexamer-monomer conversion is one of the central aspects of insulin formulations for injection. The hexamer is far more stable than the monomer, which is desirable for practical reasons; however, the monomer is a much faster-reacting drug because diffusion rate is inversely related to particle size. A fast-reacting drug means insulin injections do not have to precede mealtimes by hours, which in turn gives people with diabetes more flexibility in their daily schedules. Insulin can aggregate and form fibrillar interdigitated beta-sheets. This can cause injection amyloidosis, and prevents the storage of insulin for long periods.
Synthesis, physiological effects, and degradation.
Synthesis.
Insulin is produced in the pancreas and released when any of several stimuli are detected. These stimuli include ingested protein and glucose in the blood produced from digested food. Carbohydrates can be polymers of simple sugars or the simple sugars themselves. If the carbohydrates include glucose, then that glucose will be absorbed into the bloodstream and blood glucose level will begin to rise. In target cells, insulin initiates a signal transduction, which has the effect of increasing glucose uptake and storage. Finally, insulin is degraded, terminating the response.
In mammals, insulin is synthesized in the pancreas within the β-cells of the islets of Langerhans. One million to three million islets of Langerhans (pancreatic islets) form the endocrine part of the pancreas, which is primarily an exocrine gland. The endocrine portion accounts for only 2% of the total mass of the pancreas. Within the islets of Langerhans, beta cells constitute 65–80% of all the cells.
Insulin consists of two polypeptide chains, the A- and B- chains, linked together by disulfide bonds. It is however first synthesized as a single polypeptide called preproinsulin in pancreatic β-cells. Preproinsulin contains a 24-residue signal peptide which directs the nascent polypeptide chain to the rough endoplasmic reticulum (RER). The signal peptide is cleaved as the polypeptide is translocated into lumen of the RER, forming proinsulin. In the RER the proinsulin folds into the correct conformation and 3 disulfide bonds are formed. About 5–10 min after its assembly in the endoplasmic reticulum, proinsulin is transported to the trans-Golgi network (TGN) where immature granules are formed. Transport to the TGN may take about 30 min.
Proinsulin undergoes maturation into active insulin through the action of cellular endopeptidases known as prohormone convertases (PC1 and PC2), as well as the exoprotease carboxypeptidase E. The endopeptidases cleave at 2 positions, releasing a fragment called the C-peptide, and leaving 2 peptide chains, the B- and A- chains, linked by 2 disulfide bonds. The cleavage sites are each located after a pair of basic residues (lysine-64 and arginine-65, and arginine-31 and -32). After cleavage of the C-peptide, these 2 pairs of basic residues are removed by the carboxypeptidase. The C-peptide is the central portion of proinsulin, and the primary sequence of proinsulin goes in the order "B-C-A" (the B and A chains were identified on the basis of mass and the C-peptide was discovered later).
The resulting mature insulin is packaged inside mature granules waiting for metabolic signals (such as leucine, arginine, glucose and mannose) and vagal nerve stimulation to be exocytosed from the cell into the circulation.
The endogenous production of insulin is regulated in several steps along the synthesis pathway:
Insulin and its related proteins have been shown to be produced inside the brain, and reduced levels of these proteins are linked to Alzheimer's disease.
Release.
Beta cells in the islets of Langerhans release insulin in two phases. The first phase release is rapidly triggered in response to increased blood glucose levels. The second phase is a sustained, slow release of newly formed vesicles triggered independently of sugar. The description of first phase release is as follows:
This is the primary mechanism for release of insulin. Other substances known to stimulate insulin release include the amino acids arginine and leucine, parasympathetic release of acetylcholine (via phospholipase C), sulfonylurea, cholecystokinin (CCK, via phospholipase C), and the gastrointestinally derived incretins glucagon-like peptide-1 (GLP-1) and glucose-dependent insulinotropic peptide (GIP).
Release of insulin is strongly inhibited by the stress hormone norepinephrine (noradrenaline), which leads to increased blood glucose levels during stress. It appears that release of catecholamines by the sympathetic nervous system has conflicting influences on insulin release by beta cells, because insulin release is inhibited by α2-adrenergic receptors and stimulated by β2-adrenergic receptors. The net effect of norepinephrine from sympathetic nerves and epinephrine from adrenal glands on insulin release is inhibition due to dominance of the α-adrenergic receptors.
When the glucose level comes down to the usual physiologic value, insulin release from the β-cells slows or stops. If blood glucose levels drop lower than this, especially to dangerously low levels, release of hyperglycemic hormones (most prominently glucagon from islet of Langerhans alpha cells) forces release of glucose into the blood from cellular stores, primarily liver cell stores of glycogen. By increasing blood glucose, the hyperglycemic hormones prevent or correct life-threatening hypoglycemia.
Evidence of impaired first-phase insulin release can be seen in the glucose tolerance test, demonstrated by a substantially elevated blood glucose level at 30 minutes, a marked drop by 60 minutes, and a steady climb back to baseline levels over the following hourly time points.
Oscillations.
Even during the digestion, in general, one or two hours following a meal, insulin release from the pancreas is not continuous, but oscillates with a period of 3–6 minutes, changing from generating a blood insulin concentration more than about 800 pmol/l to less than 100 pmol/l. This is thought to avoid downregulation of insulin receptors in target cells, and to assist the liver in extracting insulin from the blood. This oscillation is important to consider when administering insulin-stimulating medication, since it is the oscillating blood concentration of insulin release, which should, ideally, be achieved, not a constant high concentration. This may be achieved by delivering insulin rhythmically to the portal vein or by islet cell transplantation to the liver. It is hoped that future insulin pumps will address this characteristic. (See also Pulsatile Insulin.)
Blood content.
The blood content of insulin can be measured in international units, such as µIU/mL or in molar concentration, such as pmol/L, where 1 µIU/mL equals 6.945 pmol/L. A typical blood level between meals is 8–11 μIU/mL (57–79 pmol/L).
Signal transduction.
Special transporter proteins in cell membranes allow glucose from the blood to enter a cell. These transporters are, indirectly, under blood insulin's control in certain body cell types (e.g., muscle cells). Low levels of circulating insulin, or its absence, will prevent glucose from entering those cells (e.g., in type 1 diabetes). More commonly, however, there is a decrease in the sensitivity of cells to insulin (e.g., the reduced insulin sensitivity characteristic of type 2 diabetes), resulting in decreased glucose absorption. In either case, there is 'cell starvation' and weight loss, sometimes extreme. In a few cases, there is a defect in the release of insulin from the pancreas. Either way, the effect is the same: elevated blood glucose levels.
Activation of insulin receptors leads to internal cellular mechanisms that directly affect glucose uptake by regulating the number and operation of protein molecules in the cell membrane that transport glucose into the cell. The genes that specify the proteins that make up the insulin receptor in cell membranes have been identified, and the structures of the interior, transmembrane section, and the extra-membrane section of receptor have been solved.
Two types of tissues are most strongly influenced by insulin, as far as the stimulation of glucose uptake is concerned: muscle cells (myocytes) and fat cells (adipocytes). The former are important because of their central role in movement, breathing, circulation, etc., and the latter because they accumulate excess food energy against future needs. Together, they account for about two-thirds of all cells in a typical human body.
Insulin binds to the extracellular portion of the alpha subunits of the insulin receptor. This, in turn, causes a conformational change in the insulin receptor that activates the kinase domain residing on the intracellular portion of the beta subunits. The activated kinase domain autophosphorylates tyrosine residues on the C-terminus of the receptor as well as tyrosine residues in the IRS-1 protein.
After the signal has been produced, termination of signaling is then needed. As mentioned below in the section on degradation, endocytosis and degradation of the receptor bound to insulin is a main mechanism to end signaling. In addition, signaling can be terminated by dephosphorylation of the tyrosine residues by tyrosine phosphatases. Serine/Threonine kinases are also known to reduce the activity of insulin. Finally, with insulin action being associated with the number of receptors on the plasma membrane, a decrease in the amount of receptors also leads to termination of insulin signaling.
The structure of the insulin–insulin receptor complex has been determined using the techniques of X-ray crystallography.
Physiological effects.
The actions of insulin on the global human metabolism level include:
The actions of insulin (indirect and direct) on cells include:
Insulin also influences other body functions, such as vascular compliance and cognition. Once insulin enters the human brain, it enhances learning and memory and benefits verbal memory in particular. Enhancing brain insulin signaling by means of intranasal insulin administration also enhances the acute thermoregulatory and glucoregulatory response to food intake, suggesting that central nervous insulin contributes to the control of whole-body energy homeostasis in humans. Insulin also has stimulatory effects on gonadotropin-releasing hormone from the hypothalamus, thus favoring fertility.
Degradation.
Once an insulin molecule has docked onto the receptor and effected its action, it may be released back into the extracellular environment, or it may be degraded by the cell. The two primary sites for insulin clearance are the liver and the kidney. The liver clears most insulin during first-pass transit, whereas the kidney clears most of the insulin in systemic circulation. Degradation normally involves endocytosis of the insulin-receptor complex, followed by the action of insulin-degrading enzyme. An insulin molecule produced endogenously by the pancreatic beta cells is estimated to be degraded within about one hour after its initial release into circulation (insulin half-life ~ 4–6 minutes).
Hypoglycemia.
Although other cells can use other fuels (most prominently fatty acids), neurons depend on glucose as a source of energy in the nonstarving human. They do not require insulin to absorb glucose, unlike muscle and adipose tissue, and they have very small internal stores of glycogen. Glycogen stored in liver cells (unlike glycogen stored in muscle cells) can be converted to glucose, and released into the blood, when glucose from digestion is low or absent, and the glycerol backbone in triglycerides can also be used to produce blood glucose.
Sufficient lack of glucose and scarcity of these sources of glucose can dramatically make itself manifest in the impaired functioning of the central nervous system: dizziness, speech problems, and even loss of consciousness. Low blood glucose level is known as hypoglycemia or, in cases producing unconsciousness, "hypoglycemic coma" (sometimes termed "insulin shock" from the most common causative agent). Endogenous causes of insulin excess (such as an insulinoma) are very rare, and the overwhelming majority of insulin excess-induced hypoglycemia cases are iatrogenic and usually accidental. A few cases of murder, attempted murder, or suicide using insulin overdoses have been reported, but most insulin shocks appear to be due to errors in dosage of insulin (e.g., 20 units instead of 2) or other unanticipated factors (did not eat as much as anticipated, or exercised more than expected, or unpredicted kinetics of the subcutaneously injected insulin itself).
Possible causes of hypoglycemia include:
Diseases and syndromes.
There are several conditions in which insulin disturbance is pathologic:
Medication uses.
Biosynthetic human insulin (insulin human rDNA, INN) for clinical use is manufactured by recombinant DNA technology. Biosynthetic human insulin has increased purity when compared with extractive animal insulin, enhanced purity reducing antibody formation. Researchers have succeeded in introducing the gene for human insulin into plants as another method of producing insulin ("biopharming") in safflower. This technique is anticipated to reduce production costs.
Several analogs of human insulin are available. These insulin analogs are closely related to the human insulin structure, and were developed for specific aspects of glycemic control in terms of fast action (prandial insulins) and long action (basal insulins). The first biosynthetic insulin analog was developed for clinical use at mealtime (prandial insulin), Humalog (insulin lispro), it is more rapidly absorbed after subcutaneous injection than regular insulin, with an effect 15 minutes after injection. Other rapid-acting analogues are NovoRapid and Apidra, with similar profiles. All are rapidly absorbed due to sequence that will reduce formation of dimers and hexamers (monomeric insulins are more rapidly absorbed). Fast acting insulins do not require the injection-to-meal interval previously recommended for human insulin and animal insulins. The other type is long acting insulin; the first of these was Lantus (insulin glargine). These have a steady effect for an extended period from 18 to 24 hours. Likewise, another protracted insulin analogue (Levemir) is based on a fatty acid acylation approach. A myristyric acid molecule is attached to this analogue, which in turn associates the insulin molecule to the abundant serum albumin, which in turn extends the effect and reduces the risk of hypoglycemia. Both protracted analogues need to be taken only once daily, and are used for type 1 diabetics as the basal insulin. A combination of a rapid acting and a protracted insulin is also available, making it more likely for patients to achieve an insulin profile that mimics that of the body´s own insulin release.
Insulin is usually taken as subcutaneous injections by single-use syringes with needles, via an insulin pump, or by repeated-use insulin pens with disposable needles. Inhaled insulin is also available in U.S. market now. 
Unlike many medicines, insulin currently cannot be taken orally because, like nearly all other proteins introduced into the gastrointestinal tract, it is reduced to fragments (even single amino acid components), whereupon all activity is lost. There has been some research into ways to protect insulin from the digestive tract, so that it can be administered orally or sublingually. While experimental, several companies now have various formulations in human clinical trials, and one, the India-based Biocon, has formed an agreement with BMS to produce an oral-insulin alternative.
Zoology.
In 2015 it was reported that the cone snails "Conus geographus" and "Conus tulipa", venomous sea snails that hunt small fish, use modified forms of insulin in their venom cocktails. The insulin toxin, closer in structure to fishes' than to snails' native insulin, slows down the prey fishes by lowering their blood glucose levels.
History.
Discovery.
In 1869, while studying the structure of the pancreas under a microscope, Paul Langerhans, a medical student in Berlin, identified some previously unnoticed tissue clumps scattered throughout the bulk of the pancreas. The function of the "little heaps of cells", later known as the "islets of Langerhans", initially remained unknown, but Edouard Laguesse later suggested they might produce secretions that play a regulatory role in digestion. Paul Langerhans' son, Archibald, also helped to understand this regulatory role. The term "insulin" originates from "insula", the Latin word for islet/island.
In 1889, the Polish-German physician Oskar Minkowski, in collaboration with Joseph von Mering, removed the pancreas from a healthy dog to test its assumed role in digestion. Several days after the removal of the dog's pancreas, Minkowski's animal-keeper noticed a swarm of flies feeding on the dog's urine. On testing the urine, they found sugar, establishing for the first time a relationship between the pancreas and diabetes. In 1901 Eugene Lindsay Opie took another major step forward when he clearly established the link between the islets of Langerhans and diabetes: "Diabetes mellitus . . . is caused by destruction of the islets of Langerhans and occurs only when these bodies are in part or wholly destroyed". Before Opie's work, medical science had clearly established the link between the pancreas and diabetes, but not the specific role of the islets.
Over the next two decades researchers made several attempts to isolate - as a potential treatment - whatever the islets produced. In 1906 George Ludwig Zuelzer achieved partial success in treating dogs with pancreatic extract, but he was unable to continue his work. Between 1911 and 1912, E.L. Scott at the University of Chicago used aqueous pancreatic extracts, and noted "a slight diminution of glycosuria", but was unable to convince his director of his work's value; it was shut down. Israel Kleiner demonstrated similar effects at Rockefeller University in 1915, but World War I interrupted his work and he did not return to it.
In 1916 Nicolae Paulescu, a Romanian professor of physiology at the University of Medicine and Pharmacy in Bucharest, developed an aqueous pancreatic extract which, when injected into a diabetic dog, had a normalizing effect on blood-sugar levels. He had to interrupt his experiments because of World War I, and in 1921 he wrote four papers about his work carried out in Bucharest and his tests on a diabetic dog. Later that year, he published "Research on the Role of the Pancreas in Food Assimilation".
Extraction and purification.
In October 1920, Canadian Frederick Banting concluded that it was the very digestive secretions that Minkowski had originally studied that were breaking down the islet secretion(s), thereby making it impossible to extract successfully. He jotted a note to himself: "Ligate pancreatic ducts of the dog. Keep dogs alive till acini degenerate leaving islets. Try to isolate internal secretion of these and relieve glycosurea."
The idea was the pancreas's internal secretion, which, it was supposed, regulates sugar in the bloodstream, might hold the key to the treatment of diabetes. A surgeon by training, Banting knew certain arteries could be tied off that would lead to atrophy of most of the pancreas, while leaving the islets of Langerhans intact. He theorized a relatively pure extract could be made from the islets once most of the rest of the pancreas was gone.
In the spring of 1921, Banting traveled to Toronto to explain his idea to J.J.R. Macleod, who was Professor of Physiology at the University of Toronto, and asked Macleod if he could use his lab space to test the idea. Macleod was initially skeptical, but eventually agreed to let Banting use his lab space while he was on holiday for the summer. He also supplied Banting with ten dogs on which to experiment, and two medical students, Charles Best and Clark Noble, to use as lab assistants, before leaving for Scotland. Since Banting required only one lab assistant, Best and Noble flipped a coin to see which would assist Banting for the first half of the summer. Best won the coin toss, and took the first shift as Banting's assistant. Loss of the coin toss may have proved unfortunate for Noble, given that Banting decided to keep Best for the entire summer, and eventually shared half his Nobel Prize money and a large part of the credit for the discovery of insulin with the winner of the toss. Had Noble won the toss, his career might have taken a different path. Banting's method was to tie a ligature around the pancreatic duct; when examined several weeks later, the pancreatic digestive cells had died and been absorbed by the immune system, leaving thousands of islets. They then isolated an extract from these islets, producing what they called "isletin" (what we now know as insulin), and tested this extract on the dogs starting July 27. Banting and Best were then able to keep a pancreatectomized dog named Marjorie alive for the rest of the summer by injecting her with the crude extract they had prepared. Removal of the pancreas in test animals in essence mimics diabetes, leading to elevated blood glucose levels. Marjorie was able to remain alive because the extracts, containing isletin, were able to lower her blood glucose levels.
Banting and Best presented their results to Macleod on his return to Toronto in the fall of 1921, but Macleod pointed out flaws with the experimental design, and suggested the experiments be repeated with more dogs and better equipment. He then supplied Banting and Best with a better laboratory, and began paying Banting a salary from his research grants. Several weeks later, the second round of experiments was also a success; and Macleod helped publish their results privately in Toronto that November. However, they needed six weeks to extract the isletin, which forced considerable delays. Banting suggested they try to use fetal calf pancreas, which had not yet developed digestive glands; he was relieved to find this method worked well. With the supply problem solved, the next major effort was to purify the extract. In December 1921, Macleod invited the biochemist James Collip to help with this task, and, within a month, the team felt ready for a clinical test.
On January 11, 1922, Leonard Thompson, a 14-year-old diabetic who lay dying at the Toronto General Hospital, was given the first injection of insulin. However, the extract was so impure, Thompson suffered a severe allergic reaction, and further injections were canceled. Over the next 12 days, Collip worked day and night to improve the ox-pancreas extract, and a second dose was injected on January 23. This was completely successful, not only in having no obvious side-effects but also in completely eliminating the glycosuria sign of diabetes. The first American patient was Elizabeth Hughes Gossett, the daughter of the governor of New York. The first patient treated in the U.S. was future woodcut artist James D. Havens; Dr. John Ralston Williams imported insulin from Toronto to Rochester, New York, to treat Havens.
Children dying from diabetic ketoacidosis were kept in large wards, often with 50 or more patients in a ward, mostly comatose. Grieving family members were often in attendance, awaiting the (until then, inevitable) death.
In one of medicine's more dramatic moments, Banting, Best, and Collip went from bed to bed, injecting an entire ward with the new purified extract. Before they had reached the last dying child, the first few were awakening from their coma, to the joyous exclamations of their families.
Banting and Best never worked well with Collip, regarding him as something of an interloper, and Collip left the project soon after.
Over the spring of 1922, Best managed to improve his techniques to the point where large quantities of insulin could be extracted on demand, but the preparation remained impure. The drug firm Eli Lilly and Company had offered assistance not long after the first publications in 1921, and they took Lilly up on the offer in April. In November, Lilly made a major breakthrough and was able to produce large quantities of highly refined insulin. Insulin was offered for sale shortly thereafter.
Synthesis.
Purified animal-sourced insulin was the only type of insulin available to diabetics until genetic advances occurred later with medical research. The amino acid structure of insulin was characterized in the early 1950s by Frederick Sanger, and the first synthetic insulin was produced simultaneously in the labs of Panayotis Katsoyannis at the University of Pittsburgh and Helmut Zahn at RWTH Aachen University in the early 1960s.
The first genetically engineered, synthetic "human" insulin was produced using "E. coli" in 1978 by Arthur Riggs and Keiichi Itakura at the Beckman Research Institute of the City of Hope in collaboration with Herbert Boyer at Genentech. Genentech, founded by Swanson, Boyer and Eli Lilly and Company, went on in 1982 to sell the first commercially available biosynthetic human insulin under the brand name Humulin. The vast majority of insulin currently used worldwide is now biosynthetic recombinant "human" insulin or its analogues.
Recombinant insulin is produced either in yeast (usually "Saccharomyces cerevisiae") or "E. coli". In yeast, insulin may be engineered as a single-chain protein with a KexII endoprotease (a yeast homolog of PCI/PCII) site that separates the insulin A chain from a c-terminally truncated insulin B chain. A chemically synthesized c-terminal tail is then grafted onto insulin by reverse proteolysis using the inexpensive protease trypsin; typically the lysine on the c-terminal tail is protected with a chemical protecting group to prevent proteolysis. The ease of modular synthesis and the relative safety of modifications in that region accounts for common insulin analogs with c-terminal modifications (e.g. lispro, aspart, glulisine). The Genentech synthesis and completely chemical synthesis such as that by Bruce Merrifield are not preferred because the efficiency of recombining the two insulin chains is low, primarily due to competition with the precipitation of insulin B chain.
Nobel Prizes.
The Nobel Prize committee in 1923 credited the practical extraction of insulin to a team at the University of Toronto and awarded the Nobel Prize to two men: Frederick Banting and J.J.R. Macleod. They were awarded the Nobel Prize in Physiology or Medicine in 1923 for the discovery of insulin. Banting, insulted that Best was not mentioned, shared his prize with him, and Macleod immediately shared his with James Collip. The patent for insulin was sold to the University of Toronto for one half-dollar.
The primary structure of insulin was determined by British molecular biologist Frederick Sanger. It was the first protein to have its sequence be determined. He was awarded the 1958 Nobel Prize in Chemistry for this work.
In 1969, after decades of work, Dorothy Hodgkin determined the spatial conformation of the molecule, the so-called tertiary structure, by means of X-ray diffraction studies. She had been awarded a Nobel Prize in Chemistry in 1964 for the development of crystallography.
Rosalyn Sussman Yalow received the 1977 Nobel Prize in Medicine for the development of the radioimmunoassay for insulin.
George Minot, co-recipient of the 1934 Nobel Prize for the development of the first effective treatment for pernicious anemia, had diabetes mellitus. Dr. William Castle observed that the 1921 discovery of insulin, arriving in time to keep Minot alive, was therefore also responsible for the discovery of a cure for pernicious anemia.
Nobel Prize controversy.
The work published by Banting, Best, Collip and Macleod represented the preparation of purified insulin extract suitable for use on human patients. Although Paulescu discovered the principles of the treatment his saline extract could not be used on humans, and he was not mentioned in the 1923 Nobel Prize. Professor Ian Murray was particularly active in working to correct "the historical wrong" against Nicolae Paulescu. Murray was a professor of physiology at the Anderson College of Medicine in Glasgow, Scotland, the head of the department of Metabolic Diseases at a leading Glasgow hospital, vice-president of the British Association of Diabetes, and a founding member of the International Diabetes Federation. Murray wrote:
Insufficient recognition has been given to Paulescu, the distinguished Romanian scientist, who at the time when the Toronto team were commencing their research had already succeeded in extracting the antidiabetic hormone of the pancreas and proving its efficacy in reducing the hyperglycaemia in diabetic dogs.
In a recent private communication Professor Tiselius, head of the Nobel Institute, has expressed his personal opinion that Paulescu was equally worthy of the award in 1923.
Further reading.
</dl>

</doc>
<doc id="14896" url="http://en.wikipedia.org/wiki?curid=14896" title="Inductor">
Inductor

An inductor, also called a coil or reactor, is a passive two-terminal electrical component which resists changes in electric current passing through it. It consists of a conductor such as a wire, usually wound into a coil. When a current flows through it, energy is stored temporarily in a magnetic field in the coil. When the current flowing through an inductor changes, the time-varying magnetic field induces a voltage in the conductor, according to Faraday’s law of electromagnetic induction, which opposes the change in current that created it.
An inductor is characterized by its "inductance", the ratio of the voltage to the rate of change of current, which has units of henries (H). Inductors have values that typically range from 1 µH (10−6H) to 1 H. Many inductors have a magnetic core made of iron or ferrite inside the coil, which serves to increase the magnetic field and thus the inductance. Along with capacitors and resistors, inductors are one of the three passive linear circuit elements that make up electric circuits. Inductors are widely used in alternating current (AC) electronic equipment, particularly in radio equipment. They are used to block AC while allowing DC to pass; inductors designed for this purpose are called chokes. They are also used in electronic filters to separate signals of different frequencies, and in combination with capacitors to make tuned circuits, used to tune radio and TV receivers.
Overview.
Inductance ("L") results from the magnetic field around a current-carrying conductor; the electric current through the conductor creates a magnetic flux. Mathematically speaking, inductance is determined by how much magnetic flux "φ" through the circuit is created by a given current "i"
Inductors that have ferromagnetic cores are nonlinear; the inductance changes with the current, in this more general case inductance is defined as
Any wire or other conductor will generate a magnetic field when current flows through it, so every conductor has some inductance. The inductance of a circuit depends on the geometry of the current path as well as the magnetic permeability of nearby materials. An inductor is a component consisting of a wire or other conductor shaped to increase the magnetic flux through the circuit, usually in the shape of a coil or helix. Winding the wire into a coil increases the number of times the magnetic flux lines link the circuit, increasing the field and thus the inductance. The more turns, the higher the inductance. The inductance also depends on the shape of the coil, separation of the turns, and many other factors. By adding a "magnetic core" made of a ferromagnetic material like iron inside the coil, the magnetizing field from the coil will induce magnetization in the material, increasing the magnetic flux. The high permeability of a ferromagnetic core can increase the inductance of a coil by a factor of several thousand over what it would be without it.
Constitutive equation.
Any change in the current through an inductor creates a changing flux, inducing a voltage across the inductor. By Faraday's law of induction, the voltage induced by any change in magnetic flux through the circuit is 
From (1) above 
So inductance is also a measure of the amount of electromotive force (voltage) generated for a given rate of change of current. For example, an inductor with an inductance of 1 henry produces an EMF of 1 volt when the current through the inductor changes at the rate of 1 ampere per second. This is usually taken to be the constitutive relation (defining equation) of the inductor.
The dual of the inductor is the capacitor, which stores energy in an electric field rather than a magnetic field. Its current-voltage relation is obtained by exchanging current and voltage in the inductor equations and replacing L with the capacitance C.
Lenz's law.
The polarity (direction) of the induced voltage is given by Lenz's law, which states that it will be such as to oppose the change in current. For example, if the current through an inductor is increasing, the induced voltage will be positive at the terminal through which the current enters and negative at the terminal through which it leaves. The energy from the external circuit necessary to overcome this potential "hill" is being stored in the magnetic field of the inductor; the inductor is said to be "charging" or "energizing". If the current is decreasing, the induced voltage will be negative at the terminal through which the current enters. Energy from the magnetic field is being returned to the circuit; the inductor is said to be "discharging".
Ideal and real inductors.
In circuit theory, inductors are idealized as obeying the mathematical relation (2) above precisely. An "ideal inductor" has inductance, but no resistance or capacitance, and does not dissipate or radiate energy. However real inductors have side effects which cause their behavior to depart from this simple model. They have resistance (due to the resistance of the wire and energy losses in core material), and parasitic capacitance (due to the electric field between the turns of wire which are at slightly different potentials). At high frequencies the capacitance begins to affect the inductor's behavior; at some frequency, real inductors behave as resonant circuits, becoming self-resonant. Above the resonant frequency the capacitive reactance becomes the dominant part of the impedance. At higher frequencies, resistive losses in the windings increase due to skin effect and proximity effect.
Inductors with ferromagnetic cores have additional energy losses due to hysteresis and eddy currents in the core, which increase with frequency. At high currents, iron core inductors also show gradual departure from ideal behavior due to nonlinearity caused by magnetic saturation of the core. An inductor may radiate electromagnetic energy into surrounding space and circuits, and may absorb electromagnetic emissions from other circuits, causing electromagnetic interference (EMI). Real-world inductor applications may consider these parasitic parameters as important as the inductance.
Applications.
Inductors are used extensively in analog circuits and signal processing. Applications range from the use of large inductors in power supplies, which in conjunction with filter capacitors remove residual hums known as the mains hum or other fluctuations from the direct current output, to the small inductance of the ferrite bead or torus installed around a cable to prevent radio frequency interference from being transmitted down the wire. Inductors are used as the energy storage device in many switched-mode power supplies to produce DC current. The inductor supplies energy to the circuit to keep current flowing during the "off" switching periods.
An inductor connected to a capacitor forms a tuned circuit, which acts as a resonator for oscillating current. Tuned circuits are widely used in radio frequency equipment such as radio transmitters and receivers, as narrow bandpass filters to select a single frequency from a composite signal, and in electronic oscillators to generate sinusoidal signals. 
Two (or more) inductors in proximity that have coupled magnetic flux (mutual inductance) form a transformer, which is a fundamental component of every electric utility power grid. The efficiency of a transformer may decrease as the frequency increases due to eddy currents in the core material and skin effect on the windings. The size of the core can be decreased at higher frequencies. For this reason, aircraft use 400 hertz alternating current rather than the usual 50 or 60 hertz, allowing a great saving in weight from the use of smaller transformers.
Inductors are also employed in electrical transmission systems, where they are used to limit switching currents and fault currents. In this field, they are more commonly referred to as reactors.
Because inductors have complicated side effects (detailed below) which cause them to depart from ideal behavior, because they can radiate electromagnetic interference (EMI), and most of all because of their bulk which prevents them from being integrated on semiconductor chips, the use of inductors is declining in modern electronic devices, particularly compact portable devices. Real inductors are increasingly being replaced by active circuits such as the gyrator which can synthesize inductance using capacitors.
Inductor construction.
An inductor usually consists of a coil of conducting material, typically insulated copper wire, wrapped around a core either of plastic or of a ferromagnetic (or ferrimagnetic) material; the latter is called an "iron core" inductor. The high permeability of the ferromagnetic core increases the magnetic field and confines it closely to the inductor, thereby increasing the inductance. Low frequency inductors are constructed like transformers, with cores of electrical steel laminated to prevent eddy currents. 'Soft' ferrites are widely used for cores above audio frequencies, since they do not cause the large energy losses at high frequencies that ordinary iron alloys do. Inductors come in many shapes. Most are constructed as enamel coated wire (magnet wire) wrapped around a ferrite bobbin with wire exposed on the outside, while some enclose the wire completely in ferrite and are referred to as "shielded". Some inductors have an adjustable core, which enables changing of the inductance. Inductors used to block very high frequencies are sometimes made by stringing a ferrite bead on a wire.
Small inductors can be etched directly onto a printed circuit board by laying out the trace in a spiral pattern. Some such planar inductors use a planar core.
Small value inductors can also be built on integrated circuits using the same processes that are used to make transistors. Aluminium interconnect is typically used, laid out in a spiral coil pattern. However, the small dimensions limit the inductance, and it is far more common to use a circuit called a "gyrator" that uses a capacitor and active components to behave similarly to an inductor.
Types of inductor.
Air core inductor.
The term "air core coil" describes an inductor that does not use a magnetic core made of a ferromagnetic material. The term refers to coils wound on plastic, ceramic, or other nonmagnetic forms, as well as those that have only air inside the windings. Air core coils have lower inductance than ferromagnetic core coils, but are often used at high frequencies because they are free from energy losses called core losses that occur in ferromagnetic cores, which increase with frequency. A side effect that can occur in air core coils in which the winding is not rigidly supported on a form is 'microphony': mechanical vibration of the windings can cause variations in the inductance.
Radio frequency inductor.
At high frequencies, particularly radio frequencies (RF), inductors have higher resistance and other losses. In addition to causing power loss, in resonant circuits this can reduce the Q factor of the circuit, broadening the bandwidth. In RF inductors, which are mostly air core types, specialized construction techniques are used to minimize these losses. The losses are due to these effects:
To reduce parasitic capacitance and proximity effect, RF coils are constructed to avoid having many turns lying close together, parallel to one another. The windings of RF coils are often limited to a single layer, and the turns are spaced apart. To reduce resistance due to skin effect, in high-power inductors such as those used in transmitters the windings are sometimes made of a metal strip or tubing which has a larger surface area, and the surface is silver-plated.
Ferromagnetic core inductor.
Ferromagnetic-core or iron-core inductors use a magnetic core made of a ferromagnetic or ferrimagnetic material such as iron or ferrite to increase the inductance. A magnetic core can increase the inductance of a coil by a factor of several thousand, by increasing the magnetic field due to its higher magnetic permeability. However the magnetic properties of the core material cause several side effects which alter the behavior of the inductor and require special construction:
Laminated core inductor.
Low-frequency inductors are often made with laminated cores to prevent eddy currents, using construction similar to transformers. The core is made of stacks of thin steel sheets or laminations oriented parallel to the field, with an insulating coating on the surface. The insulation prevents eddy currents between the sheets, so any remaining currents must be within the cross sectional area of the individual laminations, reducing the area of the loop and thus reducing the energy losses greatly. The laminations are made of low-coercivity silicon steel, to reduce hysteresis losses.
Ferrite-core inductor.
For higher frequencies, inductors are made with cores of ferrite. Ferrite is a ceramic ferrimagnetic material that is nonconductive, so eddy currents cannot flow within it. The formulation of ferrite is xxFe2O4 where xx represents various metals. For inductor cores soft ferrites are used, which have low coercivity and thus low hysteresis losses. Another similar material is powdered iron cemented with a binder.
Toroidal core inductor.
In an inductor wound on a straight rod-shaped core, the magnetic field lines emerging from one end of the core must pass through the air to reenter the core at the other end. This reduces the field, because much of the magnetic field path is in air rather than the higher permeability core material. A higher magnetic field and inductance can be achieved by forming the core in a closed magnetic circuit. The magnetic field lines form closed loops within the core without leaving the core material. The shape often used is a toroidal or doughnut-shaped ferrite core. Because of their symmetry, toroidal cores allow a minimum of the magnetic flux to escape outside the core (called "leakage flux"), so they radiate less electromagnetic interference than other shapes. Toroidal core coils are manufactured of various materials, primarily ferrite, powdered iron and laminated cores.
Choke.
A choke is designed specifically for blocking higher-frequency alternating current (AC) in an electrical circuit, while allowing lower frequency or DC current to pass. It usually consists of a coil of insulated wire often wound on a magnetic core, although some consist of a donut-shaped "bead" of ferrite material strung on a wire. Like other inductors, chokes resist changes to the current passing through them, and so alternating currents of higher frequency, which reverse direction rapidly, are resisted more than currents of lower frequency; the choke's impedance increases with frequency. Its low electrical resistance allows both AC and DC to pass with little power loss, but it can limit the amount of AC passing through it due to its reactance.
Variable inductor.
Probably the most common type of variable inductor today is one with a moveable ferrite magnetic core, which can be slid or screwed in or out of the coil. Moving the core farther into the coil increases the permeability, increasing the magnetic field and the inductance. Many inductors used in radio applications (usually less than 100 MHz) use adjustable cores in order to tune such inductors to their desired value, since manufacturing processes have certain tolerances (inaccuracy). Sometimes such cores for frequencies above 100 MHz are made from highly conductive non-magnetic material such as aluminum. They decrease the inductance because the magnetic field must bypass them.
Air core inductors can use sliding contacts or multiple taps to increase or decrease the number of turns included in the circuit, to change the inductance. A type much used in the past but mostly obsolete today has a spring contact that can slide along the bare surface of the windings. The disadvantage of this type is that the contact usually short-circuits one or more turns. These turns act like a single-turn short-circuited transformer secondary winding; the large currents induced in them cause power losses.
A type of continuously variable air core inductor is the "variometer". This consists of two coils with the same number of turns connected in series, one inside the other. The inner coil is mounted on a shaft so its axis can be turned with respect to the outer coil. When the two coils' axes are collinear, with the magnetic fields pointing in the same direction, the fields add and the inductance is maximum. When the inner coil is turned so its axis is at an angle with the outer, the mutual inductance between them is smaller so the total inductance is less. When the inner coil is turned 180° so the coils are collinear with their magnetic fields opposing, the two fields cancel each other and the inductance is very small. This type has the advantage that it is continuously variable over a wide range. It is used in antenna tuners and matching circuits to match low frequency transmitters to their antennas.
Another method to control the inductance without any moving parts requires an additional DC current bias winding which controls the permeability of an easily saturable core material. See Magnetic amplifier.
Circuit theory.
The effect of an inductor in a circuit is to oppose changes in current through it by developing a voltage across it proportional to the rate of change of the current. An ideal inductor would offer no resistance to a constant direct current; however, only superconducting inductors have truly zero electrical resistance.
The relationship between the time-varying voltage "v"("t") across an inductor with inductance "L" and the time-varying current "i"("t") passing through it is described by the differential equation:
When there is a sinusoidal alternating current (AC) through an inductor, a sinusoidal voltage is induced. The amplitude of the voltage is proportional to the product of the amplitude ("I"P) of the current and the frequency ("f") of the current.
In this situation, the phase of the current lags that of the voltage by π/2 (90°). For sinusoids, as the voltage across the inductor goes to its maximum value, the current goes to zero, and as the voltage across the inductor goes to zero, the current through it goes to its maximum value.
If an inductor is connected to a direct current source with value "I" via a resistance "R", and then the current source is short-circuited, the differential relationship above shows that the current through the inductor will discharge with an exponential decay:
Reactance.
The ratio of the peak voltage to the peak current in an inductor energised from a sinusoidal source is called the reactance and is denoted "X"L. The suffix is to distinguish inductive reactance from capacitive reactance due to capacitance.
Thus,
Reactance is measured in the same units as resistance (ohms) but is not actually a resistance. A resistance will dissipate energy as heat when a current passes. This does not happen with an inductor; rather, energy is stored in the magnetic field as the current builds and later returned to the circuit as the current falls. Inductive reactance is strongly frequency dependent. At low frequency the reactance falls, and for a steady current (zero frequency) the inductor behaves as a short-circuit. At increasing frequency, on the other hand, the reactance increases and at a sufficiently high frequency the inductor approaches an open circuit.
Laplace circuit analysis (s-domain).
When using the Laplace transform in circuit analysis, the impedance of an ideal inductor with no initial current is represented in the "s" domain by:
where
If the inductor does have initial current, it can be represented by:
where
where
Inductor networks.
Inductors in a parallel configuration each have the same potential difference (voltage). To find their total equivalent inductance ("L"eq):
The current through inductors in series stays the same, but the voltage across each inductor can be different. The sum of the potential differences (voltage) is equal to the total voltage. To find their total inductance:
These simple relationships hold true only when there is no mutual coupling of magnetic fields between individual inductors.
Stored energy.
Neglecting losses, the energy (measured in joules, in SI) stored by an inductor is equal to the amount of work required to establish the current through the inductor, and therefore the magnetic field. This is given by:
where "L" is inductance and "I" is the current through the inductor.
This relationship is only valid for linear (non-saturated) regions of the magnetic flux linkage and current relationship.
In general if one decides to find the energy stored in a LTI inductor that has initial current in a specific time between formula_22 and formula_23 can use this:
"Q" factor.
An ideal inductor would have no resistance or energy losses. However, real inductors have winding resistance from the metal wire forming the coils. Since the winding resistance appears as a resistance in series with the inductor, it is often called the "series resistance". The inductor's series resistance converts electric current through the coils into heat, thus causing a loss of inductive quality. The quality factor (or "Q") of an inductor is the ratio of its inductive reactance to its resistance at a given frequency, and is a measure of its efficiency. The higher the Q factor of the inductor, the closer it approaches the behavior of an ideal, lossless, inductor. High Q inductors are used with capacitors to make resonant circuits in radio transmitters and receivers. The higher the Q is, the narrower the bandwidth of the resonant circuit.
The Q factor of an inductor can be found through the following formula, where "L" is the inductance, "R" is the inductor's effective series resistance, "ω" is the radian operating frequency, and the product "ωL" is the inductive reactance:
Notice that "Q" increases linearly with frequency if "L" and "R" are constant. Although they are constant at low frequencies, the parameters vary with frequency. For example, skin effect, proximity effect, and core losses increase "R" with frequency; winding capacitance and variations in permeability with frequency affect "L".
Qualitatively, at low frequencies and within limits, increasing the number of turns "N" improves "Q" because "L" varies as "N"2 while "R" varies linearly with "N". Similarly, increasing the radius "r" of an inductor improves "Q" because "L" varies as "r"2 while "R" varies linearly with "r". So high "Q" air core inductors often have large diameters and many turns. Both of those examples assume the diameter of the wire stays the same, so both examples use proportionally more wire (copper). If the total mass of wire is held constant, then there would be no advantage to increasing the number of turns or the radius of the turns because the wire would have to be proportionally thinner.
Using a high permeability ferromagnetic core can greatly increase the inductance for the same amount of copper, so the core can also increase the Q. Cores however also introduce losses that increase with frequency. The core material is chosen for best results for the frequency band. At VHF or higher frequencies an air core is likely to be used.
Inductors wound around a ferromagnetic core may saturate at high currents, causing a dramatic decrease in inductance (and Q). This phenomenon can be avoided by using a (physically larger) air core inductor. A well designed air core inductor may have a Q of several hundred.
Inductance formulas.
The table below lists some common simplified formulas for calculating the approximate inductance of several inductor constructions.

</doc>
<doc id="14899" url="http://en.wikipedia.org/wiki?curid=14899" title="Insulin pump">
Insulin pump

An insulin pump is a medical device used for the administration of insulin in the treatment of diabetes mellitus, also known as continuous subcutaneous insulin infusion therapy.
The device configuration may vary depending on design. A traditional pump includes:
Other configurations are possible. For instance, more recent models may include disposable or semi-disposable designs for the pumping mechanism and may eliminate tubing from the infusion set.
An insulin pump is an alternative to multiple daily injections of insulin by insulin syringes or an insulin pen and allows for intensive insulin therapy when used in conjunction with blood glucose monitoring and carb counting.
Dosing.
An insulin pump allows the replacement of slow-acting insulin for basal needs with a continuous infusion of rapid-acting insulin.
The insulin pump delivers a single type of rapid-acting insulin in two ways:
Bolus shape.
An insulin pump user can influence the profile of the rapid-acting insulin by shaping the bolus. Users can experiment with bolus shapes to determine what is best for any given food, which means that they can improve control of blood sugar by adapting the bolus shape to their needs.
A standard bolus is an infusion of insulin pumped completely at the onset of the bolus. It's the most similar to an injection. By pumping with a "spike" shape, the expected action is the fastest possible bolus for that type of insulin. The standard bolus is most appropriate when eating high carb low protein low fat meals because it will return blood sugar to normal levels quickly.
An extended bolus is a slow infusion of insulin spread out over time. By pumping with a "square wave" shape, the bolus avoids a high initial dose of insulin that may enter the blood and cause low blood sugar before digestion can facilitate sugar entering the blood. The extended bolus also extends the action of insulin well beyond that of the insulin alone. The extended bolus is appropriate when covering high fat high protein meals such as steak, which will be raising blood sugar for many hours past the onset of the bolus. The extended bolus is also useful for those with slow digestion (such as with gastroparesis or Coeliac disease).
A combination bolus/multiwave bolus is the combination of a standard bolus spike with an extended bolus square wave. This shape provides a large dose of insulin up front, and then also extends the tail of the insulin action. The combination bolus is appropriate for high carb high fat meals such as pizza, pasta with heavy cream sauce, and chocolate cake.
A super bolus is a method of increasing the spike of the standard bolus. Since the action of the bolus insulin in the blood stream will extend for several hours, the basal insulin could be stopped or reduced during this time. This facilitates the "borrowing" of the basal insulin and including it into the bolus spike to deliver the same total insulin with faster action than can be achieved with spike and basal rate together. The super bolus is useful for certain foods (like sugary breakfast cereals) which cause a large post-prandial peak of blood sugar. It attacks the blood sugar peak with the fastest delivery of insulin that can be practically achieved by pumping.
Bolus timing.
Since the pump user is responsible to manually start a bolus, this provides an opportunity for the user to pre-bolus to improve upon the insulin pump's capability to prevent post-prandial hyperglycemia. A pre-bolus is simply a bolus of insulin given before it is actually needed to cover carbohydrates eaten.
There are two situations where a pre-bolus is helpful:
Similarly, a low blood sugar level or a low glycemic food might be best treated with a bolus "after" a meal is begun. The blood sugar level, the type of food eaten, and a person's individual response to food and insulin have an impact on the ideal time to bolus with the pump.
Basal rate patterns.
The pattern for delivering basal insulin throughout the day can also be customized with a pattern to suit the pump user.
Basal rate determination.
Basal insulin requirements will vary between individuals and periods of the day. The basal rate for a particular time period is determined by fasting while periodically evaluating the blood sugar level. Neither food nor bolus insulin must be taken for 4 hours before or during the evaluation period. If the blood sugar level changes dramatically during evaluation, then the basal rate can be adjusted to increase or decrease insulin delivery to keep the blood sugar level approximately steady.
For instance, to determine an individual's morning basal requirement, they must skip breakfast. On waking, they would test their blood glucose level periodically until lunch. Changes in blood glucose level are compensated with adjustments in the morning basal rate. The process is repeated over several days, varying the fasting period, until a 24-hour basal profile has been built up which keeps fasting blood sugar levels relatively steady. Once the basal rate is matched to the fasting basal insulin need, the pump user will then gain the flexibility to skip or postpone meals such as sleeping late on the weekends or working overtime on a weekday.
Many factors can change insulin requirements and require an adjustment to the basal rate:
A pump user should be educated by their diabetes care professional about basal rate determination before beginning pump therapy.
Temporary basal rates.
Since the basal insulin is provided as a rapid-acting insulin, the basal insulin can be immediately increased or decreased as needed with a temporary basal rate. Examples when this is helpful include:
History of insulin pumps.
In 1963 Dr. Arnold Kadish designed the first insulin pump to be worn as a backpack. A more wearable version was later devised by Dean Kamen in 1976. Kamen formed a company called "AutoSyringe" to market the product, which he sold to Baxter Health Care in 1981.
The insulin pump was first endorsed in the United Kingdom in 2003, by the National Institute for Health and Care Excellence.
Acceptability.
Use of insulin pumps is increasing throughout the world because of:
Recent developments.
New insulin pumps are becoming "smart" as new features are added to their design. These simplify the tasks involved in delivering an insulin bolus.
Hypoglycemia and new developments.
An important step forward is the MySentry system for an automated alert in case of nocturnal hypoglycemia, the development of algorithms for suspension of pump action in case of sensor detected imminent hypoglycemia, and the availability of stable glucagon solutions which will enable automated glucagon delivery from a second sensor activated pump.
Security.
In August 2011, an IBM researcher, Jay Radcliffe, demonstrated a security flaw in insulin pumps. Radcliffe was able to hack the wireless interface used to control the pump remotely. Pump manufacturer Medtronic later said security research by McAfee uncovered a flaw in its pumps that could be exploited.
Choosing a pump.
When the time comes to select an insulin pump, there are numerous options. The "perfect" pump varies by person. Factors such as weight, color, cost, canula insertion angles, special features, and easy usage play a vital role in the selection process. Patient factors relevant to infusion set selection include patient’s age, immune system function, body characteristics, activities, personal preferences, and history of diabetic ketoacidosis.
Popular pumps include the Medtronic MiniMed, Accu-check Combo System, Asante Snap, Tandem t:slim, Animas One Touch Ping, and OmniPod. The process of selecting a pump usually involves the patient meeting with representatives from companies they are interested in. It is recommended for patients to do as much research as possible. All pumps have the same goal, and each takes a different route. have been developed to assist with pump choice.

</doc>
<doc id="14900" url="http://en.wikipedia.org/wiki?curid=14900" title="ISO 3166">
ISO 3166

ISO 3166 is a standard published by the International Organization for Standardization (ISO) that defines codes for the names of countries, dependent territories, special areas of geographical interest, and their principal subdivisions (e.g., provinces or states). The official name of the standard is "Codes for the representation of names of countries and their subdivisions".
Parts.
It consists of three parts:
Editions.
The first edition of ISO 3166 was published in 1974, which included only alphabetic country codes. The second edition, published in 1981, also included numeric country codes, with the third and fourth editions published in 1988 and 1993 respectively. The fifth edition, published between 1997 and 1999, was expanded into three parts to include codes for subdivisions and former countries.
ISO 3166 Maintenance Agency.
The ISO 3166 standard is maintained by the ISO 3166 Maintenance Agency (ISO 3166/MA), located at the ISO central office in Geneva, Switzerland. Originally it was located at the Deutsches Institut für Normung (DIN) in Berlin, Germany. Its principal tasks are:
Members.
There are ten experts with voting rights on the ISO 3166/MA. Five are representatives of national standards organizations:
The other five are representatives of major United Nations agencies or other international organizations who are all users of ISO 3166-1:
The ISO 3166/MA has further associated members who do not participate in the votes but who, through their expertise, have significant influence on the decision-taking procedure in the maintenance agency.
External links.
Related websites

</doc>
<doc id="14904" url="http://en.wikipedia.org/wiki?curid=14904" title="Intensive insulinotherapy">
Intensive insulinotherapy

Intensive insulinotherapy is a therapeutic regimen for diabetes mellitus treatment. This newer approach contrasts with conventional insulinotherapy. Rather than minimize the number of insulin injections per day (a technique which demands a rigid schedule for food and activities), the intensive approach favors flexible meal times with variable carbohydrate as well as flexible physical activities. The trade-off is the increase from 2 or 3 injections per day to 4 or more injections per day, which was considered "intensive" relative to the older approach. In North America in 2004, many endocrinologists prefer the term Flexible Insulin Therapy (FIT) to "intensive therapy" and use it to refer to any method of replacing insulin that attempts to mimic the pattern of small continuous basal insulin secretion of a working pancreas combined with larger insulin secretions at mealtimes. The semantic distinction reflects changing treatment.
Rationale for intensive or flexible treatment.
Long-term studies like the UK Prospective Diabetes Study ("UKPDS") and the Diabetes control and complications trial ("DCCT") showed that intensive insulinotherapy achieved blood glucose levels closer to non-diabetic people and that this was associated with reduced frequency and severity of blood vessel damage. Damage to large and small blood vessels (macro- and microvascular disease) is central to the development of complications of diabetes mellitus.
This evidence convinced most physicians who specialize in diabetes care that an important goal of treatment is to make the biochemical profile of the diabetic patient (blood lipids, HbA1c, etc.) as close to the values of non-diabetic people as possible. This is especially true for young patients with many decades of life ahead.
A general description of intensive or flexible therapy.
A working pancreas continually secretes small amounts of insulin into the blood to maintain normal glucose levels, which would otherwise rise from glucose release by the liver, especially during the early morning dawn phenomenon. This insulin is referred to as "basal insulin secretion", and constitutes almost half the insulin produced by the normal pancreas.
Bolus insulin is produced during the digestion of meals. Insulin levels rise immediately as we begin to eat, remaining higher than the basal rate for 1 to 4 hours. This meal-associated ("prandial") insulin production is roughly proportional to the amount of carbohydrate in the meal.
Intensive or flexible therapy involves supplying a continual supply of insulin to serve as the "basal insulin", supplying meal insulin in doses proportional to nutritional load of the meals, and supplying extra insulin when needed to correct high glucose levels. These three components of the insulin regimen are commonly referred to as basal insulin, bolus insulin, and high glucose correction insulin.
Two common intensive/flexible regimens: pens, injection ports, and pumps.
One method of intensive insulinotherapy is based on multiple daily injections (sometimes referred to in medical literature as "MDI"). Meal insulin is supplied by injection of rapid-acting insulin before each meal in an amount proportional to the meal. Basal insulin is provided as a once or twice daily injection of dose of a long-acting insulin.
In an MDI regimen, long-acting insulins are preferred for basal use. An older insulin used for this purpose is ultralente, and beef ultralente in particular was considered for decades to be the gold standard of basal insulin. Long-acting insulin analogs such as insulin glargine (brand name "Lantus", made by Sanofi-Aventis) and insulin detemir (brand name "Levemir", made by Novo Nordisk) are also used, with insulin glargine used more than insulin detemir. Rapid-acting insulin analogs such as lispro (brand name "Humalog", made by Eli Lilly and Company) and aspart (brand name "Novolog"/"Novorapid", made by Novo Nordisk and "Apidra" made by Sanofi Aventis) are preferred by many clinicians over older regular insulin for meal coverage and high correction. Many people on MDI regimens carry insulin pens to inject their rapid-acting insulins instead of traditional syringes. Some people on an MDI regimen also use injection ports such as the I-port to minimize the number of daily skin punctures.
The other method of intensive/flexible insulin therapy is an insulin pump. It is a small mechanical device about the size of a deck of cards. It contains a syringe-like reservoir with about three days' insulin supply. This is connected by thin, disposable, plastic tubing to a needle-like cannula inserted into the patient's skin and held in place by an adhesive patch. The infusion tubing and cannula must be removed and replaced every few days.
An insulin pump can be programmed to infuse a steady amount of rapid-acting insulin under the skin. This steady infusion is termed the basal rate and is designed to supply the background insulin needs. Each time the patient eats, he or she must press a button on the pump to deliver a specified dose of insulin to cover that meal. Extra insulin is also given the same way to correct a high glucose reading. Although current pumps can include a glucose sensor, they cannot automatically respond to meals or to rising or falling glucose levels.
Both MDI and pumping can achieve similarly excellent glycemic control. Some people prefer injections because they are less expensive than pumps and do not require the wearing of a continually attached device. However, the clinical literature is very clear that patients whose basal insulin requirements tend not to vary throughout the day or do not require dosage precision smaller than 0.5 IU, are much less likely to realize much significant advantage of pump therapy. Another perceived advantage of pumps is the freedom from syringes and injections, however, infusion sets still require less frequent injections to guide infusion sets into the subcutaneous tissue.
Intensive/flexible insulin therapy requires frequent blood glucose checking. To achieve the best balance of blood sugar with either intensive/flexible method, a patient must check his or her glucose level with a meter monitoring of blood glucose several times a day. This allows optimization of the basal insulin and meal coverage as well as correction of high glucose episodes.
Advantages and disadvantages of intensive/flexible insulin therapy.
The two primary advantages of intensive/flexible therapy over more traditional two or three injection regimens are: 
Major disadvantages of intensive/flexible therapy are that it requires greater amounts of education and effort to achieve the goals, and it increases the daily cost for glucose monitoring four or more times a day. This cost can substantially increase when the therapy is implemented with an insulin pump and/or continuous glucose monitor.
It is a common notion that more frequent hypoglycemia is a disadvantage of intensive/flexible regimens. The frequency of hypoglycemia increases with increasing effort to achieve normal blood glucoses with most insulin regimens, but hypoglycemia can be minimized with appropriate glucose targets and control strategies. The difficulties lie in remembering to test, estimating meal size, taking the meal bolus and eating within the prescribed time, and being aware of snacks and meals that are not the expected size. When implemented correctly, flexible regimens offer greater ability to achieve good glycemic control with easier accommodation to variations of eating and physical activity.
Semantics of changing care: why "flexible" is replacing "intensive" therapy.
Over the last two decades, the evidence that better glycemic control (i.e., keeping blood glucose and HbA1c levels as close to normal as possible) reduces the rates of many complications of diabetes has become overwhelming. As a result, diabetes specialists have expended increasing effort to help most people with diabetes achieve blood glucose levels as close to normal as achievable. It takes about the same amount of effort to achieve good glycemic control with a traditional two or three injection regimen as it does with flexible therapy: frequent glucose monitoring, attention to timing and amounts of meals. Many diabetes specialists no longer think of flexible insulin therapy as "intensive" or "special" treatment for a select group of patients but simply as standard care for most patients with type 1 diabetes.
Treatment device used in intensive insulinotherapy.
The insulin pump is one device used in intensive insulinotherapy. The insulin pump is about the size of a beeper. It can be programmed to send a steady stream of insulin as "basal insulin". It contains a reservoir or cartridge holding several days' worth of insulin, the tiny battery-operated pump, and the computer chip that regulates how much insulin is pumped. The infusion set is a thin plastic tube with a fine needle at the end. There are also newer "pods" which do not require tubing. It carries the insulin from the pump to the infusion site beneath the skin. It sends a larger amount before eating meals as "bolus" doses.
The insulin pump replaces insulin injections. This device is useful for people who regularly forget to inject themselves or for people who don't like injections. This machine does the injecting by replacing the slow-acting insulin for basal needs with an ongoing infusion of rapid-acting insulin.
Basal Insulin: the insulin that controls blood glucose levels between meals and overnight. It controls glucose in the fasting state.
Boluses: the insulin that is released when food is eaten or to correct a high reading.
Another device used in intensive insulinotherapy is the injection port. An injection port is a small disposable device, similar to the infusion set used with an insulin pump, configured to accept a syringe. Standard insulin injections are administered through the injection port. When using an injection port, the syringe needle always stays above the surface of the skin, thus reducing the number of skin punctures associated with intensive insulinotheraphy.

</doc>
<doc id="14906" url="http://en.wikipedia.org/wiki?curid=14906" title="Interwiki links">
Interwiki links

Interwiki linking ("W-link") is a facility for creating links to the many wikis on the World Wide Web. Users avoid pasting in entire URLs (as they would for regular web pages) and instead use a shorthand similar to links within the same wiki (intrawiki links).
Unlike domain names on the Internet, there is no globally defined list of interwiki prefixes, so owners of a wiki must define an interwiki map (InterMap) appropriate to their needs. Users generally have to create separate accounts for each wiki they intend to use (unless they intend to edit anonymously). Variations in text formatting and layout can also hinder a seamless transition from one wiki to the next.
By making wiki links simpler to type for the members of a particular community, these features help bring the different wikis closer together. Furthering that goal, interwiki "bus tours" (similar to webrings) have been created to explain the purposes and highlights of different wikis. Such examples on Wikipedia include and .
Syntax.
Interwiki link notation varies, depending largely on the syntax a wiki uses for markup. The two most common link patterns in wikis are CamelCase and free links (arbitrary phrases surrounded by some set delimiter, such as double square brackets). CURIE syntax -an emerging W3C standard- uses a single set of square brackets.
Interwiki links on a CamelCase-based wiki frequently take the form of "Code:PageName", where "Code" is the defined InterMap prefix for another wiki. Thus, a link "WikiPedia:InterWiki" could be rendered in HTML as a link to an article on Wikipedia: for example, . Linking from a CamelCase-wiki to a page that contains spaces in its title typically requires replacing the spaces with underscores (e.g. WikiPedia:Main_Page).
Interwiki links on wikis based on free links, such as Wikipedia, typically follow the same principle, but using the delimiters that would be used for internal links. These links can then be parsed and escaped as they would be if they were internal, allowing easier typing of spaces but potentially causing problems with other special characters. For example, on Wikipedia, codice_1 appears as , and codice_1 (former syntax: codice_1) appears as .
The MediaWiki software has an additional feature which uses similar notation to create automatic interlanguage links — for instance, the link codice_1 (with no leading colon) automatically creates a reference labeled "Other languages: | ..." at the top and bottom of, or in a sidebar next to, the article display. Various other wiki software systems have features for "semi-internal" links of this kind, such as support for namespaces or multiple sub-communities.
Most InterMap implementations simply substitute the interwiki prefix with a full URL prefix, so many non-wiki websites can also be referred to using the system. A reference to a definition on the Free On-line Dictionary of Computing, for instance, could take the form codice_1 which would tell the system to append and display the link as . This makes it very easy to link to commonly referenced resources from within a wiki page, without the need to even know the form of the URL in question.
The interwiki concept can equally be applied to links "from" non-wiki websites. Advogato, for instance, offers a syntax for creating shorthand links based on a MeatBall-derived InterMap.
Implementation.
Internally, a wiki that uses interwiki links needs to have a mapping from wiki-code links to full URLs. For example, codice_1 might appear as , but link to codice_7.
Since most wiki systems use URLs for individual pages where the page's title appears at the end of an otherwise unchanging address, the simplest way of defining such mappings is by substituting the interwiki prefix for the unchanging part of the URL. So in the example above, the codice_8 has simply been replaced by codice_9 in creating the target of the HTML rendered link.
Rather than creating a new list from scratch for every wiki, it is often useful to obtain a copy of that from another site. Sites such as MeatballWiki and the UseModWiki site contain comprehensive lists which are often used for this purpose - the former being publicly editable in the same way as any other wiki page, and the latter being verified as usable but potentially out of date. MediaWiki's default list of interwiki links is derived from an old version of MeatballWiki's list.

</doc>
<doc id="14907" url="http://en.wikipedia.org/wiki?curid=14907" title="Inverse function">
Inverse function

In mathematics, an inverse function is a function that "reverses" another function. That is, if f is a function mapping x to y, then the inverse function of f maps y back to x.
Definitions.
Let f be a function whose domain is the set X, and whose image (range) is the set Y. Then f is "invertible" if there exists a function g with domain Y and image X, with the property:
If f is invertible, the function g is unique, which means that there is exactly one function g satisfying this property (no more, no less). That function g is then called "the" inverse of f, and is usually denoted as "f" −1.
Stated otherwise, a function is invertible if and only if its inverse relation is a function on the range Y, in which case the inverse relation is the inverse function.
Not all functions have an inverse. For this rule to be applicable, each element "y" ∈ "Y" must correspond to no more than one "x" ∈ "X"; a function f with this property is called one-to-one or an injection. If f and "f" −1 are functions on X and Y respectively, then both are bijections. The inverse of an injection that is not a bijection is a partial function, that means for some "y" ∈ "Y" it is undefined.
Example: squaring and square root functions.
The function "f"("x") = "x"2 may or may not be invertible, depending on what kinds of numbers are being considered (the "domain").
If the domain is the real numbers, then each possible result "y" (except 0) corresponds to two different starting points in X – one positive and one negative, and so this function is not invertible: as it is impossible to deduce an input from its output. Such a function is called non-injective or information-losing.
If the domain of the function is restricted to the nonnegative reals then the function is injective and invertible.
Inverses in higher mathematics.
The definition given above is commonly adopted in set theory and calculus. In higher mathematics, the notation
means "f is a function mapping elements of a set X to elements of a set Y". The source, X, is called the domain of f, and the target, Y, is called the codomain. The codomain contains the range of f as a subset, and is part of the definition of f.
When using codomains, the inverse of a function "f": "X" → "Y" is required to have domain Y and codomain X. For the inverse to be defined on all of Y, every element of Y must lie in the range of the function f. A function with this property is called "onto" or "surjective". Thus, a function with a codomain is invertible if and only if it is both "injective" (one-to-one) and surjective (onto). Such a function is called a one-to-one correspondence or a bijection, and has the property that every element "y" ∈ "Y" corresponds to exactly one element "x" ∈ "X".
Inverses and composition.
If f is an invertible function with domain X and range Y, then
Using the composition of functions we can rewrite this statement as follows:
where id"X" is the identity function on the set X; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism.
Considering function composition helps to understand the notation "f" −1. Repeatedly composing a function with itself is called iteration.If f is applied n times, starting with the value x, then this is written as "f" "n"("x"); so , etc. Since "f" −1("f" ("x")) = "x", composing "f" −1 and "f" "n" yields "f" "n"−1, "undoing" the effect of one application of f.
Note on notation.
Whereas the notation "f" −1("x") might be misunderstood, "f"("x")−1 certainly denotes the multiplicative inverse of "f"("x") and has nothing to do with inversion of f.
The expression sin−1 "x" does not represent the multiplicative inverse to sin "x", but the inverse of the sine function applied to x (actually a partial inverse; see below). To avoid confusion, an inverse trigonometric function is often indicated by the prefix "arc" (for Latin "arcus"). For instance, the inverse of the sine function is typically called the arcsine function, written as arcsin. Similarly, the inverse of a hyperbolic function is indicated by the prefix "ar" (for Latin "area").
Properties.
Uniqueness.
If an inverse function exists for a given function f, it is unique: it must be the inverse relation.
Symmetry.
There is a symmetry between a function and its inverse. Specifically, if f is an invertible function with domain X and range Y, then its inverse "f" −1 has domain Y and range X, and the inverse of "f" −1 is the original function f. In symbols, for functions "f":"X"→"Y" and "g":"Y"→"X",
This follows from the connection between function inverse and relation inverse, because inversion of relations is an involution.
This statement is an obvious consequence of the deduction that for f to be invertible it must be injective (first definition of the inverse) or bijective (second definition). The property of involutive symmetry can be concisely expressed by the following formula:
The inverse of a composition of functions is given by the formula
Notice that the order of g and f have been reversed; to undo f followed by g, we must first undo g and then undo f.
For example, let "f"("x") = 3"x" and let "g"("x") = "x" + 5. Then the composition "g" ∘ "f" is the function that first multiplies by three and then adds five:
To reverse this process, we must first subtract five, and then divide by three:
This is the composition
("f" −1 ∘ "g" −1)("y").
Self-inverses.
If X is a set, then the identity function on X is its own inverse:
More generally, a function "f" : "X" → "X" is equal to its own inverse if and only if the composition "f" ∘ "f" is equal to id"X". Such a function is called an involution.
Inverses in calculus.
Single-variable calculus is primarily concerned with functions that map real numbers to real numbers. Such functions are often defined through formulas, such as:
A function f from the real numbers to the real numbers possesses an inverse as long as it is one-to-one, i.e. as long as the graph of "y" = "f"("x") has, for each possible y value only one corresponding x value, and thus passes the horizontal line test.
The following table shows several standard functions and their inverses:
Formula for the inverse.
One approach to finding a formula for "f" −1, if it exists, is to solve the equation "y" = "f"("x") for x. For example, if f is the function
then we must solve the equation "y" = (2"x" + 8)3 for x:
Thus the inverse function "f" −1 is given by the formula
Sometimes the inverse of a function cannot be expressed by a formula with a finite number of terms. For example, if f is the function
then f is one-to-one, and therefore possesses an inverse function "f" −1. The formula for this inverse has an infinite number of terms:<br>
Graph of the inverse.
If f is invertible, then the graph of the function
is the same as the graph of the equation
This is identical to the equation "y" = "f"("x") that defines the graph of f, except that the roles of x and y have been reversed. Thus the graph of "f" −1 can be obtained from the graph of f by switching the positions of the x and y axes. This is equivalent to reflecting the graph across the line
"y" = "x".
Inverses and derivatives.
A continuous function f is one-to-one (and hence invertible) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function
is invertible, since the derivative
"f′"("x") = 3"x"2 + 1 is always positive.
If the function f is differentiable, then the inverse "f" −1 will be differentiable as long as "f′"("x") ≠ 0. The derivative of the inverse is given by the inverse function theorem:
If we set "y" = "f"("x"), then the formula above can be written
This result follows from the chain rule (see the article on inverse functions and differentiation).
The inverse function theorem can be generalized to functions of several variables. Specifically, a differentiable multivariable function "f ": R"n" → R"n" is invertible in a neighborhood of a point p as long as the Jacobian matrix of f at p is invertible. In this case, the Jacobian of "f" −1 at "f"("p") is the matrix inverse of the Jacobian of f at p.
Real-world examples.
1. Let f be the function that converts a temperature in degrees Celsius to a temperature in degrees Fahrenheit:
then its inverse function converts degrees Fahrenheit to degrees Celsius:
since
2. Suppose f assigns each child in a family its birth year. An inverse function would output which child was born in a given year. However, if the family has twins (or triplets) then the output cannot be known when the input is the common birth year. As well, if a year is given in which no child was born then a child cannot be named. But if each child was born in a separate year, and if we restrict attention to the three years in which a child was born, then we do have an inverse function. For example,
3. Let R be the function that leads to an x percentage rise of some quantity, and F be the function producing an x percentage fall. Applied to $100 with x = 10%, we find that applying the first function followed by the second does not restore the original value of $100, demonstrating the fact that, despite appearances, these two functions are not inverses of each other.
Generalizations.
Partial inverses.
Even if a function f is not one-to-one, it may be possible to define a partial inverse of f by restricting the domain. For example, the function
is not one-to-one, since "x"2 = (−"x")2. However, the function becomes one-to-one if we restrict to the domain "x" ≥ 0, in which case
(If we instead restrict to the domain "x" ≤ 0, then the inverse is the negative of the square root of y.) Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function:
Sometimes this multivalued inverse is called the full inverse of f, and the portions (such as √x and −√x) are called "branches". The most important branch of a multivalued function (e.g. the positive square root) is called the "principal branch", and its value at y is called the "principal value" of "f" −1("y").
For a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the picture to the right).
These considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since
for every real x (and more generally sin("x" + 2π"n") = sin("x") for every integer n). However, the sine is one-to-one on the interval
[−π/2, π/2], and the corresponding partial inverse is called the arcsine. This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between −π/2 and π/2. The following table describes the principal branch of each inverse trigonometric function:
Left and right inverses.
If "f": "X" → "Y", a left inverse for f (or "retraction" of f) is a function "g": "Y" → "X" such that
That is, the function g satisfies the rule
Thus, g must equal the inverse of f on the image of f, but may take any values for elements of Y not in the image. A function f with a left inverse is necessarily injective. In classical mathematics, every injective function f necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion {0,1} → R of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set {0,1}.
A right inverse for f (or "section" of f) is a function "h": "Y" → "X" such that
That is, the function h satisfies the rule
Thus, "h"("y") may be any of the elements of X that map to y under f. A function f has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice).
An inverse which is both a left and right inverse must be unique. Likewise, if g is a left inverse for f, then g may or may not be a right inverse for f; and if g is a right inverse for f, then g is not necessarily a left inverse for f. For example let "f": R → [0, ∞) denote the squaring map, such that "f"("x") = "x"2 for all x in R, and let g: [0, ∞) → R denote the square root map, such that √x for all "x" ≥ 0. Then "f"("g"("x")) = "x" for all x in [0, ∞); that is, g is a right inverse to f. However, g is not a left inverse to f, since, e.g., "g"("f"(−1)) = 1 ≠ −1.
Preimages.
If "f": "X" → "Y" is any function (not necessarily invertible), the preimage (or inverse image) of an element "y" ∈ "Y" is the set of all elements of X that map to y:
The preimage of y can be thought of as the image of y under the (multivalued) full inverse of the function f.
Similarly, if S is any subset of Y, the preimage of S is the set of all elements of X that map to S:
For example, take a function "f": R → R, where "f": "x" ↦ "x"2. This function is not invertible for reasons discussed . Yet preimages may be defined for subsets of the codomain:
The preimage of a single element "y" ∈ "Y" – a singleton set {"y"} – is sometimes called the "fiber" of y. When Y is the set of real numbers, it is common to refer to "f" −1({"y"}) as a "level set".

</doc>
<doc id="14909" url="http://en.wikipedia.org/wiki?curid=14909" title="Inertia">
Inertia

Inertia is the resistance of any physical object to any change in its state of motion, including changes to its speed and direction. It is the tendency of objects to keep moving in a straight line at constant velocity. The principle of inertia is one of the fundamental principles of classical physics that are used to describe the motion of objects and how they are affected by applied forces. Inertia comes from the Latin word, "iners", meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his "Philosophiæ Naturalis Principia Mathematica", which states:
The "vis insita", or innate force of matter, is a power of resisting by which every body, as much as in it lies, endeavours to preserve its present state, whether it be of rest or of moving uniformly forward in a straight line.
In common usage, the term "inertia" may refer to an object's "amount of resistance to change in velocity" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term "inertia" is more properly understood as shorthand for "the principle of inertia" as described by Newton in his First Law of Motion: that an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.
On the surface of the Earth, inertia is often masked by the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest), and gravity. This misled classical theorists such as Aristotle, who believed that objects would move only as long as force was applied to them:
...it [body] stops when the force which is pushing the travelling object has no longer power to push it along...
History and development of the concept.
Early understanding of motion.
Prior to the Renaissance, the most generally accepted theory of motion in Western philosophy was based on Aristotle (around 335 BC to 322 BC) who said that, in the absence of an external motive power, all objects (on Earth) would come to rest and that moving objects only continue to move so long as there is a power inducing them to do so. Aristotle explained the continued motion of projectiles, which are separated from their projector, by the action of the surrounding medium which continues to move the projectile in some way. Aristotle concluded that such violent motion in a void was impossible.
Despite its general acceptance, Aristotle's concept of motion was disputed on several occasions by notable philosophers over nearly two millennia. For example Lucretius (following, presumably, Epicurus) stated that the 'default state' of matter was motion not stasis. In the 6th century John Philoponus criticized the inconsistency between Aristotle's discussion of projectiles, where the medium keeps projectiles going, and his discussion of the void, where the medium would hinder a body's motion. Philoponus proposed that motion was not maintained by the action of a surrounding medium but by some property imparted to the object when it was set in motion. Although this was not the modern concept of inertia, for there was still the need for a power to keep a body in motion, it proved a fundamental step in that direction. This view was strongly opposed by Averroes and by many scholastic philosophers who supported Aristotle. However this view did not go unchallenged in the Islamic world, where Philoponus did have several supporters who further developed his ideas.
Theory of impetus.
In the 14th century, Jean Buridan rejected the notion that a motion-generating property, which he named "impetus", dissipated spontaneously. Buridan's position was that a moving object would be arrested by the resistance of the air and the weight of the body which would oppose its impetus. Buridan also maintained that impetus increased with speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Despite the obvious similarities to more modern ideas of inertia, Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also believed that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.
Buridan's thought was followed up by his pupil Albert of Saxony (1316–1390) and the Oxford Calculators, who performed various experiments that further undermined the classical, Aristotelian view. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.
Shortly before Galileo's theory of inertia, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:
"…[Any] portion of corporeal matter which moves by itself when an impetus has been impressed on it by any external motive force has a natural tendency to move on a rectilinear, not a curved, path."
Benedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.
Classical inertia.
The law of inertia states that it is the tendency of an object to resist a change in motion. According to Newton, an object will stay at rest or stay in motion (i.e. 'maintain its velocity' in modern terms) unless acted on by a net external force, whether it results from gravity, friction, contact, or some other source. The Aristotelian division of motion into mundane and celestial became increasingly problematic in the face of the conclusions of Nicolaus Copernicus in the 16th century, who argued that the earth (and everything on it) was in fact never "at rest", but was actually in constant motion around the sun. Galileo, in his further development of the Copernican model, recognized these problems with the then-accepted nature of motion and, at least partially as a result, included a restatement of Aristotle's description of motion in a void as a basic physical principle:
A body moving on a level surface will continue in the same direction at a constant speed unless disturbed. 
Galileo writes that 'all external impediments removed, a heavy body on a spherical surface concentric with the earth will maintain itself in that state in which it has been; if placed in movement towards the west (for example), it will maintain itself in that movement'. This notion which is termed 'circular inertia' or 'horizontal circular inertia' by historians of science, is a precursor to, but distinct from, Newton's notion of rectilinear inertia. For Galileo, a motion is 'horizontal' if it does not carry the moving body towards or away from the centre of the earth, and for him 'a ship, for instance, having once received some impetus through the tranquil sea, would move continually around our globe without ever stopping'.
It is also worth noting that Galileo later went on to conclude that based on this initial premise of inertia, it is impossible to tell the difference between a moving object and a stationary one without some outside reference to compare it against. This observation ultimately came to be the basis for Einstein to develop the theory of Special Relativity.
Concepts of inertia in Galileo's writings would later come to be refined, modified and codified by Isaac Newton as the first of his Laws of Motion (first published in Newton's work, "Philosophiae Naturalis Principia Mathematica", in 1687):
Unless acted upon by a net unbalanced force, an object will maintain a constant velocity.
Note that "velocity" in this context is defined as a vector, thus Newton's "constant velocity" implies both constant speed and constant direction (and also includes the case of zero speed, or no motion). Since initial publication, Newton's Laws of Motion (and by extension this first law) have come to form the basis for the branch of physics known as classical mechanics.
The actual term "inertia" was first introduced by Johannes Kepler in his "Epitome Astronomiae Copernicanae" (published in three parts from 1618–1621); however, the meaning of Kepler's term (which he derived from the Latin word for "idleness" or "laziness") was not quite the same as its modern interpretation. Kepler defined inertia only in terms of a resistance to movement, once again based on the presumption that rest was a natural state which did not need explanation. It was not until the later work of Galileo and Newton unified rest and motion in one principle that the term "inertia" could be applied to these concepts as it is today.
Nevertheless, despite defining the concept so elegantly in his laws of motion, even Newton did not actually use the term "inertia" to refer to his First Law. In fact, Newton originally viewed the phenomenon he described in his First Law of Motion as being caused by "innate forces" inherent in matter, which resisted any acceleration. Given this perspective, and borrowing from Kepler, Newton actually attributed the term "inertia" to mean "the innate force possessed by an object which resists changes in motion"; thus Newton defined "inertia" to mean the cause of the phenomenon, rather than the phenomenon itself. However, Newton's original ideas of "innate resistive force" were ultimately problematic for a variety of reasons, and thus most physicists no longer think in these terms. As no alternate mechanism has been readily accepted, and it is now generally accepted that there may not be one which we can know, the term "inertia" has come to mean simply the phenomenon itself, rather than any inherent mechanism. Thus, ultimately, "inertia" in modern classical physics has come to be a name for the same phenomenon described by Newton's First Law of Motion, and the two concepts are now considered to be equivalent.
Relativity.
Albert Einstein's theory of special relativity, as proposed in his 1905 paper, "On the Electrodynamics of Moving Bodies," was built on the understanding of inertia and inertial reference frames developed by Galileo and Newton. While this revolutionary theory did significantly change the meaning of many Newtonian concepts such as mass, energy, and distance, Einstein's concept of inertia remained unchanged from Newton's original meaning (in fact the entire theory was based on Newton's definition of inertia). However, this resulted in a limitation inherent in special relativity that the principle of relativity could only apply to reference frames that were "inertial" in nature (meaning when no acceleration was present). In an attempt to address this limitation, Einstein proceeded to develop his general theory of relativity ("The Foundation of the General Theory of Relativity," 1916), which ultimately provided a unified theory for both "inertial" and "noninertial" (accelerated) reference frames. However, in order to accomplish this, in general relativity Einstein found it necessary to redefine several fundamental concepts (such as gravity) in terms of a new concept of "curvature" of space-time, instead of the more traditional system of forces understood by Newton.
As a result of this redefinition, Einstein also redefined the concept of "inertia" in terms of geodesic deviation instead, with some subtle but significant additional implications. The result of this is that according to general relativity, when dealing with very large scales, the traditional Newtonian idea of "inertia" does not actually apply, and cannot necessarily be relied upon. Luckily, for sufficiently small regions of spacetime, the special theory can be used, in which inertia still means the same (and works the same) as in the classical model.
Another profound conclusion of the theory of special relativity, perhaps the most well-known, was that energy and mass are not separate things, but are, in fact, interchangeable. This new relationship, however, also carried with it new implications for the concept of inertia. The logical conclusion of special relativity was that if mass exhibits the principle of inertia, then inertia must also apply to energy. This theory, and subsequent experiments confirming some of its conclusions, have also served to radically expand the definition of inertia in some contexts to apply to a much wider context including energy as well as matter.
Interpretations.
Mass and inertia.
Physics and mathematics appear to be less inclined to use the popular concept of inertia as "a tendency to maintain momentum" and instead favor the mathematically useful definition of inertia as the measure of a body's resistance to changes in velocity or simply a body's inertial mass.
This was clear in the beginning of the 20th century, when the theory of relativity was not yet created. Mass, "m", denoted something like an amount of substance or quantity of matter. And at the same time mass was the quantitative measure of inertia of a body.
The mass of a body determines the momentum formula_1 of the body at given velocity formula_2; it is a proportionality factor in the formula:
The factor "m" is referred to as inertial mass.
But mass, as related to the 'inertia' of a body, can also be defined by the formula:
Here, "F" is force, "m" is inertial mass, and "a" is acceleration.
By this formula, the greater its mass, the less a body accelerates under given force. Masses formula_5 defined by formula (1) and (2) are equal because formula (2) is a consequence of formula (1) if mass does not depend on time and velocity. Thus, "mass is the quantitative or numerical measure of a body’s inertia, that is of its resistance to being accelerated".
This meaning of a "body's inertia" therefore is altered from the popular meaning as "a tendency to maintain momentum" to a description of the measure of how difficult it is to change the velocity of a body. But it is consistent with the fact that motion in one reference frame can disappear in another, so it is the change in velocity that is important.
Inertial mass.
There is no measurable difference between gravitational mass and inertial mass. The gravitational mass is defined by the quantity of gravitational field material a mass possesses, including its energy. The "inertial mass" (relativistic mass) is a function of the acceleration a mass has undergone and its resultant speed. A mass that has been accelerated to speeds close to the speed of light has its "relativistic mass" increased, and that is why the magnetic field strength in particle accelerators must be increased to force the mass's path to curve. In practice, "inertial mass" is normally taken to be "invariant mass" and so is identical to gravitational mass without the energy component.
Gravitational mass is measured by comparing the force of gravity of an unknown mass to the force of gravity of a known mass. This is typically done with some sort of balance. Equal masses will match on a balance because the gravitational field applies to them equally, producing identical weight. This assumption breaks down near supermassive objects such as black holes and neutron stars due to tidal effects. It also breaks down in weightless environments, because no matter what objects are compared, it will yield a balanced reading.
Inertial mass is found by applying a known net force to an unknown mass, measuring the resulting acceleration, and applying Newton's Second Law, m = F/a. This gives an accurate value for mass, limited only by the accuracy of the measurements. When astronauts need to be measured in the weightlessness of free fall, they actually find their inertial mass in a special chair called a body mass measurement device (BMMD).
At high speeds, and especially near the speed of light, inertial mass can be determined by measuring the magnetic field strength and the curvature of the path of an electrically-charged mass such as an electron.
No physical difference has been found between gravitational and inertial mass in a given inertial frame. In experimental measurements, the two always agree within the margin of error for the experiment. Einstein used the fact that gravitational and inertial mass were equal to begin his general theory of relativity in which he postulated that gravitational mass was the same as inertial mass, and that the acceleration of gravity is a result of a 'valley' or slope in the space-time continuum that masses 'fell down.' Dennis Sciama later showed that the reaction force produced by the combined gravity of all matter in the universe upon an accelerating object is mathematically equal to the object's inertia , but this would only be a workable physical explanation if by some mechanism the gravitational effects operated instantaneously.
At high speeds, relativistic mass always exceeds gravitational mass. If the mass is made to travel close to the speed of light, its "inertial mass" (relativistic) as observed from a stationary frame would be very great while its gravitational mass would remain at its rest value, but the gravitational effect of the extra energy would exactly balance the measured increase in inertial mass.
Inertial frames.
In a location such as a steadily moving railway carriage, a dropped ball (as seen by an observer in the carriage) would behave as it would if it were dropped in a stationary carriage. The ball would simply descend vertically. It is possible to ignore the motion of the carriage by defining it as an inertial frame. In a moving but non-accelerating frame, the ball behaves normally because the train and its contents continue to move at a constant velocity. Before being dropped, the ball was traveling with the train at the same speed, and the ball's inertia ensured that it continued to move in the same speed and direction as the train, even while dropping. Note that, here, it is inertia which ensured that, not its mass.
In an inertial frame all the observers in uniform (non-accelerating) motion will observe the same laws of physics. However observers in another inertial frame can make a simple, and intuitively obvious, transformation (the Galilean transformation), to convert their observations. Thus, an observer from outside the moving train could deduce that the dropped ball within the carriage fell vertically downwards.
However, in reference frames which are experiencing acceleration (non-inertial reference frames), objects appear to be affected by fictitious forces. For example, if the railway carriage were accelerating, the ball would not fall vertically within the carriage but would appear to an observer to be deflected because the carriage and the ball would not be traveling at the same speed while the ball was falling. Other examples of fictitious forces occur in rotating frames such as the earth. For example, a missile at the North Pole could be aimed directly at a location and fired southwards. An observer would see it apparently deflected away from its target by a force (the Coriolis force) but in reality the southerly target has moved because earth has rotated while the missile is in flight. Because the earth is rotating, a useful inertial frame of reference is defined by the stars, which only move imperceptibly during most observations.The law of inertia is also known as Isaac Newton's first law of motion.
In summary, the principle of inertia is intimately linked with the principles of conservation of energy and conservation of momentum.
Rotational inertia.
Another form of inertia is "rotational inertia" (→ moment of inertia), which refers to the fact that a rotating rigid body maintains its state of uniform rotational motion. Its angular momentum is unchanged, unless an external torque is applied; this is also called conservation of angular momentum. Rotational inertia depends on the object remaining structurally intact as a rigid body, and also has practical consequences; For example, a gyroscope uses the property that it resists any change in the axis of rotation.
Source of inertia; speculative theories.
Various efforts by notable physicists such as Ernst Mach (see Mach's principle), Albert Einstein, Dennis William Sciama, and Bernard Haisch have been put towards the study and theorizing of inertia. "An object at rest tends to stay at rest. An object in motion tends to stay in motion." A new theory has been proposed that explains inertia mechanistically as being due to the effect of Rindler horizons on the zero point field 

</doc>
<doc id="14910" url="http://en.wikipedia.org/wiki?curid=14910" title="Ibanez">
Ibanez

Ibanez (アイバニーズ, Aibanīzu) is a Japanese guitar brand owned by Hoshino Gakki. Based in Nagoya, Aichi, Japan, Hoshino Gakki were one of the first Japanese musical instrument companies to gain a significant foothold in import guitar sales in the United States and Europe, as well as the first brand of guitars to mass-produce the seven-string guitar and eight-string guitar.
History.
The Hoshino Gakki company began in 1908 as the musical instrument sales division of the "Hoshino Shoten", a bookstore company. The Ibanez brand name dates back to 1929 when Hoshino Gakki began importing Salvador Ibáñez guitars from Spain. After Telésforo Julve bought the company in 1933, Hoshino Gakki decided to make Spanish acoustic guitars in 1935, at first using the "Ibanez Salvador" brand name, and then later using the "Ibanez" brand name.
The modern era of Ibanez guitars began in 1957 and the late 1950s and 1960s Ibanez catalogues show guitars with some wild looking designs, manufactured by Guyatone, Kiso Suzuki Violin, and their own Tama factory. After Tama factory stopped guitar manufacturing in 1966, Hoshino Gakki used the Teisco and FujiGen Gakki guitar factories to manufacture Ibanez guitars, and after the Teisco guitar factory (Teisco String Instrument, Company) once closed down in 1969/1970 Hoshino Gakki used the FujiGen Gakki guitar factory to make most Ibanez guitars.
In the 1960s Japanese guitar makers started to mainly copy American guitar designs and Ibanez branded copies of Gibson, Fender and Rickenbacker models started to appear. This resulted in the so-called Ibanez lawsuit period. During this lawsuit period Ibanez started producing guitars under the Mann name to avoid authorities in the US and Canada. After the lawsuit period Hoshino Gakki introduced Ibanez models that were definitely not copies of the Gibson or Fender designs such as the Iceman and The Ibanez Roadstar. The company has produced its own guitar designs ever since. The late 1980s and early 1990s were an important period for the Ibanez brand. Hoshino Gakki's relationship with Frank Zappa's former guitarist Steve Vai resulted in the introduction of the Ibanez JEM and the Ibanez Universe models and after the earlier successes of the Roadstar and Iceman models in the late 1970s/early 1980s, Hoshino Gakki entered the superstrat market with the RG series which were a lower priced version of the Ibanez JEM model.
Hoshino Gakki also had semi acoustic, nylon and steel stringed acoustic guitars manufactured under the Ibanez name. Most Ibanez guitars were made for Hoshino Gakki by the FujiGen guitar factory in Japan up until the mid-to-late 1980s and from then on Ibanez guitars have also been made in other Asian countries such as Korea, China and Indonesia. During the early 1980s the FujiGen guitar factory also produced most of the Roland guitar synthesizers, including the Stratocaster-style Roland G-505, the twin-humbucker Roland G-202 (endorsed by Eric Clapton, Dean Brown, Jeff Baxter, Yannis Spathas, Steve Howe, Mike Rutherford, Andy Summers and Steve Hackett) and the Ibanez X-ING IMG-2010.
Cimar and Starfield were guitar brands owned by Hoshino Gakki. In the 1970s, Hoshino Gakki and Kanda Shokai shared some guitar designs and so some Ibanez and Greco guitars have the same features. The Kanda Shokai Greco guitars were sold in Japan and the Hoshino Gakki Ibanez guitars were sold outside of Japan. From 1982, Ibanez guitars have also been sold in Japan as well as being sold outside of Japan.
Guitar brands such as Antoria and Mann shared some Ibanez guitar designs. The Antoria guitar brand was managed by JT Coppock Leeds Ltd England. CSL was a brand name managed by Charles Summerfield Ltd England. Maurice Summerfield of the Charles Summerfield Ltd company contributed some design ideas to Hoshino Gakki and also imported Ibanez and CSL guitars into the UK with Hoshino Gakki cooperation from 1964–1987. The Maxxas brand name came about because Hoshino Gakki thought that the guitar did not fit in with the Ibanez model range and was therefore named Maxxas by Rich Lasner from Hoshino USA.
Lawsuit.
Harry Rosenbloom, founder of the (now-closed) Medley Music of Bryn Mawr, Pennsylvania, was manufacturing handmade guitars under the name "Elger." By 1965 Rosenbloom had decided to stop manufacturing guitars and chose to become the exclusive North American distributor for Ibanez guitars. In September 1972, Hoshino began a partnership with Elger Guitars to import guitars from Japan. In September 1981, Elger was renamed "Hoshino U.S.A.", retaining the company headquarters in Bensalem, Pennsylvania as a distribution and quality-control center.
On June 28, 1977, in the Philadelphia Federal District Court, a lawsuit was filed by the Norlin Corporation, the parent company of Gibson Guitars against Elger/Hoshino U.S.A.'s use of the Gibson headstock design and logo. Hoshino settled out of court in early 1978 and the case was officially closed on February 2, 1978.
After the lawsuit Hoshino Gakki abandoned the strategy of copying "classic" U.S.A. electric guitar designs—having already introduced a plethora of original designs. Hoshino was producing Artist models of their own design from 1974, introducing a set neck model in 1975. In 1977 they upgraded and extended their Artist range and introduced a number of other top quality original designs made to match or surpass famous American brands; the Performer and short-lived Concert ranges which competed with the Les Paul; through neck Musicians; Studios in fixed and through neck construction; the radically shaped Iceman and the Roadster which morphed into the Roadstar range, precursor to the popular superstrat era in the mid-1980s. The newer Ibanez models began incorporating more modern elements into their design such as radical body shapes, slimmer necks, 2-octave fingerboards, slim pointed headstocks, higher-output electronics, humbucker/single-coil/humbucker (H/S/H) pickup configurations, locking tremolo bridges and different finishes.
Guitars.
Discontinued Guitars.
Discontinued Guitars
Discontinued Signature Models
Effect pedals.
In the 1970s, the Nisshin Onpa company, who owned the Maxon brand name, developed and began selling a series of effect pedals in Japan. Hoshino Gakki licensed these for sale using the name Ibanez outside of Japan. These two companies eventually began doing less and less business together until Nisshin Onpa ceased manufacturing the TS-9 reissue for Hoshino Gakki in 2002.

</doc>
<doc id="14912" url="http://en.wikipedia.org/wiki?curid=14912" title="Incest">
Incest

Incest is sexual activity between family members or close relatives. This typically includes sexual activity between people in a consanguineous relationship (blood relations), and sometimes those related by affinity, such as individuals of the same household, step relatives, those related by adoption or marriage, or members of the same clan or lineage.
The incest taboo is and has been one of the most widespread of all cultural taboos, both in present and in many past societies. Most modern societies have laws regarding incest or social restrictions on closely consanguineous marriages. In societies where it is illegal, consensual adult incest is seen by some as a victimless crime. Some cultures extend the incest taboo to relatives with no consanguinity such as milk-siblings, step-siblings, and adoptive siblings. Third-degree relatives (such as half-aunt, half-nephew, first cousin) on average share 12.5% genes, and sexual relations between them is viewed differently in various cultures, from being discouraged to being socially acceptable. The children of incestuous relationships were regarded as illegitimate, and are still so regarded in some societies today. In most cases, the parents did not have the option to marry to remove that status, as incestuous marriages were and are normally also prohibited.
A common justification given for the incest taboo is the impact inbreeding may have on children of incestuous sex. Children whose biological parents have a close genetic relationship have an increased risk of congenital disorders, death, and disability due at least in part to genetic diseases caused by the inbreeding. Unintended sexual relations between genetically related persons may also arise when either or both biological parents are unknown or uncertain, as in the case of children born as a result of casual or extramarital sexual relations, anonymous sperm donation, surrogacy, or adoption. On the other hand, most prohibitions on incest extend the categories of prohibited relationships to affinity relationships such as in-law relations, step relations, and relations through adoption, among others. As such, the incest taboo is not solely based on inbreeding, and also applies to sexual activity between relatives (genetically related or otherwise) who cannot have children or to sexual activity between relatives where conception is not likely to occur (for example, because of the use of contraception).
In some societies, such as those of Ancient Egypt and others, brother–sister, father–daughter, mother–son, cousin–cousin, aunt–nephew, uncle–niece, and other combinations of relations were practiced among royalty as a means of perpetuating the royal lineage. Some societies, such as the Balinese and some Inuit tribes, have different views about what constitutes illegal and immoral incest. However, sexual relations with a first-degree relative (such as a parent or sibling) are almost universally forbidden.
Terminology.
The English word "incest" is derived from the Latin "incestus", which has a general meaning of "impure, unchaste".
It was introduced into Middle English, both in the generic Latin sense (preserved throughout the Middle English period) and in the narrow modern sense.
The derived adjective "incestuous" appears in the 16th century.
Before the Latin term came in, incest was known in Old English as "sibbleger" (from "sibb" 'kinship' + "leger" 'to lie') or "mǣġhǣmed" (from "mǣġ" 'kin, parent' + "hǣmed" 'sexual intercourse') but in time, both words fell out of use.
History.
Antiquity.
In ancient China, first cousins with the same surnames (i.e., those born to the father's brothers) were not permitted to marry, while those with different surnames (i.e., maternal cousins and paternal cousins born to the father's sisters) were.
Several of the Egyptian Pharaohs married their siblings and had several children with them. For example, Tutankhamun married his half-sister Ankhesenamun, and was himself the child of an incestuous union between Akhenaten and an unidentified sister-wife. It is now generally accepted that sibling marriages were widespread among all classes in Egypt during the Graeco-Roman period. Numerous papyri and the Roman census declarations attest to many husbands and wives being brother and sister, of the same father and mother. The most famous of these relationships were in the Ptolemaic royal family; Cleopatra VII was married to her younger brother, Ptolemy XIII, while her mother and father, Cleopatra V and Ptolemy XII, had also been brother and sister.
The fable of "Oedipus", with a theme of inadvertent incest between a mother and son, ends in disaster and shows ancient taboos against incest as Oedipus is punished for incestuous actions by blinding himself. In the "sequel" to Oedipus, "Antigone", his four children are also punished for their parents' incestuousness. Incest appears in the commonly accepted version of the birth of Adonis, when his mother, Myrrha has sex with her father Cinyras during a festival, disguised as a prostitute.
In Ancient Greece, Spartan King Leonidas I, hero of the legendary Battle of Thermopylae, was married to his niece Gorgo, daughter of his half-brother Cleomenes I. Greek law allowed marriage between a brother and sister if they had different mothers. For example, some accounts say that Elpinice was for a time married to her half-brother Cimon.
Incest is mentioned and condemned in Virgil's "Aeneid" Book VI: "hic thalamum invasit natae vetitosque hymenaeos;" "This one invaded a daughter's room and a forbidden sex act".
Roman civil law prohibited marriages within four degrees of consanguinity but had no degrees of affinity with regards to marriage. Roman civil laws prohibited any marriage between parents and children, either in the ascending or descending line "ad infinitum". Adoption was considered the same as affinity in that an adoptive father could not marry an unemancipated daughter or granddaughter even if the adoption had been dissolved. Incestuous unions were discouraged and considered "nefas" (against the laws of gods and man) in ancient Rome. In AD 295 incest was explicitly forbidden by an imperial edict, which divided the concept of "incestus" into two categories of unequal gravity: the "incestus iuris gentium," which was applied to both Romans and non-Romans in the Empire, and the "incestus iuris civilis," which concerned only Roman citizens. Therefore, for example, an Egyptian could marry an aunt, but a Roman could not. Despite the act of incest being unacceptable within the Roman Empire, Roman Emperor Caligula is rumored to have had sexual relationships with all three of his sisters (Julia Livilla, Drusilla, and Agrippina the Younger). Emperor Claudius, after executing his previous wife, married his brother's daughter Agrippina the Younger, and changed the law to allow an otherwise illegal union. The law prohibiting marrying a sister's daughter remained. The taboo against incest in Ancient Rome is demonstrated by the fact that politicians would use charges of incest (often false charges) as insults and means of political disenfranchisement.
In Norse mythology, there are themes of brother-sister marriage, a prominent example being between Njörðr and his unnamed sister (perhaps Nerthus), parents of Freyja and Freyr. Loki in turn also accuses Freyja and Freyr of having a sexual relationship.
Biblical references.
According to the Book of Jubilees, Cain married his sister Awan. 
According to of the Hebrew Bible, the Patriarch Abraham and his wife Sarah were half-siblings, both being children of Terah, but with different mothers. According to 2 Samuel, Amnon, King David's son, raped his half-sister, Tamar ().
In Genesis 19:30-38, living in an isolated area after the destruction of Sodom and Gomorrah, Lot's two daughters conspired to inebriate and seduce their father due to the lack of available partners. Because of intoxication, Lot "perceived not" when his firstborn, and the following night his younger daughter, lay with him. (Genesis 19:32-35)
From the Middle Ages onward.
Many European monarchs were related due to political marriages, sometimes resulting in distant cousins (and even first cousins) being married. This was especially true in the Habsburg, Hohenzollern, Savoy and Bourbon royal houses. However, relations between siblings, that may have been tolerated in other cultures, was considered abhorrent. For example, the accusation that Anne Boleyn and her brother George Boleyn had committed incest was one of the reasons that both siblings were executed in May 1536.
Incestuous marriages were also seen in the royal houses of ancient Japan and Korea, Inca Peru, Ancient Hawaii, and, at times, Central Africa, Mexico, and Thailand. Like the pharaohs of ancient Egypt, the Inca rulers married their sisters. Huayna Capac, for instance, was the son of Topa Inca Yupanqui and the Inca's sister and wife.
Half-sibling marriages were found in ancient Japan such as the marriage of Emperor Bidatsu and his half-sister Empress Suiko. Japanese Prince Kinashi no Karu had sexual relationships with his full sister Princess Karu no Ōiratsume, although the action was regarded as foolish. In order to prevent the influence of the other families, a half-sister of Korean Goryeo Dynasty monarch Gwangjong became his wife in the 10th century. Her name was Daemok. Brother-sister marriages were common during some Roman periods as some census records have shown.
In the South Indian state of Tamil Nadu, it is a widely practiced custom for men to marry their sisters' daughters. The Tamil word for maternal uncle is 'mama' which is also a term generally used by wives to address their husbands.
Prevalence and statistics.
Incest between an adult and a person under the age of consent is considered a form of child sexual abuse that has been shown to be one of the most extreme forms of childhood abuse; it often results in serious and long-term psychological trauma, especially in the case of parental incest. Its prevalence is difficult to generalize, but research has estimated 10–15% of the general population as having at least one such sexual contact, with less than 2% involving intercourse or attempted intercourse. Among women, research has yielded estimates as high as 20%.
Father-daughter incest was for many years the most commonly reported and studied form of incest. More recently, studies have suggested that sibling incest, particularly older brothers having sexual relations with younger siblings, is the most common form of incest, with some studies finding sibling incest occurring more frequently than other forms of incest. Some studies suggest that adolescent perpetrators of sibling abuse choose younger victims, abuse victims over a lengthier period, use violence more frequently and severely than adult perpetrators, and that sibling abuse has a higher rate of penetrative acts than father or stepfather incest, with father and older brother incest resulting in greater reported distress than stepfather incest.
Types.
Between adults and children.
Incest between an adult and a child is usually considered a form of child sexual abuse and for many years has been the most reported form of incest. Father–daughter and stepfather–stepdaughter incest is the most commonly reported form of adult-child incest, with most of the remaining involving a mother or stepmother. Many studies found that stepfathers tend to be far more likely than biological fathers to engage in this form of incest. One study of adult women in San Francisco estimated that 17% of women were abused by stepfathers and 2% were abused by biological fathers. This is thought to be explained by the Westermarck effect. Father–son incest is reported less often, but it is not known how close the frequency is to heterosexual incest because it is likely more under-reported. Prevalence of incest between parents and their children is difficult to assess due to secrecy and privacy.
In a 1999 news story, "BBC" reported, "Close-knit family life in India masks an alarming amount of sexual abuse of children and teenage girls by family members, a new report suggests. Delhi organisation RAHI said 76% of respondents to its survey had been abused when they were children - 40% of those by a family member."
According to the National Center for Victims of Crime a large proportion of rape committed in the United States is perpetrated by a family member:
Research indicates that 46% of children who are raped are victims of family members (Langan and Harlow, 1994). The majority of American rape victims (61%) are raped before the age of 18; furthermore, 29% of all rapes occurred when the victim was less than 11 years old. 11% of rape victims are raped by their fathers or step-fathers, and another 16% are raped by other relatives".
A study of victims of father–daughter incest in the 1970s showed that there were "common features" within families before the occurrence of incest: estrangement between the mother and the daughter, extreme paternal dominance, and reassignment of some of the mother's traditional major family responsibility to the daughter. Oldest and only daughters were more likely to be the victims of incest. It was also stated that the incest experience was psychologically harmful to the woman in later life, frequently leading to feelings of low self-esteem, very unhealthy sexual activity, contempt for other women, and other emotional problems.
Adults who as children were incestuously victimized by adults often suffer from low self-esteem, difficulties in interpersonal relationships, and sexual dysfunction, and are at an extremely high risk of many mental disorders, including depression, anxiety, phobic avoidance reactions, somatoform disorder, substance abuse, borderline personality disorder, and complex post-traumatic stress disorder. Research by Leslie Margolin indicates that mother-son incest does not trigger some innate biological response, but that the effects are more directly related to the symbolic meanings attributed to this act by the participants.
The Goler clan in Nova Scotia is a specific instance in which child sexual abuse in the form of forced adult/child and sibling/sibling incest took place over at least three generations. A number of Goler children were victims of sexual abuse at the hands of fathers, mothers, uncles, aunts, sisters, brothers, cousins, and each other. During interrogation by police, several of the adults openly admitted to engaging in many forms of sexual activity, up to and including full intercourse, multiple times with the children. Sixteen adults (both men and women) were charged with hundreds of allegations of incest and sexual abuse of children as young as five. In July 2012, twelve children were removed from the 'Colt' family (a pseudonym) in New South Wales, Australia, after the discovery of four generations of incest. Child protection workers and psychologists said interviews with the children indicated "a virtual sexual free-for-all."
In Japan, there is a popular misconception that mother-son incestuous contact is common, due to the manner in which it is depicted in the press and popular media. According to Hideo Tokuoka, "When Americans think of incest, they think of fathers and daughters; in Japan one thinks of mothers and sons" due to the extensive media coverage of mother-son incest there. Some western researchers assumed that mother-son incest is common in Japan, but research into victimization statistics from police and health-care systems discredits this; it shows that the vast majority of sexual abuse, including incest, in Japan is perpetrated by men against young girls. The Mainichi Daily News column WaiWai, by Australian journalist Ryann Connell, featured often-sensationalist stories, principally translated from and based on articles appearing in Japanese tabloids. On June 28, 2008, The Mainichi newspaper announced punitive measures. Mainichi said, "We continued to post articles that contained incorrect information and indecent sexual content. These articles, many of which were not checked and properly investigated should not have been dispatched. We apologize deeply for causing many people trouble and for betraying the public's trust in the Mainichi Shimbun." 
Between childhood siblings.
Childhood sibling–sibling incest is considered to be widespread but rarely reported. Sibling-sibling incest becomes child-on-child sexual abuse when it occurs without consent, without equality, or as a result of coercion. In this form, it is believed to be the most common form of intrafamilial abuse. The most commonly reported form of abusive sibling incest is abuse of a younger sibling by an older sibling. A 2006 study showed a large portion of adults who experienced sibling incest abuse have distorted or disturbed beliefs (such as that the act was "normal") both about their own experience and the subject of sexual abuse in general.
Sibling abusive incest is most prevalent in families where one or both parents are often absent or emotionally unavailable, with the abusive siblings using incest as a way to assert their power over a weaker sibling. Absence of the father in particular has been found to be a significant element of most cases of sexual abuse of female children by a brother. The damaging effects on both childhood development and adult symptoms resulting from brother–sister sexual abuse are similar to the effects of father–daughter, including substance abuse, depression, suicidality, and eating disorders.
Between consenting adults.
Sexual activity between adult close relatives may arise from genetic sexual attraction. This form of incest has not been widely reported in the past, but recent evidence has indicated that this behavior does take place, possibly more often than many people realize. Internet chatrooms and topical websites exist that provide support for incestuous couples.
Proponents of incest between consenting adults draw clear boundaries between the behavior of consenting adults and rape, child molestation, and abusive incest. According to one incest participant who was quoted for an article in "The Guardian":
You can't help who you fall in love with, it just happens. I fell in love with my sister and I'm not ashamed ... I only feel sorry for my mom and dad, I wish they could be happy for us. We love each other. It's nothing like some old man who tries to fuck his three-year-old, that's evil and disgusting ... Of course we're consenting, that's the most important thing. We're not fucking perverts. What we have is the most beautiful thing in the world.
In "Slate", William Saletan drew a legal connection between gay sex and incest between consenting adults. As he described in his article, in 2003, U.S. Senator Rick Santorum commented on a pending U.S. Supreme Court case involving sodomy laws (primarily as a matter of Constitutional rights to Privacy and Equal Protection under the Law). He stated: "If the Supreme Court says that you have the right to consensual sex within your home, then you have the right to bigamy, you have the right to polygamy, you have the right to incest, you have the right to adultery." However, David Smith of the Human Rights Campaign professed outrage that Santorum placed being gay on the same moral and legal level as someone engaging in incest. Saletan argued that, legally and morally, there is essentially no difference between the two, and went on to support incest between consenting adults being covered by a legal right to privacy. UCLA law professor Eugene Volokh has made similar arguments. In a more recent article, Saletan said that incest is wrong because it introduces the possibility of irreparably damaging family units by introducing "a notoriously incendiary dynamic—sexual tension—into the mix".
Aunts, uncles, nieces or nephews.
In the Netherlands, marrying one's nephew or niece is legal, but only with the explicit permission of the Dutch Government, due to the possible risk of genetic defects among the offspring. Nephew-niece marriages predominantly occur among foreign immigrants. In November 2008, the Christian democratic (CDA) party's Scientific Institute announced that it wanted a ban on marriages between nephews and nieces.
Consensual sex between adults (persons of 18 years and older) is always lawful in the Netherlands and Belgium, even among closely related family members. Sexual acts between an adult family member and a minor are illegal, though they are not classified as incest, but as abuse of the authority such an adult has over a minor, comparable to that of a teacher, coach or priest. (Dutch)
In Florida, consensual adult sexual intercourse with someone known to be your aunt, uncle, niece or nephew constitutes a felony of the third degree. Other states also commonly prohibit marriages between such kin. The legality of sex with a half-aunt or half-uncle varies state by state.
In the United Kingdom, incest includes only sexual intercourse with a parent, grandparent, child or sibling but the more recently introduced offence of "sex with an adult relative" extends also as far as half-siblings, uncles, aunts, nephews and nieces. The term 'incest' however remains widely used in popular culture to describe any form of sexual activity with a relative.
Between adult siblings.
The most public case of consensual adult sibling incest in recent years is the case of a brother-sister couple from Germany, Patrick Stübing and Susan Karolewski. Because of violent behavior on the part of the father, the brother was taken in at the age of 3 by foster parents, who adopted him later. At the age of 23 he learned about his biological parents, contacted his mother, and met her and his then 16-year-old sister for the first time. The now-adult brother moved in with his birth family shortly thereafter. After their mother died suddenly six months later, the couple became intimately close, and had their first child together in 2001. By 2004, they had four children together: Eric, Sarah, Nancy, and Sofia. The public nature of their relationship, and the repeated prosecutions and even jail time they have served as a result, has caused some in Germany to question whether incest between consenting adults should be punished at all. An article about them in "Der Spiegel" states that the couple are happy together. According to court records, the first three children have mental and physical disabilities, and have been placed in foster care. In April 2012, at the European Court of Human Rights, Patrick Stuebing lost his case that the conviction violated his right to a private and family life. September 24, 2014 the German Ethics Council has recommended that the government abolish laws criminalizing incest between siblings, arguing that such bans impinge upon citizens.
Some societies differentiate between full sibling and half sibling relations. In ancient societies, full sibling and half sibling marriages occurred. Sexual relations with half-aunts or half-uncles are illegal in some US states.
Cousin relationships.
Marriages and sexual relationships between first cousins are stigmatized as incest in some cultures, but tolerated in much of the world. Currently, 24 US states prohibit marriages between first cousins, and another seven permit them only under special circumstances.
The United Kingdom permits both marriage and sexual relations between first cousins. Cousin marriages are rare, accounting for less than 1% of marriages in Western Europe, North America and Oceania, while reaching 9% in South America, East Asia and South Europe and up to 25% in regions of the Middle East, North Africa and South Asia. Communities such as the Dhond and the Bhittani of Pakistan clearly prefer marriages between cousins as believe they ensure purity of the descent line, provide intimate knowledge of the spouses, and ensure that patrimony will not pass into the hands of "outsiders".
There are some cultures in Asia which stigmatize cousin marriage, in some instances even marriages between second cousins or more remotely related people. This is notably true in the culture of Korea. In South Korea, before 1997, anyone with the same last name and clan were prohibited from marriage. In light of this law being held unconstitutional, South Korea now only prohibits up to third cousins (see Article 809 of the Korean Civil Code). Hmong culture prohibits the marriage of anyone with the same last name - to do so would result in being shunned by the entire community, and they are usually stripped of their last name. Some Hindu communities in India prohibit cousin marriages.
In a review of 48 studies on the children parented by cousins, the rate of birth defects was twice that of non-related couples: 4% for cousin couples as opposed to 2% for the general population. Thus most, 96%, of the babies born to cousins were healthy.
Defined through marriage.
Some cultures include relatives by marriage in incest prohibitions; these relationships are called affinity rather than consanguinity. For example, the question of the legality and morality of a widower who wished to marry his deceased wife's sister was the subject of long and fierce debate in the United Kingdom in the 19th century, involving, among others, Matthew Boulton and Charles La Trobe. The marriages were entered into in Scotland and Switzerland respectively, where they were legal. In medieval Europe, standing as a godparent to a child also created a bond of affinity. But in other societies, a deceased spouse's sibling was considered the ideal person to marry. The Hebrew Bible forbids a man from marrying his brother's widow with the exception that, if his brother died childless, the man is instead required to marry his brother's widow so as to "raise up seed to him" (per ).
In Islamic law, marriage among close blood relations like parents, step-parent, parents in-law, siblings, step-siblings, the children of siblings, aunts and uncles is prohibited, while first or second cousins may marry. Marrying the widow of a brother, or the sister of deceased or divorced wife is also allowed.
Inbreeding.
Offspring of biologically related parents are subject to the possible impact of inbreeding. Such offspring have a higher possibility (see Coefficient of relationship) of congenital birth defects because it increases the proportion of zygotes that are homozygous for deleterious recessive alleles that produce such disorders (see Inbreeding depression). Because most such alleles are rare in populations, it is unlikely that two unrelated marriage partners will both be heterozygous carriers. However, because close relatives share a large fraction of their alleles, the probability that any such rare deleterious allele present in the common ancestor will be inherited from both related parents is increased dramatically with respect to non-inbred couples. Contrary to common belief, inbreeding does not in itself alter allele frequencies, but rather increases the relative proportion of homozygotes to heterozygotes. However, because the increased proportion of deleterious homozygotes exposes the allele to natural selection, in the long run its frequency decreases more rapidly in inbred population. In the short term, incestuous reproduction is expected to produce increases in spontaneous abortions of zygotes, perinatal deaths, and postnatal offspring with birth defects. This also means that the closer two persons are related, the more severe are the biological costs of inbreeding. This fact likely explains why inbreeding between close relatives, such as siblings, is less common than inbreeding between cousins.
There may also be other deleterious effects besides those caused by recessive diseases. Thus, similar immune systems may be more vulnerable to infectious diseases (see Major histocompatibility complex and sexual selection).
A 1994 study found a mean excess mortality with inbreeding among first cousins of 4.4%. Children of parent-child or sibling-sibling unions are at increased risk compared to cousin-cousin unions. Studies suggest that 20-36% of these children will die or have major disability due to the inbreeding. A study of 29 offspring resulting from brother-sister or father-daughter incest found that 20 had congenital abnormalities, including four directly attributable to autosomal recessive alleles.
Animals.
Many mammal species, including humanity's closest primate relatives, tend to avoid mating with close relatives, especially if there are alternative partners available. However, some chimpanzees have been recorded attempting to mate with their mothers. Male rats have been recorded engaging in mating with their sisters, but they tend to prefer non-related females over their sisters.
Livestock breeders often practice controlled breeding to eliminate undesirable characteristics within a population, which is also coupled with culling of what is considered unfit offspring, especially when trying to establish a new and desirable trait in the stock.
Laws.
Laws regarding sexual activity between close relatives vary considerably between jurisdictions, and depend on the type of sexual activity and the nature of the family relationship of the parties involved, as well as the age and sex of the parties. Prohibition of incest laws may extend to restrictions on marriage rights, which also vary between jurisdictions. Most jurisdictions prohibit parent-child and sibling marriages, while other also prohibit first-cousin and uncle-niece and aunt-nephew marriages. In most places, incest is illegal, regardless of the ages of the two partners. In other countries, incestuous relationships between consenting adults (with the age varying by location) are permitted, including in the Netherlands, France, and Spain. In Sweden, the only type of incestuous relationship allowed by law is that between half-siblings and they must seek government counseling before marriage.
Religious views.
Jewish.
In three places in the Torah, there are lists of family members between whom it is prohibited to have sexual relations; each of these lists is progressively shorter. The biblical lists are not symmetrical – the implied rules for women and men are not the same. Relationships compare as follows:
  Forbidden for men
  Forbidden for women
  Forbidden for both men and women
Apart from the questionable case of the daughter , the first incest list in the Holiness code roughly produces the same rules as were followed in early (pre-Islamic) Arabic culture; in Islam, these pre-existing rules were made statutory.
In the 4th century BCE, the Soferim ("scribes") declared that there were relationships within which marriage constituted incest, in addition to those mentioned by the Torah. These additional relationships were termed "seconds" (Hebrew: "sheniyyot"), and included the wives of a man's grandfather and grandson. The classical rabbis prohibited marriage between a man and any of these "seconds" of his, on the basis that doing so would act as a "safeguard" against infringing the biblical incest rules, although there was inconclusive debate about exactly what the limits should be for the definition of "seconds".
Marriages forbidden in the Torah were regarded by the rabbis of the Middle Ages as invalid – as if they had never occurred; any children born to such a couple were regarded as bastards under Jewish law, and the relatives of the spouse were not regarded as forbidden relations for a further marriage. On the other hand, those relationships which were prohibited due to qualifying as "seconds", and so forth, were regarded as wicked, but still valid; while they might have pressured such a couple to divorce, any children of the union were still seen as legitimate.
Christian.
The Catholic Church does not generally permit the marriage if a doubt exists on whether the potential spouses are related by blood relations in any degree of the direct line or in the second degree of the collateral line.
Definitions of incest varied throughout history. The Fourth Lateran Council held in 1215 attempted to codify that marriage was forbidden up to and including third cousins, though permissible beyond this for fourth cousins, third cousins once removed, etc.
In the Eastern Orthodox Church, marriages are banned between second cousins or closer and between second uncles / aunts and second nieces / nephews (between first cousins once removed) or closer. Marrying one's godparent or deceased spouse's sibling is also prohibited, although marrying one's stepchild is not - e.g. Vyacheslav Ivanov exercised his right to marry his stepdaughter after her mother's (his first wife's) death.
The Book of Common Prayer of the Anglican Communion allows marriages up to and including first cousins.
Islamic.
The Quran gives specific rules regarding incest, which prohibit a man from marrying or having sexual relationships with:
The main differences (apart from relationships between a man and his daughter) are:
Zoroastrian.
In Zoroastrianism, incest between parent-child or brothers-sisters is virtue. Incest is called Xvaetvadatha. Incest is performed with all the statuses. Friedrich Nietzsche, in his book "The Birth of Tragedy", cites that among Zoroastrians a wise priest is borne only by incest.
Hindu.
Rigveda regard incest to be "evil". Hinduism speaks of incest in abhorrent terms. Hindus are fearful of the bad effects of incest and thus practice strict rules of both endogamy and exogamy, as well as same family tree ("gotra") or bloodline ("Pravara"). Marriages within the gotra ("swagotra" marriages) are banned under the rule of exogamy in the traditional matrimonial system. People within the gotra are regarded as kin and marrying such a person would be thought of as incest. i.e. Marriage with paternal cousins is strictly prohibited. In fact marriage between two people whose parents are related paternally up to several generations is expressly prohibited. Gotra is transferred down the male lineage while the Gotra of a female changes upon marriage. i.e., upon marriage a woman would belong to her husband's lineage.
The marriage with a person having same Gotra as of the original Gotras (lineage) is prohibited.
Buddhist.
Buddhist societies take a strong ethical stand in human affairs and sexual behavior in particular. Most variations of Buddhism decide locally about the details of incest as a wrongdoing, according to local cultural standards. Sexual misconduct is mentioned but the definition of what constitutes misconduct sex is an individual issue. The most common formulation of Buddhist ethics are the Five Precepts and the Noble Eightfold Path: one should neither be attached to nor crave sensual pleasure. These precepts take the form of voluntary, personal undertakings, not divine mandate or instruction. The third of the Five Precepts is "To refrain from committing sexual misconduct". 'Sexual misconduct' means any sexual conduct involving violence, manipulation or deceit – conduct that therefore leads to suffering and trouble.
Buddhist monks and nuns strictly forbid any type of sexual misconduct, and therefore depends on the culture of the area, not on mandate from Buddhism itself.

</doc>
<doc id="14914" url="http://en.wikipedia.org/wiki?curid=14914" title="Industrial Revolution">
Industrial Revolution

The Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, improved efficiency of water power, the increasing use of steam power, and the development of machine tools. It also included the change from wood and other bio-fuels to coal. Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested; the textile industry was also the first to use modern production methods.
The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.
The Industrial Revolution began in Great Britain, and spread to Western Europe and North America within a few decades. The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals, plants and fire.
The First Industrial Revolution evolved into the Second Industrial Revolution in the transition years between 1840 and 1870, when technological and economic progress continued with the increasing adoption of steam transport (steam-powered railways, boats and ships), the large-scale manufacture of machine tools and the increasing use of machinery in steam-powered factories.
Etymology.
The earliest recorded use of the term "Industrial Revolution" seems to have been in a letter of 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book "", Raymond Williams states in the entry for "Industry": "The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century." The term "Industrial Revolution" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui description in 1837 of "la révolution industrielle". Friedrich Engels in "The Condition of the Working Class in England" in 1844 spoke of "an industrial revolution, a revolution which at the same time changed the whole of civil society". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.
Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term "revolution" is a misnomer. This is still a subject of debate among historians.
Important technological developments.
The commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:
Textile manufacture.
In the late 17th and early 18th centuries the British government passed a series of Calico Acts in order to protect the domestic woollen industry from the increasing amounts of cotton fabric imported from India.
The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.
On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel it took anywhere from four to eight spinners to supply one hand loom weaver. The flying shuttle patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box.
Lewis Paul patented the roller spinning machine and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743, a factory opened in Northampton with fifty spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.
In 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40 spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.
The spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clock maker John Kay, who was hired by Arkwright. For each spindle, the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.
Samuel Crompton's Spinning Mule, introduced in 1779, was a combination of the spinning jenny and the water frame in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce good quality calico cloth.
Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom, that was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.
The demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. With a cotton gin a man could remove seed from as much upland cotton in one day as would have previously taken a woman working two months to process at one pound per day.
Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. This in turn fed a weaving industry that advanced with improvements to shuttles and the loom or 'frame'. The output of an individual labourer increased dramatically, with the effect that the new machines were seen as a threat to employment, and early innovators were attacked and their inventions destroyed.
To capitalise upon these advances, it took a class of entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power—first horse power and then water power—which made cotton manufacture a mechanised industry. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.
Metallurgy.
A major change in the metal industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was more abundant than wood.
Use of coal in smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulfur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. The foundry cupola is a different (and later) innovation.
This was followed by Abraham Darby, who made great strides using coke to fuel his blast furnaces at Coalbrookdale in 1709. However, the coke pig iron he made was used mostly for the production of cast-iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs. Coke pig iron was hardly used to produce bar iron in forges until the mid-1750s, when his son Abraham Darby II built Horsehay and Ketley furnaces (not far from Coalbrookdale). By then, coke pig iron was cheaper than charcoal pig iron. Since cast iron was becoming cheaper and more plentiful, it began being a structural material following the building of the innovative Iron Bridge in 1778 by Abraham Darby III.
Bar iron for smiths to forge into consumer goods was still made in finery forges, as it long had been. However, new processes were adopted in the ensuing years. The first is referred to today as potting and stamping, but this was superseded by Henry Cort's puddling process.
Henry Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Rolling replaced hammering for consolidating wrought iron and expelling some of the dross. Rolling was 15 times faster than hammering with a trip hammer. Puddling produced a structural grade iron at a relatively low cost.
Puddling was a means of decarburizing pig iron by slow oxidation, with iron ore as the oxygen source, as the iron was manually stirred using a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Puddling was done in a reverberatory furnace, allowing coal or coke to be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised.
Up to that time, British iron manufacturers had used considerable amounts of imported iron to supplement native supplies. This came principally from Sweden from the mid-17th century and later also from Russia from the end of the 1720s. However, from 1785, imports decreased because of the new iron making technology, and Britain became an exporter of bar iron as well as manufactured wrought iron consumer goods.
Hot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using waste exhaust heat to preheat combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coal or two-thirds using coke; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.
Two decades before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.
The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.
Steam power.
The development of the stationary steam engine was an important element of the Industrial Revolution; however, for most of the period of the Industrial Revolution, the majority of industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp. Small power requirements continued to be provided by animal and human muscle until the late 19th century.
The first real attempt at industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and tried in a few mines (hence its "brand name", "The Miner's Friend"). Savery's pump was economical in small horspower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.
The first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were successfully put to use in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a lot of capital to build, and produced about 5 hp. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s, the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.
A fundamental change in working principles was brought about by Scotsman James Watt. In close collaboration with Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder thereby making the low pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20-25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry, for the manufacture of such engines, in 1795.
By 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from 5 to 10 hp.
The development of machine tools, such as the lathe, planing and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.
Until about 1800, the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained portative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, the Cornish engineer Richard Trevithick, and the American, Oliver Evans began to construct higher pressure non-condensing steam engines, exhausting against the atmosphere. This allowed an engine and boiler to be combined into a single unit compact enough to be used on mobile road and rail locomotives and steam boats.
In the early 19th century after the expiration of Watt's patent, the steam engine underwent many improvements by a host of inventors and engineers.
Machine tools.
The Industrial Revolution created a demand for metal parts used in machinery. This led to the development of several machine tools for cutting metal parts. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.
Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal was kept to a minimum. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Hand methods of production were very laborious and costly and precision was difficult to achieve. Pre-industrial machinery was built by various craftsmen—millwrights built water and wind mills, carpenters made wooden framing, and smiths and turners made metal parts.
The first large machine tool was the cylinder boring machine used for boring the large-diameter cylinders on early steam engines. The planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.
Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He was hired away by Joseph Bramah for the production of high security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. Maudslay's lathe was called one of history's most important inventions.
Maudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.
James Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.
The impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. In the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the economy, by value added, in the U.S.
Chemicals.
The large scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around 100 lb in each of the chambers, at least a tenfold increase.
The production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash,
and also to potash (potassium carbonate) derived from hardwood ashes.
These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.
The development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.
After 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead the practice was to hire German-trained chemists.
Cement.
In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about 1400 °C, then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel.
Cement was used on a large scale in the construction of the London sewerage system a generation later.
Gas lighting.
Another major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton and Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting had an impact on social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed night life to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.
Glass making.
A new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832, this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure..
Paper machine.
A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.
The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.
Agriculture.
The British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy.
Industrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.
Jethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.
Joseph Foljambe's "Rotherham plough" of 1730, was the first commercially successful iron plough. The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.
Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.
Mining.
Coal mining in Britain, particularly in South Wales started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable.
Coal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.
Other developments.
Other developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.
Transportation.
At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagon ways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century.
The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.
Canals.
Canals were the first technology to allow bulk materials to be economically transported long distances inland. 
This was because a horse could pull a barge with a load dozens of times larger than the load that was pullable in a cart cart.
Building of canals dates to ancient times. The Grand Canal in China, "the world's largest artificial waterway and oldest canal still in existence," parts of which were started between the 6th and 4th centuries BC, is 1121 mi long and links Hangzhou with Beijing.
In the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ as of 2013), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.
By the 1820s, a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.
Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.
Roads.
Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stage coaches carried the rich, and the less wealthy could pay to ride on carriers carts.
Railways.
Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy
Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, U.K.
“ A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”
Steam locomotives began being built after the introduction of high pressure steam engines around 1800. These engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.
On 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.
Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.
Social effects.
Standards of living.
The effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.
Some economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that "for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behavior is mentioned by the classical economists, even as a theoretical possibility." Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s.
Food and nutrition.
Chronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years, and only slightly higher in Britain. The US population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years.
In Britain and the Netherlands, food supply had been increasing and prices falling before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. Before the Industrial Revolution, advances in agriculture or technology soon led to an increase in population, which again strained food and other resources, limiting increases in per capita income. This condition is called the Malthusian trap, and it was finally overcome by industrialisation.
Transportation improvements, such as canals and improved roads, also lowered food costs. Railroads were introduced near the end of the Industrial Revolution.
Housing.
Living conditions during the Industrial Revolution varied from splendour for factory owners to squalor for workers.
In "The Condition of the Working Class in England" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shantytowns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. Eight to ten unrelated mill workers often shared a room, often with no furniture, and slept on a pile of straw or sawdust. Toilet facilities were shared if they existed. Disease spread through a contaminated water supply. Also, people were at risk of developing pathologies due to persistent dampness.
The famines that troubled rural areas did not happen in industrial areas. But urban people—especially small children—died due to diseases spreading through the cramped living conditions. Tuberculosis (spread in congested dwellings), lung diseases from the mines, cholera from polluted water and typhoid were also common.
Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of professionals, such as lawyers and doctors, who lived in much better conditions.
Conditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved.
Clothing and consumer goods.
Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating.
Population increase.
According to Robert Hughes in "The Fatal Shore", the population of England and Wales, which had remained steady at 6 million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.
The Industrial Revolution was the first period in history during which there was a simultaneous increase in population and in per capita income.
Labour conditions.
Social structure and working conditions.
In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry.
Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life. However, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel—child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.
Factories and urbanisation.
Industrialisation led to the creation of the factory. Arguably the first highly mechanised was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, since the silk industry there was a closely guarded secret, the state of the industry there is unknown. Because Lombe's factory was not successful and there was no follow through, the rise of the modern factory dates to somewhat later when cotton spinning was mechanised.
The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed "Cottonopolis", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.
For much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.
The transition to industrialisation was not without difficulty. For example, a group of English workers known as Luddites formed to protest against industrialisation and sometimes sabotaged factories.
In other industries the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.
By 1746, an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.
Child labour.
The Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although "infant" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.
Child labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10%-20% of an adult male's wage. Children as young as four were employed. Beatings and long hours were common, with some child coal miners and hurriers working from 4am until 5pm. Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions. Many children developed lung cancer and other diseases and died before the age of 25. Workhouses would sell orphans and abandoned children as "pauper apprentices", working without wages for board and lodging. Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape. Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated. Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw. Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.
Reports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.
Politicians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. These laws decreased the number of child labourers, however child labour remained in Europe and the United States up to the 20th century.
Luddites.
The rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.
Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.
Organisation of labour.
The Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of "combinations" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.
The main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted.
In 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than 10 shillings a week, although by this time wages had been reduced to 7 shillings a week and were due to be further reduced to 6. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs.
In the 1830s and 1840s the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its "Charter" of reforms received over three million signatures but was rejected by Parliament without consideration.
Working people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.
Unions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.
Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to became the British Labour Party.
Other effects.
The application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which reinforced rising literacy and demands for mass political participation.
During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.
The growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.
Industrialisation beyond Great Britain.
Continental Europe.
Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830. The Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.
Belgium.
Belgium was the second country, after Britain, in which the industrial revolution took place and the first in continental Europe:
Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.
Wallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word "houille" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its "Sillon industriel", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, (...) there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: "It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain." "The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent." Michel De Coster, Professor at the Université de Liège wrote also: "The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory (...) But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated" 
Demographic effects.
Wallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the "Sillon industriel", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:
The industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres (...) at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent (...) Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.
France.
The industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear "take-off". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:
Germany.
Based on its leadership in chemical research in the universities and industrial laboratories, Germany became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.
Germany's political disunity—with three dozen states—and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France
Sweden.
During the period 1790–1815 Sweden experienced two parallel economic movements: an agricultural revolution with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a protoindustrialisation, with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a consumption revolution starting in the 1820s.
During 1815–1850 the protoindustries developed into more specialized and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.
During 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.
During 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.
United States.
The United States originally used horse-powered machinery to power its earliest factories, but eventually switched to water power. As a result, industrialisation was essentially limited to New England and the rest of Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. However, raw materials (especially cotton) came from the Southern United States. It was not until after the Civil War in the 1860s that steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to fully spread across the nation.
Thomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.
In 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of "America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than 45 mi from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.
Merchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using 5.6 mi of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.
The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.
Japan.
The industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform program to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (O-yatoi gaikokujin).
In 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the USA to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.
Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.
Second Industrial Revolutions.
Steel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a "Second Industrial Revolution", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.
This second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industries, and was marked by a transition of technological leadership from Britain to the United States and Germany.
The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.
A new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.
By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.
Intellectual paradigms and criticism.
Capitalism.
The advent of the Age of Enlightenment provided an intellectual framework which welcomed the practical application of the growing body of scientific knowledge—a factor evidenced in the systematic development of the steam engine, guided by scientific analysis, and the development of the political and sociological analyses, culminating in Scottish economist Adam Smith's "The Wealth of Nations". One of the main arguments for capitalism, presented for example in the book "The Improving State of the World", is that industrialisation increases wealth for all, as evidenced by raised life expectancy, reduced working hours, and no work for children and the elderly.
Socialism.
Socialism emerged as a critique of capitalism. Marxism began essentially as a reaction to the Industrial Revolution. According to Karl Marx, industrialisation polarised society into the bourgeoisie (those who own the means of production, the factories and the land) and the much larger proletariat (the working class who actually perform the labour necessary to extract something valuable from the means of production). He saw the industrialisation process as the logical dialectical progression of feudal economic modes, necessary for the full development of capitalism, which he saw as in itself a necessary precursor to the development of socialism and eventually communism.
Romanticism.
During the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of "nature" in art and language, in contrast to "monstrous" machines and factories; the "Dark satanic mills" of Blake's poem "And did those feet in ancient time". Mary Shelley's novel "Frankenstein" reflected concerns that scientific progress might be two-edged.
Causes.
The causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century. As national border controls became more effective, the spread of disease was lessened, thereby preventing the epidemics common in previous times. The percentage of children who lived past infancy rose significantly, leading to a larger workforce. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the surplus population who could no longer find employment in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century.
Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.
Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that "the archetypal model for the industrial era was the clock". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.
The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia till 1753, 1789 in France and 1839 in Spain.
Governments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly may have prevented other inventors, such as Richard Trevithick, William Murdoch or Jonathan Hornblower, from introducing improved steam engines, thereby retarding the industrial revolution by about 16 years.
Causes in Europe.
One question of active interest to historians is why the industrial revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East, or at other times like in Classical Antiquity or the Middle Ages. Numerous factors have been suggested, including education, technological changes (see Scientific Revolution in Europe), "modern" government, "modern" work attitudes, ecology, and culture. However, most historians contest the assertion that Europe and China were roughly equal because modern estimates of per capita income on Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars.
Some historians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.
Regarding India, the Marxist historian Rajani Palme Dutt said: "The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain." In contrast to China, India was split up into many competing kingdoms, with the three major ones being the Marathas, Sikhs and the Mughals. In addition, the economy was highly dependent on two sectors—agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.
Causes in Britain.
Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the industrial revolution. Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland; (3) the rule of law (respecting the sanctity of contracts); (4) a straightforward legal system which allowed the formation of joint-stock companies (corporations); and (5) a free market (capitalism).
Geographical and natural resource advantages of Great Britain were the fact that it had extensive coast lines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe.
There were two main values that really drove the industrial revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own industrial revolutions.
The debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.
Instead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position—an island separated from the rest of mainland Europe.
Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.
The stable political situation in Britain from around 1688, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's The Servile State.)
Britain's population grew 280% 1550–1820, while the rest of Western Europe grew 50-80%. 70% of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs 3 to for fodder while even early steam engines produced 4 times more mechanical energy.
In 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.
Transfer of knowledge.
Knowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.
Another means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' ("i.e." science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, "They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual "Transactions".
There were publications describing technology. Encyclopaedias such as Harris's "Lexicon Technicum" (1704) and Abraham Rees's "Cyclopaedia" (1802–1819) contain much of value. "Cyclopaedia" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the "Descriptions des Arts et Métiers" and Diderot's "Encyclopédie" explained foreign methods with fine engraved plates.
Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the "Annales des Mines", published accounts of travels made by French engineers who observed British methods on study tours.
Protestant work ethic.
Another theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work.
The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.
Dissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences—areas of scholarship vital to the development of manufacturing technologies.
Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.

</doc>
<doc id="14918" url="http://en.wikipedia.org/wiki?curid=14918" title="International Court of Justice">
International Court of Justice

The International Court of Justice (French: "Cour internationale de justice"; commonly referred to as the World Court or ICJ) is the primary judicial branch of the United Nations. It is based in the Peace Palace in The Hague, Netherlands. Its main functions are to settle legal disputes submitted to it by states and to provide advisory opinions on legal questions submitted to it by duly authorized international branches, agencies, and the UN General Assembly.
Activities.
Established in 1945 by the UN Charter, the Court began work in 1946 as the successor to the Permanent Court of International Justice. The Statute of the International Court of Justice, similar to that of its predecessor, is the main constitutional document constituting and regulating the Court.
The Court's workload covers a wide range of judicial activity. After the court ruled that the U.S.'s covert war against Nicaragua was in violation of international law (Nicaragua v. United States), the United States withdrew from compulsory jurisdiction in 1986. The United States accepts the court's jurisdiction only on a case-by-case basis. Chapter XIV of the United Nations Charter authorizes the UN Security Council to enforce Court rulings. However, such enforcement is subject to the veto power of the five permanent members of the Council, which the United States used in the "Nicaragua" case.
Composition.
The ICJ is composed of fifteen judges elected to nine-year terms by the UN General Assembly and the UN Security Council from a list of persons nominated by the national groups in the Permanent Court of Arbitration. The election process is set out in Articles 4–19 of the ICJ statute. Elections are staggered with five judges elected every three years, in order to ensure continuity within the court.
Should a judge die in office, the practice has generally been to elect a judge in a special election to complete the term. No two may be nationals of the same country. According to Article 9, the membership of the Court is supposed to represent the "main forms of civilization and of the principal legal systems of the world". Essentially, this has meant common law, civil law and socialist law (now post-communist law).
There is an informal understanding that the seats of the Court will be distributed so that there are: five seats for Western countries, three for African states (including one judge of francophonic civil law, one of anglophonic common law and one Arab), two for Eastern European states, three for Asian states and two for Latin American and Caribbean states. The five permanent members of the United Nations Security Council (France, Russia, China, the United Kingdom, and the United States) always have a judge on the Court, thereby occupying three of the Western seats, one of the Asian seats and one of the Eastern European seats. The exception was China, which did not have a judge on the Court from 1967 to 1985, because it did not put forward a candidate.
Article 6 of the Statute provides that all judges should be "elected regardless of their nationality among persons of high moral character", who are either qualified for the highest judicial office in their home states or known as lawyers with sufficient competence in international law. Judicial independence is dealt with specifically in Articles 16–18. Judges of the ICJ are not able to hold any other post, nor act as counsel. In practice the Members of the Court have their own interpretation of these rules. This allows them to be involved in outside arbitration and hold professional posts as long as there is no conflict of interest. A judge can be dismissed only by a unanimous vote of other members of the Court. Despite these provisions, the independence of ICJ judges has been questioned. For example, during the "Nicaragua Case", the USA issued a communiqué suggesting that it could not present sensitive material to the Court because of the presence of judges from Eastern bloc states.
Judges may deliver joint judgments or give their own separate opinions. Decisions and Advisory Opinions are by majority and, in the event of an equal division, the President's vote becomes decisive. Judges may also deliver separate dissenting opinions.
"Ad hoc" judges.
Article 31 of the statute sets out a procedure whereby "ad hoc" judges sit on contentious cases before the Court. This system allows any party to a contentious case who otherwise does not have one of that party's nationals sitting on the Court to select one additional person to sit as a judge on that case only. It is possible that as many as seventeen judges may sit on one case.
This system may seem strange when compared with domestic court processes, but its purpose is to encourage states to submit cases to the Court. For example, if a state knows it will have a judicial officer who can participate in deliberation and offer other judges local knowledge and an understanding of the state's perspective, that state may be more willing to submit to the Court's jurisdiction. Although this system does not sit well with the judicial nature of the body, it is usually of little practical consequence. "Ad hoc" judges usually (but not always) vote in favor of the state that appointed them and thus cancel each other out.
Chambers.
Generally, the Court sits as full bench, but in the last fifteen years it has on occasion sat as a chamber. Articles 26–29 of the statute allow the Court to form smaller chambers, usually 3 or 5 judges, to hear cases. Two types of chambers are contemplated by Article 26: firstly, chambers for special categories of cases, and second, the formation of "ad hoc" chambers to hear particular disputes. In 1993 a special chamber was established, under Article 26(1) of the ICJ statute, to deal specifically with environmental matters (although this chamber has never been used).
"Ad hoc" chambers are more frequently convened. For example, chambers were used to hear the "Gulf of Maine Case" (Canada/USA). In that case, the parties made clear they would withdraw the case unless the Court appointed judges to the chamber who were acceptable to the parties. Judgments of chambers may have less authority than full Court judgments, or may diminish the proper interpretation of universal international law informed by a variety of cultural and legal perspectives. On the other hand, the use of chambers might encourage greater recourse to the Court and thus enhance international dispute resolution.
Current composition.
As of 9 February 2015, the composition of the Court is as follows:
Jurisdiction.
As stated in Article 93 of the UN Charter, all 193 UN members are automatically parties to the Court's statute. Non-UN members may also become parties to the Court's statute under the Article 93(2) procedure. For example, before becoming a UN member state, Switzerland used this procedure in 1948 to become a party, and Nauru became a party in 1988. Once a state is a party to the Court's statute, it is entitled to participate in cases before the Court. However, being a party to the statute does not automatically give the Court jurisdiction over disputes involving those parties. The issue of jurisdiction is considered in the two types of ICJ cases: contentious issues and advisory opinions.
Contentious issues.
In contentious cases (adversarial proceedings seeking to settle a dispute), the ICJ produces a binding ruling between states that agree to submit to the ruling of the court. Only states may be parties in contentious cases. Individuals, corporations, parts of a federal state, NGOs, UN organs and self-determination groups are excluded from direct participation in cases, although the Court may receive information from public international organizations. This does not preclude non-state interests from being the subject of proceedings if one state brings the case against another. For example, a state may, in case of "diplomatic protection", bring a case on behalf of one of its nationals or corporations.
Jurisdiction is often a crucial question for the Court in contentious cases. (See Procedure below.) The key principle is that the ICJ has jurisdiction only on the basis of consent. Article 36 outlines four bases on which the Court's jurisdiction may be founded.
Advisory opinion.
 An advisory opinion is a function of the Court open only to specified United Nations bodies and agencies. On receiving a request, the Court decides which States and organizations might provide useful information and gives them an opportunity to present written or oral statements. Advisory Opinions were intended as a means by which UN agencies could seek the Court's help in deciding complex legal issues that might fall under their respective mandates.
In principle, the Court's advisory opinions are only consultative in character but are influential and widely respected. Whilst certain instruments or regulations can provide in advance that the advisory opinion shall be specifically binding on particular agencies or states, they are inherently non-binding under the Statute of the Court. This non-binding character does not mean that advisory opinions are without legal effect, because the legal reasoning embodied in them reflects the Court's authoritative views on important issues of international law and, in arriving at them, the Court follows essentially the same rules and procedures that govern its binding judgments delivered in contentious cases submitted to it by sovereign states.
An advisory opinion derives its status and authority from the fact that it is the official pronouncement of the principal judicial organ of the United Nations.
Advisory Opinions have often been controversial because the questions asked are controversial or the case was pursued as an indirect way of bringing what is really a contentious case before the Court. Examples of advisory opinions can be found in the section advisory opinions in the List of International Court of Justice cases article. One such well-known advisory opinion is the "Nuclear Weapons Case".
ICJ and the Security Council.
Article 94 establishes the duty of all UN members to comply with decisions of the Court involving them. If parties do not comply, the issue may be taken before the Security Council for enforcement action. There are obvious problems with such a method of enforcement. If the judgment is against one of the permanent five members of the Security Council or its allies, any resolution on enforcement would then be vetoed. This occurred, for example, after the "Nicaragua" case, when Nicaragua brought the issue of the U.S.'s non-compliance with the Court's decision before the Security Council. Furthermore, if the Security Council refuses to enforce a judgment against any other state, there is no method of forcing the state to comply. Furthermore, the most effective form to take action for the Security Council, coercive action under Chapter VII of the United Nations Charter, can be justified only if international peace and security are at stake. The Security Council has never done this so far.
The relationship between the ICJ and the Security Council, and the separation of their powers, was considered by the Court in 1992 in the "Pan Am" case. The Court had to consider an application from Libya for the order of provisional measures to protect its rights, which, it alleged, were being infringed by the threat of economic sanctions by the United Kingdom and United States. The problem was that these sanctions had been authorized by the Security Council, which resulted in a potential conflict between the Chapter VII functions of the Security Council and the judicial function of the Court. The Court decided, by eleven votes to five, that it could not order the requested provisional measures because the rights claimed by Libya, even if legitimate under the Montreal Convention, prima facie could not be regarded as appropriate since the action was ordered by the Security Council. In accordance with Article 103 of the UN Charter, obligations under the Charter took precedence over other treaty obligations. Nevertheless the Court declared the application admissible in 1998. A decision on the merits has not been given since the parties (United Kingdom, United States and Libya) settled the case out of court in 2003.
There was a marked reluctance on the part of a majority of the Court to become involved in a dispute in such a way as to bring it potentially into conflict with the Council. The Court stated in the "Nicaragua" case that there is no necessary inconsistency between action by the Security Council and adjudication by the ICJ. However, where there is room for conflict, the balance appears to be in favor of the Security Council.
Should either party fail "to perform the obligations incumbent upon it under a judgment rendered by the Court", the Security Council may be called upon to "make recommendations or decide upon measures" if the Security Council deems such actions necessary. In practice, the Court's powers have been limited by the unwillingness of the losing party to abide by the Court's ruling, and by the Security Council's unwillingness to impose consequences. However, in theory, "so far as the parties to the case are concerned, a judgment of the Court is binding, final and without appeal," and "by signing the Charter, a State Member of the United Nations undertakes to comply with any decision of the International Court of Justice in a case to which it is a party."
For example, the United States had previously accepted the Court's compulsory jurisdiction upon its creation in 1946, but in "Nicaragua v. United States" withdrew its acceptance following the Court's judgment in 1984 that called on the US to "cease and to refrain" from the "unlawful use of force" against the government of Nicaragua. The Court ruled (with only the American judge dissenting) that the United States was "in breach of its obligation under the Treaty of Friendship with Nicaragua not to use force against Nicaragua" and ordered the United States to pay war reparations (see note 2).
Examples of contentious cases.
"To see a list of all the contentious cases and advisory opinions see" "List of International Court of Justice Cases"
Law applied.
When deciding cases, the Court applies international law as summarised in of the ICJ Statute, which provides that in arriving at its decisions the Court shall apply international conventions, international custom, and the "general principles of law recognized by civilized nations". It may also refer to academic writing ("the teachings of the most highly qualified publicists of the various nations") and previous judicial decisions to help interpret the law, although the Court is not formally bound by its previous decisions under the doctrine of stare decisis. makes clear that the common law notion of precedent or "stare decisis" does not apply to the decisions of the ICJ. The Court's decision binds only the parties to that particular controversy. Under 38(1)(d), however, the Court may consider its own previous decisions.
If the parties agree, they may also grant the Court the liberty to decide "ex aequo et bono" ("in justice and fairness"), granting the ICJ the freedom to make an equitable decision based on what is fair under the circumstances. This provision has not been used in the Court's history. So far the International Court of Justice has dealt with about 130 cases.
Procedure.
The ICJ is vested with the power to make its own rules. Court procedure is set out in "Rules of Court of the International Court of Justice 1978" (as amended on 29 September 2005).
Cases before the ICJ will follow a standard pattern. The case is lodged by the applicant who files a written memorial setting out the basis of the Court's jurisdiction and the merits of its claim. The respondent may accept the Court's jurisdiction and file its own memorial on the merits of the case.
Preliminary objections.
A respondent who does not wish to submit to the jurisdiction of the Court may raise Preliminary Objections. Any such objections must be ruled upon before the Court can address the merits of the applicant's claim. Often a separate public hearing is held on the Preliminary Objections and the Court will render a judgment. Respondents normally file Preliminary Objections to the jurisdiction of the Court and/or the admissibility of the case. Inadmissibility refers to a range of arguments about factors the Court should take into account in deciding jurisdiction; for example, that the issue is not justiciable or that it is not a "legal dispute".
In addition, objections may be made because all necessary parties are not before the Court. If the case necessarily requires the Court to rule on the rights and obligations of a state that has not consented to the Court's jurisdiction, the Court will not proceed to issue a judgment on the merits.
If the Court decides it has jurisdiction and the case is admissible, the respondent will then be required to file a Memorial addressing the merits of the applicant's claim. Once all written arguments are filed, the Court will hold a public hearing on the merits.
Once a case has been filed, any party (but usually the Applicant) may seek an order from the Court to protect the "status quo" pending the hearing of the case. Such orders are known as Provisional (or Interim) Measures and are analogous to interlocutory injunctions in United States law. Article 41 of the statute allows the Court to make such orders. The Court must be satisfied to have prima facie jurisdiction to hear the merits of the case before granting provisional measures.
Applications to intervene.
In cases where a third state's interests are affected, that state may be permitted to intervene in the case, and participate as a full party. Under Article 62, a state "with an interest of a legal nature" may apply; however, it is within the Court's discretion whether or not to allow the intervention. Intervention applications are rare — the first successful application occurred in 1991.
Judgment and remedies.
Once deliberation has taken place, the Court will issue a majority opinion. Individual judges may issue separate opinions (if they agree with the outcome reached in the judgment of the court but differ in their reasoning) or dissenting opinions (if they disagree with the majority). No appeal is possible, though any party may ask for the court to clarify if there is a dispute as to the meaning or scope of the court's judgment.
Remedies**
Criticisms.
The International Court has been criticized with respect to its rulings, its procedures, and its authority. As with United Nations criticisms as a whole, many of these criticisms refer more to the general authority assigned to the body by member states through its charter than to specific problems with the composition of judges or their rulings. Major criticisms include:

</doc>
<doc id="14919" url="http://en.wikipedia.org/wiki?curid=14919" title="International Standard Book Number">
International Standard Book Number

The International Standard Book Number (ISBN) is a unique numeric commercial book identifier. 
An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation based, and varies from state to state often depending on how large the publishing industry is within a state.
The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).
Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.
Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.
History.
The Standard Book Numbering (SBN) code is a numeric commercial book identifier based upon 9-digits created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin, for the booksellers and stationers WHSmith and others in 1965. The ISBN configuration of recognition was generated in 1967 by David Whitaker (regarded as the "Father of the ISBN") and Emery Koltay (who later became director of the ).
The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the 9-digit SBN code until 1974. An SBN may be converted to an ISBN by prefixing the digit "0". ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.
Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.
Overview.
An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):
A 13-digit ISBN can be separated into its parts ("prefix element", "registration group", "registrant", "publication" and "check digit"), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts ("registration group", "registrant", "publication" and "check digit") of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.
How ISBNs are issued.
ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture. In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.
Australia: ISBNs are issued by the commercial library services agency , and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.
Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.
India: The Raja Rammohun Roy Library Foundation (RRRLF), part of the Ministry of Culture, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.
Italy: The privately held company "EDISER srl", owned by "Associazione Italiana Editori" (Italian Publishers Association) is responsible for issuing ISBNs. The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.
Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.
South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.
United Kingdom and Republic of Ireland: The privately held company "Nielsen Book Services Ltd", part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.
United States: In the United States, the privately held company R.R. Bowker issues ISBNs. There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.
Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.
Registration group identifier.
The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979). Registration group identifiers have primarily been allocated within the 978 prefix element. The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976. Books published in rare languages typically have longer group identifiers.
Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.
The original 9-digit standard book number (SBN) had no registration group identifier, but affixing a zero (0) as a prefix to a 9-digit SBN creates a valid 10-digit ISBN.
Registrant element.
The national ISBN agency assigns the registrant element (cf. ) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most book stores only handle ISBN bearing merchandise.
A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.
Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.
By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.
Pattern for English language ISBNs.
English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:
Check digits.
A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the message.
ISBN-10 check digits.
The 2001 edition of the official manual of the says that the ISBN-10 check digit – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.
For example, for an ISBN-10 of 0-306-40615-2:
Formally, using modular arithmetic, we can say:
It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in "ascending" order from 1 to 10, is a multiple of 11. For this example:
Formally, we can say:
The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.)
The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.
In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).
ISBN-10 check digit calculation.
Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN – excluding the check digit, itself – is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal 11; therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 - 0 = 11 which is invalid.
For example, the check digit for an ISBN-10 of 0-306-40615-"?" is calculated as follows:
Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2.
The value formula_6 required to satisfy this condition might be 10; if so, an 'X' should be used.
ISBN-13 check digit calculation.
The 2005 edition of the International ISBN Agency's official manual describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.
Formally, using modular arithmetic, we can say:
The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.
For example, the ISBN-13 check digit of 978-0-306-40615-"?" is calculated as follows:
 s = 9×1 + 7×3 + 8×1 + 0×3 + 3×1 + 0×3 + 6×1 + 4×3 + 0×1 + 6×3 + 1×1 + 5×3
 = 9 + 21 + 8 + 0 + 3 + 0 + 6 + 12 + 0 + 18 + 1 + 15
 = 93
 93 / 10 = 9 remainder 3
 10 – 3 = 7
Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.
Formally, the ISBN-13 check digit calculation is:
This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.
Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).
ISBN-10 to ISBN-13 conversion.
The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.
Errors in usage.
Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0590764845 is shared by two books - "Ninja gaiden® : a novel based on the best-selling game by Tecmo" (1990) and "Wacky Laws" (1997), both published by Scholastic.
Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN". However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.
eISBN.
Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.
EAN format used in barcodes, and upgrading.
Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price. For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).
Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix will be introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.
Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing ISBN 10s. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.

</doc>
<doc id="14921" url="http://en.wikipedia.org/wiki?curid=14921" title="IP address">
IP address

An Internet Protocol address (IP address) is a numerical label assigned to each device (e.g., computer, printer) participating in a computer network that uses the Internet Protocol for communication. An IP address serves two principal functions: host or network interface identification and location addressing. Its role has been characterized as follows: "A name indicates what we seek. An address indicates where it is. A route indicates how to get there."
The designers of the Internet Protocol defined an IP address as a 32-bit number and this system, known as Internet Protocol Version 4 (IPv4), is still in use today. However, because of the growth of the Internet and the predicted depletion of available addresses, a new version of IP (IPv6), using 128 bits for the address, was developed in 1995. IPv6 was standardized as RFC 2460 in 1998, and its deployment has been ongoing since the mid-2000s.
IP addresses are usually written and displayed in human-readable notations, such as 172.16.254.1 (IPv4), and 2001:db8:0:1234:0:567:8:1 (IPv6).
The Internet Assigned Numbers Authority (IANA) manages the IP address space allocations globally and delegates five regional Internet registries (RIRs) to allocate IP address blocks to local Internet registries (Internet service providers) and other entities.
IP versions.
Two versions of the Internet Protocol (IP) are in use: IP Version 4 and IP Version 6. Each version defines an IP address differently. Because of its prevalence, the generic term "IP address" typically still refers to the addresses defined by IPv4. The gap in version sequence between IPv4 and IPv6 resulted from the assignment of number 5 to the experimental Internet Stream Protocol in 1979, which however was never referred to as IPv5.
IPv4 addresses.
In IPv4 an address consists of 32 bits which limits the address space to (232) possible unique addresses. IPv4 reserves some addresses for special purposes such as private networks (~18 million addresses) or multicast addresses (~270 million addresses).
IPv4 addresses are canonically represented in dot-decimal notation, which consists of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., 172.16.254.1. Each part represents a group of 8 bits (octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.
Subnetting.
In the early stages of development of the Internet Protocol, network administrators interpreted an IP address in two parts: network number portion and host number portion. The highest order octet (most significant eight bits) in an address was designated as the "network number" and the remaining bits were called the "rest field" or "host identifier" and were used for host numbering within a network.
This early method soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the Internet addressing specification was revised with the introduction of classful network architecture.
Classful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the "class" of the address. Three classes ("A", "B", and "C") were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes ("B" and "C").
The following table gives an overview of this now obsolete system.
Classful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of the network in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes.
Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.
Private addresses.
Early network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be uniquely assigned to a particular computer or device. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.
Computers not connected to the Internet, such as factory machines that communicate only with each other via TCP/IP, need not have globally unique IP addresses. Three ranges of IPv4 addresses for private networks were reserved in RFC 1918. These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry.
Today, when needed, such private networks typically connect to the Internet through network address translation (NAT).
Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets; for example, many home routers automatically use a default address range of 192.168.0.0 through 192.168.0.255 (192.168.0.0/24).
IPv4 address exhaustion.
IPv4 address exhaustion has decreased the supply of unallocated Internet Protocol Version 4 (IPv4) addresses available for assignment to Internet service providers and end user organizations since the 1980s. IANA's primary address pool was exhausted on 3 February 2011, when the last five blocks were allocated to the five RIRs. APNIC was the first RIR to exhaust its regional pool on 15 April 2011, except for a small amount of address space reserved for the transition to IPv6, intended to be allocated in a restricted process.
IPv6 addresses.
The rapid exhaustion of IPv4 address space, despite conservation techniques, prompted the Internet Engineering Task Force (IETF) to explore new technologies to expand the addressing capability in the Internet. The permanent solution was deemed to be a redesign of the Internet Protocol itself. This new generation of the Internet Protocol, intended to replace IPv4 on the Internet, was eventually named "Internet Protocol Version 6" (IPv6) in 1995. The address size was increased from 32 to 128 bits or 16 octets. This, even with a generous assignment of network blocks, is deemed sufficient for the foreseeable future. Mathematically, the new address space provides the potential for a maximum of 2128, or about addresses.
The primary intent of the new design was not to provide just a sufficient quantity of addresses, but also to allow an efficient aggregation of subnetwork routing prefixes at routing nodes. As a result, routing table sizes are smaller, and the smallest possible individual allocation is a subnet for 264 hosts, which is the square of the size of the entire IPv4 Internet. At these levels, actual address utilization rates will be small on any IPv6 network segment. The new design also provides the opportunity to separate the addressing infrastructure of a network segment, that is the local administration of the segment's available space, from the addressing prefix used to route external traffic for a network. IPv6 has facilities that automatically change the routing prefix of entire networks, should the global connectivity or the routing policy change, without requiring internal redesign or manual renumbering.
The large number of IPv6 addresses allows large blocks to be assigned for specific purposes and, where appropriate, to be aggregated for efficient routing. With a large address space, there is no need to have complex address conservation methods as used in CIDR.
Many modern desktop and enterprise server operating systems include native support for the IPv6 protocol, but it is not yet widely deployed in other devices, such as home networking routers, voice over IP (VoIP) and multimedia equipment, and network peripherals.
Private addresses.
Just as IPv4 reserves addresses for private or internal networks, blocks of addresses are set aside in IPv6 for private addresses. In IPv6, these are referred to as unique local addresses (ULA). RFC 4193 sets aside the routing prefix fc00::/7 for this block which is divided into two /8 blocks with different implied policies. The addresses include a 40-bit pseudorandom number that minimizes the risk of address collisions if sites merge or packets are misrouted.
Early designs used a different block for this purpose (fec0::), dubbed site-local addresses. However, the definition of what constituted "sites" remained unclear and the poorly defined addressing policy created ambiguities for routing. This address range specification was abandoned and must not be used in new systems.
Addresses starting with fe80:, called link-local addresses, are assigned to interfaces for communication on the link only. The addresses are automatically generated by the operating system for each network interface. This provides instant and automatic network connectivity for any IPv6 host and means that if several hosts connect to a common hub or switch, they have a communication path via their link-local IPv6 address. This feature is used in the lower layers of IPv6 network administration (e.g. Neighbor Discovery Protocol).
None of the private address prefixes may be routed on the public Internet.
IP subnetworks.
IP networks may be divided into subnetworks in both IPv4 and IPv6. For this purpose, an IP address is logically recognized as consisting of two parts: the "network prefix" and the "host identifier", or "interface identifier" (IPv6). The subnet mask or the CIDR prefix determines how the IP address is divided into network and host parts.
The term "subnet mask" is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the "routing prefix". For example, an IPv4 address and its subnet mask may be 192.0.2.1 and 255.255.255.0, respectively. The CIDR notation for the same IP address and subnet is 192.0.2.1/24, because the first 24 bits of the IP address indicate the network and subnet.
IP address assignment.
Internet Protocol addresses are assigned to a host either anew at the time of booting, or permanently by fixed configuration of its hardware or software. Persistent configuration is also known as using a "static IP address". In contrast, in situations when the computer's IP address is assigned newly each time, this is known as using a "dynamic IP address".
Methods.
Static IP addresses are manually assigned to a computer by an administrator. The exact procedure varies according to platform. This contrasts with dynamic IP addresses, which are assigned either by the computer interface or host software itself, as in Zeroconf, or assigned by a server using Dynamic Host Configuration Protocol (DHCP). Even though IP addresses assigned using DHCP may stay the same for long periods of time, they can generally change. In some cases, a network administrator may implement dynamically assigned static IP addresses. In this case, a DHCP server is used, but it is specifically configured to always assign the same IP address to a particular computer. This allows static IP addresses to be configured centrally, without having to specifically configure each computer on the network in a manual procedure.
In the absence or failure of static or stateful (DHCP) address configurations, an operating system may assign an IP address to a network interface using state-less auto-configuration methods, such as Zeroconf.
Uses of dynamic address assignment.
IP addresses are most frequently assigned dynamically on LANs and broadband networks by the Dynamic Host Configuration Protocol (DHCP). They are used because it avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows many devices to share limited address space on a network if only some of them will be online at a particular time. In most current desktop operating systems, dynamic IP configuration is enabled by default so that a user does not need to manually enter any settings to connect to a network with a DHCP server. DHCP is not the only technology used to assign IP addresses dynamically. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol.
Sticky dynamic IP address.
A "sticky dynamic IP address" is an informal term used by cable and DSL Internet access subscribers to describe a dynamically assigned IP address which seldom changes. The addresses are usually assigned with DHCP. Since the modems are usually powered on for extended periods of time, the address leases are usually set to long periods and simply renewed. If a modem is turned off and powered up again before the next expiration of the address lease, it will most likely receive the same IP address.
Address autoconfiguration.
RFC 3330 defines an address block, 169.254.0.0/16, for the special use in link-local addressing for IPv4 networks. In IPv6, every interface, whether using static or dynamic address assignments, also receives a local-link address automatically in the block fe80::/10.
These addresses are only valid on the link, such as a local network segment or point-to-point connection, that a host is connected to. These addresses are not routable and like private addresses cannot be the source or destination of packets traversing the Internet.
When the link-local IPv4 address block was reserved, no standards existed for mechanisms of address autoconfiguration. Filling the void, Microsoft created an implementation that is called Automatic Private IP Addressing (APIPA). APIPA has been deployed on millions of machines and has, thus, become a de facto standard in the industry. In RFC 3927, the IETF defined a formal standard for this functionality, entitled "Dynamic Configuration of IPv4 Link-Local Addresses".
Uses of static addressing.
Some infrastructure situations have to use static addressing, such as when finding the Domain Name System (DNS) host that will translate domain names to IP addresses. Static addresses are also convenient, but not absolutely necessary, to locate servers inside an enterprise. An address obtained from a DNS server comes with a time to live, or caching time, after which it should be looked up to confirm that it has not changed. Even static IP addresses do change as a result of network administration (RFC 2072).
Routing.
IP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.
Unicast addressing.
The most common concept of an IP address is in unicast addressing, available in both IPv4 and IPv6. It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but it is not a one-to-one correspondence. Some individual PCs have several distinct unicast addresses, each for its own distinct purpose. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.
Broadcast addressing.
In IPv4 it is possible to send data to all possible destinations ("all-hosts broadcast"), which permits the sender to send the data only once, and all receivers receive a copy of it. In the IPv4 protocol, the address 255.255.255.255 is used for local broadcast. In addition, a directed (limited) broadcast can be made by combining the network prefix with a host suffix composed entirely of binary 1s. For example, the destination address used for a directed broadcast to devices on the 192.0.2.0/24 network is 192.0.2.255. IPv6 does not implement broadcast addressing and replaces it with multicast to the specially-defined all-nodes multicast address.
Multicast addressing.
A multicast address is associated with a group of interested receivers. In IPv4, addresses 224.0.0.0 through 239.255.255.255 (the former Class D addresses) are designated as multicast addresses. IPv6 uses the address block with the prefix ff00::/8 for multicast applications. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all receivers that have joined the corresponding multicast group.
Anycast addressing.
Like broadcast and multicast, anycast is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is logically closest in the network. Anycast address is an inherent feature of only IPv6. In IPv4, anycast addressing implementations typically operate using the shortest-path metric of BGP routing and do not take into account congestion or other attributes of the path. Anycast methods are useful for global load balancing and are commonly used in distributed DNS systems.
Public addresses.
A "public IP address", in common parlance, is synonymous with a "globally routable unicast IP address".
Both IPv4 and IPv6 define address ranges that are reserved for private networks and link-local addressing. The term public IP address often used excludes these types of addresses.
Modifications to IP addressing.
IP blocking and firewalls.
Firewalls perform Internet Protocol blocking to protect networks from unauthorized access. They are common on today[ [update]]'s Internet. They control access to networks based on the IP address of a client computer. Whether using a blacklist or a whitelist, the IP address that is blocked is the perceived IP address of the client, meaning that if the client is using a proxy server or network address translation, blocking one IP address may block many individual computers.
IP address translation.
Multiple client devices can appear to share IP addresses: either because they are part of a shared hosting web server environment or because an IPv4 network address translator (NAT) or proxy server acts as an intermediary agent on behalf of its customers, in which case the real originating IP addresses might be hidden from the server receiving a request. A common practice is to have a NAT hide a large number of IP addresses in a private network. Only the "outside" interface(s) of the NAT need to have Internet-routable addresses.
Most commonly, the NAT device maps TCP or UDP port numbers on the side of the larger, public network to individual private addresses on the masqueraded network.
In small home networks, NAT functions are usually implemented in a residential gateway device, typically one marketed as a "router". In this scenario, the computers connected to the router would have private IP addresses and the router would have a public address to communicate on the Internet. This type of router allows several computers to share one public IP address.
Diagnostic tools.
Computer operating systems provide various diagnostic tools to examine their network interface and address configuration. Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems can use ifconfig, netstat, route, lanstat, fstat, or iproute2 utilities to accomplish the task.

</doc>
<doc id="14922" url="http://en.wikipedia.org/wiki?curid=14922" title="If and only if">
If and only if

↔⇔≡Logical symbols representing "iff"
 
In logic and related fields such as mathematics and philosophy, if and only if (shortened iff) is a biconditional logical connective between statements.
In that it is biconditional, the connective can be likened to the standard material conditional ("only if", equal to "if ... then") combined with its reverse ("if"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other, i.e., either both statements are true, or both are false. It is controversial whether the connective thus defined is properly rendered by the English "if and only if", with its pre-existing meaning. There is nothing to stop one from "stipulating" that we may read this connective as "only if and if", although this may lead to confusion.
In writing, phrases commonly used, with debatable propriety, as alternatives to P "if and only if" Q include "Q is necessary and sufficient for P", "P is equivalent (or materially equivalent) to Q" (compare material implication), "P precisely if Q", "P precisely (or exactly) when Q", "P exactly in case Q", and "P just in case Q". Many authors regard "iff" as unsuitable in formal writing; others use it freely.
In "logic formulae", logical symbols are used instead of these phrases; see the discussion of notation.
Definition.
The truth table of "p ↔ q" is as follows:
Note that it is equivalent to that produced by the XNOR gate, and opposite to that produced by the XOR gate.
Usage.
Notation.
The corresponding logical symbols are "↔", "⇔" and "≡", and sometimes "iff". These are usually treated as equivalent. However, some texts of mathematical logic (particularly those on first-order logic, rather than propositional logic) make a distinction between these, in which the first, ↔, is used as a symbol in logic formulas, while ⇔ is used in reasoning about those logic formulas (e.g., in metalogic). In Łukasiewicz's notation, it is the prefix symbol 'E'.
Another term for this logical connective is exclusive nor.
Proofs.
In most logical systems, one proves a statement of the form "P iff Q" by proving "if P, then Q" and "if Q, then P". Proving this pair of statements sometimes leads to a more natural proof, since there are not obvious conditions in which one would infer a biconditional directly. An alternative is to prove the disjunction "(P and Q) or (not-P and not-Q)", which itself can be inferred directly from either of its disjuncts—that is, because "iff" is truth-functional, "P iff Q" follows if P and Q have both been shown true, or both false.
Origin of iff.
Usage of the abbreviation "iff" first appeared in print in John L. Kelley's 1955 book "General Topology."
Its invention is often credited to Paul Halmos, who wrote "I invented 'iff,' for 'if and only if'—but I could never believe I was really its first inventor."
Distinction from "if" and "only if".
Sufficiency is the inverse of necessity. That is to say, given "P"→"Q" (i.e. if "P" then "Q"), "P" would be a sufficient condition for "Q", and "Q" would be a necessary condition for "P". Also, given "P"→"Q", it is true that "¬Q"→"¬P" (where ¬ is the negation operator, i.e. "not"). This means that the relationship between "P" and "Q", established by "P"→"Q", can be expressed in the following, all equivalent, ways:
As an example, take (1), above, which states "P"→"Q", where "P" is "the fruit in question is an apple" and "Q" is "Madison will eat the fruit in question". The following are four equivalent ways of expressing this very relationship:
So we see that (2), above, can be restated in the form of "if...then" as "If Madison will eat the fruit in question, then it is an apple"; taking this in conjunction with (1), we find that (3) can be stated as "If the fruit in question is an apple, then Madison will eat it; AND if Madison will eat the fruit, then it is an apple".
Advanced considerations.
Philosophical interpretation.
A sentence that is composed of two other sentences joined by "iff" is called a "biconditional". "Iff" joins two sentences to form a new sentence. It should not be confused with logical equivalence which is a description of a relation between two sentences. The biconditional "A iff B" "uses" the sentences "A" and "B", describing a relation between the states of affairs which "A" and "B" describe. By contrast ""A" is logically equivalent to "B"" "mentions" both sentences: it describes a logical relation between those two sentences, and not a factual relation between whatever matters they describe. See use–mention distinction for more on the difference between "using" a sentence and "mentioning" it.
The distinction is a very confusing one, and has led many a philosopher astray. Certainly it is the case that when "A" is logically equivalent to "B", "A "iff" B" is true. But the converse does not hold. Reconsidering the sentence:
There is clearly no logical equivalence between the two halves of this particular biconditional. For more on the distinction, see W. V. Quine's "Mathematical Logic", Section 5.
One way of looking at "A if and only if B" is that it means "A if B" (B implies A) and "A only when B" (not B implies not A). "Not B implies not A" means A implies B, so then there is two way implication.
Definitions.
In philosophy and logic, "iff" is used to indicate definitions, since definitions are supposed to be universally quantified biconditionals. In mathematics and elsewhere, however, the word "if" is normally used in definitions, rather than "iff". This is due to the observation that "if" in the English language has a definitional meaning, separate from its meaning as a propositional connective. This separate meaning can be explained by noting that a definition (for instance: A group is "abelian" if it satisfies the commutative law; or: A grape is a "raisin" if it is well dried) is not an equivalence to be proved, but a rule for interpreting the term defined.
Examples.
Here are some examples of true statements that use "iff" - true biconditionals (the first is an example of a definition, so it would normally have been written with "if"):
Analogs.
Other words are also sometimes emphasized in the same way by repeating the last letter; for example "orr" for "Or and only Or" (the exclusive disjunction).
The statement "(A iff B)" is equivalent to the statement "(not A or B) and (not B or A)," and is also equivalent to the statement "(not A and not B) or (A and B)".
It is also equivalent to: not[(A or B) and (not A or not B)],
or more simply:
which converts into
and
which were given in verbal interpretations above.
More general usage.
Iff is used outside the field of logic, wherever logic is applied, especially in mathematical discussions. It has the same meaning as above: it is an abbreviation for "if and only if", indicating that one statement is both necessary and sufficient for the other. This is an example of mathematical jargon. (However, as noted above, "if", rather than "iff", is more often used in statements of definition.)
The elements of "X" are "all and only" the elements of "Y" is used to mean: "for any "z" in the domain of discourse, "z" is in "X" if and only if "z" is in "Y"."

</doc>
<doc id="14923" url="http://en.wikipedia.org/wiki?curid=14923" title="IP">
IP

IP may refer to:

</doc>
<doc id="14926" url="http://en.wikipedia.org/wiki?curid=14926" title="List of Italian dishes">
List of Italian dishes

This is a list of Italian dishes and foods. Italian cuisine has developed through centuries of social and political changes, with roots as far back as the 4th century BC. Italian cuisine has its origins in Etruscan, ancient Greek, and ancient Roman cuisines.
Significant changes occurred with the discovery of the New World and the introduction of potatoes, tomatoes, bell peppers and maize, now central to the cuisine but not introduced in quantity until the 18th century. Italian cuisine is noted for its regional diversity, abundance of difference in taste, and is known to be one of the most popular in the world, with influences abroad. 
Pizza and spaghetti, both associated with the Neapolitan traditions of cookery, are especially popular abroad, but the varying geographical conditions of the twenty regions of Italy, together with the strength of local traditions, afford a wide range of dishes.
Dishes and recipes.
Rice dishes.
Rice ("Riso") dishes are very common in Northern Italy, especially in the Lombardia and Veneto regions, though rice dishes are found in all the country.
Special occasions.
Feast of the Seven Fishes
Unique dishes by region.
Toscana.
Tuscan bread specialties
Umbria.
Specialties of the Norcineria (Umbrian Butcher)
Marche.
Unique ham and sausage specialties
Apulia ("Puglia").
Apulian bread specialties
Ingredients.
Most important ingredients (see also Italian Herbs and Spices):
Other common ingredients:

</doc>
<doc id="14928" url="http://en.wikipedia.org/wiki?curid=14928" title="Isaac Ambrose">
Isaac Ambrose

Isaac Ambrose (1604 – 20 January 1664) was an English Puritan divine. He graduated with a BA. from Brasenose College, Oxford, on 1624. He obtained the cure of St Edmund’s Church, Castleton, Derbyshire in 1627. He was one of king's four preachers in Lancashire in 1631. He was twice imprisoned by commissioners of array. He worked for establishment of Presbyterianism; successively at Leeds, Preston, and Garstang, from whence he was ejected for nonconformity in 1662. He also published religious works.
Biography.
Ambrose was born in 1604. He was the son of Richard Ambrose, vicar of Ormskirk, and was probably descended from the Ambroses of Lowick in Furness, a well-known Roman Catholic family. He entered Brasenose College, Oxford, in 1621, in his seventeenth year.
Having graduated B.A. in 1624 and been ordained, Ambroses received in 1627 the little cure of Castleton in Derbyshire. By the influence of William Russell, earl of Bedford, he was appointed one of the king's itinerant preachers in Lancashire, and after living for a time in Garstang, he was selected by the Lady Margaret Hoghton as vicar of Preston. He associated himself with Presbyterianism, and was on the celebrated committee for the ejection of "scandalous and ignorant ministers and schoolmasters" during the Commonwealth.
So long as Ambrose continued at Preston he was favoured with the warm friendship of the Hoghton family, their ancestral woods and the tower near Blackburn affording him sequestered places for those devout meditations and "experiences" that give such a charm to his diary, portions of which are quoted in his "Prima Media" and "Ultima" (1650, 1659). The immense auditory of his sermon ("Redeeming the Time") at the funeral of Lady Hoghton was long a living tradition all over the county. On account of the feeling engendered by the civil war Ambrose left his great church of Preston in 1654, and became minister of Garstang, whence, however, in 1662 he was ejected along two thousand ministers who refused to conform (see Great Ejection). His after years were passed among old friends and in quiet meditation at Preston. He died of apoplexy about 20 January 1664.
Character assessment.
As a religious writer Ambrose has a vividness and freshness of imagination possessed by scarcely any of the Puritan Nonconformists. Many who have no love for Puritan doctrine, nor sympathy with Puritan experience, have appreciated the pathos and beauty of his writings, and his "Looking unto Jesus" long held its own in popular appreciation with the writings of John Bunyan.
Dr Edmund Calamy the Elder (1600–1666) wrote about him:
Ambrose was a man of that substantial worth, that eminent piety, and that exemplary life, both as a minister and a christian, that it is to be lamented the world should not have the benefit of particular memoirs of him.
In the opinion of John Eglington Bailey (his biographer in the DNB), his character has been misrepresented by Wood. He was of a peaceful disposition; and though he put his name to the fierce "Harmonious Consent", he was not naturally a partisan. He evaded the political controversies of the time. His gentleness of character and earnest presentation of the gospel attached him to his people. He was much given to secluding himself, retiring every May into the woods of Hoghton Tower and remaining there a month.
Bailey continues that Dr. Halley justly characterises him as the most meditative puritan of Lancashire. This quality pervades his writings, which abound, besides, in deep feeling and earnest piety. Mr. Hunter has called attention to his recommendation of diaries as a means of advancing personal piety, and has remarked, in reference to the fragments from Ambrose's diary quoted in the "Media", that "with such passages before us we cannot but lament that the carelessness of later times should have suffered such a curious and valuable document to perish; for perished it is to be feared it has".

</doc>
<doc id="14933" url="http://en.wikipedia.org/wiki?curid=14933" title="International Convention for the Regulation of Whaling">
International Convention for the Regulation of Whaling

The International Convention for the Regulation of Whaling is an international environmental agreement signed in 1946 in order to "provide for the proper conservation of whale stocks and thus make possible the orderly development of the whaling industry". It governs the commercial, scientific, and aboriginal subsistence whaling practices of fifty-nine member nations.
It was signed by 15 nations in Washington, D.C. on 2 December 1946 and took effect on 10 November 1948. Its protocol (which represented the first substantial revision of the convention and extended the definition of a "whale-catcher" to include helicopters as well as ships) was signed in Washington on 19 November 1956. The convention is a successor to the International Agreement for the Regulation of Whaling, signed in London on 8 June 1937, and the protocols for that agreement signed in London on 24 June 1938, and 26 November 1945.
The objectives of the agreement are the protection of all whale species from overhunting, the establishment of a system of international regulation for the whale fisheries to ensure proper conservation and development of whale stocks, and safeguarding for future generations the great natural resources represented by whale stocks. The primary instrument for the realization of these aims is the International Whaling Commission which was established pursuant to this convention. The commission has made many revisions to the schedule that makes up the bulk of the convention. The Commission process has also reserved for governments the right to carry out scientific research which involves killing of whales.
There have been consistent disagreement over the scope of the convention. According to the IWC:
The 1946 Convention does not define a ‘whale’, although a list of names in a number of languages was annexed to the Final Act of the Convention. Some Governments take the view that the IWC has the legal competence to regulate catches of only these named great whales (the baleen whales and the sperm whale). Others believe that all cetaceans, including the smaller dolphins and porpoises, also fall within IWC jurisdiction.
Signatories.
As of January 2014, membership consists of 89 Governments from countries around the World. The initial signatory states were Argentina, Australia, Brazil, Canada, Chile, Denmark, France, the Netherlands, New Zealand, Norway, Peru, South Africa, the Soviet Union, the United Kingdom and the United States.
Withdrawals.
As of January 2014, eight states that were formerly parties to the convention have withdrawn by denouncing it. These states are Canada (which withdrew on 30 June 1982), Egypt, Greece, Jamaica, Mauritius, Philippines, the Seychelles and Venezuela. Belize, Brazil, Dominica, Ecuador, Iceland, Japan, New Zealand, and Panama have all withdrawn from the convention for a period of time after ratification but subsequently have ratified it a second time. The Netherlands, Norway, and Sweden have all withdrawn from the convention twice, only to have accepted it a third time.

</doc>
<doc id="14934" url="http://en.wikipedia.org/wiki?curid=14934" title="International Organization for Standardization">
International Organization for Standardization

The International Organization for Standardization (ISO) is an international standard-setting body composed of representatives from various national standards organizations.
Founded on 23 February 1947, the organization promotes worldwide proprietary, industrial and commercial standards. It is headquartered in Geneva, Switzerland, and as of 2013 works in 164 countries.
It was one of the first organizations granted general consultative status with the United Nations Economic and Social Council.
Overview.
ISO, the International Organization for Standardization, is an independent, non-governmental organization, the members of which are the standards organisation of the 163 member countries. It is the world's largest developer of voluntary international standards and facilitates world trade by providing common standards between nations. Nearly twenty thousand standards have been set covering everything from manufactured products and technology to food safety, agriculture and healthcare.
Use of the standards ensures that products and services are safe, reliable and of good quality. The standards help businesses increase productivity while minimizing errors and waste. By enabling products from different markets to be directly compared, they facilitate companies in entering new markets and assist in the development of global trade on a fair basis. The standards also serve to safeguard consumers and the end-users of products and services, ensuring that certified products conform to the minimum standards set internationally.
Name and abbreviations.
The three official languages of the ISO are English, French, and Russian. The name of the organization in French is "Organisation internationale de normalisation", and in Russian, "Международная организация по стандартизации". According to the ISO, as its name in different languages would have different abbreviations ("IOS" in English, "OIN" in French, etc.), the organization adopted "ISO" as its abbreviated name in reference to the Greek word "isos" (ἴσος, meaning "equal"). However, during the meetings founding the new organization and choosing its name, this Greek word was not evoked, so this explanation may have been imagined later.
Both the name "ISO" and the logo are registered trademarks, and their use is restricted.
History.
The organization today known as ISO began in 1926 as the International Federation of the National Standardizing Associations (ISA). It was suspended in 1942 during World War II, but after the war ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization; the new organization officially began operations in February 1947.
Structure.
ISO is a voluntary organization whose members are recognized authorities on standards, each one representing one country. Members meet annually at a General Assembly to discuss ISO's strategic objectives. The organization is coordinated by a Central Secretariat based in Geneva.
A Council with a rotating membership of 20 member bodies provides guidance and governance, including setting the Central Secretariat's annual budget.
The Technical Management Board is responsible for over 250 technical committees, who develop the ISO standards.
IEC joint committees.
ISO has formed joint committees with the International Electrotechnical Commission (IEC) to develop standards and terminology in the areas of electrical, electronic and related technologies.
ISO/IEC JTC 1.
ISO/IEC Joint Technical Committee 1 (JTC 1) was created in 1987 to "[d]evelop, maintain, promote and facilitate IT standards".
ISO/IEC JTC 2.
ISO/IEC Joint Technical Committee 2 (JTC 2) was created in 2009 for the purpose of "[s]tandardization in the field of energy efficiency and renewable energy sources".
Membership.
ISO has 163 national members, out of the 206 total countries in the world.
ISO has three membership categories:
Participating members are called "P" members, as opposed to observing members, who are called "O" members.
Financing.
ISO is funded by a combination of: 
International Standards and other publications.
ISO's main products are international standards. ISO also publishes technical reports, technical specifications, publicly available specifications, technical corrigenda, and guides.
These are meta-standards covering "matters related to international standardization". They are named using the format "ISO[/IEC] Guide N:yyyy: Title".For example:
Document copyright.
ISO documents are copyrighted and ISO charges for most copies. It does not, however, charge for most draft copies of documents in electronic format. Although they are useful, care must be taken using these drafts as there is the possibility of substantial change before they become finalized as standards. Some standards by ISO and its official U.S. representative (and, via the U.S. National Committee, the International Electrotechnical Commission) are made freely available.
Standardization process.
A standard published by ISO/IEC is the last stage of a long process that commonly starts with the proposal of new work within a committee. Here are some abbreviations used for marking a standard with its status:
Abbreviations used for amendments:
Other abbreviations:
International Standards are developed by ISO technical committees (TC) and subcommittees (SC) by a process with six steps:
The TC/SC may set up working groups (WG) of experts for the preparation of a working drafts. Subcommittees may have several working groups, which can have several Sub Groups (SG).
It is possible to omit certain stages, if there is a document with a certain degree of maturity at the start of a standardization project, for example a standard developed by another organization. ISO/IEC directives allow also the so-called "Fast-track procedure". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS) if the document was developed by an international standardizing body recognized by the ISO Council.
The first step—a proposal of work (New Proposal) is approved at the relevant subcommittee or technical committee (e.g., SC29 and JTC1 respectively in the case of Moving Picture Experts Group - ISO/IEC JTC1/SC29/WG11). A working group (WG) of experts is set up by the TC/SC for the preparation of a working draft. When the scope of a new work is sufficiently clarified, some of the working groups (e.g., MPEG) usually make open request for proposals—known as a "call for proposals". The first document that is produced for example for audio and video coding standards is called a verification model (VM) (previously also called a "simulation and test model"). When a sufficient confidence in the stability of the standard under development is reached, a working draft (WD) is produced. This is in the form of a standard but is kept internal to working group for revision. When a working draft is sufficiently solid and the working group is satisfied that it has developed the best technical solution to the problem being addressed, it becomes committee draft (CD). If it is required, it is then sent to the P-members of the TC/SC (national bodies) for ballot.
The CD becomes final committee draft (FCD) if the number of positive votes is above the quorum. Successive committee drafts may be considered until consensus is reached on the technical content. When it is reached, the text is finalized for submission as a draft International Standard (DIS). The text is then submitted to national bodies for voting and comment within a period of five months. It is approved for submission as a final draft International Standard (FDIS) if a two-thirds majority of the P-members of the TC/SC are in favour and not more than one-quarter of the total number of votes cast are negative. ISO will then hold a ballot with National Bodies where no technical changes are allowed (yes/no ballot), within a period of two months. It is approved as an International Standard (IS) if a two-thirds majority of the P-members of the TC/SC is in favour and not more than one-quarter of the total number of votes cast are negative. After approval, only minor editorial changes are introduced into the final text. The final text is sent to the ISO Central Secretariat, which publishes it as the International Standard.
Products named after ISO.
The fact that many of the ISO-created standards are ubiquitous has led, on occasion, to common use of "ISO" to describe the actual product that conforms to a standard. Some examples of this are:
Criticism.
With the exception of a small number of isolated standards, ISO standards are normally not available free of charge, but for a purchase fee, which has been seen by some as too expensive for small open source projects.
The ISO/IEC JTC1 fast-track procedures ("Fast-track" as used by OOXML and "PAS" as used by OpenDocument) have garnered criticism in relation to the standardization of Office Open XML (ISO/IEC 29500). Martin Bryan, outgoing Convenor of ISO/IEC JTC1/SC34 WG1, is quoted as saying:
I would recommend my successor that it is perhaps time to pass WG1’s outstanding standards over to OASIS, where they can get approval in less than a year and then do a PAS submission to ISO, which will get a lot more attention and be approved much faster than standards currently can be within WG1.
The disparity of rules for PAS, Fast-Track and ISO committee generated standards is fast making ISO a laughing stock in IT circles. The days of open standards development are fast disappearing. Instead we are getting 'standardization by corporation'.
Computer security entrepreneur and Ubuntu investor, Mark Shuttleworth, commented on the Standardization of Office Open XML process by saying "I think it de-values the confidence people have in the standards setting process," and Shuttleworth alleged that ISO did not carry out its responsibility. He also noted that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers and resellers sympathetic to Office Open XML.
When you have a process built on trust and when that trust is abused, ISO should halt the process... ISO is an engineering old boys club and these things are boring so you have to have a lot of passion … then suddenly you have an investment of a lot of money and lobbying and you get artificial results. The process is not set up to deal with intensive corporate lobbying and so you end up with something being a standard that is not clear.
Further reading.
</dl>

</doc>
<doc id="14936" url="http://en.wikipedia.org/wiki?curid=14936" title="Individualist anarchism">
Individualist anarchism

Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasize the individual and their will over external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict. Thereafter, it expanded through Europe and the United States. Benjamin R. Tucker, a famous 19th-century individualist anarchist, held that "if the individual has the right to govern himself, all external government is tyranny."
Overview.
Among the early influences on individualist anarchism were William Godwin, Henry David Thoreau (transcendentalism), Josiah Warren ("sovereignty of the individual"), Lysander Spooner ("natural law"), Pierre Joseph Proudhon (mutualism), Anselme Bellegarrigue, Herbert Spencer ("law of equal liberty"), and Max Stirner (egoism).
Individualist anarchism of different kinds have a few things in common. These are:
1. The concentration on the individual and his/her will in preference to any construction such as morality, ideology, social custom, religion, metaphysics, ideas or the will of others.
2. The rejection of or reservations about the idea of revolution, seeing it as a time of mass uprising which could bring about new hierarchies. Instead they favor more evolutionary methods of bringing about anarchy through alternative experiences and experiments and education which could be brought about today. This is also because it is not seen as desirable for individuals to wait for revolution to start experiencing alternative experiences outside what is offered in the current social system.
3. The view that relationships with other persons or things can be in one's own interest only and can be as transitory and without compromises as desired since in individualist anarchism sacrifice is usually rejected. In this way, Max Stirner recommended associations of egoists. Individual experience and exploration therefore is emphasized.
The egoist form of individualist anarchism, derived from the philosophy of Max Stirner, supports the individual doing exactly what he pleases — taking no notice of God, state, or moral rules. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality" — he supported property by force of might rather than moral right. Stirner advocated self-assertion and foresaw "associations of egoists" drawn together by respect for each other's ruthlessness.
For American anarchist historian Eunice Minette Schuster, American individualist anarchism "stresses the isolation of the individual — his right to his own tools, his mind, his body, and to the products of his labor. To the artist who embraces this philosophy it is "aesthetic" anarchism, to the reformer, ethical anarchism, to the independent mechanic, economic anarchism. The former is concerned with philosophy, the latter with practical demonstration. The economic anarchist is concerned with constructing a society on the basis of anarchism. Economically he sees no harm whatever in the private possession of what the individual produces by his own labor, but only so much and no more. The aesthetic and ethical type found expression in the transcendentalism, humanitarianism, and romanticism of the first part of the nineteenth century, the economic type in the pioneer life of the West during the same period, but more favorably after the Civil War." It is for this reason that it has been suggested that in order to understand American individualist anarchism one must take into account "the social context of their ideas, namely the transformation of America from a pre-capitalist to a capitalist society ... the non-capitalist nature of the early U.S. can be seen from the early dominance of self-employment (artisan and peasant production). At the beginning of the 19th century, around 80% of the working (non-slave) male population were self-employed. The great majority of Americans during this time were farmers working their own land, primarily for their own needs." and so "Individualist anarchism is clearly a form of artisanal socialism ... while communist anarchism and anarcho-syndicalism are forms of industrial (or proletarian) socialism." Contemporary individualist anarchist Kevin Carson characterizes American individualist anarchism saying that "Unlike the rest of the socialist movement, the individualist anarchists believed that the natural wage of labor in a free market was its product, and that economic exploitation could only take place when capitalists and landlords harnessed the power of the state in their interests. Thus, individualist anarchism was an alternative both to the increasing statism of the mainstream socialist movement, and to a classical liberal movement that was moving toward a mere apologetic for the power of big business." 
In European individualist anarchism a different social context helped the rise of European individualist illegalism and as such "The illegalists were proletarians who had nothing to sell but their labour power, and nothing to discard but their dignity; if they disdained waged-work, it was because of its compulsive nature. If they turned to illegality it was due to the fact that honest toil only benefited the employers and often entailed a complete loss of dignity, while any complaints resulted in the sack; to avoid starvation through lack of work it was necessary to beg or steal, and to avoid conscription into the army many of them had to go on the run." And so a European tendency of individualist anarchism advocated violent individual acts of individual reclamation, propaganda by the deed and criticism of organization. Such individualist anarchist tendencies include French illegalism and Italian anti-organizational insurrectionarism. Bookchin reports that at the end of the 19th century and the beginning of the 20th "it was in times of severe social repression and deadening social quiescence that individualist anarchists came to the foreground of libertarian activity—and then primarily as terrorists. In France, Spain, and the United States, individualistic anarchists committed acts of terrorism that gave anarchism its reputation as a violently sinister conspiracy.".
Another important tendency within individualist anarchist currents emphasizes individual subjective exploration and defiance of social conventions. Individualist anarchist philosophy attracted "amongst artists, intellectuals and the well-read, urban middle classes in general."
As such Murray Bookchin describes a lot of individualist anarchism as people who "expressed their opposition in uniquely personal forms, especially in fiery tracts, outrageous behavior, and aberrant lifestyles in the cultural ghettos of fin de siecle New York, Paris, and London. As a credo, individualist anarchism remained largely a bohemian lifestyle, most conspicuous in its demands for sexual freedom ('free love') and enamored of innovations in art, behavior, and clothing.". In this way free love currents and other radical lifestyles such as naturism had popularity among individualist anarchists.
For Catalan historian Xavier Diez, individualist anarchism "under its iconoclastic, antiintelectual, antitheist run, which goes against all sacralized ideas or values it entailed, a philosophy of life which could be considered a reaction against the sacred gods of capitalist society. Against the idea of nation, it opposed its internationalism. Against the exaltation of authority embodied in the military institution, it opposed its antimilitarism. Against the concept of industrial civilization, it opposed its naturist vision".
In regards to economic questions, there are diverse positions. There are adherents to mutualism (Proudhon, Émile Armand, early Benjamin Tucker), egoistic disrespect for "ghosts" such as private property and markets (Stirner, John Henry Mackay, Lev Chernyi, later Tucker), and adherents to anarcho-communism (Albert Libertad, illegalism, Renzo Novatore). Anarchist historian George Woodcock finds a tendency in individualist anarchism of a "distrust (of) all co-operation beyond the barest minimum for an ascetic life".
On the issue of violence opinions have gone from a violentist point of view mainly exemplified by illegalism and insurrectionary anarchism to one that can be called anarcho-pacifist. In the particular case of Spanish individualist anarchist Miguel Gimenez Igualada, he went from illegalist practice in his youth towards a pacifist position later in his life.
Early influences.
William Godwin.
William Godwin can be considered an individualist anarchist and philosophical anarchist who was influenced by the ideas of the Age of Enlightenment, and developed what many consider the first expression of modern anarchist thought. Godwin was, according to Peter Kropotkin, "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work." Godwin advocated extreme individualism, proposing that all cooperation in labor be eliminated. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us "of more worth and importance' than others depending on our utility in bringing about social good. Therefore he does not believe in equal rights, but the person's life that should be favored that is most conducive to the general good. Godwin opposed government because it infringes on the individual's right to "private judgement" to determine which actions most maximize utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, minus the utilitarianism, was developed into a more extreme form later by Stirner.
Godwin took individualism to the radical extent of opposing individuals performing together in orchestras, writing in "Political Justice" that "everything understood by the term co-operation is in some sense an evil." The only apparent exception to this opposition to cooperation is the spontaneous association that may arise when a society is threatened by violent force. One reason he opposed cooperation is he believed it to interfere with an individual's ability to be benevolent for the greater good. Godwin opposes the idea of government, but wrote that a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. He expressly opposed democracy, fearing oppression of the individual by the majority, though he believed it to be preferable to dictatorship.
Godwin supported individual ownership of property, defining it as "the empire to which every man is entitled over the produce of his own industry." However, he also advocated that individuals give to each other their surplus property on the occasion that others have a need for it, without involving trade (e.g. "gift economy"). Thus, while people have the right to private property, they "should" give it away as enlightened altruists. This was to be based on utilitarian principles; he said: "Every man has a right to that, the exclusive possession of which being awarded to him, a greater sum of benefit or pleasure will result than could have arisen from its being otherwise appropriated." However, benevolence was not to be enforced, being a matter of free individual "private judgement." He did not advocate a community of goods or assert collective ownership as is embraced in communism, but his belief that individuals ought to share with those in need was influential on the later development of anarchist communism.
Godwin's political views were diverse and do not perfectly agree with any of the ideologies that claim his influence; writers of the "Socialist Standard", organ of the Socialist Party of Great Britain, consider Godwin both an individualist and a communist; anarcho-capitalist Murray Rothbard did not regard Godwin as being in the individualist camp at all, referring to him as the "founder of communist anarchism";<ref name=rothbard/burke>Rothbard, Murray. "."</ref> and historian Albert Weisbord considers him an individualist anarchist without reservation. Some writers see a conflict between Godwin's advocacy of "private judgement" and utilitarianism, as he says that ethics requires that individuals give their surplus property to each other resulting in an egalitarian society, but, at the same time, he insists that all things be left to individual choice. Many of Godwin's views changed over time, as noted by Kropotkin.
William Godwin's influenced "the socialism of Robert Owen and Charles Fourier. After success of his British venture, Owen himself established a cooperative community within the United States at New Harmony, Indiana during 1825. One member of this commune was Josiah Warren (1798–1874), considered to be the first individualist anarchist. After New Harmony failed Warren shifted his ideological loyalties from socialism to anarchism (which was no great leap, given that Owen's socialism had been predicated on Godwin's anarchism)."
Pierre-Joseph Proudhon.
Pierre-Joseph Proudhon (1809–1865) was the first philosopher to label himself an "anarchist." Some consider Proudhon to be an individualist anarchist, while others regard him to be a social anarchist. Some commentators do not identify Proudhon as an individualist anarchist due to his preference for association in large industries, rather than individual control. Nevertheless, he was influential among some of the American individualists; in the 1840s and 1850s, Charles A. Dana, and William B. Greene introduced Proudhon's works to the United States. Greene adapted Proudhon's mutualism to American conditions and introduced it to Benjamin R. Tucker.
Proudhon opposed government privilege that protects capitalist, banking and land interests, and the accumulation or acquisition of property (and any form of coercion that led to it) which he believed hampers competition and keeps wealth in the hands of the few. Proudhon favoured a right of individuals to retain the product of their labour as their own property, but believed that any property beyond that which an individual produced and could possess was illegitimate. Thus, he saw private property as both essential to liberty and a road to tyranny, the former when it resulted from labour and was required for labour and the latter when it resulted in exploitation (profit, interest, rent, tax). He generally called the former "possession" and the latter "property." For large-scale industry, he supported workers associations to replace wage labour and opposed the ownership of land.
Proudhon maintained that those who labour should retain the entirety of what they produce, and that monopolies on credit and land are the forces that prohibit such. He advocated an economic system that included private property as possession and exchange market but without profit, which he called mutualism. It is Proudhon's philosophy that was explicitly rejected by Joseph Dejacque in the inception of anarchist-communism, with the latter asserting directly to Proudhon in a letter that "it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature." An individualist rather than anarchist communist, Proudhon said that "communism ... is the very denial of society in its foundation ..." and famously declared that "property is theft!" in reference to his rejection of ownership rights to land being granted to a person who is not using that land.
After Dejacque and others split from Proudhon due to the latter's support of individual property and an exchange economy, the relationship between the individualists, who continued in relative alignment with the philosophy of Proudhon, and the anarcho-communists was characterised by various degrees of antagonism and harmony. For example, individualists like Tucker on the one hand translated and reprinted the works of collectivists like Mikhail Bakunin, while on the other hand rejected the economic aspects of collectivism and communism as incompatible with anarchist ideals.
Mutualism.
Mutualism is an anarchist school of thought which can be traced to the writings of Pierre-Joseph Proudhon, who envisioned a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labor in the free market. Integral to the scheme was the establishment of a mutual-credit bank which would lend to producers at a minimal interest rate only high enough to cover the costs of administration. Mutualism is based on a labor theory of value which holds that when labour or its product is sold, in exchange, it ought to receive goods or services embodying "the amount of labor necessary to produce an article of exactly similar and equal utility". Some mutualists believe that if the state did not intervene, as a result of increased competition in the marketplace, individuals would receive no more income than that in proportion to the amount of labor they exert. Mutualists oppose the idea of individuals receiving an income through loans, investments, and rent, as they believe these individuals are not labouring. Some of them argue that if state intervention ceased, these types of incomes would disappear due to increased competition in capital. Though Proudhon opposed this type of income, he expressed: "... I never meant to ... forbid or suppress, by sovereign decree, ground rent and interest on capital. I believe that all these forms of human activity should remain free and optional for all."
Insofar as they ensure the workers right to the full product of their labor, mutualists support markets and private property in the product of labor. However, they argue for conditional titles to land, whose private ownership is legitimate only so long as it remains in use or occupation (which Proudhon called "possession.") Proudhon's Mutualism supports labor-owned cooperative firms and associations for "we need not hesitate, for we have no choice. . . it is necessary to form an ASSOCIATION among workers . . . because without that, they would remain related as subordinates and superiors, and there would ensue two . . . castes of masters and wage-workers, which is repugnant to a free and democratic society" and so "it becomes necessary for the workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism." As for capital goods (man-made, non-land, "means of production"), mutualist opinion differs on whether these should be commonly managed public assets or private property.
Mutualists, following Proudhon, originally considered themselves to be libertarian socialists. However, "some mutualists have abandoned the labor theory of value, and prefer to avoid the term "socialist." But they still retain some cultural attitudes, for the most part, that set them off from the libertarian right." Mutualists have distinguished themselves from state socialism, and don't advocate social control over the means of production. Benjamin Tucker said of Proudhon, that "though opposed to socializing the ownership of capital, Proudhon aimed nevertheless to socialize its effects by making its use beneficial to all instead of a means of impoverishing the many to enrich the few ... by subjecting capital to the natural law of competition, thus bringing the price of its own use down to cost."
Max Stirner.
Johann Kaspar Schmidt (October 25, 1806 – June 26, 1856), better known as Max Stirner (the "nom de plume" he adopted from a schoolyard nickname he had acquired as a child because of his high brow, in German 'Stirn'), was a German philosopher, who ranks as one of the literary fathers of nihilism, existentialism, post-modernism and anarchism, especially of individualist anarchism. Stirner's main work is "The Ego and Its Own", also known as "The Ego and His Own" ("Der Einzige und sein Eigentum" in German, which translates literally as "The Only One and his Property"). This work was first published in 1844 in Leipzig, and has since appeared in numerous editions and translations.
Egoism.
Max Stirner's philosophy, sometimes called "egoism," is a form of individualist anarchism. Max Stirner was a Hegelian philosopher whose "name appears with familiar regularity in historically oriented surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism." In 1844, his "The Ego and Its Own" ("Der Einzige and sein Eigentum" which may literally be translated as "The Unique Individual and His Property") was published, which is considered to be "a founding text in the tradition of individualist anarchism." Stirner does not recommend that the individual try to eliminate the state but simply that they disregard the state when it conflicts with one's autonomous choices and go along with it when doing so is conducive to one's interests. He says that the egoist rejects pursuit of devotion to "a great idea, a good cause, a doctirine, a system, a lofty calling," saying that the egoist has no political calling but rather "lives themselves out" without regard to "how well or ill humanity may fare thereby." Stirner held that the only limitation on the rights of the individual is that individual's power to obtain what he desires. He proposes that most commonly accepted social institutions—including the notion of State, property as a right, natural rights in general, and the very notion of society—were mere "spooks" in the mind. Stirner wants to "abolish not only the state but also society as an institution responsible for its members." Stirner advocated self-assertion and foresaw Unions of Egoists, non-systematic associations, which Stirner proposed in as a form of organization in place of the state. A Union is understood as a relation between egoists which is continually renewed by all parties' support through an act of will. Even murder is permissible "if it is right for me," though it is claimed by egoist anarchists that egoism will foster genuine and spontaneous union between individuals.
For Stirner, property simply comes about through might: "Whoever knows how to take, to defend, the thing, to him belongs property." And, "What I have in my power, that is my own. So long as I assert myself as holder, I am the proprietor of the thing." He says, "I do not step shyly back from your property, but look upon it always as my property, in which I respect nothing. Pray do the like with what you call my property!". His concept of "egoistic property" not only a lack of moral restraint on how one obtains and uses "things", but includes other people as well. His embrace of egoism is in stark contrast to Godwin's altruism. Stirner was opposed to communism, seeing it as a form of authority over the individual.
This position on property is much different from the native American, natural law, form of individualist anarchism, which defends the inviolability of the private property that has been earned through labor and trade. However, in 1886 Benjamin Tucker rejected the natural rights philosophy and adopted Stirner's egoism, with several others joining with him. This split the American individualists into fierce debate, "with the natural rights proponents accusing the egoists of destroying libertarianism itself." Other egoists include James L. Walker, Sidney Parker, Dora Marsden and John Beverly Robinson.
In Russia, individualist anarchism inspired by Stirner combined with an appreciation for Friedrich Nietzsche attracted a small following of bohemian artists and intellectuals such as Lev Chernyi, as well as a few lone wolves who found self-expression in crime and violence. They rejected organizing, believing that only unorganized individuals were safe from coercion and domination, believing this kept them true to the ideals of anarchism. This type of individualist anarchism inspired anarcha-feminist Emma Goldman
Though Stirner's philosophy is individualist, it has influenced some libertarian communists and anarcho-communists. "For Ourselves Council for Generalized Self-Management" discusses Stirner and speaks of a "communist egoism," which is said to be a "synthesis of individualism and collectivism," and says that "greed in its fullest sense is the only possible basis of communist society." Forms of libertarian communism such as Situationism are influenced by Stirner. Anarcho-communist Emma Goldman was influenced by both Stirner and Peter Kropotkin and blended their philosophies together in her own, as shown in books of hers such as "Anarchism And Other Essays".
Early American individualist anarchism.
Josiah Warren.
 Josiah Warren is widely regarded as the first American anarchist, and the four-page weekly paper he edited during 1833, "The Peaceful Revolutionist", was the first anarchist periodical published, an enterprise for which he built his own printing press, cast his own type, and made his own printing plates.
Warren was a follower of Robert Owen and joined Owen's community at New Harmony, Indiana. Josiah Warren termed the phrase "Cost the limit of price," with "cost" here referring not to monetary price paid but the labor one exerted to produce an item. Therefore, "[h]e proposed a system to pay people with certificates indicating how many hours of work they did. They could exchange the notes at local time stores for goods that took the same amount of time to produce.". He put his theories to the test by establishing an experimental "labor for labor store" called the Cincinnati Time Store where trade was facilitated by notes backed by a promise to perform labor. The store proved successful and operated for three years after which it was closed so that Warren could pursue establishing colonies based on mutualism. These included "Utopia" and "Modern Times." Warren said that Stephen Pearl Andrews' "The Science of Society", published in 1852, was the most lucid and complete exposition of Warren's own theories. Catalan historian Xavier Diez report that the intentional communal experiments pioneered by Warren were influential in European individualist anarchists of the late 19th and early 20th centuries such as Émile Armand and the intentional communities started by them.
Henry David Thoreau.
 Henry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher, and leading transcendentalist. He is best known for his book "Walden", a reflection upon simple living in natural surroundings, and his essay, "Civil Disobedience", an argument for individual resistance to civil government in moral opposition to an unjust state. His thought is an early influence on green anarchism but with an emphasis on the individual experience of the natural world influencing later naturist currents, simple living as a rejection of a materialist lifestyle and self-sufficiency were Thoreau's goals, and the whole project was inspired by transcendentalist philosophy. "Many have seen in Thoreau one of the precursors of ecologism and anarcho-primitivism represented today in John Zerzan. For George Woodcock this attitude can be also motivated by certain idea of resistance to progress and of rejection of the growing materialism which is the nature of American society in the mid 19th century."
The essay Civil Disobedience ("Resistance to Civil Government") was first published in 1849. It argues that people should not permit governments to overrule or atrophy their consciences, and that people have a duty to avoid allowing such acquiescence to enable the government to make them the agents of injustice. Thoreau was motivated in part by his disgust with slavery and the Mexican-American War. The essay later influenced Mohandas Gandhi, Martin Luther King, Jr., Martin Buber and Leo Tolstoy through its advocacy of nonviolent resistance. It is also the main precedent for anarcho-pacifism. The American version of individualist anarchism has a strong emphasis on the non-aggression principle and individual sovereignty. Some individualist anarchists, such as Thoreau, do not speak of economics but simply the right of "disunion" from the state, and foresee the gradual elimination of the state through social evolution.
Developments and expansion.
Free love, anarcha-feminism and LGBT issues.
An important current within individualist anarchism is free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, and viewed sexual freedom as a clear, direct expression of an individual's self-ownership. Free love particularly stressed women's rights since most sexual laws, such as those governing marriage and use of birth control, discriminated against women. The most important American free love journal was "Lucifer the Lightbearer" (1883–1907) edited by Moses Harman and Lois Waisbrooker but also there existed Ezra Heywood and Angela Heywood's "The Word" (1872–1890, 1892–1893). Also M. E. Lazarus was an important American individualist anarchist who promoted free love. Later John William Lloyd, a collaborator of Benjamin Tucker´s periodical "Liberty", published in 1931 a sex manual that he called "The Karezza Method: Or Magnetation, the Art of Connubial Love".
In Europe, the main propagandist of free love within individualist anarchism was Émile Armand. He proposed the concept of "la camaraderie amoureuse" to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In France, there was also feminist activity inside individualist anarchism as promoted by individualist feminists Marie Küge, Anna Mahé, Rirette Maitrejean, and Sophia Zaïkovska.
The Brazilian individualist anarchist Maria Lacerda de Moura lectured on topics such as education, women's rights, free love, and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. She also wrote for the Spanish individualist anarchist magazine "Al Margen" alongside Miguel Gimenez Igualada
In Germany, the Stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality.
Freethought.
Freethought as a philosophical position and as activism was important in both North American and European individualist anarchism.
In the United States, "freethought was a basically anti-Christian, anti-clerical movement, whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to "Liberty" were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of "Freethought" and, for a time, "The Truth Seeker". E.C. Walker was co-editor of the excellent free-thought / free love journal "Lucifer, the Light-Bearer"". "Many of the anarchists were ardent freethinkers; reprints from freethought papers such as "Lucifer, the Light-Bearer", "Freethought" and "The Truth Seeker" appeared in "Liberty" ... The church was viewed as a common ally of the state and as a repressive force in and of itself".
In Europe a similar development occurred in French and Spanish individualist anarchist circles. "Anticlericalism, just as in the rest of the libertarian movement, is another of the frequent elements which will gain relevance related to the measure in which the (French) Republic begins to have conflicts with the church ... Anti-clerical discourse, frequently called for by the french individualist André Lorulot, will have its impacts in "Estudios" (a Spanish individualist anarchist publication). There will be an attack on institutionalized religion for the responsibility that it had in the past on negative developments, for its irrationality which makes it a counterpoint of philosophical and scientific progress. There will be a criticism of proselitism and ideological manipulation which happens on both believers and agnostics.". This tendencies will continue in French individualist anarchism in the work and activism of Charles-Auguste Bontemps and others. In the Spanish individualist anarchist magazine "Ética" and "Iniciales" "there is a strong interest in publishing scientific news, usually linked to a certain atheist and anti-theist obsession, philosophy which will also work for pointing out the incompatibility between science and religion, faith and reason. In this way there will be a lot of talk on Darwin's theories or on the negation of the existence of the soul.".
Anarcho-naturism.
Another important current, especially within French and Spanish individualist anarchist groups was naturism. Naturism promoted an ecological worldview, small ecovillages, and most prominently nudism as a way to avoid the artificiality of the industrial mass society of modernity. Naturist individualist anarchists saw the individual in his biological, physical and psychological aspects and avoided, and tried to eliminate, social determinations. An early influence in this vein was Henry David Thoreau and his famous book "Walden" Important promoters of this were Henri Zisly and Emile Gravelle who collaborated in "La Nouvelle Humanité" followed by "Le Naturien", "Le Sauvage", "L'Ordre Naturel", & "La Vie Naturelle".
This relationship between anarchism and naturism was quite important at the end of the 1920s in Spain. "The linking role played by the ‘Sol y Vida’ group was very important. The goal of this group was to take trips and enjoy the open air. The Naturist athenaeum, ‘Ecléctico’, in Barcelona, was the base from which the activities of the group were launched. First "Etica" and then "Iniciales", which began in 1929, were the publications of the group, which lasted until the Spanish Civil War. We must be aware that the naturist ideas expressed in them matched the desires that the libertarian youth had of breaking up with the conventions of the bourgeoisie of the time. That is what a young worker explained in a letter to ‘Iniciales’ He writes it under the odd pseudonym of ‘silvestre del campo’, (wild man in the country). "I find great pleasure in being naked in the woods, bathed in light and air, two natural elements we cannot do without. By shunning the humble garment of an exploited person, (garments which, in my opinion, are the result of all the laws devised to make our lives bitter), we feel there no others left but just the natural laws. Clothes mean slavery for some and tyranny for others. Only the naked man who rebels against all norms, stands for anarchism, devoid of the prejudices of outfit imposed by our money-oriented society.". "The relation between Anarchism and Naturism gives way to the Naturist Federation, in July 1928, and to the lV Spanish Naturist Congress, in September 1929, both supported by the Libertarian Movement. However, in the short term, the Naturist and Libertarian movements grew apart in their conceptions of everyday life. The Naturist movement felt closer to the Libertarian individualism of some French theoreticians such as Henri Ner (real name of Han Ryner) than to the revolutionary goals proposed by some Anarchist organisations such as the FAI, (Federación Anarquista Ibérica)".
Individualist anarchism and Friedrich Nietzsche.
The thought of German philosopher Friedrich Nietzsche has been influential in individualist anarchism, specifically in thinkers such as France's Émile Armand, the Italian Renzo Novatore, and the Colombian Biofilo Panclasta. Robert C. Holub, author of "Nietzsche: Socialist, Anarchist, Feminist" posits that "translations of Nietzsche's writings in the United States very likely appeared first in "Liberty", the anarchist journal edited by Benjamin Tucker ".
Anglo American individualist anarchism.
American mutualism and individualist utopianism.
For American anarchist historian Eunice Minette Schuster "It is apparent ... that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews ... William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form.". William Batchelder Greene (1819–1878) is best known for the works "Mutual Banking"(1850), which proposed an interest-free banking system, and "Transcendentalism", a critique of the New England philosophical school. He saw mutualism as the synthesis of "liberty and order." His "associationism ... is checked by individualism ... "Mind your own business," "Judge not that ye be not judged." Over matters which are purely personal, as for example, moral conduct, the individual is sovereign, as well as over that which he himself produces. For this reason he demands "mutuality" in marriage—the equal right of a woman to her own personal freedom and property" and feminist and spiritualist tendencies."
Contemporary American anarchist Hakim Bey reports that "Steven Pearl Andrews ... was not a fourierist (see Charles Fourier), but he lived through the brief craze for phalansteries in America & adopted a lot of fourierist principles & practices ... a maker of worlds out of words. He syncretized Abolitionism, Free Love, spiritual universalism, (Josiah) Warren, & (Charles) Fourier into a grand utopian scheme he called the Universal Pantarchy ... He was instrumental in founding several “intentional communities,” including the “Brownstone Utopia” on 14th St. in New York, & “Modern Times” in Brentwood, Long Island. The latter became as famous as the best-known fourierist communes (Brook Farm in Massachusetts & the North American Phalanx in New Jersey) — in fact, Modern Times became downright notorious (for “Free Love”) & finally foundered under a wave of scandalous publicity. Andrews (& Victoria Woodhull) were members of the infamous Section 12 of the 1st International, expelled by Marx for its anarchist, feminist, & spiritualist tendencies. "
The "Boston Anarchists".
Another form of individualist anarchism was found in the United States, as advocated by the "Boston anarchists." By default, American individualists had no difficulty accepting the concepts that "one man employ another" or that "he direct him," in his labor but rather demanded that "all natural opportunities requisite to the production of wealth be accessible to all on equal terms and that monopolies arising from special privileges created by law be abolished."
They believed state monopoly capitalism (defined as a state-sponsored monopoly) prevented labor from being fully rewarded. Voltairine de Cleyre, summed up the philosophy by saying that the anarchist individualists "are firm in the idea that the system of employer and employed, buying and selling, banking, and all the other essential institutions of Commercialism, centred upon private property, are in themselves good, and are rendered vicious merely by the interference of the State."
Even among the 19th-century American individualists, there was not a monolithic doctrine, as they disagreed amongst each other on various issues including intellectual property rights and possession versus property in land. A major schism occurred later in the 19th century when Tucker and some others abandoned their traditional support of natural rights—as espoused by Lysander Spooner- and converted to an "egoism" modeled upon Stirner's philosophy. Lysander Spooner besides his individualist anarchist activism was also an important anti-slavery activist and became a member of the First International.
Some "Boston anarchists", including Benjamin Tucker, identified themselves as "socialists", which in the 19th century was often used in the sense of a commitment to improving conditions of the working class (i.e. "the labor problem"). By around the start of the 20th century, the heyday of individualist anarchism had passed,
American individualist anarchism and the labor movement.
George Woodcock reports that the American individualist anarchists Lysander Spooner and William B. Greene had been members of the socialist First International
Two individualist anarchists who wrote in Benjamin Tucker´s "Liberty" were also important labor organizers of the time. Joseph Labadie (April 18, 1850 – October 7, 1933) was an American labor organizer, individualist anarchist, social activist, printer, publisher, essayist, and poet. In 1883 Labadie embraced a non-violent version of individualist anarchism. Without the oppression of the state, Labadie believed, humans would choose to harmonize with "the great natural laws ... without robbing [their] fellows through interest, profit, rent and taxes." However, he supported community cooperation, as he supported community control of water utilities, streets, and railroads. Although he did not support the militant anarchism of the Haymarket anarchists, he fought for the clemency of the accused because he did not believe they were the perpetrators. In 1888, Labadie organized the Michigan Federation of Labor, became its first president, and forged an alliance with Samuel Gompers. 
A colleague of Labadie's at "Liberty", Dyer Lum was another important individualist anarchist labor activist and poet of the era. A leading anarcho-syndicalist and a prominent left-wing intellectual of the 1880s, he is remembered as the lover and mentor of early anarcha-feminist Voltairine de Cleyre. Lum was a prolific writer who wrote a number of key anarchist texts, and contributed to publications including "Mother Earth", "Twentieth Century", and, "The Alarm" (the journal of the International Working People's Association) and "The Open Court" among others. Lum's political philosophy was a fusion of individualist anarchist economics—"a radicalized form of laissez-faire economics" inspired by the Boston anarchists—with radical labor organization similar to that of the Chicago anarchists of the time. Herbert Spencer and Pierre-Joseph Proudhon influenced Lum strongly in his individualist tendency. He developed a "mutualist" theory of unions and as such was active within the Knights of Labor and later promoted anti-political strategies in the American Federation of Labor. Frustration with abolitionism, spiritualism, and labor reform caused Lum to embrace anarchism and radicalize workers, as he came to believe that revolution would inevitably involve a violent struggle between the working class and the employing class. Convinced of the necessity of violence to enact social change he volunteered to fight in the American Civil War, hoping thereby to bring about the end of slavery. Kevin Carson has praised Lum's fusion of individualist laissez-faire economics with radical labor activism as "creative" and described him as "more significant than any in the Boston group".
American egoism.
Some of the American individualist anarchists later in this era, such as Benjamin Tucker, abandoned natural rights positions and converted to Max Stirner's Egoist anarchism. Rejecting the idea of moral rights, Tucker said that there were only two rights, "the right of might" and "the right of contract." He also said, after converting to Egoist individualism, "In times past ... it was my habit to talk glibly of the right of man to land. It was a bad habit, and I long ago sloughed it off ... Man's only right to land is his might over it." In adopting Stirnerite egoism (1886), Tucker rejected natural rights which had long been considered the foundation of libertarianism. This rejection galvanized the movement into fierce debates, with the natural rights proponents accusing the egoists of destroying libertarianism itself. So bitter was the conflict that a number of natural rights proponents withdrew from the pages of "Liberty" in protest even though they had hitherto been among its frequent contributors. Thereafter, Liberty championed egoism although its general content did not change significantly."
"Several periodicals were undoubtedly influenced by "Liberty"'s presentation of egoism. They included: "I" published by C.L. Swartz, edited by W.E. Gordak and J.W. Lloyd (all associates of "Liberty"); "The Ego" and "The Egoist", both of which were edited by Edward H. Fulton. Among the egoist papers that Tucker followed were the German "Der Eigene", edited by Adolf Brand, and "The Eagle" and "The Serpent", issued from London. The latter, the most prominent English-language egoist journal, was published from 1898 to 1900 with the subtitle 'A Journal of Egoistic Philosophy and Sociology'".
American anarchists who adhered to egoism include Benjamin Tucker, John Beverley Robinson, Steven T. Byington, Hutchins Hapgood, James L. Walker and Victor Yarros and E.H. Fulton. John Beverley Robinson wrote an essay called "Egoism" in which he states that "Modern egoism, as propounded by Stirner and Nietzsche, and expounded by Ibsen, Shaw and others, is all these; but it is more. It is the realization by the individual that they are an individual; that, as far as they are concerned, they are the only individual." James L. Walker published the work "The Philosophy of Egoism" in which he argued that egosim "implies a rethinking of the self-other relationship, nothing less than "a complete revolution in the relations of mankind" that avoids both the "archist" principle that legitimates domination and the "moralist" notion that elevates self-renunciation to a virtue. Walker describes himself a s an "egoistic anarchist" who believed in both contract and cooperation as practical principles to guide everyday interactions." For Walker "what really defines egoism is not mere self-interest, pleasure, or greed; it is the sovereignty of the individual, the full expression of the subjectivity of the individual ego."
Italian anti-organizationalist individualist anarchism was brought to the United States by Italian born individualists such as Giuseppe Ciancabilla and others who advocated for violent propaganda by the deed there. Anarchist historian George Woodcock reports the incident in which the important Italian social anarchist Errico Malatesta became involved "in a dispute with the individualist anarchists of Paterson, who insisted that anarchism implied no organization at all, and that every man must act solely on his impulses. At last, in one noisy debate, the individual impulse of a certain Ciancabilla directed him to shoot Malatesta, who was badly wounded but obstinately refused to name his assailant."
Enrico Arrigoni (pseudonym: Frank Brand) was an Italian American individualist anarchist Lathe operator, house painter, bricklayer, dramatist and political activist influenced by the work of Max Stirner. He took the pseudonym "Brand" from a fictional character in one of Henrik Ibsen´s plays. In the 1910s he started becoming involved in anarchist and anti-war activism around Milan. From the 1910s until the 1920s he participated in anarchist activities and popular uprisings in various countries including Switzerland, Germany, Hungary, Argentina and Cuba. He lived from the 1920s onwards in New York City and there he edited the individualist anarchist eclectic journal "Eresia" in 1928. He also wrote for other American anarchist publications such as "L' Adunata dei refrattari", "Cultura Obrera", Controcorrente and Intessa Libertaria. During the Spanish Civil War, he went to fight with the anarchists but was imprisoned and was helped on his release by Emma Goldman. Afterwards Arrigoni became a longtime member of the Libertarian Book Club in New York City. His written works include "The totalitarian nightmare" (1975), "The lunacy of the Superman" (1977), "Adventures in the country of the monoliths" (1981) and "Freedom: my dream" (1986).
Anarcho-capitalism.
19th century individualist anarchists espoused the labor theory of value. Some believe that the modern movement of anarcho-capitalism is the result of simply removing the labor theory of value from ideas of the 19th-century American individualist anarchists: "Their successors today, such as Murray Rothbard, having abandoned the labor theory of value, describe themselves as anarcho-capitalists." As economic theory changed, the popularity of the labor theory of classical economics was superseded by the subjective theory of value of neo-classical economics. Murray Rothbard, a student of Ludwig von Mises, combined the Austrian school economics of his teacher with the absolutist views of human rights and rejection of the state he had absorbed from studying the individualist American anarchists of the 19th century such as Lysander Spooner and Benjamin Tucker. In the mid-1950s Rothbard wrote an article under a pseudonym, saying that "we are not anarchists ... but not archists either ... Perhaps, then, we could call ourselves by a new name: nonarchist," concerned with differentiating himself from communist and socialistic economic views of other anarchists (including the individualist anarchists of the 19th century).
There is a strong current within anarchism which does not consider that anarcho-capitalism can be considered a part of the anarchist movement due to the fact that anarchism has historically been an anti-capitalist movement and for definitional reasons which see anarchism incompatible with capitalist forms.
Agorism.
Agorism was developed from anarcho-capitalism in the late 20th-century by Samuel Edward Konkin III (a.k.a. SEK3). The goal of agorists is a society in which all "relations between people are voluntary exchanges—a free market." Agorists are market anarchists. Most Agorists consider that property rights are natural rights deriving from the primary right of self-ownership. Because of this they are not opposed in principle to collectively held property if individual owners of the property consent to collective ownership by contract or other voluntary mutual agreement. However, Agorists are divided on the question of intellectual property rights.[δ]
Though anarcho-capitalism has been regarded as a form of individualist anarchism, some writers deny that it is a form of anarchism, or that capitalism itself is compatible with anarchism.
Left-wing market anarchism.
Left-wing market anarchism, a form of left-libertarianism and individualist anarchism is associated with scholars such as Kevin Carson, Roderick T. Long, Charles Johnson, Brad Spangler, Samuel Edward Konkin III, Sheldon Richman, Chris Matthew Sciabarra, and Gary Chartier, who stress the value of radically free markets, termed "freed markets" to distinguish them from the common conception which these libertarians believe to be riddled with statist and capitalist privileges. Referred to as left-wing market anarchists or market-oriented left-libertarians, proponents of this approach strongly affirm the classical liberal ideas of self-ownership and free markets, while maintaining that, taken to their logical conclusions, these ideas support anti-capitalist, anti-corporatist, anti-hierarchical, pro-labor positions in economics; anti-imperialism in foreign policy; and thoroughly liberal or radical views regarding such cultural issues as gender, sexuality, and race.
The genealogy of contemporary market-oriented left-libertarianism—sometimes labeled "left-wing market anarchism"'—overlaps to a significant degree with that of Steiner–Vallentyne left-libertarianism as the roots of that tradition are sketched in the book "The Origins of Left-Libertarianism". Carson–Long-style left-libertarianism is rooted in nineteenth-century mutualism and in the work of figures such as Thomas Hodgskin and the individualist anarchists Benjamin Tucker and Lysander Spooner. Left wing market anarchism identifies with Left-libertarianism (or left-wing libertarianism) which names several related but distinct approaches to politics, society, culture, and political and social theory, which stress both individual freedom and social justice. Unlike right-libertarians, they believe that neither claiming nor mixing one's labor with natural resources is enough to generate full private property rights, and maintain that natural resources (land, oil, gold, trees) ought to be held in some egalitarian manner, either unowned or owned collectively. Those left-libertarians who support private property do so under the condition that recompense is offered to the local community. Gary Chartier has joined Kevin Carson, Charles Johnson, and others (echoing the language of Benjamin Tucker and Thomas Hodgskin) in maintaining that, because of its heritage and its emancipatory goals and potential, radical market anarchism should be seen—by its proponents and by others—as part of the socialist tradition, and that market anarchists can and should call themselves "socialists."
Post-left anarchy and insurrectionary anarchism.
Murray Bookchin has identified post-left anarchy as a form of individualist anarchism in "" where he identifies "a shift among Euro-American anarchists away from social anarchism and toward individualist or lifestyle anarchism. Indeed, lifestyle anarchism today is finding its principal expression in spray-can graffiti, post-modernist nihilism, antirationalism, neo-primitivism, anti-technologism, neo-Situationist 'cultural terrorism', mysticism, and a 'practice' of staging Foucauldian 'personal insurrections'." Post-left anarchist Bob Black in his long critique of Bookchin's philosophy called "Anarchy after leftism" said about post-left anarchy that "It is, unlike Bookchinism, “individualistic” in the sense that if the freedom and happiness of the individual—i.e., each and every really existing person, every Tom, Dick and Murray—is not the measure of the good society, what is?".
A strong relationship does exist between post-left anarchism and the work of individualist anarchist Max Stirner. Jason McQuinn says that "when I (and other anti-ideological anarchists) criticize ideology, it is always from a specifically critical, anarchist perspective rooted in both the skeptical, individualist-anarchist philosophy of Max Stirner. Also Bob Black and Feral Faun/Wolfi Landstreicher strongly adhere to stirnerist egoist anarchism. Bob Black has humorously suggested the idea of "marxist stirnerism".
Hakim Bey has said "From Stirner's "Union of Self-Owning Ones" we proceed to Nietzsche's circle of "Free Spirits" and thence to Charles Fourier's "Passional Series", doubling and redoubling ourselves even as the Other multiplies itself in the eros of the group." Bey also wrote that "The Mackay Society, of which Mark & I are active members, is devoted to the anarchism of Max Stirner, Benj. Tucker & John Henry Mackay ... The Mackay Society, incidentally, represents a little-known current of individualist thought which never cut its ties with revolutionary labor. Dyer Lum, Ezra & Angela Haywood represent this school of thought; Jo Labadie, who wrote for Tucker’s "Liberty", made himself a link between the American “plumb-line” anarchists, the “philosophical” individualists, & the syndicalist or communist branch of the movement; his influence reached the Mackay Society through his son, Laurance. Like the Italian Stirnerites (who influenced us through our late friend Enrico Arrigoni) we support all anti-authoritarian currents, despite their apparent contradictions."
As far as posterior individualist anarchists, Jason McQuinn for some time used the pseudonym Lev Chernyi in honor of the Russian individualist anarchist of the same name while Feral Faun has quoted Italian individualist anarchist Renzo Novatore and has translated both Novatore. and the young Italian individualist anarchist Bruno Filippi
Kevin Carson is a contemporary mutualist economist and author of "Studies in Mutualist Political Economy". Another important current mutualist is Joe Peacott. Contemporary mutualists are among those involved in the Alliance of the Libertarian Left and in the Voluntary Cooperation Movement. A recent mutualist collective was the Boston Anarchist Drinking Brigade.
Egoism has had a strong influence on insurrectionary anarchism, as can be seen in the work of Wolfi Landstreicher. Feral Faun wrote in 1995 that:
In the game of insurgence—a lived guerilla war game—it is strategically necessary to use identities and roles. Unfortunately, the context of social relationships gives these roles and identities the power to define the individual who attempts to use them. So I, Feral Faun, became ... an anarchist ... a writer ... a Stirner-influenced, post-situationist, anti-civilization theorist ... if not in my own eyes, at least in the eyes of most people who've read my writings.
European individualist anarchism.
European individualist anarchism proceeded from the roots laid by William Godwin, Pierre Joseph Proudhon and Max Stirner. Proudhon was an early pioneer of anarchism as well as of the important individualist anarchist current of mutualism. Stirner became a central figure of individualist anarchism through the publication of his seminal work "The Ego and Its Own" which is considered to be "a founding text in the tradition of individualist anarchism." Another early figure was Anselme Bellegarrigue. IA expanded and diversified through Europe, incorporating influences from North American individualist anarchism.
European individualist anarchists include Albert Libertad, Bellegarrigue, Oscar Wilde, Émile Armand, Lev Chernyi, John Henry Mackay, Han Ryner, Adolf Brand, Miguel Gimenez Igualada, Renzo Novatore, and currently Michel Onfray. Important currents within it include free love, anarcho-naturism, and illegalism.
France.
From the legacy of Proudhon and Stirner there emerged a strong tradition of French individualist anarchism. An early important individualist anarchist was Anselme Bellegarrigue. He participated in the French Revolution of 1848, was author and editor of 'Anarchie, Journal de l'Ordre and Au fait ! Au fait ! Interprétation de l'idée démocratique' and wrote the important early Anarchist Manifesto in 1850. Catalan historian of individualist anarchism Xavier Diez reports that during his travels in the United States "he at least contacted (Henry David) Thoreau and, probably (Josiah) Warren." "Autonomie Individuelle" was an individualist anarchist publication that ran from 1887 to 1888. It was edited by Jean-Baptiste Louiche, Charles Schæffer and Georges Deherme.
Later, this tradition continued with such intellectuals as Albert Libertad, André Lorulot, Émile Armand, Victor Serge, Zo d'Axa and Rirette Maitrejean, who developed theory in the main individualist anarchist journal in France, "L'Anarchie" in 1905. Outside this journal, Han Ryner wrote "Petit Manuel individualiste" (1903). Later appeared the journal L'EnDehors created by Zo d'Axa in 1891.
 French individualist anarchists exposed a diversity of positions (per example, about violence and non-violence). For example, Émile Armand rejected violence and embraced mutualism while becoming an important propagandist for free love, while Albert Libertad and Zo d'Axa were influential in violentists circles and championed violent propaganda by the deed while adhering to communitarianism or anarcho-communism and rejecting work. Han Ryner on the other side conciled anarchism with stoicism. Nevertheless French individualist circles had a strong sense of personal libertarianism and experimentation. Naturism and free love contents started to have a strong influence in individualist anarchist circles and from there it expanded to the rest of anarchism also appearing in Spanish individualist anarchist groups. "Along with feverish activity against the social order, (Albert) Libertad was usually also organizing feasts, dances and country excursions, in consequence of his vision of anarchism as the “joy of living” and not as militant sacrifice and death instinct, seeking to reconcile the requirements of the individual (in his need for autonomy) with the need to destroy authoritarian society."
Anarchist naturism was promoted by Henri Zisly, Emile Gravelle and Georges Butaud. Butaud was an individualist "partisan of the "milieux libres", publisher of "Flambeau" ("an enemy of authority") in 1901 in Vienna. Most of his energies were devoted to creating anarchist colonies (communautés expérimentales) in which he participated in several.
"In this sense, the theoretical positions and the vital experiences of french individualism are deeply iconoclastic and scandalous, even within libertarian circles. The call of nudist naturism, the strong defence of bith control methods, the idea of "unions of egoists" with the sole justification of sexual practices, that will try to put in practice, not without difficulties, will establish a way of thought and action, and will result in sympathy within some, and a strong rejection within others."
French individualist anarchists grouped behind Émile Armand, published "L'Unique" after World War II. "L'Unique" went from 1945 to 1956 with a total of 110 numbers. Gérard de Lacaze-Duthiers was a French writer, art critic, pacifist and anarchist. Lacaze-Duthiers, an art critic for the Symbolist review journal "La Plume", was influenced by Oscar Wilde, Nietzsche and Max Stirner. His (1906) "L'Ideal Humain de l'Art" helped found the 'Artistocracy' movement—a movement advocating life in the service of art. His ideal was an anti-elitist aestheticism: "All men should be artists". Together with André Colomer and Manuel Devaldes, he founded "L'Action d'Art", an anarchist literary journal, in 1913. After World War II he contributed to the journal "L'Unique". Within the synthesist anarchist organization, the Fédération Anarchiste, there existed an individualist anarchist tendency alongside anarcho-communist and anarchosyndicalist currents. Individualist anarchists participating inside the Fédération Anarchiste included Charles-Auguste Bontemps, Georges Vincey and André Arru. The new base principles of the francophone Anarchist Federation were written by the individualist anarchist Charles-Auguste Bontemps and the anarcho-communist Maurice Joyeux which established an organization with a plurality of tendencies and autonomy of federated groups organized around synthesist principles. Charles-Auguste Bontemps was a prolific author mainly in the anarchist, freethinking, pacifist and naturist press of the time. His view on anarchism was based around his concept of "Social Individualism" on which he wrote extensively. He defended an anarchist perspective which consisted on "a collectivism of things and an individualism of persons."
In 2002, an anarchist, Libertad organized a new version of the "L'EnDehors", collaborating with "Green Anarchy" and including several contributors, such as Lawrence Jarach, Patrick Mignard, Thierry Lodé, Ron Sakolsky, and Thomas Slut. Numerous articles about capitalism, human rights, free love and social fights were published. "The EnDehors" continues now as a website, EnDehors.org.
The prolific contemporary French philosopher Michel Onfray has been writing from an individualist anarchist perspective influenced by Nietzsche, French post-structuralists thinkers such as Michel Foucault and Gilles Deleuze; and Greek classical schools of philosophy such as the Cynics and Cyrenaics. Among the books which best expose Onfray's individualist anarchist perspective include "La sculpture de soi : la morale esthétique" (The sculpture of oneself: aesthetic morality), "La philosophie féroce : exercices anarchistes", "La puissance d'exister" and "Physiologie de Georges Palante, portrait d'un nietzchéen de gauche" which focuses on French individualist philosopher Georges Palante.
Illegalism.
Illegalism is an anarchist philosophy that developed primarily in France, Italy, Belgium, and Switzerland during the early 1900s as an outgrowth of Stirner's individualist anarchism. Illegalists usually did not seek moral basis for their actions, recognizing only the reality of "might" rather than "right"; for the most part, illegal acts were done simply to satisfy personal desires, not for some greater ideal, although some committed crimes as a form of propaganda of the deed. The illegalists embraced direct action and propaganda of the deed.
Influenced by theorist Max Stirner's egoism as well as Proudhon (his view that Property is theft!), Clément Duval and Marius Jacob proposed the theory of la "reprise individuelle" (Eng: individual reclamation) which justified robbery on the rich and personal direct action against exploiters and the system.,
Illegalism first rose to prominence among a generation of Europeans inspired by the unrest of the 1890s, during which Ravachol, Émile Henry, Auguste Vaillant, and Caserio committed daring crimes in the name of anarchism, in what is known as propaganda of the deed. France's Bonnot Gang was the most famous group to embrace illegalism.
Italy.
In Italy individualist anarchism had a strong tendency towards illegalism and violent propaganda by the deed similar to French individualist anarchism but perhaps more extreme which emphazised criticism of organization be it anarchist or of other type. In this respect we can consider notorious magnicides carried out or attempted by individualists Giovanni Passannante, Sante Caserio, Michele Angiolillo, Luigi Luccheni, Gaetano Bresci who murdered king Umberto I. Caserio lived in France and coexisted within French illegalism and later assassinated French president Sadi Carnot. The theoretical seeds of current Insurrectionary anarchism were already laid out at the end of 19th century Italy in a combination of individualist anarchism criticism of permanent groups and organization with a socialist class struggle worldview. During the rise of fascism this thought also motivated Gino Lucetti, Michele Schirru and Angelo Sbardellotto in attempting the assassination of Benito Mussolini.
During the early 20th century, the intellectual work of individualist anarchist Renzo Novatore came to importance; he was influenced by Stirner, Friedrich Nietzsche, Georges Palante, Oscar Wilde, Henrik Ibsen, Arthur Schopenhauer and Charles Baudelaire. He collaborated in numerous anarchist journals and participated in futurism avant-garde currents. In his thought, he adhered to Stirnerist disrespect for private property, only recognizing property of one's own spirit. Novatore collaborated in the individualist anarchist journal "Iconoclasta!" alongside the young stirnerist illegalist Bruno Filippi
The individualist philosopher and poet Renzo Novatore belonged to the leftist section of the avant-garde movement of Futurism alongside other individualist anarcho-futurists such as Dante Carnesecchi, Leda Rafanelli, Auro d'Arcola, and Giovanni Governato. Also there was Pietro Bruzzi who published the journal "L'Individualista" in the 1920s alongside Ugo Fedeli and Francesco Ghezzi but who fell to fascist forces later. Pietro Bruzzi also collaborated with the Italian American individualist anarchist publication "Eresia" of New York City edited by Enrico Arrigoni.
In Italy in 1945, during the Founding Congress of the Italian Anarchist Federation, there was a group of individualist anarchists led by Cesare Zaccaria who was an important anarchist of the time. Later, during the IX Congress of the Italian Anarchist Federation in Carrara in 1965, a group decided to split off from this organization and created the "Gruppi di Iniziativa Anarchica". In the seventies, it was mostly composed of "veteran individualist anarchists with an of pacifism orientation, naturism, etc. ..."
In the famous Italian insurrectionary anarchist essay written by an anonymous writer, "At Daggers Drawn with the Existent, its Defenders and its False Critics", there reads "The workers who, during a wildcat strike, carried a banner saying, 'We are not asking for anything' understood that the defeat is in the claim itself ('the claim against the enemy is eternal'). There is no alternative but to take everything. As Stirner said: 'No matter how much you give them, they will always ask for more, because what they want is no less than the end of every concession'." The contemporary imprisoned Italian insurrectionary anarchist philosopher writes from an explicit individualist anarchist perspective in such essays as "Critica individualista anarchica alla modernità" (Individualist anarchist critique of modernity) Horst Fantazzini (March 4, 1939 Altenkessel, Saarland, West Germany–December 24, 2001, Bologna, Italy), was an Italian-German individualist anarchist who pursued an illegalist lifestyle and practice until his death in 2001. He gained media notoriety mainly due to his many bank robberies through Italy and other countries. In 1999 the film "Ormai è fatta!" appeared based on his life.
Spain.
While Spain was influenced by American individualist anarchism, it was more closely related to the French currents. Around the start of the 20th century, individualism in Spain gathered force through the efforts of people such as Dorado Montero, Ricardo Mella, Federico Urales, Miguel Gimenez Igualada, Mariano Gallardo, and J. Elizalde who translated French and American individualists. Important in this respect were also magazines such as "La Idea Libre", "La revista blanca", "Etica", "Iniciales", "Al margen", "Estudios" and "Nosotros". The most influential thinkers there were Max Stirner, Émile Armand and Han Ryner. Just as in France, the spread of Esperanto and anationalism had importance just as naturism and free love currents. Later, Armand and Ryner themselves started writing in the Spanish individualist press. Armand's concept of amorous camaraderie had an important role in motivating polyamory as realization of the individual.
Catalan historian Xavier Diez reports that the Spanish individualist anarchist press was widely read by members of anarcho-communist groups and by members of the anarcho-syndicalist trade union CNT. There were also the cases of prominent individualist anarchists such as Federico Urales and Miguel Gimenez Igualada who were members of the CNT and J. Elizalde who was a founding member and first secretary of the Iberian Anarchist Federation.
Spanish individualist anarchist Miguel Giménez Igualada wrote the lengthy theory book called "Anarchism" espousing his individualist anarchism. Between October 1937 and February 1938 he was editor of the individualist anarchist magazine "Nosotros", in which many works of Han Ryner and Émile Armand appeared. He also participated in the publishing of another individualist anarchist maganize "Al Margen: Publicación quincenal individualista". In his youth he engaged in illegalist activities. His thought was deeply influenced by Max Stirner, of which he was the main popularizer in Spain through his own writings. He published and wrote the preface to the fourth edition in Spanish of "The Ego and Its Own" from 1900. He proposed the creation of a "Union of Egoists", to be a Federation of Individualist Anarchists in Spain, but did not succeed. In 1956 he published an extensive treatise on Stirner, dedicated to fellow individualist anarchist Émile Armand Afterwards he traveled and lived in Argentina, Uruguay and Mexico.
Federico Urales was an important individualist anarchist who edited "La Revista Blanca". The individualist anarchism of Urales was influenced by Auguste Comte and Charles Darwin. He saw science and reason as a defense against blind servitude to authority. He was critical of influential individualist thinkers such as Nietzsche and Stirner for promoting an asocial egoist individualism and instead promoted an individualism with solidarity seen as a way to guarantee social equality and harmony. He was highly critical of anarcho-syndicalism, which he viewed as plagued by excessive bureaucracy, and thought that it tended towards reformism. Instead he favored small groups based on ideological alignement. He supported and participated in the establishment of the Iberian Anarchist Federation (FAI) in 1927.
In 1956, on exile escaping from Franco´s dictatorship Miguel Giménez Igualada published an extensive treatise on Stirner which he dedicated to fellow individualist anarchist Émile Armand On the subject of individualist anarchist theory, he publisheds "Anarchism" in 1968, during his exile in Mexico from Franco's dictatorship in Spain. He was present in the First Congress of the Mexican Anarchist Federation in 1945.
In 2000, in Spain Ateneo Libertario Ricardo Mella, Ateneo libertario Al Margen, Ateneu Enciclopèdic Popular, Ateneo Libertario de Sant Boi, Ateneu Llibertari Poble Sec y Fundació D'Estudis Llibertaris i Anarcosindicalistes republished Émile Armand's writings on Free Love and individualist anarchism in a compilation titled "Individualist anarchism and Amorous camaraderie". Recently Catalan historian Xavier Diez has dedicated extensive research on Spanish individualist anarchism as can be seen in his books "El anarquismo individualista en España: 1923–1938" and "Utopia sexual a la premsa anarquista de Catalunya. La revista Ética-Iniciales(1927–1937)" (which deals with free love thought as present in the Spanish individualist anarchist magazine "Iniciales").
Germany.
In Germany, the Scottish-German John Henry McKay became the most important propagandist for individualist anarchist ideas. He fused Stirnerist egoism with the positions of Benjamin Tucker and actually translated Tucker into German. Two semi-fictional writings of his own, "Die Anarchisten" and "Der Freiheitsucher", contributed to individualist theory through an updating of egoist themes within a consideration of the anarchist movement. English translations of these works arrived in the United Kingdom and in individualist American circles led by Tucker. McKay is also known as an important European early activist for Gay rights.
Using the pseudonym Sagitta, Mackay wrote a series of works for pederastic emancipation, titled "Die Buecher der namenlosen Liebe" ("Books of the Nameless Love"). This series was conceived in 1905 and completed in 1913 and included the "Fenny Skaller", a story of a pederast. Under the same pseudonym he also published fiction, such as "Holland" (1924) and a pederastic novel of the Berlin boy-bars, "Der Puppenjunge" ("The Hustler") (1926).
 Adolf Brand (1874–1945) was a German writer, stirnerist anarchist and pioneering campaigner for the acceptance of male bisexuality and homosexuality. Brand published a German homosexual periodical, "Der Eigene" in 1896. This was the first ongoing homosexual publication in the world. The name was taken from writings of egoist philosopher Max Stirner, who had greatly influenced the young Brand, and refers to Stirner's concept of "self-ownership" of the individual. "Der Eigene" concentrated on cultural and scholarly material, and may have had an average of around 1500 subscribers per issue during its lifetime, although the exact numbers are uncertain. Contributors included Erich Mühsam, Kurt Hiller, John Henry Mackay (under the pseudonym Sagitta) and artists Wilhelm von Gloeden, Fidus and Sascha Schneider. Brand contributed many poems and articles himself. Benjamin Tucker followed this journal from the United States.
"Der Einzige" was a German individualist anarchist magazine. It appeared in 1919, as a weekly, then sporadically until 1925 and was edited by cousins Anselm Ruest (pseud. for Ernst Samuel) and Mynona (pseud. for Salomo Friedlaender). Its title was adopted from the book "Der Einzige und sein Eigentum" (engl. trans. "The Ego and Its Own") by Max Stirner. Another influence was the thought of German philosopher Friedrich Nietzsche. The publication was connected to the local expressionist artistic current and the transition from it towards dada.
United Kingdom and Ireland.
The English enlightenment political theorist William Godwin was an important influence as mentioned before. The Irish anarchist writer of the Decadent movement Oscar Wilde influenced individualist anarchists such as Renzo Novatore and gained the admiration of Benjamin Tucker. In his important essay "The Soul of Man under Socialism" from 1891 Wilde defended socialism as the way to guarantee individualism and so he saw that "With the abolition of private property, then, we shall have true, beautiful, healthy Individualism. Nobody will waste his life in accumulating things, and the symbols for things. One will live. To live is the rarest thing in the world. Most people exist, that is all." For anarchist historian George Woodcock "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist ... for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated ... Wilde represents the anarchist as aesthete." Woodcock finds that "The most ambitious contribution to literary anarchism during the 1890s was undoubtedly Oscar Wilde "The Soul of Man under Socialism"" and finds that it is influenced mainly by the thought of William Godwin.
In the late 19th century in the United Kingdom, there existed individualist anarchists such as Wordsworth Donisthorpe, Joseph Hiam Levy, Joseph Greevz Fisher, John Badcock, Jr., Albert Tarn, and Henry Albert Seymour who were close to the United States group around Benjamin Tucker´s magazine "Liberty". In the mid-1880s Seymour published a journal called "The Anarchist". and also later took a special interest in free love as he participated in the journal "The Adult: A Journal for the Advancement of Freedom in Sexual Relationships". ""The Serpent", issued from London ... the most prominent English-language egoist journal, was published from 1898 to 1900 with the subtitle 'A Journal of Egoistic Philosophy and Sociology'". Henry Meulen was another British anarchist, he was notable for his support of free banking.
In the United Kingdom, Herbert Read was influenced highly by egoism as he later approached existentialism (see existentialist anarchism). Albert Camus devoted a section of "The Rebel" to Stirner. "Although throughout his book Camus is concerned to present "the rebel" as a preferred alternative to "the revolutionary" he nowhere acknowledges that this distinction is taken from the one that Stirner makes between "the revolutionary" and "the insurrectionist". Sidney Parker is a British egoist individualist anarchist who wrote articles and edited anarchist journals from 1963 to 1993 such as "Minus One", "Egoist", and "Ego". Donald Rooum is an English anarchist cartoonist and writer with a long association with Freedom Press. Rooum stated that for his thought "The most influential source is Max Stirner. I am happy to be called a Stirnerite anarchist, provided 'Stirnerite' means one who agrees with Stirner's general drift, not one who agrees with Stirner's every word." "An Anarchist FAQ" reports that "From meeting anarchists in Glasgow during the Second World War, long-time anarchist activist and artist Donald Rooum likewise combined Stirner and anarcho-communism." 
In the hybrid of post-structuralism and anarchism called post-anarchism the British Saul Newman has written a lot on Stirner and his similarities to post-structuralism. He writes:
Max Stirner's impact on contemporary political theory is often neglected. However in Stirner's political thinking there can be found a surprising convergence with poststructuralist theory, particularly with regard to the function of power. Andrew Koch, for instance, sees Stirner as a thinker who transcends the Hegelian tradition he is usually placed in, arguing that his work is a precursor poststructuralist ideas about the foundations of knowledge and truth.
Newman has published several essays on Stirner. "War on the State: Stirner and Deleuze's Anarchism" and "Empiricism, pluralism, and politics in Deleuze and Stirner" discusses what he sees are similarities between Stirner's thought and that of Gilles Deleuze. In "Spectres of Stirner: a Contemporary Critique of Ideology" he discusses the conception of ideology in Stirner. In "Stirner and Foucault: Toward a Post-Kantian Freedom" similarities between Stirner and Michel Foucault. Also he wrote "Politics of the ego: Stirner's critique of liberalism".
Russia.
Individualist anarchism was one of the three categories of anarchism in Russia, along with the more prominent anarchist communism and anarcho-syndicalism. The ranks of the Russian individualist anarchists were predominantly drawn from the intelligentsia and the working class. For anarchist historian Paul Avrich "The two leading exponents of individualist anarchism, both based in Moscow, were Aleksei Alekseevich Borovoi and Lev Chernyi (Pavel Dmitrievich Turchaninov). From Nietzsche, they inherited the desire for a complete overturn of all values accepted by bourgeois societypolitical, moral, and cultural. Furthermore, strongly influenced by Max Stirner and Benjamin Tucker, the German and American theorists of individualist anarchism, they demanded the total liberation of the human personality from the fetters of organized society."
Some Russian individualists anarchists "found the ultimate expression of their social alienation in violence and crime, others attached themselves to avant-garde literary and artistic circles, but the majority remained "philosophical" anarchists who conducted animated parlor discussions and elaborated their individualist theories in ponderous journals and books."
Lev Chernyi was an important individualist anarchist involved in resistance against the rise to power of the Bolshevik Party. He adhered mainly to Stirner and the ideas of Benjamin Tucker. In 1907, he published a book entitled "Associational Anarchism", in which he advocated the "free association of independent individuals.". On his return from Siberia in 1917 he enjoyed great popularity among Moscow workers as a lecturer. Chernyi was also Secretary of the Moscow Federation of Anarchist Groups, which was formed in March 1917. He was an advocate "for the seizure of private homes", which was an activity seen by the anarchists after the October revolution as direct expropriation on the bourgoise. He died after being accused of participation in an episode in which this group bombed the headquarters of the Moscow Committee of the Communist Party. Although most likely not being really involved in the bombing, he might have died of torture.
Chernyi advocated a Nietzschean overthrow of the values of bourgeois Russian society, and rejected the voluntary communes of anarcho-communist Peter Kropotkin as a threat to the freedom of the individual. Scholars including Avrich and Allan Antliff have interpreted this vision of society to have been greatly influenced by the individualist anarchists Max Stirner, and Benjamin Tucker. Subsequent to the book's publication, Chernyi was imprisoned in Siberia under the Russian Czarist regime for his revolutionary activities.
On the other hand Aleksei Borovoi (1876?–1936), was a professor of philosophy at Moscow University, "a gifted orator and the author of numerous books, pamphlets, and articles which attempted to reconcile individualist anarchism with the doctrines of syndicallism". He wrote among other theoretical works, "Anarkhizm" in 1918 just after the October revolution and "Anarchism and Law". For him "the chief importance is given not to Anarchism as the aim but to Anarchy as the continuous quest for the aim". He manifests there that "No social ideal, from the point of view of anarchism, could be referred to as absolute in a sense that supposes it’s the crown of human wisdom, the end of social and ethical quest of man."
Latin American individualist anarchism.
Argentine anarchist historian Angel Cappelletti reports that in Argentina "Among the workers that came from Europe in the 2 first decades of the century, there was curiously some stirnerian individualists influenced by the philosophy of Nietzsche, that saw syndicalism as a potential enemy of anarchist ideology. They established ... affinity groups that in 1912 came to, according to Max Nettlau, to the number of 20. In 1911 there appeared, in Colón, the periodical "El Único", that defined itself as ‘Publicación individualista’".
Vicente Rojas Lizcano, whose pseudonym was Biófilo Panclasta, was a Colombian individualist anarchist writer and activist. In 1904 he began using the name Biofilo Panclasta. "Biofilo" in Spanish stands for "lover of life" and "Panclasta" for "enemy of all". He visited more than fifty countries propagandizing for anarchism which in his case was highly influenced by the thought of Max Stirner and Friedrich Nietszche. Among his written works there are "Siete años enterrado vivo en una de las mazmorras de Gomezuela: Horripilante relato de un resucitado"(1932) and "Mis prisiones, mis destierros y mi vida" (1929) which talk about his many adventures while living his life as an adventurer, activist and vagabond, as well as his thought and the many times he was imprisoned in different countries.
Maria Lacerda de Moura was a Brazilian teacher, journalist, anarcha-feminist, and individualist anarchist. Her ideas regarding education were largely influenced by Francisco Ferrer. She later moved to São Paulo and became involved in journalism for the anarchist and labor press. There she also lectured on topics including education, women's rights, free love, and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. In February 1923, she launched "Renascença", a periodical linked with the anarchist, progressive, and freethinking circles of the period. Her thought was mainly influenced by individualist anarchists such as Han Ryner and Émile Armand. She maintained contact with Spanish individualist anarchist circles.
Horst Matthai Quelle was a Spanish language German anarchist philosopher influenced by Max Stirner. In 1938, at the beginning of the German economic crisis and the rise of Nazism and fascism in Europe, Quelle moved to Mexico. Quelle earned his undergraduate degree, master's and doctorate in philosophy at the National Autonomous University of Mexico, where he returned as a professor of philosophy in the 1980s. He argued that since the individual gives form to the world, he is those objects, the others and the whole universe. One of his main views was a "theory of infinite worlds" which for him was developed by pre-socratic philosophers.
During the 1990s in Argentina, there appeared a stirnerist publication called "El Único: publicacion periódica de pensamiento individualista".
Criticisms.
Philosopher Murray Bookchin criticized individualist anarchism for its opposition to democracy and its embrace of "lifestylism" at the expense of class struggle. Bookchin claimed that individualist anarchism supports only negative liberty and rejects the idea of positive liberty. Philosopher Albert Meltzer proposed that individualist anarchism differs radically from revolutionary anarchism, and that it "is sometimes too readily conceded 'that this is, after all, anarchism'." He claimed that Benjamin Tucker's acceptance of the use of a private police force (including to break up violent strikes to protect the "employer's 'freedom'") is contradictory to the definition of anarchism as "no government."
Philosopher George Bernard Shaw initially had flirtations with individualist anarchism before coming to the conclusion that it was "the negation of socialism, and is, in fact, unsocialism carried as near to its logical conclusion as any sane man dare carry it." Shaw's argument was that even if wealth was initially distributed equally, the degree of "laissez-faire" advocated by Tucker would result in the distribution of wealth becoming unequal because it would permit private appropriation and accumulation. According to academic Carlotta Anderson, American individualist anarchists accept that free competition results in unequal wealth distribution, but they "do not see that as an injustice." Tucker explained, "If I go through life free and rich, I shall not cry because my neighbor, equally free, is richer. Liberty will ultimately make all men rich; it will not make all men equally rich. Authority may (and may not) make all men equally rich in purse; it certainly will make them equally poor in all that makes life best worth living."
Footnotes.
α^ The term "individualist anarchism" is often used as a classificatory term, but in very different ways. Some sources, such as An Anarchist FAQ use the classification "social anarchism / individualist anarchism". Some see individualist anarchism as distinctly non-socialist, and use the classification "socialist anarchism / individualist anarchism" accordingly. Other classifications include "mutualist/communal" anarchism.
β^ Michael Freeden identifies four broad types of individualist anarchism. He says the first is the type associated with William Godwin that advocates self-government with a "progressive rationalism that included benevolence to others." The second type is the amoral self-serving rationality of Egoism, as most associated with Max Stirner. The third type is "found in Herbert Spencer's early predictions, and in that of some of his disciples such as Donisthorpe, foreseeing the redundancy of the state in the source of social evolution." The fourth type retains a moderated form of egoism and accounts for social cooperation through the advocacy of market relationships.
γ^ See, for example, the Winter 2006 issue of the "Journal of Libertarian Studies", dedicated to reviews of Kevin Carson's "Studies in Mutualist Political Economy." Mutualists compose one bloc, along with agorists and geo-libertarians, in the recently formed Alliance of the Libertarian Left.
δ^ Though this term is non-standard usage—by "left"—agorists mean "left" in the general sense used by left-libertarians, as defined by Roderick T. Long, as "... an integration, or I'd argue, a reintegration of libertarianism with concerns that are traditionally thought of as being concerns of the left. That includes concerns for worker empowerment, worry about plutocracy, concerns about feminism and various kinds of social equality."
ε^ Konkin wrote the article "Copywrongs" in opposition to the concept and Schulman countered SEK3's arguments in 
ζ^ Individualist anarchism is also known by the terms "anarchist individualism", "anarcho-individualism", "individualistic anarchism", "libertarian anarchism", "anarcho-libertarianism", "anarchist libertarianism" and "anarchistic libertarianism".

</doc>
<doc id="14937" url="http://en.wikipedia.org/wiki?curid=14937" title="Italo Calvino">
Italo Calvino

Italo Calvino (; ]; 15 October 1923 – 19 September 1985) was an Italian journalist and writer of short stories and novels. His best known works include the "Our Ancestors" trilogy (1952–1959), the "Cosmicomics" collection of short stories (1965), and the novels "Invisible Cities" (1972) and "If on a winter's night a traveler" (1979).
Lionised in Britain and the United States, he was the most-translated contemporary Italian writer at the time of his death, and a noted contender for the Nobel Prize for Literature.
Biography.
Parents.
Italo Calvino was born in Santiago de Las Vegas, a suburb of Havana, Cuba in 1923. His father, Mario, was a tropical agronomist and botanist who also taught agriculture and floriculture. Born 47 years earlier in San Remo, Italy, Mario Calvino had emigrated to Mexico in 1909 where he took up an important position with the Ministry of Agriculture. In an autobiographical essay, Italo Calvino explained that his father "had been in his youth an anarchist, a follower of Kropotkin and then a Socialist Reformist". In 1917, Mario left for Cuba to conduct scientific experiments, after living through the Mexican Revolution.
Calvino's mother, Eva Mameli, was a botanist and university professor. A native of Sassari in Sardinia and 11 years younger than her husband, she married while still a junior lecturer at Pavia University. Born into a secular family, Eva was a pacifist educated in the "religion of civic duty and science". Calvino described his parents as being "very different in personality from one another", suggesting perhaps deeper tensions behind a comfortable, albeit strict, middle-class upbringing devoid of conflict. As an adolescent, he found it hard relating to poverty and the working-class, and was "ill at ease" with his parents' openness to the laborers who filed into his father's study on Saturdays to receive their weekly paycheck.
Early life and education.
In 1925, less than two years after Calvino's birth, the family returned to Italy and settled permanently in San Remo on the Ligurian coast. Calvino's brother Floriano, who became a distinguished geologist, was born in 1927.
The family divided their time between the Villa Meridiana, an experimental floriculture station which also served as their home, and Mario's ancestral land at San Giovanni Battista. On this small working farm set in the hills behind San Remo, Mario pioneered in the cultivation of then exotic fruits such as avocado and grapefruit, eventually obtaining an entry in the "Dizionario biografico degli italiani" for his achievements. The vast forests and luxuriant fauna omnipresent in Calvino's early fiction such as "The Baron in the Trees" derives from this "legacy". In an interview, Calvino stated that "San Remo continues to pop out in my books, in the most diverse pieces of writing." He and Floriano would climb the tree-rich estate and perch for hours on the branches reading their favorite adventure stories. Less salubrious aspects of this "paternal legacy" are described in "The Road to San Giovanni", Calvino's memoir of his father in which he exposes their inability to communicate: "Talking to each other was difficult. Both verbose by nature, possessed of an ocean of words, in each other's presence we became mute, would walk in silence side by side along the road to San Giovanni." A fan of Rudyard Kipling's "The Jungle Book" as a child, Calvino felt that his early interest in stories made him the "black sheep" of a family that held literature in less esteem than the sciences. Fascinated by American movies and cartoons, he was equally attracted to drawing, poetry, and theatre. On a darker note, Calvino recalled that his earliest memory was of a socialist professor brutalized by Fascist lynch-squads. "I remember clearly that we were at dinner when the old professor came in with his face beaten up and bleeding, his bowtie all torn, asking for help."
Other legacies include the parents' masonic republicanism which occasionally developed into anarchic socialism. Austere, anti-Fascist freethinkers, Eva and Mario refused to give their sons any religious education. Italo attended the English nursery school St George's College, followed by a Protestant elementary private school run by Waldensians. His secondary schooling was completed at the state-run Liceo Gian Domenico Cassini where, at his parents' request, he was exempted from religious instruction but forced to justify his anticonformist stance. In his mature years, Calvino described the experience as a salutary one as it made him "tolerant of others' opinions, particularly in the field of religion, remembering how irksome it was to hear myself mocked because I did not follow the majority's beliefs". During this time, he met a brilliant student from Rome, Eugenio Scalfari, who went on to found the weekly magazine "L'Espresso" and "La Repubblica", a major Italian newspaper. The two teenagers formed a lasting friendship, Calvino attributing his political awakening to their university discussions. Seated together "on a huge flat stone in the middle of a stream near our land", he and Scalfari founded the MUL (University Liberal Movement).
Eva managed to delay her son's enrolment in the Fascist armed scouts, the "Balilla Moschettieri", and then arranged that he be excused, as a non-Catholic, from performing devotional acts in church. But later on, as a compulsory member, he could not avoid the assemblies and parades of the "Avanguardisti", and was forced to participate in the Italian occupation of the French Riviera in June 1940.
World War II.
In 1941, Calvino enrolled at the University of Turin, choosing the Agriculture Faculty where his father had previously taught courses in agronomy. Concealing his literary ambitions to please his family, he passed four exams in his first year while reading anti-Fascist works by Elio Vittorini, Eugenio Montale, Cesare Pavese, Johan Huizinga, and Pisacane, and works by Max Planck, Werner Heisenberg, and Albert Einstein on physics. Disdainful of Turin students, Calvino saw himself as enclosed in a "provincial shell" that offered the illusion of immunity from the Fascist nightmare: "We were ‘hard guys’ from the provinces, hunters, snooker-players, show-offs, proud of our lack of intellectual sophistication, contemptuous of any patriotic or military rhetoric, coarse in our speech, regulars in the brothels, dismissive of any romantic sentiment and desperately devoid of women."
Calvino transferred to the University of Florence in 1943 and reluctantly passed three more exams in agriculture. By the end of the year, the Germans had succeeded in occupying Liguria and setting up Benito Mussolini's puppet Republic of Salò in northern Italy. Now twenty years old, Calvino refused military service and went into hiding. Reading intensely in a wide array of subjects, he also reasoned politically that, of all the partisan groupings, the communists were the best organized with "the most convincing political line".
In spring 1944, Eva encouraged her sons to enter the Italian Resistance in the name of "natural justice and family virtues". Using the battlename of "Santiago", Calvino joined the "Garibaldi Brigades", a clandestine Communist group and, for twenty months, endured the fighting in the Maritime Alps until 1945 and the Liberation. As a result of his refusal to be a conscript, his parents were held hostage by the Nazis for an extended period at the Villa Meridiana. Calvino wrote of his mother's ordeal that "she was an example of tenacity and courage… behaving with dignity and firmness before the SS and the Fascist militia, and in her long detention as a hostage, not least when the blackshirts three times pretended to shoot my father in front of her eyes. The historical events which mothers take part in acquire the greatness and invincibility of natural phenomena".
Turin and communism.
Calvino settled in Turin in 1945, after a long hesitation over living there or in Milan. He often humorously belittled this choice, describing Turin as a "city that is serious but sad". Returning to university, he abandoned Agriculture for the Arts Faculty. A year later, he was initiated into the literary world by Elio Vittorini, who published his short story "Andato al comando" (1945; "Gone to Headquarters") in "Il Politecnico", a Turin-based weekly magazine associated with the university. The horror of the war had not only provided the raw material for his literary ambitions but deepened his commitment to the Communist cause. Viewing civilian life as a continuation of the partisan struggle, he confirmed his membership of the Italian Communist Party. On reading Vladimir Lenin's "State and Revolution", he plunged into post-war political life, associating himself chiefly with the worker's movement in Turin.
In 1947, he graduated with a Master's thesis on Joseph Conrad, wrote short stories in his spare time, and landed a job in the publicity department at the Einaudi publishing house run by Giulio Einaudi. Although brief, his stint put him in regular contact with Cesare Pavese, Natalia Ginzburg, Norberto Bobbio, and many other left-wing intellectuals and writers. He then left Einaudi to work as a journalist for the official Communist daily, "L'Unità", and the newborn Communist political magazine, "Rinascita". During this period, Pavese and poet Alfonso Gatto were Calvino's closest friends and mentors.
His first novel, "Il sentiero dei nidi di ragno" ("The Path to the Nest of Spiders") written with valuable editorial advice from Pavese, won the Premio Riccione on publication in 1947. With sales topping 5000 copies, a surprise success in postwar Italy, the novel inaugurated Calvino's neorealist period. In a clairvoyant essay, Pavese praised the young writer as a "squirrel of the pen" who "climbed into the trees, more for fun than fear, to observe partisan life as a fable of the forest". In 1948, he interviewed one of his literary idols, Ernest Hemingway, travelling with Natalia Ginzburg to his home in Stresa.
"Ultimo viene il corvo" ("The Crow Comes Last"), a collection of stories based on his wartime experiences, was published to acclaim in 1949. Despite the triumph, Calvino grew increasingly worried by his inability to compose a worthy second novel. He returned to Einaudi in 1950, responsible this time for the literary volumes. He eventually became a consulting editor, a position that allowed him to hone his writing talent, discover new writers, and develop into "a reader of texts". In late 1951, presumably to advance in the Communist Party, he spent two months in the Soviet Union as correspondent for "l'Unità". While in Moscow, he learned of his father's death on 25 October. The articles and correspondence he produced from this visit were published in 1952, winning the Saint-Vincent Prize for journalism.
Over a seven-year period, Calvino wrote three realist novels, "The White Schooner" (1947–1949), "Youth in Turin" (1950–1951), and "The Queen's Necklace" (1952–54), but all were deemed defective. During the eighteen months it took to complete "I giovani del Po" ("Youth in Turin"), he made an important self-discovery: "I began doing what came most naturally to me – that is, following the memory of the things I had loved best since boyhood. Instead of making myself write the book I "ought" to write, the novel that was expected of me, I conjured up the book I myself would have liked to read, the sort by an unknown writer, from another age and another country, discovered in an attic." The result was "Il visconte dimezzato" (1952; "The Cloven Viscount") composed in 30 days between July and September 1951. The protagonist, a seventeenth century viscount sundered in two by a cannonball, incarnated Calvino's growing political doubts and the divisive turbulence of the Cold War. Skillfully interweaving elements of the fable and the fantasy genres, the allegorical novel launched him as a modern "fabulist". In 1954, Giulio Einaudi commissioned his "Fiabe Italiane" (1956; "Italian Folktales") on the basis of the question, "Is there an Italian equivalent of the Brothers Grimm?" For two years, Calvino collated tales found in 19th century collections across Italy then translated 200 of the finest from various dialects into Italian. Key works he read at this time were Vladimir Propp's "Morphology of the Folktale" and "Historical Roots of Russian Fairy Tales", stimulating his own ideas on the origin, shape and function of the story.
In 1952 Calvino wrote with Giorgio Bassani for "Botteghe Oscure", a magazine named after the popular name of the party's head-offices in Rome. He also worked for "Il Contemporaneo", a Marxist weekly.
From 1955 to 1958 Calvino had an affair with Italian actress Elsa De Giorgi, a married, older woman. Excerpts of the hundreds of love letters Calvino wrote to her were published in the "Corriere della Sera" in 2004, causing some controversy.
After communism.
In 1957, disillusioned by the 1956 Soviet invasion of Hungary, Calvino left the Italian Communist Party. In his letter of resignation published in "L'Unità" on 7 August, he explained the reason of his dissent (the violent suppression of the Hungarian uprising and the revelation of Joseph Stalin's crimes) while confirming his "confidence in the democratic perspectives" of world Communism. He withdrew from taking an active role in politics and never joined another party. Ostracized by the PCI party leader Palmiro Togliatti and his supporters on publication of "Becalmed in the Antilles" ("La gran bonaccia delle Antille"), a satirical allegory of the party's immobilism, Calvino began writing "The Baron in the Trees". Completed in three months and published in 1957, the fantasy is based on the "problem of the intellectual's political commitment at a time of shattered illusions". He found new outlets for his periodic writings in the journals "Città aperta" and "Tempo presente", the magazine "Passato e presente", and the weekly "Italia Domani". With Vittorini in 1959, he became co-editor of "Il Menabò", a cultural journal devoted to literature in the modern industrial age, a position he held until 1966.
Despite severe restrictions in the US against foreigners holding communist views, Calvino was allowed to visit the United States, where he stayed six months from 1959 to 1960 (four of which he spent in New York), after an invitation by the Ford Foundation. Calvino was particularly impressed by the "New World": "Naturally I visited the South and also California, but I always felt a New Yorker. My city is New York." The letters he wrote to Einaudi describing this visit to the United States were first published as "American Diary 1959–1960" in "Hermit in Paris" in 2003.
In 1962 Calvino met Argentinian translator Esther Judith Singer ("Chichita") and married her in 1964 in Havana, during a trip in which he visited his birthplace and was introduced to Ernesto "Che" Guevara. On 15 October 1967, a few days after Guevara's death, Calvino wrote a tribute to him that was published in Cuba in 1968, and in Italy thirty years later. He and his wife settled in Rome in the via Monte Brianzo where their daughter, Giovanna, was born in 1965. Once again working for Einaudi, Calvino began publishing some of his "Cosmicomics" in "Il Caffè", a literary magazine.
Later life and work.
Vittorini's death in 1966 greatly affected Calvino. He went through what he called an "intellectual depression", which the writer himself described as an important passage in his life: "...I ceased to be young. Perhaps it's a metabolic process, something that comes with age, I'd been young for a long time, perhaps too long, suddenly I felt that I had to begin my old age, yes, old age, perhaps with the hope of prolonging it by beginning it early."
In the fermenting atmosphere that evolved into 1968's cultural revolution (the French May), he moved with his family to Paris in 1967, setting up home in a villa in the Square de Châtillon. Nicknamed "L'ironique amusé", he was invited by Raymond Queneau in 1968 to join the Oulipo ("Ouvroir de littérature potentielle") group of experimental writers where he met Roland Barthes, Georges Perec, and Claude Lévi-Strauss, all of whom influenced his later production. That same year, he turned down the Viareggio Prize for "Ti con zero" ("Time and the Hunter") on the grounds that it was an award given by "institutions emptied of meaning". He accepted, however, both the Asti Prize and the Feltrinelli Prize for his writing in 1970 and 1972, respectively. In two autobiographical essays published in 1962 and 1970, Calvino described himself as "atheist" and his outlook as "non-religious".
<poem>
The catalogue of forms is endless: until every shape has found its city, new cities will continue to be born. When the forms exhaust their variety and come apart, the end of cities begins.</poem>
”
From "Invisible Cities" (1974) 
Calvino had more intense contacts with the academic world, with notable experiences at the Sorbonne (with Barthes) and the University of Urbino. His interests included classical studies: Honoré de Balzac, Ludovico Ariosto, Dante, Ignacio de Loyola, Cervantes, Shakespeare, Cyrano de Bergerac, and Giacomo Leopardi. Between 1972–1973 Calvino published two short stories, "The Name, the Nose" and the Oulipo-inspired "The Burning of the Abominable House" in the Italian edition of Playboy. He became a regular contributor to the important Italian newspaper "Corriere della Sera", spending his summer vacations in a house constructed in Roccamare near Castiglione della Pescaia, Tuscany.
In 1975 Calvino was made Honorary Member of the American Academy. Awarded the Austrian State Prize for European Literature in 1976, he visited Mexico, Japan, and the United States where he gave a series of lectures in several American towns. After his mother died in 1978 at the age of 92, Calvino sold Villa Meridiana, the family home in San Remo. Two years later, he moved to Rome in Piazza Campo Marzio near the Pantheon and began editing the work of Tommaso Landolfi for Rizzoli. Awarded the French Légion d'honneur in 1981, he also accepted to be jury president of the 29th Venice Film Festival.
During the summer of 1985, Calvino prepared a series of texts on literature for the Charles Eliot Norton Lectures to be delivered at Harvard University in the fall. On 6 September, he was admitted to the ancient hospital of Santa Maria della Scala in Siena where he died during the night between 18 and 19 September of a cerebral hemorrhage. His lecture notes were published posthumously in Italian in 1988 and in English as "Six Memos for the Next Millennium" in 1993.
Selected bibliography.
A selected bibliography of Calvino's writings, listing the works that have been translated into and published in English, and a few major untranslated works. More exhaustive bibliographies can be found in Martin McLaughlin's "Italo Calvino", and Beno Weiss's "Understanding Italo Calvino".
Legacy.
The "Scuola Italiana Italo Calvino", an Italian curriculum school in Moscow, Russia, is named after him.
References.
Online.
</dl>
Further reading.
General

</doc>
<doc id="14939" url="http://en.wikipedia.org/wiki?curid=14939" title="Intercontinental ballistic missile">
Intercontinental ballistic missile

An intercontinental ballistic missile (ICBM) is a guided ballistic missile with a minimum range of more than 5500 km primarily designed for nuclear weapons delivery (delivering one or more thermonuclear warheads). Similarly conventional, chemical and biological weapons can also be delivered with varying effectiveness, but have never been deployed on ICBMs. Most modern designs support multiple independently targetable reentry vehicles (MIRVs), allowing a single missile to carry several warheads, each of which can strike a different target.
Early ICBMs had limited accuracy (circular error probable) that allowed them to be used only against the largest targets such as cities. They were seen as a "safe" basing option, one that would keep the deterrent force close to home where it would be difficult to attack. Attacks against (especially hardened) military targets, if desired, still demanded the use of a more precise manned bomber. This is due to the inverse-square law, which predicts that the amount of energy dispersed from a single point release of energy (such as a thermonuclear blast) dissipates by the inverse of the square of distance from the single point of release. The result is that the power of a nuclear explosion to rupture hardened structures is greatly decreased by the distance from the impact point of the nuclear weapon. So a near-direct hit is generally necessary, as only diminishing returns are gained by increasing bomb yield.
Second- and third-generation designs (e.g. the LGM-118 Peacekeeper) dramatically improved accuracy to the point where even the smallest point targets can be successfully attacked.
ICBMs are differentiated by having greater range and speed than other ballistic missiles: intermediate-range ballistic missiles (IRBMs), medium-range ballistic missiles (MRBMs), short-range ballistic missiles (SRBMs) (these shorter-range ballistic missiles are known collectively as theatre ballistic missiles). There is no single, standardized definition of what ranges would be categorized as intercontinental, intermediate, medium, or short.
History.
World War II.
The development of the world's first practical design for an ICBM, A9/10, intended for use in bombing New York and other American cities, was undertaken in Nazi Germany by the team of Wernher von Braun under "Projekt Amerika". The ICBM A9/A10 rocket initially was intended to be guided by radio, but was changed to be a piloted craft after the failure of Operation Elster. The second stage of the A9/A10 rocket was tested a few times in January and February 1945. The progenitor of the A9/A10 was the German V-2 rocket, also designed by von Braun and widely used at the end of World War II to bomb British and Belgian cities. All of these rockets used liquid propellants. Following the war, von Braun and other leading German scientists were relocated to the United States to work directly for the US Army through Operation Paperclip, developing the IRBMs, ICBMs, and launchers.
This technology was also predicted by US Army General Hap Arnold, who wrote in 1943:
Someday, not too distant, there can come streaking out of somewhere – we won't be able to hear it, it will come so fast – some kind of gadget with an explosive so powerful that one projectile will be able to wipe out completely this city of Washington.
Cold War.
In the immediate post-war era, the US and USSR both started rocket research programs based on the German wartime designs, especially the V-2. In the US, each branch of the military started its own programs, leading to considerable duplication of effort. In the USSR, rocket research was centrally organized, although several teams worked on different designs. Early designs from both countries were short-range missiles, like the V-2, but improvements quickly followed.
In the USSR early development was focused on missiles able to attack European targets. This changed in 1953 when Sergei Korolyov was directed to start development of a true ICBM able to deliver newly developed hydrogen bombs. Given steady funding throughout, the R-7 developed with some speed. The first launch took place on 15 May 1957 and led to an unintended crash 400 km from the site. The first successful test followed on 21 August 1957; the R-7 flew over 6000 km and became the world's first ICBM. The first strategic-missile unit became operational on 9 February 1959 at Plesetsk in north-west Russia.
It was the same R-7 launch vehicle that placed the first artificial satellite in space, Sputnik, on 4 October 1957. The first human spaceflight in history was accomplished on a derivative of R-7, Vostok, on 12 April 1961, by Soviet cosmonaut Yuri Gagarin. A deeply modernized version of the R-7 is still used as the launch vehicle for the Soviet/Russian Soyuz spacecraft, marking more than 50 years of operational history of Sergei Korolyov's original rocket design.
The U.S. initiated ICBM research in 1946 with the RTV-A-2 Hiroc project. This was a three-stage effort with the ICBM development not starting until the third stage. However, funding was cut after only three partially successful launches in 1948 of the second stage design, used to test variations on the V-2 design. With overwhelming air superiority and truly intercontinental bombers, the newly forming US Air Force did not take the problem of ICBM development seriously. Things changed in 1953 with the Soviet testing of their first Thermonuclear weapon, but it was not until 1954 that the Atlas missile program was given the highest national priority. The Atlas A first flew on 11 June 1957; the flight lasted only about 24 seconds before the rocket blew up. The first successful flight of an Atlas missile to full range occurred 28 November 1958. The first armed version of the Atlas, the Atlas D, was declared operational in January 1959 at Vandenberg, although it had not yet flown. The first test flight was carried out on 9 July 1959, and the missile was accepted for service on 1 September.
The R-7 and Atlas each required a large launch facility, making them vulnerable to attack, and could not be kept in a ready state. Failure rates were very high throughout the early years of ICBM technology. Human spaceflight programs (Vostok, Mercury, Voskhod, Gemini, etc.) served as a highly visible means of demonstrating confidence in reliability, with successes translating directly to national defense implications. The US was well behind the Soviet Union in the Space Race, so U.S. President John F. Kennedy increased the stakes with the Apollo program, which used Saturn rocket technology that had been funded by President Dwight D. Eisenhower.
These early ICBMs also formed the basis of many space launch systems. Examples include R-7, Atlas, Redstone, Titan, and Proton, which was derived from the earlier ICBMs but never deployed as an ICBM. The Eisenhower administration supported the development of solid-fueled missiles such as the LGM-30 Minuteman, Polaris and Skybolt. Modern ICBMs tend to be smaller than their ancestors, due to increased accuracy and smaller and lighter warheads, and use solid fuels, making them less useful as orbital launch vehicles.
The Western view of the deployment of these systems was governed by the strategic theory of Mutual Assured Destruction. In the 1950s and 1960s, development began on Anti-Ballistic Missile systems by both the U.S. and USSR; these systems were restricted by the 1972 ABM treaty. The first successful ABM test were conducted by the USSR in 1961, that later deployed a fully operating system defending Moscow in the 1970s (see Moscow ABM system).
The 1972 SALT treaty froze the number of ICBM launchers of both the USA and the USSR at existing levels, and allowed new submarine-based SLBM launchers only if an equal number of land-based ICBM launchers were dismantled. Subsequent talks, called SALT II, were held from 1972 to 1979 and actually reduced the number of nuclear warheads held by the USA and USSR. SALT II was never ratified by the United States Senate, but its terms were nevertheless honored by both sides until 1986, when the Reagan administration "withdrew" after accusing the USSR of violating the pact.
In the 1980s, President Ronald Reagan launched the Strategic Defense Initiative as well as the MX and Midgetman ICBM programs.
China developed a minimal independent nuclear deterrent entering its own cold war after an ideological split with the Soviet Union beginning in the early 1960s. After first testing a domestic built nuclear weapon in 1964, it went on to develop various warheads and missiles. Beginning in the early 1970s, the liquid fuelled DF-5 ICBM was developed and used as a satellite launch vehicle in 1975. The DF-5, with range of 10000 to long enough to strike the western US and the USSR, was silo deployed with the first pair in service by 1981 with possibly twenty missiles in service by the late 1990s. China also deployed the JL-1 Medium-range ballistic missile with a reach of 1700 km aboard the ultimately unsuccessful type 92 submarine.
Post–Cold War.
In 1991, the United States and the Soviet Union agreed in the START I treaty to reduce their deployed ICBMs and attributed warheads.
s of 2009[ [update]], all five of the nations with permanent seats on the United Nations Security Council have operational long-range ballistic missile systems: all except China have operational submarine-launched missiles, and Russia, the United States and China also have land-based ICBMs (the US' missiles are silo-based, while China and Russia have both silo and road-mobile (DF-31, RT-2PM2 Topol-M) missiles). 
Israel is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008; an upgraded version is in development.
India successfully test fired Agni V, with a strike range of more than 5000 km on 19 April 2012, claiming entry into the ICBM club. The missile's actual range is speculated by foreign researchers to be up to 8000 km with India having downplayed its capabilities to avoid causing concern to other countries. 
It is speculated by some intelligence agencies that North Korea is developing an ICBM. North Korea successfully put a satellite into space on 12 December 2012 using the 32 m Unha-3 rocket. The United States claimed that the launch was in fact a way to test an ICBM. (See Timeline of first orbital launches by country)
In July 2014 China announced the development of its newest generation of ICBM, the Dongfeng-41 (DF-41), which has a range of 12,000 kilometres (7,500 miles), capable of reaching the United States, and which analysts believe is capable of being MIRVed.
Most countries in the early stages of developing ICBMs have used liquid propellants, with the known exceptions being the Indian Agni-V, the planned South African RSA-4 ICBM and the now in service Israeli Jericho 3.
Flight phases.
The following flight phases can be distinguished:
Modern ICBMs.
Modern ICBMs typically carry multiple independently targetable reentry vehicles ("MIRVs"), each of which carries a separate nuclear warhead, allowing a single missile to hit multiple targets. MIRV was an outgrowth of the rapidly shrinking size and weight of modern warheads and the Strategic Arms Limitation Treaties which imposed limitations on the number of launch vehicles (SALT I and SALT II). It has also proved to be an "easy answer" to proposed deployments of ABM systems—it is far less expensive to add more warheads to an existing missile system than to build an ABM system capable of shooting down the additional warheads; hence, most ABM system proposals have been judged to be impractical. The first operational ABM systems were deployed in the U.S. during the 1970s. Safeguard ABM facility was located in North Dakota and was operational from 1975 to 1976. The USSR deployed its ABM-1 Galosh system around Moscow in the 1970s, which remains in service. Israel deployed a national ABM system based on the Arrow missile in 1998, but it is mainly designed to intercept shorter-ranged theater ballistic missiles, not ICBMs. The Alaska-based United States national missile defense system attained initial operational capability in 2004.
ICBMs can be deployed from multiple platforms:
The last three kinds are mobile and therefore hard to find.
During storage, one of the most important features of the missile is its serviceability. One of the key features of the first computer-controlled ICBM, the Minuteman missile, was that it could quickly and easily use its computer to test itself.
In flight, a Booster (rocketry)) pushes the warhead and then falls away. Most modern boosters are solid-fueled rocket motors, which can be stored easily for long periods of time. Early missiles used liquid-fueled rocket motors. Many liquid-fueled ICBMs could not be kept fuelled all the time as the Cryogenic fuel liquid oxygen boiled off and caused ice formation, and therefore fueling the rocket was necessary before launch. This procedure was a source of significant operational delay, and might allow the missiles to be destroyed by enemy counterparts before they could be used. To resolve this problem the United Kingdom invented the missile silo that protected the missile from a first strike and also hid fuelling operations underground.
Once the booster falls away, the warhead continues on an unpowered ballistic trajectory, much like an artillery shell or cannonball. The warhead is encased in a cone-shaped reentry vehicle and is difficult to detect in this phase of flight as there is no rocket exhaust or other emissions to mark its position to defenders. The high speeds of the warheads make them difficult to intercept and allow for little warning striking targets many thousands of kilometers away from the launch site (and due to the possible locations of the submarines: anywhere in the world) within approximately 30 minutes.
Many authorities say that missiles also release aluminized balloons, electronic noisemakers, and other items intended to confuse interception devices and radars (see penetration aid).
As the nuclear warhead reenters the Earth's atmosphere its high speed causes compression of the air, leading to a dramatic rise in temperature which would destroy it if it were not shielded in some way. As a result, warhead components are contained within an aluminium honeycomb substructure, sheathed in Pyrolytic carbon-epoxy Synthetic resin composite material, with a heat shield layer on top which is constructed out of Three-dimensional quartz phenolic.
Circular error probable is crucial, because halving the Circular error probable decreases the needed warhead energy by a factor of four (see inverse-square law). Accuracy is limited by the accuracy of the navigation system and the available geodetic information.
Strategic missile systems are thought to use custom integrated circuits designed to calculate navigational differential equations thousands to millions of FLOPS in order to reduce navigational errors caused by calculation alone. These circuits are usually a network of binary addition circuits that continually recalculate the missile's position. The inputs to the navigation circuit are set by a general purpose computer according to a navigational input schedule loaded into the missile before launch.
One particular weapon developed by the Soviet Union (Fractional Orbital Bombardment System) had a partial orbital trajectory, and unlike most ICBMs its target could not be deduced from its orbital flight path. It was decommissioned in compliance with arms control agreements, which address the maximum range of ICBMs and prohibit orbital or fractional-orbital weapons.
Specific ICBMs.
Land-based ICBMs.
Russia, the United States, China and India are the only countries currently known to possess land-based ICBMs, Israel has also tested ICBMs but is not open about actual deployment.
The United States currently operates 450 ICBMs in three USAF bases. The only model deployed is LGM-30G Minuteman-III.
All previous USAF Minuteman II missiles have been destroyed in accordance with START, and their launch silos have been sealed or sold to the public. To comply with the START II most U.S. multiple independently targetable reentry vehicles, or MIRVs, have been eliminated and replaced with single warhead missiles. The powerful MIRV-capable Peacekeeper missiles were phased out in 2005. However, since the abandonment of the START II treaty, the U.S. is said to be considering retaining 800 warheads on an existing 450 missiles.
The Russian Strategic Rocket Forces have 369 ICBMs able to deliver 1,247 nuclear warheads, 58 silo-based R-36M2 (SS-18), 70 silo-based UR-100N (SS-19), 171 mobile RT-2PM "Topol" (SS-25), 52 silo-based RT-2UTTH "Topol M" (SS-27), 18 mobile RT-2UTTH "Topol M" (SS-27), 6 (15 in December 2011) mobile RS-24 "Yars" (SS-29) "(Future replacement for R-36 & UR-100N missiles)"
China has developed several long range ICBMs, like the DF-31. The Dongfeng 5 or DF-5 is a 3-stage liquid fuel ICBM and has an estimated range of 13,000 kilometers. The DF-5 had its first flight in 1971 and was in operational service 10 years later. One of the downsides of the missile was that it took between 30 and 60 minutes to fuel. The Dong Feng 31 (a.k.a. CSS-10) is a medium-range, three-stage, solid-propellant intercontinental ballistic missile, and is a land-based variant of the submarine-launched JL-2.
The DF-41 or CSS-X-10 can carry up to 10 nuclear warheads, which are mIRVs and has a range of approximately 12000 –. The DF-41 deployed in underground Xinjiang, Qinghai,Gansu and Inner Mongolia area. The mysterious underground subway ICBM carrier systems they called"Underground Great Wall Project".
Israel is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008. It is possible for the missile to be equipped with a single 750 kg nuclear warhead or up to three MIRV warheads. It is believed to be based on the Shavit space launch vehicle and is estimated to have a range of 4800 to. In November 2011 Israel tested an ICBM believed to be an upgraded version of the Jericho III.
India has a series of ballistic missiles called Agni, of which the latest is MIRV capable Agni-V. On 19 April 2012, India successfully test fired its first Agni-V, a three-stage solid fueled missile, with a strike range of more than 7500 km. The missile was test-fired for the second time on 15 September 2013.
Submarine-launched.
All current designs of submarine launched ballistic missiles have intercontinental range except the current generation of short range Indian SLBMs. Current operators of such missiles are the United States, Russia, United Kingdom, France, India and the People's Republic of China.
Missile defense.
An anti-ballistic missile is a missile which can be deployed to counter an incoming nuclear or non-nuclear ICBM. ICBMs can be intercepted in three regions of their trajectory: boost phase, mid-course phase or terminal phase. Currently the US, Russia, China, France, India, and Israel have developed anti-ballistic missile systems, of which the Russian A-135 anti-ballistic missile system, US Ground-Based Midcourse Defense, Chinese KT series, and Indian Prithvi and Advanced Air Defence Systems have the capability to intercept ICBMs carrying nuclear, chemical, biological, or conventional warheads. Other systems can intercept ballistic missiles but are not as effective against an ICBM.

</doc>
<doc id="14943" url="http://en.wikipedia.org/wiki?curid=14943" title="Irish traditional music session">
Irish traditional music session

Irish traditional music sessions are mostly informal gatherings at which people play Irish traditional music. The Irish language word for "session" is "seisiún". This article discusses tune-playing, although "session" can also refer to a singing session or a mixed session (tunes and songs).
Barry Foy's "Field Guide to the Irish Music Session" defines a session as:
"...a gathering of Irish traditional musicians for the purpose of celebrating their common interest in the music by playing it together in a relaxed, informal setting, while in the process generally beefing up the mystical cultural mantra that hums along uninterruptedly beneath all manifestations of Irishness worldwide."
Social and cultural aspects.
The general session scheme is that someone starts a tune, and those who know it join in. Good session etiquette requires not playing if one does not know the tune, and waiting until a tune one knows comes along. In an "open" session, anyone who is able to play Irish music is welcome. 
Most often there are more-or-less recognized session leaders; sometimes there are no leaders. At times a song will be sung or a slow air played by a single musician between sets.
The objective in a session is not to provide music for an audience of passive listeners; although the "punters" (non-playing attendees) often come for the express purpose of listening, the music is most of all for the musicians themselves. The session is an experience that is shared, not a performance that is bought and sold.
The sessions are a key aspect of traditional music; some say it is the main sphere in which the music is formulated and innovated. Further, the sessions enable less advanced musicians to practice in a group. 
Socially, sessions have often been compared to an evening of playing card games, where the conversation and camaraderie are an essential component. In many rural communities in Ireland, sessions are an integral part of community life.
Musical aspects.
Typically, the first tune is followed by another two or three tunes in a "set". The art of putting together a set is hard to put into words, but the tunes must flow from one to another in terms of key and melodic structure, without being so similar as to all sound the same. The tunes of a set will usually all be of the same sort, i.e. all jigs or all reels, although on rare occasions and amongst a more skilled group of players a complementary tune of a different sort will be included, such as a slip jig amongst the jigs. Although bands sometimes arrange sets of reels and jigs together, this is uncommon in an Irish session context. 
Some sets are specific to a locale, or even to a single session, whilst others, like the "Coleman set" of reels ("The Tarbolton"/"The Longford Collector"/The Sailor's Bonnet"), represent longstanding combinations that have been played together for decades. Sets are sometimes thrown together "ad hoc", which sometimes works brilliantly and sometimes fails on the spot.
After the set ends, someone will usually start another.
Locations and Times.
Sessions are usually held in public houses. A pub owner might have one or two musicians paid to come regularly in order for the session to have a base. Sunday afternoons and weekday nights (especially Tuesday and Wednesday) are common times for sessions to be scheduled, on the theory that these are the least likely times for dances and concerts to be held, and therefore the times that professional musicians will be most able to show.
Sessions can be held in homes or at various public places in addition to pubs; often at a festival sessions will be got together in the beer tent or in the vendor's booth of a music-loving craftsman or dealer. When a particularly large musical event "takes over" an entire village, spontaneous sessions may erupt on the street corners. Sessions may also take place occasionally at wakes. House sessions are not as common now as they were in the past. This can be seen in the book Peig by Peig Sayers. In the early stages of the book when Peig was young they often went to sessions at peoples houses in a practice called 'bothántiocht'.

</doc>
<doc id="14946" url="http://en.wikipedia.org/wiki?curid=14946" title="Ice">
Ice

Ice (from the Old English ""īs", in turn from the Proto-Germanic "*isaz"") is water, frozen into a solid state. Depending on the presence of impurities such as particles of soil or bubbles of air, it can appear transparent or a more or less opaque bluish-white color.
In the Solar System, ice occurs naturally from as close to the Sun as Mercury to as far as the Oort cloud. Beyond the Solar System, it occurs as interstellar ice. It is abundant on Earth's surface – particularly in the polar regions and above the snow line – and, as a common form of precipitation and deposition, plays a key role in Earth's water cycle and climate. It falls as snowflakes and hail or occurs as frost, icicles or ice spikes.
Ice molecules exhibit different phases (packing geometries) that depend on temperature and pressure. Virtually all the ice on Earth's surface and in its atmosphere is of a hexagonal crystalline structure denoted as ice I (spoken as "ice one h"). The most common phase transition to ice I occurs when liquid water is cooled below (, ) at standard atmospheric pressure. It may also be deposited directly by water vapor, as happens in the formation of frost. The transition from ice to water is melting and from ice directly to water vapor is sublimation.
Ice is used in a variety of ways, including cooling, winter sports and ice sculpture.
Characteristics.
As a naturally-occurring crystalline inorganic solid with an ordered structure, ice is considered a mineral. It possesses a regular crystalline structure based on the molecule of water, which consists of a single oxygen atom covalently bonded to two hydrogen atoms, or H-O-H. However, many of the physical properties of water and ice are controlled by the formation of hydrogen bonds between adjacent oxygen and hydrogen atoms; while it is a weak bond, it is nonetheless critical in controlling the structure of both water and ice.
An unusual property of ice frozen at atmospheric pressure is that the solid is approximately 8.3% less dense than liquid water. The density of ice is 0.9167 g/cm3 at 0 °C, whereas water has a density of 0.9998 g/cm³ at the same temperature. Liquid water is densest, essentially 1.00 g/cm³, at 4 °C and becomes less dense as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid. Density of ice increases slightly with decreasing temperature and has a value of 0.9340 g/cm³ at −180 °C (93 K).
When water freezes, it increases in volume (about 9% for freshwater). The effect of expansion during freezing can be dramatic, and ice expansion is a basic cause of freeze-thaw weathering of rock in nature and damage to building foundations and roadways from frost heaving. It is also a common cause of the flooding of houses when water pipes burst due to the pressure of expanding water when it freezes.
The result of this process is that ice (in its most common form) floats on liquid water, which is an important feature in Earth's biosphere. It has been argued that without this property, natural bodies of water would freeze, in some cases permanently, from the bottom up, resulting in a loss of bottom-dependent animal and plant life in fresh and sea water. Sufficiently thin ice sheets allow light to pass through while protecting the underside from short-term weather extremes such as wind chill. This creates a sheltered environment for bacterial and algal colonies. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids, which in turn provide food for animals such as krill and specialised fish like the bald notothen, fed upon in turn by larger animals such as emperor penguins and minke whales.
When ice melts, it absorbs as much energy as it would take to heat an equivalent mass of water by 80 °C. During the melting process, the temperature remains constant at 0 °C. While melting, any energy added breaks the hydrogen bonds between ice (water) molecules. Energy becomes available to increase the thermal energy (temperature) only after enough hydrogen bonds are broken that the ice can be considered liquid water. The amount of energy consumed in breaking hydrogen bonds in the transition from ice to water is known as the "heat of fusion".
As with water, ice absorbs light at the red end of the spectrum preferentially as the result of an overtone of an oxygen-hydrogen (O-H) bond stretch. Compared with water, this absorption is shifted toward slightly lower energies. Thus, ice appears blue, with a slightly greener tint than for liquid water. Since absorption is cumulative, the color effect intensifies with increasing thickness or if internal reflections cause the light to take a longer path through the ice.
Other colors can appear in the presence of light absorbing impurities, where the impurity is dictating the color rather than the ice itself. For instance, icebergs containing impurities (e.g., sediments, algae, air bubbles) can appear brown, grey or green.
Slipperiness.
Ice was originally thought to be slippery due to the pressure of an object coming into contact with the ice, melting a thin layer of the ice and allowing the object to glide across the surface. For example, the blade of an ice skate, upon exerting pressure on the ice, would melt a thin layer, providing lubrication between the ice and the blade. This explanation, called "pressure melting", originated in the 19th century. It, however, did not account for skating on ice temperatures lower than −4.0 °C, which is often skated upon.
In the 20th century, an alternative explanation, called "friction heating," was proposed, whereby friction of the material was the cause of the ice layer melting. However, this theory also failed to explain skating at low temperature. Neither sufficiently explained why ice is slippery when standing still even at below-zero temperatures.
It is now believed that ice is slippery because ice molecules in contact with air cannot properly bond with the molecules of the mass of ice beneath (and thus are free to move like molecules of liquid water). These molecules remain in a semi-liquid state, providing lubrication regardless of pressure against the ice exerted by any object. However, the significance of this hypothesis is disputed by experiments showing a high coefficient of friction for ice using atomic force microscopy.
Natural formation.
The term that collectively describes all of the parts of the Earth's surface where water is in frozen form is the "cryosphere." Ice is an important component of the global climate, particularly in regard to the water cycle. Glaciers and snowpacks are an important storage mechanism for fresh water; over time, they may sublimate or melt. Snowmelt is an important source of seasonal fresh water.
The World Meteorological Organization defines several kinds of ice depending on origin, size, shape, influence and so on. Clathrate hydrates are forms of ice that contain gas molecules trapped within its crystal lattice.
Ice on the oceans.
Ice that is found at sea may be in the form of drift ice floating in the water, fast ice fixed to a shoreline or anchor ice if attached to the sea bottom. Ice which calves (breaks off) from an ice shelf or glacier may become an ice berg. Sea ice can be forced together by currents and winds to form pressure ridges up to 12 m tall. Navigation through areas of sea ice occurs in openings called "polynyas" or "leads" or requires the use of a special ship called an "icebreaker".
Ice on land and structures.
Ice on land ranges from the largest type called an "ice sheet" to smaller ice caps and ice fields to glaciers and ice streams to the snow line and snow fields.
Aufeis is layered ice that forms in Arctic and subarctic stream valleys. Ice, frozen in the stream bed, blocks normal groundwater discharge, and causes the local water table to rise, resulting in water discharge on top of the frozen layer. This water then freezes, causing the water table to rise further and repeat the cycle. The result is a stratified ice deposit, often several meters thick.
Freezing rain is a type of winter storm called an ice storm where rain falls and then freezes producing a glaze of ice. Ice can also form icicles, similar to stalactites in appearance, or stalagmite-like forms as water drips and re-freezes.
The term "ice dam" has three meanings (others discussed below). On structures, an ice dam is the buildup of ice on a sloped roof which stops melt water from draining properly and can cause damage from water leaks in buildings.
Ice on rivers and streams.
Ice which forms on moving water tends to be less uniform and stable than ice which forms on calm water. Ice jams (sometimes called 
"ice dams"), when broken chunks of ice pile up, are the greatest ice hazard on rivers. Ice jams can cause flooding, damage structures in or near the river, and damage vessels on the river. Ice jams can cause some hydropower industrial facilities to completely shut down. An ice dam is a blockage from the movement of a glacier which may produce a proglacial lake. Heavy ice flows in rivers can also damage vessels and require the use of an icebreaker to keep navigation possible.
Ice discs are circular formations of ice surrounded by water in a river.
Pancake ice is a formation of ice generally created in areas with less calm conditions.
Ice on lakes.
Ice forms on calm water from the shores, a thin layer spreading across the surface, and then downward. Ice on lakes is generally four types: Primary, secondary, superimposed and agglomerate. Primary ice forms first. Secondary ice forms below the primary ice in a direction parallel to the direction of the heat flow. Superimposed ice forms on top of the ice surface from rain or water which seeps up through cracks in the ice which often settles when loaded with snow.
"Shelf ice"' occurs when floating pieces of ice are driven by the wind piling up on the windward shore.
Candle ice is a form of rotten ice that develops in columns perpendicular to the surface of a lake.
Ice in the air.
Rime ice.
Rime is a type of ice formed on cold objects when drops of water crystallize on them. This can be observed in foggy weather, when the temperature drops during the night. Soft rime contains a high proportion of trapped air, making it appear white rather than transparent, and giving it a density about one quarter of that of pure ice. Hard rime is comparatively dense.
Ice pellets.
Ice pellets are a form of precipitation consisting of small, translucent balls of ice. This form of precipitation is also referred to as "sleet" by the United States National Weather Service. (In Commonwealth English "sleet" refers to a mixture of rain and snow). Ice pellets are usually smaller than hailstones. They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is "PL".
Ice pellets form when a layer of above-freezing air is located between 1500 and above the ground, with sub-freezing air both above and below it. This causes the partial or complete melting of any snowflakes falling through the warm layer. As they fall back into the sub-freezing layer closer to the surface, they re-freeze into ice pellets. However, if the sub-freezing layer beneath the warm layer is too small, the precipitation will not have time to re-freeze, and freezing rain will be the result at the surface. A temperature profile showing a warm layer above the ground is most likely to be found in advance of a warm front during the cold season, but can occasionally be found behind a passing cold front.
Hail.
Like other precipitation, hail forms in storm clouds when supercooled water droplets freeze on contact with condensation nuclei, such as dust or dirt. The storm's updraft blows the hailstones to the upper part of the cloud. The updraft dissipates and the hailstones fall down, back into the updraft, and are lifted up again. Hail has a diameter of 5 mm or more. Within METAR code, GR is used to indicate larger hail, of a diameter of at least 6.4 mm and GS for smaller. Stones just larger than golf ball-sized are one of the most frequently reported hail sizes. Hailstones can grow to 15 cm and weigh more than 0.5 kg. In large hailstones, latent heat released by further freezing may melt the outer shell of the hailstone. The hailstone then may undergo 'wet growth', where the liquid outer shell collects other smaller hailstones. The hailstone gains an ice layer and grows increasingly larger with each ascent. Once a hailstone becomes too heavy to be supported by the storm's updraft, it falls from the cloud.
Hail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing 0 C. Hail-producing clouds are often identifiable by their green coloration. The growth rate is maximized at about -13 C, and becomes vanishingly small much below -30 C as supercooled water droplets become rare. For this reason, hail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of 11000 ft. Entrainment of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporational cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is actually less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater depth. Hail in the tropics occurs mainly at higher elevations.
Snowflakes.
Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than -18 C, because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice; then the droplet freezes around this "nucleus." Experiments show that this "homogeneous" nucleation of cloud droplets only occurs at temperatures lower than -35 C. In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Our understanding of what particles make efficient ice nuclei is poor – what we do know is they are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei are used in cloud seeding. The droplet then grows by condensation of water vapor onto the ice surfaces.
Diamond dust.
So-called "diamond dust," also known as ice needles or ice crystals, forms at temperatures approaching -40 C due to air with slightly higher moisture from aloft mixing with colder, surface based air. The METAR identifier for diamond dust within international hourly weather reports is "IC".
Production.
Ice is now mechanically produced on a large scale, but before refrigeration was developed ice was harvested from natural sources for human use.
Ice harvesting.
Ice has long been valued as a means of cooling. In 400 BC Iran, Persian engineers had already mastered the technique of storing ice in the middle of summer in the desert. The ice was brought in during the winters from nearby mountains in bulk amounts, and stored in specially designed, naturally cooled "refrigerators", called yakhchal (meaning "ice storage"). This was a large underground space (up to 5000 m³) that had thick walls (at least two meters at the base) made of a special mortar called "sārooj", composed of sand, clay, egg whites, lime, goat hair, and ash in specific proportions, and which was known to be resistant to heat transfer. This mixture was thought to be completely water impenetrable. The space often had access to a qanat, and often contained a system of windcatchers which could easily bring temperatures inside the space down to frigid levels on summer days. The ice was used to chill treats for royalty.
There were thriving industries in 16th/17th century England whereby low lying areas along the Thames Estuary were flooded during the winter, and ice harvested in carts and stored inter-seasonally in insulated wooden houses as a provision to an icehouse often located in large country houses, and widely used to keep fish fresh when caught in distant waters. This was allegedly copied by an Englishman who had seen the same activity in China. Ice was imported into England from Norway on a considerable scale as early as 1823.
In the United States, the first cargo of ice was sent from New York City to Charleston, South Carolina in 1799, and by the first half of the 19th century, ice harvesting had become big business. Frederic Tudor, who became known as the “Ice King,” worked on developing better insulation products for the long distance shipment of ice, especially to the tropics; this became known as the ice trade.
Trieste sent ice to Egypt, Corfu, and Zante; Switzerland sent it to France; and Germany sometimes was supplied from Bavarian lakes. Until recently, the Hungarian Parliament building used ice harvested in the winter from Lake Balaton for air conditioning.
Ice houses were used to store ice formed in the winter, to make ice available all year long, and early refrigerators were known as iceboxes, because they had a block of ice in them. In many cities, it was not unusual to have a regular ice delivery service during the summer. The advent of artificial refrigeration technology has since made delivery of ice obsolete.
Ice is still harvested for ice and snow sculpture events. For example, a swing saw is used to get ice for the Harbin International Ice and Snow Sculpture Festival each year from the frozen surface of the Songhua River.
Commercial production.
Ice is now produced on an industrial scale, for uses including food storage and processing, chemical manufacturing, concrete mixing and curing, and consumer or packaged ice. Most commercial icemakers produce three basic types of fragmentary ice: flake, tubular and plate, using a variety of techniques. Large batch ice makers can produce up to 75 tons of ice per day.
Ice production is a large business; in 2002, there were 426 commercial ice-making companies in the United States, with a combined value of shipments of $595,487,000.
For small-scale ice production, many modern home refrigerators can also make ice with a built in icemaker, which will typically make ice cubes or crushed ice. Stand-alone icemaker units that make ice cubes are often called ice machines.
Uses.
Sports.
Ice also plays a central role in winter recreation and in many sports such as ice skating, tour skating, ice hockey, bandy, ice fishing, ice climbing, curling, broomball and sled racing on bobsled, luge and skeleton. Many of the different sports played on ice get international attention every four years during the Winter Olympic Games.
A sort of sailboat on blades gives rise to ice yachting. The human quest for excitement has even led to ice racing, where drivers must speed on lake ice, while also controlling the skid of their vehicle (similar in some ways to dirt track racing). The sport has even been modified for ice rinks.
Ice and transportation.
Ice can also be an obstacle; for harbors near the poles, being ice-free is an important advantage; ideally, all year long. Examples are Murmansk (Russia), Petsamo (Russia, formerly Finland) and Vardø (Norway). Harbors which are not ice-free are opened up using icebreakers.
Ice forming on roads is a dangerous winter hazard. Black ice is very difficult to see, because it lacks the expected frosty surface. Whenever there is freezing rain or snow which occurs at a temperature near the melting point, it is common for ice to build up on the windows of vehicles. Driving safely requires the removal of the ice build-up. Ice scrapers are tools designed to break the ice free and clear the windows, though removing the ice can be a long and laborious process.
Far enough below the freezing point, a thin layer of ice crystals can form on the inside surface of windows. This usually happens when a vehicle has been left alone after being driven for a while, but can happen while driving, if the outside temperature is low enough. Moisture from the driver's breath is the source of water for the crystals. It is troublesome to remove this form of ice, so people often open their windows slightly when the vehicle is parked in order to let the moisture dissipate, and it is now common for cars to have rear-window defrosters to solve the problem. A similar problem can happen in homes, which is one reason why many colder regions require double-pane windows for insulation.
When the outdoor temperature stays below freezing for extended periods, very thick layers of ice can form on lakes and other bodies of water, although places with flowing water require much colder temperatures. The ice can become thick enough to drive onto with automobiles and trucks. Doing this safely requires a thickness of at least 30 cm (one foot).
For ships, ice presents two distinct hazards. Spray and freezing rain can produce an ice build-up on the superstructure of a vessel sufficient to make it unstable, and to require it to be hacked off or melted with steam hoses. And icebergs – large masses of ice floating in water (typically created when glaciers reach the sea) – can be dangerous if struck by a ship when underway. Icebergs have been responsible for the sinking of many ships, the most famous probably being the "Titanic".
For aircraft, ice can cause a number of dangers. As an aircraft climbs, it passes through air layers of different temperature and humidity, some of which may be conducive to ice formation. If ice forms on the wings or control surfaces, this may adversely affect the flying qualities of the aircraft. During the first non-stop flight across the Atlantic, the British aviators Captain John Alcock and Lieutenant Arthur Whitten Brown encountered such icing conditions – Brown left the cockpit and climbed onto the wing several times to remove ice which was covering the engine air intakes of the Vickers Vimy aircraft they were flying.
A particular icing vulnerability associated with reciprocating internal combustion engines is the carburetor. As air is sucked through the carburetor into the engine, the local air pressure is lowered, which causes adiabatic cooling. So, in humid near-freezing conditions, the carburetor will be colder, and tend to ice up. This will block the supply of air to the engine, and cause it to fail. For this reason, aircraft reciprocating engines with carburetors are provided with carburetor air intake heaters. The increasing use of fuel injection—which does not require carburetors—has made "carb icing" less of an issue for reciprocating engines.
Jet engines do not experience carb icing, but recent evidence indicates that they can be slowed, stopped, or damaged by internal icing in certain types of atmospheric conditions much more easily than previously believed. In most cases, the engines can be quickly restarted and flights are not endangered, but research continues to determine the exact conditions which produce this type of icing, and find the best methods to prevent, or reverse it, in flight.
Phases.
Ice may be any one of the 17 known solid crystalline phases of water, or in an amorphous solid state at various densities.
Most liquids under increased pressure freeze at "higher" temperatures because the pressure helps to hold the molecules together. However, the strong hydrogen bonds in water make it different: For some pressures higher than 1 atm, water freezes at a temperature "below" 0 °C, as shown in the phase diagram below. The melting of ice under high pressures is thought to contribute to the movement of glaciers.
Ice, water, and water vapour can coexist at the triple point, which is exactly 0.01 °C (273.16 K) at a pressure of 611.73 Pa (the Kelvin is in fact defined as 1/273.16 of the difference between this triple point and absolute zero). Unlike most other solids, ice is difficult to superheat. In an experiment, ice at −3 °C was superheated to about 17 °C for about 250 picoseconds.
Subjected to higher pressures and varying temperatures, ice can form in sixteen separate known phases. With care all these phases except ice X can be recovered at ambient pressure and low temperature. The types are differentiated by their crystalline structure, ordering and density. There are also two metastable phases of ice under pressure, both fully hydrogen-disordered; these are IV and XII. Ice XII was discovered in 1996. In 2006, XIII and XIV were discovered. Ices XI, XIII, and XIV are hydrogen-ordered forms of ices Ih, V, and XII respectively. In 2009, ice XV was found at extremely high pressures and −143 °C. At even higher pressures, ice is predicted to become a metal; this has been variously estimated to occur at 1.55 TPa or 5.62 TPa.
As well as crystalline forms, solid water can exist in amorphous states as amorphous ice (ASW) of varying densities. Water in the interstellar medium is dominated by amorphous ice, making it likely the most common form of water in the universe. Low-density ASW (LDA), also known as hyperquenched glassy water, may be responsible for noctilucent clouds on earth and is usually formed by deposition of water vapor in cold or vacuum conditions. High density ASW (HDA) is formed by compression of ordinary ice Ih or LDA at GPa pressures. Very-high density ASW (VHDA) is HDA slightly warmed to 160K under 1–2 GPa pressures.
In outer space, hexagonal crystalline ice (the predominant form found on Earth) is extremely rare. Amorphous ice is more common; however, hexagonal crystalline ice can be formed via volcanic action.
Other ices.
The solid phases of several other volatile substances are also referred to as "ices"; generally a volatile is classed as an ice if its melting point lies above around 100 K. The best known example is dry ice, the solid form of carbon dioxide.
A "magnetic analogue" of ice is also realized in some insulating magnetic materials in which the magnetic moments mimic the position of protons in water ice and obey energetic constraints similar to the Bernal-Fowler ice rules arising from the geometrical frustration of the proton configuration in water ice. These materials are called spin ice.

</doc>
<doc id="14951" url="http://en.wikipedia.org/wiki?curid=14951" title="Ionic bonding">
Ionic bonding

Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions. These ions represent atoms that have lost one or more electrons (known as cations) and atoms that have gained one or more electrons (known as an anion). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like NH4+ or SO42− . In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal in order for both atoms to obtain a full valence shell.
It is important to recognize that "clean" ionic bonding – in which one atom "steals" an electron from another – cannot exist: All ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term "ionic bonding" is given when the ionic character is greater than the covalent character—that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. 
Ionic compounds conduct electricity when molten or in solution, but typically not as a solid. There are exceptions to this rule, such as rubidium silver iodide, where the silver ion can be quite mobile. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water. Here, the opposite trend roughly holds: The weaker the cohesive forces, the greater the solubility. 
Overview.
Atoms that have an almost full or almost empty valence shells tend to be very reactive. Atoms that are strongly electronegative (as is the case with halogens) often only have one or two missing electrons in their valence shell, and frequently bond with other molecules or gain electrons to form anions. Atoms that are weakly electronegative (such as alkali metals) have relatively few valence electrons that can easily be lost to atoms that are strongly electronegative. As a result, weakly electronegative atoms tend to lose their electrons and form cations.
Formation.
Ionic bonding can result from a redox reaction when atoms of an element (usually metal), whose ionization energy is low, release some of their electrons to achieve a stable electron configuration. In doing so, cations are formed. The atom of another element (usually nonmetal), whose electron affinity is positive, then accepts the electron(s), again to attain a stable electron configuration, and after accepting electron(s) the atom becomes an anion. Typically, the stable electron configuration is one of the noble gases for elements in the s-block and the p-block, and particular stable electron configurations for d-block and f-block elements. The electrostatic attraction between the anions and cations leads to the formation of a solid with a crystallographic lattice in which the ions are stacked in an alternating fashion. In such a lattice, it is usually not possible to distinguish discrete molecular units, so that the compounds formed are not molecular in nature. However, the ions themselves can be complex and form molecular ions like the acetate anion or the ammonium cation.
For example, common table salt is sodium chloride. When sodium (Na) and chlorine (Cl) are combined, the sodium atoms each lose an electron, forming cations (Na+), and the chlorine atoms each gain an electron to form anions (Cl−). These ions are then attracted to each other in a 1:1 ratio to form sodium chloride (NaCl).
However, to maintain charge neutrality, strict ratios between anions and cations are observed so that ionic compounds, in general, obey the rules of stoichiometry despite not being molecular compounds. For compounds that are transitional to the alloys and possess mixed ionic and metallic bonding, this may not be the case anymore. Many sulfides, e.g., do form non-stoichiometric compounds.
Many ionic compounds are referred to as salts as they can also be formed by the neutralization reaction of an Arrhenius base like NaOH with an Arrhenius acid like HCl
The salt NaCl is then said to consist of the acid rest Cl− and the base rest Na+.
The removal of electrons from the cation is endothermic, raising the system's overall energy. There may also be energy changes associated with breaking of existing bonds or the addition of more than one electron to form anions. However, the action of the anion's accepting the cation's valence electrons and the subsequent attraction of the ions to each other releases (lattice) energy and, thus, lowers the overall energy of the system.
Ionic bonding will occur only if the overall energy change for the reaction is favorable. In general, the reaction is exothermic, but, e.g., the formation of mercuric oxide (HgO) is endothermic. The charge of the resulting ions is a major factor in the strength of ionic bonding, e.g. a salt C+A− is held together by electrostatic forces roughly four times weaker than C2+A2− according to Coulombs law, where C and A represent a generic cation and anion respectively. Of course the sizes of the ions and the particular packing of the lattice are ignored in this simple argument.
Structures.
Ionic compounds in the solid state form lattice structures. The two principal factors in determining the form of the lattice are the relative charges of the ions and their relative sizes. Some structures are adopted by a number of compounds; for example, the structure of the rock salt sodium chloride is also adopted by many alkali halides, and binary oxides such as MgO. Pauling's rules provide guidelines for predicting and rationalizing the crystal structures of ionic crystals
Bond strength.
For a solid crystalline ionic compound the enthalpy change in forming the solid from gaseous ions is termed the lattice energy.
The experimental value for the lattice energy can be determined using the Born-Haber cycle. It can also be calculated (predicted) using the Born-Landé equation as the sum of the electrostatic potential energy, calculated by summing interactions between cations and anions, and a short-range repulsive potential energy term. The electrostatic potential can be expressed in terms of the inter-ionic separation and a constant (Madelung constant) that takes account of the geometry of the crystal. The Born-Landé equation gives a reasonable fit to the lattice energy of, e.g., sodium chloride, where the calculated (predicted) value is −756 kJ/mol, which compares to −787 kJ/mol using the Born-Haber cycle.
Polarization effects.
Ions in crystal lattices of purely ionic compounds are spherical; however, if the positive ion is small and/or highly charged, it will distort the electron cloud of the negative ion, an effect summarised in Fajans' rules. This polarization of the negative ion leads to a build-up of extra charge density between the two nuclei, i.e., to partial covalency. Larger negative ions are more easily polarized, but the effect is usually important only when positive ions with charges of 3+ (e.g., Al3+) are involved. However, 2+ ions (Be2+) or even 1+ (Li+) show some polarizing power because their sizes are so small (e.g., LiI is ionic but has some covalent bonding present). Note that this is not the ionic polarization effect that refers to displacement of ions in the lattice due to the application of an electric field.
Comparison with covalent bonding.
In ionic bonding, the atoms are bound by attraction of opposite ions, whereas, in covalent bonding, atoms are bound by sharing electrons to attain stable electron configurations. In covalent bonding, the molecular geometry around each atom is determined by valence shell electron pair repulsion VSEPR rules, whereas, in ionic materials, the geometry follows maximum packing rules. One could say that covalent bonding is more "directional" in the sense that the energy penalty for not adhering to the optimum bond angles is large, whereas ionic bonding has no such penalty. There are no shared electron pairs to repel each other, the ions should simply be packed as efficiently as possible. This often leads to much higher coordination numbers. In NaCl, each ion has 6 neighbors and all bond angles are 90 degrees. In CsCl the coordination number is 8. By comparison carbon typically has a maximum of four neighbors.
Purely ionic bonding cannot exist, as the proximity of the entities involved in the bonding allows some degree of sharing electron density between them. Therefore, all ionic bonding has some covalent character. Thus, bonding is considered ionic where the ionic character is greater than the covalent character. The larger the difference in electronegativity between the two types of atoms involved in the bonding, the more ionic (polar) it is. Bonds with partially ionic and partially covalent character are called polar covalent bonds. For example, Na–Cl and Mg–O interactions have a few percent covalency, while Si–O bonds are usually ~50% ionic and ~50% covalent. Pauling estimated that an electronegativity difference of 1.7 (on the Pauling scale) corresponds to 50% ionic character, so that a difference greater than 50% corresponds to a bond which is predominantly ionic.
Ionic character in covalent bonds can be directly measured for atoms having quadrupolar nuclei (2H, 14N, 81,79Br, 35,37Cl or 127I). These nuclei are generally objects of NQR nuclear quadrupole resonance and NMR nuclear magnetic resonance studies. Interactions between the nuclear quadrupole moments Q and the electric field gradients (EFG) are characterized via the nuclear quadrupole coupling constants QCC = e2qzzQ/h 
where the eqZZ term corresponds to the principal component of the EFG tensor and 
e is the elementary charge. In turn, the electric field gradient opens the way to description of bonding modes in molecules when the QCC values are accurately determined by NMR or NQR methods. 
In general, when ionic bonding occurs in the solid (or liquid) state, it is not possible to talk about a single "ionic bond" between two individual atoms, because the cohesive forces that keep the lattice together are of a more collective nature. This is quite different in the case of covalent bonding, where we can often speak of a distinct bond localized between two particular atoms. However, even if ionic bonding is combined with some covalency, the result is "not" necessarily discrete bonds of a localized character. In such cases, the resulting bonding often requires description in terms of a band structure consisting of gigantic molecular orbitals spanning the entire crystal. Thus, the bonding in the solid often retains its collective rather than localized nature. When the difference in electronegativity is decreased, the bonding may then lead to a semiconductor, a semimetal or eventually a metallic conductor with metallic bonding.
Electrical conductivity.
Ionic compounds, if molten or dissolved, can conduct electricity because the ions in these conditions are free to move and carry electrons between the anode and the cathode. In the solid form, however, they typically cannot conduct because the electrons are held together too tightly for them to move. However, some ionic compounds can conduct electricity when solid. This is due to migration of the ions (in particular Ag+ and Cu+) themselves under the influence of an electric field. These compounds are known fast ion conductors.

</doc>
<doc id="14952" url="http://en.wikipedia.org/wiki?curid=14952" title="IBF (disambiguation)">
IBF (disambiguation)

IBF may refer to:

</doc>
<doc id="14958" url="http://en.wikipedia.org/wiki?curid=14958" title="Immune system">
Immune system

The immune system is a system of many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue. In many species, the immune system can be classified into subsystems, such as the innate immune system versus the adaptive immune system, or humoral immunity versus cell-mediated immunity.
Pathogens can rapidly evolve and adapt, and thereby avoid detection and neutralization by the immune system; however, multiple defense mechanisms have also evolved to recognize and neutralize pathogens. Even simple unicellular organisms such as bacteria possess a rudimentary immune system, in the form of enzymes that protect against bacteriophage infections. Other basic immune mechanisms evolved in ancient eukaryotes and remain in their modern descendants, such as plants and insects. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt over time to recognize specific pathogens more efficiently. Adaptive (or acquired) immunity creates immunological memory after an initial response to a specific pathogen, leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.
Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer.
Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.
History of immunology.
Immunology is a science that examines the structure and function of the immune system. It originates from medicine and early studies on the causes of immunity to disease. The earliest known reference to immunity was during the plague of Athens in 430 BC. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. In the 18th century, Pierre-Louis Moreau de Maupertuis made experiments with scorpion venom and observed that certain dogs and mice were immune to this venom. This and other observations of acquired immunity were later exploited by Louis Pasteur in his development of vaccination and his proposed germ theory of disease. Pasteur's theory was in direct opposition to contemporary theories of disease, such as the miasma theory. It was not until Robert Koch's 1891 proofs, for which he was awarded a Nobel Prize in 1905, that microorganisms were confirmed as the cause of infectious disease. Viruses were confirmed as human pathogens in 1901, with the discovery of the yellow fever virus by Walter Reed.
Immunology made a great advance towards the end of the 19th century, through rapid developments, in the study of humoral immunity and cellular immunity. Particularly important was the work of Paul Ehrlich, who proposed the side-chain theory to explain the specificity of the antigen-antibody reaction; his contributions to the understanding of humoral immunity were recognized by the award of a Nobel Prize in 1908, which was jointly awarded to the founder of cellular immunology, Elie Metchnikoff.
Layered defense.
The immune system protects organisms from infection with layered defenses of increasing specificity. In simple terms, physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all plants and animals. If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.
Both innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, "self" molecules are those components of an organism's body that can be distinguished from foreign substances by the immune system. Conversely, "non-self" molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (short for "anti"body "gen"erators) and are defined as substances that bind to specific immune receptors and elicit an immune response.
Innate immune system.
Microorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms, or when damaged, injured or stressed cells send out alarm signals, many of which (but not all) are recognized by the same receptors as those that recognize pathogens. Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way. This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms.
Surface barriers.
Several barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of many leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection. However, as organisms cannot be completely sealed from their environments, other systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.
Chemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the β-defensins. Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials. Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens. In the stomach, gastric acid and proteases serve as powerful chemical defenses against ingested pathogens.
Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in their environment, such as pH or available iron. This reduces the probability that pathogens will reach sufficient numbers to cause illness. However, since most antibiotics non-specifically target bacteria and do not affect fungi, oral antibiotics can lead to an "overgrowth" of fungi and cause conditions such as a vaginal candidiasis (a yeast infection). There is good evidence that re-introduction of probiotic flora, such as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections.
Inflammation.
Inflammation is one of the first responses of the immune system to infection. The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue. Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.
Complement system.
The complement system is a biochemical cascade that attacks the surfaces of foreign cells. It contains over 20 different proteins and is named for its ability to "complement" the killing of pathogens by antibodies. Complement is the major humoral component of the innate immune response. Many species have complement systems, including non-mammals like plants, fish, and some invertebrates.
In humans, this response is activated by complement binding to antibodies that have attached to these microbes or the binding of complement proteins to carbohydrates on the surfaces of microbes. This recognition signal triggers a rapid killing response. The speed of the response is a result of signal amplification that occurs following sequential proteolytic activation of complement molecules, which are also proteases. After complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. This produces a catalytic cascade that amplifies the initial signal by controlled positive feedback. The cascade results in the production of peptides that attract immune cells, increase vascular permeability, and opsonize (coat) the surface of a pathogen, marking it for destruction. This deposition of complement can also kill cells directly by disrupting their plasma membrane.
Cellular barriers.
Leukocytes (white blood cells) act like independent, single-celled organisms and are the second arm of the innate immune system. The innate leukocytes include the phagocytes (macrophages, neutrophils, and dendritic cells), mast cells, eosinophils, basophils, and natural killer cells. These cells identify and eliminate pathogens, either by attacking larger pathogens through contact or by engulfing and then killing microorganisms. Innate cells are also important mediators in the activation of the adaptive immune system.
Phagocytosis is an important feature of cellular innate immunity performed by cells called 'phagocytes' that engulf, or eat, pathogens or particles. Phagocytes generally patrol the body searching for pathogens, but can be called to specific locations by cytokines. Once a pathogen has been engulfed by a phagocyte, it becomes trapped in an intracellular vesicle called a phagosome, which subsequently fuses with another vesicle called a lysosome to form a phagolysosome. The pathogen is killed by the activity of digestive enzymes or following a respiratory burst that releases free radicals into the phagolysosome. Phagocytosis evolved as a means of acquiring nutrients, but this role was extended in phagocytes to include engulfment of pathogens as a defense mechanism. Phagocytosis probably represents the oldest form of host defense, as phagocytes have been identified in both vertebrate and invertebrate animals.
Neutrophils and macrophages are phagocytes that travel throughout the body in pursuit of invading pathogens. Neutrophils are normally found in the bloodstream and are the most abundant type of phagocyte, normally representing 50% to 60% of the total circulating leukocytes. During the acute phase of inflammation, particularly as a result of bacterial infection, neutrophils migrate toward the site of inflammation in a process called chemotaxis, and are usually the first cells to arrive at the scene of infection. Macrophages are versatile cells that reside within tissues and produce a wide array of chemicals including enzymes, complement proteins, and regulatory factors such as interleukin 1. Macrophages also act as scavengers, ridding the body of worn-out cells and other debris, and as antigen-presenting cells that activate the adaptive immune system.
Dendritic cells (DC) are phagocytes in tissues that are in contact with the external environment; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines. They are named for their resemblance to neuronal dendrites, as both have many spine-like projections, but dendritic cells are in no way connected to the nervous system. Dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens to T cells, one of the key cell types of the adaptive immune system.
Mast cells reside in connective tissues and mucous membranes, and regulate the inflammatory response. They are most often associated with allergy and anaphylaxis. Basophils and eosinophils are related to neutrophils. They secrete chemical mediators that are involved in defending against parasites and play a role in allergic reactions, such as asthma. Natural killer (NK cells) cells are leukocytes that attack and destroy tumor cells, or cells that have been infected by viruses.
Natural killer cells.
Natural killer cells, or NK cells, are a component of the innate immune system which does not directly attack invading microbes. Rather, NK cells destroy compromised host cells, such as tumor cells or virus-infected cells, recognizing such cells by a condition known as "missing self." This term describes cells with low levels of a cell-surface marker called MHC I (major histocompatibility complex) – a situation that can arise in viral infections of host cells. They were named "natural killer" because of the initial notion that they do not require activation in order to kill cells that are "missing self." For many years it was unclear how NK cells recognize tumor cells and infected cells. It is now known that the MHC makeup on the surface of those cells is altered and the NK cells become activated through recognition of "missing self". Normal body cells are not recognized and attacked by NK cells because they express intact self MHC antigens. Those MHC antigens are recognized by killer cell immunoglobulin receptors (KIR) which essentially put the brakes on NK cells.
Adaptive immune system.
The adaptive immune system evolved in early vertebrates and allows for a stronger immune response as well as immunological memory, where each pathogen is "remembered" by a signature antigen. The adaptive immune response is antigen-specific and requires the recognition of specific "non-self" antigens during a process called antigen presentation. Antigen specificity allows for the generation of responses that are tailored to specific pathogens or pathogen-infected cells. The ability to mount these tailored responses is maintained in the body by "memory cells". Should a pathogen infect the body more than once, these specific memory cells are used to quickly eliminate it.
Lymphocytes.
The cells of the adaptive immune system are special types of leukocytes, called lymphocytes. B cells and T cells are the major types of lymphocytes and are derived from hematopoietic stem cells in the bone marrow. B cells are involved in the humoral immune response, whereas T cells are involved in cell-mediated immune response.
Both B cells and T cells carry receptor molecules that recognize specific targets. T cells recognize a "non-self" target, such as a pathogen, only after antigens (small fragments of the pathogen) have been processed and presented in combination with a "self" receptor called a major histocompatibility complex (MHC) molecule. There are two major subtypes of T cells: the killer T cell and the helper T cell. In addition there are suppressor T cells which have a role in modulating immune response. Killer T cells only recognize antigens coupled to Class I MHC molecules, while helper T cells only recognize antigens coupled to Class II MHC molecules. These two mechanisms of antigen presentation reflect the different roles of the two types of T cell. A third, minor subtype are the γδ T cells that recognize intact antigens that are not bound to MHC receptors.
In contrast, the B cell antigen-specific receptor is an antibody molecule on the B cell surface, and recognizes whole pathogens without any need for antigen processing. Each lineage of B cell expresses a different antibody, so the complete set of B cell antigen receptors represent all the antibodies that the body can manufacture.
Killer T cells.
Killer T cells are a sub-group of T cells that kill cells that are infected with viruses (and other pathogens), or are otherwise damaged or dysfunctional. As with B cells, each type of T cell recognizes a different antigen. Killer T cells are activated when their T cell receptor (TCR) binds to this specific antigen in a complex with the MHC Class I receptor of another cell. Recognition of this MHC:antigen complex is aided by a co-receptor on the T cell, called CD8. The T cell then travels throughout the body in search of cells where the MHC I receptors bear this antigen. When an activated T cell contacts such cells, it releases cytotoxins, such as perforin, which form pores in the target cell's plasma membrane, allowing ions, water and toxins to enter. The entry of another toxin called granulysin (a protease) induces the target cell to undergo apoptosis. T cell killing of host cells is particularly important in preventing the replication of viruses. T cell activation is tightly controlled and generally requires a very strong MHC/antigen activation signal, or additional activation signals provided by "helper" T cells (see below).
Helper T cells.
Helper T cells regulate both the innate and adaptive immune responses and help determine which immune responses the body makes to a particular pathogen. These cells have no cytotoxic activity and do not kill infected cells or clear pathogens directly. They instead control the immune response by directing other cells to perform these tasks.
Helper T cells express T cell receptors (TCR) that recognize antigen bound to Class II MHC molecules. The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation. Helper T cells have a weaker association with the MHC:antigen complex than observed for killer T cells, meaning many receptors (around 200–300) on the helper T cell must be bound by an MHC:antigen in order to activate the helper cell, while killer T cells can be activated by engagement of a single MHC:antigen molecule. Helper T cell activation also requires longer duration of engagement with an antigen-presenting cell. The activation of a resting helper T cell causes it to release cytokines that influence the activity of many cell types. Cytokine signals produced by helper T cells enhance the microbicidal function of macrophages and the activity of killer T cells. In addition, helper T cell activation causes an upregulation of molecules expressed on the T cell's surface, such as CD40 ligand (also called CD154), which provide extra stimulatory signals typically required to activate antibody-producing B cells.
Gamma delta T cells.
Gamma delta T cells (γδ T cells) possess an alternative T cell receptor (TCR) as opposed to CD4+ and CD8+ (αβ) T cells and share the characteristics of helper T cells, cytotoxic T cells and NK cells. The conditions that produce responses from γδ T cells are not fully understood. Like other 'unconventional' T cell subsets bearing invariant TCRs, such as CD1d-restricted Natural Killer T cells, γδ T cells straddle the border between innate and adaptive immunity. On one hand, γδ T cells are a component of adaptive immunity as they rearrange TCR genes to produce receptor diversity and can also develop a memory phenotype. On the other hand, the various subsets are also part of the innate immune system, as restricted TCR or NK receptors may be used as pattern recognition receptors. For example, large numbers of human Vγ9/Vδ2 T cells respond within hours to common molecules produced by microbes, and highly restricted Vδ1+ T cells in epithelia respond to stressed epithelial cells.
B lymphocytes and antibodies.
A B cell identifies pathogens when antibodies on its surface bind to a specific foreign antigen. This antigen/antibody complex is taken up by the B cell and processed by proteolysis into peptides. The B cell then displays these antigenic peptides on its surface MHC class II molecules. This combination of MHC and antigen attracts a matching helper T cell, which releases lymphokines and activates the B cell. As the activated B cell then begins to divide, its offspring (plasma cells) secrete millions of copies of the antibody that recognizes this antigen. These antibodies circulate in blood plasma and lymph, bind to pathogens expressing the antigen and mark them for destruction by complement activation or for uptake and destruction by phagocytes. Antibodies can also neutralize challenges directly, by binding to bacterial toxins or by interfering with the receptors that viruses and bacteria use to infect cells.
Alternative adaptive immune system.
Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.
Immunological memory.
When B cells and T cells are activated and begin to replicate, some of their offspring become long-lived memory cells. Throughout the lifetime of an animal, these memory cells remember each specific pathogen encountered and can mount a strong response if the pathogen is detected again. This is "adaptive" because it occurs during the lifetime of an individual as an adaptation to infection with that pathogen and prepares the immune system for future challenges. Immunological memory can be in the form of either passive short-term memory or active long-term memory.
Passive memory.
Newborn infants have no prior exposure to microbes and are particularly vulnerable to infection. Several layers of passive protection are provided by the mother. During pregnancy, a particular type of antibody, called IgG, is transported from mother to baby directly across the placenta, so human babies have high levels of antibodies even at birth, with the same range of antigen specificities as their mother. Breast milk or colostrum also contains antibodies that are transferred to the gut of the infant and protect against bacterial infections until the newborn can synthesize its own antibodies. This is passive immunity because the fetus does not actually make any memory cells or antibodies—it only borrows them. This passive immunity is usually short-term, lasting from a few days up to several months. In medicine, protective passive immunity can also be transferred artificially from one individual to another via antibody-rich serum.
Active memory and immunization.
Long-term "active" memory is acquired following infection by activation of B and T cells. Active immunity can also be generated artificially, through vaccination. The principle behind vaccination (also called immunization) is to introduce an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen without causing disease associated with that organism. This deliberate induction of an immune response is successful because it exploits the natural specificity of the immune system, as well as its inducibility. With infectious disease remaining one of the leading causes of death in the human population, vaccination represents the most effective manipulation of the immune system mankind has developed.
Most viral vaccines are based on live attenuated viruses, while many bacterial vaccines are based on acellular components of micro-organisms, including harmless toxin components. Since many antigens derived from acellular vaccines do not strongly induce the adaptive response, most bacterial vaccines are provided with additional adjuvants that activate the antigen-presenting cells of the innate immune system and maximize immunogenicity.
Disorders of human immunity.
The immune system is a remarkably effective structure that incorporates specificity, inducibility and adaptation. Failures of host defense do occur, however, and fall into three broad categories: immunodeficiencies, autoimmunity, and hypersensitivities.
Immunodeficiencies.
Immunodeficiencies occur when one or more of the components of the immune system are inactive. The ability of the immune system to respond to pathogens is diminished in both the young and the elderly, with immune responses beginning to decline at around 50 years of age due to immunosenescence. In developed countries, obesity, alcoholism, and drug use are common causes of poor immune function. However, malnutrition is the most common cause of immunodeficiency in developing countries. Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production. Additionally, the loss of the thymus at an early age through genetic mutation or surgical removal results in severe immunodeficiency and a high susceptibility to infection.
Immunodeficiencies can also be inherited or 'acquired'. Chronic granulomatous disease, where phagocytes have a reduced ability to destroy pathogens, is an example of an inherited, or congenital, immunodeficiency. AIDS and some types of cancer cause acquired immunodeficiency.
Autoimmunity.
Overactive immune responses comprise the other end of immune dysfunction, particularly the autoimmune disorders. Here, the immune system fails to properly distinguish between self and non-self, and attacks part of the body. Under normal circumstances, many T cells and antibodies react with "self" peptides. One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.
Hypersensitivity.
Hypersensitivity is an immune response that damages the body's own tissues. They are divided into four classes (Type I – IV) based on the mechanisms involved and the time course of the hypersensitive reaction. Type I hypersensitivity is an immediate or anaphylactic reaction, often associated with allergy. Symptoms can range from mild discomfort to death. Type I hypersensitivity is mediated by IgE, which triggers degranulation of mast cells and basophils when cross-linked by antigen.
Type II hypersensitivity occurs when antibodies bind to antigens on the patient's own cells, marking them for destruction. This is also called antibody-dependent (or cytotoxic) hypersensitivity, and is mediated by IgG and IgM antibodies.
Immune complexes (aggregations of antigens, complement proteins, and IgG and IgM antibodies) deposited in various tissues trigger Type III hypersensitivity reactions. Type IV hypersensitivity (also known as cell-mediated or "delayed type hypersensitivity") usually takes between two and three days to develop. Type IV reactions are involved in many autoimmune and infectious diseases, but may also involve "contact dermatitis" (poison ivy). These reactions are mediated by T cells, monocytes, and macrophages.
Other mechanisms and evolution.
It is likely that a multicomponent, adaptive immune system arose with the first vertebrates, as invertebrates do not generate lymphocytes or an antibody-based humoral response. Many species, however, utilize mechanisms that appear to be precursors of these aspects of vertebrate immunity. Immune systems appear even in the structurally most simple forms of life, with bacteria using a unique defense mechanism, called the restriction modification system to protect themselves from viral pathogens, called bacteriophages. Prokaryotes also possess acquired immunity, through a system that uses CRISPR sequences to retain fragments of the genomes of phage that they have come into contact with in the past, which allows them to block virus replication through a form of RNA interference. Offensive elements of the immune systems are also present in unicellular eukaryotes, but studies of their roles in defense are few.
Pattern recognition receptors are proteins used by nearly all organisms to identify molecules associated with pathogens. Antimicrobial peptides called defensins are an evolutionarily conserved component of the innate immune response found in all animals and plants, and represent the main form of invertebrate systemic immunity. The complement system and phagocytic cells are also used by most forms of invertebrate life. Ribonucleases and the RNA interference pathway are conserved across all eukaryotes, and are thought to play a role in the immune response to viruses.
Unlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.
Tumor immunology.
Another important role of the immune system is to identify and eliminate tumors. The "transformed cells" of tumors express antigens that are not found on normal cells. To the immune system, these antigens appear foreign, and their presence causes immune cells to attack the transformed tumor cells. The antigens expressed by tumors have several sources; some are derived from oncogenic viruses like human papillomavirus, which causes cervical cancer, while others are the organism's own proteins that occur at low levels in normal cells but reach high levels in tumor cells. One example is an enzyme called tyrosinase that, when expressed at high levels, transforms certain skin cells (e.g. melanocytes) into tumors called melanomas. A third possible source of tumor antigens are proteins normally important for regulating cell growth and survival, that commonly mutate into cancer inducing molecules called oncogenes.
The main response of the immune system to tumors is to destroy the abnormal cells using killer T cells, sometimes with the assistance of helper T cells. Tumor antigens are presented on MHC class I molecules in a similar way to viral antigens. This allows killer T cells to recognize the tumor cell as abnormal. NK cells also kill tumorous cells in a similar way, especially if the tumor cells have fewer MHC class I molecules on their surface than normal; this is a common phenomenon with tumors. Sometimes antibodies are generated against tumor cells allowing for their destruction by the complement system.
Clearly, some tumors evade the immune system and go on to become cancers. Tumor cells often have a reduced number of MHC class I molecules on their surface, thus avoiding detection by killer T cells. Some tumor cells also release products that inhibit the immune response; for example by secreting the cytokine TGF-β, which suppresses the activity of macrophages and lymphocytes. In addition, immunological tolerance may develop against tumor antigens, so the immune system no longer attacks the tumor cells.
Paradoxically, macrophages can promote tumor growth when tumor cells send out cytokines that attract macrophages, which then generate cytokines and growth factors that nurture tumor development. In addition, a combination of hypoxia in the tumor and a cytokine produced by macrophages induces tumor cells to decrease production of a protein that blocks metastasis and thereby assists spread of cancer cells.
Physiological regulation.
Hormones can act as immunomodulators, altering the sensitivity of the immune system. For example, female sex hormones are known immunostimulators of both adaptive and innate immune responses. Some autoimmune diseases such as lupus erythematosus strike women preferentially, and their onset often coincides with puberty. By contrast, male sex hormones such as testosterone seem to be immunosuppressive. Other hormones appear to regulate the immune system as well, most notably prolactin, growth hormone and vitamin D.
When a T-cell encounters a foreign pathogen, it extends a vitamin D receptor. This is essentially a signaling device that allows the T-cell to bind to the active form of vitamin D, the steroid hormone calcitriol. T-cells have a symbiotic relationship with vitamin D. Not only does the T-cell extend a vitamin D receptor, in essence asking to bind to the steroid hormone version of vitamin D, calcitriol, but the T-cell expresses the gene CYP27B1, which is the gene responsible for converting the pre-hormone version of vitamin D, calcidiol into the steroid hormone version, calcitriol. Only after binding to calcitriol can T-cells perform their intended function. Other immune system cells that are known to express CYP27B1 and thus activate vitamin D calcidiol, are dendritic cells, keratinocytes and macrophages.
It is conjectured that a progressive decline in hormone levels with age is partially responsible for weakened immune responses in aging individuals. Conversely, some hormones are regulated by the immune system, notably thyroid hormone activity. The age-related decline in immune function is also related to decreasing vitamin D levels in the elderly. As people age, two things happen that negatively affect their vitamin D levels. First, they stay indoors more due to decreased activity levels. This means that they get less sun and therefore produce less cholecalciferol via UVB radiation. Second, as a person ages the skin becomes less adept at producing vitamin D.
Sleep and Rest.
The immune system is affected by sleep and rest, and sleep deprivation is detrimental to immune function. Complex feedback loops involving cytokines, such as interleukin-1 and tumor necrosis factor-α produced in response to infection, appear to also play a role in the regulation of non-rapid eye movement (REM) sleep. Thus the immune response to infection may result in changes to the sleep cycle, including an increase in slow-wave sleep relative to REM sleep.
When suffering from sleep deprivation, active immunizations may have a diminished effect and may result in lower antibody production, and a lower immune response, than would be noted in a well-rested individual. Additionally, proteins such as NFIL3, which have been shown to be closely intertwined with both T-cell differentiation and our circadian rhythms, can be affected through the disturbance of natural light and dark cycles through instances of sleep deprivation, shift work, etc. As a result these disruptions can lead to an increase in chronic conditions such as heart disease, chronic pain, and asthma.
In addition to the negative consequences of sleep deprivation, sleep and the intertwined circadian system have been shown to have strong regulatory effects on immunological functions affecting both the innate and the adaptive immunity. First, during the early slow-wave-sleep stage, a sudden drop in blood levels of cortisol, epinephrine, and norepinephrine induce increased blood levels of the hormones leptin, pituitary growth hormone, and prolactin. These signals induce a pro-inflammatory state through the production of the pro-inflammatory cytokines interleukin-1, interleukin-12, TNF-alpha and IFN-gamma. These cytokines then stimulate immune functions such as immune cells activation, proliferation, and differentiation. It is during this time that undifferentiated, or less differentiated, like naïve and central memory T cells, peak (i.e. during a time of a slowly evolving adaptive immune response). In addition to these effects, the milieu of hormones produced at this time (leptin, pituitary growth hormone, and prolactin) support the interactions between APCs and T-cells, a shift of the Th1/Th2 cytokine balance towards one that supports Th1, an increase in overall Th cell proliferation, and naïve T cell migration to lymph nodes. This milieu is also thought to support the formation of long-lasting immune memory through the initiation of Th1 immune responses.
In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.
Nutrition and diet.
Overnutrition is associated with diseases such as diabetes and obesity, which are known to affect immune function. More moderate malnutrition, as well as certain specific trace mineral and nutrient deficiencies, can also compromise the immune response.
Foods rich in certain fatty acids may foster a healthy immune system. Likewise, fetal undernourishment can cause a lifelong impairment of the immune system.
Manipulation in medicine.
The immune response can be manipulated to suppress unwanted responses resulting from autoimmunity, allergy, and transplant rejection, and to stimulate protective responses against pathogens that largely elude the immune system (see immunization) or cancer.
Immunosuppression.
Immunosuppressive drugs are used to control autoimmune disorders or inflammation when excessive tissue damage occurs, and to prevent transplant rejection after an organ transplant.
Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine.
Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.
Immunostimulation.
Cancer immunotherapy covers the medical ways to stimulate the immune system to attack cancer tumours.
Predicting immunogenicity.
Larger drugs (>500 Da) can provoke a neutralizing immune response, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids; however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set. A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells. The emerging field of bioinformatics-based studies of immunogenicity is referred to as "immunoinformatics". Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.
Manipulation by pathogens.
The success of any pathogen depends on its ability to elude host immune responses. Therefore, pathogens evolved several methods that allow them to successfully infect a host, while evading detection or destruction by the immune system. Bacteria often overcome physical barriers by secreting enzymes that digest the barrier, for example, by using a type II secretion system. Alternatively, using a type III secretion system, they may insert a hollow tube into the host cell, providing a direct route for proteins to move from the pathogen to the host. These proteins are often used to shut down host defenses.
An evasion strategy used by several pathogens to avoid the innate immune system is to hide within the cells of their host (also called intracellular pathogenesis). Here, a pathogen spends most of its life-cycle inside host cells, where it is shielded from direct contact with immune cells, antibodies and complement. Some examples of intracellular pathogens include viruses, the food poisoning bacterium "Salmonella" and the eukaryotic parasites that cause malaria ("Plasmodium falciparum") and leishmaniasis ("Leishmania spp."). Other bacteria, such as "Mycobacterium tuberculosis", live inside a protective capsule that prevents lysis by complement. Many pathogens secrete compounds that diminish or misdirect the host's immune response. Some bacteria form biofilms to protect themselves from the cells and proteins of the immune system. Such biofilms are present in many successful infections, e.g., the chronic "Pseudomonas aeruginosa" and "Burkholderia cenocepacia" infections characteristic of cystic fibrosis. Other bacteria generate surface proteins that bind to antibodies, rendering them ineffective; examples include "Streptococcus" (protein G), "Staphylococcus aureus" (protein A), and "Peptostreptococcus magnus" (protein L).
The mechanisms used to evade the adaptive immune system are more complicated. The simplest approach is to rapidly change non-essential epitopes (amino acids and/or sugars) on the surface of the pathogen, while keeping essential epitopes concealed. This is called antigenic variation. An example is HIV, which mutates rapidly, so the proteins on its viral envelope that are essential for entry into its host target cell are constantly changing. These frequent changes in antigens may explain the failures of vaccines directed at this virus. The parasite "Trypanosoma brucei" uses a similar strategy, constantly switching one type of surface protein for another, allowing it to stay one step ahead of the antibody response. Masking antigens with host molecules is another common strategy for avoiding detection by the immune system. In HIV, the envelope that covers the virion is formed from the outermost membrane of the host cell; such "self-cloaked" viruses make it difficult for the immune system to identify them as "non-self" structures.

</doc>
<doc id="14959" url="http://en.wikipedia.org/wiki?curid=14959" title="Immunology">
Immunology

Immunology is a branch of biomedical science that covers the study of all aspects of the immune system in all organisms. It deals with the physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (autoimmune diseases, hypersensitivities, immune deficiency, transplant rejection); the physical, chemical and physiological characteristics of the components of the immune system "in vitro", "in situ" and "in vivo". Immunology has applications in several disciplines of science, and as such is further divided.
Even before the concept of immunity (from "immunis", Latin for "exempt") was developed, numerous early physicians characterized organs that would later prove to be part of the immune system. The key primary lymphoid organs of the immune system are the thymus and bone marrow, and secondary lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and skin and liver. When health conditions warrant, immune system organs including the thymus, spleen, portions of bone marrow, lymph nodes and secondary lymphatic tissues can be surgically excised for examination while patients are still alive.
Many components of the immune system are actually cellular in nature and not associated with any specific organ but rather are embedded or circulating in various tissues located throughout the body.
Classical immunology.
Classical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.
The study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.
The humoral (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B lymphocytes, while antigens are defined as anything that elicits the generation of antibodies ("anti"body "gen"erators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.
Immunological research continues to become more specialized, pursuing non-classical models of immunity and functions of cells, organs and systems not previously associated with the immune system (Yemeserach 2010).
Clinical immunology.
Clinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.
The diseases caused by disorders of the immune system fall into two broad categories:
Other immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.
The most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ ("helper") T cells, dendritic cells and macrophages by the Human Immunodeficiency Virus (HIV).
Clinical immunologists also study ways to prevent the immune system's attempts to destroy allografts (transplant rejection).
Developmental immunology.
The body’s capability to react to antigen depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child’s immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like "Staphylococcus" and "Pseudomonas". In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T cells. Also, T cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B cells develop early during gestation but are not fully active.
Maternal factors also play a role in the body’s immune response. At birth, most of the immunoglobulin present is maternal IgG. Because IgM, IgD, IgE and IgA don’t cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six to nine months after birth, a child’s immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.
During adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-β-oestradiol (an oestrogen) and, in males, is testosterone. Oestradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids act directly not only on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.
The female sex hormone 17-β-oestradiol has been shown to regulate the level of immunological response, while some male androgens such as testosterone seem to suppress the stress response to infection. Other androgens, however, such as DHEA, increase immune response. As in females, the male sex hormones seem to have more control of the immune system during puberty and post-puberty than during the rest of a male's adult life.
Physical changes during puberty such as thymic involution also affect immunological response.
Immunotherapy.
The use of immune system components to treat a disease or disorder is known as immunotherapy. Immunotherapy is most commonly used in the context of the treatment of cancers together with chemotherapy (drugs) and radiotherapy (radiation). However, immunotherapy is also often used in the immunosuppressed (such as HIV patients) and people suffering from other immune deficiencies or autoimmune diseases.
Like IL2,IL10,GM-CSF B,INF a .
Diagnostic immunology.
The specificity of the bond between antibody and antigen has made it an excellent tool in the detection of substances in a variety of diagnostic techniques. Antibodies specific for a desired antigen can be conjugated with an isotopic (radio) or fluorescent label or with a color-forming enzyme in order to detect it. However, the similarity between some antigens can lead to false positives and other errors in such tests by antibodies cross-reacting with antigens that aren't exact matches.
Cancer immunology.
The study of the interaction of the immune system with cancer cells can lead to diagnostic tests and therapies with which to find and fight cancer.
Reproductive immunology.
This area of the immunology is devoted to the study of immunological aspects of the reproductive process including fetus acceptance. The term has also been used by fertility clinics to address fertility problems, recurrent miscarriages, premature deliveries and dangerous complications such as pre-eclampsia.
Theoretical immunology.
Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held, among others, by Robert Koch and Emil von Behring, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.
In the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.
More recently, several theoretical frameworks have been suggested in immunology, including "autopoietic" views, "cognitive immune" views, the "danger model" (or "danger theory", and the "discontinuity" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.
Immunologist.
According to the American Academy of Allergy, Asthma, and Immunology (AAAAI), "an immunologist is a research scientist who investigates the immune system of vertebrates (including the human immune system). Immunologists include research scientists (PhDs) who work in laboratories. Immunologists also include physicians who, for example, treat patients with immune system disorders. Some immunologists are physician-scientists who combine laboratory research with patient care."
Career in immunology.
Bioscience is the overall major in which undergraduate students who are interested in general well-being take in college. Immunology is a branch of bioscience for undergraduate programs but the major gets specified as students move on for graduate program in immunology. The aim of immunology is to study the health of humans and animals through effective yet consistent research, (AAAAI, 2013). The most important thing about being immunologists is the research because it is the biggest portion of their jobs.
Most graduate immunology schools follow the AAI courses immunology which are offered throughout numerous schools in the U.S. For example, in New York State, there are several universities that offer the AAI courses immunology: Albany Medical College, Cornell University, Icahn School of Medicine at Mount Sinai, New York University Langone Medical Center, University at Albany (SUNY), University at Buffalo (SUNY), University of Rochester Medical Center and Upstate Medical University (SUNY). The AAI immunology courses include an Introductory Course and an Advance Course. The Introductory Course is a course that gives students an overview of the basics of immunology.
In addition, this Introductory Course gives students more information to complement general biology or science training. It also has two different parts: Part I is an introduction to the basic principles of immunology and Part II is a clinically-oriented lecture series. On the other hand, the Advanced Course is another course for those who are willing to expand or update their understanding of immunology. It is advised for students who want to attend the Advanced Course to have a background of the principles of immunology. Most schools require students to take electives in other to complete their degrees. A Master’s degree requires two years of study following the attainment of a Bachelor’s degree. For a Doctoral or Ph.D. program it is required to take two additional years of study.
The expectation of occupational growth in immunology is an increase of 36 percent from 2010 to 2020. The median annual wage was $76,700 in May 2010. However, the lowest 10 percent of immunologists earned less than $41,560, and the top 10 percent earned more than $142,800, (Bureau of Labor Statistics, 2013). The practice of immunology itself is not specified by the U.S. Department of Labor but it belongs to the practice of life science in general.

</doc>
<doc id="14960" url="http://en.wikipedia.org/wiki?curid=14960" title="IPA (disambiguation)">
IPA (disambiguation)

IPA commonly refers to:
IPA may also refer to:

</doc>
<doc id="14961" url="http://en.wikipedia.org/wiki?curid=14961" title="Ice beer">
Ice beer

Ice beer is a marketing term for pale lager beer brands which have undergone some degree of fractional freezing somewhat similar to the German Eisbock production method. These brands generally have higher alcohol content than typical beer and generally have a low price relative to their alcohol content.
Process.
The process of "icing" beer involves lowering the temperature of a batch of beer until ice crystals form. Since alcohol has a much lower freezing point (-114 °C; -173.2 °F) than water and doesn't form crystals, when the ice is filtered off, the alcohol concentration increases. The process is known as "fractional freezing" or "freeze distillation".<REF></ref>
History.
Eisbock was developed in the Kulmbach region of Germany by brewing a strong, dark lager, then freezing the beer and removing some of the ice. This would concentrate the aroma and taste of the beer, and also raise the alcoholic strength of the finished beer.
Eisbock was introduced to Canada in 1989 by the microbrewery Niagara Falls Brewing Company. The brewers started with a strong dark lager (15.3 degrees Plato/1.061 original gravity, 6% alcohol by volume), then used the traditional German method of freezing and removing ice to concentrate aroma and flavours while increasing the alcoholic strength to 8% abv. Niagara Falls Eisbock was released annually as a seasonal winter beer; each year the label would feature a different historic view of the nearby Niagara Falls in the winter. This continued each year until the company was sold in 1994. 
Despite this precedent, the large Canadian brewer Molson (now part of Molson Coors) claimed to have made the first ice beer in North America when it introduced "Canadian Ice" in April 1993. However, Molson's main competitor in Canada, Labatt (now part of Anheuser-Busch InBev) claimed to have patented the ice beer process earlier. When Labatt introduced an ice beer in August 1993, capturing a 10% market share in Canada, this instigated the so-called "Ice Beer Wars" of the 1990s. 
Both companies use a similar process of freezing the beer and removing ice crystals to raise alcoholic content. Labatt's "Maximum Ice," for example, is 7.1% abv. 
Miller acquired the U.S. marketing and distribution rights to Molson's products, and first introduced the Molson product in the United States in August 1993 as "Molson Ice". Miller also introduced the "Icehouse" brand under the "Plank Road Brewery" brand name shortly thereafter, and it is still sold nationwide.
Anheuser-Busch introduced "Bud Ice" (5.5% abv) in 1994 and it remains one of the country's top-selling ice beers. "Bud Ice" has a somewhat lower alcohol content than most other ice beer brands. In 1995, Anheuser-Busch also introduced two other major brands: "Busch Ice" (5.9% abv, introduced 1995) and "Natural Ice" (also 5.9% abv, also introduced in 1995). "Natural Ice" is the No. 1 selling ice beer brand in the United States; its low price makes it very popular on college campuses all over the country.
Characteristics and regulation.
The ice beers are typically known for their high alcohol-to-dollar ratio. In some areas, a substantial number of ice beer products are considered to often be bought by "street drunks", and are prohibited for sale. For example, most of the products that are explicitly listed as prohibited in the beer and malt liquor category in the Seattle area are ice beers.

</doc>
<doc id="14962" url="http://en.wikipedia.org/wiki?curid=14962" title="Identity element">
Identity element

In mathematics, an identity element (or neutral element) is a special type of element of a set with respect to a binary operation on that set. It leaves other elements unchanged when combined with them. This is used for groups and related concepts.
The term "identity element" is often shortened to "identity" (as will be done in this article) when there is no possibility of confusion.
Let ("S", ∗) be a set S with a binary operation ∗ on it (known as a magma). Then an element e of S is called a left identity if "e" ∗ "a" = "a" for all a in S, and a right identity if "a" ∗ "e" = "a" for all a in S. If e is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.
An identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). The distinction is used most often for sets that support both binary operations, such as rings. The multiplicative identity is often called the unit in the latter context, where, though, a unit is often used in a broader sense, to mean an element with a multiplicative inverse.
Properties.
As the last example (a semigroup) shows, it is possible for ("S", ∗) to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if l is a left identity and r is a right identity then "l" = "l" ∗ "r" = "r". In particular, there can never be more than one two-sided identity. If there were two, e and f, then "e" ∗ "f" would have to be equal to both e and f.
It is also quite possible for ("S", ∗) to have "no" identity element. A common example of this is the cross product of vectors. The absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied – so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive semigroup of positive natural numbers.

</doc>
<doc id="14963" url="http://en.wikipedia.org/wiki?curid=14963" title="International Tropical Timber Agreement, 1983">
International Tropical Timber Agreement, 1983

The International Tropical Timber Agreement (ITTA, 1983) is an agreement to provide an effective framework for cooperation between tropical timber producers and consumers and to encourage the development of national policies aimed at sustainable utilization and conservation of tropical forests and their genetic resources.
The International Tropical Timber Organization was established under this agreement.
Opened for signature - November 18, 1983
Entered into force - April 1, 1985; this agreement expired when the International Tropical Timber Agreement, 1994, went into force.
Parties.
Fifty eight parties signed up to the agreement:
Australia, Austria, Belgium, Bolivia, Brazil, Burma, Cameroon, Canada, People's Republic of China, Colombia, Democratic Republic of the Congo, Republic of the Congo, Côte d'Ivoire, Denmark, Ecuador, Egypt, European Union, Fiji, Finland, France, Gabon, Germany, Ghana, Greece, Guyana, Honduras, India, Indonesia, Ireland, Italy, Japan, South Korea, Liberia, Luxembourg, Malaysia, Nepal, Netherlands, New Zealand, Norway, Panama, Papua New Guinea, Peru, Philippines, Portugal, Russia, Spain, Sweden, Switzerland, Thailand, Togo, Trinidad and Tobago, United Kingdom, United States, Venezuela
References.
 This article incorporates public domain material from the document .

</doc>
<doc id="14964" url="http://en.wikipedia.org/wiki?curid=14964" title="International Tropical Timber Agreement, 1994">
International Tropical Timber Agreement, 1994

International Tropical Timber Agreement, 1994 (ITTA, 1994) was drafted to ensure that by the year 2000 exports of tropical timber originated from sustainably managed sources and to establish a fund to assist tropical timber producers in obtaining the resources necessary to reach this objective.
It defined the mandate of the International Tropical Timber Organization.
The agreement was opened for signature on January 26, 1994, and entered into force on January 1, 1997.
It replaced the International Tropical Timber Agreement, 1983, and was superseded by the International Tropical Timber Agreement, 2006.
Parties.
Sixty-two parties ultimately ratified the agreement:
Australia, Austria, Belgium, Bolivia, Brazil, Burma, Cambodia, Cameroon, Canada, Central African Republic, People's Republic of China, Colombia, Democratic Republic of the Congo, Republic of the Congo, Côte d'Ivoire, Denmark, Ecuador, Egypt, European Union, Fiji, Finland, France, Gabon, Germany, Ghana, Greece, Guatemala, Guyana, Honduras, India, Indonesia, Ireland, Italy, Japan, South Korea, Liberia, Luxembourg, Malaysia, Mexico, Nepal, Netherlands, New Zealand, Nigeria, Norway, Panama, Papua New Guinea, Peru, Philippines, Poland, Portugal, Spain, Suriname, Sweden, Switzerland, Thailand, Togo, Trinidad and Tobago, United Kingdom, United States, Uruguay, Vanuatu, Venezuela
References.
 This article incorporates public domain material from the document .

</doc>
<doc id="14967" url="http://en.wikipedia.org/wiki?curid=14967" title="Instrumental">
Instrumental

An instrumental is a musical composition or recording without lyrics, or singing, although it might include some inarticulate vocal input; the music is primarily or exclusively produced by musical instruments.
In a song that is otherwise sung, a section not sung but played with instruments can be called an instrumental interlude. If the instruments are percussion instruments, the interlude can be called a percussion interlude. These interludes are a form of break in the song.
In popular music.
In commercial popular music, instrumental tracks are sometimes renderings of a corresponding release that features vocals, but they may also be compositions originally conceived without vocals. An instrumental version of a song which otherwise features vocals is also known as a -1 (pronounced "minus one").
The opposite of instrumental music is a cappella.
For genres in which a non-vocal song or interlude is conceived using electronic media, rather than with true musical instruments, the term instrumental is nonetheless used for it.
Borderline cases.
Some recordings which include brief examples of the human voice are typically considered instrumentals. Examples include singles with the following:
A few songs categorized as instrumentals may even include actual vocals, if they appear only as a short part of an extended piece (e.g., "Unchained Melody" (Les Baxter) or "TSOP (The Sound of Philadelphia)" or "Pick Up the Pieces" or "The Hustle" or "Fly, Robin, Fly" or "Do It Any Way You Wanna" or "Gonna Fly Now" (Bill Conti)). Falling just outside that definition is "Theme From Shaft" by Isaac Hayes.

</doc>
<doc id="14968" url="http://en.wikipedia.org/wiki?curid=14968" title="Regular icosahedron">
Regular icosahedron

In geometry, a regular icosahedron ( or ) is a convex polyhedron with 20 faces, 30 edges and 12 vertices. It is one of the five Platonic solids.
It has five equilateral triangular faces meeting at each vertex. It is represented by its Schläfli symbol {3,5}, or sometimes by its vertex figure as 3.3.3.3.3 or 35. It is the dual of the dodecahedron, which is represented by {5,3}, having three pentagonal faces around each vertex.
A regular icosahedron is a gyroelongated pentagonal bipyramid and a biaugmented pentagonal antiprism in any of six orientations.
The name comes from " "είκοσι" (eíkosi)", meaning "twenty", and " "εδρα" (hédra)", meaning "seat". The plural can be either "icosahedrons" or "icosahedra" (-).
Dimensions.
If the edge length of a regular icosahedron is "a", the radius of a circumscribed sphere (one that touches the icosahedron at all vertices) is
and the radius of an inscribed sphere (tangent to each of the icosahedron's faces) is
while the midradius, which touches the middle of each edge, is
where "φ" (also called "τ") is the golden ratio.
Area and volume.
The surface area "A" and the volume "V" of a regular icosahedron of edge length "a" are:
The latter is times the volume of a general tetrahedron with apex at the center of the
inscribed sphere, where the volume of the tetrahedron is one third times the base area √3a2/4 times its height ri.
The volume filling factor of the circumscribed sphere is
Cartesian coordinates.
The following Cartesian coordinates define the vertices of an icosahedron with edge-length 2, centered at the origin:
where "φ"=formula_7 is the golden ratio (also written "τ"). Note that these vertices form five sets of three concentric, mutually orthogonal golden rectangles, whose edges form Borromean rings.
If the original icosahedron has edge length 1, its dual dodecahedron has edge length formula_8, one divided by the golden ratio.
The 12 edges of a regular octahedron can be subdivided in the golden ratio so that the resulting vertices define a regular icosahedron. This is done by first placing vectors along the octahedron's edges such that each face is bounded by a cycle, then similarly subdividing each edge into the golden mean along the direction of its vector. The five octahedra defining any given icosahedron form a regular polyhedral compound, while the two icosahedra that can be defined in this way from any given octahedron form a uniform polyhedron compound.
Spherical coordinates.
The locations of the vertices of a regular icosahedron can be described using spherical coordinates, for instance as latitude and longitude. If two vertices are taken to be at the north and south poles (latitude ±90°), then the other ten vertices are at latitude ±arctan(1/2) ≈ ±26.57°. These ten vertices are at evenly spaced longitudes (36° apart), alternating between north and south latitudes.
This scheme takes advantage of the fact that the regular icosahedron is a pentagonal gyroelongated bipyramid, with D5d dihedral symmetry—that is, it is formed of two congruent pentagonal pyramids joined by a pentagonal antiprism.
Orthogonal projections.
The icosahedron has three special orthogonal projections, centered on a face, an edge and a vertex:
Spherical tiling.
The icosahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.
Construction by a system of equiangular lines.
The following construction of the icosahedron avoids tedious computations in the number field formula_9 necessary in more elementary approaches.
The existence of the icosahedron amounts to the existence of six equiangular lines in formula_10. Indeed, intersecting such a system of equiangular lines with a Euclidean sphere centered at their common intersection yields the twelve vertices of a regular icosahedron as can easily be checked. Conversely, supposing the existence of a regular icosahedron, lines defined by its six pairs of opposite vertices form an equiangular system.
In order to construct such an equiangular system, we start with this 6×6 square matrix:
A straightforward computation yields (where "I" is the 6×6 identity matrix). This implies that "A" has eigenvalues formula_12 and formula_13, both with multiplicity 3 since "A" is symmetric and of trace zero.
The matrix formula_14 induces thus a Euclidean structure on the quotient space formula_15 which is isomorphic to formula_10 since the kernel formula_17 of formula_18 has dimension 3. The image under the projection formula_19 of the six coordinate axes formula_20 in formula_21 forms thus a system of six equiangular lines in formula_10 intersecting pairwise at a common acute angle of formula_23. Orthogonal projection of ±"v"1, ..., ±"v"6 onto the formula_13-eigenspace of "A" yields thus the twelve vertices of the icosahedron.
A second straightforward construction of the icosahedron uses representation theory of the alternating group "A"5 acting by direct isometries on the icosahedron.
Symmetry.
The rotational symmetry group of the regular icosahedron is isomorphic to the alternating group on five letters. This non-abelian simple group is the only non-trivial normal subgroup of the symmetric group on five letters. Since the Galois group of the general quintic equation is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the Abel–Ruffini theorem uses this simple fact, and Felix Klein wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation, . See icosahedral symmetry: related geometries for further history, and related symmetries on seven and eleven letters.
The full symmetry group of the icosahedron (including reflections) is known as the full icosahedral group, and is isomorphic to the product of the rotational symmetry group and the group "C"2 of size two, which is generated by the reflection through the center of the icosahedron.
Stellations.
The icosahedron has a large number of stellations. According to specific rules defined in the book "The Fifty-Nine Icosahedra", 59 stellations were identified for the regular icosahedron. The first form is the icosahedron itself. One is a regular Kepler–Poinsot polyhedron. Three are regular compound polyhedra.
Facetings.
The small stellated dodecahedron, great dodecahedron, and great icosahedron are three facetings of the regular icosahedron. They share the same vertex arrangement. They all have 30 edges. The regular icosahedron and great dodecahedron share the same edge arrangement but differ in faces (triangles vs pentagons), as do the small stellated dodecahedron and great icosahedron (pentagrams vs triangles).
Geometric relations.
There are distortions of the icosahedron that, while no longer regular, are nevertheless vertex-uniform. These are invariant under the same rotations as the tetrahedron, and are somewhat analogous to the snub cube and snub dodecahedron, including some forms which are chiral and some with Th-symmetry, i.e. have different planes of symmetry from the tetrahedron.
The icosahedron is unique among the Platonic solids in possessing a dihedral angle not less than 120°. Its dihedral angle is approximately 138.19°. Thus, just as hexagons have angles not less than 120° and cannot be used as the faces of a convex regular polyhedron because such a construction would not meet the requirement that at least three faces meet at a vertex and leave a positive defect for folding in three dimensions, icosahedra cannot be used as the cells of a convex regular polychoron because, similarly, at least three cells must meet at an edge and leave a positive defect for folding in four dimensions (in general for a convex polytope in "n" dimensions, at least three facets must meet at a peak and leave a positive defect for folding in "n"-space). However, when combined with suitable cells having smaller dihedral angles, icosahedra can be used as cells in semi-regular polychora (for example the snub 24-cell), just as hexagons can be used as faces in semi-regular polyhedra (for example the truncated icosahedron). Finally, non-convex polytopes do not carry the same strict requirements as convex polytopes, and icosahedra are indeed the cells of the icosahedral 120-cell, one of the ten non-convex regular polychora.
An icosahedron can also be called a gyroelongated pentagonal bipyramid. It can be decomposed into a gyroelongated pentagonal pyramid and a pentagonal pyramid or into a pentagonal antiprism and two equal pentagonal pyramids.
Uniform colorings and subsymmetries.
There are 3 uniform colorings of the icosahedron. These colorings can be represented as 11213, 11212, 11111, naming the 5 triangular faces around each vertex by their color.
The icosahedron can be considered a snub tetrahedron, as snubification of a regular tetrahedron gives a regular icosahedron having chiral tetrahedral symmetry. It can also be constructed as an alternated truncated octahedron, having pyritohedral symmetry. The pyritohedral symmetry version is sometimes called a pseudoicosahedron, and is dual to the pyritohedron.
Uses and natural forms.
Biology.
Many viruses, e.g. herpes virus, have icosahedral shells. Viral structures are built of repeated identical protein subunits known as capsomeres, and the icosahedron is the easiest shape to assemble using these subunits. A "regular" polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome.
Various bacterial organelles with an icosahedral shape were also found. The icosahedral shell encapsulating enzymes and labile intermediates are built of different types of proteins with BMC domains.
In 1904, Ernst Haeckel described a number of species of Radiolaria, including "Circogonia icosahedra", whose skeleton is shaped like a regular icosahedron. A copy of Haeckel's illustration for this radiolarian appears in the article on regular polyhedra.
Chemistry.
The closo-carboranes are chemical compounds with shape very close to isosahedron. Icosahedral twinning also occurs in crystals, especially nanoparticles.
Many borides and allotropes of boron contain boron B12 icosahedron as a basic structure unit.
Toys and games.
In several roleplaying games, such as "Dungeons & Dragons", the twenty-sided die (d20 for short) is commonly used in determining success or failure of an action. This die is in the form of a regular icosahedron. It may be numbered from "0" to "9" twice (in which form it usually serves as a ten-sided die, or d10), but most modern versions are labeled from "1" to "20". See d20 System.
An icosahedron is the three-dimensional game board for Icosagame, formerly known as the Ico Crystal Game.
An icosahedron is used in the board game Scattergories to choose a letter of the alphabet. Six letters are omitted (Q, U, V, X, Y, and Z).
Inside a Magic 8-Ball, various answers to yes-no questions are inscribed on a regular icosahedron.
Others.
R. Buckminster Fuller and Japanese cartographer Shoji Sadao designed a world map in the form of an unfolded icosahedron, called the Fuller projection, whose maximum distortion is only 2%.
The "Sol de la Flor" light shade consists of twenty panels, which meet at the corners of an icosahedron in rosettes resembling the overlapping petals of a frangipani flower.
If each edge of an icosahedron is replaced by a one ohm resistor, the resistance between opposite vertices is 0.5 ohms, and that between adjacent vertices 11/30 ohms.
The company logo of the TDK Corporation contains a geometric figure which is based on the stellation diagram of the icosahedron.
An icosahedron was used for a logo for the Australian TV company; Grundy Television.
Icosahedral graph.
The skeleton of the icosahedron (the vertices and edges) form a graph. It is one of 5 Platonic graphs, each a skeleton of its Platonic solid.
The high degree of symmetry of the polygon is replicated in the properties of this graph, which is distance-transitive, distance-regular, and symmetric. The automorphism group has order 120. The vertices can be colored with 4 colors, the edges with 5 colors, and the diameter is 3.
The icosahedral graph is Hamiltonian: there is a cycle containing all the vertices. It is also a planar graph.
Related polyhedra and polytopes.
There are 4 related Johnson solids, including pentagonal faces with a subset of the 12 vertices:
The icosahedron can be transformed by a truncation sequence into its dual, the dodecahedron:
As a snub tetrahedron, and alternation of a truncated octahedron it also exists in the tetrahedral and octahedral symmetry families:
This polyhedron is topologically related as a part of sequence of regular polyhedra with Schläfli symbols {3,"n"}, continuing into the hyperbolic plane.
The regular icosahedron, seen as a "snub tetrahedron", is a member of a sequence of snubbed polyhedra and tilings with vertex figure (3.3.3.3."n") and Coxeter–Dynkin diagram . These figures and their duals have (n32) rotational symmetry, being in the Euclidean plane for n=6, and hyperbolic plane for any higher n. The series can be considered to begin with n=2, with one set of faces degenerated into digons.
The icosahedron can tessellate hyperbolic space in the order-3 icosahedral honeycomb, with 3 icosahedra around each edge, 12 icosahedra around each vertex, with Schläfli symbol {3,5,3}. It is one of four regular tessellations in the hyperbolic 3-space.
References.
</dl>

</doc>
<doc id="14971" url="http://en.wikipedia.org/wiki?curid=14971" title="Industrial archaeology of Dartmoor">
Industrial archaeology of Dartmoor

The industrial archaeology of Dartmoor covers a number of the industries which have, over the ages, taken place on Dartmoor, and the remaining evidence surrounding them. Currently only three industries are economically significant, yet all three will inevitably leave their own traces on the moor: china clay mining, farming and tourism.
A good general guide to the commercial activities on Dartmoor at the end of the 19th century is William Crossing's "The Dartmoor Worker".
Mining.
In former times, lead, silver, tin and copper were mined extensively on Dartmoor. The most obvious evidence of mining to the casual visitor to Dartmoor are the remains of the old engine-house at Wheal Betsy which is alongside the A386 road between Tavistock and Okehampton. The word "Wheal" has a particular meaning in Devon and Cornwall being either a tin or a copper mine, however in the case of Wheal Betsy it was principally lead and silver which were mined. 
Once widely practised by many miners across the moor, by the early 1900s only a few tinners remained, and mining had almost completely ceased twenty years later. Some of the more significant mines were Eylesbarrow, Knock Mine, Vitifer Mine and Hexworthy Mine. The last active mine in the Dartmoor area was Great Rock Mine, which shut down in 1969.
Quarrying.
Dartmoor granite has been used in many Devon and Cornish buildings. The prison at Princetown was built from granite taken from Walkhampton Common. When the horse tramroad from Plymouth to Princetown was completed in 1823, large quantities of granite were more easily transported.
There were three major granite quarries on the moor: Haytor, Foggintor and Merrivale. The granite quarries around Haytor were the source of the stone used in several famous structures, including the New London Bridge, completed in 1831. This granite was transported from the moor via the Haytor Granite Tramway, stretches of which are still visible.
The extensive quarries at Foggintor provided granite for the construction of London's Nelson's Column in the early 1840s, and New Scotland Yard was faced with granite from the quarry at Merrivale. Merrivale Quarry continued excavating and working its own granite until the 1970s, producing gravestones and agricultural rollers. Work at Merrivale continued until the 1990s, for the last 20 years imported stone such as gabbro from Norway and Italian marble was dressed and polished. The unusual pink granite at Great Trowlesworthy Tor was also quarried, and there were many other small granite quarries dotted around the moor. Various metamorphic rocks were also quarried in the metamorphic aureole around the edge of the moor, most notably at Meldon.
Gunpowder factory.
In 1844 a factory for making gunpowder was built on the open moor, not far from Postbridge. Gunpowder was needed for the tin mines and granite quarries then in operation on the moor. The buildings were widely spaced from one another for safety and the mechanical power for grinding ("incorporating") the powder was derived from waterwheels driven by a leat.
Now known as "Powdermills" or "Powder Mills", there are extensive remains of this factory still visible. Two chimneys still stand and the walls of the two sturdily-built incorporating mills with central waterwheels survive well: they were built with substantial walls but flimsy roofs so that in the event of an explosion, the force of the blast would be directed safely upwards. The ruins of a number of ancillary buildings also survive. A proving mortar—a type of small cannon used to gauge the strength of the gunpowder—used by the factory still lies by the side of the road to the nearby pottery.
Peat-cutting.
Peat-cutting for fuel occurred at some locations on Dartmoor until certainly the 1970s, usually for personal use. The right of Dartmoor commoners to cut peat for fuel is known as "turbary". These rights were conferred a long time ago, pre-dating most written records. The area once known as the "Turbary of Alberysheved" between the River Teign and the headwaters of the River Bovey is mentioned in the Perambulation of the Forest of Dartmoor of 1240 (by 1609 the name of the area had changed to Turf Hill).
An attempt was made to commercialise the cutting of peat in 1901 at Rattle Brook Head, however this quickly failed.
Warrens.
From at least the 13th century until early in the 20th, rabbits were kept on a commercial scale, both for their flesh and their fur. Documentary evidence for this exists in place names such as Trowlesworthy Warren (mentioned in a document dated 1272) and Warren House Inn. The physical evidence, in the form of pillow mounds is also plentiful, for example there are 50 pillow mounds at Legis Tor Warren. The sophistication of the warreners is shown by the existence of vermin traps that were placed near the warrens to capture weasels and stoats attempting to get at the rabbits.
The significance of the term "warren" nowadays is not what it once was. In the Middle Ages it was a privileged place, and the creatures of the warren were protected by the king 'for his princely delight and pleasure'.
The subject of warrening on Dartmoor was addressed in Eden Phillpotts' story "The River".
Farming.
Farming has been practised on Dartmoor since time immemorial. The dry-stone walls which separate fields and mark boundaries give an idea of the extent to which the landscape has been shaped by farming. There is little or no arable farming within the moor, mostly being given over to livestock farming on account of the thin and rocky soil. Some Dartmoor farms are remote in the extreme.

</doc>
<doc id="14972" url="http://en.wikipedia.org/wiki?curid=14972" title="Idempotence">
Idempotence

Idempotence ( ) is the property of certain operations in mathematics and computer science, that can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract algebra (in particular, in the theory of projectors and closure operators) and functional programming (in which it is connected to the property of referential transparency). The GET, PUT, and DELETE methods of the Hypertext Transfer Protocol (HTTP) computer protocol are a common example of idempotence.
The term was introduced by Benjamin Peirce in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means "(the quality of having) the same power", from "idem" + "potence" (same + power).
There are several meanings of idempotence, depending on what the concept is applied to:
Definitions.
Unary operation.
A unary operation formula_1, that is, a map from some set formula_2 into itself, is called idempotent if, for all formula_3 in formula_2,
In particular, the identity function formula_6, defined by formula_7, is idempotent, as is the constant function formula_8, where formula_9 is an element of formula_2, defined by formula_11.
An important class of idempotent functions is given by projections in a vector space. An example of a projection is the function formula_12 defined by formula_13, which projects an arbitrary point in 3D space to a point on the formula_14-plane, where the third coordinate (formula_15) is equal to 0.
A unary operation formula_16 is idempotent if it maps each element of formula_2 to a fixed point of formula_1. We can partition a set with formula_19 elements into formula_20 chosen fixed points and formula_21 non-fixed points, and then formula_22 is the number of different idempotent functions. Hence, taking into account all possible partitions,
is the total number of possible idempotent functions on the set. The integer sequence of the number of idempotent functions as given by the sum above for formula_24 starts with formula_25. (sequence in OEIS)
Neither the property of being idempotent nor that of being not is preserved under composition of unary functions. As an example for the former, "f"("x") = "x" mod 3 and "g"("x") = max("x",5) are both idempotent, but "f"∘"g" is not, although "g"∘"f" happens to be. As an example for the latter, the negation function ¬ on truth values isn't idempotent, but ¬∘¬ is.
Idempotent elements and binary operations.
Given a binary operation formula_26 on a set formula_2, an element formula_3 is said to be idempotent (with respect to formula_26) if:
In particular an identity element of formula_26, if it exists, is idempotent with respect to the operation formula_26.
The binary operation itself is called idempotent if every element of formula_2 is idempotent. That is, for all formula_34 when formula_35 denotes set membership:
For example, the operations of set union and set intersection are both idempotent, as are logical conjunction and logical disjunction, and, in general, the meet and join operations of a lattice.
Connections.
The connections between the three notions are as follows.
Common examples.
Functions.
As mentioned above, the identity map and the constant maps are always idempotent maps. The absolute value function of a real or complex argument, and the floor function of a real argument are idempotent.
The function that assigns to every subset formula_37 of some topological space formula_38" the closure of formula_37 is idempotent on the power set formula_40 of formula_38. It is an example of a closure operator; all closure operators are idempotent functions.
The operation of subtracting the average of a list of numbers from every number in the list is idempotent. For example, consider the numbers formula_42. The average formula_43 is formula_44. Subtracting 7 from every number in the list yields formula_45. The average formula_43 of that list is formula_47. Subtracting 0 from every number in that list yields the same list.
Formal languages.
The Kleene star and Kleene plus operators used to express repetition in formal languages are idempotent.
Idempotent ring elements.
An idempotent element of a ring is, by definition, an element that is idempotent for the ring's multiplication. That is, an element "a" is idempotent precisely when "a"2 = a.
Idempotent elements of rings yield direct decompositions of modules, and play a role in describing other homological properties of the ring. 
While "idempotent" usually refers to the multiplication operation of a ring, there are rings in which both operations are idempotent: Boolean algebras are such an example.
Other examples.
In Boolean algebra, both the logical and and the logical or operations are idempotent. This implies that every element of Boolean algebra is idempotent with respect to both of these operations. Specifically, formula_48 and formula_49 for all formula_3. 
In linear algebra, projections are idempotent. In fact, the projections of a vector space are exactly the idempotent elements of the ring of linear transformations of the vector space. After fixing a basis, it can be shown that the matrix of a projection with respect to this basis is an idempotent matrix.
An idempotent semiring (also sometimes called a "dioid") is a semiring whose "addition" (not multiplication) is idempotent. If both operations of the semiring are idempotent, then the semiring is called "doubly idempotent".
Computer science meaning.
In computer science, the term idempotent is used more comprehensively to describe an operation that will produce the same results if executed once or multiple times. This may have a different meaning depending on the context in which it is applied. In the case of methods or subroutine calls with side effects, for instance, it means that the modified state remains the same after the first call. In functional programming, though, an idempotent function is one that has the property "f"("f"("x")) = "f"("x") for any value "x".
This is a very useful property in many situations, as it means that an operation can be repeated or retried as often as necessary without causing unintended effects. With non-idempotent operations, the algorithm may have to keep track of whether the operation was already performed or not.
Examples.
Looking up some customer's name and address in a database are typically idempotent (in fact "nullipotent"), since this will not cause the database to change. Similarly, changing a customer's address is typically idempotent, because the final address will be the same no matter how many times it is submitted. However, placing an order for a car for the customer is typically not idempotent, since running the method/call several times will lead to several orders being placed. Canceling an order is idempotent, because the order remains canceled no matter how many requests are made.
A composition of idempotent methods or subroutines, however, is not necessarily idempotent if a later method in the sequence changes a value that an earlier method depends on – idempotence is not closed under composition. For example, suppose the initial value of a variable is 3 and there is a sequence that reads the variable, then changes it to 5, and then reads it again. Each step in the sequence is idempotent: both steps reading the variable have no side effects and changing a variable to 5 will always have the same effect no matter how many times it is executed. Nonetheless, executing the entire sequence once produces the output (3, 5), but executing it a second time produces the output (5, 5), so the sequence is not idempotent.
In the HyperText Transfer Protocol (HTTP), idempotence and safety are the major attributes that separate HTTP verbs. Of the major HTTP verbs, GET, PUT, and DELETE are idempotent (if implemented according to the standard), but POST is not. These verbs represent very abstract operations in computer science: GET retrieves a resource; PUT stores content at a resource; and DELETE eliminates a resource. As in the example above, reading data usually has no side effects, so it is idempotent (in fact nullipotent). Storing a given set of content is usually idempotent, as the final value stored remains the same after each execution. And deleting something is generally idempotent, as the end result is always the absence of the thing deleted.
In Event Stream Processing, idempotence refers to the ability of a system to produce the same outcome, even if an event or message is received more than once.
In a load-store architecture, instructions that might possibly cause a page fault are idempotent. So if a page fault occurs, the OS can load the page from disk and then simply re-execute the faulted instruction.
In a processor where such instructions are not idempotent, dealing with page faults is much more complex.
Applied examples.
Applied examples that many people could encounter in their day-to-day lives include elevator call buttons and crosswalk buttons. The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial activation and the request being satisfied have no effect.

</doc>
<doc id="14973" url="http://en.wikipedia.org/wiki?curid=14973" title="Ithaca, New York">
Ithaca, New York

The city of Ithaca is a city in central New York and the county seat of Tompkins County, as well as the largest community in the Ithaca-Tompkins County metropolitan area (which also contains the separate municipalities of the town of Ithaca, the village of Cayuga Heights, the village of Lansing and other towns and villages in Tompkins County). The city of Ithaca sits on the southern shore of Cayuga Lake, in Central New York. It is named for the Greek island of Ithaca.
Ithaca is home to Cornell University, an Ivy League school of over 20,000 students, most of whom study on Cornell’s Ithaca campus. Ithaca College is located just south of the city in the town of Ithaca, adding to Ithaca’s "college town" focus and atmosphere. Nearby is Tompkins Cortland Community College (TC3). These three colleges influence Ithaca's seasonal population. In 2010, the city's population was 30,014, and the metropolitan area had a population of 101,564.
Namgyal Monastery in Ithaca is the North American seat of Tenzin Gyatso, the 14th Dalai Lama.
History.
Early history.
The inhabitants of the Ithaca area at the time Europeans began arriving were the Saponi and Tutelo Indians, dependent tribes of the Cayuga Indians who were part of the Iroquois confederation. These tribes settled on Cayuga-controlled hunting lands at the south end of Cayuga Lake as well as in Pony (originally Sapony) Hollow of Newfield, New York, after being forced from North Carolina by European invasion. They were driven from the area by the Sullivan Expedition which destroyed the Tutelo village of Coregonal, located near the junction of state routes 13 and 13A just south of the Ithaca city limits. Indian presence in the current City of Ithaca was limited to a temporary hunting camp at the base of Cascadilla Gorge. The destruction of Iroquois confederation power opened the region to settlement by people of European origin, a process which began in 1789. In 1790, an official program began for distributing land in the area as a reward for service to the American soldiers of the Revolutionary War; most local land titles trace back to the Revolutionary war grants. Lots were drawn in 1791; informal settlement had already started.
Partition of the Military Tract.
As part of this process, the Central New York Military Tract, which included northern Tompkins County, was surveyed by Simeon DeWitt. His clerk Robert Harpur had a fondness for ancient Greek and Roman history as well as English authors and philosophers (as evidenced by the nearby townships of Dryden and Locke). The Commissioners of Lands of New York State (chairman Gov. George Clinton) followed Harpur's recommendations at a meeting in 1790. The Military Tract township in which proto-Ithaca was located he named the Town of Ulysses, the Latin form of the Greek Odysseus from Homer's Odyssey. A few years later DeWitt moved to Ithaca, then called variously "The Flats," "The City," or "Sodom," and named it for the Greek island home of Ulysses (still the surrounding township at the time — nowadays Ulysses is a separate town in Tompkins County). Contrary to popular myth, DeWitt did not name many of the classical references found in Upstate New York such as Syracuse and Troy; these were from the general classical fervor of the times.
Growth.
In the 1820s and 1830, Ithaca held high hopes of becoming a major city when the Ithaca and Owego Railroad was completed in 1832 to connect the Erie Canal navigation with the Susquehanna River to the south. In 1821, the village set itself off by incorporation at the same time the Town of Ithaca parted with the parent town of Ulysses. These hopes survived the depression of 1837 when the railroad was re-organized as the Cayuga & Susquehanna and re-engineered with switchbacks in the late 1840s; much of this route is now used by the South Hill Recreation Way. However, easier routes soon became available, such as the Syracuse, Binghamton & New York (1854). In the decade following the Civil War railroads were built from Ithaca to surrounding points (Geneva, New York; Cayuga, New York; Cortland, New York; Elmira, New York; Athens, Pennsylvania) mainly with financing from Ezra Cornell. However, the geography of the city has always prevented it from lying on a major transportation artery. When the Lehigh Valley Railroad built its main line from Pennsylvania to Buffalo in 1890 it bypassed Ithaca (running via eastern Schuyler County on easier grades), as the Delaware, Lackawanna and Western Railroad had done in the 1850s.
The late 19th century gave birth to the two major postsecondary educational institutions Ithaca has today. In 1865, Ezra Cornell founded Cornell University. It was opened as a coeducational institution. Women first enrolled in 1870. Ezra Cornell also established a public library for the city. Ithaca College was founded as the Ithaca Conservatory of Music in 1892. Ithaca College was originally located in the downtown area, but relocated to South Hill in the 1960s.
Ithaca became a city in 1888 and became a small manufacturing and retail center. The Ithaca Gun Factory opened in 1880. The largest industrial company (and associated building) in the area was Morse Chain, elements of which were absorbed into Emerson Power Transmission on South Hill and Borg Warner Automotive in Lansing, New York. After World War II, National Cash Register and the Langmuir Research Labs of General Electric were also major employers.
During the early 20th century, Ithaca was an important center in the silent film industry. These films often featured the local natural scenery. Many of these films were the work of Leopold Wharton and his brother Theodore Wharton in their studio on the site of what is now Stewart Park. After the film industry centralized in Hollywood, production in Ithaca effectively ceased. Few of the silent films made in Ithaca are preserved today.
Recent history.
For decades, the Ithaca Gun Company tested their shotguns behind the plant on Lake Street; the shot fell into Fall Creek (a tributary of Cayuga Lake) right at the base of Ithaca Falls. A major clean-up effort sponsored by the United States Superfund took place from 2002 to 2004. After many years of debate and environmental concerns, the old Ithaca Gun building has been dismantled and is scheduled to be replaced by an apartment complex.
The former Morse Chain company factory on South Hill, now owned by Emerson Power Transmission, was the site of extensive groundwater and soil contamination. Emerson Power Transmission has been working with the state and South Hill residents to determine the extent and danger of the contamination and aid in cleanup.
Geography and climate.
The valley in which Cayuga Lake is located is long and narrow with a north-south orientation. Ithaca is at the southern end (the "head") of the lake, but the valley continues to the southwest behind the city. Originally a river valley, it was deepened and widened by the action of Pleistocene ice sheets over the last several hundred thousand years. The lake, which drains to the north, formed behind a dam of glacial moraine. The rock is predominantly Devonian and, north of Ithaca, is relatively fossil rich. Glacial erratics can be found in the area. The world-renowned fossils found in this area can be examined at the Museum of the Earth.
Ithaca was founded on flat land just south of the lake — land that formed in fairly recent geological times when silt filled the southern end of the lake. The city ultimately spread to the adjacent hillsides, which rise several hundred feet above the central flats: East Hill, West Hill, and South Hill. Its sides are fairly steep, and a number of the streams that flow into the valley from east or west have cut deep canyons, usually with several waterfalls.
Ithaca experiences a moderate continental climate. Winters are long, cold, and snowy, with temperatures reaching 0 °F or lower on an average 9.9 nights annually and an average of 67 in of snow per season. The largest snowfall in one day was 26.0 in on February 14, 1914. Summers are warm and humid, with usually comfortable temperatures. Readings of 90 °F or higher occur on an average of just 5.2 days per year, and 100 °F+ temperatures have only occurred ten times since record-keeping began in 1893. The average date of the first freeze is October 5, and the average date of the last freeze is May 15, giving Ithaca a growing season of 141 days. The average date of the first and last snowfalls are November 12 and April 7, respectively. Extreme temperatures range from −25 °F as recently as February 2, 1961 up to 103 °F on July 9, 1936.
The valley flatland has slightly milder weather in winter, and occasionally Ithacans experience simultaneous snow on the hills and rain in the valley. The phenomenon of mixed precipitation (rain, wind, and snow), common in the late fall and early spring, is known tongue-in-cheek as "ithacation" to many of the local residents.
The natural vegetation of the Ithaca area, seen in areas unbuilt and unfarmed, is northern temperate broadleaf forest, dominated by deciduous trees.
Due to the microclimates created by the impact of the lakes, the region surrounding Ithaca (Finger Lakes American Viticultural Area) experiences a short but adequate growing season for winemaking. As such the region is home to many wineries.
Education.
Ithaca is a major educational center in Central New York. The city is home to Cornell University which overlooks the town from East Hill, and Ithaca College, situated on South Hill. The two schools bring a substantial student population, with about 21,000 students enrolled at Cornell and about 6,400 at Ithaca College. Tompkins Cortland Community College is located in the neighboring town of Dryden, New York, and has an extension center in downtown Ithaca. Empire State College offers non-traditional college courses to adults in downtown Ithaca.
The Ithaca City School District, which encompasses Ithaca and the surrounding area, enrolls about 5,500 K-12 students in eight elementary schools, two middle schools, Ithaca High School, and the Lehman Alternative Community School. There are also several private elementary and secondary schools in the Ithaca area, including Immaculate Conception School, the Cascadilla School, the New Roots Charter School, the Elizabeth Ann Clune Montessori School, and the Ithaca Waldorf School. Ithaca has two networks for supporting its home-schooling families: Loving Education At Home (LEAH) and the Northern Light Learning Center (NLLC).
Ithaca draws students from around the United States and the rest of the world with its various vocational schools and specialty institutes, such as the 1000-hour program at the Finger Lakes School of Massage. Ithaca's Suzuki school, Ithaca Talent Education, provides musical training for children of all ages and also teacher training for undergraduate and graduate-level students. The Community School of Music and Art uses an extensive scholarship system to offer classes and lessons to any student, regardless of age, background, economic status or artistic ability.
Economy.
The economy of Ithaca is based on education and manufacturing with high tech and tourism in strong supporting roles. As of 2006, Ithaca remains one of the few expanding economies in economically troubled New York State outside of New York City, and draws commuters from the neighboring rural counties of Cortland, Tioga, and Schuyler, as well as from the more urbanized Chemung County.
With some level of success, Ithaca has tried to maintain a traditional downtown shopping area that includes the Ithaca Commons pedestrian mall and Center Ithaca, a small mixed-use complex built at the end of the urban renewal era. Another commercial center, Collegetown, is located next to the Cornell campus. It features a number of restaurants, shops, and bars, and an increasing number of high rise apartments and is primarily frequented by Cornell University students.
Ithaca has many of the businesses characteristic of small American university towns: used bookstores, art house cinemas, craft stores, and vegetarian-friendly restaurants. The collective Moosewood Restaurant, founded in 1973, was the wellspring for a number of vegetarian cookbooks; Bon Appetit magazine ranked it among the thirteen most influential restaurants of the 20th century. Ithaca has many local restaurants and chains both in the city and town with a range of ethnic foods. The innovative and popular Ithaca Bakery chain, and the Ithaca Farmers Market also provide a range of foods.
Culture.
Ithacans support the Ithaca Farmers Market, professional theaters (Kitchen Theatre Company, Hangar Theatre, Icarus Theatre), a civic orchestra, much parkland, the Sciencenter, a hands-on science museum for people of all ages, an independent movie theater (Cinemapolis), and the Museum of the Earth. Ithaca is noted for its annual artistic celebration of community: The Ithaca Festival (and its parade), the Circus Eccentrithaca. The Constance Saltonstall Foundation for the Arts provides grants and Summer Fellowships at the Saltonstall Arts Colony for New York State artists and writers. Ithaca also hosts what is described as the third-largest used-book sale in the United States.
Other festivals occur annually, usually centered around food, music, and/or spirits. These include The Apple Festival in the fall, with many different varieties of apples and apple products; Chili Fest in February, a local contest involving many local restaurants who compete to make the best chili in several different categories; the Finger Lakes International Dragon Boat Festival in July; Porchfest in late September, which includes an eclectic mix of local musicians performing throughout the day on an array of porches in Fall Creek homes; and Ithaca Brew Fest in Stewart Park in September, usually featuring tastings of over 100 varieties of beer from regional, national, and international Craft brewers, food from local vendors, and music from local bands.
Ithaca has also pioneered the Ithaca Health Fund, a popular cooperative health insurance. Ithaca is also home to one of the United States' first local currency systems, Ithaca Hours, developed by Paul Glover (building on the pioneering work of Ralph Borsodi and Robert Swann).
It is claimed locally that in 1891, Rev. John M. Scott and a local druggist, Chester Platt, invented the ice cream sundae in Ithaca, though other cities make the same claim. The local Unitarian church, where Rev. Scott preached, has an annual "Sundae Sunday" every September in commemoration.
Music and musicians.
Ithaca is known for its resident musicians, who contribute to a music scene which is unusually talented and diverse for such a small town. These musicians have come from many backgrounds to pursue their careers in Ithaca; the School of Music at Ithaca College attracts talented musicians, some of whom remain in Ithaca after graduating and take up work as performing musicians or in the sound engineering field. Ithaca is the seat of the "Official Orchestra of the City of Ithaca", commonly known under the name Cayuga Chamber Orchestra.
Several notable musicians have relocated from other countries to Ithaca in order to begin their careers, most notably Samite of Uganda, Mamadou Diabaté of Mali and Malang Jobateh of Senegal. Other regionally, nationally and internationally known performers and musical groups that call Ithaca home include: Donna the Buffalo, The Burns Sisters, jazz cellist Hank Roberts, Johnny Dowd, Jimkata, John Brown's Body, Ayurveda, The Gunpoets, The Blind Spots, The Sim Redmond Band, Nate & Kate, The Horse Flies, Technicolor Trailer Park, Mike Brindisi & The New York Rock, Who You Are, Willie B, Kevin Kinsella, and X Ambassadors. The Spin Doctors began their career in Ithaca. Traditional folk music is a staple and is featured weekly on North America's longest running live folk concert broadcast WVBR 93.5 FM's Bound for Glory.
In the nearby village of Trumansburg, the Finger Lakes Grassroots Festival of Music and Dance is held every third week in July. Initiated as a benefit for Aids research at the State Theater in Ithaca by the band Donna the Buffalo, it has successfully occurred every year for the past 20 years. The Grassroots Festival has brought hundreds and hundreds of bands through the region, further enriching the local musical palate with every new introduction of musical style and culture. Several local bands call it home as either a figurative birthplace or a nurturing environment within which to develop new forms of music. Other notable local music festivals include the Ithaca Festival, Musefest, the Summertime Block Party, the Juneteenth Celebration and Rock the Arts.
Media.
The dominant local newspaper in Ithaca is a morning daily, The "Ithaca Journal", founded 1815. The paper is owned by Gannett, Inc., publishers of "USA Today". The alternative weekly newspaper "Ithaca Times" is distributed free of charge. Other area publications include "Tompkins Weekly", the "Ithaca Community News", "14850 Magazine", the "Cornell Daily Sun", the "Ithacan", and the "Tattler". (The latter three are run by student staffs at Cornell University, Ithaca College, and Ithaca High School, respectively.)
Ithaca is also home to several radio stations. WVBR 93.5 FM is associated with Cornell University in the sense that it is owned and predominantly staffed by an association composed of enrolled Cornell students; but it is an independent, financially self-supporting commercial station in the rock format playing a mix of modern and classic rock during the week and specialty shows on the weekend. WICB 91.7 FM is an award-winning, non-commercial, student-run station owned by Ithaca College. WPIE 1160 AM/107.1 FM "ESPN Ithaca" is a sports talk station locally owned by Taughannock Media with a transmitter in Trumansburg and translator in Ithaca. The Cayuga Radio Group, a subsidiary of Saga Communications, Inc., owns country WQNY "Q-Country" 103.7 FM, WYXL "Lite Rock" 97.3 FM, news/talk WHCU 870 AM, progressive talk WNYY 1470 AM, as well as classic rock "I-100" WIII 99.9 FM, with its main transmitter in Cortland and a repeating station at 100.3 FM in Ithaca. Saga also has lower-powered "translator" stations "Hits 103.3" and "98.7 The Vine" on the FM dial. WFIZ "Z95.5" is also in the area, broadcasting a top-40, CHR format. Classic rock "The Wall" WLLW 99.3 and 96.3, based in Seneca Falls, has a transmitter in Ithaca. There is also NPR and classical programming available on WSQG 90.9 FM, NPR/college programming on WEOS repeater 88.1 FM, and Christian music and talk Family Life Network on 88.9 FM.
Politics.
Politically, the city's population has a significant tilt toward liberalism and the Democratic Party. A November 2004 study by ePodunk lists it as New York's most liberal town. This contrasts with the more conservative leanings of the surrounding Upstate New York region, and is also somewhat more liberal than the rest of Tompkins County. In 1988, Jesse Jackson received the most votes in Ithaca in the Democratic Presidential primary. In 2000, Ralph Nader received more votes for President than George W. Bush in the City of Ithaca, and 11% county-wide. In 2008, Barack Obama, running against New York State's Senator Hillary Clinton, won Tompkins County in the Democratic Presidential Primary, the only county that he won in New York State. Obama went on to win Tompkins County (including Ithaca) by a wide margin of 41% over his opponent John McCain in the November 2008 election.
Local government.
The name Ithaca designates two governmental entities in the area, the Town of Ithaca and the City of Ithaca.
The Town of Ithaca is one of the nine towns comprising Tompkins County. (Towns in New York are something like townships in other states; every county outside New York City is subdivided into towns.) The City of Ithaca is surrounded by, but legally independent of, the Town. The Town of Ithaca contains the village of Cayuga Heights, a small incorporated upper-middle class suburb located to the northeast of the City of Ithaca.
The City of Ithaca has a mayor-council government. The charter of the City of Ithaca provides for a full-time mayor and city judge, each independent and elected at large. Since 1995, the mayor has been elected to a four-year term, and since 1989, the city judge has been elected to a six-year term. Since 1983, the city has been divided into five wards, each electing two members to the city council, known as the Common Council, for staggered four-year terms. In March 2015, the Common Council unanimously adopted a resolution recognizing freedom from domestic violence as a fundamental human right.
The Town government consists of an executive, the Town Supervisor, elected to a four-year term, and a Town Council of five members also elected for terms of four years.
The majority of local property taxes are actually assessed by an entirely independent agency with entirely different borders, the Ithaca City School District.
City–Town consolidation.
In December 2005, the City and Town governments began discussing opportunities for increased government consolidation, including the possibility of joining the two into a single entity. This topic had been previously discussed in 1963 and 1969.
The possibility of consolidation is controversial for Town residents who could be forced to pay higher taxes as they help shoulder the higher debt burden that the City has taken on. Some Town residents also worry that consolidation could lead to increased sprawl and traffic congestion. However, most of the Town's population is already concentrated in hamlets in proximity to the City's borders and Town residents take advantage of City amenities. Mayor Walter Lynn of Cayuga Heights called consolidation discussion a "waste of time."
Greater Ithaca.
The term "Greater Ithaca" encompasses both the City and Town of Ithaca, as well as several smaller settled places within or adjacent to the Town:
Demographics.
Ithaca is the larger principal city of the Ithaca-Cortland CSA, a Combined Statistical Area that includes the Ithaca metropolitan area (Tompkins County) and the Cortland micropolitan area (Cortland County), which had a combined population of 145,100 at the 2000 census.
As of the census of 2000, there were 29,287 people, 10,287 households, and 2,962 families residing in the city. The population density was 5,360.9 people per square mile (2,071.0/km²). There were 10,736 housing units at an average density of 1,965.2 per square mile (759.2/km²). The racial makeup of the city was 73.97% White, 13.65% Asian, 6.71% Black or African American, 0.39% Native American, 0.05% Pacific Islander, 1.86% from other races, and 3.36% from two or more races. Hispanic or Latino of any race were 5.31% of the population.
There were 10,287 households out of which 14.2% had children under the age of 18 living with them, 19.0% were married couples living together, 7.8% had a female householder with no husband present, and 71.2% were non-families. 43.3% of all households were made up of individuals and 7.4% had someone living alone who was 65 years of age or older. The average household size was 2.13 and the average family size was 2.81.
In the city the population was spread out with 9.2% under the age of 18, 53.8% from 18 to 24, 20.1% from 25 to 44, 10.6% from 45 to 64, and 6.3% who were 65 years of age or older. The median age was 22 years. For every 100 females there were 102.6 males. For every 100 females age 18 and over, there were 102.2 males.
The median income for a household in the city was $21,441, and the median income for a family was $42,304. Males had a median income of $29,562 versus $27,828 for females. The per capita income for the city was $13,408. About 13.2% of individuals and 4.2% of families were below the poverty line.
Transportation.
Roads.
Ithaca is in the rural Finger Lakes region about 225 mi northwest of New York City; the nearest larger cities, Binghamton and Syracuse, are an hour's drive away by car, Rochester and Scranton are two hours, Buffalo and Albany are three. New York City and Philadelphia are about four hours away. Cleveland, Boston, Washington DC, and Montreal are about five hours away.
Ithaca lies at over a half hour's drive from any interstate highway, and all car trips to Ithaca involve some driving on two-lane state rural highways. The city is at the convergence of many regional two-lane state highways: Routes 13, 13A, 34, 79, 89, 96, 96B, and 366. These are usually not congested except in Ithaca proper. However, Route 79 between the I-81 access at Whitney Point and Ithaca receives a significant amount of Ithaca-bound congestion right before Ithaca's colleges reopen after breaks.
There is frequent intercity bus service by Greyhound Lines, New York Trailways, and Shortline (Coach USA), particularly to Binghamton and New York City, with limited service to Rochester, Buffalo and Syracuse, and (via connections in Binghamton) to Utica and Albany. The bus station serving all these companies is the former Delaware, Lackawanna & Western railway station on Meadow St. between W State and W Seneca streets, a little over half a mile west of downtown Ithaca. Cornell University runs a premium Campus to Campus bus between its Ithaca campus and its medical school in New York City which is open to the public.
Ithaca is the center of an extensive bus public transportation network. TCAT, Inc (Tompkins Consolidated Area Transit, Inc.) is a not-for-profit corporation that provides public transportation for Tompkins County New York. TCAT was reorganized as a non-profit corporation in 2004 and is primarily supported locally by Cornell University, the City of Ithaca and Tompkins County. TCAT's ridership increased from 2.7 million in 2004 to 4.4 million in 2013. http://www.tcatbus.com/files/all/tcat_2013_yearbook_-_final.pdf TCAT operates 33 routes, many running seven days a week. It has frequent service to downtown, Cornell, Ithaca College, and the Shops at Ithaca Mall in the neighboring Town of Lansing, but less frequent service to many residential and rural areas, including Trumansburg and Newfield. Chemung County Transit (C-TRAN) runs weekday commuter service from Chemung County to Ithaca. Cortland Transit runs commuter service to Cornell University. Tioga County Public Transit operates three routes to Ithaca and Cornell, but will cease operating on November 30, 2014.
GADABOUT Transportation Services, Inc. provides demand-response paratransit service for seniors over 60 and people with disabilities. Ithaca Dispatch provides local and regional taxi service. In addition, Ithaca Airline Limousine and IthaCar Service connect to the local airports.
In July 2008, a non-profit called Ithaca Carshare began a carsharing service in Ithaca. Ithaca Carshare has a fleet of 23 vehicles shared by over 1300 members as of March 2013 and has become a popular service among both city residents and the college communities. Vehicles are located throughout Ithaca downtown and the two major institutions. With Ithaca Carshare as the first locally run carsharing organization in New York State, others have since launched in Buffalo and Syracuse. Independent studies have shown that for each Ithaca Carshare vehicle in the fleet, 15 fewer personally owner cars are owned.
Rideshare services to promote carpooling and vanpooling are operated by ZIMRIDE and VRIDE. A community mobility education program, Way2Go is operated by Cornell Cooperative Extension of Tompkins County. Way2Go's website provides consumer information and videos. Way2Go works collaboratively to help people save money, stress less, go green and improve mobility options. The 2-1-1 Tompkins/Cortland Help line connects people with services, including transportation, in the community, by telephone and web on a 24/7 basis. The information and referral service is operated by the Human Services Coalition of Tompkins County, Inc. Together, 2-1-1 Information and Referral and Way2Go are a one-call, one-click resource designed to mobility services information for Ithaca and throughout Tompkins County.
As a growing urban area, Ithaca is facing steady increases in levels of vehicular traffic on the city grid and on the state highways. Outlying areas have limited bus service, and many people consider a car essential. However, many consider Ithaca a walkable and bikeable community. One positive trend for the health of downtown Ithaca is the new wave of increasing urban density in and around the Ithaca Commons. Because the downtown area is the region's central business district, dense mixed-use development that includes housing may increase the proportion of people who can walk to work and recreation, and mitigate the likely increased pressure on already busy roads as Ithaca grows. The downtown area is also the area best served by frequent public transportation. Still, traffic congestion around the Commons is likely to progressively increase.
Airports.
Ithaca is served by Ithaca Tompkins Regional Airport, located about three miles to the northeast of the city center. US Airways Express offers flights to its hub at Philadelphia, operated by Piedmont Airlines using de Havilland Canada Dash 8 turboprop airliners. Delta Connection provides service to its hub at Detroit Metro airport, operated by Endeavor Air using Bombardier CRJ-200 airliners. United Express offers three daily flights to Newark Liberty International Airport, operated by CommutAir using Bombardier Dash 8 turboprops. Some residents choose to travel to Syracuse Hancock International Airport, Greater Binghamton Airport, Elmira-Corning Regional Airport or Greater Rochester International Airport for more airline service options.
Railways.
Norfolk Southern freight trains reach Ithaca from Sayre, Pennsylvania, mainly to deliver coal to AES Cayuga, a coal power plant (known as Milliken Station during NYSEG ownership) and haul out salt from the Cargill salt mine, both on the east shore of Cayuga Lake. There is no passenger rail service, although from the 1870s through the 1950s there were trains to Buffalo via Geneva, New York; to New York City via Wilkes-Barre, Pennsylvania (Lehigh Valley Railroad) and Scranton, Pennsylvania (DL&W); to Auburn, New York; and to the US northeast via Cortland, New York; service to Buffalo and New York City lasted until 1961. The Lehigh Valley's top New York City-Ithaca-Buffalo passenger train, "The Black Diamond", was optimistically publicized as 'The Handsomest Train in the World', perhaps to compensate for its roundabout route to Buffalo. It was named after the railroad's largest commodity, anthracite coal.
Ithaca was the fourth community in New York state with a street railway; streetcars ran from 1887 to summer 1935.
Reputation.
In addition to its liberal politics, Ithaca is commonly listed among the most culturally liberal of American small cities. The "Utne Reader" named Ithaca "America's most enlightened town" in 1997. According to ePodunk's Gay Index, Ithaca has a score of 231, versus a national average score of 100.
Like many small college towns, Ithaca has also received accolades for having a high overall quality of life. In 2004, "Cities Ranked and Rated" named Ithaca the best "emerging city" to live in the United States. In 2006, the Internet realty website "Relocate America" named Ithaca the fourth best city in the country to relocate to. In July 2006, Ithaca was listed as one of the "12 Hippest Hometowns for Vegetarians" by "VegNews Magazine" and chosen by "Mother Earth News" as one of the "12 Great Places You've Never Heard Of."
In 2012, the city was listed among the 10 best places to retire in the U.S. by U.S. News.
Ithaca was also ranked 13th among America's Best College Towns by "Travel + Leisure" in 2013 and ranked as the #1 Best College Town in America in the American Institute for Economic Research's 2013-2014 College Destination Index.
In its earliest years during frontier days, what is now Ithaca was briefly known by the names "The Flats" and "Sodom," the name of the Biblical city of sin, due to its reputation as a town of "notorious immorality", a place of horse racing, gambling, profanity, Sabbath breaking, and readily available liquor. These names did not last long; Simeon DeWitt renamed the town Ithaca in the early 19th century, though nearby Robert H. Treman State Park still contains Lucifer Falls.
In popular culture.
Movies/TV show.
See also The Whartons Studio for films shot in Ithaca prior to 1920.

</doc>
<doc id="14975" url="http://en.wikipedia.org/wiki?curid=14975" title="Ivy League">
Ivy League

The Ivy League is a collegiate athletic conference comprising sports teams from eight private institutions of higher education in the Northeastern United States. The conference name is also commonly used to refer to those eight schools as a group. The eight institutions are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, the University of Pennsylvania, Princeton University, and Yale University. The term "Ivy League" has connotations of academic excellence, selectivity in admissions, and social elitism.
The term became official after the formation of the NCAA Division I athletic conference in 1954. Seven of the eight schools were founded during the United States colonial period; the exception is Cornell, which was founded in 1865. Ivy League institutions account for seven of the nine Colonial Colleges chartered before the American Revolution, the other two being Rutgers University and College of William & Mary.
Ivy League schools are generally viewed as some of the most prestigious, and are ranked among the best universities worldwide. All eight universities place in the top sixteen of the "U.S. News & World Report" 2015 university rankings, including the top four schools and six of the top eleven. U.S. News has named a member of the Ivy League as the best national university in each of the past fifteen years ending with the 2015 rankings: Princeton eight times, Harvard twice and the two schools tied for first five times.
Undergraduate enrollments range from about 4,000 to 14,000, making them larger than those of a typical private liberal arts college and smaller than a typical public state university. Total enrollments, including graduate students, range from approximately 6,100 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $3.2 billion to Harvard's $36.4 billion, the largest financial endowment of any academic institution in the world.
Members.
Ivy League universities have some of the largest university financial endowments in the world, which allows the universities to provide many resources for their academic programs and research endeavors. As of 2014, Harvard University has an endowment of $36.4 billion. Additionally, each university receives millions of dollars in research grants and other subsidies from federal and state government.
History.
Origin of the name.
Students have long revered the ivied walls of older colleges. "Planting the ivy" was a customary class day ceremony at many colleges in the 1800s. In 1893 an alumnus told "The Harvard Crimson", "In 1850, class day was placed upon the University Calendar... the custom of planting the ivy, while the ivy oration was delivered, arose about this time." At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as "Ivy Day" in 1874. Ivy planting ceremonies are reported for Yale, Simmons, Bryn Mawr and many others. Princeton's "Ivy Club" was founded in 1879.
The first usage of "Ivy" in reference to a group of colleges is from sportswriter Stanley Woodward (1895–1965).
A proportion of our eastern ivy colleges are meeting little fellows another Saturday before plunging into the strife and the turmoil.—Stanley Woodward, "New York Tribune", October 14, 1933, describing the football season
The first known instance of the term "Ivy League" being used appeared in "The Christian Science Monitor" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. However, at this time, none of these institutions made efforts to form an athletic league.
The Ivy League universities are also called the "Ancient Eight" or simply the "Ivies".
A common folk etymology attributes the name to the Roman numerals for four (IV), asserting that there was such a sports league originally with four members. The "Morris Dictionary of Word and Phrase Origins" helped to perpetuate this belief. The supposed "IV League" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a 4th school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Yale and Columbia met on November 23, 1876 at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.
Pre–Ivy League.
Seven of the Ivy League schools were founded before the American Revolution; Cornell was founded just after the American Civil War. These seven were the primary colleges in the Northern and Middle Colonies, and their early faculties and founding boards were largely, therefore, drawn from other Ivy League institutions. There were also some British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, the University of Edinburgh, and elsewhere on their boards. Similarly, the founder of The College of William & Mary, in 1693, was a British graduate of the University of Edinburgh. Cornell provided Stanford University with its first president.
The influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities for each of these states. In 1801 a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California came from Yale, hence the school colors of University of California are Yale Blue and California Gold.
Some of the Ivy League schools have identifiable Protestant roots, while others were founded as nonsectarian schools. Church of England "King's College" broke up during the Revolution and was reformed as public nonsectarian Columbia College. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and such relics as compulsory chapel often lasted well into the twentieth century. Penn and Brown were officially founded as nonsectarian schools. Brown's charter promised no religious tests and "full liberty of conscience", but placed control in the hands of a board of twenty-two Baptists, five Quakers, four Congregationalists, and five Episcopalians. Cornell has been strongly nonsectarian from its founding.
"Ivy League" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This sense dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.
After the Second World War, the present Ivy League institutions slowly widened their selection of students. They had always had distinguished faculties; some of the first Americans with doctorates had taught for them; but they now decided that they could not both be world-class research institutions and be competitive in the highest ranks of American college sport; in addition, the schools experienced the scandals of any other big-time football programs, although more quietly.
History of the athletic league.
19th and early 20th centuries.
The first formal athletic league involving eventual Ivy League schools (or any US colleges, for that matter) was created in 1870 with the formation of the Rowing Association of American Colleges. The RAAC hosted a de facto national championship in rowing during the period 1870–1894. In 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete
A basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.
In 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies.
In February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie. Two years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.
Before the formal establishment of the Ivy League, there was an "unwritten and unspoken agreement among certain Eastern colleges on athletic relations". In 1935, the Associated Press reported on an example of collaboration between the schools:The athletic authorities of the so-called "Ivy League" are considering drastic measures to curb the increasing tendency toward riotous attacks on goal posts and other encroachments by spectators on playing fields.—The Associated Press, "The New York Times"
Despite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:
I can say with certainty that in the last five years—and markedly in the last three months—there has been a strong drift among the eight or ten universities of the East which see a good deal of one another in sport toward a closer bond of confidence and cooperation and toward the formation of a common front against the threat of a breakdown in the ideals of amateur sport in the interests of supposed expediency.
Please do not regard that statement as implying the organization of an Eastern conference or even a poetic "Ivy League". That sort of thing does not seem to be in the cards at the moment.
Within a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of "the formation of an Ivy League" gained enough traction among the undergraduate bodies of the universities that the "Columbia Daily Spectator", "The Cornell Daily Sun", "The Dartmouth", "The Harvard Crimson", "The Daily Pennsylvanian", "The Daily Princetonian" and the "Yale Daily News" would simultaneously run an editorial entitled "Now Is the Time", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:The Ivy League exists already in the minds of a good many of those connected with football, and we fail to see why the seven schools concerned should be satisfied to let it exist as a purely nebulous entity where there are so many practical benefits which would be possible under definite organized association. The seven colleges involved fall naturally together by reason of their common interests and similar general standards and by dint of their established national reputation they are in a particularly advantageous position to assume leadership for the preservation of the ideals of intercollegiate athletics.
The Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, "The Oneida", won the race and was presented with trophy black walnut oars from then presidential nominee General Franklin Pierce.
The proposal did not succeed—on January 11, 1937, the athletic authorities at the schools rejected the "possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track." However, they noted that the league "has such promising possibilities that it may not be dismissed and must be the subject of further consideration."
Post-World War II.
In 1945 the presidents of the eight schools signed the first "Ivy Group Agreement", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton Presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:The members of the Group reaffirm their prohibition of athletic scholarships. Athletes shall be admitted as students and awarded financial aid only on the basis of the same academic standards and economic need as are applied to all other students.
In 1954, the date generally accepted as the birth of the Ivy League, the presidents extended the Ivy Group Agreement to all intercollegiate sports effective with the 1955-56 basketball season. As part of the transition, Brown, the only Ivy that hadn't joined the EIBL, did so for the 1954-55 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.
As late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie "Animal House" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, "The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men’s colleges."
In 1982 the Ivy League considered adding two members, with the United States Military Academy, the United States Naval Academy, and Northwestern University as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.
Academics.
Admissions.
The Ivy League schools are highly selective, with acceptance rates since 2000 ranging from 6 to 16 percent at each of the universities. Admitted students come from around the world, although students from New England and the Northeastern United States make up a significant proportion of students.
Prestige.
Members of the League have been highly ranked by various university rankings.
Further, Ivy League members have produced many Nobel laureates, winners of the Nobel Prize and the Nobel Memorial Prize in Economic Sciences. Listed from in order from greatest number of Nobel laureates are: Harvard with 153 Nobel winners, the most out of any university in the world. This is followed by Columbia with 101 winners, Yale with 52, Cornell with 45, Princeton with 37, and Penn with 29 Nobel laureates. These figures are self-reported by the universities themselves, who use widely varying definitions for which Nobel winners are claimed as affiliates, for example, only degree-holding alumni or active faculty or former faculty, visiting faculty, adjunct faculty, etc. Many universities are notorious for claiming laureates with only tenuous informal connections in order to inflate their count of winners.
Collaboration.
Collaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group Presidents, composed of each university president. During meetings, the presidents often discuss common procedures and initiatives for the universities.
Libraries.
Up until recently, seven of the eight schools (Harvard excluded) participated in the Borrow Direct interlibrary loan program, making a total of 88 million items available to participants with a waiting period of four working days. This ILL program is not affiliated with the formal Ivy arrangement. Harvard and MIT joined the Direct Borrow partnership in January 2011, together contributing over 70 million books to the existing collection.
Culture.
Fashion and lifestyle.
Different fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and Preppy are styles often associated with the Ivy League and its culture.
Ivy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.
Preppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Princeton, and Yale.
Some typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colours, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colour combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney & Bourke became associated with preppy style.'
Today, these styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as "Classic American style" or "Traditional American style".
Social elitism.
The Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old Money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper middle and upper class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.
Phrases such as "Ivy League snobbery" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads "the aridity of snobbery which he knew infected the Ivy League colleges". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):
"We Ivy Leaguers [read: mostly white and Anglo] know that an Ivy League degree is a mark of the kind of person who is likely to succeed in this organization.
The phrase "Ivy League" historically has been perceived as connected not only with academic excellence, but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Princeton, Cornell, Columbia, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider "Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt" as candidates for membership, he exhorted:
"It would be well for the proponents of the Ivy League to make it clear (to themselves especially) that the proposed group would be inclusive but not "exclusive" as this term is used with a slight up-tilting of the tip of the nose.
Aspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having "foreign-policy views born in Harvard Yard's boutique." "New York Times" columnist Maureen Dowd asked "Wasn't this a case of the pot calling the kettle elite?" Bush explained however that, unlike Harvard, Yale's reputation was "so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it... Harvard boutique to me has the connotation of liberalism and elitism" and said Harvard in his remark was intended to represent "a philosophical enclave" and not a statement about class. Columnist Russell Baker opined that "Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets." Still, the last four presidents have all attended Ivy League schools for at least part of their education— George H.W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), and Barack Obama (Columbia undergrad, Harvard Law School).
U.S. presidents in the Ivy League.
Of the forty-three men who have served as President of the United States, fourteen have graduated from an Ivy League university. Of them, eight have degrees from Harvard, five from Yale, three from Columbia, two from Princeton, and one from Pennsylvania. John Adams was the first president to graduate from university, graduating from Harvard University in 1755.
Student demographics.
Geographic distribution.
Students of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, it is no surprise that most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.
Socioeconomics and social class.
Students of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and middle class American families.
In 2013, 46% of Harvard College students came from families in the top 3.8% of all American households (over $200,000 per annum). In 2012, the bottom 25% of the American income distribution accounted for only 3-4% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper middle and/or upper class. (The median household income in the U.S. in 2013 was $52,700.)
In the 2011-2012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) comprised 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.
Competition and athletics.
Ivy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. Unlike all other Division I basketball conferences, the Ivy League has no tournament for the league title; the school with the best conference record represents the conference in the Division I NCAA Men's and Women's Basketball Tournament (with a playoff, or playoffs, in the case of a tie). Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA Basketball Tournament.
On average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools.
Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies.
In the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 13, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the "Father of American Football," held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 17. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and 1999 "Mr. Irrelevant" Jim Finn (Penn).
Beginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS in 2006). The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, the Ivy League has not played any postseason games at all since 1956 due to the league's concerns about the extended December schedule's effects on academics. For this reason, any Ivy League team invited to the FCS playoffs turns down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which was most recently expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football is the only sport in which the Ivy League declines to compete for a national title.
In addition to varsity football, Penn, Princeton and Cornell also field teams in the eight-team Collegiate Sprint Football League, in which all players must weight 172 pounds or less. Penn and Princeton are the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.
The Ivy League is home to some of the oldest college rugby teams. Although these teams are not "varsity" sports, they compete annually in the Ivy Rugby Conference.
Historical results.
The table above includes the number of team championships won from the beginning of official Ivy League competition (1956–57 academic year) through 2011–12. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished six times by Harvard and 21 times by Princeton, including a conference-record 15 championships in 2010-11. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005-06). In the 33 academic years beginning 1979-80, Princeton has averaged 11 championships per year, one-third of the conference total of 33 sponsored sports.
In the seven academic years beginning 2005-06, Harvard has won Ivy titles in 22 different sports, two-thirds of the league total, and Princeton has won championships in 31 different sports, all except wrestling and men's tennis.
Rivalries.
Rivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; "Puck Frinceton", and "Pennetrate the Puss" t-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion). Harvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014.
Rivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have each had two unbeaten seasons since 2001). In men's lacrosse, Cornell and Princeton are perennial rivals, and they are the only two Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002-03, with one exception (Columbia women won indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 11 football games and all but one of the last 13 crew races.
Intra-Conference Football Rivalries.
The Yale-Princeton series is the nation's second longest, exceeded only by "The Rivalry" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional sports were fledgling baseball leagues, these high profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.
National team championships.
Through July 2, 2014
Other Ivies.
Marketing groups, journalists, and some educators sometimes promote other colleges as "Ivies," as in Little Ivies (colloquialism referring to a group of small, selective American liberal arts colleges), Public Ivies, or Southern Ivies. These uses of "Ivy" are intended to promote the other schools by comparing them to the Ivy League. For example, in the 2007 edition of Newsweek's "How to Get Into College Now", the editors designated twenty-five schools as "New Ivies."
The term "Ivy Plus" is sometimes used to refer to the Ancient Eight plus several other schools for purposes of alumni associations, university affiliations, or endowment comparisons. In his book "Untangling the Ivy League", Zawel writes, "The inclusion of non-Ivy League schools under this term is commonplace for some schools and extremely rare for others. Among these other schools, Massachusetts Institute of Technology and Stanford University are almost always included. The University of Chicago and Duke University are often included as well."

</doc>
<doc id="14976" url="http://en.wikipedia.org/wiki?curid=14976" title="Ithaca Hours">
Ithaca Hours

The Ithaca HOUR is a local currency used in Ithaca, New York and is the oldest and largest local currency system in the United States that is still operating. It has inspired other similar systems in Madison, Wisconsin; Corvallis, Oregon; and a proposed system in the Lehigh Valley, Pennsylvania. One Ithaca HOUR is valued at US$10 and is generally recommended to be used as payment for one hour's work, although the rate is negotiable.
The currency.
Ithaca HOURS are not backed by national currency and cannot be freely converted to national currency, although some businesses may agree to buy them.
HOURS are printed on high-quality paper and use faint graphics that would be difficult to reproduce, and each bill is stamped with a serial number, in order to discourage counterfeiting.
In 2002, a one-tenth hour bill was introduced, partly due to the encouragement and funding from Alternatives Federal Credit Union and feedback from retailers who complained about the awkwardness of only having larger denominations to work with; the bills bear the signatures of both HOURS president Steve Burke and the president of AFCU.
While the Ithaca Hour continues to exist, in recent years it has fallen into disuse. Media accounts from the year 2011 indicate that the number of businesses accepting Hours has declined. Several reasons are attributed to this. First has been the founder, Paul Glover, moving out of town. While in Ithaca, Glover had acted as an evangelist and networker for Hours, helping to spread their use and helping businesses find ways to spend Hours they had received. Secondly, a general shift away from cash transactions towards electronic transfers with debit or credit cards. Glover has emphasized that every local currency needs at least one full-time networker to "promote, facilitate and troubleshoot" currency circulation.
Origin.
Ithaca Hours were started by Paul Glover in November 1991. The system has historical roots in scrip and alternative and local currencies that proliferated in America during the Great Depression.
While doing research into local economics during 1989, Glover had seen an "Hour" note 19th century British industrialist Robert Owen issued to his workers for spending at his company store. After Ithaca Hours began, he discovered that Owen's Hours were based on Josiah Warren's "Time Store" notes of 1827.
In May 1991, local student Patrice Jennings interviewed Glover about the Ithaca LETS enterprise. This conversation strongly reinforced his interest in trade systems. Jennings's research on the Ithaca LETS and its failure was integral to the development of the HOUR currency; conversations between Jennings and Glover helped to ensure that HOURS used knowledge of what had not worked with the LETS system.
Within a few days, he had designs for the HOUR and Half HOUR notes. He established that each HOUR would be worth the equivalent of $10, which was about the average hourly amount that workers earned in surrounding Tompkins County, although the exact rate of exchange for any given transaction was to be decided by the parties themselves. At GreenStar Cooperative Market, a local food co-op, Glover approached Gary Fine, a local massage therapist, with photocopied samples. Fine became the first person to sign a list formally agreeing to accept HOURS in exchange for services. Soon after, Jim Rohrrsen, the proprietor of a local toy store, became the first retailer to sign-up to accept Ithaca HOURS in exchange for merchandise.
When the system was first started, 90 people agreed to accept HOURS as pay for their services. They all agreed to accept HOURS despite the lack of a business plan or guarantee. Glover then began to ask for small donations to help pay for printing HOURS.
Fine Line Printing completed the first run of 1,500 HOURS and 1,500 Half HOURS in October 1991. These notes, the first modern local currency, were nearly twice as large as the current Ithaca HOURS. Because they didn't fit well in people's wallets, almost all of the original notes have been removed from circulation.
The first issue of Ithaca Money was printed at Our Press, a printing shop in Chenango Bridge, New York, on October 16, 1991. The next day Glover issued 10 HOURS to Ithaca Hours, the organization he founded to run the system, as the first of four reimbursements for the cost of printing HOURS. The day after that, October 18, 1991, 382 HOURS were disbursed and prepared for mailing to the first 93 pioneers.
On October 19, 1991, Glover bought a samosa from Catherine Martinez at the Farmers' Market with Half HOUR #751—the first use of an HOUR. Several other Market vendors enrolled that day.
Stacks of the Ithaca Money newspaper were distributed all over town with an invitation to "join the fun."
A Barter Potluck was held at GIAC on November 12, 1991, the first of many monthly gatherings where food and skills were exchanged, acquaintances made, and friendships renewed.
Management & philosophy.
In 1996, Glover was running the Ithaca Hours system from his home, and the system had an advisory board and a governing board called the "Barter Potluck". The board and Glover put forth the idea that economic interactions should be based on harmony rather than on more Hobbsian forms of competition. In one interview, Glover stated that "There's a growing movement called "ecological economics" and Ithaca HOURS is part of that cosmos. Last year I wrote an article which discusses moving us toward the provision of food, fuel, clothing, housing, transportation, [and other] necessities in ways which are healing of nature, or which are less depleting at least and which bring people together on the basis of their shared pride, not arrogance." Thus one underlying principle of the local currency movement is to create "fair trade" with a minimum of conflict or exploitation of either people or natural resources.
The Advisory Board incorporated the Ithaca HOUR system as Ithaca Hours, Inc. in October 1998, and hosted the first elections for Board of Directors in March 1999. The first Board of Directors included Monica Hargraves, Dan Cogan, Margaret McCasland, Erica Van Etten, Greg Spence Wolf, Bob LeRoy, LeGrace Benson, Wally Woods, Jennifer Elges, and Donald Stephenson. In May 1999 Glover turned the administration of Ithaca HOURS over to the newly elected Board of Directors. Glover has continued to support Ithaca Hours through community outreach to present, most notably through the Ithaca Health Fund (now incorporated as part
of the Ithaca Health Alliance) and Ithaca Community News.
The current Board of Directors, 2014-2015, includes Erik Lehmann (Chair), Danielle Klock, and Bob LeRoy.
Economic development.
Several million dollars value of HOURS have been traded since 1991 among thousands of residents and over 500 area businesses, including the Cayuga Medical Center, Alternatives Federal Credit Union, the public library, many local farmers, movie theatres, restaurants, healers, plumbers, carpenters, electricians, and landlords.
One of the primary functions of the Ithaca Hours system is to promote local economic development. Businesses who receive Hours must spend them on local goods and services, thus building a network of inter-supporting local businesses. While non-local businesses are welcome to accept Hours, those businesses need to spend them on local goods and services to be economically sustainable.
In their mission to promote local economic development, the Board of Directors also makes interest-free loans of Ithaca HOURS to local businesses and grants to local non-profit organizations.

</doc>
<doc id="14979" url="http://en.wikipedia.org/wiki?curid=14979" title="Interstellar cloud">
Interstellar cloud

Interstellar cloud is the generic name given to an accumulation of gas, plasma and dust in our and other galaxies. Put differently, an interstellar cloud is a denser-than-average region of the interstellar medium. Depending on the density, size and temperature of a given cloud, the hydrogen in it can be neutral (H I regions), ionized (H II regions) (i.e. a plasma), or molecular (molecular clouds). Neutral and ionized clouds are sometimes also called diffuse clouds, while molecular clouds are sometimes also referred to as dense clouds.
Chemical compositions.
Analyzing the composition of interstellar clouds is achieved by studying electromagnetic radiation that we receive from them. Large radio telescopes scan the intensity in the sky of particular frequencies of electromagnetic radiation which are characteristic of certain molecules' spectra. Some interstellar clouds are cold and tend to give out EM radiation of large wavelengths. We can produce a map of the abundance of these molecules to produce an understanding of the varying composition of the clouds. In hot clouds, there are often ions of many elements, whose spectra can be seen in visible and ultraviolet light.
Radio telescopes can also scan over the frequencies from one point in the map, recording the intensities of each type of molecule. Peaks of frequencies mean that an abundance of that molecule or atom is present in the cloud. The height of the peak is proportional to the relative percentage that it makes up.
Unexpected chemicals detected in interstellar clouds.
Until recently the rates of reactions in interstellar clouds were expected to be very slow, with minimal products being produced due to the low temperature and density of the clouds. However, organic molecules were observed in the spectra that scientists would not have expected to find under these conditions, such as formaldehyde, methanol, and vinyl alcohol. The reactions needed to create such substances are familiar to scientists only at the much higher temperatures and pressures of earth and earth-based laboratories. The fact that they were found indicates that these chemical reactions in interstellar clouds take place faster than suspected, likely in gas-phase reactions unfamiliar to organic chemistry as observed on earth. These reactions are studied in the CRESU experiment.
Interstellar clouds also provide a medium to study the presence and proportions of metals in space. The presence and ratios of these elements may help develop theories on the means of their production, especially when their proportions are inconsistent with those expected to arise from stars as a result of fusion and thereby suggest alternate means, such as cosmic ray spallation.
High-velocity cloud.
These interstellar clouds possess a velocity higher than can be explained by the rotation of the Milky Way. By definition, these clouds must have a vlsr greater than 90 km s−1, where vlsr is the local standard rest velocity. They are detected primarily in the 21 cm line of neutral hydrogen, and typically have a lower portion of heavy elements than is normal for interstellar clouds in the Milky Way.
Theories intended to explain these unusual clouds include materials left over from the formation of our galaxy, or tidally-displaced matter drawn away from other galaxies or members of the Local Group. An example of the latter is the Magellanic Stream. To narrow down the origin of these clouds, a better understanding of their distances and metallicity is needed.
High-velocity clouds are identified with an HVC prefix, as with HVC 127-41-330.

</doc>
<doc id="14980" url="http://en.wikipedia.org/wiki?curid=14980" title="Imhotep">
Imhotep

Imhotep (; also spelled Immutef, Im-hotep, or Ii-em-Hotep; called "Imuthes" (Ἰμούθης) by the Greeks; fl. 27th century BC (c. 2650–2600 BC); Egyptian: "ỉỉ-m-ḥtp" "*jā-im-ḥātap" meaning "the one who comes in peace, is with peace") was an Egyptian polymath who served under the Third Dynasty king Djoser as chancellor to the pharaoh and high priest of the sun god Ra (or Re) at Heliopolis. He is considered by some to be the earliest known architect and engineer and physician in early history, though two other physicians, Hesy-Ra and Merit-Ptah, lived around the same time. The full list of his titles is:
He was one of only a few commoners ever to be accorded divine status after death. The center of his cult was Memphis. From the First Intermediate Period onward Imhotep was also revered as a poet and philosopher. His sayings were famously referenced in poems: "I have heard the words of Imhotep and Hordedef with whose discourses men speak so much."
The location of Imhotep's self-constructed tomb was well hidden from the beginning and it remains unknown, despite efforts to find it. The consensus is that it is hidden somewhere at Saqqara. Imhotep's historicity is confirmed by two contemporary inscriptions made during his lifetime on the base or pedestal of one of Djoser's statues (Cairo JE 49889) and also by a graffito on the enclosure wall surrounding Sekhemkhet's unfinished step-pyramid. The latter inscription suggests that Imhotep outlived Djoser by a few years and went on to serve in the construction of King Sekhemkhet's pyramid, which was abandoned due to this ruler's brief reign.
Attribution of achievements and inventions.
Architecture and engineering.
Imhotep was one of the chief officials of the Pharaoh Djoser. Egyptologists ascribe to him the design of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt in 2630 – 2611 BC. He may have been responsible for the first known use of columns to support a building.
As an instigator of Egyptian culture, Imhotep's idealized image lasted well into the Ptolemaic period. The Egyptian historian Manetho credited him with inventing the method of a stone-dressed building during Djoser's reign, though he was not the first to actually build with stone. Stone walling, flooring, lintels, and jambs had appeared sporadically during the Archaic Period, though it is true that a building of the Step Pyramid's size and made entirely out of stone had never before been constructed. Before Djoser, pharaohs were buried in mastaba tombs.
Medicine.
Imhotep was an important figure in Ancient Egyptian medicine. He was the author of a medical treatise remarkable for being devoid of magical thinking: the so-called Edwin Smith papyrus containing anatomical observations, ailments, and cures. The surviving papyrus was probably written around 1700 BC but may be a copy of texts written a thousand years earlier. However, this attribution of authorship is speculative. Today the Papyrus is on show at the Brooklyn Children's Museum, New York City. The 48 cases contained within the Edwin Smith Surgical Papyrus concern:
Deification.
Two thousand years after his death, Imhotep's status was raised to that of a deity of medicine and healing. He was identified or confused with Thoth, the god of architecture, mathematics, medicine and patron of the scribes, having Imhotep's cult merging with that of his former tutelary god. Taking this into consideration, he was thus associated with Amenhotep son of Hapu, who was another deified architect, in the region of Thebes where they were worshipped as "brothers" in temples dedicated to Thoth and later in Hermopolis following the syncretist concept of Hermes-Thot, a concept that led to another syncretic belief, that of Hermes Trismegistus and hermeticism. Imhotep was also linked to Asklepios by the Greeks.
Birth myths.
According to myth, Imhotep's mother was a mortal named "Kheredu-ankh", elevated later to semi-divine status by claims that she was the daughter of Banebdjedet. Conversely, since Imhotep was known as the "Son of Ptah," his mother was sometimes claimed to be Sekhmet, the patron of Upper Egypt whose consort was Ptah. Also according to myths, his father was also an architect and was named Kanofer.
Legacy.
According to the "Encyclopædia Britannica", "The evidence afforded by Egyptian and Greek texts support the view that Imhotep's reputation was very respected in early times ... His prestige increased with the lapse of centuries and his temples in Greek times were the centers of medical teachings."
It is Imhotep, says Sir William Osler, who was the real "Father of Medicine", "the first figure of a physician to stand out clearly from the mists of antiquity."
Descriptions of Imhotep by James Henry Breasted "et al." :
'In priestly wisdom, in magic, in the formulation of wise proverbs; in medicine and architecture; this remarkable figure of Zoser's reign left so notable a reputation that his name was never forgotten. He was the patron spirit of the later scribes, to whom they regularly poured out a libation from the water-jug of their writing outfit before beginning their work.'
'Imhotep extracted medicine from plants.'
'Imhotep was portrayed as a priest with a shaven head, seated and holding a papyrus roll. Occasionally he was shown clothed in the archaic costume of a priest.'
'Of the details of his life, very little has survived though numerous statues and statuettes of him have been found. Some show him as an ordinary man who is dressed in plain attire. Others show him as a sage who is seated on a chair with a roll of papyrus on his knees or under his arm. Later, his statuettes show him with a god like beard, standing, and carrying the ankh and a scepter.'
'He is represented seated with a papyrus scroll across his knees, wearing a skullcap and a long linen kilt. We can interpret the papyrus as suggesting the sources of knowledge kept by scribes in the "House of Life". The headgear identifies Imhotep with Ptah, and his priestly linen garment symbolizes his religious purity.'
Imhotep's dreams.
The Upper Egyptian Famine Stela, dating from the Ptolemaic period, bears an inscription containing a legend about a famine of seven years during the reign of Djoser. Imhotep is credited with having been instrumental in ending it. One of his priests explained the connection between the god Khnum and the rise of the Nile to the king, who then had a dream in which the Nile god spoke to him, promising to end the drought.
Biographical papyrus.
A papyrus from the ancient Egyptian temple of Tebtunis, dating to the 2nd century AD, preserves a long story in the demotic script about Imhotep. King Djoser plays a prominent role in the story, which also mentions Imhotep's family; his father the god Ptah, his mother Khereduankh, and his little-sister Renpetneferet. At one point Djoser desires the young Renpetnefereret, and Imhotep disguises himself and tries to rescue her. The text also refers to the royal tomb of Djoser by which the Step Pyramid must be meant. An anachronistic detail is a battle between the Egyptian and Assyrian armies where Imhotep fights an Assyrian sorceress in a duel of magic.

</doc>
<doc id="14981" url="http://en.wikipedia.org/wiki?curid=14981" title="Ictinus">
Ictinus

Ictinus (; Greek: Ἰκτῖνος, "Iktinos") was an architect active in the mid 5th century BC. Ancient sources identify Ictinus and Callicrates as co-architects of the Parthenon.
Pausanias identifies Ictinus as architect of the Temple of Apollo at Bassae. That temple was Doric on the exterior, Ionic on the interior, and incorporated a Corinthian column, the earliest known, at the center rear of the cella. Sources also identify Ictinus as architect of the Telesterion at Eleusis, a gigantic hall used in the Eleusinian Mysteries.
The artist Jean Auguste Dominique Ingres painted a scene showing Ictinus together with the lyric poet Pindar. The painting is known as "Pindar and Ictinus" and is exhibited at the National Gallery, London.

</doc>
<doc id="14982" url="http://en.wikipedia.org/wiki?curid=14982" title="Isidore of Miletus">
Isidore of Miletus

Isidore of Miletus (Greek: Ἰσίδωρος) was one of the two main Byzantine Greek architects (Anthemius of Tralles was the other) that Emperor Justinian I commissioned to design the church of Hagia Sophia in Constantinople from 532-537 A.D. He also created the first comprehensive compilation of Archimedes' works.
Summary.
Isidore of Miletus was a renowned scientist and mathematician before Emperor Justinian I hired him, “Isidorus taught stereometry and physics at the universities, first of Alexandria then of Constantinople, and wrote a commentary on an older treatise on vaulting.” Isidore is also renowned for producing the first comprehensive compilation of Archimedes work.
Emperor Justinian I appointed his architects to rebuild the Hagia Sophia following his victory over protesters within the capital city of his Roman Empire, Constantinople. The first basilica was completed in 360 A.D. and remodelled from 404 A.D. to 415 A.D., but had been damaged in 532 A.D. in the course of the Nika Riot, “The temple of Sophia, the baths of Zeuxippus, and the imperial courtyard from the Propylaia all the way to the so-called House of Ares were burned up and destroyed, as were both of the great porticoes that lead to the forum that is named after Constantine, houses of prosperous people, and a great deal of other properties.”
The warring factions of Byzantine society, the Blues and the Greens, opposed each other in the chariot races at the Hippodrome and often resorted to violence. During the Nika Riot, more than thirty thousand people died. Emperor Justinian I ensured that his new structure would not be burned down, like its predecessors, by commissioning architects that would build the church mainly out of stone, rather than wood, “He compacted it of baked brick and mortar, and in many places bound it together with iron, but made no use of wood, so that the church should no longer prove combustible.”
Isidore of Miletus and Anthemius of Tralles originally planned on a main hall of the Hagia Sophia that measured 230 feet by 250 feet, making it the largest church in Constantinople, but the original dome was nearly 20 feet lower than it was constructed, “Justinian suppressed these riots and took the opportunity of marking his victory by erecting in 532-7 the new Hagia Sophia, one of the largest, most lavish, and most expensive buildings of all time.”
Although Isidore of Miletus and Anthemius of Tralles were not formally educated in architecture, they were scientists that could organize the logistics of drawing thousands of labourers and unprecedented loads of rare raw materials from around the Roman Empire to create the Hagia Sophia for Emperor Justinian I. The finished product was built in admirable form for the Roman Emperor, “All of these elements marvellously fitted together in mid-air, suspended from one another and reposing only on the parts adjacent to them, produce a unified and most remarkable harmony in the work, and yet do not allow the spectators to rest their gaze upon any one of them for a length of time.”
Conclusion.
The Hagia Sophia architects innovatively combined the longitudinal structure of a Roman basilica and the central plan of a drum-supported dome, in order to withstand the high magnitude earthquakes of the Marmara Region, “However, in May 558, little more than 20 years after the Church’s dedication, following the earthquakes of August 553 and December 557, parts of the central dome and its supporting structure system collapsed.” The Hagia Sophia was repeatedly cracked by earthquakes and was quickly repaired. Isidore of Miletus’ nephew, Isidore the Younger, introduced the new dome design that can be viewed in the Hagia Sophia in present day Istanbul, Turkey.
After a great earthquake in 989 ruined the dome of Hagia Sophia, the Byzantine officials summoned Trdat the Architect to Byzantium to organize repairs. The restored dome was completed by 994.

</doc>
<doc id="14984" url="http://en.wikipedia.org/wiki?curid=14984" title="International Atomic Energy Agency">
International Atomic Energy Agency

The International Atomic Energy Agency (IAEA) is an international organization that seeks to promote the peaceful use of nuclear energy, and to inhibit its use for any military purpose, including nuclear weapons. The IAEA was established as an autonomous organization on 29 July 1957. Though established independently of the United Nations through its own international treaty, the IAEA Statute, the IAEA reports to both the United Nations General Assembly and Security Council.
The IAEA has its headquarters in Vienna, Austria. The IAEA has two "Regional Safeguards Offices" which are located in Toronto, Canada, and in Tokyo, Japan. The IAEA also has two liaison offices which are located in New York City, United States, and in Geneva, Switzerland. In addition, the IAEA has three laboratories located in Vienna and Seibersdorf, Austria, and in Monaco.
The IAEA serves as an intergovernmental forum for scientific and technical cooperation in the peaceful use of nuclear technology and nuclear power worldwide. The programs of the IAEA encourage the development of the peaceful applications of nuclear technology, provide international safeguards against misuse of nuclear technology and nuclear materials, and promote nuclear safety (including radiation protection) and nuclear security standards and their implementation.
The IAEA and its former Director General, Mohamed ElBaradei, were jointly awarded the Nobel Peace Prize on 7 October 2005. The IAEA's current Director General is Yukiya Amano.
History.
In 1953, the President of the United States, Dwight D. Eisenhower, proposed the creation of an international body to both regulate and promote the peaceful use of atomic power (nuclear power), in his Atoms for Peace address to the UN General Assembly. In September 1954, the United States proposed to the General Assembly the creation of an international agency to take control of fissile material, which could be used either for nuclear power or for nuclear weapons. This agency would establish a kind of "nuclear bank."
The United States also called for an international scientific conference on all of the peaceful aspects of nuclear power. By November 1954, it had become clear that the Soviet Union would reject any international custody of fissile material, but that a "clearing house" for nuclear transactions might be possible. From 8 to 20 August 1955, the United Nations held the International Conference on the Peaceful Uses of Atomic Energy in Geneva, Switzerland. In October 1956, a Conference on the IAEA Statute was held at the Headquarters of the United Nations to approve the founding document for the IAEA, which was negotiated in 1955-1956 by a group of twelve countries. The Statute of the IAEA was approved on 23 October 1956 and came into force on 29 July 1957.
Former U.S. Congressman W. Sterling Cole served as the IAEA's first Director General from 1957 to 1961. Cole served only one term, after which the IAEA was headed by two Swedes for nearly four decades: the scientist Sigvard Eklund held the job from 1961 to 1981, followed by former Swedish Foreign Minister Hans Blix, who served from 1981 to 1997. Blix was succeeded as Director General by Mohamed ElBaradei of Egypt, who served until November 2009.
Beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The same happened after the Fukushima disaster in Fukushima, Japan.
Both the IAEA and its then Director General, ElBaradei, were awarded the Nobel Peace Prize in 2005. In ElBaradei's acceptance speech in Oslo, he stated that only one percent of the money spent on developing new weapons would be enough to feed the entire world, and that, if we hope to escape self-destruction, then nuclear weapons should have no place in our collective conscience, and no role in our security.
On 2 July 2009, Yukiya Amano of Japan was elected as the Director General for the IAEA, defeating Abdul Samad Minty of South Africa and Luis E. Echávarri of Spain. On 3 July 2009, the Board of Governors voted to appoint Yukiya Amano "by acclamation," and IAEA General Conference in September 2009 approved. He took office on 1 December 2009.
Structure and function.
General.
The IAEA's mission is guided by the interests and needs of Member States, strategic plans and the vision embodied in the IAEA Statute (see below). Three main pillars – or areas of work – underpin the IAEA's mission: Safety and Security; Science and Technology; and Safeguards and Verification
The IAEA as an autonomous organization is not under direct control of the UN, but the IAEA does report to both the UN General Assembly and Security Council. Unlike most other specialized international agencies, the IAEA does much of its work with the Security Council, and not with the United Nations Economic and Social Council. The structure and functions of the IAEA are defined by its founding document, the IAEA Statute (see below). The IAEA has three main bodies: the Board of Governors, the General Conference, and the Secretariat.
The IAEA exists to pursue the "safe, secure and peaceful uses of nuclear sciences and technology" (Pillars 2005). The IAEA executes this mission with three main functions: the inspection of existing nuclear facilities to ensure their peaceful use, providing information and developing standards to ensure the safety and security of nuclear facilities, and as a hub for the various fields of science involved in the peaceful applications of nuclear technology.
The IAEA recognizes knowledge as the nuclear energy industry’s most valuable asset and resource, without which the industry cannot operate safely and economically. Following the IAEA General Conference since 2002 resolutions the Nuclear Knowledge Management, a formal programme was established to address Member States' priorities in the 21st century.
In 2004, the IAEA developed a Programme of Action for Cancer Therapy (PACT). PACT responds to the needs of developing countries to establish, to improve, or to expand radiotherapy treatment programs. The IAEA is raising money to help efforts by its Member States to save lives and to reduce suffering of cancer victims.
The IAEA has established programs to help developing countries in planning to build systematically the capability to manage a nuclear power program, including the Integrated Nuclear Infrastructure Group, which has carried out Integrated Nuclear Infrastructure Review missions in Indonesia, Jordan, Thailand and Vietnam. The IAEA reports that roughly 60 countries are considering how to include nuclear power in their energy plans.
To enhance the sharing of information and experience among IAEA Member States concerning the seismic safety of nuclear facilities, in 2008 the IAEA established the International Seismic Safety Centre. This centre is establishing safety standards and providing for their application in relation to site selection, site evaluation and seismic design.
Board of Governors.
The Board of Governors is one of two policy making bodies of the IAEA. The Board consists of 22 member states elected by the General Conference, and at least 10 member states nominated by the outgoing Board. The outgoing Board designates the ten members who are the most advanced in atomic energy technology, plus the most advanced members from any of the following areas that are not represented by the first ten: North America, Latin America, Western Europe, Eastern Europe, Africa, Middle East and South Asia, South East Asia, the Pacific, and the Far East. These members are designated for one year terms. The General Conference elects 22 members from the remaining nations to two-year terms. Eleven are elected each year. The 22 elected members must also represent a stipulated geographic diversity. The 35 Board members for the period 2012–2013 are: Algeria, Argentina, Australia, Belgium, Brazil, Bulgaria, Canada, China, Costa Rica, Cuba, Egypt, France, Germany, Greece, Hungary, India, Indonesia, Italy, Japan, the Republic of Korea, Libya, Mexico, Nigeria, Norway, Pakistan, Poland, the Russian Federation, Saudi Arabia, South Africa, Sweden, Thailand, the United Kingdom, Tanzania, the United States of America and Uruguay.
The Board, in its five yearly meetings, is responsible for making most of the policy of the IAEA. The Board makes recommendations to the General Conference on IAEA activities and budget, is responsible for publishing IAEA standards and appoints the Director General subject to General Conference approval. Board members each receive one vote. Budget matters require a two-thirds majority. All other matters require only a simple majority. The simple majority also has the power to stipulate issues that will thereafter require a two-thirds majority. Two-thirds of all Board members must be present to call a vote. The Board elects its own chairman.
General Conference.
The General Conference is made up of all 164 member states. It meets once a year, typically in September, to approve the actions and budgets passed on from the Board of Governors. The General Conference also approves the nominee for Director General and requests reports from the Board on issues in question (Statute). Each member receives one vote. Issues of budget, Statute amendment and suspension of a member's privileges require a two- thirds majority and all other issues require a simple majority. Similar to the Board, the General Conference can, by simple majority, designate issues to require a two- thirds majority. The General Conference elects a President at each annual meeting to facilitate an effective meeting. The President only serves for the duration of the session (Statute).
The main function of the General Conference is to serve as a forum for debate on current issues and policies. Any of the other IAEA organs, the Director General, the Board and member states can table issues to be discussed by the General Conference (IAEA Primer). This function of the General Conference is almost identical to the General Assembly of the United Nations.
Secretariat.
The Secretariat is the professional and general service staff of the IAEA. The Secretariat is headed by the Director General. The Director General is responsible for enforcement of the actions passed by the Board of Governors and the General Conference. The Director General is selected by the Board and approved by the General Conference for renewable four-year terms. The Director General oversees six departments that do the actual work in carrying out the policies of the IAEA: Nuclear Energy, Nuclear Safety and Security, Nuclear Sciences and Applications, Safeguards, Technical Cooperation, and Management.
The IAEA budget is in two parts. The regular budget funds most activities of the IAEA and is assessed to each member nation (€344 million in 2014). The Technical Cooperation Fund is funded by voluntary contributions with a general target in the US$90 million range.
Missions.
The IAEA is generally described as having three main missions:
Peaceful uses.
According to Article II of the IAEA Statute, the objective of the IAEA is "to accelerate and enlarge the contribution of atomic energy to peace, health and prosperity throughout the world." Its primary functions in this area, according to Article III, are to encourage research and development, to secure or provide materials, services, equipment and facilities for Member States, to foster exchange of scientific and technical information and training.
Three of the IAEA's six Departments are principally charged with promoting the peaceful uses of nuclear energy. The Department of Nuclear Energy focuses on providing advice and services to Member States on nuclear power and the nuclear fuel cycle. The Department of Nuclear Sciences and Applications focuses on the use of non-power nuclear and isotope techniques to help IAEA Member States in the areas of water, energy, health, biodiversity, and agriculture. The Department of Technical Cooperation provides direct assistance to IAEA Member States, through national, regional, and inter-regional projects through training, expert missions, scientific exchanges, and provision of equipment.
Safeguards.
Article II of the IAEA Statute defines the Agency's twin objectives as promoting peaceful uses of atomic energy and "ensur[ing], so far as it is able, that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose." To do this, the IAEA is authorized in Article III.A.5 of the Statute "to establish and administer safeguards designed to ensure that special fissionable and other materials, services, equipment, facilities, and information made available by the Agency or at its request or under its supervision or control are not used in such a way as to further any military purpose; and to apply safeguards, at the request of the parties, to any bilateral or multilateral arrangement, or at the request of a State, to any of that State's activities in the field of atomic energy."
The Department of Safeguards is responsible for carrying out this mission, through technical measures designed to verify the correctness and completeness of states' nuclear declarations.
Nuclear safety.
The IAEA classifies safety as one of its top three priorities. It spends 8.9 percent of its 352 million-euro ($469 million) regular budget in 2011 on making plants secure from accidents. Its resources are used on the other two priorities: technical cooperation and preventing nuclear weapons proliferation.
The IAEA itself says that, beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The IAEA says that the same happened after the Fukushima disaster in Fukushima, Japan.
In June 2011, the IAEA chief said he had "broad support for his plan to strengthen international safety checks on nuclear power plants to help avoid any repeat of Japan's Fukushima crisis". Peer-reviewed safety checks on reactors worldwide, organized by the IAEA, have been proposed.
Criticism.
Russian nuclear accident specialist Iouli Andreev is critical of the response to Fukushima, and says that the IAEA did not learn from the 1986 Chernobyl disaster. He has accused the IAEA and corporations of "wilfully ignoring lessons from the world's worst nuclear accident 25 years ago to protect the industry's expansion". The IAEA's role "as an advocate for nuclear power has made it a target for protests".
The journal "Nature" has reported that the IAEA response to the Fukushima I nuclear accidents in Japan was "sluggish and sometimes confusing", drawing calls for the agency to "take a more proactive role in nuclear safety". But nuclear experts say that the agency's complicated mandate and the constraints imposed by its member states mean that reforms will not happen quickly or easily, although its INES "emergency scale is very likely to be revisited" given the confusing way in which it was used in Japan.
Some scientists say that the 2011 Japanese nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide. There are several problems with the IAEA says Najmedin Meshkati of University of Southern California:
It recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organization overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT).
The journal "Nature" has reported that "the world must strengthen the ability of the International Atomic Energy Agency to make independent assessments of nuclear safety" and that "the public would be better served by an IAEA more able to deliver frank and independent assessments of nuclear crises as they unfold".
Membership.
The process of joining the IAEA is fairly simple. Normally, a State would notify the Director General of its desire to join, and the Director would submit the application to the Board for consideration. If the Board recommends approval, and the General Conference approves the application for membership, the State must then submit its instrument of acceptance of the IAEA Statute to the United States, which functions as the depositary Government for the IAEA Statute. The State is considered a member when its acceptance letter is deposited. The United States then informs the IAEA, which notifies other IAEA Member States. Signature and ratification of the Nuclear Non-Proliferation Treaty (NPT) are not preconditions for membership in the IAEA.
The IAEA has 164 member states. Most UN members and the Holy See are Member States of the IAEA. Cape Verde (2007), Tonga (2011), Comoros (2014) and Vanuatu (2014), as non-member states, have been approved for membership and will become a Member State once they deposit the necessary legal instruments.
Four states have withdrawn from the IAEA. North Korea was a Member State from 1974–1994, but withdrew after the Board of Governors found it in non-compliance with its safeguards agreement and suspended most technical cooperation. Nicaragua became a member in 1957, withdrew its membership in 1970, and rejoined in 1977, Honduras joined in 1957, withdrew in 1967, and rejoined in 2003, while Cambodia joined in 1958, withdrew in 2003, and rejoined in 2009.

</doc>
<doc id="14985" url="http://en.wikipedia.org/wiki?curid=14985" title="International Civil Aviation Organization">
International Civil Aviation Organization

The International Civil Aviation Organization (ICAO, pronounced /aɪˈkeɪoʊ/; French: "Organisation de l'aviation civile internationale", OACI), is a specialized agency of the United Nations. It codifies the principles and techniques of international air navigation and fosters the planning and development of international air transport to ensure safe and orderly growth. Its headquarters are located in the "Quartier International" of Montreal, Quebec, Canada.
The ICAO Council adopts standards and recommended practices concerning air navigation, its infrastructure, flight inspection, prevention of unlawful interference, and facilitation of border-crossing procedures for international civil aviation. ICAO defines the protocols for air accident investigation followed by in countries signatory to the Convention on International Civil Aviation ("Chicago Convention").
The Air Navigation Commission (ANC) is the technical body within ICAO. The Commission is composed of 19 Commissioners, nominated by the ICAO's contracting states, and appointed by the ICAO Council. Commissioners serve as independent experts, who although nominated by their states, do not serve as state or political representatives. The development of Aviation Standards and Recommended Practices is done under the direction of the ANC through the formal process of ICAO Panels. Once approved by the Commission, standards are sent to the Council, the political body of ICAO, for consultation and coordination with the Member States before final adoption.
ICAO is distinct from the International Air Transport Association (IATA), a trade association representing 240 of the world’s airlines, also headquartered in Montreal, or with the Civil Air Navigation Services Organisation (CANSO), an organization for Air Navigation Service Providers (ANSPs) with its headquarters at Amsterdam Airport Schiphol in the Netherlands. These are trade associations representing specific aviation interests, whereas ICAO is a body of the United Nations.
History.
The forerunner to ICAO was the "International Commission for Air Navigation" (ICAN). It held its first convention in 1903 in Berlin, Germany but no agreements were reached among the eight countries that attended. At the second convention in 1906, also held in Berlin, 27 countries attended. The third convention, held in London in 1912 allocated the first radio callsigns for use by aircraft. ICAN continued to operate until 1945.
Fifty-two countries signed the Convention on International Civil Aviation, also known as the Chicago Convention, in Chicago, Illinois, on 7 December 1944. Under its terms, a Provisional International Civil Aviation Organization (PICAO) was to be established, to be replaced in turn by a permanent organization when 26 countries ratified the convention. Accordingly, PICAO began operating on 6 June 1945, replacing ICAN. The 26th country ratified the Convention on 5 March 1947 and, consequently PICAO was disestablished on 4 April 1947 and replaced by ICAO, which began operations the same day. In October 1947, ICAO became an agency of the United Nations linked to the United Nations Economic and Social Council (ECOSOC).
2013 proposal to relocate headquarters to Qatar.
In April 2013, Qatar offered to serve as the new permanent seat of the Organization starting in 2016. The offer needed to be considered by all of ICAO's 191 Member States at the next convening of the triennial ICAO Assembly, which took place from 24 September through 4 October 2013. A minimum of three-fifths (60%) of ICAO's Member States needed to agree to the Qatar proposal for it to be approved. According to ICAO spokesman Anthony Philbin, there had never been an official request to move ICAO since its creation. Qatar, which has promised to construct a massive new headquarters for ICAO and cover all moving expenses, stated that Montreal "was too far from Europe and Asia", "had cold winters," was hard to attend due to the refusal of the Canadian government to provide visas in a timely manner, and that the taxes imposed on ICAO by Canada were too high.
According to the "Globe and Mail", the move to relocate the ICAO was at least partly motivated by the Pro-Israel foreign policy of Canadian Prime Minister Stephen Harper. Citing anonymous sources, the "Globe and Mail" reported that Arab ambassadors to the United Nations met in April 2013 in New York, where, among other things, they "devoted a section of their agenda to countering Canada, including mustering allies from other countries to vote against Ottawa in international organizations." It was also reported that "Some Arab countries are eyeing moves to back [Qatar] by campaigning to win the votes of other states." The Globe commented that "Arab nations already looking to deal a blow to Ottawa for its stand on Palestinian issues could wield influence if they united behind the ICAO campaign" and that "Losing ICAO's Montreal headquarters would be more than the diplomatic embarrassment the Harper Conservatives."
France, Britain and the United States announced that they opposed moving the organization.
Approximately one month later, Qatar withdrew its bid to move ICAO headquarters, meaning that the organization would remain in Montreal. Reportedly, Qatar's withdrawal came after a separate proposal to the ICAO's governing council to move the ICAO triennial conference to Doha was defeated by a vote of 22–14. According to French delegate Michel Wachenheim, "This conference of the general assembly was to be held in Montreal, as it always is … and twenty-two of the 36 said no, they thought that moving it (to Doha) four months before a general assembly was far too complicated." Wachenheim also stated that, "at our meeting this (Friday) morning, we learned that Qatar had withdrawn its offer (to move the HQ)."
Statute.
The 9th edition of the Convention on International Civil Aviation includes modifications from 1948 up to year 2006. ICAO refers to its current edition of the Convention as the "Statute", and designates it as ICAO Doc 7300/9. The Convention has 19 Annexes that are listed by title in the article Convention on International Civil Aviation.
Membership.
s of 2011[ [update]], there are 191 , consisting of 190 of the 193 UN members (all but Dominica, Liechtenstein, and Tuvalu), plus the Cook Islands.
Liechtenstein has delegated Switzerland to implement the treaty to make it applicable in the territory of Liechtenstein. 
Governing Council.
The Governing Council is elected every 3 years and consists of 36 members divided into 3 categories. The present Council was elected on 1 October 2013 at the 38th Assembly of ICAO at Montreal. The Structure of present Council is as follows:
PART I – (States of chief importance in air transport) – Australia, Brazil, Canada, China, France, Germany, Italy, Japan, Russian Federation, United Kingdom and the United States. All of them have been re-elected.
PART II – (States which make the largest contribution to the provision of facilities for international civil air navigation) – Argentina, Egypt, India, Mexico, Nigeria, Norway, Portugal, Saudi Arabia, Singapore, South Africa, Spain and Venezuela. Except Norway, Portugal and Venezuela, all others have been re-elected.
PART III– (States ensuring geographic representation)- Bolivia, Burkina Faso, Cameroon, Chile, Dominican Republic, Kenya, Libya, Malaysia, Nicaragua, Poland, Republic of Korea, United Arab Emirates and United Republic of Tanzania. Bolivia, Chile, Dominican Republic, Kenya, Libya, Nicaragua, Poland and United Republic of Tanzania have been elected for the first time.
Standards.
ICAO also standardizes certain functions for use in the airline industry, such as the Aeronautical Message Handling System (AMHS). This makes it a standards organization.
Each country should have an accessible Aeronautical Information Publication (AIP), based on standards defined by ICAO, containing information essential to air navigation. Countries are required to update their AIP manuals every 28 days and so provide definitive regulations, procedures and information for each country about airspace and airports. ICAO's standards also dictate that temporary hazards to aircraft are regularly published using NOTAMs.
ICAO defines an International Standard Atmosphere (also known as ICAO Standard Atmosphere), a model of the standard variation of pressure, temperature, density, and viscosity with altitude in the Earth's atmosphere. This is useful in calibrating instruments and designing aircraft.
ICAO standardizes machine-readable passports worldwide. Such passports have an area where some of the information otherwise written in textual form is written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition. This enables border controllers and other law enforcement agents to process such passports quickly, without having to input the information manually into a computer. ICAO publishes Doc 9303 "Machine Readable Travel Documents", the technical standard for machine-readable passports. A more recent standard is for biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smartcards. Like some smartcards, the passport book design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.
ICAO is active in infrastructure management, including Communication, Navigation, Surveillance / Air Traffic Management (CNS/ATM) systems, which employ digital technologies (like satellite systems with various levels of automation) in order to maintain a seamless global air traffic management system.
Registered codes.
Both ICAO and IATA have their own airport and airline code systems.
ICAO uses 4-letter airport codes (vs. IATA's 3-letter codes). The ICAO code is based on the region and country of the airport—for example, Charles de Gaulle Airport has an ICAO code of LFPG, where L indicates Southern Europe, F, France, PG, Paris de Gaulle, while Orly Airport has the code LFPO (the 3rd letter sometimes refers to the particular flight information region (FIR) or the last two may be arbitrary). In most of the world, ICAO and IATA codes are unrelated; for example, Charles de Gaulle Airport has an IATA code of CDG and Orly, ORY. However, the location prefix for continental United States is K and ICAO codes are usually the IATA code with this prefix. For example, the ICAO code for Los Angeles International Airport is KLAX. Canada follows a similar pattern, where a prefix of C is usually added to an IATA code to create the ICAO code. For example, Edmonton International Airport is YEG or CYEG. (In contrast, airports in Hawaii are in the Pacific region and so have ICAO codes that start with PH; Kona International Airport's code is PHKO.) Note that not all airports are assigned codes in both systems; for example, airports that do not have airline service do not need an IATA code.
ICAO also assigns 3-letter airline codes (versus the more-familiar 2-letter IATA codes—for example, UAL vs. UA for United Airlines). ICAO also provides telephony designators to aircraft operators worldwide, a one- or two-word designator used on the radio, usually, but not always, similar to the aircraft operator name. For example, the identifier for Japan Airlines International is JAL and the designator is Japan Air, but Aer Lingus is EIN and Shamrock. Thus, a Japan Airlines flight numbered 111 would be written as "JAL111" and pronounced "Japan Air One One One" on the radio, while a similarly numbered Aer Lingus would be written as "EIN111" and pronounced "Shamrock One One One". In the US, FAA practices require the digits of the flight number to be spoken in group format ("Japan Air One Eleven" in the above example) while individual digits are used for the aircraft tail number used for unscheduled civil flights. 
ICAO maintains the standards for aircraft registration ("tail numbers"), including the alphanumeric codes that identify the country of registration. For example, airplanes registered in the United States have tail numbers starting with N.
ICAO is also responsible for issuing alphanumeric aircraft type codes containing two to four characters. These codes provide the identification that is typically used in flight plans. The Boeing 747 would use B741, B742, B743, etc., depending on the particular variant.
Regions and regional offices.
ICAO has a headquarters, seven regional offices, and one regional sub-office :
ICAO and climate change.
Emissions from international aviation are specifically excluded from the targets agreed under the Kyoto Protocol. Instead, the Protocol invites developed countries to pursue the limitation or reduction of emissions through the International Civil Aviation Organization. ICAO's environmental committee continues to consider the potential for using market-based measures such as trading and charging, but this work is unlikely to lead to global action. It is currently developing guidance for states who wish to include aviation in an emissions trading scheme (ETS) to meet their Kyoto commitments, and for airlines who wish to participate voluntarily in a trading scheme.
Emissions from domestic aviation are included within the Kyoto targets agreed by countries. This has led to some national policies such as fuel and emission taxes for domestic air travel in the Netherlands and Norway, respectively. Although some countries tax the fuel used by domestic aviation, there is no duty on kerosene used on international flights.
ICAO is currently opposed to the inclusion of aviation in the European Union Emission Trading Scheme (EU ETS). The EU, however, is pressing ahead with its plans to include aviation.
Investigations of air disasters.
Most air accident investigations are carried out by an agency of a country that is associated in some way with the accident. For example, the Air Accidents Investigation Branch conducts accident investigations on behalf of the British Government. ICAO has conducted three investigations involving air disasters, of which two were passenger airliners shot down while in international flight over hostile territory.

</doc>
<doc id="14986" url="http://en.wikipedia.org/wiki?curid=14986" title="International Maritime Organization">
International Maritime Organization

The International Maritime Organization (IMO), known as the Inter-Governmental Maritime Consultative Organization (IMCO) until 1982, was established in Geneva in 1948 and came into force ten years later, meeting for the first time in 1959.
Headquartered in London, United Kingdom, the IMO is a specialised agency of the United Nations with 171 Member States and three Associate Members. The IMO's primary purpose is to develop and maintain a comprehensive regulatory framework for shipping and its remit today includes safety, environmental concerns, legal matters, technical co-operation, maritime security and the efficiency of shipping. IMO is governed by an Assembly of members and is financially administered by a Council of members elected from the Assembly. The work of IMO is conducted through five committees and these are supported by technical subcommittees. Member organisations of the UN organizational family may observe the proceedings of the IMO. Observer status is granted to qualified non-governmental organisations.
IMO is supported by a permanent secretariat of employees who are representative of its members. The secretariat is composed of a Secretary-General who is periodically elected by the Assembly, and various divisions such as those for marine safety, environmental protection, and a conference section.
History.
Inter-Governmental Maritime Consultative Organization (IMCO) was formed to fulfill a desire to bring the regulation of the safety of shipping into an international framework, for which the creation of the United Nations provided an opportunity. Hitherto such international conventions had been initiated piecemeal, notably the Safety of Life at Sea Convention (SOLAS), first adopted in 1914 following the "Titanic" disaster. IMCO's first task was to update that Convention; the resulting 1960 Convention was subsequently recast and updated in 1974 and it is that Convention that has been subsequently modified and updated to adapt to changes in safety requirements and technology.
According to Master Mariner John Christianson of the Massachusetts Maritime Academy, when IMCO began its operations in 1958 certain other pre-existing instruments were brought under its aegis, most notable the International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) 1954. Throughout its existence IMCO, renamed the IMO in 1982, has continued to produce new and updated instruments across a wide range of maritime issues covering not only safety of life and marine pollution but also encompassing safe navigation, search and rescue, wreck removal, tonnage measurement, liability and compensation, ship recycling, the training and certification of seafarers, and piracy. More recently SOLAS has been amended to bring an increased focus on maritime security through the International Ship and Port Facility Security (ISPS) Code. The IMO has also increased its focus on air emissions from ships.
In 1983 the IMO established the World Maritime University in Malmö, Sweden.
Membership.
To become a member of the IMO, a state ratifies a multilateral treaty known as the Convention on the International Maritime Organization. As of 2014, there are 171 member states of the IMO, which includes 170 of the UN members and the Cook Islands. The first state to ratify the convention was the United Kingdom in 1949. The most recent member to join was Zambia, which became an IMO member in 2014.
Associate members: Faroe Islands, Hong Kong and Macao.
UN member states that are not members of IMO are generally landlocked countries, including: Afghanistan, Andorra, Armenia, Belarus, Bhutan, Botswana, Burkina Faso, Burundi, Central African Republic, Chad, Kyrgyzstan, Laos, Lesotho, Liechtenstein, Mali, Federated States of Micronesia, Nauru, Niger, Rwanda, South Sudan, Swaziland, Tajikistan, and Uzbekistan.
Structure.
The Organization consists of an Assembly, a Council and five main Committees: the Maritime Safety Committee; the Marine Environment Protection Committee; the Legal Committee; the Technical Co-operation Committee and the Facilitation Committee. A number of Sub-Committees support the work of the main technical committees.
Legal instruments.
IMO is the source of approximately 60 legal instruments that guide the regulatory development of its member states to improve safety at sea, facilitate trade among seafaring states and protect the maritime environment. The most well known is the International Convention for the Safety of Life at Sea (SOLAS), as well as International Convention on Oil Pollution Preparedness, Response and Co-operation (OPRC). Others include the International Oil Pollution Compensation Funds. It also functions as a depository of yet to be ratified treaties, such as the International Convention on Liability and Compensation for Damage in Connection with the Carriage of Hazardous and Noxious Substances by Sea, 1996 (HNS Convention) and Nairobi International Convention of Removal of Wrecks (2007).
IMO regularly enacts regulations, which are broadly enforced by national and local maritime authorities in member countries, such as the International Regulations for Preventing Collisions at Sea (COLREG). The IMO has also enacted a Port State Control (PSC) authority, allowing domestic maritime authorities such as coast guards to inspect foreign-flag ships calling at ports of the many port states. Memoranda of Understanding (protocols) were signed by some countries unifying Port State Control procedures among the signatories.
Current issues.
Recent initiatives at the IMO have included amendments to SOLAS, which upgraded fire protection standards on passenger ships, the International Convention on Standards of Training, Certification and Watchkeeping for Seafarers (STCW) which establishes basic requirements on training, certification and watchkeeping for seafarers and to the Convention on the Prevention of Maritime Pollution (MARPOL 73/78), which required double hulls on all tankers.
In December 2002, new amendments to the 1974 SOLAS Convention were enacted. These amendments gave rise to the International Ship and Port Facility Security (ISPS) Code, which went into effect on 1 July 2004. The concept of the code is to provide layered and redundant defences against smuggling, terrorism, piracy, stowaways, etc. The ISPS Code required most ships and port facilities engaged in international trade to establish and maintain strict security procedures as specified in ship and port specific Ship Security Plans and Port Facility Security Plans.
The IMO is also responsible for publishing the International Code of Signals for use between merchant and naval vessels.
The First Intersessional Meeting of IMO's Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway (23–27 June 2008), tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC).
IMO is harmonising information available to seafarers and shore-side traffic services called e-Navigation. An e-Navigation strategy was ratified in 2005, and an implementation plan is being developed through three IMO sub-committees. The plan is expected to be completed in 2012.
IMO has also served as a key partner and enabler of US international and interagency efforts to establish Maritime Domain Awareness.
Governance of IMO.
The governing body of the International Maritime Organization is the Assembly which meets every two years. In between Assembly sessions a Council, consisting of 40 Member States elected by the Assembly, acts as the governing body. The technical work of the International Maritime Organization is carried out by a series of Committees. The Secretariat consists of some 300 international civil servants headed by a Secretary-General.
Secretary-General.
The current Secretary-General is Koji Sekimizu (Japan), elected for a four-year term at the 106th session of the IMO Council in June 2011 and at the 27th session of the IMO's Assembly in November 2011. His mandate started on 1 January 2012.
Previous Secretaries-General:
Technical committees.
The technical work of the International Maritime Organisation is carried out by a series of Committees. This includes:
Maritime Safety Committee.
It is regulated in the Article 28(a) of the Convention on the IMO:
ARTICLE 28
(a) The Maritime Safety Committee shall consider any matter within the scope of the Organization concerned with aids to navigation, construction and equipment of vessels, manning from a safety standpoint, rules for the prevention of collisions, handling of dangerous cargoes, maritime safety procedures and requirements, hydrographic information, log-books and navigational records, marine casualty investigation, salvage and rescue, and any other matters directly affecting maritime safety.
(b) The Maritime Safety Committee shall provide machinery for performing any duties assigned to it by this Convention, the Assembly or the Council, or any duty within the scope of this Article which may be assigned to it by or under any other international instrument and accepted by the Organization.
(c) Having regard to the provisions of Article 25, the Maritime Safety Committee, upon request by the Assembly or the Council or, if it deems such action useful in the interests of its own work, shall maintain such close relationship with other bodies as may further the purposes of the Organization
The Maritime Safety Committee is the most senior of these and is the main Technical Committee; it oversees the work of its nine sub-committees and initiates new topics. One broad topic it deals with is the effect of the human element on casualties; this work has been put to all of the sub-committees, but meanwhile, the Maritime Safety Committee has developed a code for the management of ships which will ensure that agreed operational procedures are in place and followed by the ship and shore-side staff.
Sub-Committees.
The MSC and MEPC are assisted in their work by a number of sub-committees which are open to all Member States:
Until 2013 there were nine Sub-Committees as follows:
Resolutions.
Resolution MSC.255(84), of 16 May 2008, adopts the "Code of the International Standards and Recommended Practices for a Safety Investigation into a Marine Casualty or Marine Incident". It is also known as the Casualty Investigation Code.

</doc>
<doc id="14987" url="http://en.wikipedia.org/wiki?curid=14987" title="International Labour Organization">
International Labour Organization

The International Labour Organization (ILO) is a United Nations agency dealing with labour issues, particularly international labour standards, social protection, and work opportunities for all. 185 of the 193 UN member states are members of the ILO.
In 1969, the organization received the Nobel Peace Prize for improving peace among classes, pursuing decent work and justice for workers, and providing technical assistance to other developing nations.
The ILO registers complaints against entities that are violating international rules; however, it does not impose sanctions on governments.
Governance, organization, and membership.
Unlike other United Nations specialized agencies, the International Labour Organization has a tripartite governing structure – representing governments, employers, and workers (usually with a ratio of 2:1:1). The rationale behind the tripartite structure is the creation of free and open debate among governments and social partners.
The ILO secretariat (staff) is referred to as the International Labour Office.
Governing Body.
The Governing Body decides the agenda of the International Labour Conference, adopts the draft programme and budget of the organization for submission to the conference, elects the director-general, requests information from member states concerning labour matters, appoints commissions of inquiry and supervises the work of the International Labour Office.
Juan Somavía was the ILO's director-general from 1999 until October 2012, when Guy Ryder was elected as his replacement.
This guiding body is composed of 28 government representatives, 14 workers' representatives, and 14 employers' representatives.
Ten of the government seats are held by member states that are nations of "chief industrial importance," as first considered by an "impartial committee." The nations are Brazil, China, France, Germany, India, Italy, Japan, the Russian Federation, the United Kingdom and the United States. The terms of office are three years.
International Labour Conference.
The ILO organizes the International Labour Conference in Geneva every year in June, where conventions and recommendations are crafted and adopted. Also known as the parliament of Labour, the conference also makes decisions about the ILO's general policy, work programme and budget.
Each member state has four representatives at the conference: two government delegates, an employer delegate and a worker delegate. All of them have individual voting rights, and all votes are equal, regardless of the population of the delegate's member state. The employer and worker delegates are normally chosen in agreement with the "most representative" national organizations of employers and workers. Usually, the workers' delegates coordinate their voting, as do the employers' delegates. . All delegate have the same rights, and are not required to vote in blocs.
Conventions.
Through July 2011, the ILO has adopted 189 conventions. If these conventions are ratified by enough governments, they become in force. However, ILO conventions are considered international labour standards regardless of ratifications. When a convention comes into force, it creates a legal obligation for ratifying nations to apply its provisions.
Every year the International Labour Conference's Committee on the Application of Standards examines a number of alleged breaches of international labour standards. Governments are required to submit reports detailing their compliance with the obligations of the conventions they have ratified. Conventions that have not been ratified by member states have the same legal force as do recommendations.
In 1998, the 86th International Labour Conference adopted the "Declaration on Fundamental Principles and Rights at Work". This declaration contains four fundamental policies:
The ILO asserts that its members have an obligation to work towards fully respecting these principles, embodied in relevant ILO Conventions. The ILO Conventions which embody the fundamental principles have now been ratified by most member states.
Recommendations.
Recommendations do not have the binding force of conventions and are not subject to ratification. Recommendations may be adopted at the same time as conventions to supplement the latter with additional or more detailed provisions. In other cases recommendations may be adopted separately and may address issues separate from particular conventions.
Membership.
As of 2013, 185 of the 193 member states of the United Nations are members of the ILO. The UN member states which are not members of the ILO are Andorra, Bhutan, Liechtenstein, Micronesia, Monaco, Nauru, North Korea and Tonga.
The ILO constitution permits any member of the UN to become a member of the ILO. To gain membership, a nation must inform the Director-General that it accepts all the obligations of the ILO constitution.
Members of the ILO under the League of Nations automatically became members when the organization's new constitution came into effect after World War II. In addition, any original member of the United Nations and any state admitted to the U.N. thereafter may join. Other states can be admitted by a two-thirds vote of all delegates, including a two-thirds vote of government delegates, at any ILO General Conference.
Position within the UN.
The International Labour Organization (ILO) is a specialized agency of the United Nations (UN). As with other UN specialized agencies (or programmes) working on international development, the ILO is also a member of the United Nations Development Group.
History.
Origins.
While the ILO was established as an agency of the League of Nations following World War I, its founders had made great strides in social thought and action before 1919. The core members all knew one another from earlier private professional and ideological networks, in which they exchanged knowledge, experiences, and ideas on social policy. Prewar "epistemic communities", such as the International Association for Labour Legislation (IALL), founded in 1900, and political networks, such as the Socialist Second International, were a decisive factor in the institutionalization of international labour politics.
In the post–World War I euphoria, the idea of a "makeable society" was an important catalyst behind the social engineering of the ILO architects. As a new discipline, international labour law became a useful instrument for putting social reforms into practice. The utopian ideals of the founding members—social justice and the right to decent work—were changed by diplomatic and political compromises made at the Paris Peace Conference of 1919, showing the ILO's balance between idealism and pragmatism.
Over the course of the First World War, the international labour movement proposed a comprehensive programme of protection for the working classes, conceived as compensation for labour's support during the war. Post-war reconstruction and the protection of labour unions occupied the attention of many nations during and immediately after World War I. In Great Britain, the Whitley Commission, a subcommittee of the Reconstruction Commission, recommended in its July 1918 Final Report that "industrial councils" be established throughout the world. The British Labour Party had issued its own reconstruction programme in the document titled "Labour and the New Social Order". In February 1918, the third Inter-Allied Labour and Socialist Conference (representing delegates from Great Britain, France, Belgium and Italy) issued its report, advocating an international labour rights body, an end to secret diplomacy, and other goals. And in December 1918, the American Federation of Labor (AFL) issued its own distinctively apolitical report, which called for the achievement of numerous incremental improvements via the collective bargaining process.
IFTU Bern Conference.
As the war drew to a close, two competing visions for the post-war world emerged. The first was offered by the International Federation of Trade Unions (IFTU), which called for a meeting in Bern, Switzerland, in July 1919. The Bern meeting would consider both the future of the IFTU and the various proposals which had been made in the previous few years. The IFTU also proposed including delegates from the Central Powers as equals. Samuel Gompers, president of the AFL, boycotted the meeting, wanting the Central Powers delegates in a subservient role as an admission of guilt for their countries' role in the bringing about war. Instead, Gompers favoured a meeting in Paris which would only consider President Woodrow Wilson's Fourteen Points as a platform. Despite the American boycott, the Bern meeting went ahead as scheduled. In its final report, the Bern Conference demanded an end to wage labour and the establishment of socialism. If these ends could not be immediately achieved, then an international body attached to the League of Nations should enact and enforce legislation to protect workers and trade unions.
Commission on International Labour Legislation.
Meanwhile, the Paris Peace Conference sought to dampen public support for communism. Subsequently, the Allied Powers agreed that clauses should be inserted into the emerging peace treaty protecting labour unions and workers' rights, and that an international labour body be established to help guide international labour relations in the future. The advisory Commission on International Labour Legislation was established by the Peace Conference to draft these proposals. The Commission met for the first time on 1 February 1919, and Gompers was elected chairman.
Two competing proposals for an international body emerged during the Commission's meetings. The British proposed establishing an international parliament to enact labour laws which each member of the League would be required to implement. Each nation would have two delegates to the parliament, one each from labour and management. An international labour office would collect statistics on labour issues and enforce the new international laws. Philosophically opposed to the concept of an international parliament and convinced that international standards would lower the few protections achieved in the United States, Gompers proposed that the international labour body be authorized only to make recommendations, and that enforcement be left up to the League of Nations. Despite vigorous opposition from the British, the American proposal was adopted.
Gompers also set the agenda for the draft charter protecting workers' rights. The Americans made 10 proposals. Three were adopted without change: That labour should not be treated as a commodity; that all workers had the right to a wage sufficient to live on; and that women should receive equal pay for equal work. A proposal protecting the freedom of speech, press, assembly, and association was amended to include only freedom of association. A proposed ban on the international shipment of goods made by children under the age of 16 was amended to ban goods made by children under the age of 14. A proposal to require an eight-hour work day was amended to require the eight-hour work day "or" the 40-hour work week (an exception was made for countries where productivity was low). Four other American proposals were rejected. Meanwhile, international delegates proposed three additional clauses, which were adopted: One or more days for weekly rest; equality of laws for foreign workers; and regular and frequent inspection of factory conditions.
The Commission issued its final report on 4 March 1919, and the Peace Conference adopted it without amendment on 11 April. The report became Part XIII of the Treaty of Versailles.
Interwar period.
The first annual conference (referred to as the International Labour Conference, or ILC) began on 29 October 1919 at the Pan American Union (building) in Washington, D.C. and adopted the first six International Labour Conventions, which dealt with hours of work in industry, unemployment, maternity protection, night work for women, minimum age,night work for young persons in industry. The prominent French socialist Albert Thomas became its first Director General.
Despite open disappointment and sharp critique, the revived International Federation of Trade Unions (IFTU) quickly adapted itself to this mechanism. The IFTU increasingly oriented its international activities around the lobby work of the ILO.
At the time of establishment, the U.S. government was not a member of ILO, as the US Senate rejected the Covenant of the League of Nations, and the United States could not join any of its agencies. Following the election of Franklin Delano Roosevelt to the U.S. presidency, the new administration made renewed efforts to join the ILO even without League membership. On 19 June 1934, the U.S. Congress passed a joint resolution authorizing the President to join ILO without joining the League of Nations as a whole. On 22 June 1934, the ILO adopted a resolution inviting the U.S. government to join the organization. On 20 August 1934, the U.S. government responded positively and took its seat at the ILO.
Wartime and the United Nations.
During the Second World War, when Switzerland was surrounded by German troops, ILO Director John G. Winant made the decision to leave Geneva. In August 1940, the Government of Canada officially invited the ILO to be housed at McGill University in Montreal. Forty staff members were transferred to the temporary offices and continued to work from McGill until 1948.
The ILO became the first specialized agency of the United Nations system after the demise of the League in 1946. Its constitution, as amended, includes the Declaration of Philadelphia (1944) on the aims and purposes of the organization.
Cold War era.
In July 1970, the United States withdrew 50% of its financial support to the ILO following the appointment of an Assistant-Director General from the Soviet Union. This appointment (by the ILO's British Director-General, C. Wilfred Jenks) drew particular criticism from AFL-CIO president George Meany and from Congressman John E. Rooney. However, the funds were eventually paid.
On 12 June 1975, the ILO voted to grant the Palestinian Liberation Organization observer status at its meetings. Representatives of the United States and Israel walked out of the meeting. The U.S. House of Representatives subsequently decided to withhold funds. The United States gave notice of full withdrawal on 6 November 1975, stating that the organization had become politicized. The United States also suggested that representation from communist countries was not truly "tripartite"—including government, workers, and employers—because of the structure of these economies. The withdrawal became effective on 1 November 1977.
The United States returned to the organization in 1980 after extracting some concessions from the organization. It was partly responsible for the ILO's shift away from a human rights approach and towards support for the Washington Consensus. Economist Guy Standing wrote "the ILO quietly ceased to be an international body attempting to redress structural inequality and became one promoting employment equity".
Programs.
Labour statistics.
The ILO is a major provider of labour statistics. Labour statistics are an important tool for its member states to monitor their progress toward improving labour standards. As part of their statistical work, ILO maintains several databases. This database covers 11 major data series for over 200 countries. In addition, ILO publishes a number of compilations of labour statistics, such as the (KILM). KILM covers 20 main indicators on labour participation rates, employment, unemployment, educational attainment, labour cost, and economic performance. Many of these indicators have been prepared by other organizations. For example, the Division of International Labour Comparisons of the U.S. Bureau of Labor Statistics prepares the hourly compensation in manufacturing indicator.
The U.S. Department of Labor also publishes a yearly report containing a "List of Goods Produced by Child Labor or Forced Labor" issued by the Bureau of International Labor Affairs. The December 2014 updated edition of the report listed a total of 74 countries and 136 goods.
Training and teaching units.
The International Training Centre of the International Labour Organization (ITCILO) is based in Turin, Italy. Together with the University of Turin, Faculty of Law, the ITC offers training for ILO officers and secretariat members, as well as offering educational programmes. For instance, the ITCILO offers a Master of Laws (LL.M.) programme in Management of Development, which aims specialize professionals in the field of cooperation and development.
Child labour.
The term child labour is often defined as work that deprives children of their childhood, potential, dignity, and is harmful to their physical and mental development.
Child labour refers to work that:
In its most extreme forms, child labour involves children being enslaved, separated from their families, exposed to serious hazards and illnesses and/or left to fend for themselves on the streets of large cities – often at a very early age. Whether or not particular forms of "work" can be called child labour depends on the child's age, the type and hours of work performed, the conditions under which it is performed and the objectives pursued by individual countries. The answer varies from country to country, as well as among sectors within countries. 
Not all work done by children falls under the classification of child labour and therefore should not be so readily targeted for elimination. Children's or adolescents' participation in work that does not negatively affect their health and personal development or interfere with their schooling, is generally regarded as being something positive. This includes activities such as helping their parents around the home, assisting in a family business or earning pocket money outside school hours and during school holidays. These kinds of activities contribute to children's development and to the welfare of their families; they provide them with skills and experience, and help to prepare them to be productive members of society during their adult life.
ILO's response to child labour.
The ILO's International Programme on the Elimination of Child Labour (IPEC) was created in 1992 with the overall goal of the progressive elimination of child labour, which was to be achieved through strengthening the capacity of countries to deal with the problem and promoting a worldwide movement to combat child labour. IPEC currently has operations in 88 countries, with an annual expenditure on technical cooperation projects that reached over US$74 million, €50 million in 2006. It is the largest programme of its kind globally and the biggest single operational programme of the ILO.
The number and range of IPEC's partners have expanded over the years and now include employers' and workers' organizations, other international and government agencies, private businesses, community-based organizations, NGOs, the media, parliamentarians, the judiciary, universities, religious groups and, of course, children and their families.
IPEC's work to eliminate child labour is an important facet of the ILO's Decent Work Agenda. Child labour not only prevents children from acquiring the skills and education they need for a better future, it also perpetuates poverty and affects national economies through losses in competitiveness, productivity and potential income.
ILO's Exceptions in Indigenous Communities.
Because of different cultural views involving labor, the International Labour Organization (ILO) developed a series of culturally sensitive mandates including Conventions No. 169, 107, 138, and 182 to protect indigenous culture, traditions, and identities. Conventions No. 138 and 182 lead in the fight against child labour, while No. 107 and 169 promote the right of indigenous and tribal peoples and protect their right to define their own developmental priorities. The ILO recognizes these changes are necessary to respect the culture and traditions of other communities while also looking after the welfare of children.
In many indigenous communities, parents believe children learn important life lessons through the act of work and through the participation in daily life. Working is seen as a learning process preparing children of the future tasks they will eventually have to do as an adult. It is a belief that the family's and child well-being and survival is a shared responsibility between members of the whole family. They also see work as an intrinsic part of their child's developmental process. While these attitudes toward child work remain, many children and parents from indigenous communities still highly value education. ILO wants to include these communities in the fight against exploitative child labor while being sensitive to their traditions and values.
Issues.
Forced labour.
The ILO has considered the fight against forced labour to be one of its main priorities. During the interwar years, the issue was mainly considered a colonial phenomenon, and the ILO's concern was to establish minimum standards protecting the inhabitants of colonies from the worst abuses committed by economic interests. After 1945, the goal became to set a uniform and universal standard, determined by the higher awareness gained during World War II of politically and economically motivated systems of forced labour, but debates were hampered by the Cold War and by exemptions claimed by colonial powers. Since the 1960s, declarations of labour standards as a component of human rights have been weakened by government of postcolonial countries claiming a need to exercise extraordinary powers over labour in their role as emergency regimes promoting rapid economic development.
In June 1998 the International Labour Conference adopted a Declaration on Fundamental Principles and Rights at Work and its Follow-up that obligates member States to respect, promote and realize freedom of association and the right to collective bargaining, the elimination of all forms of forced or compulsory labour, the effective abolition of child labour, and the elimination of discrimination in respect of employment and occupation.
With the adoption of the Declaration, the International Labour Organization (ILO) created the InFocus Programme on Promoting the Declaration which is responsible for the reporting processes and technical cooperation activities associated with the Declaration; and it carries out awareness raising, advocacy and knowledge functions.
In November 2001, following the publication of the in Focus Programme's first Global Report on forced labour, the ILO Governing Body created a , as part of broader efforts to promote the 1998 Declaration on Fundamental Principles and Rights at Work and its Follow-up.
Since its inception, SAP-FL has focused on raising global awareness of forced labour in its different forms, and mobilising action against its manifestation. Several thematic and country-specific studies and surveys have since been undertaken, on such diverse aspects of forced labour as bonded labour, human trafficking, forced domestic work, rural servitude, and forced prison labour.
The Special Action Programme to combat Forced Labour (SAP-FL) has spearheaded the ILO's work in this field since early 2002. The programme is designed to:
Minimum wage law.
To protect the right of labours for fixing minimum wage, ILO has created Minimum Wage-Fixing Machinery Convention, 1928, Minimum Wage Fixing Machinery (Agriculture) Convention, 1951 and Minimum Wage Fixing Convention, 1970 as minimum wage law.
HIV/AIDS.
The International Labour Organization (ILO) is the lead UN-agency on HIV workplace policies and programmes and private sector mobilization. The ILO recognizes that HIV has a potentially devastating impact on labour and productivity and represents an enormous burden for working people, their families and communities. ILOAIDS is the branch of the ILO dedicated to this issue.
The ILO has been involved with the HIV response since 1998. In June 2001, the ILO's Governing Body adopted a pioneering Code of Practice on HIV/AIDS and the World of Work, which was launched during a special session of the UN General Assembly.
The same year, ILO became a cosponsor of the Joint United Nations Programme on HIV/AIDS (UNAIDS).
In 2010, the 99th International Labour Conference adopted the ILO's Recommendation concerning HIV and AIDS and the World of Work, 2010 (No. 200), the first international labour standard on HIV and AIDS. The Recommendation lays out a comprehensive set of principles to protect the rights of HIV-positive workers and their families, while scaling up prevention in the workplace. Working under the theme of 'Preventing HIV, Protecting Human Rights at Work,' ILOAIDS undertakes a range of policy advisory, research and technical support functions in the area of HIV and AIDS and the world of work. ILO also works on promoting social protection as a means of reducing vulnerability to HIV and mitigating its impact on those living with or affected by HIV.
ILOAIDS is currently engaged in the "Getting to Zero" campaign to arrive at zero new infections, zero AIDS-related deaths and zero-discrimination by 2015. Building on this campaign, ILOAIDS is executing a programme of voluntary and confidential counselling and testing at work, known as VCT@WORK.
Migrant workers.
As the word "migrant" suggests, migrant workers refer to those who moves from place to place to do their job. For the rights of migrant workers, ILO has adopted conventions, including Migrant Workers (Supplementary Provisions) Convention, 1975 and United Nations Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families in 1990.
Domestic workers.
Domestic workers are those who perform a variety of tasks for and in other peoples' homes. For example, they may cook / clean the house and look after children. Yet they are often the ones with the least consideration, excluded from labour and social protection. This is mainly due to the fact that women have traditionally carried out the tasks without pay. For the rights and decent work of domestic workers including migrant domestic workers, ILO has adopted Convention on domestic workers on 16 June 2011.
ILO and globalization.
Seeking a process of globalization that is inclusive, democratically governed and provides opportunities and tangible benefits for all countries and people. The World Commission on the Social Dimension of Globalization was established by the ILO's Governing Body in February 2002 at the initiative of the Director-General in response to the fact that there did not appear to be a space within the multilateral system that would cover adequately and comprehensively the social dimension of the various aspects of globalization. The World Commission Report, A Fair Globalization: Creating Opportunities for All, is the first attempt at structured dialogue among representatives of constituencies with different interests and opinions on the social dimension of globalization, aimed at finding common ground on one of the most controversial and divisive subjects of our time.

</doc>
<doc id="14990" url="http://en.wikipedia.org/wiki?curid=14990" title="IMO">
IMO

IMO.
IMO or Imo may refer to:

</doc>
